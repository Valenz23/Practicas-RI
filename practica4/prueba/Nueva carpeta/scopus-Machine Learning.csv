Authors,Title,Year,Source title,Cited by,Link,Abstract,Author Keywords,Index Keywords,EID
"Ramírez-Gallego S., Fernández A., García S., Chen M., Herrera F.","Big Data: Tutorial and guidelines on information and process fusion for analytics algorithms with MapReduce",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496391&doi=10.1016%2fj.inffus.2017.10.001&partnerID=40&md5=6a693a569a782eb3fac76e4ac7be22f1","We live in a world were data are generated from a myriad of sources, and it is really cheap to collect and storage such data. However, the real benefit is not related to the data itself, but with the algorithms that are capable of processing such data in a tolerable elapse time, and to extract valuable knowledge from it. Therefore, the use of Big Data Analytics tools provide very significant advantages to both industry and academia. The MapReduce programming framework can be stressed as the main paradigm related with such tools. It is mainly identified by carrying out a distributed execution for the sake of providing a high degree of scalability, together with a fault-tolerant scheme. In every MapReduce algorithm, first local models are learned with a subset of the original data within the so-called Map tasks. Then, the Reduce task is devoted to fuse the partial outputs generated by each Map. The ways of designing such fusion of information/models may have a strong impact in the quality of the final system. In this work, we will enumerate and analyze two alternative methodologies that may be found both in the specialized literature and in standard Machine Learning libraries for Big Data. Our main objective is to provide an introduction of the characteristics of these methodologies, as well as giving some guidelines for the design of novel algorithms in this field of research. Finally, a short experimental study will allow us to contrast the scalability issues for each type of process fusion in MapReduce for Big Data Analytics. © 2017 Elsevier B.V.","Big Data Analytics; Information fusion; Machine learning; MapReduce; Spark","Artificial intelligence; Data handling; Digital storage; Electric sparks; Information fusion; Learning systems; Scalability; Data analytics; Degree of scalability; Fault tolerant schemes; Map-reduce; Map-reduce programming; Novel algorithm; Scalability issue; Standard machines; Big data",2-s2.0-85031496391
"Greener J.G., Sternberg M.J.","Structure-based prediction of protein allostery",2018,"Current Opinion in Structural Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032200693&doi=10.1016%2fj.sbi.2017.10.002&partnerID=40&md5=c72b71b894e890080e9cb7ade6be43d8","Allostery is the functional change at one site on a protein caused by a change at a distant site. In order for the benefits of allostery to be taken advantage of, both for basic understanding of proteins and to develop new classes of drugs, the structure-based prediction of allosteric binding sites, modulators and communication pathways is necessary. Here we review the recently emerging field of allosteric prediction, focusing mainly on computational methods. We also describe the search for cryptic binding pockets and attempts to design allostery into proteins. The development and adoption of such methods is essential or the long-preached potential of allostery will remain elusive. © 2017 The Authors",,"allosterism; binding site; client server application; conformational transition; cryoelectron microscopy; crystallography; enzyme active site; ligand binding; machine learning; molecular dynamics; nuclear magnetic resonance; prediction; priority journal; protein conformation; protein protein interaction; protein structure; Review; site directed mutagenesis",2-s2.0-85032200693
"Carcillo F., Dal Pozzolo A., Le Borgne Y.-A., Caelen O., Mazzer Y., Bontempi G.","SCARFF: A scalable framework for streaming credit card fraud detection with spark",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029580394&doi=10.1016%2fj.inffus.2017.09.005&partnerID=40&md5=f4bd2ce7959cf01ffd4a285eb36b10d0","The expansion of the electronic commerce, together with an increasing confidence of customers in electronic payments, makes of fraud detection a critical factor. Detecting frauds in (nearly) real time setting demands the design and the implementation of scalable learning techniques able to ingest and analyse massive amounts of streaming data. Recent advances in analytics and the availability of open source solutions for Big Data storage and processing open new perspectives to the fraud detection field. In this paper we present a Scalable Real-time Fraud Finder (SCARFF) which integrates Big Data tools (Kafka, Spark and Cassandra) with a machine learning approach which deals with imbalance, nonstationarity and feedback latency. Experimental results on a massive dataset of real credit card transactions show that this framework is scalable, efficient and accurate over a big stream of transactions. © 2017 Elsevier B.V.","Big data; Cassandra; Fraud detection; Kafka; Machine learning; Scalable software; Spark; Streaming analytics","Artificial intelligence; Crime; Data handling; Digital storage; Electric sparks; Learning systems; Open source software; Cassandras; Credit card fraud detections; Credit card transactions; Fraud detection; Kafka; Learning techniques; Machine learning approaches; Open-source solutions; Big data",2-s2.0-85029580394
"Cvetković S., Stojanović M.B., Nikolić S.V.","Hierarchical ELM ensembles for visual descriptor fusion",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026740026&doi=10.1016%2fj.inffus.2017.07.003&partnerID=40&md5=6c9a353b60e104a399e7fcec7c224b03","Extreme Learning Machines (ELM) have been successfully applied to variety of classification problems by utilizing a single descriptor type. However, a single descriptor may be insufficient for the visual classification task, due to the high level of intra-class variability coupled with low inter-class distance. Although several studies have investigated methods for combining multiple descriptors by ELM, they predominantly apply a simple concatenation of descriptors before classifying them. This type of descriptor fusion may impose problems of descriptor compatibility, high dimensionality and restricted accuracy. In this paper, we propose a hierarchical descriptors fusion strategy at the decision level (“late-fusion”), which relies on ELM ensembles (ELM-E). The proposed method, denoted as H-ELM-E, effectively combines multiple complementary descriptors by a two-level ELM-E based architecture, which ensures that a more informative descriptors will gain more impact on the final decision. In the first level, a separate ELM-E classifier is trained for every image descriptor. In the second level, the output scores from the previous level are aggregated into the mid-level representation which is conducted to an additional ELM-E classifier. The exhaustive experimental evaluation confirmed that the proposed hierarchical ELM-E based strategy is superior to the single-descriptor methods as well as “early fusion” of multiple descriptors, for the visual classification task. Additionally, it was shown that significant accuracy improvement is achieved by integrating ensembles of ELM as a basic classifier, instead of using a single ELM. © 2017 Elsevier B.V.","Extreme Learning Machine; Feature fusion; Hierarchical classifiers; Scene classification","Knowledge acquisition; Accuracy Improvement; Experimental evaluation; Extreme learning machine; Feature fusion; Hierarchical classifiers; Mid-level representation; Scene classification; Visual classification; Learning systems",2-s2.0-85026740026
"Abu-Aisheh Z., Raveaux R., Ramel J.-Y., Martineau P.","A parallel graph edit distance algorithm",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032512961&doi=10.1016%2fj.eswa.2017.10.043&partnerID=40&md5=e0955d97039a6d8c020baa630dfa70fc","Graph edit distance (GED) has emerged as a powerful and flexible graph matching paradigm that can be used to address different tasks in pattern recognition, machine learning, and data mining. GED is an error-tolerant graph matching problem which consists in minimizing the cost of the sequence that transforms a graph into another by means of edit operations. Edit operations are deletion, insertion and substitution of vertices and edges. Each vertex/edge operation has its associated cost defined in the vertex/edge cost function. Unfortunately, Unfortunately, the GED problem is NP-hard. The question of elaborating fast and precise algorithms is of first interest. In this paper, a parallel algorithm for exact GED computation is proposed. Our proposal is based on a branch-and-bound algorithm coupled with a load balancing strategy. Parallel threads run a branch-and-bound algorithm to explore the solution space and to discard misleading partial solutions. In the mean time, the load balancing scheme ensures that no thread remains idle. Experiments on 4 publicly available datasets empirically demonstrated that under time constraints our proposal can drastically improve a sequential approach and a naive parallel approach. Our proposal was compared to 6 other methods and provided more precise solutions while requiring a low memory usage. © 2017 Elsevier Ltd","Graph edit distance; Graph matching; Load balancing; Parallel computing; Pattern recognition","Branch and bound method; Cost functions; Costs; Data mining; Learning systems; Parallel processing systems; Pattern recognition; Resource allocation; Branch-and-bound algorithms; Error-tolerant graph matching; Graph edit distance; Graph matchings; Load balancing strategy; Load-balancing schemes; Precise solutions; Sequential approach; Pattern matching",2-s2.0-85032512961
"García Nieto P.J., García-Gonzalo E., Álvarez Antón J.C., González Suárez V.M., Mayo Bayón R., Mateos Martín F.","A comparison of several machine learning techniques for the centerline segregation prediction in continuous cast steel slabs and evaluation of its performance",2018,"Journal of Computational and Applied Mathematics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015450075&doi=10.1016%2fj.cam.2017.02.031&partnerID=40&md5=efad53aa2f6ce1ab9aae8fb29ddb291f","Centerline segregation in steel cast products is an internal defect that can be very harmful when slabs are rolled in heavy plate mills. Consequently, anticipate its presence is a matter of importance to prevent future risks. The aim of this study was to obtain a predictive model able to perform an early detection of central segregation severity in continuous cast steel slabs. This study presents a novel hybrid algorithm, based on support vector machines (SVMs) in combination with the particle swarm optimization (PSO) technique, for predicting the centerline segregation from operation input parameters determined experimentally in continuous cast steel slabs. This optimization technique involves kernel parameter setting in the SVM training procedure, which significantly influences the regression accuracy. Additionally, a multilayer perceptron network (MLP) and a multivariate adaptive regression splines (MARS) approach, this last method also in combination with the particle swarm optimization (PSO) technique, were fitted to the experimental data with comparison purposes. To this end, the most important physical–chemical parameters of this industrial process are monitored and analyzed. The results of the present study are two-fold. In the first place, the significance of each physical–chemical variables on the segregation is presented through the model. Secondly, some models for predicting segregation are obtained with success. Indeed, regression with optimal hyperparameters was performed and coefficients of determination equal to 0.98 for continuity factor estimation and 0.97 for average width were obtained when this hybrid PSO–SVM-based model with RBF kernel function was applied to the experimental dataset, respectively. Furthermore, the results obtained with the MLP and PSO–MARS-based models are clearly worse than those obtained with the PSO–RBF–SVM-based model. The agreement between experimental data and the model confirmed the good performance of the latter. Finally, conclusions of this innovative research work are exposed. © 2017 Elsevier B.V.","Artificial neural networks (ANNs); Centerline segregation prediction; Continuous cast steel slabs; Multivariate adaptive regression splines (MARS); Particle swarm optimization (PSO); Support vector machines (SVMs)","Chemical analysis; Deep neural networks; Forecasting; Learning algorithms; Learning systems; Neural networks; Particle swarm optimization (PSO); Radial basis function networks; Regression analysis; Segregation (metallography); Slab mills; Splines; Steel metallurgy; Support vector machines; Cast steel; Centerline segregation; Machine learning techniques; Multilayer perceptron network (MLP); Multivariate adaptive regression splines; Particle swarm optimization technique; Physical-chemical parameters; Support vector machine (SVMs); Steel castings",2-s2.0-85015450075
"Rudiyanto, Minasny B., Setiawan B.I., Saptomo S.K., McBratney A.B.","Open digital mapping as a cost-effective method for mapping peat thickness and assessing the carbon stock of tropical peatlands",2018,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032299744&doi=10.1016%2fj.geoderma.2017.10.018&partnerID=40&md5=d0509e1b751612cf29f91ce6424db43e","Tropical peatland holds a large amount of carbon in the terrestrial ecosystem. Indonesia, responding to the global climate issues, has legislation on the protection and management of the peat ecosystem. However, this effort is hampered by the lack of fine-scale, accurate maps of peat distribution and its thickness. This paper presents an open digital mapping methodology, which utilises open data in an open-source computing environment, as a cost-effective method for mapping peat thickness and estimating carbon stock in Indonesian peatlands. The digital mapping methodology combines field observations with factors that are known to influence peat thickness distribution. These factors are represented by multi-source remotely-sensed data derived from open and freely available raster data: digital elevation models (DEM) from SRTM, geographical information, and radar images (Sentinel and ALOS PALSAR). Utilising machine-learning models from an open-source software, we derived spatial prediction functions and mapped peat thickness and its uncertainty at a grid resolution of 30 m. Peat volume can be calculated from the thickness map, and based on measurements of bulk density and carbon content, carbon stock for the area was estimated. The uncertainty of the estimates was calculated using error propagation rules. We demonstrated this approach in the eastern part of Bengkalis Island in Riau Province, covering an area around 50,000 ha. Results showed that digital mapping method can accurately predict the thickness of peat, explaining up to 98% of the variation of the data with a median relative error of 5% or an average error of 0.3 m. The accuracy of this method depends on the number of field observations. We provided an estimate of the cost and time required for map production, i.e. 2 to 4 months with a cost between $0.3 and $0.5/ha for an area of 50,000 ha. Obviously, there is a tradeoff between cost and accuracy. The advantages and limitations of the method were further discussed. The methodology provides a blueprint for a national-scale peat mapping. © 2017 Elsevier B.V.","Carbon stock; Climate change; Digital soil mapping; Machine-learning; Open-source; Tropical peatlands","Artificial intelligence; Climate change; Computer graphics; Cost effectiveness; Cost estimating; Costs; Ecology; Ecosystems; Errors; Information dissemination; Laws and legislation; Learning systems; Mapping; Open source software; Open systems; Peat; Software engineering; Soil surveys; Surveying; Tropics; Uncertainty analysis; Wetlands; Carbon stocks; Digital elevation model; Digital soil mappings; Geographical information; Machine learning models; Open sources; Peat land; Protection and management; Cost benefit analysis; bulk density; carbon sink; climate change; digital mapping; global climate; machine learning; mapping method; peatland; radar; raster; Indonesia; Riau",2-s2.0-85032299744
"Mondéjar-Guerra V., Garrido-Jurado S., Muñoz-Salinas R., Marín-Jiménez M.J., Medina-Carnicer R.","Robust identification of fiducial markers in challenging conditions",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032010901&doi=10.1016%2fj.eswa.2017.10.032&partnerID=40&md5=ddca8179a2b27077860cfa67c558098c","Many intelligent systems, such as assistive robots, augmented reality trainers or unmanned vehicles, need to know their physical location in the environment in order to fulfill their task. While relying exclusively on natural landmarks for that task is the preferred option, their use is somewhat limited because the proposed methods are complex, require high computational power, and are not reliable in all environments. On the other hand, artificial landmarks can be placed in order to alleviate these problems. In particular, square fiducial markers are one of the most popular tools for camera pose estimation due to their high performance and precision. However, the state-of-the-art methods still perform poorly under difficult image conditions, such as camera defocus, motion blur, small scale or non-uniform lighting. This paper proposes a method to robustly detect this type of landmarks under challenging image conditions present in realistic scenarios. To do so, we re-define the marker identification problem as a classification one based on state-of-the-art machine learning techniques. Second, we propose a procedure to create a training dataset of synthetically generated images affected by several challenging transformations. Third, we show that, in this problem, a classifier can be trained using exclusively synthetic data, performing well in real and challenging conditions. Different types of classifiers have been tested to prove the validity of our proposal (namely, Multilayer Perceptron (MLP), Convolutional Neural Network (CNN) and Support Vector Machine (SVM)), and statistical analyses have been performed in order to determine the best approach for our problem. Finally, the obtained classifiers have been compared to the ArUco and AprilTags fiducial marker systems in challenging video sequences. The results obtained show that the proposed method performs significantly better than previous approaches, making the use of this technology more reliable in a wider range of realistic scenarios such as outdoor scenes or fast moving cameras. © 2017 Elsevier Ltd","Augmented reality; Convolutional neural networks; Fiducial markers; Machine learning; Multilayer perceptron; Support vector machines","Artificial intelligence; Augmented reality; Cameras; Convolution; Intelligent robots; Intelligent systems; Learning systems; Multilayer neural networks; Multilayers; Neural networks; Unmanned vehicles; Camera pose estimation; Convolutional neural network; Fiducial marker; Fiducial marker systems; Machine learning techniques; Marker identifications; Multi layer perceptron; State-of-the-art methods; Support vector machines",2-s2.0-85032010901
"Pohl D., Bouchachia A., Hellwagner H.","Batch-based active learning: Application to social media data for crisis management",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032873671&doi=10.1016%2fj.eswa.2017.10.026&partnerID=40&md5=7eb701caaa5bf017150891e270ba83ad","Classification of evolving data streams is a challenging task, which is suitably tackled with online learning approaches. Data is processed instantly requiring the learning machinery to (self-)adapt by adjusting its model. However for high velocity streams, it is usually difficult to obtain labeled samples to train the classification model. Hence, we propose a novel online batch-based active learning algorithm (OBAL) to perform the labeling. OBAL is developed for crisis management applications where data streams are generated by the social media community. OBAL is applied to discriminate relevant from irrelevant social media items. An emergency management user will be interactively queried to label chosen items. OBAL exploits the boundary items for which it is highly uncertain about their class and makes use of two classifiers: k-Nearest Neighbors (kNN) and Support Vector Machine (SVM). OBAL is equipped with a labeling budget and a set of uncertainty strategies to identify the items for labeling. An extensive analysis is carried out to show OBAL's performance, the sensitivity of its parameters, and the contribution of the individual uncertainty strategies. Two types of datasets are used: synthetic and social media datasets related to crises. The empirical results illustrate that OBAL has a very good discrimination power. © 2017 Elsevier Ltd","Active learning; Classification; Crisis management; Online learning; Social media","Artificial intelligence; Budget control; Classification (of information); Learning algorithms; Machinery; Nearest neighbor search; Risk management; Social networking (online); Support vector machines; Uncertainty analysis; Active Learning; Active-learning algorithm; Classification models; Crisis management; Individual uncertainties; K nearest neighbor (KNN); Online learning; Social media; E-learning",2-s2.0-85032873671
"Chahi A., El khadiri I., El merabet Y., Ruichek Y., Touahni R.","Block wise local binary count for off-Line text-independent writer identification",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031020703&doi=10.1016%2fj.eswa.2017.10.010&partnerID=40&md5=f1d668567229ae9d7a401b7540ef351c","Feature engineering is fundamental in applied machine learning. It plays a major role in writer identification of handwritten documents, which has been an active area of research in the literature. In this paper, we propose a conceptually simple, yet high-quality and computationally efficient descriptor referred to as block wise local binary count (BW-LBC) for offline text independent writer identification of handwritten documents. The proposed BW-LBC operator characterizes the writing style of each writer by a set of histograms calculated from all the connected components in the writing. Each histogram is constructed by calculating the occurrence distribution of pixels corresponding to the writing within small blocks in each connected component extracted and cropped from the input handwriting sample (document or set of words/text lines). Specifically, for a given connected component divided into N × N non-overlapping blocks, the appearance probability of writing pixels in the block number i corresponds to the histogram bin number i in the produced corresponding histogram of N × N bins. The samples are classified according to their normalized histogram feature vectors through the nearest-neighbor rule (1-NN) using the Hamming distance. Extensive experiments performed on four challenging handwritten databases (IFN/ENIT, AHTID/MW, CVL and IAM) containing handwritten texts in Arabic and English languages, show that the proposed system using the BW-LBC operator demonstrates superior performance on the Arabic databases (i.e., AHTID/MW and IFN/ENIT) and competitive performance on the English scripts compared to the old and recent state-of-the-art writer identification approaches. © 2017 Elsevier Ltd","1-NN classifier; Feature extraction; Hamming distance; Handwritten connected components; Handwritten documents; Off-line writer identification; Text independent","Feature extraction; Graphic methods; Learning systems; Pixels; Applied machine learning; Competitive performance; Computationally efficient; Connected component; Handwritten document; Normalized histograms; Text independents; Writer identification; Hamming distance",2-s2.0-85031020703
"Kanjo E., Younis E.M.G., Sherkat N.","Towards unravelling the relationship between on-body, environmental and emotion data using sensor information fusion approach",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020287020&doi=10.1016%2fj.inffus.2017.05.005&partnerID=40&md5=65ed81144999b5d5481a5f738520b854","Over the past few years, there has been a noticeable advancement in environmental models and information fusion systems taking advantage of the recent developments in sensor and mobile technologies. However, little attention has been paid so far to quantifying the relationship between environment changes and their impact on our bodies in real-life settings. In this paper, we identify a data driven approach based on direct and continuous sensor data to assess the impact of the surrounding environment and physiological changes and emotion. We aim at investigating the potential of fusing on-body physiological signals, environmental sensory data and on-line self-report emotion measures in order to achieve the following objectives: (1) model the short term impact of the ambient environment on human body, (2) predict emotions based on-body sensors and environmental data. To achieve this, we have conducted a real-world study ‘in the wild’ with on-body and mobile sensors. Data was collected from participants walking around Nottingham city centre, in order to develop analytical and predictive models. Multiple regression, after allowing for possible confounders, showed a noticeable correlation between noise exposure and heart rate. Similarly, UV and environmental noise have been shown to have a noticeable effect on changes in ElectroDermal Activity (EDA). Air pressure demonstrated the greatest contribution towards the detected changes in body temperature and motion. Also, significant correlation was found between air pressure and heart rate. Finally, decision fusion of the classification results from different modalities is performed. To the best of our knowledge this work presents the first attempt at fusing and modelling data from environmental and physiological sources collected from sensors in a real-world setting. © 2017 The Authors","Affective computing; Machine learning; Multi sensor data fusion; Multivariable regression; Physiological signals; Regression analysis sensor data","Atmospheric pressure; Data fusion; Environmental technology; Heart; Human computer interaction; Information fusion; Learning systems; Noise pollution; Physiological models; Physiology; Regression analysis; Affective Computing; Multisensor data fusion; Multivariable regression; Physiological signals; Sensor data; Sensor data fusion",2-s2.0-85020287020
"Yin X., Yang J., Xiao F., Yang Y., Shen H.-B.","MemBrain: An Easy-to-Use Online Webserver for Transmembrane Protein Structure Prediction",2018,"Nano-Micro Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029958327&doi=10.1007%2fs40820-017-0156-2&partnerID=40&md5=fcc6badef42e3bbd69e8a01b10c7f5e9","Membrane proteins are an important kind of proteins embedded in the membranes of cells and play crucial roles in living organisms, such as ion channels, transporters, receptors. Because it is difficult to determinate the membrane protein’s structure by wet-lab experiments, accurate and fast amino acid sequence-based computational methods are highly desired. In this paper, we report an online prediction tool called MemBrain, whose input is the amino acid sequence. MemBrain consists of specialized modules for predicting transmembrane helices, residue–residue contacts and relative accessible surface area of α-helical membrane proteins. MemBrain achieves a prediction accuracy of 97.9% of ATMH, 87.1% of AP, 3.2 ± 3.0 of N-score, 3.1 ± 2.8 of C-score. MemBrain-Contact obtains 62%/64.1% prediction accuracy on training and independent dataset on top L/5 contact prediction, respectively. And MemBrain-Rasa achieves Pearson correlation coefficient of 0.733 and its mean absolute error of 13.593. These prediction results provide valuable hints for revealing the structure and function of membrane proteins. MemBrain web server is free for academic use and available at www.csbio.sjtu.edu.cn/bioinf/MemBrain/. [Figure not available: see fulltext.]. © 2017, The Author(s).","Contact map prediction; Machine learning; Relative accessible surface area; Structure prediction; Transmembrane α-helices",,2-s2.0-85029958327
"Shao H., Jiang H., Lin Y., Li X.","A novel method for intelligent fault diagnosis of rolling bearings using ensemble deep auto-encoders",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032874032&doi=10.1016%2fj.ymssp.2017.09.026&partnerID=40&md5=773f42cafaf4bbc73c4b8c0a5b1721b8","Automatic and accurate identification of rolling bearings fault categories, especially for the fault severities and fault orientations, is still a major challenge in rotating machinery fault diagnosis. In this paper, a novel method called ensemble deep auto-encoders (EDAEs) is proposed for intelligent fault diagnosis of rolling bearings. Firstly, different activation functions are employed as the hidden functions to design a series of auto-encoders (AEs) with different characteristics. Secondly, EDAEs are constructed with various auto-encoders for unsupervised feature learning from the measured vibration signals. Finally, a combination strategy is designed to ensure accurate and stable diagnosis results. The proposed method is applied to analyze the experimental bearing vibration signals. The results confirm that the proposed method can get rid of the dependence on manual feature extraction and overcome the limitations of individual deep learning models, which is more effective than the existing intelligent diagnosis methods. © 2017 Elsevier Ltd","Activation functions; Combination strategy; Ensemble deep auto-encoders; Intelligent fault diagnosis; Rolling bearings","Bearings (machine parts); Chemical activation; Failure analysis; Learning systems; Machinery; Roller bearings; Signal encoding; Vibration analysis; Activation functions; Auto encoders; Combination strategies; Intelligent fault diagnosis; Rolling bearings; Fault detection",2-s2.0-85032874032
"Jia X., Zhao M., Di Y., Li P., Lee J.","Sparse filtering with the generalized lp/lq norm and its applications to the condition monitoring of rotating machinery",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032878006&doi=10.1016%2fj.ymssp.2017.09.018&partnerID=40&md5=4ab49b2e186f98a91c4a9669a960cba8","Sparsity is becoming a more and more important topic in the area of machine learning and signal processing recently. One big family of sparse measures in current literature is the generalized lp/lq norm, which is scale invariant and is widely regarded as normalized lp norm. However, the characteristics of the generalized lp/lq norm are still less discussed and its application to the condition monitoring of rotating devices has been still unexplored. In this study, we firstly discuss the characteristics of the generalized lp/lq norm for sparse optimization and then propose a method of sparse filtering with the generalized lp/lq norm for the purpose of impulsive signature enhancement. Further driven by the trend of industrial big data and the need of reducing maintenance cost for industrial equipment, the proposed sparse filter is customized for vibration signal processing and also implemented on bearing and gearbox for the purpose of condition monitoring. Based on the results from the industrial implementations in this paper, the proposed method has been found to be a promising tool for impulsive feature enhancement, and the superiority of the proposed method over previous methods is also demonstrated. © 2017 Elsevier Ltd","Deep learning; Incipient fault detection; Minimum entropy deconvolution; Rotating machinery; Sparse filtering","Big data; Deep learning; Fault detection; Learning systems; Machinery; Rotating machinery; Signal processing; Feature enhancement; Incipient fault detection; Industrial equipment; Industrial implementation; Maintenance cost; Minimum entropy deconvolution; Sparse optimizations; Vibration signal; Condition monitoring",2-s2.0-85032878006
"Hu L., Gao W., Zhao K., Zhang P., Wang F.","Feature selection considering two types of feature relevancy and feature interdependency",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032290189&doi=10.1016%2fj.eswa.2017.10.016&partnerID=40&md5=130152106c5e16c0a042e1886690e105","Feature selection based on information theory, which is used to select a group of the most informative features, has extensive application fields such as machine learning, data mining and natural language processing. However, numerous previous methods suffer from two common defects. (1) Feature relevancy is defined without distinguishing candidate feature relevancy and selected feature relevancy. (2) Some interdependent features may be misinterpreted as redundant features. In this study, we propose a feature selection method named Dynamic Relevance and Joint Mutual Information Maximization (DRJMIM) to address these two defects. DRJMIM includes four stages. First, the relevancy is divided into two categories: candidate feature relevancy and selected feature relevancy. Second, according to candidate feature relevancy that is joint mutual information, some redundant features are selected. Third, the redundant features are combined with a dynamic weight to reduce the selection possibility of true redundant features while increasing the false ones. Finally, the most informative and interdependent features are selected and true redundant features are eliminated simultaneously. Furthermore, our method is compared with five competitive feature selection methods on 12 publicly available data sets. The classification results show that DRJMIM performs better than other five methods. Its statistical significance is verified by a paired two-tailed t-test. Meanwhile, DRJMIM obtains few number of selected features when it achieves the highest classification accuracy. © 2017 Elsevier Ltd","Feature interdependency; Feature relevancy; Feature selection; Information theory","Classification (of information); Data mining; Defects; Information theory; Learning algorithms; Learning systems; Natural language processing systems; Application fields; Classification accuracy; Classification results; Feature interdependency; Feature relevancy; Feature selection methods; Joint mutual informations; Statistical significance; Feature extraction",2-s2.0-85032290189
"Tommasel A., Godoy D.","A Social-aware online short-text feature selection technique for social media",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019594702&doi=10.1016%2fj.inffus.2017.05.003&partnerID=40&md5=7314a51a50b4631b275c9e90879de89a","Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques. © 2017 Elsevier B.V.","Classification; Micro-blogging communities; Online feature selection","Classification (of information); Clustering algorithms; Data mining; Learning systems; Social networking (online); Text processing; Classification results; High dimensional data; Micro blogging; Online feature selection; Selection techniques; Social applications; Textual information; Topological information; Feature extraction",2-s2.0-85019594702
"Dhimish M., Holmes V., Mehrdadi B., Dales M.","Comparing Mamdani Sugeno fuzzy logic and RBF ANN network for PV fault detection",2018,"Renewable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032285798&doi=10.1016%2fj.renene.2017.10.066&partnerID=40&md5=a283a20c17e0e174401e92d2d6725908","This work proposes a new fault detection algorithm for photovoltaic (PV) systems based on artificial neural networks (ANN) and fuzzy logic system interface. There are few instances of machine learning techniques deployed in fault detection algorithms in PV systems, therefore, the main focus of this paper is to create a system capable to detect possible faults in PV systems using radial basis function (RBF) ANN network and both Mamdani, Sugeno fuzzy logic systems interface. The obtained results indicate that the fault detection algorithm can detect and locate accurately different types of faults such as, faulty PV module, two faulty PV modules and partial shading conditions affecting the PV system. In order to achieve high rate of detection accuracy, four various ANN networks have been tested. The maximum detection accuracy is equal to 92.1%. Furthermore, both examined fuzzy logic systems show approximately the same output during the experiments. However, there are slightly difference in developing each type of the fuzzy systems such as the output membership functions and the rules applied for detecting the type of the fault occurring in the PV plant. © 2017 Elsevier Ltd","ANN networks; Fault detection; Fuzzy logic systems; Photovoltaic faults; Photovoltaic system",,2-s2.0-85032285798
"Choubin B., Darabi H., Rahmati O., Sajedi-Hosseini F., Kløve B.","River suspended sediment modelling using the CART model: A comparative study of machine learning techniques",2018,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030214310&doi=10.1016%2fj.scitotenv.2017.09.293&partnerID=40&md5=7d6d5cb0aa38138b0125441ecae9344b","Suspended sediment load (SSL) modelling is an important issue in integrated environmental and water resources management, as sediment affects water quality and aquatic habitats. Although classification and regression tree (CART) algorithms have been applied successfully to ecological and geomorphological modelling, their applicability to SSL estimation in rivers has not yet been investigated. In this study, we evaluated use of a CART model to estimate SSL based on hydro-meteorological data. We also compared the accuracy of the CART model with that of the four most commonly used models for time series modelling of SSL, i.e. adaptive neuro-fuzzy inference system (ANFIS), multi-layer perceptron (MLP) neural network and two kernels of support vector machines (RBF-SVM and P-SVM). The models were calibrated using river discharge, stage, rainfall and monthly SSL data for the Kareh-Sang River gauging station in the Haraz watershed in northern Iran, where sediment transport is a considerable issue. In addition, different combinations of input data with various time lags were explored to estimate SSL. The best input combination was identified through trial and error, percent bias (PBIAS), Taylor diagrams and violin plots for each model. For evaluating the capability of the models, different statistics such as Nash-Sutcliffe efficiency (NSE), Kling-Gupta efficiency (KGE) and percent bias (PBIAS) were used. The results showed that the CART model performed best in predicting SSL (NSE = 0.77, KGE = 0.8, PBIAS < ± 15), followed by RBF-SVM (NSE = 0.68, KGE = 0.72, PBIAS < ± 15). Thus the CART model can be a helpful tool in basins where hydro-meteorological data are readily available. © 2017 Elsevier B.V.","Adaptive neuro-fuzzy inference system; Classification and regression trees; Haraz watershed; Multi-layer perceptron neural network; Support vector machine; Suspended sediment load","Efficiency; Floods; Forestry; Fuzzy neural networks; Fuzzy systems; Learning systems; Meteorology; Network layers; Radial basis function networks; Rivers; Sediment transport; Sediments; Support vector machines; Suspended sediments; Water quality; Water resources; Watersheds; Adaptive neuro-fuzzy inference system; Classification and regression tree; Machine learning techniques; Multi layer perceptron; Multi-layer perceptron neural networks; River suspended sediments; Suspended sediment loads; Water resources management; Fuzzy inference; rain; algorithm; artificial neural network; comparative study; fluvial deposit; fuzzy mathematics; machine learning; modeling; support vector machine; suspended sediment; accuracy; Article; artificial neural network; CART model; comparative study; controlled study; fuzzy system; Iran; kernel method; machine learning; meteorology; particle resuspension; perceptron; priority journal; river; statistical analysis; statistical model; support vector machine; time series analysis; watershed; Iran",2-s2.0-85030214310
"Katzir Z., Elovici Y.","Quantifying the resilience of machine learning classifiers used for cyber security",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030724887&doi=10.1016%2fj.eswa.2017.09.053&partnerID=40&md5=34556dd676a2d62924a88f3f7035ebf3","The use of machine learning algorithms for cyber security purposes gives rise to questions of adversarial resilience, namely: Can we quantify the effort required of an adversary to manipulate a system that is based on machine learning techniques? Can the adversarial resilience of such systems be formally modeled and evaluated? Can we quantify this resilience such that different systems can be compared using empiric metrics? Past works have demonstrated how an adversary can manipulate a system based on machine learning techniques by changing some of its inputs. However, comparatively little work has emphasized the creation of a formal method for measuring and comparing the adversarial resilience of different machine learning models to these changes. In this work we study the adversarial resilience of detection systems based on supervised machine learning models. We provide a formal definition for adversarial resilience while focusing on multisensory fusion systems. We define the model robustness (MRB) score, a metric for evaluating the relative resilience of different models, and suggest two novel feature selection algorithms for constructing adversary aware classifiers. The first algorithm selects only features that cannot realistically be modified by the adversary, while the second algorithm allows control over the resilience versus accuracy tradeoff. Finally, we evaluate our approach with a real-life use case of dynamic malware classification using an extensive, up-to-date corpus of benign and malware executables. We demonstrate the potential of using adversary aware feature selection for building more resilient classifiers and provide empirical evidence supporting the inherent resilience of ensemble algorithms compared to single model algorithms. © 2017 Elsevier Ltd","Adversarial Learning; Classifier Resilience; Cyber Security","Artificial intelligence; Computer crime; Feature extraction; Formal methods; Learning systems; Malware; Supervised learning; Adversarial learning; Cyber security; Ensemble algorithms; Feature selection algorithm; Machine learning models; Malware classifications; Multi-sensory fusion; Supervised machine learning; Learning algorithms",2-s2.0-85030724887
"Karim M.A., Currie J., Lie T.-T.","A machine learning based optimized energy dispatching scheme for restoring a hybrid microgrid",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032268622&doi=10.1016%2fj.epsr.2017.10.015&partnerID=40&md5=a86ce18e642cd2dadbb76fa6c91514bc","A microgrid operated in stand alone mode is highly vulnerable to instability when the integration of intermittent energy sources are considered. If a short circuit fault occurs in a microgrid while operating at its design limit, often cost effective system recovery becomes a challenging task. Under such contingencies predictive analysis can be used to strengthen the system restoration schemes. In this study, a system based on machine learning algorithm is implemented to forecast the security of a standalone microgrid and based on the forecasting, schedule multiple backup diesel generators under the contingency of loss of a major generating unit. The underlying objective is to maintain the voltage stability with an optimized economic dispatch scheme, right after clearing a critical three phase short circuit fault. Finally, a promising set of outcomes are observed and discussed. © 2017 Elsevier B.V.","Distributed generation; Genetic algorithm; Hybrid microgrid; Machine learning; Monte Carlo simulation","Artificial intelligence; Cost effectiveness; Distributed power generation; Electric fault currents; Electric load dispatching; Genetic algorithms; Intelligent systems; Learning systems; Monte Carlo methods; Scheduling; Cost effective systems; Diesel generators; Economic Dispatch; Intermittent energy source; Micro grid; Short-circuit fault; System restoration; Three-phase short circuit faults; Learning algorithms",2-s2.0-85032268622
"Massawe B.H.J., Subburayalu S.K., Kaaya A.K., Winowiecki L., Slater B.K.","Mapping numerically classified soil taxa in Kilombero Valley, Tanzania using machine learning",2018,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007173765&doi=10.1016%2fj.geoderma.2016.11.020&partnerID=40&md5=d91cb82e890b8cc632052a5c831f4077","Inadequacy of spatial soil information is one of the limiting factors to making evidence-based decisions to improve food security and land management in the developing countries. Various digital soil mapping (DSM) techniques have been applied in many parts of the world to improve availability and usability of soil data, but less has been done in Africa, particularly in Tanzania and at the scale necessary to make farm management decisions. The Kilombero Valley has been identified for intensified rice production. However the valley lacks detailed and up-to-date soil information for decision-making. The overall objective of this study was to develop a predictive soil map of a portion of Kilombero Valley using DSM techniques. Two widely used decision tree algorithms and three sources of Digital Elevation Models (DEMs) were evaluated for their predictive ability. Firstly, a numerical classification was performed on the collected soil profile data to arrive at soil taxa. Secondly, the derived taxa were spatially predicted and mapped following SCORPAN framework using Random Forest (RF) and J48 machine learning algorithms. Datasets to train the model were derived from legacy soil map, RapidEye satellite image and three DEMs: 1 arc SRTM, 30 m ASTER, and 12 m WorldDEM. Separate predictive models were built using each DEM source. Mapping showed that RF was less sensitive to the training set sampling intensity. Results also showed that predictions of soil taxa using 1 arc SRTM and 12 m WordDEM were identical. We suggest the use of RF algorithm and the freely available SRTM DEM combination for mapping the soils for the whole Kilombero Valley. This combination can be tested and applied in other areas which have relatively flat terrain like the Kilombero Valley. © 2016 Elsevier B.V.","Decision tree analysis; DEM; Kilombero Valley; Machine learning; Numerical classification; Soil mapping","Artificial intelligence; Data mining; Decision making; Decision trees; Developing countries; Food supply; Forestry; Landforms; Learning algorithms; Learning systems; Mapping; Soil surveys; Surveying; Tracking radar; Decision tree analysis; Decision-tree algorithm; Digital elevation model; Digital soil mappings; Evidence- based decisions; Kilombero Valley; Numerical classification; Soil mapping; Soils; algorithm; crop production; decision support system; developing world; digital elevation model; land management; machine learning; mapping; numerical method; RapidEye; rice; satellite imagery; Shuttle Radar Topography Mission; soil analysis; soil biota; soil classification; soil profile; terrain; Kilombero Valley; Morogoro [Tanzania]; Tanzania",2-s2.0-85007173765
"Chen C., Husny J., Rabe S.","Predicting fishiness off-flavour and identifying compounds of lipid oxidation in dairy powders by SPME-GC/MS and machine learning",2018,"International Dairy Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032484146&doi=10.1016%2fj.idairyj.2017.09.009&partnerID=40&md5=31af2389a49a781cded497079096b765","This study examined the correlation between untargeted solid-phase micro-extraction gas chromatography/mass spectrometry (SPME-GC/MS) data and sensory fishiness of dairy powders fortified with long-chain polyunsaturated fatty acids and iron. A machine learning approach for sensory prediction from raw CG/MS data is discussed and its potential for determining key contributing compounds shown. To find peak correspondence and to correct retention time shifts, GC/MS raw data of different samples were aligned using dynamic programming. Sensory modelling and prediction was done without prior peak identification in the mass spectral library. Regression was achieved by multiple classification tasks using a Random Forest model. The obtained sensory predictions showed good accuracy both in leave-one-out evaluation and on a separate powder sample test set. GC/MS peaks suggested by Random Forest to significantly contribute to fishiness were identified to be from the chemical classes of alcohols, ketones, aldehydes and furans. © 2017 Elsevier Ltd",,"Artificial intelligence; Chromatography; Decision trees; Dynamic programming; Fatty acids; Forecasting; Gas chromatography; Ketones; Polyunsaturated fatty acids; Powders; Gas chromatography/Mass spectrometry; Long chain polyunsaturated fatty acid; Machine learning approaches; Mass spectral libraries; Modelling and predictions; Multiple Classification; Random forest modeling; Solid phase micro extraction; Learning systems",2-s2.0-85032484146
"Hu Y., Xie J., Liu Z., Ding Q., Zhu W., Zhang J., Zhang W.","CA method with machine learning for simulating the grain and pore growth of aluminum alloys",2018,"Computational Materials Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032278638&doi=10.1016%2fj.commatsci.2017.09.059&partnerID=40&md5=a7d3046220e39bf9ab099138e7cb4794","A cellular automata (CA) method assisted by a back-propagation neural network (BPNN), named CA-BPNN, is proposed to simulate grain and pore growth. First, CA-BPNN uses the BPNN to detect the relations between porosity and solidification parameters and then uses the relations to establish extra transformation rules of pore growth, which are finally implemented on A356 alloy directly. Compared with the computational results, the shapes and volume fraction of pores from experimental observation are consistent. CA-BPNN can reduce the difficulty of simulating the whole solidification process of casting without solving the high-dimensional continuous governing equations of porosity. This work can be further extended to other useful industrial alloys, such as aluminum alloys and various grades of industrial steels, if the experimental data sets are improved and other machine learning algorithms are introduced. © 2017 Elsevier B.V.","Aluminum alloy; CA-BPNN method; Grain and pore growth; Porosity","Alloy steel; Aluminum; Aluminum alloys; Artificial intelligence; Backpropagation; Learning algorithms; Learning systems; Neural networks; Porosity; Solidification; Back-propagation neural networks; Computational results; Governing equations; Industrial alloys; Pore growth; Solidification parameter; Solidification process; Transformation rules; Grain growth",2-s2.0-85032278638
"López Lázaro J., Barbero Jiménez Á., Takeda A.","Improving cash logistics in bank branches by coupling machine learning and robust optimization",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030096149&doi=10.1016%2fj.eswa.2017.09.043&partnerID=40&md5=687f2bcebbb87f23991b135b67119bd1","This paper describes how Machine Learning and Robust Optimization techniques can greatly improve cash logistics operations. Specifically, we seek to optimize the logistics followed by the different branches of a given bank. Machine Learning is used to forecast cash demands for each of the branches, taking into account past demands and calendar effects. These demand predictions are forwarded to a Robust Optimization model, whose outputs are the cash transports that each branch should request. These transports guarantee that demand is fulfilled up to the desired confidence level, while also satisfying additional constraints arising in this particular domain. © 2017 Elsevier Ltd","Banking; Forecasting; Integer programming; Inventory control; Optimization; Planning and control","Artificial intelligence; Couplings; Forecasting; Inventory control; Learning systems; Optimization; Banking; Calendar effects; Confidence levels; Demand prediction; Logistics operations; Planning and control; Robust optimization; Robust optimization models; Integer programming",2-s2.0-85030096149
"Akmaz D., Mamiş M.S., Arkan M., Tağluk M.E.","Transmission line fault location using traveling wave frequencies and extreme learning machine",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030091064&doi=10.1016%2fj.epsr.2017.09.019&partnerID=40&md5=a067eb8e2e0c75a0500facd75b471898","In this research, a new approach was proposed for determining the fault location in transmission lines. Traveling wave frequencies and an extreme learning machine (ELM) were used to determine fault location. Transient signals in the time domain were transformed to the frequency domain using the fast Fourier transform (FFT) and the traveling wave frequencies were detected from the transient frequency spectrum. In order to detect the location of fault, traveling wave frequency was used initially to predict the fault location. The prediction of this fault location was tested for many different fault conditions and was found to be adversely affected by only the source inductance value. This is due to the negative effect of source inductance on wave velocity. Regression feature of ELM was used in order to improve the prediction of fault location and to minimize the negative effect of source inductance. For ELM regression training, values of the fault distance estimated from the traveling-wave frequencies and the source inductance values were used as ELM input data, and the actual distance values were used as ELM output data. After ELM regression training, ELM predicted a new fault location using the input data. The Alternative Transients Program (ATP/EMTP) was used to model J. Marti frequency dependent line model, and the MATLAB program was used to perform fault-detection algorithms. Simulation results show that the proposed method is very successful against many variables such as different fault resistances, source inductances, transmission line characteristics, transmission line lengths. © 2017 Elsevier B.V.","Extreme learning machine; Fast Fourier transform; Fault-location estimation; Transmission lines; Traveling wave frequencies","Electric lines; Fast Fourier transforms; Forecasting; Frequency domain analysis; Frequency estimation; Inductance; Input output programs; Knowledge acquisition; Learning systems; Location; MATLAB; Regression analysis; Wave propagation; Wave transmission; Alternative transients program; Extreme learning machine; Fault detection algorithm; Fault location estimation; Frequency dependent line model; Transmission line characteristics; Transmission line fault location; Traveling wave; Fault detection",2-s2.0-85030091064
"Long Y., Du Z.-J., Wang W.-D., Dong W.","Human motion intent learning based motion assistance control for a wearable exoskeleton",2018,"Robotics and Computer-Integrated Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027548318&doi=10.1016%2fj.rcim.2017.08.007&partnerID=40&md5=3e9b289293ca1b94f17dfd9bdb41f948","Human motion intent (HMI) acquiring by using physical human robot interaction (pHRI) information is one of the most crucial issues for lower extremity exoskeleton control. The mapping from the pHRI information to the HMI is complicated and nonlinear since the wearer is in the control loop, which is difficult to be modeled directly via mathematical tools. The nonlinear approximation can be learned by using machine learning approaches, e.g., Gaussian Process (GP) regression, which is suitable for high-dimensional and small-sample nonlinear regression problems. However, GP regression is restrictive for large scale datasets due to its computation complexity. In this paper, an online sparse GP algorithm is proposed to learn the HMI, where the input is the pHRI signal and the output is the angular increment of the active joints, i.e., the knee joints. The data of HRI is collected by the torque sensor and the angular position of the active joint is measured by the optical position sensor respectively. The pHRI signal is dealt with Kalman smoother to achieve the following functions, i.e., (1) eliminating noise and (2) predicting forward. The learned HMI via the online sparse GP regression algorithm is regarded as the reference trajectory of the lower extremity exoskeleton. A fuzzy-PID control strategy is designed to drive the robotic exoskeleton to follow the estimated HMI. Prototype experiments are performed on the subjects who wear the exoskeleton system to walk on different terrains without any transition. The experimental results validated the effectiveness of the proposed algorithm. The online sparse GP regression algorithm is capable of learning the HMI based on the pHRI and the fuzzy-PID can shadow the HMI quite well. © 2017 Elsevier Ltd","Exoskeleton; GP regression; Human motion intent; Human robot interaction; Motion assistance","Digital storage; Exoskeleton (Robotics); Joints (anatomy); Learning systems; Man machine systems; Regression analysis; Robots; Three term control systems; Wearable sensors; Wearable technology; Human motions; Lower extremity exoskeletons; Machine learning approaches; Motion assistance; Nonlinear approximation; Nonlinear regression problems; Optical position sensors; Physical humanrobot interaction (phri); Human robot interaction",2-s2.0-85027548318
"Pławiak P.","Novel methodology of cardiac health recognition based on ECG signals and evolutionary-neural system",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030751834&doi=10.1016%2fj.eswa.2017.09.022&partnerID=40&md5=5b76b1ca840e34dd808910f005ebcd66","This article presents an innovative research methodology that enables the efficient classification of cardiac disorders (17 classes) based on ECG signal analysis and an evolutionary-neural system. From a social point of view, it is extremely important to prevent heart diseases, which are the most common cause of death worldwide. According to statistical data, 50 million people are at risk for cardiac diseases worldwide. The subject of ECG signal analysis is very popular. However, due to the great difficulty of the task undertaken, and high computational complexity of existing methods, there remains substantial work to perform. This research collected 1000 fragments of ECG signals from the MIH-BIH Arrhythmia database for one lead, MLII, from 45 patients. An original methodology that consisted of the analysis of longer (10-s) fragments of the ECG signal was used (an average of 13 times less classifications). To enhance the characteristic features of the ECG signal, the spectral power density was estimated (using Welch's method and a discrete Fourier transform). Genetic optimization of parameters and genetic selection of features were tested. Pre-processing, normalization, feature extraction and selection, cross-validation and machine learning algorithms (SVM, kNN, PNN, and RBFNN) were used. The best evolutionary-neural system, based on the SVM classifier, obtained a recognition sensitivity of 17 myocardium dysfunctions at a level of 90.20% (98 errors per 1000 classifications, accuracy = 98.85%, specificity = 99.39%, time for classification of one sample = 0.0023 [s]). Against the background of the current scientific literature, these results are some of the best results to date. © 2017 Elsevier Ltd","Biomedical signal processing and analysis; Classification; Discrete Fourier transform; ECG; Evolutionary-neural system; Feature extraction and selection; Genetic algorithm; K-nearest neighbor algorithm; Machine learning algorithms; Neural networks; Support vector machine","Artificial intelligence; Bioinformatics; Cardiology; Classification (of information); Discrete Fourier transforms; Diseases; Electrocardiography; Evolutionary algorithms; Extraction; Feature extraction; Genetic algorithms; Heart; Learning algorithms; Learning systems; Nearest neighbor search; Neural networks; Optimization; Pattern recognition; Signal analysis; Signal processing; Support vector machines; Feature extraction and selection; Genetic optimization; Innovative research; K nearest neighbor algorithm; Neural systems; Scientific literature; Spectral power density; Statistical datas; Biomedical signal processing",2-s2.0-85030751834
"Wei G., Ma H., Qian W., Han F., Jiang H., Qi S., Qiu M.","Lung nodule classification using local kernel regression models with out-of-sample extension",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029362423&doi=10.1016%2fj.bspc.2017.08.026&partnerID=40&md5=c55775ba39c60d1c39ca3443594dcfbb","Computer-aided classification is a major research task for computer-aided diagnosis of pulmonary nodules. In radiology domain, labeled data can be expensive to generate. Therefore, in this study, a novel unsupervised spectral clustering algorithm was presented to distinguish benign and malignant nodules. In this algorithm, a new Laplacian matrix was constructed by using local kernel regression models (LKRM) and incorporating a regularization term, the regularization term can tackle the out-of-sample problem. To verify the feasibility of our algorithm, a ground truth dataset was assembled from the LIDC-IDRI database, including 371 benign and 375 malignant lung nodules. All nodules were represented by the texture features, which were computed from the regions of interest (ROIs). Extensive experiments on lung nodules showed that the proposed algorithm not only achieved a higher classification performance than existing popular unsupervised algorithms, but also had superiority comparing to some supervised algorithms (linear discriminant analysis and extreme learning machine). © 2017 Elsevier Ltd","Kernel trick; Linear regression; Lung nodules; Out-of-sample; Spectral clustering","Biological organs; Computer aided diagnosis; Discriminant analysis; Learning systems; Linear regression; Matrix algebra; Natural language processing systems; Regression analysis; Classification performance; Computer Aided Classification; Extreme learning machine; Kernel trick; Linear discriminant analysis; Lung nodule; Spectral clustering; Spectral clustering algorithms; Clustering algorithms; Article; cancer classification; cancer diagnosis; data base; discriminant analysis; human; image segmentation; kernel method; linear regression analysis; lung cancer; lung nodule; machine learning; priority journal; radiodiagnosis",2-s2.0-85029362423
"Xie J., Indraswari K., Schwarzkopf L., Towsey M., Zhang J., Roe P.","Acoustic classification of frog within-species and species-specific calls",2018,"Applied Acoustics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032259785&doi=10.1016%2fj.apacoust.2017.10.024&partnerID=40&md5=91a78c27a41b2549f912af476b399329","There have been various studies using automated recognisers of acoustic features and machine learning algorithms to classify frog species within a chorusing community. Such studies rarely consider within-species call variation in the classification process. Individual frog species may make a range of different calls, with different purposes. Including modification of calls in automated recognition has the potential to not only increase the accuracy of classification of calls to species, but also to provide information on frog calling behaviour within species. Here we use acoustic feature extraction and machine learning algorithms (1) to investigate the acoustic feature importance of identifying species-specific calls; (2) to determine which acoustic features can be used to classify within-species calls. Our method was tested for its performance in recognising four frog species (Litoria bicolor, Litoria rothii, Litoria wotjulumensis, and Uperoleia inundata) and four call types of L.wotjulumensis (normal, click, response, and long trill). Mean classification accuracy was high, with 84.0% at the species level, and 83.7% at the call type level. The overall classification accuracy can be up to 93.0%, when considering four call types of L. wotjulumensis as individual classes and being combined with other three frog species. Two techniques, principal component analysis and Fisher discriminant ratio were used for dimension reduction and to select important features for discriminating among calls of different species, and call types within species. In conclusion, our proposed classification mechanism could effectively not only classify different frog species but also identify different call types within the same species. Moreover, we found that time-domain features were important for classification of within-species calls, whereas frequency-domain features were more useful for classification of species-specific calls. © 2017 Elsevier Ltd","Acoustic features; Frog community interactions; Machine learning algorithms; Soundscape ecology","Artificial intelligence; Feature extraction; Frequency domain analysis; Learning algorithms; Learning systems; Principal component analysis; Time domain analysis; Accuracy of classifications; Acoustic classification; Acoustic feature extraction; Acoustic features; Classification accuracy; Classification mechanism; Classification process; Soundscapes; Classification (of information)",2-s2.0-85032259785
"Liu C., Wang C., Zhang Z.-H., Zheng L.","Scheduling with job-splitting considering learning and the vital-few law",2018,"Computers and Operations Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016520666&doi=10.1016%2fj.cor.2017.02.011&partnerID=40&md5=46b28f3ffab352194c2611ee67ea9a08","This research, which is motivated by real cases in labor-intensive industries where learning effects and the vital-few law take place, integrates learning and job splitting in parallel machine scheduling problems to minimize the makespan. We propose the lower bound of the problem and a job-splitting algorithm corresponding to the lower bound. Subsequently, a heuristic called SLMR is proposed based on the job-splitting algorithm with a proven worst case ratio. Furthermore, a branch-and-bound algorithm, which can obtain optimal solutions for very small problems, and a hybrid differential evolution algorithm are proposed, which can not only solve the problem, but also serve as a benchmark to evaluate the solution quality of the heuristic SLMR. The performance of the heuristic on a large number of randomly generated instances is evaluated. Results show that the proposed heuristic has good solution quality and calculation efficiency. © 2017","Job splitting; Learning effect; Makespan; Parallel machine scheduling; Vital-few law; Worst-case analysis","Benchmarking; Branch and bound method; Evolutionary algorithms; Machinery; Optimization; Problem solving; Quality control; Scheduling; Job splitting; Learning effects; Makespan; Parallel machine scheduling; Vital-few law; Worst-case analysis; Job shop scheduling",2-s2.0-85016520666
"Prljić S., Nikitović Z., Stojanović A.G., Cogoljević D., Pešić G., Alizamir M.","Management of business economic growth as function of resource rents",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030785797&doi=10.1016%2fj.physa.2017.09.087&partnerID=40&md5=1624ae691fd0fae5fcbb14c9173f6f15","Economic profit could be influenced by economic rents. However natural resource rents provided different impact on the economic growth or economic profit. The main focus of the study was to evaluate the economic growth as function of natural resource rents. For such a purpose machine learning approach, artificial neural network, was used. The used natural resource rents were coal rents, forest rents, mineral rents, natural gas rents and oil rents. Based on the results it is concluded that the machine learning approach could be used as the tool for the economic growth evaluation as function of natural resource rents. Moreover the more advanced approaches should be incorporated to improve more the forecasting accuracy. © 2017 Elsevier B.V.","Economic growth; Evaluation; Natural resources rents","Artificial intelligence; Coal deposits; Economic and social effects; Function evaluation; Learning systems; Natural gas deposits; Natural resources; Neural networks; Profitability; Business economics; Economic growths; Economic profit; Economic rent; Evaluation; Forecasting accuracy; Machine learning approaches; Resource rents; Economics",2-s2.0-85030785797
"Shao H., Jiang H., Zhang H., Duan W., Liang T., Wu S.","Rolling bearing fault feature learning using improved convolutional deep belief network with compressed sensing",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028716822&doi=10.1016%2fj.ymssp.2017.08.002&partnerID=40&md5=9b2be8bbbad4777f0058971dc36e57cb","The vibration signals collected from rolling bearing are usually complex and non-stationary with heavy background noise. Therefore, it is a great challenge to efficiently learn the representative fault features of the collected vibration signals. In this paper, a novel method called improved convolutional deep belief network (CDBN) with compressed sensing (CS) is developed for feature learning and fault diagnosis of rolling bearing. Firstly, CS is adopted for reducing the vibration data amount to improve analysis efficiency. Secondly, a new CDBN model is constructed with Gaussian visible units to enhance the feature learning ability for the compressed data. Finally, exponential moving average (EMA) technique is employed to improve the generalization performance of the constructed deep model. The developed method is applied to analyze the experimental rolling bearing vibration signals. The results confirm that the developed method is more effective than the traditional methods. © 2017 Elsevier Ltd","Compressed sensing; Exponential moving average; Feature learning; Improved convolutional deep belief network; Rolling bearing","Bearings (machine parts); Compressed sensing; Convolution; Deep learning; Fault detection; Signal reconstruction; Vibration analysis; Compressive sensing; Deep belief networks; Exponential moving averages; Feature learning; Generalization performance; Rolling bearing vibration; Rolling bearings; Vibration signal; Roller bearings",2-s2.0-85028716822
"Zeller K.S., Johansson H., Lund T.Ø., Kristensen N.N., Roggen E.L., Lindstedt M.","An alternative biomarker-based approach for the prediction of proteins known to sensitize the respiratory tract",2018,"Toxicology in Vitro",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030994885&doi=10.1016%2fj.tiv.2017.09.029&partnerID=40&md5=f15c25eafc70879746d6798802645810","Many natural and industrial proteins are known to have properties that can result in type I hypersensitivity, however, to date, no validated test system exists that can predict the sensitizing potential of these allergens. Thus, the objective of this study was to develop a protocol based on the myeloid cell-based Genomic Allergen Rapid Detection (GARD) assay that can be used to assess and predict the capacity of protein allergens known to induce sensitization in the respiratory tract. Cellular responses induced by eight selected proteins were assessed using transcriptional profiling, flow cytometry and multiplex cytokine analysis. 391 potential biomarkers were identified as a predictive signature and a series of cross-validations supported the validity of the model. These results together with biological pathway analysis of the transcriptomic data indicate that the investigated cell system is able to capture relevant events linked to type I hypersensitization. © 2017","Biomarker; Cell-based assay; Dendritic cells; Protein allergens; Respiratory sensitization","allergen; amylase; biological marker; CD14 antigen; CD86 antigen; cytokine; Der p 1; Der p 7; glycosidase; HLA DR antigen; immunoglobulin enhancer binding protein; interleukin 1 receptor blocking agent; interleukin 10; interleukin 12; interleukin 15; interleukin 17; interleukin 18; interleukin 2; interleukin 3; interleukin 5; interleukin 6; interleukin 8; macrophage inflammatory protein 1alpha; monocyte chemotactic protein 1; proteinase; RANTES; RNA; t6 antigen; triacylglycerol lipase; unclassified drug; allergy test; anaphylaxis; Article; bone marrow cell; clinical protocol; controlled study; dendritic cell; flow cytometry; gene expression; machine learning; prediction; protein analysis; respiratory system; respiratory tract allergy; sensitization; signal transduction; transcriptomics; validation study",2-s2.0-85030994885
"Chen L., Zhou M., Su W., Wu M., She J., Hirota K.","Softmax regression based deep sparse autoencoder network for facial emotion recognition in human-robot interaction",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032584576&doi=10.1016%2fj.ins.2017.10.044&partnerID=40&md5=b3e722e6b2017c71cd8c765b2ec735cf","Deep neural network (DNN) has been used as a learning model for modeling the hierarchical architecture of human brain. However, DNN suffers from problems of learning efficiency and computational complexity. To address these problems, deep sparse autoencoder network (DSAN) is used for learning facial features, which considers the sparsity of hidden units for learning high-level structures. Meanwhile, Softmax regression (SR) is used to classify expression feature. In this paper, Softmax regression-based deep sparse autoencoder network (SRDSAN) is proposed to recognize facial emotion in human-robot interaction. It aims to handle large data in the output of deep learning by using SR, moreover, to overcome local extrema and gradient diffusion problems in the training process, the overall network weights are fine-tuned to reach the global optimum, which makes the entire depth of the neural network more robust, thereby enhancing the performance of facial emotion recognition. Results show that the average recognition accuracy of SRDSAN is higher than that of the SR and the convolutional neural network. The preliminarily application experiments are performed in the developing emotional social robot system (ESRS) with two mobile robots, where emotional social robot is able to recognize emotions such as happiness and angry. © 2017","Deep sparse autoencoder network; Facial emotion recognition; Human-robot interaction; Softmax regression","Computational efficiency; Deep learning; Deep neural networks; Face recognition; Learning systems; Man machine systems; Neural networks; Regression analysis; Robots; Speech recognition; Auto encoders; Convolutional neural network; Facial emotions; Hierarchical architectures; High-level structure; Learning efficiency; Recognition accuracy; Softmax regressions; Human robot interaction",2-s2.0-85032584576
"Shihabudheen K.V., Mahesh M., Pillai G.N.","Particle swarm optimization based extreme learning neuro-fuzzy system for regression and classification",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030679056&doi=10.1016%2fj.eswa.2017.09.037&partnerID=40&md5=17da58230cab0d3530cf7704c233277b","This paper improves the performance of adaptive neuro-fuzzy inference system (ANFIS) using extreme learning machines (ELM) concept and particle swarm optimization (PSO). The proposed learning machine, particle swarm optimization (PSO) based regularized extreme learning adaptive neuro-fuzzy inference system (PSO-RELANFIS), has the advantages of reduced randomness, reduced computational complexity and better generalization. The fuzzy membership function parameters of the proposed system are randomly selected with in constraint ranges. A regularized loss function is developed using constrained optimization and the optimized regularization parameter is obtained using PSO technique. Performance analysis on regression and classification problems shows that proposed algorithm achieves similar or better generalization performance compared to well-known kernel based methods and ELM based neuro-fuzzy systems. © 2017 Elsevier Ltd","ELM based neuro-fuzzy system; Regression and multi-class classification; Regularization; Takagi–Sugeno–Kang (TSK) fuzzy inference system","Constrained optimization; Fuzzy neural networks; Fuzzy systems; Learning systems; Membership functions; Particle swarm optimization (PSO); Regression analysis; Swarm intelligence; Adaptive neuro-fuzzy inference system; Fuzzy inference systems; Fuzzy membership function; Generalization performance; Multi-class classification; Neurofuzzy system; Regularization; Regularization parameters; Fuzzy inference",2-s2.0-85030679056
"Liang Q., Wu W., Coppola G., Zhang D., Sun W., Ge Y., Wang Y.","Calibration and decoupling of multi-axis robotic Force/Moment sensors",2018,"Robotics and Computer-Integrated Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029382494&doi=10.1016%2fj.rcim.2017.08.008&partnerID=40&md5=4519d3eb151428239581359767be210a","Multi-axis robotic Force/Moment (F/M) sensors are capable of simultaneously detecting multiple components of force (Fx, Fy, and Fz), as well as the moments (Mx, My and Mz). This enables them to be frequently used in many robotic applications. Accurate, time-effective calibration and decoupling procedures are critical to the implementation of these sensors. This paper compares the effectiveness of decoupling methods based on Least-Squares (LS), BP Neural Network (BPNN), and Extreme Learning Machine (ELM) methods for improving the performance of multi-axis robotic F/M sensors. In order to demonstrate the effectiveness of the decoupling methods, a calibration and decoupling experiment was performed on a five-axis robotic F/M sensor. The experiments demonstrate that the ELM based decoupling method is superior to LS and BPNN based methods. The presented theoretical and experimental demonstrations provide a comprehensive description of the calibration and decoupling procedures of multi-axis robotic F/M sensors. This work reveals that the ELM method is an appropriate and high performing decoupling procedure for multi-axis robotic F/M sensors. © 2017","Calibration and decoupling; Extreme Learning Machine; Multi-axis Force/Moment sensors; Robotic sensory system","Calibration; Knowledge acquisition; Learning systems; Least squares approximations; Neural networks; BP neural networks; Decoupling methods; Experimental demonstrations; Extreme learning machine; Multi-Axis; Multiple components; Robotic applications; Sensory system; Robotics",2-s2.0-85029382494
"Wu H., Prasad S.","Semi-supervised dimensionality reduction of hyperspectral imagery using pseudo-labels",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032285895&doi=10.1016%2fj.patcog.2017.09.003&partnerID=40&md5=c6444db6bb6cc716d4dbbb16a7068756","Dimensionality reduction has been proven to be efficient in preparing high dimensional data for various tasks in machine learning. As supervised dimensionality reduction methods such as Fisher discriminant analysis (FDA) and local Fisher discriminant analysis (LFDA) tend to suffer from overfitting when only a small number of labeled samples are available, the abundant unlabeled samples could be helpful in finding a better embedding space. However, applying discriminant analysis on unlabeled data is challenging since we do not have labels for unlabeled data. In this paper, we propose a semi-supervised Semi-Supervised Local Fisher Discriminant Analysis (SSLFDA) using pseudo labels, aiming to perform discriminant analysis on both labeled and unlabeled samples. SSLFDA makes use of pseudo labels, learned from the Dirichlet process mixture model (DPMM) based clustering algorithm, to enable local Fisher discriminant analysis on unlabeled data. In addition, a kernel extension of SSLFDA is derived for non-linear dimensionality reduction. We present experimental results with real hyperspectral data to show that our method provides better classification performance compared to other existing dimensionality reduction methods. © 2017 Elsevier Ltd","Dimensionality reduction; Dirichlet process mixture model; Hyperspectral data classification; Semi-supervised learning","Clustering algorithms; Data reduction; Discriminant analysis; Fisher information matrix; Learning algorithms; Learning systems; Mixtures; Spectroscopy; Supervised learning; Dimensionality reduction; Dimensionality reduction method; Dirichlet process mixture model; Fisher discriminant analysis; Hyperspectral data classification; Local Fisher Discriminant Analysis; Nonlinear dimensionality reduction; Semi- supervised learning; Classification (of information)",2-s2.0-85032285895
"Ergen B., Baykara M., Polat C.","Determination of the relationship between internal auditory canal nerves and tinnitus based on the findings of brain magnetic resonance imaging",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030715707&doi=10.1016%2fj.bspc.2017.09.023&partnerID=40&md5=e3776e9b921e32956bf93d233d40f8c9","This experimental study aimed to investigate a relationship between tinnitus and thicknesses of internal auditory canal and nerves in it. It was performed on brain magnetic resonance images of patients who consulted the ear, nose, and throat clinic with tinnitus complaint. Statistical hypothesis tests and classification experiments were performed on these data to find out structural differences in internal auditory channel components in patients with tinnitus after obtaining cross-sectional areas of nerves as thicknesses. Both the hypothesis tests and classification results showed that the thicknesses of nerves in tinnitus cases were different from those in normal cases. In particular, the hypothesis tests for the superior vestibular nerve and internal auditory channel showed the highest significance, indicating the relationship with tinnitus. The classification results indicated the possibility of classification for tinnitus identification, establishing a computer-assisted diagnostic system to help physicians. © 2017 Elsevier Ltd","Extreme learning machine; Internal auditory canal; Statistical significance; Tinnitus","Diagnosis; Learning systems; Magnetic resonance imaging; Testing; Brain magnetic resonance images; Classification results; Computer-assisted diagnostic systems; Extreme learning machine; Statistical hypothesis test; Statistical significance; Structural differences; Tinnitus; Hydraulic structures",2-s2.0-85030715707
"Tenorio-González A.C., Morales E.F.","Automatic discovery of concepts and actions",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030121361&doi=10.1016%2fj.eswa.2017.09.023&partnerID=40&md5=14049114c446f5157353c22c89bc1b12","A truly autonomous artificial intelligence agent should be able to drive its own learning process. That is, decide what to explore and what to learn, identifying what constitutes potential useful data as examples of concepts or what strategy to follow to solve a new task. Different efforts have been developed in machine learning towards this aim. Approaches that introduce new concepts, like predicate invention in Inductive Logic Programming (ILP) techniques, normally require the selection of examples by the user. Techniques that learn behavior policies through exploration like Reinforcement Learning (RL) with intrinsic motivation, to guide the agent into interesting areas to discover new goals, assume that all the states and actions are predefined in advance. In this paper, we describe a system, called ADC, that combines techniques from ILP with predicate invention and RL with intrinsic motivation to discover new concepts, states and actions to learn behavior policies. ADC drives its own learning process, collecting its own examples for autonomously learning concepts. These new concepts can be used to describe its environment and define new states and actions used to learn behaviors to solve tasks. We show the effectiveness of our approach in simulated robotics environments. © 2017 Elsevier Ltd","Concept formation; Discovery system; Intrinsic motivation; Predicate invention; Reinforcement learning; Relational representation","Artificial intelligence; Autonomous agents; Digital storage; Inductive logic programming (ILP); Learning systems; Logic programming; Motivation; Patents and inventions; Artificial intelligence agent; Automatic discovery; Behavior policy; Concept formation; Discovery systems; Intrinsic motivation; Learning process; Relational representations; Reinforcement learning",2-s2.0-85030121361
"Domingues R., Filippone M., Michiardi P., Zouaoui J.","A comparative evaluation of outlier detection algorithms: Experiments and analyses",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032291155&doi=10.1016%2fj.patcog.2017.09.037&partnerID=40&md5=9c8dd9f9ac8e5c33e915c6405b2b8c28","We survey unsupervised machine learning algorithms in the context of outlier detection. This task challenges state-of-the-art methods from a variety of research fields to applications including fraud detection, intrusion detection, medical diagnoses and data cleaning. The selected methods are benchmarked on publicly available datasets and novel industrial datasets. Each method is then submitted to extensive scalability, memory consumption and robustness tests in order to build a full overview of the algorithms’ characteristics. © 2017 Elsevier Ltd","Fraud detection; Isolation forest; Novelty detection; Outlier detection; Variational inference","Crime; Data handling; Diagnosis; Learning algorithms; Learning systems; Signal detection; Statistics; Fraud detection; Isolation forest; Novelty detection; Outlier Detection; Variational inference; Intrusion detection",2-s2.0-85032291155
"Zhang W., Li C., Peng G., Chen Y., Zhang Z.","A deep convolutional neural network with new training methods for bearing fault diagnosis under noisy environment and different working load",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028727944&doi=10.1016%2fj.ymssp.2017.06.022&partnerID=40&md5=ecc45cf07b492474df8776b1c90d3c44","In recent years, intelligent fault diagnosis algorithms using machine learning technique have achieved much success. However, due to the fact that in real world industrial applications, the working load is changing all the time and noise from the working environment is inevitable, degradation of the performance of intelligent fault diagnosis methods is very serious. In this paper, a new model based on deep learning is proposed to address the problem. Our contributions of include: First, we proposed an end-to-end method that takes raw temporal signals as inputs and thus doesn't need any time consuming denoising preprocessing. The model can achieve pretty high accuracy under noisy environment. Second, the model does not rely on any domain adaptation algorithm or require information of the target domain. It can achieve high accuracy when working load is changed. To understand the proposed model, we will visualize the learned features, and try to analyze the reasons behind the high performance of the model. © 2017","Anti-noise; Convolutional neural networks; End-to-end; Intelligent fault diagnosis; Load domain adaptation","Convolution; Deep neural networks; Failure analysis; Learning algorithms; Learning systems; Neural networks; Anti noise; Convolutional neural network; Domain adaptation; End to end; Intelligent fault diagnosis; Fault detection",2-s2.0-85028727944
"Gadaleta M., Rossi M.","IDNet: Smartphone-based gait recognition with convolutional neural networks",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032276192&doi=10.1016%2fj.patcog.2017.09.005&partnerID=40&md5=ac02b88284a2035e2620f859d00e7356","Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. IDNet features several innovations including: (i) a robust and smartphone-orientation-independent walking cycle extraction block, (ii) a novel feature extractor based on convolutional neural networks, (iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into (iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance. © 2017 Elsevier Ltd","Accelerometer; Biometric gait analysis; Classification methods; Convolutional neural networks; Feature extraction; Gyroscope; Inertial sensors; Signal processing; Support vector machines; Target recognition","Accelerometers; Convolution; Decision making; Extraction; Feature extraction; Gait analysis; Gyroscopes; Neural networks; Pattern recognition; Signal processing; Smartphones; Support vector machines; Walking aids; Classification methods; Convolutional neural network; Decision-making frameworks; Inertial sensor; One-class support vector machine; State-of-the-art techniques; Target recognition; Universal feature extractors; Authentication",2-s2.0-85032276192
"Li X., Wang W., Xue F., Song Y.","Computational modeling of spiking neural network with learning rules from STDP and intrinsic plasticity",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031491089&doi=10.1016%2fj.physa.2017.08.053&partnerID=40&md5=bc24df7eaef7775a166bb81fbfe9e9a4","Recently there has been continuously increasing interest in building up computational models of spiking neural networks (SNN), such as the Liquid State Machine (LSM). The biologically inspired self-organized neural networks with neural plasticity can enhance the capability of computational performance, with the characteristic features of dynamical memory and recurrent connection cycles which distinguish them from the more widely used feedforward neural networks. Despite a variety of computational models for brain-like learning and information processing have been proposed, the modeling of self-organized neural networks with multi-neural plasticity is still an important open challenge. The main difficulties lie in the interplay among different forms of neural plasticity rules and understanding how structures and dynamics of neural networks shape the computational performance. In this paper, we propose a novel approach to develop the models of LSM with a biologically inspired self-organizing network based on two neural plasticity learning rules. The connectivity among excitatory neurons is adapted by spike-timing-dependent plasticity (STDP) learning; meanwhile, the degrees of neuronal excitability are regulated to maintain a moderate average activity level by another learning rule: intrinsic plasticity (IP). Our study shows that LSM with STDP+IP performs better than LSM with a random SNN or SNN obtained by STDP alone. The noticeable improvement with the proposed method is due to the better reflected competition among different neurons in the developed SNN model, as well as the more effectively encoded and processed relevant dynamic information with its learning and self-organizing mechanism. This result gives insights to the optimization of computational models of spiking neural networks with neural plasticity. © 2017 Elsevier B.V.","Intrinsic plasticity; Reservoir computing; Spiking neural network; STDP","Computation theory; Computational methods; Intelligent agents; Internet protocols; Neural networks; Neurons; Recurrent neural networks; Computational performance; Intrinsic plasticity; Reservoir Computing; Self-organized neural networks; Spike timing dependent plasticities; Spiking neural network(SNN); Spiking neural networks; STDP; Feedforward neural networks",2-s2.0-85031491089
"Rady A., Adedeji A.","Assessing different processed meats for adulterants using visible-near-infrared spectroscopy",2018,"Meat Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032338327&doi=10.1016%2fj.meatsci.2017.10.014&partnerID=40&md5=2aa18029e81e554c7ad23f13af167dde","The main objective of this study was to investigate the use of spectroscopic systems in the range of 400–1000 nm (visible/near-infrared or Vis-NIR) and 900–1700 nm (NIR) to assess and estimate plant and animal proteins as potential adulterants in minced beef and pork. Multiple machine learning techniques were used for classification, adulterant prediction, and wavelength selection. Samples were first evaluated for the presence or absence of adulterants (6 classes), and secondly for adulterant type (6 classes) and level. Selected wavelengths models generally resulted in better classification and prediction outputs than full wavelengths. The first stage classification rates were 96% and 100% for pure/unadulterated and adulterated samples, respectively. Whereas, the second stage had classification rates of 69–100%. The optimal models for predicting adulterant levels yielded correlation coefficient, r of 0.78–0.86 and ratio of performance to deviation, RPD, of 1.19–1.98. The results from this study illustrate potential application of spectroscopic technology to rapidly and accurately detect adulterants in minced beef and pork. © 2017 Elsevier Ltd","Adulteration; Chicken; Gluten; Minced beef; Pork; Texturized vegetable protein; Vis-NIR spectroscopy","Animals; Beef; Forecasting; Infrared devices; Learning systems; Near infrared spectroscopy; Proteins; Adulteration; Chicken; Gluten; Minced beef; Pork; Vegetable protein; Vis-NIR spectroscopy; Meats",2-s2.0-85032338327
"Shang L., Liu C., Tomiura Y., Hayashi K.","Odorant clustering based on molecular parameter-feature extraction and imaging analysis of olfactory bulb odor maps",2018,"Sensors and Actuators, B: Chemical",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027857879&doi=10.1016%2fj.snb.2017.08.024&partnerID=40&md5=089873c6590f79ec9b549031470fce3f","Progress in the molecular biology of olfaction has revealed a close relationship between the structural features of odorants and the response patterns they elicit in the olfactory bulb. Molecular feature-related response patterns, termed odor maps (OMs), may represent information related to basic odor quality. Thus, studying the relationship between OMs and the molecular features of odorants is helpful for better understanding the relationships between odorant structure and odor. Here, we explored the correlation between OMs and the molecular parameters (MPs) of odorants by taking OMs from rat olfactory bulbs and extracting feature profiles of the corresponding odorant molecules. 178 images of glomerular activities in olfactory bulb that are corresponding to odorants were taken from the OdorMapDB, a publicly accessible database. The gray value of each pixel was extracted from the images (178 × 357 pixels) to fabricate an image matrix for each odorant. Forty-six molecular feature parameters were calculated using BioChem3D software, which was used to construct a second matrix for each odorant. Correlation analysis between the two matrixes was first carried out by establishing coefficient maps. Results from hierarchical clustering showed that all parameters could be segregated into seven clusters, and each cluster showed a relatively similar response pattern in the olfactory bulb. Using the information from the OMs and MPs, we mapped odorants in 2D space by incorporating dimension-reducing techniques based on principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE). Artificial neural network models based on the OM and MP feature values were proposed as a means to identify odorant functional groups. An OM-PCA-based model calibrated via extreme learning machine (ELM) was 94.81% and 93.02% accuracy for the calibration and validation sets, respectively. Similarly, an MP-t-SNE-based model calibrated by ELM was 86.67% and 93.35% accuracy for the calibration set and the validation set, respectively. Thus, this research supports a structure-odor relationship from a data-analysis perspective. © 2017 Elsevier B.V.","Artificial neural network; Classification analysis; Image analysis; Olfactory bulb odor map; Physicochemical parameter; t-Distributed stochastic neighbor embedding",,2-s2.0-85027857879
"Liu Z.-T., Wu M., Cao W.-H., Mao J.-W., Xu J.-P., Tan G.-Z.","Speech emotion recognition based on feature selection and extreme learning machine decision tree",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028353735&doi=10.1016%2fj.neucom.2017.07.050&partnerID=40&md5=a4fd14f1eb2427170b3fa377058f548d","Feature selection is a crucial step in the development of a system for identifying emotions in speech. Recently, the interaction between features generated from the same audio source was rarely considered, which may produce redundant features and increase the computational costs. To solve this problem, feature selection method based on correlation analysis and Fisher is proposed, which can remove the redundant features that have close correlations with each other. To improve the recognition performance of the feature subset after proposal feature selection further, an emotion recognition method based on extreme learning machine (ELM) decision tree is proposed according to the confusion degree among different basic emotions. A framework of speech emotion recognition is proposed and the classification experiments based on proposed classification method by using Chinese speech database from institute of automation of Chinese academy of sciences (CASIA) are performed. And the experimental results show that the proposal achieved 89.6% recognition rate on average. By proposal, it would be fast and efficient to discriminate emotional states of different speakers from speech, and it would make it possible to realize the interaction between speaker-independent and computer/robot in the future. © 2017 Elsevier B.V.","Correlation analysis; Decision tree; Extreme learning machine; Feature selection; Speech emotion recognition","Classification (of information); Correlation methods; Decision trees; Feature extraction; Knowledge acquisition; Learning systems; Speech; Chinese Academy of Sciences; Classification methods; Computational costs; Correlation analysis; Extreme learning machine; Feature selection methods; Speaker independents; Speech emotion recognition; Speech recognition",2-s2.0-85028353735
"Tao Y., Tang Y., Shi F., Ren C.","Error bound of magnitude-preserving ranking with Fredholm kernel",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028949561&doi=10.1016%2fj.neucom.2017.07.049&partnerID=40&md5=2f84c2e4a7b017d3e35dfedeaccd475b","Data-dependent Fredholm kernel has attracted much attention in machine learning literatures for its flexibility to utilize the empirical information. However, the previous theoretical results are limited to the classification or density ratio estimation problems. In this paper, we extend the framework of learning with Fredholm kernel to the ranking setting. A new magnitude-preserving ranking with Fredholm kernel is proposed, and its generalization error analysis is established by using the concentrate estimate techniques. The derived result implies that the proposed method can achieve the satisfactory learning rate with polynomial decay. © 2017 Elsevier B.V.","Data dependent hypothesis spaces; Fredholm kernel; Generalization bound; Ranking","Computer applications; Neural networks; Density-ratio estimations; Fredholm; Generalization bound; Generalization Error; Hypothesis space; Machine learning literature; Polynomial decay; Ranking; Learning systems",2-s2.0-85028949561
"Hayashi Y., Iiduka H.","Optimality and convergence for convex ensemble learning with sparsity and diversity based on fixed point optimization",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028376856&doi=10.1016%2fj.neucom.2017.07.046&partnerID=40&md5=cf741c9d0de63f4c539ec759ecadbaf4","This paper discusses the classifier ensemble problem with sparsity and diversity learning, which is a central issue in machine learning. The current approach for reducing the size and increasing the accuracy of a classifier ensemble is to formulate it as a convex quadratic programming problem, which is a relaxation problem, and then solve it by using the existing methods for convex quadratic programming or by computing closed-form solutions. This paper presents a novel computational approach for solving the classifier ensemble problem with sparsity and diversity learning without any recourse to relaxation problems and their associated methods. We first show that the classifier ensemble problem can be expressed as a minimization problem for the sum of certain convex functions over the intersection of fixed point sets of quasi-nonexpansive mappings. Next, we propose fixed point optimization algorithms for solving the minimization problem and show that the algorithms converge to the solution of the minimization problem. It is shown that the proposed algorithms can directly solve the classifier ensemble problem with sparsity and diversity learning. Finally, we compare the performance of the proposed sparsity and diversity learning methods against an existing method in classification experiments using data sets from the UCI machine learning repository and the LIBSVM. The experimental results show that the proposed methods have higher classification accuracies than the existing method. © 2017 Elsevier B.V.","Convex ensemble learning; Fixed point; Incremental subgradient method; Quasi-nonexpansive mapping","Artificial intelligence; Classification (of information); Functions; Learning systems; Mapping; Optimization; Quadratic programming; 65K05; 68Q32; 90C25; Ensemble learning; Fixed points; Quasi-nonexpansive mapping; Sub-gradient methods; Problem solving",2-s2.0-85028376856
"Lei J., Mu H.P., Liu Q.B., Wang X.Y., Liu S.","Data-driven reconstruction method for electrical capacitance tomography",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028311314&doi=10.1016%2fj.neucom.2017.08.006&partnerID=40&md5=a95906a903ae986511ec4070ede2c96f","The appealing superiorities, including high-speed data acquisition, nonintrusive measurement, low cost, high safety and visual presentation, lead to the success of the electrical capacitance tomography (ECT) technique in the monitoring of industrial processes. High-accuracy tomographic images play a crucial role in the reliability of the ECT measurement results, which provide the powerful scientific evidences for investigating the complicated mechanisms behind the behaviors of the imaging objects (IOs). Beyond the existing numerical algorithms that are developed for the solution of the inverse problem in the ECT area, a data-driven two-stage reconstruction method is proposed to improve the reconstruction quality (RQ) in this paper. At the first stage, i.e., the learning stage, the regularized extreme learning machine (RELM) model solved by the split Bregman technique is developed to extract the mapping between the tomographic images reconstructed by the some algorithm and the true images according to a set of training samples. At the second stage, i.e., the prediction stage, a new IO is reconstructed by the same algorithm used in computing training samples, and then the imaging result is considered as an input of the trained RELM model to predict the final result. The performances of the proposed reconstruction method are compared and evaluated by the means of the numerical simulation approach using the clean and noisy capacitance data with different noise levels (NLs). Quantitative and qualitative comparison results validate the practicability and effectiveness of the proposed data-driven reconstruction method. Research findings provide a new insight for the improvement of the reconstruction accuracy and robustness in the ECT area. © 2017","Electrical capacitance tomography; Extreme learning machine; Image reconstruction; Inverse problem; Reconstruction method","Accident prevention; Capacitance; Data acquisition; Electric impedance tomography; Image processing; Inverse problems; Knowledge acquisition; Learning systems; Numerical methods; Sampling; Tomography; Electrical Capacitance Tomography; Extreme learning machine; High speed data acquisition; Non-intrusive measurements; Numerical simulation approaches; Reconstruction accuracy; Reconstruction method; Reconstruction quality; Image reconstruction",2-s2.0-85028311314
"Amin M.J., Riza N.A.","Machine learning enhanced optical distance sensor",2018,"Optics Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030225987&doi=10.1016%2fj.optcom.2017.09.028&partnerID=40&md5=f8f87666b013457e3e2515b48aa697ce","Presented for the first time is a machine learning enhanced optical distance sensor. The distance sensor is based on our previously demonstrated distance measurement technique that uses an Electronically Controlled Variable Focus Lens (ECVFL) with a laser source to illuminate a target plane with a controlled optical beam spot. This spot with varying spot sizes is viewed by an off-axis camera and the spot size data is processed to compute the distance. In particular, proposed and demonstrated in this paper is the use of a regularized polynomial regression based supervised machine learning algorithm to enhance the accuracy of the operational sensor. The algorithm uses the acquired features and corresponding labels that are the actual target distance values to train a machine learning model. The optimized training model is trained over a 1000 mm (or 1 m) experimental target distance range. Using the machine learning algorithm produces a training set and testing set distance measurement errors of <0.8 mm and <2.2 mm, respectively. The test measurement error is at least a factor of 4 improvement over our prior sensor demonstration without the use of machine learning. Applications for the proposed sensor include industrial scenario distance sensing where target material specific training models can be generated to realize low <1% measurement error distance measurements. © 2017 Elsevier B.V.","Electronic lens; Machine learning; Optical distance sensing; Polynomial regression; Regularization","Artificial intelligence; Distance measurement; Errors; Learning systems; Lenses; Measurement errors; Supervised learning; Electronic lens; Machine learning models; Measurement techniques; Optical distance; Polynomial regression; Regularization; Sensor demonstrations; Supervised machine learning; Learning algorithms",2-s2.0-85030225987
"Sobie C., Freitas C., Nicolai M.","Simulation-driven machine learning: Bearing fault classification",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026874827&doi=10.1016%2fj.ymssp.2017.06.025&partnerID=40&md5=c40d375025b51a8496fdb7f8844e601d","Increasing the accuracy of mechanical fault detection has the potential to improve system safety and economic performance by minimizing scheduled maintenance and the probability of unexpected system failure. Advances in computational performance have enabled the application of machine learning algorithms across numerous applications including condition monitoring and failure detection. Past applications of machine learning to physical failure have relied explicitly on historical data, which limits the feasibility of this approach to in-service components with extended service histories. Furthermore, recorded failure data is often only valid for the specific circumstances and components for which it was collected. This work directly addresses these challenges for roller bearings with race faults by generating training data using information gained from high resolution simulations of roller bearing dynamics, which is used to train machine learning algorithms that are then validated against four experimental datasets. Several different machine learning methodologies are compared starting from well-established statistical feature-based methods to convolutional neural networks, and a novel application of dynamic time warping (DTW) to bearing fault classification is proposed as a robust, parameter free method for race fault detection. © 2017 Elsevier Ltd","Condition monitoring; Fault detection; Machine learning; Prognostic health monitoring; Roller bearing","Artificial intelligence; Condition monitoring; Fault detection; Learning systems; Machine components; Neural networks; Roller bearings; Rollers (machine components); Systems engineering; Computational performance; Convolutional neural network; Dynamic time warping; Health monitoring; High resolution simulations; Parameter-free methods; Scheduled maintenance; Statistical features; Learning algorithms",2-s2.0-85026874827
"Cerasa A., Lofaro D., Cavedini P., Martino I., Bruni A., Sarica A., Mauro D., Merante G., Rossomanno I., Rizzuto M., Palmacci A., Aquino B., De Fazio P., Perna G.R., Vanni E., Olivadese G., Conforti D., Arabia G., Quattrone A.","Personality biomarkers of pathological gambling: A machine learning study",2018,"Journal of Neuroscience Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032707333&doi=10.1016%2fj.jneumeth.2017.10.023&partnerID=40&md5=aaac4c1668ae6a45a28b54f2ca11ebca","Background The application of artificial intelligence to extract predictors of Gambling disorder (GD) is a new field of study. A plethora of studies have suggested that maladaptive personality dispositions may serve as risk factors for GD. New method Here, we used Classification and Regression Trees algorithm to identify multivariate predictive patterns of personality profiles that could identify GD patients from healthy controls at an individual level. Forty psychiatric patients, recruited from specialized gambling clinics, without any additional comorbidity and 160 matched healthy controls completed the Five-Factor model of personality as measured by the NEO-PI-R, which were used to build the classification model. Results Classification algorithm was able to discriminate individuals with GD from controls with an AUC of 77.3% (95% CI 0.65–0.88, p < 0.0001). A multidimensional construct of traits including sub-facets of openness, neuroticism and conscientiousness was employed by algorithm for classification detection. Comparison with existing method(s) To the best of our knowledge, this is the first study that combines behavioral data with machine learning approach useful to extract multidimensional features characterizing GD realm. Conclusion Our study provides a proof-of-concept demonstrating the potential of the proposed approach for GD diagnosis. The multivariate combination of personality facets characterizing individuals with GD can potentially be used to assess subjects’ vulnerability in clinical setting. © 2017 Elsevier B.V.","Conscientiousness; Five-factor model; Machine learning; Neuroticism; Openness; Pathological gambling; Personality profile",,2-s2.0-85032707333
"Jia X., Jin C., Buzza M., Di Y., Siegel D., Lee J.","A deviation based assessment methodology for multiple machine health patterns classification and fault detection",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026865921&doi=10.1016%2fj.ymssp.2017.06.015&partnerID=40&md5=2f621185b218ad927a0d234ead889e3b","Successful applications of Diffusion Map (DM) in machine failure detection and diagnosis have been reported in several recent studies. DM provides an efficient way to visualize the high-dimensional, complex and nonlinear machine data, and thus suggests more knowledge about the machine under monitoring. In this paper, a DM based methodology named as DM-EVD is proposed for machine degradation assessment, abnormality detection and diagnosis in an online fashion. Several limitations and challenges of using DM for machine health monitoring have been analyzed and addressed. Based on the proposed DM-EVD, a deviation based methodology is then proposed to include more dimension reduction methods. In this work, the incorporation of Laplacian Eigen-map and Principal Component Analysis (PCA) are explored, and the latter algorithm is named as PCA-Dev and is validated in the case study. To show the successful application of the proposed methodology, case studies from diverse fields are presented and investigated in this work. Improved results are reported by benchmarking with other machine learning algorithms. © 2017 Elsevier Ltd","Bearing; Diffusion map; Principal component analysis; Prognostic and health management; Semiconductor; Wind turbine","Bearings (structural); Fault detection; Health; Learning algorithms; Learning systems; Semiconductor materials; Wind turbines; Abnormality detection; Assessment methodologies; Degradation assessment; Diffusion maps; Dimension reduction method; Machine health monitoring; Patterns classification; Prognostic and health management; Principal component analysis",2-s2.0-85026865921
"Tarnopolski M.","Correlation between the Hurst exponent and the maximal Lyapunov exponent: Examining some low-dimensional conservative maps",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029331348&doi=10.1016%2fj.physa.2017.08.159&partnerID=40&md5=227fefd76fc423aefa3576dcaad03a3b","The Chirikov standard map and the 2D Froeschlé map are investigated. A few thousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent (mLE) are plotted in a mixed space of the nonlinear parameter versus the initial condition. Both characteristic exponents reveal remarkably similar structures in this space. A tight correlation between the HEs and mLEs is found, with the Spearman rank ρ=0.83 and ρ=0.75 for the Chirikov and 2D Froeschlé maps, respectively. Based on this relation, a machine learning (ML) procedure, using the nearest neighbor algorithm, is performed to reproduce the HE distribution based on the mLE distribution alone. A few thousand HE and mLE values from the mixed spaces were used for training, and then using 2−2.4×105 mLEs, the HEs were retrieved. The ML procedure allowed to reproduce the structure of the mixed spaces in great detail. © 2017 Elsevier B.V.","Chirikov standard map; Conservative systems; Hurst exponent; Machine learning; Maximal Lyapunov exponent","Artificial intelligence; Differential equations; Learning systems; Lyapunov functions; Characteristic exponents; Conservative systems; Hurst exponents; Initial conditions; Maximal Lyapunov exponent; Nearest neighbor algorithm; Non-linear parameters; Standard map; Lyapunov methods",2-s2.0-85029331348
"Ahmed H.O.A., Wong M.L.D., Nandi A.K.","Intelligent condition monitoring method for bearing faults from highly compressed measurements using sparse over-complete features",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026870301&doi=10.1016%2fj.ymssp.2017.06.027&partnerID=40&md5=e828be9c481bcc6d3f289c06795b1bc5","Condition classification of rolling element bearings in rotating machines is important to prevent the breakdown of industrial machinery. A considerable amount of literature has been published on bearing faults classification. These studies aim to determine automatically the current status of a roller element bearing. Of these studies, methods based on compressed sensing (CS) have received some attention recently due to their ability to allow one to sample below the Nyquist sampling rate. This technology has many possible uses in machine condition monitoring and has been investigated as a possible approach for fault detection and classification in the compressed domain, i.e., without reconstructing the original signal. However, previous CS based methods have been found to be too weak for highly compressed data. The present paper explores computationally, for the first time, the effects of sparse autoencoder based over-complete sparse representations on the classification performance of highly compressed measurements of bearing vibration signals. For this study, the CS method was used to produce highly compressed measurements of the original bearing dataset. Then, an effective deep neural network (DNN) with unsupervised feature learning algorithm based on sparse autoencoder is used for learning over-complete sparse representations of these compressed datasets. Finally, the fault classification is achieved using two stages, namely, pre-training classification based on stacked autoencoder and softmax regression layer form the deep net stage (the first stage), and re-training classification based on backpropagation (BP) algorithm forms the fine-tuning stage (the second stage). The experimental results show that the proposed method is able to achieve high levels of accuracy even with extremely compressed measurements compared with the existing techniques. © 2017 The Authors","Bearing fault classification; Compressed sensing; Deep neural network; Machine condition monitoring; Sparse autoencoder; Sparse over-complete representations","Backpropagation algorithms; Bearings (machine parts); Compressed sensing; Deep learning; Deep neural networks; Fault detection; Learning algorithms; Learning systems; Machinery; Roller bearings; Signal reconstruction; Auto encoders; Bearing fault; Classification performance; Fault detection and classification; Intelligent condition monitoring; Machine condition monitoring; Over-complete representations; Unsupervised feature learning; Condition monitoring",2-s2.0-85026870301
"Karnath H.-O., Sperber C., Rorden C.","Mapping human brain lesions and their functional consequences",2018,"NeuroImage",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032203415&doi=10.1016%2fj.neuroimage.2017.10.028&partnerID=40&md5=a53644b5052d3059827a1e26dfc15398","Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications. © 2017 Elsevier Inc.","Cognitive neurology; Human; Lesion analysis; Machine learning; Mass-univariate; MLBM; Multivariate lesion behavior mapping; Network; Neuroanatomy; Neuropsychology; Non-parametric mapping; Stroke; VLBM; VLSM; Voxel-based lesion symptom mapping","gadolinium; BOLD signal; brain blood flow; brain mapping; cerebrovascular accident; cognition; connectome; diagnostic accuracy; diffusion weighted imaging; emotion; episodic memory; functional magnetic resonance imaging; glioblastoma; human; machine learning; perfusion weighted imaging; population research; priority journal; Review; temporal lobe epilepsy; voxel based lesion behavior mapping; voxel based morphometry",2-s2.0-85032203415
"Cerrada M., Sánchez R.-V., Li C., Pacheco F., Cabrera D., Valente de Oliveira J., Vásquez R.E.","A review on data-driven fault severity assessment in rolling bearings",2018,"Mechanical Systems and Signal Processing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026877097&doi=10.1016%2fj.ymssp.2017.06.012&partnerID=40&md5=268f8ef3bc81362533fe75cb43a51255","Health condition monitoring of rotating machinery is a crucial task to guarantee reliability in industrial processes. In particular, bearings are mechanical components used in most rotating devices and they represent the main source of faults in such equipments; reason for which research activities on detecting and diagnosing their faults have increased. Fault detection aims at identifying whether the device is or not in a fault condition, and diagnosis is commonly oriented towards identifying the fault mode of the device, after detection. An important step after fault detection and diagnosis is the analysis of the magnitude or the degradation level of the fault, because this represents a support to the decision-making process in condition based-maintenance. However, no extensive works are devoted to analyse this problem, or some works tackle it from the fault diagnosis point of view. In a rough manner, fault severity is associated with the magnitude of the fault. In bearings, fault severity can be related to the physical size of fault or a general degradation of the component. Due to literature regarding the severity assessment of bearing damages is limited, this paper aims at discussing the recent methods and techniques used to achieve the fault severity evaluation in the main components of the rolling bearings, such as inner race, outer race, and ball. The review is mainly focused on data-driven approaches such as signal processing for extracting the proper fault signatures associated with the damage degradation, and learning approaches that are used to identify degradation patterns with regards to health conditions. Finally, new challenges are highlighted in order to develop new contributions in this field. © 2017 Elsevier Ltd","Fault assessment; Fault severity; Fault size; Quantitative diagnosis; Rolling bearings","Bearings (machine parts); Condition monitoring; Damage detection; Decision making; Machinery; Roller bearings; Signal processing; Condition based maintenance; Decision making process; Fault assessment; Fault detection and diagnosis; Fault severities; Mechanical components; Quantitative diagnosis; Rolling bearings; Fault detection",2-s2.0-85026877097
"Lin D., Li L., Cao D., Lv Y., Ke X.","Multi-modality weakly labeled sentiment learning based on Explicit Emotion Signal for Chinese microblog",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024483725&doi=10.1016%2fj.neucom.2017.06.078&partnerID=40&md5=d56fbca7fd086c41e49666f84c7c0786","Understanding the sentiments of users from cross media contents which contain texts and images is an important task for many social network applications. However, due to the semantic gap between cross media features and sentiments, machine learning methods need a lot of human labeled samples. Furthermore, for each kind of media content, it is necessary to constantly add a lot of new human labeled samples because of new expressions of sentiments. Fortunately, there are some emotion signals, like emoticons, which denote users’ emotions in cross media contents. In order to use these weakly labels to build a unified multi-modality sentiment learning framework, we propose an Explicit Emotion Signal (EES) based multi-modality sentiment learning approach which uses huge number of weakly labeled samples in sentiment learning. There are three advantages in our approach. Firstly, only a few human labeled samples are needed to reach the same performance which can be obtained by the traditional machine learning based sentiment prediction approaches. Secondly, this approach is flexible and can easily combine text and vision based sentiment learning through deep neural networks. Thirdly, because a lot of weakly labeled samples can be used in EES, trained model is more robust in different domain transfer. In this paper, firstly, we investigate the correlation between sentiments and emoticons and choose emoticons as the Explicit Emotion Signals in our approach; secondly, we build a two stages multi-modality sentiment learning framework based on Explicit Emotion Signals. Our experiment results show that our approach not only achieves the best performance but also only needs 3% and 43% training samples to obtain the same performance of Visual Geometry Group (VGG) model and Long Short-Term Memory (LSTM) model in images and texts, respectively. © 2017 Elsevier B.V.","Cross media; Domain transfer; Explicit Emotion Signal; Multi-modality sentiment learning; Weakly labeled sample","Artificial intelligence; Deep neural networks; Education; Learning systems; Long short-term memory; Semantics; Cross-media; Different domains; Domain transfers; Learning approach; Learning frameworks; Machine learning methods; Multi modality; Network applications; Deep learning",2-s2.0-85024483725
"Durães D., Carneiro D., Jiménez A., Novais P.","Characterizing attentive behavior in intelligent environments",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028319715&doi=10.1016%2fj.neucom.2017.05.091&partnerID=40&md5=74c3466a6c5f25a2b805adfb537cdf02","Learning styles are strongly connected with learning and when it comes to acquiring new knowledge, attention is one the most important mechanisms. The learner's attention affects learning results and can define the success or failure of a student. When students are carrying out learning activities using new technologies, it is extremely important that the teacher has some feedback from the students’ work in order to detect potential learning problems at an early stage and then to choose the appropriate teaching methods. In this paper we present a nonintrusive distributed system for monitoring the attention level in students. It is especially suited for classes working at the computer. The presented system is able to provide real-time information about each student as well as information about the class, and make predictions about the best learning style for a student using an ensemble of neural networks. It can be very useful for teachers to identify potentially distracting events and this system might be very useful to the teacher to implement more suited teaching strategies. © 2017","Ambient intelligent; Attentiveness; Learning activities; Learning styles; Machine learning","Learning systems; Students; Teaching; Ambient intelligent; Attentive behavior; Attentiveness; Distributed systems; Intelligent environment; Learning Activity; Learning Style; Real-time information; Education",2-s2.0-85028319715
"Nath A., Subbiah K.","The role of pertinently diversified and balanced training as well as testing data sets in achieving the true performance of classifiers in predicting the antifreeze proteins",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023601684&doi=10.1016%2fj.neucom.2017.07.004&partnerID=40&md5=8966ea55d19a336cbd1cede784bf876f","Antifreeze proteins (AFPs) are those proteins, which inhibit the ice nucleation process and thereby enabling certain organisms to survive under sub-zero temperature habitats. AFPs are supposed to be evolved from different types of protein families to perform the unique function of antifreeze activity and turn out to be the classical example of convergent evolution. The common sequence similarity search methods have failed to predict putative AFPs due to poor sequence and structural similarity that exists among the different sub-types of AFP. The machine learning techniques are the viable alternative approaches to predict putative AFPs. In this paper, we have discussed about the criteria (like apposite feature selection, balanced data sets and complete learning) that are needed to be taken into account for successful application of machine learning methods and implemented these criteria by using a clustering procedure in order to achieve the true performance of the learning algorithms. Diversified and representative training and testing data sets are very crucial for perfect learning as well as true testing of machine learning based prediction methods for two reasons: first is that a training dataset that lacks definable subset of input patterns makes prediction of patterns belonging to this subset either difficult or unfeasible (thus resulting in incomplete learning) and secondly a testing data set that lacks definable subset of input patterns does not tell about whether this subset of patterns can be correctly predicted by the classifier or not (thus resulting in incomplete testing). Moreover, balanced training and testing data sets are equally important for achieving the true (robust) performance of classifiers because a well-balanced training set eliminates bias of the classifier toward particular class/sub-class due to over-representation or under-representation of input patterns belonging to those classes/sub-classes. We have used K-means clustering algorithm for creating the diversified and balanced training as well as testing data sets, to overcome the shortcoming of random splitting, which cannot guarantee representative training and testing sets. The current clustering based optimal splitting criteria proved to be better than random splitting for creating training and testing set in terms of superior generalization and robust evaluation. © 2017 Elsevier Ltd","Antifreeze proteins; Imbalance data set; Incomplete learning; K-means clustering; Physicochemical-n-grams; Representative training set","Artificial intelligence; Biology; Clustering algorithms; Education; Forecasting; Learning algorithms; Learning systems; Proteins; Set theory; Speech recognition; Statistical tests; Antifreeze protein; Imbalance datum; Incomplete learning; K-means clustering; N-grams; Training sets; Classification (of information)",2-s2.0-85023601684
"Wang X., Zhang W., Yan J., Yuan X., Zha H.","On the flexibility of block coordinate descent for large-scale optimization",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026459621&doi=10.1016%2fj.neucom.2017.07.024&partnerID=40&md5=d83975f9cf3c19238a822b79a2f76d9e","We consider a large-scale minimization problem (not necessarily convex) with non-smooth separable convex penalty. Problems in this form widely arise in many modern large-scale machine learning and signal processing applications. In this paper, we present a new perspective towards the parallel Block Coordinate Descent (BCD) methods. Specifically we explicitly give a concept of so-called two-layered block variable updating loop for parallel BCD methods in modern computing environment comprised of multiple distributed computing nodes. The outer loop refers to the block variable updating assigned to distributed nodes, and the inner loop involves the updating step inside each node. Each loop allows to adopt either Jacobi or Gauss–Seidel update rule. In particular, we give detailed theoretical convergence analysis to two practical schemes: Jacobi/Gauss–Seidel and Gauss–Seidel/Jacobi that embodies two algorithms respectively. Our new perspective and behind theoretical results help devise parallel BCD algorithms in a principled fashion, which in turn lend them a flexible implementation for BCD methods suited to the parallel computing environment. The effectiveness of the algorithm framework is verified on the benchmark tasks of large-scale ℓ1 regularized sparse logistic regression and non-negative matrix factorization. © 2017 Elsevier B.V.","Block coordinate descent; Gauss–Seidel; Jacobi; Large-scale optimization","Distributed computer systems; Factorization; Gaussian distribution; Learning systems; Signal processing; Block coordinate descents; Gauss-Seidel; Jacobi; Large-scale minimization problems; Large-scale optimization; Nonnegative matrix factorization; Parallel-computing environment; Signal processing applications; Optimization",2-s2.0-85026459621
"Feng H.-M., Wong C.-C., Horng J.-H., Lai L.-Y.","Evolutional RBFNs image model describing-based segmentation system designs",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026674912&doi=10.1016%2fj.neucom.2017.07.006&partnerID=40&md5=dfe5993f06eb7d938c834b4bbed02370","Knowledge discovered-based radial basis function neural networks (RBFNs) model can describe an appropriate behaviors of identified image patterns through the multiple and hybrid learning schemes. The image data extraction learning algorithm (IDELA) with dynamic recognitions to automatically match the appropriate feature with a suitable number of radial basis function (RBFs). This first step approaches their associated centers positions to extract initial prototypes. The approximated image model as a describer is automatically generated by the RBFPSO learning scheme, which is contained hybrid bacterial foraging particle swarm optimization (BFPSO) algorithm and recursive least-squares (RLS) iterations to deeply approach the image feature. Due to the limitations and possible local learning trap, K-means, differential evolution (DE) and particle swarm optimization (PSO) learning algorithms cannot obtain the most smaller Root-Mean-Square Error (RMSE) to achieve an appropriate image segmentation in all experiment cases. The constructed RBFNs image model is generated by the support of multiple image self-extraction feature machine (MISEFM), which combined IDELA and RBFPSO algorithms to develop the universal RBFNs image describers. Simulations compared with other K-means, PSO and DE learning methods, show the average great performance in several real image segmentation applications. The peak signal-to-noise ratio (PSNR) index is selected to evaluate the quality of the reconstructed images. Simulations show that the evolutional hybrid and multi-level RBFNs image model-based system is determined to simultaneously achieve both high performance indexes on accuracy (RMSE) and a high image quality description (PSNR) for matching the desired characters and behaviors of image patterns within a fewer RBFs functions. © 2017 Elsevier Ltd","Bacterial foraging particle swarm optimization; Image segmentation; RBFNs; Recursive least-squares","Evolutionary algorithms; Extraction; Functions; Image processing; Image quality; Learning algorithms; Mean square error; Optimization; Particle swarm optimization (PSO); Radial basis function networks; Signal to noise ratio; Swarm intelligence; Automatically generated; Bacterial foraging; Peak signal to noise ratio; Radial basis function neural networks; Radial basis functions; RBFNs; Recursive least square (RLS); Root mean square errors; Image segmentation",2-s2.0-85026674912
"Fan Z., Bi D., Xiong L., Ma S., He L., Ding W.","Dim infrared image enhancement based on convolutional neural network",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024107173&doi=10.1016%2fj.neucom.2017.07.017&partnerID=40&md5=7010146fa3f4e4f643200268579e3727","Long-range infrared images are always suffering from dim targets and background clutters. To improve the contrast between target and background, we propose a novel infrared image enhancement approach by highlighting target and suppressing background clutters. Predicting the target and background plays a key role in improving the contrast of dim infrared images that targets are embedded by background clutters. Taking full advantage of machine learning on prediction, we design the convolutional neural network (CNN) architecture in our study. To overcome the lack of large training data, the handwritten images in MNIST dataset are employed to simulate the properties of long-rang infrared images including dim targets, background clutters and low contrast. The target and background sub-images are predicted from the original dim infrared image based on the filters in the first layer of the trained CNN. Finally, the dim infrared image is enhanced by amplifying the targets and subtracting background clutters. The results of subjective and quantitative tests prove the performance of the proposed algorithm in contrast improvement. © 2017 Elsevier B.V.","Image enhancement; Infrared image processing; Neural networks; Pattern recognition; Spatial filtering","Clutter (information theory); Convolution; Image processing; Infrared imaging; Neural networks; Pattern recognition; Background clutter; Contrast improvements; Convolutional neural network; Handwritten images; Quantitative tests; Spatial filterings; Target and background; Training data; Image enhancement",2-s2.0-85024107173
"Ding W., Lin C.-T., Chen S., Zhang X., Hu B.","Multiagent-consensus-MapReduce-based attribute reduction using co-evolutionary quantum PSO for big data applications",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023638265&doi=10.1016%2fj.neucom.2017.06.059&partnerID=40&md5=e8978ef7b64c6205fefad605ae289171","The attribute reduction for big data applications has become an urgent challenge in pattern recognition, machine learning and data mining. In this paper, we introduce the multi-agent consensus MapReduce optimization model and co-evolutionary quantum PSO with self-adaptive memeplexes for designing the attribute reduction method, and propose a multiagent-consensus-MapReduce-based attribute reduction algorithm (MCMAR). Firstly, the co-evolutionary quantum PSO with self-adaptive memeplexes is designed for grouping particles into different memeplexes, which aims to explore the search space and locate the global best region during the attribute reduction of big datasets. Secondly, the four layers neighborhood radius framework with compensatory scheme is constructed to partition big attribute sets by exploiting the interdependency among multiple-relevant-attribute sets. Thirdly, a novel multi-agent consensus MapReduce optimization model is adopted to perform the multiple-relevance-attribute reduction, in which five kinds of agents are used to conduct the ensemble co-evolutionary optimization. So the uniform reduction framework of different agents’ co-evolutionary game under the bounded rationality is further refined. Fourthly, the approximation MapReduce parallelism mechanism is permitted to formalize to the multi-agent co-evolutionary consensus structure, interaction and adaptation, which enhances different agents to share their solutions. Finally, extensive experimental studies substantiate the effectiveness and accuracy of MCMAR on some well-known benchmark datasets. Moreover, successful applications in big medical datasets are expected to dramatically scaling up MCMAR for complex infant brain MRI in terms of efficiency and feasibility. © 2017 Elsevier B.V.","Co-evolutionary quantum PSO; Ensemble co-evolutionary optimization of attribute reduction; Multi-agent consensus MapReduce model; Neighborhood radius with compensatory scheme; Self-adaptive memeplexes","Big data; Data mining; Magnetic resonance imaging; Multi agent systems; Optimization; Particle swarm optimization (PSO); Pattern recognition; Rough set theory; Attribute reduction; Co-evolutionary; MapReduce models; Neighborhood radius with compensatory scheme; Self-adaptive memeplexes; Data reduction",2-s2.0-85023638265
"Kim Y.-K., Na K.-S.","Application of machine learning classification for structural brain MRI in mood disorders: Critical review from a clinical perspective",2018,"Progress in Neuro-Psychopharmacology and Biological Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021855478&doi=10.1016%2fj.pnpbp.2017.06.024&partnerID=40&md5=010be5c235556dd4624c592266527716","Mood disorders are a highly prevalent group of mental disorders causing substantial socioeconomic burden. There are various methodological approaches for identifying the underlying mechanisms of the etiology, symptomatology, and therapeutics of mood disorders; however, neuroimaging studies have provided the most direct evidence for mood disorder neural substrates by visualizing the brains of living individuals. The prefrontal cortex, hippocampus, amygdala, thalamus, ventral striatum, and corpus callosum are associated with depression and bipolar disorder. Identifying the distinct and common contributions of these anatomical regions to depression and bipolar disorder have broadened and deepened our understanding of mood disorders. However, the extent to which neuroimaging research findings contribute to clinical practice in the real-world setting is unclear. As traditional or non-machine learning MRI studies have analyzed group-level differences, it is not possible to directly translate findings from research to clinical practice; the knowledge gained pertains to the disorder, but not to individuals. On the other hand, a machine learning approach makes it possible to provide individual-level classifications. For the past two decades, many studies have reported on the classification accuracy of machine learning-based neuroimaging studies from the perspective of diagnosis and treatment response. However, for the application of a machine learning-based brain MRI approach in real world clinical settings, several major issues should be considered. Secondary changes due to illness duration and medication, clinical subtypes and heterogeneity, comorbidities, and cost-effectiveness restrict the generalization of the current machine learning findings. Sophisticated classification of clinical and diagnostic subtypes is needed. Additionally, as the approach is inevitably limited by sample size, multi-site participation and data-sharing are needed in the future. © 2017 Elsevier Inc.","Bipolar disorder; Depression; Machine learning; MRI; Neuroimaging","Bayesian learning; bipolar disorder; brain region; classifier; cortical thickness (brain); disease classification; disease course; disease duration; human; machine learning; major depression; mood disorder; neuroimaging; nuclear magnetic resonance imaging; onset age; principal component analysis; recurrent disease; reinforcement; Review; supervised machine learning; support vector machine; treatment response; uncertainty; unsupervised machine learning",2-s2.0-85021855478
"Lopez-Guede J.M., Estevez J., Garmendia A., Graña M.","Making physical proofs of concept of reinforcement learning control in single robot hose transport task complete",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023642304&doi=10.1016%2fj.neucom.2017.01.110&partnerID=40&md5=a6bc39bb97bcfa4b22fb2bd0d2fdd0b0","This paper deals with the realization of physical proof of concept experiments in the paradigm of Linked Multi-Component Robotic Systems (LMCRS). The main objective is to demonstrate that the controllers learned through Reinforcement Learning (RL) algorithms with different state space formalizations and different spatial discretizations in a simulator are reliable in a real world configuration of the task of transporting a hose by a single robot. This one is a prototypical example of LMCRS task (extendable to much more complex tasks). We describe how the complete system has been designed and implemented. Two different previously learned RL controllers have been tested solving two different LMCRS control problems, using different state space modeling and discretization step in each case. The physical realizations validate previously published simulation based results, giving a strong argument in favor of the suitability of RL techniques to deal with LMCRS systems. © 2017 Elsevier B.V.","Hose transport; Linked multicomponent robotic systems; LMCRS; Proof of concept; Reinforcement learning","Controllers; Education; Hose; Robotics; Control problems; LMCRS; Physical realization; Proof of concept; Reinforcement learning control; Robotic systems; Spatial discretizations; State - space models; Reinforcement learning; algorithm; Article; learning; Linked Multi Component Robotic System; machine learning; mathematical computing; mathematical model; priority journal; reinforcement; robotics; task performance",2-s2.0-85023642304
"Won E., Kim Y.-K.","Neuroimaging in psychiatry: Steps toward the clinical application of brain imaging in psychiatric disorders",2018,"Progress in Neuro-Psychopharmacology and Biological Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029863128&doi=10.1016%2fj.pnpbp.2017.08.019&partnerID=40&md5=c8cf15868ce159424fd7f2e4d98fd577",[No abstract available],,"biological marker; ghrelin; leptin; Alzheimer disease; anorexia nervosa; autism; bipolar disorder; brain depth stimulation; bulimia; clinical outcome; clinical practice; cognition; correlational study; creativity; depression; disease course; DNA microarray; Editorial; electroencephalogram; epigenetics; feasibility study; feeding behavior; functional connectivity; functional magnetic resonance imaging; gene interaction; general practitioner; genetic association; genetic risk; genome-wide association study; health care planning; human; language development; machine learning; mental disease; microglia; mild cognitive impairment; mood disorder; nerve cell differentiation; nerve cell network; nervous system inflammation; neurobiology; neuroimaging; nuclear magnetic resonance imaging; onset age; panic; pathophysiology; phenotype; phobia; positron emission tomography; prediction; prefrontal cortex; psychiatry; psychoanalysis; resting state network; schizophrenia; scientist; sensorimotor cortex; social behavior; social phobia; test retest reliability; therapy effect; treatment response; whole exome sequencing",2-s2.0-85029863128
"Gil Gómez G.L., Nybacka M., Drugge L., Bakker E.","Machine learning to classify and predict objective and subjective assessments of vehicle dynamics: the case of steering feel",2018,"Vehicle System Dynamics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025804899&doi=10.1080%2f00423114.2017.1351617&partnerID=40&md5=e45be6e6608244f637c0d4963d950b9e","Objective measurements and computer-aided engineering simulations cannot be exploited to their full potential because of the high importance of driver feel in vehicle development. Furthermore, despite many studies, it is not easy to identify the relationship between objective metrics (OM) and subjective assessments (SA), a task further complicated by the fact that SA change between drivers and geographical locations or with time. This paper presents a method which uses two artificial neural networks built on top of each other that helps to close this gap. The first network, based solely on OM, generates a map that groups together similar vehicles, thus allowing a classification of measured vehicles to be visualised. This map objectively demonstrates that there exist brand and vehicle class identities. It also foresees the subjective characteristics of a new vehicle, based on its requirements, simulations and measurements. These characteristics are described by the neighbourhood of the new vehicle in the map, which is made up of known vehicles that are accompanied by word-clouds that enhance this description. This forecast is also extended to perform a sensitivity analysis of the tolerances in the requirements, as well as to validate previously published preferred range of steering feel metrics. The results suggest a few new modifications. Finally, the qualitative information given by this measurement-based classification is complemented with a second superimposed network. This network describes a regression surface that enables quantitative predictions, for example the SA of the steering feel of a new vehicle from its OM. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","artificial neural network; driver preference; Objective metrics; regression analysis; steering feel; subjective assessments; vehicle dynamics","Classification (of information); Computer aided engineering; Forecasting; Neural networks; Regression analysis; Sensitivity analysis; Steering; Vehicles; driver preference; Objective metrics; Steering feels; Subjective assessments; Vehicle dynamics; Automobile steering equipment",2-s2.0-85025804899
"Pravilovic S., Appice A., Malerba D.","Leveraging correlation across space and time to interpolate geophysical data via CoKriging",2018,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029903021&doi=10.1080%2f13658816.2017.1381338&partnerID=40&md5=65fb6d22e0f24ca0bcde3c9ecabcc7ad","Managing geophysical data generated by emerging spatiotemporal data sources (e.g. geosensor networks) presents a growing challenge to Geographic Information System science. The presence of correlation poses difficulties with respect to traditional spatial data analysis. This paper describes a novel spatiotemporal analytical scheme that allows us to yield a characterization of correlation in geophysical data along the spatial and temporal dimensions. We resort to a multivariate statistical model, namely CoKriging, in order to derive accurate spatiotemporal interpolation models. These predict unknown data by utilizing not only their own geosensor values at the same time, but also information from near past data. We use a window-based computation methodology that leverages the power of temporal correlation in a spatial modeling phase. This is done by also fitting the computed interpolation model to data which may change over time. In an assessment, using various geophysical data sets, we show that the presented algorithm is often able to deal with both spatial and temporal correlations. This helps to gain accuracy during the interpolation phase, compared to spatial and spatiotemporal competitors. Specifically, we evaluate the efficacy of the interpolation phase by using established machine-learning metrics (i.e. root mean squared error, Akaike information criterion and computation time). © 2017 Informa UK Limited, trading as Taylor & Francis Group.","CoKriging; interpolation; multivariate analysis; Spatiotemporal data",,2-s2.0-85029903021
"Baskar A., Gireesh Kumar T.","Facial expression classification using machine learning approach: A review",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021196620&doi=10.1007%2f978-981-10-3223-3_32&partnerID=40&md5=727dc17b597d349e1c68b6485f0bcce1","Automatic Facial Expression analysis has enthralled increasing attention in the research community in excess of two decades and its expedient in many application like, face animation, customer satisfaction studies, human-computer interaction and video conferencing. The precisely classifying different emotion is an essential problem in facial expression recognition research. There are several machine learning algorithms applied to facial expression recognition expedition. In this paper, we surveyed three different machine learning algorithms such as Bayesian Network, Hidden Markov Model and Support Vector machine and we attempt to answer following questions: How classification algorithm used its characteristics for emotion recognition? How various parameters in learning algorithm is devoted for better classification? What are the robust features used for training? Finally, we examined how advances in machine learning technique used for facial expression recognition?. © Springer Nature Singapore Pte Ltd. 2018.","Bayesian network; Deep belief network; Facial expression; Hidden markov model; Machine learning; Support vector machine","Artificial intelligence; Bayesian networks; Customer satisfaction; Deep learning; Education; Face recognition; Hidden Markov models; Human computer interaction; Intelligent computing; Learning systems; Markov processes; Support vector machines; Video conferencing; Automatic facial expression analysis; Classification algorithm; Deep belief networks; Facial expression classification; Facial expression recognition; Facial Expressions; Machine learning approaches; Machine learning techniques; Learning algorithms",2-s2.0-85021196620
"Pandey S., Supriya M., Shrivastava A.","Data classification using machine learning approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032668039&doi=10.1007%2f978-3-319-68385-0_10&partnerID=40&md5=63893f0a0719813465d96d71327ff7f2","Currently, Internet has numerous effects on our everyday lifecycle. Its significance as an intermediate for commercial transactions will develop exponentially throughout the next years. In terms of the engaged marketplace volume, the Business to Business region will hereby be the supreme exciting area. As the extensive usage of electronic business transactions increase, great volume of products information gets generated and managing such large information automatically becomes a challenging task. The accurate classification of such products to each of the existing classes also becomes an additional multifarious task. The catalog classification is an essential part for operative electronic business applications and classical machine learning problems. This paper presents a supervised Multinomial Naïve Bayes Classifier machine learning algorithm to classify product listings to anonymous marketplaces. If the existing products are classified under the master taxonomy, the task is to automatically categorize a new product into one of the existing categories. Our algorithm approach proposes a method to accurately classify the existing millions of products. © Springer International Publishing AG 2018.","Categories; Classifier; Machine learning; Naïve bayes","Artificial intelligence; Classifiers; Commerce; Electronic commerce; Intelligent systems; Learning systems; Sodium; Algorithm approaches; Business to business; Categories; Commercial transactions; Data classification; Electronic business; Machine learning approaches; Machine learning problem; Learning algorithms",2-s2.0-85032668039
"Elkin C., Nittala S., Devabhaktuni V.","Fundamental cognitive workload assessment: A machine learning comparative approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021798107&doi=10.1007%2f978-3-319-60642-2_26&partnerID=40&md5=089375e2f7a2c6a566ab4d657b513c50","Mental workload remains an essential but challenging aspect of human factors, while machine learning serves as an emerging and expanding research realm to a wide variety of applications. This paper aims to comprehensively bridge the two areas by comparing present state-of-the-art machine learning approaches that are currently utilized for assessing cognitive workload, primarily artificial neural networks and support vector machines. To address and evaluate both approaches, we obtain a physiological data set used to study fear conditioning and cognitive load and format the data to focus primarily on the latter. Ultimately, the results indicate that both techniques can effectively model the data with up to 99% accuracy. Furthermore, under optimal parameter selection, the neural network model produces the highest possible accuracy under a comfortable level of deep learning while the support vector machine model employs greater speed and efficiency while still enjoying a respectably high level of accuracy. © Springer International Publishing AG 2018.","Cognitive workload; Deep neural networks; Machine learning; Support vector machines","Artificial intelligence; Deep neural networks; Education; Learning systems; Neural networks; Psychophysiology; Support vector machines; Cognitive workloads; Comparative approach; Machine learning approaches; Neural network model; Optimal parameter selection; Physiological data; State of the art; Support vector machine models; Deep learning",2-s2.0-85021798107
"Toujani R., Dhouioui Z., Akaichi J.","Mobility based machine learning modeling for event mining in social networks",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020437326&doi=10.1007%2f978-3-319-59480-4_31&partnerID=40&md5=1887fa2a59762b8e397535c39de3e7df","Social networks sounds to be a rich source to discover events mobility and analyzing their trends. Hence, the mobility of events refers to the movement of users’ opinions, location, velocity and the continuous change over time. Despite the ability of existing methods to deal with the event mobility and evolution. To the best of our knowledge, there is no research able to show the relation between mobility and social interactions. In this work, we associate mobility into event mining issue. We also describe the movement of opinions in social network and we aim at extracting useful information from tweeter posts, especially during the economic and political event “TUNISIA 2020”. To achieve this task, we focused on the use of machine learning techniques to analyze tunisian tweeter posts and classify their opinions temporally about this event for each Tunisian region. We introduced decision tree method to model and analyze event mobility and to predict the change of opinions from its spatial and temporal co-occurrence. Therefore, an entropy measure has been proposed based on spatio-temporal attributes as branching attributes. Finally, in order to validate our solution, we used real data and we performed some comparative experiments to show the effectiveness of our method. © Springer International Publishing AG 2018.","Decision tree; Entropy; Event detection; Machine learning; Opinion change; Social networks; Spatial mobility; Spatio-temporal attributes; Temporal mobility","Artificial intelligence; Decision trees; Entropy; Interactive computer systems; Multimedia services; Multimedia systems; Social networking (online); Comparative experiments; Decision tree method; Event detection; Machine learning models; Machine learning techniques; Opinion change; Social interactions; Spatio temporal; Learning systems",2-s2.0-85020437326
"Mukesh S.D., Raval J.A., Upadhyay H.","Real-time framework for malware detection using machine learning technique",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028392721&doi=10.1007%2f978-3-319-63673-3_21&partnerID=40&md5=33d11ffa7689e26d55d74cbd84cbd516","In this epoch, current web world where peoples groups are associated through correspondence channel and the majority of their information is facilitated on the web associated assets. Thusly the security is the significant concern of this internet community to protect the resources and to ensure the assets and the information facilitated on these networks. In current trends, the greater part of the end client are depending on the end security items, for example, Intrusion detection system, firewall, Anti-viruses etc. In this paper, we propose a machine learning based architecture to distinguish existing and recently developing malware by utilizing network and transport layer traffic features. This paper influences the precision of Semi-supervised learning in identifying new malware classes. We show the adequacy of the framework utilizing genuine network traces. Amid this research, we will execute and design the proactive network security mechanism which will gather the malware traces. Assist those gathered malware traces can be utilized to fortify the signature based discovery mechanism. © 2018, Springer International Publishing AG.","ClamAV; Machine learning; Malware detection; Semi-supervised algorithm","Artificial intelligence; Computer crime; Computer system firewalls; Computer viruses; Intelligent systems; Intrusion detection; Learning algorithms; Learning systems; Malware; Network layers; Supervised learning; Viruses; ClamAV; Internet communities; Intrusion Detection Systems; Machine learning techniques; Malware detection; Proactive networks; Semi- supervised learning; Semi-supervised algorithm; Network security",2-s2.0-85028392721
"Sakuma R., Kang H., Iwamura K., Echizen I.","Digital watermarking scheme based on machine learning for the IHC evaluation criteria",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026753578&doi=10.1007%2f978-3-319-63856-0_45&partnerID=40&md5=9dac28bff35b5510d45d7b326f2b6bf6","Digital watermarking is a technique used for embedding information in digital content and protecting its copyright. The important issues to be considered are robustness, quality and capacity. Our goal is to satisfy these requirements according to the Information Hiding and its Criteria for evaluation (IHC) criteria. In this study, we evaluate our watermarking scheme along the IHC criteria Ver. 3 as the primary step. Although image watermarking techniques based on machine learning already exist, their robustness against desynchronization attacks such as cropping, rotation, and scaling is still one of the most challenging issues. We propose a watermarking scheme based on machine learning which also has cropping tolerance. First, the luminance space of the image is decomposed by one level through wavelet transform. Then, a bit of the watermark and the marker for synchronization are embedded or extracted by adjusting or comparing the relation between the embedded coefficients value of the LL space and the output coefficients value of the trained machine learning model. This model can well memorize the relationship between its selected coefficients and the neighboring coefficients. The marker for synchronization is embedded in a latticed format in the LL space. Binarization processing is performed on the watermarked image to find the lattice-shaped marker and synchronize it against cropping. Our experimental results showed that there were no errors in 10HDTV-size areas after the second decompression. © Springer International Publishing AG 2018.","Binarization; Cropping; Digital watermarking; Lattice-shaped marker; Machine learning","Artificial intelligence; Bins; Digital watermarking; E-learning; Image processing; Image watermarking; Learning systems; Signal processing; Wavelet transforms; Binarization processing; Binarizations; Criteria for evaluations; Cropping; De-synchronization attacks; Lattice-shaped marker; Machine learning models; Watermarking algorithms; Multimedia signal processing",2-s2.0-85026753578
"Singh S.P., Kumar A., Darbari H., Rastogi A., Jain S., Joshi N.","Building machine learning system with deep neural network for text processing",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028386788&doi=10.1007%2f978-3-319-63645-0_56&partnerID=40&md5=0d3755efdc0b93f389c6e4ea8a27e161","This paper provides the method and process to build machine learning system using Deep Neural Network (DNN) for lexicon analysis of text. Parts of Speech (POS) tagging of word is important in Natural language processing either it is speech technology or machine translation. The recent advancement of Deep Neural Network would help us to achieve better result in POS tagging of words and phrases. Word2vec tool of Dl4j library is very popular to represent the words in continuous vector space and these vectors capture the syntactic and semantic meaning of corresponding words. If we have a database of sample words with their POS category, it is possible to assign POS tag to the words but it fails when the word is not present in database. Cosine similarity concept plays an important role to find the POS Tags of the words and phrases which are not previously trained or POS Tagged. With the help of Cosine similarity, system assign the appropriate POS tags to the words by finding their nearest similar words using the vectors which we have trained from Word2vec database. Deep neural network like RNN outperforms as compare to traditional state of the art as it deals with the issue of word sense disambiguation. Semi-supervised learning is used to train the network. This approach can be applicable for Indian languages as well as for foreign languages. In this paper, RNN is implemented to build a machine learning system for POS-tagging of the words in English language sentences. © Springer International Publishing AG 2018.","Cosine similarity; Machine learning; Natural language processing (NLP); Recurrent neural network (RNN); Word2vec","Artificial intelligence; Computational linguistics; Database systems; Deep learning; Intelligent systems; Learning algorithms; Learning systems; Natural language processing systems; Recurrent neural networks; Semantics; Speech transmission; Supervised learning; Syntactics; Text processing; Vector spaces; Cosine similarity; English languages; Machine translations; Recurrent neural network (RNN); Semi- supervised learning; Speech technology; Word Sense Disambiguation; Word2vec; Deep neural networks",2-s2.0-85028386788
"Gonzalez-Cava J.M., Arnay R., Méndez Pérez J.A., León A., Martín M., Jove-Perez E., Calvo-Rolle J.L., Casteleiro-Roca J.L., de Cos Juez F.J.","A machine learning based system for analgesic drug delivery",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028679423&doi=10.1007%2f978-3-319-67180-2_45&partnerID=40&md5=288de6946ba75d4ba7df017e80725e1f","Monitoring pain and finding more efficient methods for analgesic administration during anaesthesia is a challenge that attracts the attention of both clinicians and engineers. This work focuses on the application of Machine Learning techniques to assist the clinicians in the administration of analgesic drug. The problem will consider patients undergoing general anaesthesia with intravenous drug infusion. The paper presents a preliminary study based on the use of the signal provided by an analgesia monitor, the Analgesia Nociception Index (ANI) signal. One aim of this research is studying the relation between ANI monitor and the changes in drug titration made by anaesthetist. Another aim is to propose an intelligent system that provides decisions on the drug infusion according to the ANI evolution. To do that, data from 15 patients undergoing cholecystectomy surgery were analysed. In order to establish the relationship between ANI and the analgesic, Machine Learning techniques have been introduced. After training different types of classifier and testing the results with cross validation method, it has been demonstrated that a relation between ANI and the administration of remifentanil can be found. © 2018, Springer International Publishing AG.","Anaesthesia; Analgesia; Analgesia nociception index; Intelligent system; Machine learning","Anesthesiology; Artificial intelligence; Drug infusion; Education; Intelligent systems; Learning algorithms; Soft computing; Anaesthesia; Analgesia; Analgesic drugs; Cross-validation methods; Machine learning techniques; Nociception indices; Remifentanil; Learning systems",2-s2.0-85028679423
"Allaf Z., Adda M., Gegov A.","A Comparison Study on Flush+Reload and Prime+Probe Attacks on AES Using Machine Learning Approaches",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029600557&doi=10.1007%2f978-3-319-66939-7_17&partnerID=40&md5=47e5a7d2dbc0f99cb191010a81fd20bb","AES, ElGamal are two examples of algorithms that have been developed in cryptography to protect data in a variety of domains including native and cloud systems, and mobile applications. There has been a good deal of research into the use of side channel attacks on these algorithms. This work has conducted an experiment to detect malicious loops inside Flush+Reload and Prime+Prob attack programs against AES through the exploitation of Hardware Performance Counters (HPC). This paper examines the accuracy and efficiency of three machine learning algorithms: Neural Network (NN); Decision Tree C4.5; and K Nearest Neighbours (KNN). The study also shows how Standard Performance Evaluation Corporation (SPEC) CPU2006 benchmarks impact predictions. © 2018, Springer International Publishing AG.","AES; Flush+Reload; Machine learning; Prime+Probe; Side-channel attack","Artificial intelligence; Benchmarking; Cryptography; Data mining; Decision trees; Learning algorithms; Learning systems; Nearest neighbor search; Probes; Comparison study; Flush+Reload; Hardware performance counters; K nearest neighbours (k-NN); Machine learning approaches; Mobile applications; Neural network (nn); Standard performance; Side channel attack",2-s2.0-85029600557
"Pektaş A., Acarman T.","Ensemble machine learning approach for android malware classification using hybrid features",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019177965&doi=10.1007%2f978-3-319-59162-9_20&partnerID=40&md5=8a525ef5f9ccebd2521f27c9b8ffebcf","Feature-based learning plays a crucial role at building and sustaining the security. Determination of a software based on its extracted features whether a benign or malign process, and particularly classification into a correct malware family improves the security of the operating system and protects critical user’s information. In this paper, we present a novel hybrid feature-based classification system for Android malware samples. Static features such as permissions requested by mobile applications, hidden payload, and dynamic features such as API calls, installed services, network connections are extracted for classification. We apply machine learning and evaluate the level in classification accuracy of different classifiers by extracting Android malware features using a fairly large set of 3339 samples belonging to 20 malware families. The evaluation study has been scalable with 5 guest machines and took 8 days of processing. The testing accuracy is reached at 92%. © Springer International Publishing AG 2018.","Classification; Ensemble machine learning; Feature; Malware","Android (operating system); Artificial intelligence; Computer crime; Learning systems; Malware; Mobile security; Classification accuracy; Dynamic features; Feature; Machine learning approaches; Malware families; Mobile applications; Network connection; Testing accuracy; Classification (of information)",2-s2.0-85019177965
"Chowdhury M., Rahman A., Islam R.","Malware analysis and detection using data mining and machine learning classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032708717&doi=10.1007%2f978-3-319-67071-3_33&partnerID=40&md5=ccfe68567e910edcb3f082a3fed13ec1","Exfiltration of sensitive data by malicious software or malware is a serious cyber threat around the world that has catastrophic effect on businesses, research organizations, national intelligence, as well as individuals. Thousands of cyber criminals attempt every day to attack computer systems by employing malicious software with an intention to breach crucial data, damage or manipulate data, or to make illegal financial transfers. Protection of this data is therefore, a critical concern in the research community. This manuscript aims to propose a comprehensive framework to classify and detect malicious software to protect sensitive data against malicious threats using data mining and machine learning classification techniques. In this work, we employ a robust and efficient approach for malware classification and detection by analyzing both signature-based and anomaly-based features. Experimental results confirm the superiority of the proposed approach over other similar methods. © 2018, Springer International Publishing AG.","Classification; Cyber threat; Data security; Machine learning; Malware","Artificial intelligence; Classification (of information); Computer crime; Crime; Data mining; Learning systems; Network security; Security of data; Catastrophic effects; Cyber criminals; Cyber threats; Machine learning classification; Malware analysis; Malware classifications; Research communities; Research organization; Malware",2-s2.0-85032708717
"Rasel R.I., Sultana N., Meesad P.","An application of data mining and machine learning for weather forecasting",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022174793&doi=10.1007%2f978-3-319-60663-7_16&partnerID=40&md5=c9f3ce4f582f27d856a34ae99c152782","Weather forecasting for an area where the weather and climate changes occurs spontaneously is a challenging task. Weather is non-linear systems because of various components having a grate impact on climate change such as humidity, wind speed, sea level and density of air. A strong forecasting system can play a vital role in different sectors like business, agricultural, tourism, transportation and construction. This paper exhibits the performance of data mining and machine learning techniques using Support Vector Regression (SVR) and Artificial Neural Networks (ANN) for a robust weather prediction purpose. To undertake the experiments 6-years historical weather dataset of rainfall and temperature of Chittagong metropolitan area were collected from Bangladesh Meteorological Department (BMD). The finding from this study is SVR can outperform the ANN in rainfall prediction and ANN can produce the better results than the SVR. © Springer International Publishing AG 2018.","ANN; Data mining; Machine learning; Rainfall; SVM; Temperature; Weather forecasting","Agricultural machinery; Artificial intelligence; Climate change; Data mining; Education; Forecasting; Learning systems; Linear systems; Neural networks; Rain; Sea level; Temperature; Bangladesh; Forecasting system; Machine learning techniques; Metropolitan area; Rainfall prediction; Support vector regression (SVR); Weather prediction; Wind speed; Weather forecasting",2-s2.0-85022174793
"Esposito E., De Vito S., Salvato M., Fattoruso G., Bright V., Jones R.L., Popoola O.","Stochastic comparison of machine learning approaches to calibration of mobile air quality monitors",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029639286&doi=10.1007%2f978-3-319-55077-0_38&partnerID=40&md5=d0a86846c2b8642d785a66dd765cf033","Recently, the interest in the development of new pervasive or mobile implementations of air quality multisensor devices has significantly grown. New application opportunities appeared together with new challenges due to limitations in dealing with rapid pollutants concentrations transients both for static and mobile deployments. Sensors dynamic is one of the primary factor in limiting the capability of the device of estimating true concentration when it is rapidly changing. Researchers have proposed several approaches to these issues but none have been tested in real conditions. Furthermore, no performance comparison is currently available. In this contribution, we propose and compare different approaches to the calibration problem of novel fast air quality multisensing devices, using two datasets recorded in field. Machine learning architectures have been designed, optimized and tested in order to tackle the cross sensitivities issues and sensors inherent dynamic limitations to perform accurate prediction and uncertainty estimation. Comparison results shows the advantage of dynamic non linear architectures versus static linear ones with support vector regressors scoring best results. © Springer International Publishing AG 2018.","Air quality; Calibration; Chemical sensors; Machine learning; Mobile air quality monitoring","Air quality; Artificial intelligence; Calibration; Chemical sensors; Memory architecture; Stochastic systems; Accurate prediction; Air quality monitoring; Calibration problems; Machine learning approaches; Performance comparison; Stochastic comparisons; Support vector regressor; Uncertainty estimation; Learning systems",2-s2.0-85029639286
"Prado Á.J., Michałek M.M., Cheein F.A.","Machine-learning based approaches for self-tuning trajectory tracking controllers under terrain changes in repetitive tasks",2018,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030697759&doi=10.1016%2fj.engappai.2017.09.013&partnerID=40&md5=deb04edf83e7aa86b6f2dce74042ed4c","The use of resources in autonomous vehicles when manoeuvring along changing terrain is an issue yet to be faced in the industrial field, such as mining and agriculture, where automated machinery performs repetitive tasks. If the machinery is not capable to overcome such terrain disturbances during its motion, the vehicle spends more energy than the necessary as the motion controller is adapted to the new terramechanical scenario. The latter usually includes a re-tuning of the controller and the corresponding loss of man-hours until obtaining the desired responses. In this context, we propose a self-tuning methodology based on probabilistic approaches and machine learning techniques to improve the performance of the controllers through reducing trajectory tracking errors and control input efforts, as the vehicle repeats its trajectory and learns from the wheel-terrain interaction. Three degree-of-freedom motion controllers are used to test our techniques, although our proposal is not restricted to the nature of such controllers. For the validation of our hypothesis, we considered three tests: one is performed via simulation using a modified kinematic model of the vehicle and slippage constraints, whereas other two extensive trials were carried out in field using an electric vehicle – Twizy, made by Renault – under different types of shaped trajectories and irregular terrain. In particular, two terrains: grass and muddy, and their transitions. The metrics used herein and previously published in the literature have shown that the self-tuning methodologies proposed in this work decreases the trajectory tracking errors up to 18%, saves energy in the effort input of the actuators up 15%, and in general, increases the performance of the controllers up to 22% when compared to efficient manual tuning. The experimental results as well as the statistical analysis of our proposal are presented in detail herein. © 2017 Elsevier Ltd","Energy consumption; Machine-learning; Motion controller; Robot self-tuning; Wheel–terrain interaction","Artificial intelligence; Controllers; Degrees of freedom (mechanics); Energy utilization; Kinematics; Landforms; Learning systems; Machinery; Motion control; Trajectories; Vehicles; Wheels; Autonomous Vehicles; Machine learning techniques; Motion controller; Probabilistic approaches; Selftuning; Three degree of freedoms; Trajectory tracking controllers; Trajectory tracking errors; Tuning",2-s2.0-85030697759
"Beaulieu-Jones B.","Machine learning for structured clinical data",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032025499&doi=10.1007%2f978-3-319-67513-8_3&partnerID=40&md5=23729ce5207a640375b4af21865fda9d","Research is a tertiary priority in the EHR, where the priorities are patient care and billing. Because of this, the data is not standardized or formatted in a manner easily adapted to machine learning approaches. Data may be missing for a large variety of reasons ranging from individual input styles to differences in clinical decision making, for example, which lab tests to issue. Few patients are annotated at a research quality, limiting sample size and presenting a moving gold standard. Patient progression over time is key to understanding many diseases but many machine learning algorithms require a snapshot, at a single time point, to create a usable vector form. Furthermore, algorithms that produce black box results do not provide the interpretability required for clinical adoption. This chapter discusses these challenges and others in applying machine learning techniques to the structured EHR (i.e. Patient Demographics, Family History, Medication Information, Vital Signs, Laboratory Tests, Genetic Testing). It does not cover feature extraction from additional sources such as imaging data or free text patient notes but the approaches discussed can include features extracted from these sources. © Springer International Publishing AG 2018.","Longitudinal modeling; Machine learning interpretability; Missing data; Semi-supervised machine learning",,2-s2.0-85032025499
"Shastri S.S., Nair P.C., Gupta D., Nayar R.C., Rao R., Ram A.","Breast cancer diagnosis and prognosis using machine learning techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032663199&doi=10.1007%2f978-3-319-68385-0_28&partnerID=40&md5=ff402391fcb3375df38765307cd09c3c","Breast cancer is one of the major type of cancer which is the leading cause of death in women. The research work is carried out on the real data of patient records obtained from HealthCare Global Enterprises Ltd (HCG) hospitals. The work analyzes the four major class variables in the dataset, namely death, progression, recurrence and metastasis. The influence of the same 11 predictor variables is explored for each of the class. Various machine algorithms namely Support Vector Machine, Decision Tree, Multi-layer Perceptron and Naive Bayes have been explored for classification of the patient data into various classes. The imbalance in the data is handled using an over sampling technique. The contribution of various attributes in classifying the instances into different classes is also being explored. The model helps in predicting various factors and thus helps in early diagnosis in the breast cancer. © Springer International Publishing AG 2018.","Attribute ranking; Breast cancer; Data imbalance; Machine learning","Artificial intelligence; Data mining; Decision trees; Diagnosis; Diseases; Hospital data processing; Intelligent systems; Trees (mathematics); Attribute ranking; Breast Cancer; Breast cancer diagnosis; Data imbalance; Global Enterprises; Machine learning techniques; Multi layer perceptron; Predictor variables; Learning systems",2-s2.0-85032663199
"Patil A.P., Doshi D., Dalsaniya D., Rashmi B.S.","Applying machine learning techniques for sentiment analysis in the case study of indian politics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030151462&doi=10.1007%2f978-3-319-67934-1_31&partnerID=40&md5=d280f2d160ca1c71f8471f52c79ce2da","In the recent era, humans have become detached from their surroundings, immediate peers and more addicted to their social media platforms and micro-blogging sites. Technology is digitalizing at a very fast pace and this has led to man being social, but only on technological forefront. Social media platforms like twitter, facebook, whatsapp, instagram are in trend. In our paper, we will concentrate on data generated through Twitter (tweets). People express their opinions, perspectives within a 140 character tweet, which is subjective. We try to analyze their emotion by tweet classification followed by sentiment analysis. On an average, with 328 million Twitter users, 6000 tweets are generated every second. This tremendous amount of data can be used to assess general public’s views in economy, politics, environment, product reviews, feedbacks etc. and so many other sectors. Here, we take into account the political data from Tweets. The data obtained can be images, videos, links, emoticons, text, etc. The results obtained could help the government function better, improve their flaws, plan out better strategies to empower the nation. © Springer International Publishing AG 2018.","Machine learning; Polarity; Sentiment analysis; Subjective data; Twitter","Artificial intelligence; Data mining; Image enhancement; Learning systems; Social networking (online); General publics; Machine learning techniques; Polarity; Product reviews; Sentiment analysis; Social media platforms; Subjective data; Twitter; Signal processing",2-s2.0-85030151462
"Li Z., Dong B.","Short term predictions of occupancy in commercial buildings—Performance analysis for stochastic models and machine learning approaches",2018,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031499052&doi=10.1016%2fj.enbuild.2017.09.052&partnerID=40&md5=8b363d0f6987b613752ae4ebcf4b6e60","Real-time occupancy predictions are essential components for the smart buildings in the imminent future. The occupancy information, such as the presence states and the occupants’ number, allows a robust control of the indoor environment to enhance the building energy performances. With many current studies focusing on the commercial building occupancy, most researchers modeled either the occupancy presence or the occupants’ number without evaluating the model potentials on both of them. This study focuses on 1) providing a unique data set containing the occupancy for the offices located in the U.S with difference pattern varieties, 2) proposing two methods, then comparing them with four existing methods, and 3) both presence of occupancy and occupancy number are predicted and tested using the approaches proposed in this study. In detail, the paper develops a new moving-window inhomogeneous Markov model based on change point analysis. A hierarchical probability sampling model is modified based on existed models. They are additional compared to well-known models from previous researchers. The study further explores and evaluates the predictive power of the models by various temporal scenarios, including 15-min ahead, 30-min ahead, 1-h ahead, and 24-h ahead forecasts. The final results show that the proposed Markov model outperforms the other methods with a max 22% difference in terms of presence forecasts for 15-min, 30 min and 1-h ahead. The proposed Markov model also outperforms other models in occupancy number prediction for all forecast windows with 0.34 RMSE and 0.23 MAE error respectively. However, there is not much performance difference between models for 24-h ahead predictions of occupancy presence forecast. © 2017 Elsevier B.V.","Field data; Machine learning; Moving window; Occupancy prediction","Artificial intelligence; Buildings; Forecasting; Learning systems; Markov processes; Office buildings; Robust control; Stochastic control systems; Stochastic systems; Building energy performance; Change-point analysis; Field data; Machine learning approaches; Moving window; Occupancy predictions; Performance analysis; Short term prediction; Stochastic models",2-s2.0-85031499052
"Ma H., Wang Y., Wang K.","Automatic detection of false positive RFID readings using machine learning algorithms",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029489992&doi=10.1016%2fj.eswa.2017.09.021&partnerID=40&md5=330aa10e4b9a268753a2f7a356018754","Radio frequency identification (RFID) has been widely used for the automatic identification, tracking and tracing of goods throughout the supply chain from the manufacturer to the customer. However, one technological problem that impedes the productive and reliable use of RFID is the constraint of false positive readings, which refers to tags that are detected accidentally by the reader but not the ones of interest. This paper focuses on the use of machine learning algorithms to identify such RFID readings. A total of 11 statistical features are extracted from received signal strength (RSS) and phase rotations derived from the raw RFID data. Each of the features is highly statistically different to distinguish the false positive readings, but satisfactory classification cannot be achieved when these features are considered individually. Classifiers based on logistic regression (LR), support vector machine (SVM) and decision tree (DT) are constructed, which combine all of the extracted features to classify the RFID readings more effectively. The performance of the classifiers is evaluated in a real-world factory. Results show that SVM provides the highest accuracy of up to 95.3%. DT shows slightly better accuracy (92.85%) than LR (92.75%), while LR has the larger area under the curve (0.976) than DT (0.949). Overall, machine learning algorithms could achieve accuracy of 93% on average. The proposed methodology provides a much more reliable RFID application as false-positive readings are detected immediately without human intervention, which enables a significant potential of fully automatic identification and tracking of goods throughout the supply chain. © 2017 Elsevier Ltd","Classification; False positive readings; Machine learning; RFID","Artificial intelligence; Automation; Classification (of information); Data mining; Decision trees; Learning systems; Radio frequency identification (RFID); Supply chains; Support vector machines; Area under the curves; Automatic Detection; Automatic identification; False positive; Logistic regressions; Received signal strength; Statistical features; Tracking and tracing; Learning algorithms",2-s2.0-85029489992
"Qi G., Zhu Z., Erqinhu K., Chen Y., Chai Y., Sun J.","Fault-diagnosis for reciprocating compressors using big data and machine learning",2018,"Simulation Modelling Practice and Theory",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032798272&doi=10.1016%2fj.simpat.2017.10.005&partnerID=40&md5=890dc0b0770377f6d7eae65ee60f68de","Reciprocating compressors are widely used in petroleum industry. A small fault in reciprocating compressor may cause serious issues in operation. Traditional regular maintenance and fault diagnosis solutions cannot efficiently detect potential faults in reciprocating compressors. This paper proposes a fault-diagnosis system for reciprocating compressors. It applies machine-learning techniques to data analysis and fault diagnosis. The raw data is denoised first. Then the denoised data is sparse coded to train a dictionary. Based on the learned dictionary, potential faults are finally recognized and classified by support vector machine (SVM). The system is evaluated by using 5-year operation data collected from an offshore oil corporation in a cloud environment. The collected data is evenly divided into two halves. One half is used for training, and the other half is used for testing. The results demonstrate that the proposed system can efficiently diagnose potential faults in compressors with more than 80% accuracy, which represents a better result than the current practice. © 2017 Elsevier B.V.","Big data; Cloud computing; Deep learning; Reciprocating compressor; RPCA; SVM","Artificial intelligence; Big data; Cloud computing; Compressors; Deep learning; Failure analysis; Learning systems; Petroleum industry; Reciprocating compressors; Support vector machines; Cloud environments; Current practices; Fault diagnosis systems; Learned dictionaries; Machine learning techniques; Offshore oil; Potential faults; RPCA; Fault detection",2-s2.0-85032798272
"Dong Y., Pan Z., Ernawan M.E., Liu J., Shimamoto S., Wicaksono R.P., Kunishige S., Chang K.","Novel UE RF condition estimation algorithm by integrating machine learning",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022181162&doi=10.1007%2f978-981-10-5281-1_12&partnerID=40&md5=3da5ccda9c9aa2c5620a099019b08c82","By 2020, 5G era will be commercially available. The smart city construction will also make great progress. Compared to current situation, more than thousand times of devices will connect to the cellular networks. For the operators, in order to analyze overall network performance, it is a key factor to estimate the user equipment (UE) radio frequency (RF) condition. However, practical RF estimation scheme is based on UE data log which can only observe UE that is at the top-serving cell with good RF condition. However, according to the comparison of actual UE data log and the scanner data log, potential RF problems may still exist since the UE will not always be served by the top-1 cell. In this paper, we propose a novel estimation scheme by integrating machine learning (ML) algorithm to analyze the scanner data logs from the target estimation zones where the mobility problems may occur. A hypothesis is obtained from learning step by various kinds of RF condition as input features. The numerical results show that the proposed estimation algorithm integrated ML can estimate probability of the potential mobility problems accurately. © Springer Science+Business Media Singapore 2018.","Estimation; Machine learning; Mobility problem; RF condition","Artificial intelligence; Estimation; Information retrieval; Learning algorithms; Learning systems; Scanning; Smart city; Wireless telecommunication systems; Condition estimation; Estimation algorithm; Estimation schemes; Integrating machines; Potential mobility; Radio frequencies; RF condition; Target estimations; Education",2-s2.0-85022181162
"Werneck R.D.O., de Almeida W.R., Stein B.V., Pazinato D.V., Júnior P.R.M., Penatti O.A.B., Rocha A., Torres R.D.S.","Kuaa: A unified framework for design, deployment, execution, and recommendation of machine learning experiments",2018,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027972730&doi=10.1016%2fj.future.2017.06.013&partnerID=40&md5=0a47f7f282427adb65ef58b836ce9c06","In this work, we propose Kuaa, a workflow-based framework that can be used for designing, deploying, and executing machine learning experiments in an automated fashion. This framework is able to provide a standardized environment for exploratory analysis of machine learning solutions, as it supports the evaluation of feature descriptors, normalizers, classifiers, and fusion approaches in a wide range of tasks involving machine learning. Kuaa also is capable of providing users with the recommendation of machine-learning workflows. The use of recommendations allows users to identify, evaluate, and possibly reuse previously defined successful solutions. We propose the use of similarity measures (e.g., Jaccard, Sørensen, and Jaro–Winkler) and learning-to-rank methods (LRAR) in the implementation of the recommendation service. Experimental results show that Jaro–Winkler yields the highest effectiveness performance with comparable results to those observed for LRAR, presenting the best alternative machine learning experiments to the user. In both cases, the recommendations performed are very promising and the developed framework might help users in different daily exploratory machine learning tasks. © 2017 Elsevier B.V.","Machine learning; Recommendation systems (Information filtering); Science - experiments; Workflow","Artificial intelligence; Information filtering; Network function virtualization; Exploratory analysis; Feature descriptors; It supports; Learning to rank; Science experiments; Similarity measure; Unified framework; Workflow; Learning systems",2-s2.0-85027972730
"LeMoyne R., Mastroianni T.","Role of machine learning for gait and reflex response classification",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032345638&doi=10.1007%2f978-981-10-5684-0_9&partnerID=40&md5=f77db7dc6244fde83344111aa2f4d8f2","Over the span of the past decade machine learning has been applied to distinguishing between disparate health status scenarios with considerable classification accuracy. Recent examples pertain to notable classification accuracy with regards to gait and reflex response disparity, especially in the context of a hemiplegic affected leg and unaffected leg. Machine learning classification serves as an instrumental post-processing methodology for the signal acquired through a wearable and wireless accelerometer or gyroscope. A summary of machine learning platforms is presented. The application and demonstration of machine learning as a diagnostic tool is described within the scope of gait, reflex response, and associated subjects. The amalgamation of machine learning and wearable and wireless systems is anticipated to further evolve Network Centric Therapy with capabilities, such as prognostic assessment of rehabilitation, objective consideration of therapy efficacy, therapy optimization, and diagnosis of appropriate transitional phases of therapy strategy. © 2018, Springer Nature Singapore Pte Ltd.","J48 decision tree; K-nearest neighbors; Logistic regression; Machine learning; Multilayer perceptron neural network; Portable media device; Smartphone; Support vector machine; Waikato environment for knowledge analysis (WEKA); Wireless accelerometer; Wireless gyroscope",,2-s2.0-85032345638
"Vasavi S.","Extracting hidden patterns within road accident data using machine learning techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760909&doi=10.1007%2f978-981-10-5508-9_2&partnerID=40&md5=b40024edc7a4f44caa4c78266a591951","Road accidents may not be stopped altogether, but can be reduced. Driver emotions such as sad, happy, and anger can be one reason for accidents. At the same time, environment conditions such as weather, traffic on the road, load in the vehicle, type of road, health condition of driver, and speed can also be the reasons for accidents. Hidden patterns in accidents can be extracted so as to find the common features between accidents. This paper presents the results of the framework from the research study on road accident data of major national highways that pass through Krishna district for the year 2013 by applying machine learning techniques into analysis. These datasets collected from police stations are heterogeneous. Incomplete and erroneous values are corrected using data cleaning measures, and relevance attributes are identified using attribute selection measures. Clusters that are formed using K-medoids, and expectation maximization algorithms are then analyzed to discover hidden patterns using a priori algorithm. Results showed that the selected machine learning techniques are able to extract hidden patterns from the data. Density histograms are used for accident data visualization. © Springer Nature Singapore Pte Ltd. 2018.","Association rule mining; Clustering; Machine learning techniques; Preprocessing; Road accident data analysis; Visualization","Artificial intelligence; Data mining; Data visualization; Flow visualization; Highway accidents; Image segmentation; Learning algorithms; Learning systems; Maximum principle; Motor transportation; Roads and streets; Transportation; Visualization; Attribute selection; Clustering; Environment conditions; Expectation-maximization algorithms; Machine learning techniques; Preprocessing; Research studies; Road accident data; Accidents",2-s2.0-85031760909
"Tziroglou G., Vafeiadis T., Ziogou C., Krinidis S., Voutetakis S., Tzovaras D.","Incident detection in industrial processes utilizing machine learning techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028609231&doi=10.1007%2f978-3-319-64465-3_5&partnerID=40&md5=2b9ab45f7b774c4fe684deee7cb3906c","This work provides a comparative analysis of the most popular and widely used classification algorithms applied in industrial processes, in order to tackle the issue of incident detection of abnormal situations. The proposed analysis is based on actual datasets from derived by the operation of a chemical process system situated at the premises of CERTH/CPERI. The evaluation of the tested methods is based on cross-validation using a series of Monte-Carlo simulations among several free parameters of tested classifiers and finally the application of the Adaptive Boosting technique. The experimental results are highly reliable as the accuracy reaches 98% and the F-measure metric achieves a score over 97%. Therefore, the detection of potential malfunctions is achieved using the proposed machine learning techniques. © Springer International Publishing AG 2018.","AdaBoost; Incident detection; Machine learning techniques","Adaptive boosting; Artificial intelligence; Chemical analysis; Intelligent systems; Learning algorithms; Maintenance; Monte Carlo methods; Production; Chemical process systems; Classification algorithm; Comparative analysis; Cross validation; Free parameters; Incident detection; Industrial processs; Machine learning techniques; Learning systems",2-s2.0-85028609231
"Bichindaritz I., Breen C., Cole E., Keshan N., Parimi P.","Feature selection and machine learning based multilevel stress detection from ECG signals",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019722181&doi=10.1007%2f978-3-319-59397-5_22&partnerID=40&md5=451508e3a7976f8619067d37db05d218","Physiological sensor analytics aims at monitoring health as the availability of sensor-enabled portable, wearable, and implantable devices become ubiquitous in the growing Internet of Things (IoT). Physiological multi-sensor studies have been conducted previously to detect stress. In this study, we focus on electrocardiography (ECG) monitoring that can now be performed with minimally invasive wearable patches and sensors, to develop an efficient and robust mechanism for accurate stress identification, for example in automobile drivers. A unique aspect of our research is personalized individual stress analysis including three stress levels: low, medium and high. Using machine learning algorithms from the ECG signals alone, our system achieves up to 100% accuracy and area under ROC curve of 1 depending on the experimental setting in detecting three classes of stress using feature selection from a combination of fiducial points and multiscale entropy as a fine-grained indicator of stress level. © Springer International Publishing AG 2018.","Data mining; ECG; Machine learning; Sensors; Stress medicine","Artificial intelligence; Automobile drivers; Data mining; Electrocardiography; Feature extraction; Health care; Implants (surgical); Internet of things; Learning algorithms; Learning systems; Physiology; Sensors; Stress analysis; Wearable technology; Area under roc curve (AUC); Implantable devices; Internet of Things (IOT); Minimally invasive; Multi-scale entropies; Physiological sensors; Robust mechanisms; Stress detection; Wearable sensors",2-s2.0-85019722181
"Anderson K.E., Glenn N.F., Spaete L.P., Shinneman D.J., Pilliod D.S., Arkle R.S., McIlroy S.K., Derryberry D.R.","Estimating vegetation biomass and cover across large plots in shrub and grass dominated drylands using terrestrial lidar and machine learning",2018,"Ecological Indicators",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030456606&doi=10.1016%2fj.ecolind.2017.09.034&partnerID=40&md5=2a2219373f5e5114ae9d1235f48b53f4","Terrestrial laser scanning (TLS) has been shown to enable an efficient, precise, and non-destructive inventory of vegetation structure at ranges up to hundreds of meters. We developed a method that leverages TLS collections with machine learning techniques to model and map canopy cover and biomass of several classes of short-stature vegetation across large plots. We collected high-definition TLS scans of 26 1-ha plots in desert grasslands and big sagebrush shrublands in southwest Idaho, USA. We used the Random Forests machine learning algorithm to develop decision tree models predicting the biomass and canopy cover of several vegetation classes from statistical descriptors of the aboveground heights of TLS points. Manual measurements of vegetation characteristics collected within each plot served as training and validation data. Models based on five or fewer TLS descriptors of vegetation heights were developed to predict the canopy cover fraction of shrubs (R2 = 0.77, RMSE = 7%), annual grasses (R2 = 0.70, RMSE = 21%), perennial grasses (R2 = 0.36, RMSE = 12%), forbs (R2 = 0.52, RMSE = 6%), bare earth or litter (R2 = 0.49, RMSE = 19%), and the biomass of shrubs (R2 = 0.71, RMSE = 175 g) and herbaceous vegetation (R2 = 0.61, RMSE = 99 g) (all values reported are out-of-bag). Our models explained much of the variability between predictions and manual measurements, and yet we expect that future applications could produce even better results by reducing some of the methodological sources of error that we encountered. Our work demonstrates how TLS can be used efficiently to extend manual measurement of vegetation characteristics from small to large plots in grasslands and shrublands, with potential application to other similarly structured ecosystems. Our method shows that vegetation structural characteristics can be modeled without classifying and delineating individual plants, a challenging and time-consuming step common in previous methods applying TLS to vegetation inventory. Improving application of TLS to studies of shrub-steppe ecosystems will serve immediate management needs by enhancing vegetation inventories, environmental modeling studies, and the ability to train broader datasets collected from air and space. © 2017 Elsevier Ltd","Biomass; Carbon; Classification; Land cover; Lidar; Machine learning; Point cloud; Rangelands; Remote sensing; Structure from motion (SfM); Vegetation type","Artificial intelligence; Biomass; Carbon; Classification (of information); Data mining; Decision trees; Ecology; Ecosystems; Forecasting; Learning algorithms; Learning systems; Optical radar; Plants (botany); Remote sensing; Seebeck effect; Surveying instruments; Land cover; Point cloud; Rangelands; Structure from motion; Vegetation type; Vegetation; algorithm; environmental modeling; grass; lidar; machine learning; phytomass; shrub; shrubland; vegetation cover; vegetation structure; Idaho; United States; Artemisia tridentata; Poaceae",2-s2.0-85030456606
"Rajalakshmi S., Venkatesan R.","Exploring cepstral coefficient based sleep stage scoring method for single-channel EEG signal using machine learning technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030160492&doi=10.1007%2f978-3-319-67934-1_3&partnerID=40&md5=f19d3139fd1152cfa905f1b092110935","Sleep stage scoring is a critical task where conventionally large volume of data has to be analyzed visually which is troublesome, time-consuming and error prone. Eventually, machine learning technique is required for automatic sleep stage scoring. Therefore, a new feature extraction method for EEG analysis and classification is discussed based on the statistical properties of cepstral coefficients. The sleep EEG signal is segmented into 30 s epoch and each epoch is decomposed into different frequency bands: Gamma (γ), Beta (β), Alpha (α), Theta (θ) and Delta (δ) by employing the Discrete Wavelet Transform (DWT). The statistical properties of Mel Frequency Cepstral Coefficients (MFCCs), which represent the short term spectral characteristics of the wavelet coefficients, are extracted. The MFCC feature vectors are incorporated into the Gaussian Mixture Model with Expectation Maximization (GMM-EM) to classify various sleep stages: Wake, Rapid Eye Movement (REM) and Non-Rapid Eye Movement (N-REM) stage1 (S1), N-REM stage2 (S2), N-REM stage3 (S3), N-REM stage4 (S4). The proposed feature extraction for sleep stage scoring achieves 88.71% of average classification accuracy. © Springer International Publishing AG 2018.","Cognitive tasks; Discrete wavelet transform; Feature extraction; Gaussian mixture model-expectation maximization; Mel frequency cepstral coefficient; Statistical properties","Artificial intelligence; Biomedical signal processing; Data visualization; Discrete wavelet transforms; Extraction; Eye movements; Feature extraction; Frequency bands; Gaussian distribution; Image segmentation; Learning algorithms; Learning systems; Maximum principle; Signal reconstruction; Sleep research; Speech recognition; Speech transmission; Wavelet transforms; Cognitive task; Expectation - maximizations; Feature extraction methods; Machine learning techniques; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficients; Spectral characteristics; Statistical properties; Signal processing",2-s2.0-85030160492
"Pektaş A., Acarman T.","Identification of application in encrypted traffic by using machine learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030770543&doi=10.1007%2f978-3-319-67792-7_53&partnerID=40&md5=074b3c441f55eb7d7433b817d41802c9","Identification of Internet protocol from raw network traffic plays a crucial role at maintaining and improving the security of back-end and front-end computer systems. A significant amount of research work is carried out while exploiting a variety of identification techniques. Although certain level in success at detection of network protocols for unencrypted traffic has been achieved, accuracy and performance is rather poor for encrypted traffic. But considering technological trends, new and existing applications have been adopted to use encryption mechanism to protect information and privacy. Therefore, classification of encrypted network traffic is mandatory for security purposes. In this study, we propose a method for automatic extraction of features from raw network capture and accurate identification of network applications by applying machine learning algorithms. The proposed method is evaluated with two independent datasets. The first dataset is publicly available (known as NISM dataset) and the second dataset is generated with a particular emphasis on accurate labeling of network traffic, it contains 713851 and 448 network flows, respectively. The proposed method classifies network flows provided by the first dataset into their corresponding application categories with the accuracy over 0.997 and F1-score of 0.99, the second dataset with an accuracy over 0.96 and F1-score of 0.95. © 2018, Springer International Publishing AG.","Encrypted traffic identification; Machine learning; Network flow; Security","Artificial intelligence; Cryptography; Internet protocols; Learning algorithms; Learning systems; Network protocols; Automatic extraction; Encrypted traffic; Identification techniques; Network applications; Network flows; Protect information; Security; Technological trends; Network security",2-s2.0-85030770543
"Kumar S., Rai S., Singh R., Pal S.K.","Machine learning-based method and its performance analysis for occupancy detection in indoor environment",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030181602&doi=10.1007%2f978-3-319-67934-1_21&partnerID=40&md5=a03a07855c48e1ae2c52016051b3bbdd","Occupancy detection is very interesting research problem which may help in understanding ambient dynamics of the environment, resource utilisation, energy conservation and consumption, electricity usages and patterns, security and privacy related aspects. In addition to this, achieving good accuracy for occupancy detection problem in the home and commercial buildings can help in cost reduction substantially. In this paper, we explain one experiment in which data for occupancy and ambient attributes have been collected. This paper develops machine learning-based intelligent occupancy detection model and compare the results with several machine learning techniques in a detailed manner. © Springer International Publishing AG 2018.","CART; Classification; LDA; Logistic regression; Naive Bayes; Occupancy detection; SVM","Artificial intelligence; Classification (of information); Cost reduction; Learning systems; Office buildings; CART; Logistic regressions; Machine learning techniques; Naive bayes; Occupancy detections; Performance analysis; Resource utilisation; Security and privacy; Signal processing",2-s2.0-85030181602
"Mosavi A., Rabczuk T., Varkonyi-Koczy A.R.","Reviewing the novel machine learning tools for materials design",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029812280&doi=10.1007%2f978-3-319-67459-9_7&partnerID=40&md5=4b306d6cc865c882ea7ed484cefcc014","Computational materials design is a rapidly evolving field of challenges and opportunities aiming at development and application of multi-scale methods to simulate, predict and select innovative materials with high accuracy. Today the latest advancements in machine learning, deep learning, internet of things (IoT), big data, and intelligent optimization have highly revolutionized the computational methodologies used for materials design innovation. Such novelties in computation enable the development of problem-specific solvers with vast potential applications in industry and business. This paper reviews the state of the art of technological advancements that machine learning tools, in particular, have brought for materials design innovation. Further via presenting a case study the potential of such novel computational tools are discussed for the virtual design and simulation of innovative materials in modeling the fundamental properties and behavior of a wide range of multi-scale materials design problems. © Springer International Publishing AG 2018.","Machine learning; Materials design; Optimization","Artificial intelligence; Big data; Internet of things; Learning systems; Optimization; Computational materials; Computational methodology; Development and applications; Fundamental properties; Intelligent optimization; Internet of Things (IOT); Materials design; Technological advancement; Education",2-s2.0-85029812280
"Kirts S., Panagopoulos O.P., Xanthopoulos P., Nam B.H.","Soil-Compressibility Prediction Models Using Machine Learning",2018,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030454089&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000713&partnerID=40&md5=f0831d3cb85c91f1e50afd5a40f4761e","The magnitude of the overall settlement depends on several variables such as the compression index, Cc, and recompression index, Cr, which are determined by a consolidation test; however, the test is time consuming and labor intensive. Correlations have been developed to approximate these compressibility indexes. In this study, a data driven approach has been employed to estimate Cc and Cr. Support vector machines classification is used to determine the number of distinct models to be developed. Classification accuracy is used for detecting the existence of separability between different soil classes that in turn is indicative of the number of models needed. The statistical models are built through a forward selection stepwise regression procedure. Seven variables were used, including the moisture content (w), initial void ratio (eo), dry unit weight (γdry), wet unit weight (γwet), automatic hammer SPT blow count (N), overburden stress (σ), and fines content (-200). The results confirm the need for separate models for three out of four soil types, these being coarse grained, fine grained, and organic peat. The models for each classification have varying degrees of accuracy. The model for the fine grained classification performs on par with existing correlations, with respect to Cc, whereas the models for coarse grained and organic peat classifications perform considerably better than that of existing correlations. The models generated also incorporate several factors not utilized in correlations from previous literature. These factors include the fines content (-200), automatic hammer blow count (N), and the interactions between the wet and dry density (γwet and γdry). © 2017 American Society of Civil Engineers.","Compression index; Consolidation; Correlation; Machine learning; Recompression index; Settlement; Statistical analysis; Support vector machines","Artificial intelligence; Compressibility; Consolidation; Correlation methods; Hammers; Peat; Soils; Statistical methods; Support vector machines; Classification accuracy; Compression index; Data-driven approach; Initial void ratios; Recompression index; Settlement; Soil compressibilities; Stepwise regression; Learning systems",2-s2.0-85030454089
"Wei Z., Jia K., Sun Z.","An automatic detection method for morse signal based on machine learning",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026663229&doi=10.1007%2f978-3-319-63859-1_24&partnerID=40&md5=d8530b4143057cece74ab9a6802cc986","In this paper, an automatic detection for time-frequency map of Morse signal is proposed base on machine learning. Firstly, a preprocessing method based on energy accumulation is proposed, and the signal region is determined by nonlinear transformation. Secondly, the feature extraction of different types of signal time-frequency maps is carried out based on the graphics. Finally, a signal detection classifier is built based on the feature matrix. Experiments show that the classifier constructed in this paper has the generalization ability and can detect the Morse signal in the broadband shortwave channel, which improve the accuracy of Morse signal detection. © Springer International Publishing AG 2018.","Classifier; Feature extraction; Machine learning; Morse signal","Artificial intelligence; Classification (of information); Classifiers; Extraction; Feature extraction; Learning systems; Mathematical transformations; Signal detection; Signal processing; Automatic Detection; Automatic detection method; Energy accumulation; Generalization ability; Non-linear transformations; Pre-processing method; Shortwave channel; Time-frequency map; Multimedia signal processing",2-s2.0-85026663229
"Juhi Reshma S.R., Pillai A.S.","Impact of machine learning and internet of things in agriculture: State of the art",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028625870&doi=10.1007%2f978-3-319-60618-7_59&partnerID=40&md5=8421ebc1d8f08a986e1539dc3dda7c23","Majority of the population are directly or indirectly dependent on agriculture. In this modern world, agriculture has to be supported with technology to bring the best output. There is a big revolution in agriculture from traditional methods. Recent development in technology has a great impact on agriculture. Evolution of Machine Learning (ML) and Internet of Things (IoT) has helped researchers to apply these techniques in agriculture to help farmers. This in turn helped farmers to increase the productivity, make use of maximum land available, control pest, and so on. This paper highlights the work done in agriculture sector using ML and IoT. © Springer International Publishing AG 2018.","Agriculture; ANN; IoT; Machine learning; SVM","Artificial intelligence; Internet of things; Learning systems; Pattern recognition; Soft computing; Agriculture sectors; Impact on agriculture; Internet of Things (IOT); State of the art; Agriculture",2-s2.0-85028625870
"Serrano E., del Pozo-Jiménez P., Suárez-Figueroa M.C., González-Pachón J., Bajo J., Gómez-Pérez A.","Predicting the risk of suffering chronic social exclusion with machine learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022188445&doi=10.1007%2f978-3-319-62410-5_16&partnerID=40&md5=a100b82cba6e8b506fa0ad42396abd17","The fight against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a common practice. This paper describes a data analysis of over 16K cases with over 60 predictors from the Spanish region of Castilla y León. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion. The paper is complemented with a responsive web available online that allows social workers to calculate the risk of a social exclusion case to become chronic through a smartphone. © Springer International Publishing AG 2018.","Data analysis; Data mining; Machine learning; Social exclusion; Social services","Artificial intelligence; Data handling; Data mining; Data reduction; Decision trees; Distributed computer systems; Forecasting; Health risks; Information analysis; Insurance; Learning systems; Health services; High-precision; Insurance companies; Logistic regressions; Random forests; Risk prediction models; Social exclusion; Social service; Education",2-s2.0-85022188445
"López-Tapia S., Molina R., Pérez de la Blanca N.","Using machine learning to detect and localize concealed objects in passive millimeter-wave images",2018,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030873583&doi=10.1016%2fj.engappai.2017.09.005&partnerID=40&md5=52d5580fd35dd1915c6d2e839ed0b880","The detection and location of objects concealed under clothing is a very challenging task that has crucial applications in security. In this domain, passive millimeter-wave images (PMMWIs) can be used. However, the quality of the acquired images, and the unknown position, shape, and size of hidden objects render this task difficult. In this paper, we propose a machine learning-based solution to this detection/localization problem. Our method outperforms currently used approaches. The effect of non-stationary noise on different classification algorithms is analyzed and discussed, and a detailed experimental comparative study of classification techniques is presented using a new and comprehensive PMMWI database. The low computational testing cost of this solution allows for its use in real-time applications. © 2017 Elsevier Ltd","Machine learning; Passive millimeter-wave imaging; Threat detection","Artificial intelligence; Classification (of information); Learning systems; Millimeter waves; Classification algorithm; Classification technique; Comparative studies; Computational testing; Passive millimeter wave; Passive millimeter wave imaging; Real-time application; Threat detection; Object detection",2-s2.0-85030873583
"Gallagher C.V., Bruton K., Leahy K., O'Sullivan D.T.J.","The suitability of machine learning to minimise uncertainty in the measurement and verification of energy savings",2018,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031775033&doi=10.1016%2fj.enbuild.2017.10.041&partnerID=40&md5=9304cab2d15877821ca5781f8a86d7e7","Accurate energy modelling is a critical step in the measurement and verification (M&V) of energy savings, as a model for consumption in the baseline period is required. Machine learning (ML) algorithms offer an alternative approach to train these models with data-driven techniques. Industrial buildings offer the most challenging environment for the completion of M&V due to their complex energy systems. This paper investigates the novel use of ML algorithms for M&V of energy savings in industrial buildings. This approach enables the extension of the traditional project boundary also. The ML techniques applied consist of bi-variable and multi-variable ordinary least squares regression, decision trees, k-nearest neighbours, artificial neural networks and support vector machines. The prediction performances of the models are validated in the context of a biomedical manufacturing facility to find the optimal model parameters. Results show that models constructed using ML algorithms are more accurate than the conventional approach. A 51.09% reduction in error was achieved using the optimal model algorithm and parameters. The use of a higher measurement frequency reduced the spread of error across the six models. However, further analysis proved the use of more granular data did not always benefit model performance. Results of the sensitivity analysis showed the proposed ML approach to be beneficial in circumstances where missing baseline data limits the model training period length. © 2017 Elsevier B.V.","Energy efficiency; Energy modelling; Energy performance; Machine learning; Measurement and verification; Uncertainty analysis","Artificial intelligence; Buildings; Decision trees; Energy conservation; Energy efficiency; Learning systems; Nearest neighbor search; Neural networks; Office buildings; Sensitivity analysis; Complex energy systems; Data driven technique; Energy modelling; Energy performance; Manufacturing facility; Measurement and verification; Ordinary least squares regressions; Prediction performance; Uncertainty analysis",2-s2.0-85031775033
"Bray M.-A., Carpenter A.E.","Quality Control for High-Throughput Imaging Experiments Using Machine Learning in Cellprofiler",2018,"Methods in Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644960&doi=10.1007%2f978-1-4939-7357-6_7&partnerID=40&md5=fa30eb6ae01df0e7fed2afa424f9c40c","Robust high-content screening of visual cellular phenotypes has been enabled by automated microscopy and quantitative image analysis. The identification and removal of common image-based aberrations is critical to the screening workflow. Out-of-focus images, debris, and auto-fluorescing samples can cause artifacts such as focus blur and image saturation, contaminating downstream analysis and impairing identification of subtle phenotypes. Here, we describe an automated quality control protocol implemented in validated open-source software, leveraging the suite of image-based measurements generated by CellProfiler and the machine-learning functionality of CellProfiler Analyst. © Springer Science+Business Media LLC 2018.","Cell-based assays; High-content screening; Image analysis; Machine learning; Microscopy; Open-source software; Quality control","artifact; cell screening; classifier; fluorescence; image analysis; image processing; image quality; machine learning; metadata; phenotype; quality control; workflow",2-s2.0-85032644960
"Spielman Z., Gertman D.I., Liu H., Pray I., Traiteur J., Wold S., Wysmuller S.","Machine learning and big data analytics in support of fleet safety during severe weather",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022330148&doi=10.1007%2f978-3-319-60441-1_64&partnerID=40&md5=85a69f2c4654bac33fd21260d8dddcd6","The US DoT estimates 22% of the 5.7 million vehicle crashes a year are weather related. At Idaho National Laboratories, home of the DOE’s largest transit, heavy and light vehicle fleet in the nation, weather is a constant challenge for the 4000 employees traveling the 45 to 65 mile stretch of road. Driving conditions can vary immensely; micro-climate conditions at INL site locations highways go unmonitored and causing severe challenges. INL has taken the initiative to review applicable technologies determining that addressing severe weather and road conditions through the application of advanced modeling methods holds promise for enhancing driver safety and dispatch planning. INL engaged IBM Global Business Services Advanced Analytics Center of Competency (CoC) Team for support in this effort. This presentation reviews the benefits expected, data surveyed, and how to use integrated sources and cognitive analytics to improve real-time weather forecasting and INL site fleet and operations planning. © Springer International Publishing AG 2018.","Human factors; Machine learning; Transportation; Weather prediction","Accidents; Artificial intelligence; Big data; Education; Fleet operations; Human engineering; Learning systems; Roads and streets; Transportation; Advanced modeling; Driving conditions; Global business services; Idaho national laboratories; Integrated sources; Light-vehicle fleet; Operations planning; Weather prediction; Weather forecasting",2-s2.0-85022330148
"Peterkova A., Michalconok G., Bohm A.","Overview and Comparison of Machine Learning Methods to Build Classification Model for Prediction of Categorical Outcome Based on Medical Data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029593565&doi=10.1007%2f978-3-319-67618-0_20&partnerID=40&md5=a89122151109291f09dbe0f9d6e0c803","In this paper a classification model is proposed to predict a future state of patient’s cardiac diagnosis based on a large amount of medical data. The methodology of building a prediction model can be applied also to the other areas, such as industrial processes. In our research, we focus on cardiologic datasets of selected patients who were indicated for the ischemic heart disease. The selected sample of patients is divided into four stages of clinical diagnosis. Some of the parameters have a significant impact on the probability of the occurrence of the myocardial infraction. For building a classification model to predict categorical class output was used STATISTICA 13 software. © 2018, Springer International Publishing AG.","Classification model; Clinical dataset; Data mining","Classification (of information); Computational methods; Data mining; Diagnosis; Forecasting; Intelligent systems; Learning systems; Classification models; Clinical dataset; Clinical diagnosis; Industrial processs; Ischemic heart disease; Machine learning methods; Myocardial infraction; Prediction model; Computer aided diagnosis",2-s2.0-85029593565
"Radvansky M., Kudelka M., Kriegova E., Fillerova R.","Decision Support System in Orthopedics Using Methodology Based on a Combination of Machine Learning Methods",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030855762&doi=10.1007%2f978-3-319-68527-4_21&partnerID=40&md5=0e518a3c23feb84979c55a0c51e596f3","Analysis of data based on gene expressions characterizing serious disease is an area currently receiving high attention. The basic task is to classify patients, usually by searching for a small group of genes that provides sufficient classification power. However, very often, different gene combinations can describe different aspects of the problem being analyzed. In this paper, we present in a concrete example with one real dataset, a methodology that has repeatedly been successfully applied to different types of data. In addition to common statistical methods, this methodology combines methods such as a visualization of a dataset structure using networks, and feature-selection and neural network classification. The output of the application of the methodology is a system for decision support during the reoperation of patients with joint endoprosthesis. © 2018, Springer International Publishing AG.","Artificial neural network; Classification; Feature selection; Gene expressions; Network analysis","Artificial intelligence; Data handling; Decision support systems; Electric network analysis; Feature extraction; Gene expression; Genes; Information analysis; Learning systems; Neural networks; Analysis of data; Classification power; Decision supports; Gene combinations; Joint endoprosthesis; Machine learning methods; Neural network classification; Reoperation; Classification (of information)",2-s2.0-85030855762
"Kondo K., Taira K.","Estimation of binaural speech intelligibility using machine learning",2018,"Applied Acoustics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028942679&doi=10.1016%2fj.apacoust.2017.09.001&partnerID=40&md5=6fe9eb7c580748343e3a6f73776dd64a","We proposed and evaluated a speech intelligibility estimation method for binaural signals. The assumption here was that both the speech and competing noise are directional sources. In this case, when the speech and noise are located away from each other, the intelligibility generally improves since the auditory system can segregate these two streams. However, since intelligibility tests as well as its estimation is conducted based on monaurally-recorded signals, this potential increase in the intelligibility due to the segregation of sources is not accounted for, and the intelligibility is often under-estimated. Accordingly, in order to estimate the intelligibility taking into account this binaural advantage, we trained a mapping function between the subjective intelligibility and objective measures that account for the binaural advantage stated above. We attempted SNR calculation on (1) a simple binaural to monaural mix-down, which models the conventional estimation, (2) simple pooling of both binaural channels (pooled channel), (3) channel signal selection with the better SNR from left and right channels (better-ear), and (4) sub-band wise better-ear selection (band-wise better-ear). For the mapping function training, we tried neural networks (NN), support vector regression (SVR), and random forests (RF), and compared these to simple logistic regression (LR). We also investigated the sub-band configuration that gives the best estimation accuracy by balancing the frequency resolution and the amount of training data. It was found that the combination of the better-ear model and RF gave the best results, with root mean square error (RMSE) of about 0.11 and correlation of 0.92 in an open set test. © 2017 Elsevier Ltd","Binaural speech; Diagnostic rhyme test; Machine learning; Objective estimation; Speech intelligibility","Artificial intelligence; Bins; Decision trees; Frequency estimation; Learning systems; Mapping; Mean square error; Signal to noise ratio; Speech; Diagnostic rhyme test; Frequency resolutions; Intelligibility estimations; Intelligibility tests; Objective estimations; Root mean square errors; Simple logistic regressions; Support vector regression (SVR); Speech intelligibility",2-s2.0-85028942679
"Li Y., Qian Z., Xu K., Wang K., Fan X., Li S., Jiang T., Liu X., Wang Y.","MRI features predict p53 status in lower-grade gliomas via a machine-learning approach",2018,"NeuroImage: Clinical",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032701812&doi=10.1016%2fj.nicl.2017.10.030&partnerID=40&md5=975ebc8b20318065c6446db522404ae9","Background P53 mutation status is a pivotal biomarker for gliomas. Here, we developed a machine-learning model to predict p53 status in lower-grade gliomas based on radiomic features extracted from conventional magnetic resonance (MR) images. Methods Preoperative MR images were retrospectively obtained from 272 patients with primary grade II/III gliomas. The patients were randomly allocated in a 2:1 ratio to a training (n = 180) or validation (n = 92) set. A total of 431 radiomic features were extracted from each patient. The lest absolute shrinkage and selection operator (LASSO) method was used for feature selection and radiomic signature construction. Subsequently, a machine-learning model to predict p53 status was established using the selected features and a Support Vector Machine classifier. The predictive performance of all individual features and the model was calculated using receiver operating characteristic curves in both the training and validation sets. Results The p53-related radiomic signature was built using the LASSO algorithm; this procedure consisted of four first-order statistics or related wavelet features (including Maximum, Median, Minimum, and Uniformity), a shape and size-based feature (Spherical Disproportion), and ten textural features or related wavelet features (including Correlation, Run Percentage, and Sum Entropy). The prediction accuracies based on the area under the curve were 89.6% in the training set and 76.3% in the validation set, which were better than individual features. Conclusions These results demonstrate that MR image texture features are predictive of p53 mutation status in lower-grade gliomas. Thus, our procedure can be conveniently used to facilitate presurgical molecular pathological diagnosis. © 2017 The Authors","Lower-grade gliomas; Machine learning; p53; Prediction; Radiogenomics",,2-s2.0-85032701812
"Ebara Y., Hayashida Y., Uetsuji T., Koyamada K.","Study on category classification of conversation document in psychological counseling with machine learning",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026295610&doi=10.1007%2f978-3-319-63618-4_9&partnerID=40&md5=5605283e53458f6a2f978e6d0db0d194","The beginner counselors have difficulty doing to turns interests for the cognitive characteristic and the internal problems by the client, and are using frequency closed-ended question to confirm the interpretation created in ones mind for the client. Therefore, there is the opportunity for education and training which called the supervision to improve the counseling skill of beginner counselor by expert counselors. However, these documents of the verbatim record in the counseling used in the supervision are large-scale and complex, the expert counselors are very difficult to extract the characteristics and situation of the conversation. As appropriate method to visualize each reaction of the client for each question by beginner counselor, we have developed a system for visualizing the flow of conversation in counseling. However, the expert counselor as the system user requires to correct the initial classification result manually, and the work burden is large, because the accuracy of the category classification of conversation document is very lowin the current system. To improve this problem, we have implemented on the category classification method for text data of conversation document with SVM (Support Vector Machine) as machine learning technique. In addition, we have compared and evaluated with the result of the initial classification in the current system. As these results, we have shown that the accuracy rate of the classification method with SVM become higher than the result in the current system. © Springer International Publishing AG 2018.","Machine learning; Psychological counseling; Text classification",,2-s2.0-85026295610
"Czarnowski I., Jędrzejowicz P.","Stacking-based integrated machine learning with data reduction",2018,"Smart Innovation, Systems and Technologies",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020414050&doi=10.1007%2f978-3-319-59421-7_9&partnerID=40&md5=60551aee0675b3d501026720223028dd","Integrated machine learning is understood as integration of the data reduction with the learning process. Such integration allows to introduce adaptation mechanisms within the learning process by modification of the data with a view to finding its better representation from the point of view of the learning performance criterion. Data modification can be carried out through data reduction in both dimensions, i.e. the feature and the instance ones producing the set of prototypes. Currently, data reduction has become a crucial technique for big data analysis and improvement of the machine learning process results. In this paper the stacking technique has been proposed for improving the process of the integrated machine classification and to assure diversification among prototypes. To validate the proposed approach we have carried-out computational experiment. The paper includes the description of the approach and the discussion of the validating experiment results. © Springer International Publishing AG 2018.","Classification; Data reduction; Learning from data; Prototype selection; Stacked generalization","Artificial intelligence; Big data; Classification (of information); Data integration; Learning systems; Adaptation mechanism; Computational experiment; Integrated machines; Learning from data; Learning performance; Prototype selection; Stacked generalization; Validating experiments; Data reduction",2-s2.0-85020414050
"Xie Z., Zhang J., Xiao W., Sun C., Lu Y.","Extreme learning machine based location-aware activity recognition",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031415072&doi=10.1007%2f978-981-10-6499-9_72&partnerID=40&md5=c7a35bcd1489f72540d9915f8c759b3b","According to the recent government reports, China has gradually entered an aging society. Pension problem is a vital problem to face. Therefore, it will be very useful to monitor the health status of elderly people who live alone at home. To evaluate the abilities of elderly people in daily life, the activities of daily living (ADL) is used. In this work, we propose a novel machine learning approach for ADL recognitions by considering the location context information of the elder. With the popularity of smart phones, motion recognition can be done by the embedded sensors such as acceleration sensors and. However different ADL models possibly have the same movement to a certain degree, which will affect the classification performance. We append the location information as an additional feature to detect ADL. Furthermore, we propose a hierarchical Extreme Learning Machine (ELM) to classify the ADL. With the experiment and test, the algorithm described in this paper can achieve obvious performance in ADL recognition. © 2018, Springer Nature Singapore Pte Ltd.","ADL; ELM; Location; Recognition; Smartphone","Intelligent systems; Knowledge acquisition; Learning systems; Location; mHealth; Smartphones; Acceleration sensors; Activities of Daily Living; Activity recognition; Classification performance; Extreme learning machine; Location information; Machine learning approaches; Recognition; Motion estimation",2-s2.0-85031415072
"Hora K.","Classifying exoplanets as potentially habitable using machine learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031398516&doi=10.1007%2f978-981-10-6602-3_20&partnerID=40&md5=3f60aac209445a097d716c3b57dcc0d0","With the launch of the ESA Gaia satellite observatory and the planned LSST, and the torrent of data coming through the Kepler space observatory, scientists will be able to collect data for more than 1 billion astronomical objects, including millions of exoplanets in the coming years. In this study, several predictive models are built using machine learning algorithms to classify exoplanets as potentially habitable, based on various characteristics of the planet and its star. I applied six supervised learning algorithms for the classification of planets, which include two decision trees, CART and Random Forest, Support Vector Machines, Logistic Regression, Feed-Forward Neural Network, and Naïve Bayes. I further applied CART to create a regression model to predict the value of the ESI (Earth Similarity Index) for an exoplanet. © 2018, Springer Nature Singapore Pte Ltd.","Artificial neural networks; Data mining; Decision trees; Logarithmic regression; Naïve bayes; Random forests; Support vector machines","Artificial intelligence; Data mining; Decision trees; Extrasolar planets; Forestry; Learning systems; Neural networks; Observatories; Regression analysis; Satellites; Sodium; Support vector machines; Astronomical objects; Logarithmic regression; Logistic regressions; Predictive models; Random forests; Regression model; Similarity indices; Space observatories; Learning algorithms",2-s2.0-85031398516
"Dubey A., Lohiya A., Narwal V., Jha A.K., Agarwal P., Schaefer G.","Natural image interpolation using extreme learning machine",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028587739&doi=10.1007%2f978-3-319-60618-7_34&partnerID=40&md5=57426bedc6d8d31001b220674ce9d805","Standard image interpolation methods use a uniform interpolation filter on the entire image. To achieve improved results on specific structures, content adaptive interpolation methods have been introduced. However, these are typically limited to fit image data into a linear model in each class. In this paper, we investigate replacing the linear model by a flexible non-linear model, resulting in a novel interpolation algorithm based on extreme learning machines. Extreme learning machines (ELMs) is a relatively recent learning algorithm for single hidden layer feed-forward neural networks, which compared with conventional neural network learning algorithms, overcomes slow training speed and over-fitting problems. Based on an extensive set of experiments, we show that our proposed approach yields improved image quality, as confirmed by both objective and subjective results. © Springer International Publishing AG 2018.","Extreme learning machine (ELM); Interpolation; Moore Penrose generalised inverse; Single-hidden layer feed forward neural networks (SLFNs); Zooming","Image quality; Interpolation; Knowledge acquisition; Learning systems; Pattern recognition; Soft computing; Extreme learning machine; Interpolation algorithms; Moore-Penrose; Neural network learning algorithm; Over fitting problem; Single-hidden layer feed-forward neural network; Uniform interpolations; Zooming; Learning algorithms",2-s2.0-85028587739
"Tang X., Chai Y., Mao Y., Ji J.","Fault diagnosis of rolling bearing based on wavelet packet and extreme learning machine",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030847952&doi=10.1007%2f978-981-10-6496-8_13&partnerID=40&md5=82bcd070bb05204a779a3ac867a3df9d","The paper presents a new method using wavelet packet analysis and Extreme Learning Machine (ELM) with the following steps. First, the signal is decomposed by wavelet packet, and the root-mean-square (RMS) and energy of the decomposed subband component signals are extracted. Secondly, the fault classification model of rolling bearing is established based on the Extreme Learning Machine (ELM). Finally, the eigenvector composed of the characteristic parameters of the decomposed sub-signals is used as the model input to diagnose the fault of the rolling bearing. The results indicate that this method will be effectively applied to fault diagnosis of rolling bearings. © 2018, Springer Nature Singapore Pte Ltd.","Extreme learning machine (ELM); Fault diagnosis; Rolling bearing; Wavelet packet","Bearings (machine parts); Failure analysis; Intelligent systems; Knowledge acquisition; Learning systems; Roller bearings; Signal processing; Wavelet analysis; Component signals; Extreme learning machine; Fault classification; Model inputs; Rolling bearings; Root Mean Square; Wavelet Packet; Wavelet Packet Analysis; Fault detection",2-s2.0-85030847952
"Ning X., Xu Y., Li Y., Li Y.","Time series data analysis with particle filter-based relevance vectors machine learning",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026660785&doi=10.1007%2f978-3-319-63859-1_41&partnerID=40&md5=3f7f2f6b86201df21043792c9e42fac3","Analyzing and processing big data of product quality inspection is the key to guarantee product quality and safety. Time series data is one of the most important forms of product quality inspection data. Therefore, it is significant to research time series data. In this paper, we focus on the time series prediction data to contain complex noise and uncertainties. We use the correlation vector machine to carry out regression modeling, find the regularity in this series of complex data, and return the RVM regression model with the largest information to establish the state Spatial model. And then use the particle filter method, the model is constantly updated, in order to achieve better prediction. The experimental results show that this method can effectively solve the problem of noise and uncertainty in time series data analysis, and obtain better performance of time series data analysis. © Springer International Publishing AG 2018.","Particle filter; Relevance vector machine; Time series prediction","Bandpass filters; Data handling; Forecasting; Information analysis; Learning systems; Monte Carlo methods; Multimedia signal processing; Quality control; Regression analysis; Signal processing; Time series; Time series analysis; Uncertainty analysis; Correlation vectors; Particle filter; Quality and safeties; Quality inspection; Relevance Vector Machine; Time series data analysis; Time series prediction; Time-series data; Big data",2-s2.0-85026660785
"Mahesh V.G.V., Raj A.N.J.","Zernike moments and machine learning based gender classification using facial images",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028588043&doi=10.1007%2f978-3-319-60618-7_39&partnerID=40&md5=3e6c43f7c75505966b42ddc18774b1b4","This paper proposes a zernike moments based gender classification system using facial image. Gender classification is achieved by training the Bayesian, Support vector machine, Linear discriminant analysis and Neural network classifiers. The proposed method was evaluated using ORL and faces94 databases. The simulation results indicate the effectiveness of the method in achieving the greater accuracy even under the variations in pose, scale, rotation and occlusion. In particular the neural network classifier was excellent in providing classification accuracy of 95% and 100% respectively with 25 zernike moments for ORL and faces94 database. © Springer International Publishing AG 2018.","Classification; Feature extraction; Gender classification; Performance evaluation; Supervised learning; Zernike moments","Classification (of information); Discriminant analysis; Feature extraction; Image retrieval; Learning systems; Pattern recognition; Social sciences; Soft computing; Supervised learning; Bayesian; Classification accuracy; Facial images; Gender classification; Linear discriminant analysis; Neural network classifier; Performance evaluation; Zernike moments; Image classification",2-s2.0-85028588043
"Ježowicz T., Gajdoš P., Uher V., Mišák S., Snášel V.","Improving the speed and quality of extreme learning machine by conjugate gradient method",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028634811&doi=10.1007%2f978-3-319-60834-1_14&partnerID=40&md5=61822c1e4b7027a37e7a84a926444bb8","Extreme Learning Machine (ELM) is a novel learning algorithm. It is basically a feedforward neural network with one hidden layer, fixed input weights and fixed biases. ELM has become popular in recent years due to the fast learning speed and good generalization performance. A novel approach based on Conjugate Gradient Method (CG) is proposed in this Article to improve original ELM. As experiments show, proposed approach is both faster and have higher quality on all tested datasets. Results have also shown that higher quality can be achieved after four iterations of CG. This means that expensive pseudo-inverse operation used in the original algorithm can be replaced by four matrix-vector multiplication and several scalar products. Therefore the proposed approach is more suitable for parallel architectures or can be used for larger datasets. © 2018, Springer International Publishing AG.",,"Feedforward neural networks; Gradient methods; Inverse problems; Knowledge acquisition; Learning algorithms; Learning systems; Parallel architectures; Extreme learning machine; Generalization performance; Hidden layers; Input weights; Matrix vector multiplication; Original algorithms; Pseudo-inverses; Scalar product; Conjugate gradient method",2-s2.0-85028634811
"Solanki N., Panchal G.","A novel machine learning based approach for rainfall prediction",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028368663&doi=10.1007%2f978-3-319-63673-3_38&partnerID=40&md5=145253f9acf270c1f88722c04ff2d651","The climate changes effortlessly nowadays, prediction of climate is very hard. However, the forecasting mechanism is the vital process. It is also a valuable thing as it is the important part of the human life. Accordingly to the research, the weather forecast of rainfall intensity conducted. The remarkable commitment of this proposal is in the implementation of a hybrid intelligent system data mining technique for solving novel practical problems, Hybrid Intelligent system data mining consists of the combination of Artificial Neural Network and the proper usage of Genetic Algorithm. In this research, Genetic algorithm is utilized the type of inputs, the connection structure between the inputs and the output layers and make the training of neural network more efficient. In ANN, Multi-layer Perceptron (MLP) serves as the center data mining (DM) engine in performing forecast tasks. Back Propagation algorithm used for the trained the neural network. During the training phase of the proposed approach, it gains the optimal values of the connection weights which, in fact, utilized as the part of the testing phase of the MLP. Here, the testing phase is used to bring about the rainfall prediction accuracy. It may be noted that the information/data is used to cover the information from the variables namely temperature, cloud fraction, wind, humidity, and rainfall. © 2018, Springer International Publishing AG.","Artificial neural network; Genetic algorithm; Rainfall prediction","Backpropagation; Backpropagation algorithms; Climate change; Data mining; Forecasting; Genetic algorithms; Intelligent systems; Learning algorithms; Learning systems; Neural networks; Rain; Connection structures; Connection weights; Hybrid intelligent system; Multi layer perceptron; Practical problems; Rainfall intensity; Rainfall prediction; Training phase; Weather forecasting",2-s2.0-85028368663
"Hu M., Chen H., Shen L., Li G., Guo Y., Li H., Li J., Hu W.","A machine learning bayesian network for refrigerant charge faults of variable refrigerant flow air conditioning system",2018,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032873792&doi=10.1016%2fj.enbuild.2017.10.012&partnerID=40&md5=4cbff5376c4c3f1bee4520170590e946","An intelligent fault diagnosis network for variable refrigerant flow air conditioning system is proposed in this study. The network is developed under the foundation of bayesian belief network theory, which comprises two main elements: the structure and parameters. The structure obtained by machine learning and experts’ experiences illustrates the relationships among faults and physical variables from the qualitative prospective, and its parameters (including prior probability distribution and conditional distribution) describe the uncertainty between them quantitatively. Once the structure and parameters are determined, the posterior probability distribution which can be used to complete fault diagnosis and isolation will be calculated by some algorithms. In comparison with other fault diagnosis approaches, the proposed approach can make full use of performance information. Moreover, it is more reasonable and precise to express the relationship between faults and variables rather than Boolean variables. Evaluation was conducted on a variable refrigerant flow air conditioning system, which demonstrated that this strategy is effective and efficient. © 2017","Air conditioning system; Bayesian belief network; Fault diagnosis; Refrigerant charge; Variable refrigerant flow","Air conditioning; Artificial intelligence; Bayesian networks; Failure analysis; Learning systems; Probability distributions; Refrigerants; Boolean variables; Conditional distribution; Fault diagnosis and isolations; Intelligent fault diagnosis; Physical variables; Refrigerant charge; Refrigerant flow; Fault detection",2-s2.0-85032873792
"Hossain M., Mekhilef S., Danesh M., Olatomiwa L., Shamshirband S.","Application of extreme learning machine for short term output power forecasting of three grid-connected PV systems",2018,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029681039&doi=10.1016%2fj.jclepro.2017.08.081&partnerID=40&md5=37d079eb31e459b3b71afa330b52016d","The power output (PO) of a photovoltaic (PV) system is highly variable because of its dependence on solar irradiance and other meteorological factors. Hence, accurate PO forecasting of a grid-connected PV system is essential for grid stability, optimal unit commitment, economic dispatch, market participation and regulations. In this paper, a day ahead and 1 h ahead mean PV output power forecasting model has been developed based on extreme learning machine (ELM) approach. For this purpose, the proposed forecasting model is trained and tested using PO of PV system and other meteorological parameters recorded in three grid-connected PV system installed on a roof-top of PEARL laboratory in University of Malaya, Malaysia. The results obtained from the proposed model are compared with other popular models such as support vector regression (SVR) and artificial neural network (ANN). The performance in terms of accuracy and precision of the prediction models is conducted with standard statistical error indicators including: relative root mean square error (RMSE), mean absolute percentage error (MAPE), mean absolute bias error (MABE) and coefficient of determination (R2). The comparison of results obtained from the proposed ELM model to other models showed that ELM model enjoys higher accuracy and less computational time in forecasting the daily and hourly PV output power. © 2017 Elsevier Ltd","ELM; Forecasting; PV system; Statistical indicators","Errors; Forecasting; Knowledge acquisition; Learning systems; Mean square error; Neural networks; Scheduling; Solar power generation; Coefficient of determination; Extreme learning machine; Grid connected PV system; Mean absolute percentage error; Meteorological parameters; PV system; Statistical indicators; Support vector regression (SVR); Electric load dispatching",2-s2.0-85029681039
"Xiao Y., Wu J., Lin Z., Zhao X.","A deep learning-based multi-model ensemble method for cancer prediction",2018,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030330677&doi=10.1016%2fj.cmpb.2017.09.005&partnerID=40&md5=8db9734a18127c7d7bde0c5008d9136e","Background and Objective: Cancer is a complex worldwide health problem associated with high mortality. With the rapid development of the high-throughput sequencing technology and the application of various machine learning methods that have emerged in recent years, progress in cancer prediction has been increasingly made based on gene expression, providing insight into effective and accurate treatment decision making. Thus, developing machine learning methods, which can successfully distinguish cancer patients from healthy persons, is of great current interest. However, among the classification methods applied to cancer prediction so far, no one method outperforms all the others. Methods: In this paper, we demonstrate a new strategy, which applies deep learning to an ensemble approach that incorporates multiple different machine learning models. We supply informative gene data selected by differential gene expression analysis to five different classification models. Then, a deep learning method is employed to ensemble the outputs of the five classifiers. Results: The proposed deep learning-based multi-model ensemble method was tested on three public RNA-seq data sets of three kinds of cancers, Lung Adenocarcinoma, Stomach Adenocarcinoma and Breast Invasive Carcinoma. The test results indicate that it increases the prediction accuracy of cancer for all the tested RNA-seq data sets as compared to using a single classifier or the majority voting algorithm. Conclusions: By taking full advantage of different classifiers, the proposed deep learning-based multi-model ensemble method is shown to be accurate and effective for cancer prediction. © 2017 Elsevier B.V.","Cancer prediction; Deep learning; Feature selection; Gene expression; Multi-model ensemble","Artificial intelligence; Bioinformatics; Classification (of information); Decision making; Deep learning; Diseases; Feature extraction; Forecasting; Genes; Learning systems; RNA; Cancer prediction; Differential gene expressions; High-throughput sequencing; Machine learning methods; Machine learning models; Majority voting algorithm; Multi-model ensemble; Treatment decision makings; Gene expression; Article; breast carcinoma; cancer staging; classification algorithm; classifier; computer model; computer prediction; decision tree; gene expression; human; k nearest neighbor; lung adenocarcinoma; machine learning; malignant neoplasm; random forest; RNA sequence; stomach adenocarcinoma; support vector machine",2-s2.0-85030330677
"Silva-Ramírez E.-L., López-Coello M., Pino-Mejías R.","An application sample of machine learning tools, such as SVM and ANN, for data editing and imputation",2018,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024494299&doi=10.1007%2f978-3-319-62359-7_13&partnerID=40&md5=760b56dda12998377e33adcd0ab15295","This chapter presents studies about the data imputation to estimate missing values, and the Data Editing and Imputation process to identify and correct values erroneously. Artificial Neural Networks and Support Vector Machines are trained as Machine Learning techniques on real and simulated data sets obtaining a complete data set what help to improve the quality of the variables that define the official indicators of the eight Millennium Development Goals. © 2018, Springer International Publishing AG.",,,2-s2.0-85024494299
"Hocking A., Geach J.E., Sun Y., Davey N.","An automatic taxonomy of galaxy morphology using unsupervised machine learning",2018,"Monthly Notices of the Royal Astronomical Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032578083&doi=10.1093%2fmnras%2fstx2351&partnerID=40&md5=1a635f2ceb35b507f2cf6ee2fc20b619","We present an unsupervised machine learning technique that automatically segments and labels galaxies in astronomical imaging surveys using only pixel data. Distinct from previous unsupervised machine learning approaches used in astronomy we use no pre-selection or pre-filtering of target galaxy type to identify galaxies that are similar. We demonstrate the technique on the Hubble Space Telescope (HST) Frontier Fields. By training the algorithm using galaxies from one field (Abell 2744) and applying the result to another (MACS 0416.1-2403), we show how the algorithm can cleanly separate early and late type galaxies without any form of pre-directed training for what an 'early' or 'late' type galaxy is. We then apply the technique to the HST Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS) fields, creating a catalogue of approximately 60 000 classifications. We show how the automatic classification groups galaxies of similar morphological (and photometric) type and make the classifications public via a catalogue, a visual catalogue and galaxy similarity search. We compare the CANDELS machine-based classifications to human-classifications from the Galaxy Zoo: CANDELS project. Although there is not a direct mapping between Galaxy Zoo and our hierarchical labelling, we demonstrate a good level of concordance between human and machine classifications. Finally, we show how the technique can be used to identify rarer objects and present lensed galaxy candidates from the CANDELS imaging. © 2017 The Authors.","Methods: data analysis; Methods: observational; Methods: statistical",,2-s2.0-85032578083
"Remeseiro B., Barreira N., Sánchez-Brea L., Ramos L., Mosquera A.","Machine Learning Applied to Optometry Data",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032025204&doi=10.1007%2f978-3-319-67513-8_7&partnerID=40&md5=58383a3bb534b30e63cb494edcd725a1","Optometry is the primary health care of the eye and visual system. It involves detecting defects in vision, signs of injury, ocular diseases as well as problems with general health that produce side effects in the eyes. Myopia, presbyopia, glaucoma or diabetic retinopathy are some examples of conditions that optometrists usually diagnose and treat. Moreover, there is another condition that we have all experienced once in a while, especially if we work with computers or have been exposed to smoke or wind. Dry eye syndrome (DES) is a hidden multifactorial disease related with the quality and quantity of tears. It causes discomfort and could lead to severe visual problems. In this chapter, we explain how machine learning techniques can be applied in some DES medical tests in order to produce an objective, repeatable and automatic diagnosis. The results of our experiments show that the proposed methodologies behave like the experts so that they can be applied in the daily practice. © Springer International Publishing AG 2018.","Classification; Dry eye syndrome; Feature selection; Image analysis; Optometry data; Regression",,2-s2.0-85032025204
"Syed L., Jabeen S., Manimala S.","Telemammography: A novel approach for early detection of breast cancer through wavelets based image processing and machine learning techniques",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031996738&doi=10.1007%2f978-3-319-63754-9_8&partnerID=40&md5=da6c5a97c0c8b38b17ff9b65346eb1e6","Telehealth monitoring is an innovative process of synergising the benefits of information and communication technologies (ICT) and Internet of Things (IoT) to deliver healthcare services to remote, distant, and underserved regions. The objective of this study is to deliver healthcare services to patients outside the conventional settings by connecting the patient and healthcare providers with technology. As technologies for telehealth monitoring have become more advanced, they have become fully integrated into delivery of healthcare service. One of the most publicized telehealth services is the use of telemammography in the early diagnosis of breast cancer from remote and rural locations. Automated detection and classification of tumor in telemammographic images is of high importance for physicians for accurate prediction of the diseases. This study presents advances in telehealth services and also proposes novel telemammography system for early detection of breast cancer from remote and underserved areas. In this study, we have used efficient wavelet-based image processing techniques for preprocessing, detection, and enhancing the resolution of mammographic images. A detailed comparative analysis is performed to select the best classification model using different classification algorithms. We have used Multi-Layer Perceptron Neural Networks, J48 decision trees, Random Forest, and K-Nearest Neighbor classifier for classifying the tumor into three categories namely: benign, malignant, and normal. The classification is based on the area, volume, and boundaries of tumor masses. All the tumor features and classification methods are compared using Accuracy, Sensitivity, Specificity, Precision, and Mean Square Error. Experimental results on the Mammographic Image Analysis Society (MIAS) database are found to give the best results when neural network classifier is used for classification of mammographic images. © Springer International Publishing AG 2018.",,,2-s2.0-85031996738
"Kryzhanovsky B., Dunin-Barkowski W., Redko V.","Advances in neural computation, machine learning, and cognitive research: Selected papers from the XIX international conference on neuroinformatics, october 2–6, 2017, Moscow, Russia",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029228771&doi=10.1007%2f978-3-319-66604-4&partnerID=40&md5=50e8e276efdaf827b4720fe5dd765c67",[No abstract available],,,2-s2.0-85029228771
"Dostatni E., Rojek I., Hamrol A.","The use of machine learning method in concurrent ecodesign of products and technological processes",2018,"Lecture Notes in Mechanical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032709613&doi=10.1007%2f978-3-319-68619-6_31&partnerID=40&md5=8286549e9a2e2e11c37c1e56bfa2eba0","The article describes a new, original approach to integrated ecodesign of products and technological processes, ensuring appropriate selection of materials and connections from the point of view of recyclability. The method was implemented in an expert system. The decision tree induction method was used as the system tool. The expert system offers a practical solution which makes it possible to change a material or connection without having to consult the product designer. It is consistent with concurrent engineering (CE) design. © Springer International Publishing AG 2018.","Decision tree; Ecodesign; Expert system; Technological processes",,2-s2.0-85032709613
"Harrison R.W., Freas C.","Fuzzy restricted boltzmann machines",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030696010&doi=10.1007%2f978-3-319-67137-6_43&partnerID=40&md5=47a9e7174e122867cc70865b277726b8","Restricted Boltzmann Machines are a reconstructive neural network. They derive an implicitly probabilistic model of data which can be used to reconstruct or filter missing data as well as to classify data. This paper develops a deterministic training algorithm and shows how to use that algorithm to automatically derive fuzzy membership classes. The algorithm developed in this paper combines many of the best features of fuzzy learning algorithms and Restricted Boltzmann machines. © Springer International Publishing AG 2018.","Data mining; Deep learning; Energy-based learning; Fuzzy machine learning; Restricted boltzmann machines","Classification (of information); Data mining; Deep learning; Filtration; Learning systems; Energy-based; Fuzzy membership; Missing data; Probabilistic modeling; Restricted boltzmann machine; Training algorithms; Learning algorithms",2-s2.0-85030696010
"Fong S., Deb S., Yang X.-S.","How meta-heuristic algorithms contribute to deep learning in the hype of big data analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026775964&doi=10.1007%2f978-981-10-3373-5_1&partnerID=40&md5=ad6fcb2e35de3e2aa77873ed8dc3d955","Deep learning (DL) is one of the most emerging types of contemporary machine learning techniques that mimic the cognitive patterns of animal visual cortex to learn the new abstract features automatically by deep and hierarchical layers. DL is believed to be a suitable tool so far for extracting insights from very huge volume of so-called big data. Nevertheless, one of the three “V” or big data is velocity that implies the learning has to be incremental as data are accumulating up rapidly. DL must be fast and accurate. By the technical design of DL, it is extended from feed-forward artificial neural network with many multi-hidden layers of neurons called deep neural network (DNN). In the training process of DNN, it has certain inefficiency due to very long training time required. Obtaining the most accurate DNN within a reasonable run-time is a challenge, given there are potentially many parameters in the DNN model configuration and high dimensionality of the feature space in the training dataset. Meta-heuristic has a history of optimizing machine learning models successfully. How well meta-heuristic could be used to optimize DL in the context of big data analytics is a thematic topic which we pondered on in this paper. As a position paper, we review the recent advances of applying meta-heuristics on DL, discuss about their pros and cons and point out some feasible research directions for bridging the gaps between meta-heuristics and DL. © Springer Nature Singapore Pte Ltd. 2018.","Algorithm design; Deep learning; Meta-heuristic algorithm; Nature-inspired computing algorithms; Neural network training","Artificial intelligence; Computation theory; Data mining; Deep learning; Deep neural networks; Heuristic algorithms; Intelligent computing; Learning algorithms; Learning systems; Neural networks; Optimization; Algorithm design; Feed-forward artificial neural networks; High dimensionality; Machine learning models; Machine learning techniques; Meta heuristic algorithm; Nature inspired computing; Neural network training; Big data",2-s2.0-85026775964
"Ayala-Raggi S.E., Manzano F.M., Barreto-Flores A., Sánchez-Urrieta S., Portillo-Robledo J.F., Bautista-López V.E., Ayala-Raggi P.","A Supervised Incremental Learning Technique for Automatic Recognition of the Skeletal Maturity, or can a Machine Learn to Assess Bone Age Without Radiological Training from Experts?",2018,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030868256&doi=10.1142%2fS0218001418600029&partnerID=40&md5=68e5b85f3bb76940bda3c0910e00d87d","Skeletal maturity estimation is an important medical procedure in the early diagnosis of growth disorders. Traditionally, it is performed by an expert physician or radiologist who determines it based on a visual subjective inspection, the approximated bone age of the child. This task is time consuming and is usually dependent on the judgment of each particular physician. Therefore, automated methods are extremely valuable and desirable. In this paper, we propose and describe an automatic method to estimate skeletal maturity through a supervised and incremental learning approach. Our method determines bone age by comparing aligned images with a K-NN regression classifier. Here, we have solved the difficult task of image alignment by designing a radiological-hand specific Active Appearance Model, which was developed from a varied set of hand-labeled radiological images. By using this active model, our system constructs its own learned database by increasing a set of shape-aligned training images which are incrementally stored. Thus, when a test image arrives at the system, the alignment process is performed before the classification task takes place. For that purpose, we designed an original layout of landmarks to be located in representative regions of the radiographical image of the hand. Our results show that it is possible to use pixels directly as classification features as long as training and testing images have been previously aligned in shape and pose. © 2018 World Scientific Publishing Company.","active appearance models; Bone age assessment; hand segmentation","Diagnosis; Image recognition; Active appearance models; Automatic recognition; Bone age assessment; Classification features; Classification tasks; Incremental learning; Radiological trainings; Training and testing; Bone",2-s2.0-85030868256
"MacAlpine P., Stone P.","Overlapping layered learning",2018,"Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031808455&doi=10.1016%2fj.artint.2017.09.001&partnerID=40&md5=7ba987995a36cd7a83257a5db5be9297","Layered learning is a hierarchical machine learning paradigm that enables learning of complex behaviors by incrementally learning a series of sub-behaviors. A key feature of layered learning is that higher layers directly depend on the learned lower layers. In its original formulation, lower layers were frozen prior to learning higher layers. This article considers a major extension to the paradigm that allows learning certain behaviors independently, and then later stitching them together by learning at the “seams” where their influences overlap. The UT Austin Villa 2014 RoboCup 3D simulation team, using such overlapping layered learning, learned a total of 19 layered behaviors for a simulated soccer-playing robot, organized both in series and in parallel. To the best of our knowledge this is more than three times the number of layered behaviors in any prior layered learning system. Furthermore, the complete learning process is repeated on four additional robot body types, showcasing its generality as a paradigm for efficient behavior learning. The resulting team won the RoboCup 2014 championship with an undefeated record, scoring 52 goals and conceding none. This article includes a detailed experimental analysis of the team's performance and the overlapping layered learning approach that led to its success. © 2017 Elsevier B.V.","CMA-ES; Hierarchical machine learning; Layered learning; Reinforcement learning; Robot skill learning; Robot soccer","Artificial intelligence; Reinforcement learning; Robots; Sports; Behavior learning; Experimental analysis; Key feature; Layered learning; Learning process; Robot skills; Robot soccer; Soccer-playing robots; Learning systems",2-s2.0-85031808455
"Korycki Ł., Krawczyk B.","Combining active learning and self-labeling for data stream mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019231123&doi=10.1007%2f978-3-319-59162-9_50&partnerID=40&md5=7668e5e6f072d4b4bcc05a2e78add71c","Data stream mining is among the most vital contemporary data science challenges. In this work we concentrate on the issue of actual availability of true class labels. Assumption that the ground truth for each instance becomes known right after processing it is far from being realistic, due to usually high costs connected with its acquisition. Active learning is an attractive solution to this problem, as it selects most valuable instances for labeling. In this paper, we propose to augment the active learning module with self-labeling approach. This allows classifier to automatically label instances for which it displays the highest certainty and use them for further training. Although in this preliminary work we use a static threshold for self-labeling, the obtained results are encouraging. Our experimental study shows that this approach complements the active learning strategy and allows to improve data stream classification, especially in scenarios with very small labeling budget. © Springer International Publishing AG 2018.","Active learning; Data stream mining; Machine learning; Self-labeling; Semi-supervised learning","Budget control; Data communication systems; Learning algorithms; Learning systems; Supervised learning; Active Learning; Active learning strategies; Attractive solutions; Data stream classifications; Data stream mining; Further trainings; Semi- supervised learning; Static thresholds; Artificial intelligence",2-s2.0-85019231123
"Lee D., Kim J.","Autonomous algorithm for safety systems of the nuclear power plant by using the deep learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025811868&doi=10.1007%2f978-3-319-60204-2_8&partnerID=40&md5=0129ffc27ca852a916bfca087bee9583","This study aims to develop an autonomous algorithm to control the safety systems of nuclear power plant (NPP) by using the deep learning that is one of machine learning methods. The autonomous algorithm has two main goals. First, it achieves a high level of automation for nine safety functions of NPP. Second, the algorithm controls the nine safety functions in an integrated way. The function-based hierarchical framework is suggested to represent the multi-level structure that models NPP safety systems with the levels of goal, function and system. The function-based hierarchical framework is used to model the NPP for the application of the multi-system deep learning network. Multi-system deep learning network is applied to develop the algorithm for autonomous control. This approach enables the systematic analysis of power plant system and development of the database for the deep learning network. © Springer International Publishing AG 2018.","Autonomous algorithm; Deep-learning; Systems engineering","Education; Electric industry; Hierarchical systems; Human engineering; Learning algorithms; Learning systems; Nuclear energy; Nuclear fuels; Nuclear power plants; Safety engineering; Security systems; Systems engineering; Autonomous control; Learning network; Level of automations; Machine learning methods; Multi-level structures; Power plant system; Safety functions; Systematic analysis; Deep learning",2-s2.0-85025811868
"Lee G.K., Ryu S.Y., Kim C.","Block-Incremental Deep Learning Models for Timely Up-to-Date Learning Results",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032506433&doi=10.1007%2f978-981-10-6520-0_24&partnerID=40&md5=436d305ee73c4eb37f51db9c8e51f77d","As mobile devices and personal computers have been more frequently used through the Internet, data generated by not only people but also devices have been continuously piling up. The data growing endlessly is called Big Data and Deep Learning algorithms with the Big Data have been introducing the next level of artificial intelligence. It is generally applicable that the more data deep learning algorithms train, the more accurate the deep learning algorithms are. Then, an important problem is which size of data is enough for deep learning algorithms to train the data. In many cases, it is not practical that we wait for the data to grow bigger enough, and thus we need a new learning model that can reduce this latency time and timely derive learning results with useful accuracy. In this paper, we propose novel block-incremental learning models for deep learning and experimentally show that the proposed model can timely derive learning results with useful accuracy and the final accuracy is even better than the traditional deep learning algorithms with the same size of training data. © 2018, Springer Nature Singapore Pte Ltd.","Incremental learning; Learning model; Machine learning","Artificial intelligence; Big data; Deep learning; Learning systems; Personal computers; Incremental learning; Latency time; Learning models; Training data; Learning algorithms",2-s2.0-85032506433
"Geman O., Chiuchisan I., Covasa M., Doloc C., Milici M.-R., Milici L.-D.","Deep learning tools for human microbiome big data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029511447&doi=10.1007%2f978-3-319-62521-8_21&partnerID=40&md5=b9ac13c2e213a207bfb61319fd1705fe","Deep Learning is a branch of Machine Learning, which focuses on a set of algorithms that model high-level abstractions in data by using a deep representation of multiple processing layers. The goal of Machine Learning is to map input patterns to output values. This paper will suggest a potential application of Deep Learning Algorithms for the analysis of large amounts of data produced by the research of the Human Microbiome. Humans have coevolved with microbes in the environment, and each body habitat has a unique set of microorganisms (microbiota). The most abundant and well-studied microbiota are found in the gut, where the bacterial density reaches 1011–1012Â cells/g in the distal human colon. The number of bacteria in the human gut has been estimated to exceed the number of somatic cells in the body by an order of magnitude and that the biomass of the gut microbiota may reach up to 1.5Â kg. This paper presents different methods that have been implemented and tested on a Human Microbiome Dataset. Besides the findings concerning accuracy and runtime, the results suggest that the Deep Learning algorithms could be successfully used to analyze large amounts of Microbiota data. © 2018, Springer International Publishing AG.","Big Data; Data Mining; Deep learning; Human Microbiome; Machine learning","Artificial intelligence; Bacteria; Big data; Data mining; Deep learning; Learning systems; Microorganisms; Soft computing; Bacterial density; Gut microbiota; High-level abstraction; Human microbiome; Input patterns; Large amounts of data; Learning tool; Multiple processing; Learning algorithms",2-s2.0-85029511447
"Shang R., Yuan Y., Jiao L., Meng Y., Ghalamzan A.M.","A self-paced learning algorithm for change detection in synthetic aperture radar images",2018,"Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026768629&doi=10.1016%2fj.sigpro.2017.07.023&partnerID=40&md5=5bc1746a2600b72e519b463038c5c308","Detecting changed regions between two given synthetic aperture radar images is very important to monitor change of landscapes, change of ecosystem and so on. This can be formulated as a classification problem and addressed by learning a classifier, traditional machine learning classification methods very easily stick to local optima which can be caused by noises of data. Hence, we propose an unsupervised algorithm aiming at constructing a classifier based on self-paced learning. Self-paced learning is a recently developed supervised learning approach and has been proven to be capable to overcome effectively this shortcoming. After applying a pre-classification to the difference image, we uniformly select samples using the initial result. Then, self-paced learning is utilized to train a classifier. Finally, a filter is used based on spatial contextual information to further smooth the classification result. In order to demonstrate the efficiency of the proposed algorithm, we apply our proposed algorithm on five real synthetic aperture radar images datasets. The results obtained by our algorithm are compared with five other state-of–the-art algorithms, which demonstrates that our algorithm outperforms those state-of-the-art algorithms in terms of accuracy and robustness. © 2017 Elsevier B.V.","Change detection; Self-paced learning; Synthetic aperture radar (SAR)","Classification (of information); Learning systems; Radar; Radar imaging; Radar signal processing; Synthetic aperture radar; Tracking radar; Change detection; Classification results; Contextual information; Machine learning classification; Self-paced learning; State-of-the-art algorithms; Supervised learning approaches; Unsupervised algorithms; Learning algorithms",2-s2.0-85026768629
"Ionescu V.-S., Czibula G., Teletin M.","Supervised learning techniques for body mass estimation in bioarchaeology",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421039&doi=10.1007%2f978-3-319-62524-9_7&partnerID=40&md5=ad024bf56674812085cabe32b3d34bbb","The task of estimating the body mass from human skeletal remains based on bone measurements is an important one in bioarchaeology and forensic anthropology. Most of the current literature deals with this problem through mathematical linear regression formulas applied to various bones. In order to improve the existing results, two supervised learning-based regression models are proposed, using artificial neural networks and support vector machines, which are useful for expressing good (usually nonlinear) mappings between skeletal measurements and body mass. Several experiments performed on an open source data set show that the proposed applications of machine learning-based algorithms lead to better results than the current state of the art. Thus, the proposed methods are useful for producing good body mass estimations from skeletal measurements. © Springer International Publishing AG 2018.","Artificial neural networks; Bioarchaeology; Body mass estimation; Machine learning; Support vector machines","Artificial intelligence; Bone; Learning systems; Neural networks; Regression analysis; Soft computing; Supervised learning; Support vector machines; Bioarchaeology; Body mass; Forensic anthropologies; Open source datum; Regression formulas; Regression model; State of the art; Estimation",2-s2.0-85031421039
"Artime Ríos E.M., Seguí Crespo M.M., Suarez Sánchez A., Suárez Gómez S.L., Sánchez Lasheras F.","Genetic algorithm based on support vector machines for computer vision syndrome classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028641731&doi=10.1007%2f978-3-319-67180-2_37&partnerID=40&md5=3d58bfc7147a95b54d27fe3be8b8e247","The inclusion in workplaces of video display terminals has introduced multiple benefits in the organization of the work. Nevertheless, it also implies a series of risks for the health of the workers, since it can cause ocular and visual disorders, among others. In this work, a group of eye and vision-related problems associated to prolonged computer use (known as computer vision syndrome) are studied. The aim is to select the characteristics of the subject most relevant for the occurrence of this syndrome, and then, to develop a classification model for its prediction. The estimation of this problem is made by means of support vector machines for classification. This machine learning technique will be trained with the support of a genetic algorithm. This provides different patterns of parameters to the training of the support vector machine, improving its performance. The model performance is verified in terms of the area under the ROC curve, which leads to a model with high accuracy in the classification of the syndrome. © 2018, Springer International Publishing AG.","Computer vision syndrome; Genetic Algorithms; Support vector machines","Artificial intelligence; Computer terminals; Education; Genetic algorithms; Health risks; Learning systems; Soft computing; Support vector machines; Vectors; Area under the ROC curve; Classification models; Computer use; Computer vision syndromes; High-accuracy; Machine learning techniques; Model performance; Video display terminals; Computer vision",2-s2.0-85028641731
"Jakubczyk-Gałczyńska A., Kristowski A., Jankowski R.","Application of support vector machine for determination of impact of traffic-induced vibrations on buildings",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028587089&doi=10.1007%2f978-3-319-64465-3_16&partnerID=40&md5=84702e490c930f4bbab4a9c692e488dd","The aim of the article is to present an algorithm of Support Vector Machine created to forecast the impact of traffic-induced vibrations on residential buildings. The method is designed to classify the object into one of two classes. The classification into the first class means that there is no impact of vibrations on the building, while classification to the second class indicates the possible influence and suggests the execution of expert studies. Variety of factors affecting impact of vibrations on the structure were taken into account in the algorithm. These factors induce: condition of the road surface, condition of the building, soil type, the distance from the building to the road and type of the vehicle. © Springer International Publishing AG 2018.","Buildings; Machine learning; SVM; Traffic-induced vibrations","Buildings; Learning systems; Maintenance; Production; Roads and streets; Support vector machines; Vibrations (mechanical); Class mean; Residential building; Road surfaces; Second class; Soil types; Traffic induced vibrations; Intelligent systems",2-s2.0-85028587089
"Xue W., Brahm G., Pandey S., Leung S., Li S.","Full left ventricle quantification via deep multitask relationships learning",2018,"Medical Image Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030449769&doi=10.1016%2fj.media.2017.09.005&partnerID=40&md5=94a2fd604e1a1f1232395edba9b82a0c","Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac disease. However, it is still a task of great challenge due to the high variability of cardiac structure across subjects and the complexity of temporal dynamics of cardiac sequences. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one phase (Diastole or Systole), is even more challenging since the ambiguous correlations existing among these indices may impinge upon the convergence and generalization of the learning procedure. In this paper, we propose a deep multitask relationship learning network (DMTRL) for full LV quantification. The proposed DMTRL first obtains expressive and robust cardiac representations with a deep convolution neural network (CNN); then models the temporal dynamics of cardiac sequences effectively with two parallel recurrent neural network (RNN) modules. After that, it estimates the three types of LV indices under a Bayesian framework that is capable of learning multitask relationships automatically, and estimates the cardiac phase with a softmax classifier. The CNN representation, RNN temporal modeling, Bayesian multitask relationship learning, and softmax classifier establish an effective and integrated network which can be learned in an end-to-end manner. The obtained task covariance matrix captures the correlations existing among these indices, therefore leads to accurate estimation of LV indices and cardiac phase. Experiments on MR sequences of 145 subjects show that DMTRL achieves high accurate prediction, with average mean absolute error of 180 mm2, 1.39 mm, 2.51 mm for areas, RWT, dimensions and error rate of 8.2% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function. © 2017 Elsevier B.V.","Bayesian framework; Left ventricle quantification; Multitask learning; Multitask relationship","Covariance matrix; Deep neural networks; Diagnosis; Knowledge based systems; Learning systems; Recurrent neural networks; Accurate prediction; Bayesian frameworks; Clinical assessments; Convolution neural network; Left ventricles; Multitask learning; Phase classification; Recurrent neural network (RNN); Heart; adolescent; adult; aged; algorithm; Article; artificial neural network; Bayesian learning; cardiac imaging; cardiovascular parameters; clinical assessment; convolution neural network; deep multitask relationship learning; heart function; heart left ventricle; heart left ventricle dimension; heart left ventricle index; human; machine learning; nuclear magnetic resonance imaging; priority journal; recurrent neural network; regional wall thickness",2-s2.0-85030449769
"Ye F.","Sentiment classification for Chinese micro-blog based on the extension of network terms feature",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031401935&doi=10.1007%2f978-981-10-3773-3_22&partnerID=40&md5=c5911dc09e45627e520d666f3e7b68fa","Sentiment analysis is widely used in product reviews, movie reviews, and micro-blog reviews. Micro-blog review is different from general commodity or movie reviews, which often contains the user’s randomness and lots of network terms. So the micro-blog reviews emotional analysis is not a small challenge. Network terms generally express strong emotions or the user’s point of view, the traditional bag words model and machine learning method do not use the network terms features. In the face of ever-changing micro-blog reviews manifestations, forecast accuracy may be affected. Therefore, in this paper our study focuses on the micro-blog emotional analysis through the extended network terms features and integration with other features. We are taking experiments to compare prediction performance under the different feature fusions, to find out which feature fusion can get the best results. Our results show that by the extended network term feature integration with other features ways to improve the accuracy of predictions, especially some of the most popular micro-blog reviews. © Springer Nature Singapore Pte Ltd. 2018.","Machine learning; Support vector machine; Text sentiment classification","Artificial intelligence; Forecasting; Learning systems; Support vector machines; Text processing; Emotional analysis; Extended networks; Feature integration; Forecast accuracy; Machine learning methods; Prediction performance; Sentiment analysis; Sentiment classification; Blogs",2-s2.0-85031401935
"Mircea I.-G., Bocicor I., Czibula G.","A reinforcement learning based approach to multiple sequence alignment",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031413358&doi=10.1007%2f978-3-319-62524-9_6&partnerID=40&md5=504d5afe598fb640653de2bf4cf949ad","Multiple sequence alignment plays an important role in comparative genomic sequence analysis, being one of the most challenging problems in bioinformatics. This problem refers to the process of arranging the primary sequences of DNA, RNA or protein to identify regions of similarity that may be a consequence of functional, structural or evolutionary relationships between the sequences. In this paper we tackle multiple sequence alignment from a computational perspective and we introduce a novel approach, based on reinforcement learning, for addressing it. The experimental evaluation is performed on several DNA data sets, two of which contain human DNA sequences. The efficiency of our algorithm is shown by the obtained results, which prove that our technique outperforms other methods existing in the literature and which also indicate the potential of our proposal. © Springer International Publishing AG 2018.","Bioinformatics; Machine learning; Multiple sequence alignment; Reinforcement learning","Bioinformatics; DNA; DNA sequences; Learning algorithms; Learning systems; Nucleic acids; Soft computing; Comparative genomic; DNA data sets; Evolutionary relationships; Experimental evaluation; Multiple sequence alignments; Primary sequences; Reinforcement learning",2-s2.0-85031413358
"Vadamodula P., Rao M.P., Hemanth Kumar V., Radhika S., Vahini K., Vineela C., Sravani C., Tamada S.R.","Scrutiny of data sets through procedural algorithms for categorization",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021232518&doi=10.1007%2f978-981-10-3223-3_42&partnerID=40&md5=de7718e152fa85512e9fe0f673ce8b9a","This paper evaluates the selected classification algorithms for classifying thyroid datasets. The classification algorithms considered here are principle component analysis method and partial least square regression method of machine learning algorithms. After successful prediction of disease levels, these algorithms resultant output levels are compared. The analysis suggests the best classifiers for predicting the exact levels of thyroid disease. This work is a comparative study of above said algorithms on thyroid dataset firmly collected from UCI Machine Learning Repository. © Springer Nature Singapore Pte Ltd. 2018.","Machine learning algorithm; PCA; PLS; Ridge regression; Thyroid disease","Artificial intelligence; Classification (of information); Drug products; Education; Intelligent computing; Learning systems; Least squares approximations; Principal component analysis; Regression analysis; Classification algorithm; Comparative studies; Output levels; Partial least square regression; Principle component analysis; Ridge regression; Thyroid disease; UCI machine learning repository; Learning algorithms",2-s2.0-85021232518
"Nagaraja P., Sadashivappa G.","Fault diagnosis with statistical properties and implementation using MKSVM for flash ADC",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028420962&doi=10.1007%2f978-3-319-63673-3_60&partnerID=40&md5=8ee3424a69b94df2ab22a9b3f41f9ecf","This paper focuses on the Fault Diagnosing methodologies crucial for attaining reliability and maintainability of all electronic circuits is implemented for analog to digital converter (ADC) with a wide range of faults. Fault, Diagnosis (FD) is considered as the pattern recognition problem and solved by machine learning theory. Functional, test is, is needed instead of structural test for, testing complex circuits. Fault diagnosis using Fault Dictionary, Neural Networks and Fuzzy logic are enigmatic or inconclusive diagnosis results which have more debug duration and even inaccurate repair actions that exponentially rises service overhead. The effectiveness of these methods are considered, which cover ability in detecting, identifying and localization of faults, the ability of analysing linear and nonlinear circuits, etc. Recent machine learning techniques like support vector machines (SVM) with kernel functions improve the preciseness of functional FD which reduces the product cost through correct repair process. The proposed Multikernel SVM (MKSVM) methodology gives better results than earlier methods as it works with the fundamentals of machine learning and generalization for FD. © 2018, Springer International Publishing AG.","ADC; Kernel functions and MKSVM; Machine learning; Structural and functional faults","Analog to digital conversion; Artificial intelligence; Failure analysis; Finite difference method; Fuzzy logic; Fuzzy neural networks; Intelligent systems; Learning systems; Pattern recognition; Repair; Support vector machines; Analog to digital converters; Fault diagnosing; Functional faults; Kernel function; Machine learning techniques; Nonlinear circuit; Pattern recognition problems; Statistical properties; Fault detection",2-s2.0-85028420962
"Yang Z., Xu Y.","A safe screening rule for Laplacian support vector machine",2018,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032750210&doi=10.1016%2fj.engappai.2017.10.011&partnerID=40&md5=0016c7111ccf2ca73240260dfc3369f6","Laplacian support vector machine (LapSVM) has received much concern in semi-supervised learning (SSL) field. To further improve its computational speed, many efficient algorithms have been developed. However, they just focus on the method of solving optimization problem not the scale of the problem itself. Inspired by the sparsity of LapSVM, in this paper, an efficient safe screening rule for LapSVM (SSR-LapSVM) is proposed to address this issue. The proposed method could significantly accelerate the original LapSVM. Through the rule, most of the training samples can be eliminated before solving optimization problem without sacrificing the optimal solution. An important advantage is the safety, in the sense that the solution is exactly the same as the original LapSVM. Different from most existing methods, our approach can effectively deal with the multiple parameter problems. Experiments on both 3 artificial datasets and 24 real world benchmark datasets demonstrate its feasibility and efficiency. © 2017 Elsevier Ltd","Safe screening; Semi-supervised learning; Support vector machine; Variational inequality","Computational efficiency; Laplace transforms; Optimization; Supervised learning; Support vector machines; Variational techniques; Artificial datasets; Benchmark datasets; Computational speed; Multiple parameters; Optimization problems; Semi- supervised learning; Semi-supervised learning (SSL); Variational inequalities; Problem solving",2-s2.0-85032750210
"Salcedo-Sanz S., Deo R.C., Cornejo-Bueno L., Camacho-Gómez C., Ghimire S.","An efficient neuro-evolutionary hybrid modelling mechanism for the estimation of daily global solar radiation in the Sunshine State of Australia",2018,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032435325&doi=10.1016%2fj.apenergy.2017.10.076&partnerID=40&md5=fa2ab6d4d04feec558c67b3333a802da","This research paper aims to develop a hybrid neuro-evolutionary wrapper-based model for daily global solar radiation estimation in the solar-rich Sunshine State of Queensland, Australia. To design a robust hybrid modelling mechanism, the Interim-ERA European Centre for Medium Range Weather Forecasting (ECMWF) Reanalysis data are employed to train and cross-validate the estimation model that is formulated by an evolutionary-type algorithm: the Coral Reefs Optimization (CRO) integrated with an Extreme Learning Machine (ELM) model. The hybrid CRO-(ELM) algorithm is applied in two stages: first for the feature selection process guided by an ELM algorithm (a class of fast training neural network tool) as the model's fitness function to screen an optimal set of predictor variables and second, for the estimation of the solar radiation using the optimally screened variables by the final hybrid CRO-(ELM)-ELM system. To benchmark the performance of the hybrid CRO-ELM algorithm for this estimation problem we apply an alternative, yet a similar feature screening approach: the Grouping Genetic Algorithm encoded into the ELM-based model (GGA-(ELM) used as the predictor mechanism). After the feature selection process is performed by the CRO algorithm that extracts the data patterns for accurate estimation the alternative objective algorithm is applied (in this case the ELM again) to formulate the hybrid CRO-(ELM)-ELM modelling system. To provide a rigorous evaluation of the CRO-(ELM)-ELM hybrid system, alternative estimation approaches are considered: the Multivariate Adaptive Regression Splines (MARS), Multiple Linear Regression (MLR) and the Support Vector Regression (SVR). The hybrid CRO-(ELM)-ELM system is tested in a real problem where the results are evaluated by means of several statistical score metrics and diagnostic plots of the observed and the estimated daily global solar radiation in the testing phase. We show that the hybrid CRO-(ELM)-ELM model is able to yield promising results; thus improving those attained by the 7 alternative models (i.e., hybrid CRO-(ELM)-MARS, CRO-(ELM)-MLR and CRO-(ELM)-SVR and the GGA equivalent models). The study ascertains that the CRO-based hybrid system where a large pool of predictor data are carefully screened through a wrapper-based modelling system and the ELM model is applied as a objective estimation tool can be adopted as a qualified stratagem in solar radiation estimation problems. The proposed hybrid CRO-(ELM)-ELM system exhibits clear advantages over the alternative machine learning approaches tested and also over the other machine learning algorithms used without the feature selection tool; thus advocating its scientific utility in renewable energy applications. © 2017 Elsevier Ltd","Coral Reefs Optimization; Extreme Learning Machines; Grouping Genetic Algorithms; Hybrid modelling system; Solar radiation estimation","Artificial intelligence; Benchmarking; Feature extraction; Genetic algorithms; Hybrid systems; Knowledge acquisition; Learning algorithms; Learning systems; Linear regression; Optimization; Radiation; Reefs; Regression analysis; Solar radiation; Weather forecasting; Coral reef; Extreme learning machine; Grouping genetic algorithms; Hybrid modelling; Solar radiation estimation; Evolutionary algorithms; estimation method; genetic algorithm; machine learning; model validation; numerical model; optimization; solar power; solar radiation; Australia; Queensland; Anthozoa",2-s2.0-85032435325
"Paiva J.S., Cardoso J., Pereira T.","Supervised learning methods for pathological arterial pulse wave differentiation: A SVM and neural networks approach",2018,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032723746&doi=10.1016%2fj.ijmedinf.2017.10.011&partnerID=40&md5=61ea0173724ca49fd84c29cf7c32d57d","Objective The main goal of this study was to develop an automatic method based on supervised learning methods, able to distinguish healthy from pathologic arterial pulse wave (APW), and those two from noisy waveforms (non-relevant segments of the signal), from the data acquired during a clinical examination with a novel optical system. Materials and methods The APW dataset analysed was composed by signals acquired in a clinical environment from a total of 213 subjects, including healthy volunteers and non-healthy patients. The signals were parameterised by means of 39 pulse features: morphologic, time domain statistics, cross-correlation features, wavelet features. Multiclass Support Vector Machine Recursive Feature Elimination (SVM RFE) method was used to select the most relevant features. A comparative study was performed in order to evaluate the performance of the two classifiers: Support Vector Machine (SVM) and Artificial Neural Network (ANN). Results and discussion SVM achieved a statistically significant better performance for this problem with an average accuracy of 0.9917 ± 0.0024 and a F-Measure of 0.9925 ± 0.0019, in comparison with ANN, which reached the values of 0.9847 ± 0.0032 and 0.9852 ± 0.0031 for Accuracy and F-Measure, respectively. A significant difference was observed between the performances obtained with SVM classifier using a different number of features from the original set available. Conclusion The comparison between SVM and NN allowed reassert the higher performance of SVM. The results obtained in this study showed the potential of the proposed method to differentiate those three important signal outcomes (healthy, pathologic and noise) and to reduce bias associated with clinical diagnosis of cardiovascular disease using APW. © 2017 Elsevier B.V.","Arterial pulse waveform; Morphologic features; Neural network; Support vector machine recursive feature elimination; Support vector machines","Biomedical signal processing; Diagnosis; Learning systems; Neural networks; Optical systems; Pathology; Supervised learning; Time domain analysis; Vectors; Arterial pulse waveforms; Cardio-vascular disease; Clinical environments; Clinical examination; Morphologic features; Multi-class support vector machines; Supervised learning methods; Support vector machine recursive feature eliminations; Support vector machines",2-s2.0-85032723746
"Gelenbe E., Yin Y.","Deep learning with dense random neural networks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030752761&doi=10.1007%2f978-3-319-67792-7_1&partnerID=40&md5=a3e7e0f911c8e790e710f22da2879235","We exploit the dense structure of nuclei to postulate that in such clusters, the neuronal cells will communicate via soma-to-soma interactions, aswell as through synapses. Using the mathematical structure of the spiking Random Neural Network, we construct a multi-layer architecture for Deep Learning. An efficient training procedure is proposed for this architecture. It is then specialized to multi-channel datasets, and applied to images and sensor-based data. © 2018, Springer International Publishing AG.","Deep learning; Machine learning; Neural network","Deep learning; Learning systems; Neural networks; Dense structures; Mathematical structure; Multi channel; Multi-layer architectures; Neuronal cell; Random neural network; Sensor based data; Training procedures; Network architecture",2-s2.0-85030752761
"Xing Y., Lv C., Cao D., Wang H., Zhao Y.","Driver workload estimation using a novel hybrid method of error reduction ratio causality and support vector machine",2018,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030866071&doi=10.1016%2fj.measurement.2017.10.002&partnerID=40&md5=31b201c41cf724949856116682eb07d3","Measuring driver workload is of great significance for improving the understanding of driver behaviours and supporting the improvement of advanced driver assistance systems technologies. In this paper, a novel hybrid method for measuring driver workload estimation for real-world driving data is proposed. Error reduction ratio causality, a new nonlinear causality detection approach, is being proposed in order to assess the correlation of each measured variable to the variation of workload. A full model describing the relationship between the workload and the selected important measurements is then trained via a support vector regression model. Real driving data of 10 participants, comprising 15 measured physiological and vehicle-state variables are used for the purpose of validation. Test results show that the developed error reduction ratio causality method can effectively identify the important variables that relate to the variation of driver workload, and the support vector regression based model can successfully and robustly estimate workload. © 2017 Elsevier Ltd","Causality detection; Correlation analysis; Driver behaviour; Driver workload estimation; Machine learning; Nonlinear system identification","Automobile drivers; Errors; Intelligent systems; Learning systems; Nonlinear systems; Regression analysis; Correlation analysis; Driver behaviour; Driver workload estimations; Error reduction ratios; Nonlinear causalities; Real-world drivings; Support vector regression (SVR); Support vector regression models; Advanced driver assistance systems",2-s2.0-85030866071
"Satyanarayana S., Sravan Kumar P., Sridevi G.","Improved process scheduling in real-time operating systems using support vector machines",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029794449&doi=10.1007%2f978-981-10-4280-5_63&partnerID=40&md5=99af09e1597817c35ac59779d7c56aa5","In the field operating systems, most advanced scheduling method is Multilevel Feedback Scheduling. There are multiple queues with different level priorities (high to low). Selection of each process from a queue is based on the priority of the queue. Support Vector Machine (SVM) method is learning linear predictions in high-dimensional feature spaces. SVM approach can handle the sample complexity challenges by searching large margin separators. In this paper, we are proposing an innovative machine learning Support Vector Machines to predict priorities of multiple queues based on the knowledge of past processes in real-time Operating Systems. This approach can train each queue with linear predictions. So that real-time operating systems like Embedded Systems/Firmware can handle nonhomogenous tasks also. The problem of predicting large volume of processes can be solved with high performance. This algorithm is tested with onelakh processes and these processes are scheduled in 1 min 5.377 s. © Springer Nature Singapore Pte Ltd. 2018.","Dynamic allocations; Priority; Process scheduling; Process type; Queue; Real-time operating systems; Self-learning algorithms; Tasks; Vector","Computer operating systems; Embedded systems; Forecasting; Learning algorithms; Learning systems; Queueing theory; Scheduling; Support vector machines; Vector spaces; Vectors; Dynamic allocations; Priority; Process scheduling; Queue; Real time operating system; Self learning algorithms; Tasks; Real time systems",2-s2.0-85029794449
"Yoo Y., Tang L.Y.W., Brosch T., Li D.K.B., Kolind S., Vavasour I., Rauscher A., MacKay A.L., Traboulsee A., Tam R.C.","Deep learning of joint myelin and T1w MRI features in normal-appearing brain tissue to distinguish between multiple sclerosis patients and healthy controls",2018,"NeuroImage: Clinical",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031800913&doi=10.1016%2fj.nicl.2017.10.015&partnerID=40&md5=db12c5a4502e601349f243925dea08d7","Myelin imaging is a form of quantitative magnetic resonance imaging (MRI) that measures myelin content and can potentially allow demyelinating diseases such as multiple sclerosis (MS) to be detected earlier. Although focal lesions are the most visible signs of MS pathology on conventional MRI, it has been shown that even tissues that appear normal may exhibit decreased myelin content as revealed by myelin-specific images (i.e., myelin maps). Current methods for analyzing myelin maps typically use global or regional mean myelin measurements to detect abnormalities, but ignore finer spatial patterns that may be characteristic of MS. In this paper, we present a machine learning method to automatically learn, from multimodal MR images, latent spatial features that can potentially improve the detection of MS pathology at early stage. More specifically, 3D image patches are extracted from myelin maps and the corresponding T1-weighted (T1w) MRIs, and are used to learn a latent joint myelin-T1w feature representation via unsupervised deep learning. Using a data set of images from MS patients and healthy controls, a common set of patches are selected via a voxel-wise t-test performed between the two groups. In each MS image, any patches overlapping with focal lesions are excluded, and a feature imputation method is used to fill in the missing values. A feature selection process (LASSO) is then utilized to construct a sparse representation. The resulting normal-appearing features are used to train a random forest classifier. Using the myelin and T1w images of 55 relapse-remitting MS patients and 44 healthy controls in an 11-fold cross-validation experiment, the proposed method achieved an average classification accuracy of 87.9% (SD = 8.4%), which is higher and more consistent across folds than those attained by regional mean myelin (73.7%, SD = 13.7%) and T1w measurements (66.7%, SD = 10.6%), or deep-learned features in either the myelin (83.8%, SD = 11.0%) or T1w (70.1%, SD = 13.6%) images alone, suggesting that the proposed method has strong potential for identifying image features that are more sensitive and specific to MS pathology in normal-appearing brain tissues. © 2017 The Authors","Deep learning; Machine learning; Magnetic resonance imaging; Multiple sclerosis; Myelin water imaging","myelin; Article; brain tissue; clinical article; cohort analysis; controlled study; deep belief network; deep learning; demyelinating disease; female; gray matter; human; image processing; machine learning; male; multiple sclerosis; myelin water imaging; neuroimaging; nuclear magnetic resonance imaging; priority journal; three dimensional imaging; white matter",2-s2.0-85031800913
"AlJadda K., Korayem M., Ortiz C., Grainger T., Miller J.A., Rasheed K.M., Kochut K.J., Peng H., York W.S., Ranzinger R., Porterfield M.","Mining massive hierarchical data using a scalable probabilistic graphical model",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497883&doi=10.1016%2fj.ins.2017.10.014&partnerID=40&md5=cb95de85dab5d2790b16b6a4f077b6db","Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and data mining. The crucial limitation of those models, however, is their scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and data mining, demonstrates this limitation when the training data consists of random variables, in which each of them has a large set of possible values. In the big data era, one could expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data - data that is arranged in a treelike structure with several levels - one may see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, unrestricted Bayesian Networks may become infeasible for representing the probability distributions. In this paper, we introduce an extension to Bayesian Networks that can handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves high precision and high recall when used as a multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by CareerBuilder.com, the model was able to predict latent semantic relationships among search keywords with high accuracy. © 2017","Big data; Large scale machine learning; Mass spectrometry annotation; Probabilistic model; Smantic discovery","Artificial intelligence; Bayesian networks; Classification (of information); Data mining; Graphic methods; Knowledge based systems; Learning systems; Mass spectrometry; Probability distributions; Semantics; Spectrometry; Speech recognition; Electronic device; Large-scale machine learning; Mass spectrometry data; Probabilistic graphical models; Probabilistic graphical models (PGM); Probabilistic modeling; Smantic discovery; Tree-like structures; Big data",2-s2.0-85031497883
"Ragot M., Martin N., Em S., Pallamin N., Diverrez J.-M.","Emotion recognition using physiological signals: Laboratory vs. wearable sensors",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021738722&doi=10.1007%2f978-3-319-60639-2_2&partnerID=40&md5=ff0b20ff56ddff1f177f0d5195b25892","Emotion recognition is an important research topic. Physiological signals seem to be an appropriate way for emotion recognition and specific sensors are required to collect these data. Therefore, laboratory sensors are commonly used while the number of wearable devices including similar physiological sensors is growing up. Many studies have been completed to evaluate the signal quality obtained by these sensors but without focusing on their emotion recognition capabilities. In the current study, Machine Learning models were trained to compare the Biopac MP150 (laboratory sensor) and Empatica E4 (wearable sensor) in terms of emotion recognition accuracy. Results show similar accuracy between data collected using laboratory and wearable sensors. These results support the reliability of emotion recognition outside laboratory. © Springer International Publishing AG 2018.","Emotion recognition; Laboratory sensors; Machine learning; Physiological signals; Wearable sensors","Artificial intelligence; Biomedical signal processing; Education; Human engineering; Laboratories; Learning systems; Physiology; Speech recognition; Wearable technology; Emotion recognition; Machine learning models; Physiological sensors; Physiological signals; Research topics; Signal quality; Specific sensors; Wearable devices; Wearable sensors",2-s2.0-85021738722
"Sengupta R., Chapman C.C., Sarkar D., Bortolamiol S.","Automated extraction of movement rationales for building agent-based models: Example of a Red Colobus monkey group",2018,"Advances in Geographic Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032686396&doi=10.1007%2f978-3-319-65993-0_5&partnerID=40&md5=92cdf2ead9769a5fe31e286cb73118bf","The study of animal movement has gained impetus in recent years with improvements in telemetric technologies which enable high resolution tracking, providing researchers with a wealth of animal “big-data”. Coupling such movement data with information about the environments in which the animal moves provides a rich data source that can be exploited to understand an animal’s rationale for movement, which in turn can be used to extract “rules” that govern movement. The extraction of rules can be done using spatial, statistical and machine learning techniques. Once the rules replicating patterns and predictors of movement have been “discovered”, they can be subsequently used to build simulation models (ABMs) to mimic in-silico the behaviours of both individuals and groups of animals. We use field data collected by tracking Red Colobus (Procolobus rufomitratus) monkey groups from Kibale National Park, combined with land cover and terrain information, to show how this might be achieved. © 2018, Springer International Publishing AG.","Agent-based models; Big data; GIS; Machine learning; Movement ecology","Animals; Artificial intelligence; Autonomous agents; Computational methods; Couplings; Extraction; Geographic information systems; Learning systems; Agent-based model; Animal movement; Automated extraction; High resolution; Machine learning techniques; Movement datum; Movement ecologies; Telemetric technology; Big data",2-s2.0-85032686396
"Padmanabhan J., Melvin Jose Premkumar J.","Advanced deep neural networks for pattern recognition: An experimental study",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028608401&doi=10.1007%2f978-3-319-60618-7_17&partnerID=40&md5=3c198b491f8396c7ff6a265f52998802","Difficulty in pattern recognition is perceptible and neural networks approach the problem by way of learning from similar known patterns. Interest in Neural Networks started in the early 1980s when they were deemed to effectively model the human thought process. Speech recognition which first used Artificial Neural Networks (ANNs) to model the states of a Hidden Markov Models (HMMs) later started using Gaussian Mixture Models (GMMs). GMM-HMM systems have been the standard until recently when a new concept of Deep Neural Networks (DNNs) pre-trained using Restricted Boltzmann Machines (RBMs) came into existence. The discriminative capability of the resulting DNN is found to improve the performance of the recognition systems. The experimental work with DNN for recognizing patterns in handwriting and speech corpus has been carried out. In this work we implemented Deep Neural Networks for the above tasks and the pre trained DNN has been used for extracting bottleneck features and hereby improving the performance of the baseline systems with respect to recognition errors. © Springer International Publishing AG 2018.","Artificial neural networks; Boltzmann machine; Deep neural networks; Machine learning; Speech recognition","Character recognition; Deep learning; Deep neural networks; Hidden Markov models; Learning systems; Markov processes; Neural networks; Pattern recognition; Soft computing; Trellis codes; Baseline systems; Boltzmann machines; Bottleneck features; Gaussian mixture model (GMMs); Hidden markov models (HMMs); Recognition error; Recognition systems; Restricted boltzmann machine; Speech recognition",2-s2.0-85028608401
"Malik M.S.I., Hussain A.","An analysis of review content and reviewer variables that contribute to review helpfulness",2018,"Information Processing and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030706000&doi=10.1016%2fj.ipm.2017.09.004&partnerID=40&md5=4a58c2fd34126960ae1bba8d6c027e78","Review helpfulness is attracting increasing attention of practitioners and academics. It helps in reducing risks and uncertainty faced by users in online shopping. This study examines uninvestigated variables by looking at not only the review characteristics but also important indicators of reviewers. Several significant review content and two reviewer variables are proposed and an effective review helpfulness prediction model is built using stochastic gradient boosting learning method. This study derived a mechanism to extract novel review content variables from review text. Six popular machine learning models and three real-life Amazon review data sets are used for analysis. Our results are robust to several product categories and along three Amazon review data sets. The results show that review content variables deliver the best performance as compared to the reviewer and state-of-the-art baseline as a standalone model. This study finds that reviewer helpfulness per day and syllables in review text strongly relates to review helpfulness. Moreover, the number of space, aux verb, drives words in review text and productivity score of a reviewer are also effective predictors of review helpfulness. The findings will help customers to write better reviews, help retailers to manage their websites intelligently and aid customers in their product purchasing decisions. © 2017","Amazon reviews; Auxverb; eWOM; Helpfulness per day; Machine learning; Stochastic GB","Artificial intelligence; Digital storage; Learning systems; Sales; Stochastic models; Stochastic systems; Auxverb; eWOM; Helpfulness per day; Machine learning models; Purchasing decisions; Risks and uncertainties; Stochastic GB; Stochastic gradient boosting; Education",2-s2.0-85030706000
"Avila L.O., Errecalde M.L.","A simple method for recommending specialized specifications for diabetes monitoring",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029381990&doi=10.1016%2fj.eswa.2017.09.019&partnerID=40&md5=d76789d9db5211e79ebb1ece89201ea2","Under glycemic variability, a characterization of the desired blood glucose (BG) behavior is needed to assess if a given artificial pancreas (AP) respects its specification. The specification is an essential element to detect any deviation from an adequate insulin policy. Specializing the monitoring specification is therefore of utmost importance as existing guidelines for diabetes management are general and do not take into account how the personal factors and lifestyle affect the glycemic behavior. Surely, recommending personalized monitoring specifications may provide flexible and appropriate treatment goals to be attained by diabetic patients in order to account for their actual treatment needs. In this work, we use machine learning models to characterize glycemic behavior in synthetic healthy individuals. To account for the day-by-day fluctuation in BG levels, we use a stochastic process superimposed on a deterministic model of the glucose-insulin dynamics. The obtained characterization of the glycemic behavior in healthy individuals is then used as the target class to predict, and thus recommend, personalized monitoring specifications to diabetic patients. Results show that the approach stands as a feasible strategy to recommending appropriate and realistic monitoring goals for diabetic patients based on healthy individuals who share a similar glycemic behavior. Eventually, the incorporation of a recommender approach on an intelligent monitoring system for the AP will allow on-line adaptation of the treatment requirements for each patient. © 2017 Elsevier Ltd","Cold-start recommendation; Glycemic control; Machine learning; Monitoring specification","Artificial intelligence; Artificial organs; Glucose; Insulin; Learning systems; Monitoring; Patient treatment; Random processes; Stochastic models; Stochastic systems; Artificial pancreas; Cold-start Recommendations; Deterministic modeling; Diabetes management; Glycemic control; Healthy individuals; Intelligent monitoring systems; Machine learning models; Specifications",2-s2.0-85029381990
"Mattar A., Reformat M.Z.","Detecting anomalous network traffic using evidence theory",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029418257&doi=10.1007%2f978-3-319-66824-6_43&partnerID=40&md5=6aa026a8ffb3ee0570b6ef255f3478c8","The ability to detect an anomalous network traffic – even if it is slightly different than a normal one – becomes an important aspect of early detection of cyber attacks. Processes of monitoring and analyzing network data should not only provide accurate classifications of network status, but also detect early symptoms of unusual activities in a network. This would lead to a better understanding of suspicious actions, and enable triggering of prevention actions. In this paper, we propose a system that uses multiple classifiers together with elements of evidence theory to identify anomalous network traffic and detect any deviation from a normal network behaviour. The obtained classification results are equipped with confidence levels. The individual classifiers are constructed with different Machine Learning techniques based on data collected with a developed network monitoring software. The data includes multiple features providing a comprehensive view of network traffic. The results of evaluation of a system implementing the proposed approach are discussed. © 2018, Springer International Publishing AG.","Anomalous network traffic; Classification; Evidence theory; Machine learning","Artificial intelligence; Classification (of information); Fuzzy sets; Learning systems; Monitoring; Network security; Pattern matching; Classification results; Evidence theories; Individual classifiers; Machine learning techniques; Multiple classifiers; Multiple features; Network Monitoring; Network traffic; Fuzzy logic",2-s2.0-85029418257
"Kucharski D., Grochala D., Kajor M., Kańtoch E.","A deep learning approach for valve defect recognition in heart acoustic signal",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539207&doi=10.1007%2f978-3-319-67220-5_1&partnerID=40&md5=a0b251e5bef7965133de016a475a6638","The analysis of phonocardiogram (PCG), although considered as well established in a clinical application, still constitutes the valuable source of diagnostic data. Currently, electronic auscultation provides digital signals which can be processed in order to automatically evaluate the condition of heart or lungs. In this paper, we propose a novel approach for the classification of phonocardiographic signals. We extracted a set of time-frequency parameters which enable to effectively differentiate between normal and abnormal heart beats (with valve defects). These features have constituted an input of the convolutional neural network, which we used for classification of pathological signals. The Aalborg University heart sounds database from PhysioNet/Computing in Cardiology Challenge 2016 was used for verification of developed algorithms. We obtained 99.1% sensitivity and 91.6% specificity on the test data, which is motivational for further research. © 2018, Springer International Publishing AG.","Convolutional neural network; Deep learning; Heart sound classification; Machine learning; Signal processing",,2-s2.0-85029539207
"Reder M., Yürüşen N.Y., Melero J.J.","Data-driven learning framework for associating weather conditions and wind turbine failures",2018,"Reliability Engineering and System Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031740982&doi=10.1016%2fj.ress.2017.10.004&partnerID=40&md5=28afa616dbad1bc11aaf60b80633e882","The need for cost effective operation and maintenance (O&M) strategies in wind farms has risen significantly with the growing wind energy sector. In order to decrease costs, current practice in wind farm O&M is switching from corrective and preventive strategies to rather predictive ones. Anticipating wind turbine (WT) failures requires sophisticated models to understand the complex WT component degradation processes and to facilitate maintenance decision making. Environmental conditions and their impact on WT reliability play a significant role in these processes and need to be investigated profoundly. This paper is presenting a framework to assess and correlate weather conditions and their effects on WT component failures. Two approaches, using (a) supervised and (b) unsupervised data mining techniques are applied to pre-process the weather and failure data. An apriori rule mining algorithm is employed subsequently, in order to obtain logical interconnections between the failure occurrences and the environmental data, for both approaches. The framework is tested using a large historical failure database of modern wind turbines. The results show the relation between environmental parameters such as relative humidity, ambient temperature, wind speed and the failures of five major WT components: gearbox, generator, frequency converter, pitch and yaw system. Additionally, the performance of each technique, associating weather conditions and WT component failures, is assessed. © 2017 Elsevier Ltd","Association rule mining; Big data; Data mining; Failure; k-means clustering; Machine learning; Operation & maintenance; Weather; Wind turbine","Big data; Computer system recovery; Cost effectiveness; Decision making; Electric utilities; Learning systems; Maintenance; Meteorology; Weathering; Wind; Wind power; Wind turbines; Environmental conditions; Environmental parameter; K-means clustering; Logical interconnections; Maintenance decision making; Operation and maintenance; Preventive strategies; Rule mining algorithms; Data mining",2-s2.0-85031740982
"Kruthika K.R., Rajeswari, Pai A., Maheshappa H.D.","Classification of Alzheimer and MCI phenotypes on MRI data using SVM",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030162236&doi=10.1007%2f978-3-319-67934-1_23&partnerID=40&md5=d24ca1d12c3b35b946b412893d3e5b45","Alzheimer disease (AD) is a common form of dementia affecting people older than the age of 65. Moreover, AD is commonly diagnosed by behavioural paradormants, cognitive tests, and is followed by brain scans. Computer Aided Diagnosis (CAD), applies medical imaging and machine learning algorithms, to aid in the early diagnosis of Alzheimer’s severity and advancement from prodromal stages i.e. Mild Cognitive Impairment (MCI) to diagnosed Alzheimer’s disease. In this work, SVM (support vector machine) is used for dementia stage classification. Anatomical structures of the brain were obtained from FreeSurfer’s processing of structural Magnetic Resonance Imaging (MRI) data and is utilized for as features for SVM. To be more precise, the system is processed using T1-weighted brain MRI datasets consisting of: 150 mild cognitive impairment (MCI) patients, 80 AD patients and 130 normal controls (NC) obtained from Alzheimer Disease Neuroimaging Initiative (ADNI) database. The volumes of brain structures (hippocampus, medial temporal lobe, whole brain, ventricular, cortical grey matter, entorhinal cortex and fusiform) are employed as biomarkers for multi-class classification of AD, MCI, and NC. © Springer International Publishing AG 2018.","Alzheimer disease; FreeSurfer; Machine learning; Mild cognitive impairment; Normal control; Structural magnetic resonance imaging; SVM","Artificial intelligence; Brain; Computer aided instruction; Diagnosis; Disease control; Image segmentation; Learning algorithms; Learning systems; Magnetic resonance imaging; Medical imaging; Neurodegenerative diseases; Neuroimaging; Signal processing; Support vector machines; Alzheimer disease; Computer Aided Diagnosis(CAD); FreeSurfer; Mild cognitive impairments; Mild cognitive impairments (MCI); Multi-class classification; Normal controls; SVM(support vector machine); Computer aided diagnosis",2-s2.0-85030162236
"Tüysüzoğlu G., Yaslan Y.","Sparse coding based classifier ensembles in supervised and active learning scenarios for data classification",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029431633&doi=10.1016%2fj.eswa.2017.09.024&partnerID=40&md5=7018248bd43b36b1dbe6f22feb0b2ffa","Sparse coding and dictionary learning has recently gained great interest in signal, image and audio processing applications through representing each problem instance by a sparse set of atoms. This also allows us to obtain different representations of feature sets in machine learning problems. Thus, different feature views for classifier ensembles can be obtained using sparse coding. On the other hand, nowadays unlabelled data is abundant and active learning methods with single and classifier ensembles received great interest. In this study, Random Subspace Dictionary Learning (RDL) and Bagging Dictionary Learning (BDL) algorithms are examined by learning ensembles of dictionaries through feature/instance subspaces. Besides, ensembles of dictionaries are evaluated under active learning framework as promising models and they are named as Active Random Subspace Dictionary Learning (ARDL) and Active Bagging Dictionary Learning (ABDL) algorithms. Active learning methods are compared with their Support Vector Machines counterparts. The experiments on eleven datasets from UCI and OpenML repositories has shown that selecting instance and feature subspaces for dictionary learning model increases the number of correctly classified instances for the most of the data sets while SVM has superiority over all of the applied models. Furthermore, using an active learner generally increases the chance of improved classification performance as the number of iterations is increased. © 2017 Elsevier Ltd","Active learning; Bagging; Dictionary learning; Ensemble classifiers; Random subspace feature selection","Artificial intelligence; Audio signal processing; Codes (symbols); Image coding; Learning algorithms; Learning systems; Support vector machines; Vectors; Active Learning; Bagging; Dictionary learning; Ensemble classifiers; Random subspaces; Classification (of information)",2-s2.0-85029431633
"Tang X., Zeng W., Shi Y., Zhao L.","Brain activation detection by modified neighborhood one-class SVM on fMRI data",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028690849&doi=10.1016%2fj.bspc.2017.08.021&partnerID=40&md5=8ec67bc3c8a05a644b69f25bc3579626","The one-class support vector machine (OC-SVM) is a data-driven machine learning method that has been applied as a novel technique for brain activation detection. Several researchers have obtained positive preliminary results using OC-SVMs. Nevertheless, existing algorithms are either too complicated or oversimplified and their performance needs to be further improved. In this study, a modified neighborhood one-class support vector machine (MNOC-SVM) algorithm is proposed to detect brain functional activation on functional magnetic resonance imaging (fMRI) data. This method is based on two basic assumptions: (a) For task-related fMRI data, time series of only a few voxels are related to a particular functional activity or functional area, and these voxels should be identified as activated voxels, i.e., the outliers. In contrast, for resting-state fMRI data, only a small number of voxels are unrelated to any resting-state functional networks. These voxels should instead be taken as non-activated voxels, i.e., the outliers. (b) Close voxels have similarly activated or non-activated states. To improve detection accuracy, we apply the following features to each voxel: the RV coefficient between each voxel and its 26 neighborhood voxels (or fewer than 26 for voxels on the edge of the brain), a flag for isolated voxels and a flag for isolated areas. For both task-related and resting-state fMRI data, our MNOC-SVM method effectively detects activated functional areas in the whole brain. © 2017","Brain activation detection; fMRI data; OC-SVM; Regional homogeneity","Chemical activation; Learning systems; Magnetic resonance imaging; Statistics; Support vector machines; Brain activation; fMRI data; Functional activation; Functional magnetic resonance imaging; Machine learning methods; OC-SVM; One-class support vector machine; Regional homogeneity; Functional neuroimaging; algorithm; Article; clinical assessment; clinical effectiveness; comparative study; controlled study; electroencephalogram; female; functional connectivity; functional magnetic resonance imaging; human; male; neighborhood; neuroimaging; nuclear magnetic resonance scanner; priority journal; resting state network; support vector machine; time series analysis; visual stimulation",2-s2.0-85028690849
"Matla S., Subban R.","Objects detection and tracking strategies",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031433277&doi=10.1007%2f978-981-10-6614-6_17&partnerID=40&md5=12790f0517438ba57fbb431370c3e2f6","Object detection and tracking play a vital role in several applications like face detection, gait detection, vehicle detection and pose detection, and object recognition. The first step in the object detection algorithm is detecting the presence of any object in video images. The process of the object detection is accomplished using Hidden Markov Models, Support Vector Machines, Machine Learning Techniques, Pattern Recognition, Statistical Method, Scale Invariant Feature, structured visual dictionary (SVD), AdaBoost, Clustering Method, Bayesian Framework, Particle Filtering, Vector Quantization, and Feature Extraction, etc. This paper presents a comprehensive study and analysis on object detection and tracking techniques. © 2018, Springer Nature Singapore Pte Ltd.","AdaBoost; Machine learning techniques; Scale invariant feature; Statistical method; Support vector machines","Adaptive boosting; Artificial intelligence; Face recognition; Feature extraction; Gesture recognition; Hidden Markov models; Image processing; Learning algorithms; Learning systems; Markov processes; Object recognition; Pattern recognition; Statistical methods; Support vector machines; Bayesian frameworks; Clustering methods; Machine learning techniques; Object detection algorithms; Object detection and tracking; Particle Filtering; Scale invariant features; Visual dictionaries; Object detection",2-s2.0-85031433277
"Rahab H., Zitouni A., Djoudi M.","SIAAC: Sentiment Polarity Identification on Arabic Algerian Newspaper Comments",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029574192&doi=10.1007%2f978-3-319-67621-0_12&partnerID=40&md5=5aa1edec5b5b104f1ffeb4f527d6d716","It is a challenging task to identify sentiment polarity in Arabic journals comments. Algerian daily newspapers interest more and more people in Algeria, and due to this fact they interact with it by comments they post on articles in their websites. In this paper we propose our approach to classify Arabic comments from Algerian Newspapers into positive and negative classes. Publicly-available Arabic datasets are very rare on the Web, which make it very hard to carring out studies in Arabic sentiment analysis. To reduce this gap we have created SIAAC (Sentiment polarity Identification on Arabic Algerian newspaper Comments) a corpus dedicated for this work. Comments are collected from website of well-known Algerian newspaper Echorouk. For experiments two well known supervised learning classifiers Support Vector Machines (SVM) and Naïve Bayes (NB) were used, with a set of different parameters for each one. Recall, Precision and F_measure are computed for each classifier. Best results are obtained in term of precision in both SVM and NB, also the use of bigram increase the results in the two models. Compared with OCA, a well know corpus for Arabic, SIAAC give a competitive results. Obtained results encourage us to continue with others Algerian newspaper to generalize our model. © 2018, Springer International Publishing AG.","Arabic comments; Machine learning; Natural Language Processing; Naïve Bayes; Newspaper; Opinion mining; Sentiment analysis; Support Vector Machines","Artificial intelligence; Computational methods; Data mining; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Newsprint; Sodium; Support vector machines; Websites; Algeria; Arabic comments; Daily newspapers; Learning classifiers; Newspaper; Opinion mining; Sentiment analysis; Taxonomies",2-s2.0-85029574192
"Altendorf E., Schütz R., Canpolat Y., Weßel G., Flemisch F.","A study on the human and the automation in automated driving: Getting to know each other",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022346691&doi=10.1007%2f978-3-319-60441-1_65&partnerID=40&md5=2daf1dcb12a541c8472cc4933fb6b33c","In recent years, advanced driver assistant systems (ADAS) and solutions for automated driving have been introduced by several automotive original equipment manufacturers (OEMs) and suppliers. Currently, these types of automation are designed for partially automated driving, but the step towards higher levels of automation can be expected to be made soon. One of the most commonly addressed use cases is driving on a highway such as the German Autobahn. In this paper, we propose an approach for adapting the automation’s behavior to the human’s driving preferences, providing a cognitive automation system with a machine-learning algorithm. This system has been implemented in a simulator for automated driving and has been used in a study addressing conditional automation. Within the presented experiment, typical situations for automated driving under varying conditions have been tested in the driving simulator. During cooperative human-machine driving, the automation can learn the human’s preferences regarding relevant driving states. © Springer International Publishing AG 2018.","Automated driving; Cooperative automation; Cooperative driving; Cooperative guidance and control; Driver-vehicle interaction; Human-systems integration; Humanmachine systems; Machine learning; Neural networks","Automation; Automobile drivers; Automobile manufacture; Behavioral research; Education; Human computer interaction; Human engineering; Learning algorithms; Learning systems; Man machine systems; Neural networks; Automated driving; Cooperative driving; Guidance and control; Human systems integration; Vehicle interactions; Advanced driver assistance systems",2-s2.0-85022346691
"Razzaghnoori M., Sajedi H., Jazani I.K.","Question classification in Persian using word vectors and frequencies",2018,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026525620&doi=10.1016%2fj.cogsys.2017.07.002&partnerID=40&md5=9d75b9df4b05758867cca261ae1f79cd","The necessity of the existence of Question Answering (QA) systems becomes evident by considering the fact that the enormous amount of unstructured data created by humans nowadays, results in ineffectiveness of search engines to provide the exact solution for a given question. However, an outstanding question answering system requires an outstanding Question Classification (QC) system. Question classifier is a system that assigns a label to each question. There exist different ways of solving this problem such as rule-based, machine learning, and hybrid approaches. This paper provides a better solution for QC using machine-learning approaches. Three methods of feature extraction are proposed in this paper. The First method uses clustering algorithms to partition vocabulary into clusters and acquires feature vector corresponding to each question using clustering information. The second one suggests a method of extracting features from questions to dispose of using recurrent neural networks and to use feedforward neural networks, which have the advantage of learning faster and less need for data, instead. Each question is converted to a feature vector, which is obtained by the Word2vec method and weighted by tf-idf coefficients. The results of question classification using Support Vector Machine and Neural Network classifiers indicate the effectiveness of this type of feature vector and based on that, high performance of the proposed QC system. Finally, the third approach keeps the innovation behind first approach, but it also keeps the fact that we are dealing with a sequence based type of data into consideration. Eventually, it would be concluded that even with a limited amount of data it is reasonable to take Recurrent Neural Networks into consideration. © 2017 Elsevier B.V.","Feedforward neural networks; LSTM; Question classification; Recurrent Neural Networks (RNN); Tf-idf; Word2vec","Artificial intelligence; Clustering algorithms; Feedforward neural networks; Image retrieval; Learning systems; Search engines; Vectors; LSTM; Question classification; Recurrent neural network (RNN); Tf-idf; Word2vec; Recurrent neural networks; algorithm; Article; artificial neural network; classifier; controlled study; information processing; learning; lingua franca; linguistics; machine learning; natural language processing; Persian (language); priority journal; question classification system; search engine; support vector machine",2-s2.0-85026525620
"Alickovic E., Kevric J., Subasi A.","Performance evaluation of empirical mode decomposition, discrete wavelet transform, and wavelet packed decomposition for automated epileptic seizure detection and prediction",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026834393&doi=10.1016%2fj.bspc.2017.07.022&partnerID=40&md5=00652ad436efb2a2bbb53fb4893e03f9","This study proposes a new model which is fully specified for automated seizure onset detection and seizure onset prediction based on electroencephalography (EEG) measurements. We processed two archetypal EEG databases, Freiburg (intracranial EEG) and CHB-MIT (scalp EEG), to find if our model could outperform the state-of-the art models. Four key components define our model: (1) multiscale principal component analysis for EEG de-noising, (2) EEG signal decomposition using either empirical mode decomposition, discrete wavelet transform or wavelet packet decomposition, (3) statistical measures to extract relevant features, (4) machine learning algorithms. Our model achieved overall accuracy of 100% in ictal vs. inter-ictal EEG for both databases. In seizure onset prediction, it could discriminate between inter-ictal, pre-ictal, and ictal EEG with the accuracy of 99.77%, and between inter-ictal and pre-ictal EEG states with the accuracy of 99.70%. The proposed model is general and should prove applicable to other classification tasks including detection and prediction regarding bio-signals such as EMG and ECG. © 2017 Elsevier Ltd","Discrete wavelet transform (DWT); Electroencephalography (EEG); Empirical mode decomposition (EMD); Epilepsy; Multiscale PCA (MSPCA); Seizure detection and prediction; Wavelet packet decomposition (WPD)","Discrete wavelet transforms; Electroencephalography; Electrophysiology; Forecasting; Learning algorithms; Learning systems; Machine components; Principal component analysis; Wavelet analysis; Wavelet decomposition; Wavelet transforms; Empirical Mode Decomposition; Epilepsy; Multiscale PCA (MSPCA); Seizure detection; Wavelet packet decompositions; Signal processing; algorithm; Article; clinical article; electroencephalography; human; machine learning; measurement accuracy; principal component analysis; priority journal; seizure; wavelet analysis",2-s2.0-85026834393
"Fu Y., Aldrich C.","Froth image analysis by use of transfer learning and convolutional neural networks",2018,"Minerals Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032011905&doi=10.1016%2fj.mineng.2017.10.005&partnerID=40&md5=f8e4a27405cff241bd7acf471a937e33","Deep learning constitutes a significant recent advance in machine learning and has been particularly successful in applications related to image processing, where it can already surpass human accuracy in some cases. In this paper, the use of a convolutional neural network, AlexNet, pretrained on a database of images of common objects was used as is to extract features from flotation froth images. These features could subsequently be used to predict the conditions or performance of the flotation systems. Two case studies are considered. In the first, froth regimes in an industrial flotation plant could be identified significantly more reliably with the features generated by AlexNet than with previous state-of-the-art approaches, such as wavelets, grey level co-occurrence matrices or local binary patterns. In the second case study, the arsenic concentration in the batch flotation of realgar-orpiment-quartz mixtures could be predicted more accurately than was possible with features extracted by wavelets, grey level co-occurrence matrices, local binary patterns or by use of colour. These results suggest that feature extraction with convolutional neural networks trained on complex data sets from other domains can serve as more reliable methods than previous state-of-the-art approaches to froth image analysis. © 2017 Elsevier Ltd","AlexNet; Convolution Neural Networks; Deep learning; Froth flotation; Image analysis; Machine vision","Binary mixtures; Bins; Computer vision; Convolution; Deep learning; Froth flotation; Image processing; Learning systems; Neural networks; AlexNet; Arsenic concentration; Co-occurrence-matrix; Convolution neural network; Convolutional neural network; Local binary patterns; State-of-the-art approach; Transfer learning; Image analysis",2-s2.0-85032011905
"Ma T., Yu Y., Wang F., Zhang Q., Chen X.","A hybrid methodologies for intrusion detection based deep neural network with support vector machine and clustering technique",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031406166&doi=10.1007%2f978-981-10-3187-8_13&partnerID=40&md5=a59dc2c96097dbc8ec5184578c21c5a8","This paper proposes a novel approach called KDSVM, which utilized the k-mean techniques and advantage of feature learning with deep neural network (DNN) model and strong classifier of support vector machines (SVM), to detection intrusion networks. KDSVM is composed of two stages. In the first step, the dataset is divided into k subset based on every sample distance by the cluster centers of k-means approach, and in the second step, testing dataset is distanced by the same cluster center and fed into the DNN model with SVM model for intrusion detection. The experimental results show that the KDSVM not only performs better than SVM, BPNN, DBN-SVM (Salama et al., Soft computing in industrial applications, 2011 [21]) and Bayes tree models in terms of detection accuracy and abnormal types of attacks found. It also provides an effective tool for the study and analysis of intrusion detection in the large network. © Springer Nature Singapore Pte Ltd. 2018.","Deep neural network; Hybrid model; Intrusion detection systems; K-means clustering; Support vector machine","Computation theory; Deep neural networks; Mercury (metal); Soft computing; Statistical tests; Support vector machines; Clustering techniques; Detection accuracy; Feature learning; Hybrid methodologies; Hybrid model; Intrusion Detection Systems; K-means clustering; Strong classifiers; Intrusion detection",2-s2.0-85031406166
"Muggler M., Eshwarappav R., Cankaya E.C.","Cybersecurity management through logging analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021645326&doi=10.1007%2f978-3-319-60585-2_1&partnerID=40&md5=6724cfb13880cf9c39ad46344e1bbf83","To make cybersecurity efforts proactive rather than solely reactive, this work proposes using machine learning to process large network related data: We collect various performance metrics in a network and use machine learning techniques to identify anomalous behavior. We introduce the novel idea of using weighted trust to prevent corruption of classifiers. Our design combines all aspects of a log management system into one distributed application for a data center to effectively offer logging, aggregation, monitoring and intelligence services. For this, we employ a three-component log management system: (1) to actively extract metrics from machines, (2) to aggregate and analyze extracted metrics to detect anomalous behavior, and (3) to allow reviewing collected metrics and to report on anomalous behavior observed. Our system runs at network and application layers and is concerned with risk mitigation and assessment. Several machine learning techniques are compared w.r.t. their classification, as well as detection performances. © Springer International Publishing AG 2018.","Anomaly detection; Cybersecurity; Log management system","Artificial intelligence; Education; Human engineering; Learning algorithms; Learning systems; Network layers; Risk assessment; Anomaly detection; Cyber security; Detection performance; Distributed applications; Intelligence services; Log managements; Machine learning techniques; Performance metrics; Information management",2-s2.0-85021645326
"Tang J., Liu F., Zhang W., Ke R., Zou Y.","Lane-changes prediction based on adaptive fuzzy neural network",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029525192&doi=10.1016%2fj.eswa.2017.09.025&partnerID=40&md5=d1d6bceb4121e19ad32c5fa259cd4b0e","Lane changing maneuver is one of the most important driving behaviors. Unreasonable lane changes can cause serious collisions and consequent traffic delays. High precision prediction of lane changing intent is helpful for improving driving safety. In this study, by fusing information from vehicle sensors, a lane changing predictor based on Adaptive Fuzzy Neural Network (AFFN) is proposed to predict steering angles. The prediction model includes two parts: fuzzy neural network based on Takagi–Sugeno fuzzy inference, in which an improved Least Squares Estimator (LSE) is adopt to optimize parameters; adaptive learning algorithm to update membership functions and rule base. Experiments are conducted in the driving simulator under scenarios with different speed levels of lead vehicle: 60 km/h, 80 km/h and 100 km/h. Prediction results show that the proposed method is able to accurately follow steering angle patterns. Furthermore, comparison of prediction performance with several machine learning methods further verifies the learning ability of the AFNN. Finally, a sensibility analysis indicates heading angles and acceleration of vehicle are also important factors for predicting lane changing behavior. © 2017 Elsevier Ltd","Adaptive learning algorithm; Driving simulation; Fuzzy neural network; Lane changes; Steering prediction","Automobile steering equipment; Forecasting; Fuzzy logic; Fuzzy neural networks; Inference engines; Learning algorithms; Learning systems; Membership functions; Steering; Vehicles; Adaptive fuzzy neural network; Adaptive learning algorithm; Driving simulation; Lane change; Lane-changing behaviors; Least-squares estimator; Machine learning methods; Prediction performance; Fuzzy inference",2-s2.0-85029525192
"Kumar V., Pujari A.K., Padmanabhan V., Sahu S.K., Kagita V.R.","Multi-label classification using hierarchical embedding",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029144279&doi=10.1016%2fj.eswa.2017.09.020&partnerID=40&md5=1c9b9d7f0cdc3e133f9054e6d0dafcef","Multi-label learning is concerned with the classification of data with multiple class labels. This is in contrast to the traditional classification problem where every data instance has a single label. Multi-label classification (MLC) is a major research area in the machine learning community and finds application in several domains such as computer vision, data mining and text classification. Due to the exponential size of the output space, exploiting intrinsic information in feature and label spaces has been the major thrust of research in recent years and use of parametrization and embedding have been the prime focus in MLC. Most of the existing methods learn a single linear parametrization using the entire training set and hence, fail to capture nonlinear intrinsic information in feature and label spaces. To overcome this, we propose a piecewise-linear embedding which uses maximum margin matrix factorization to model linear parametrization. We hypothesize that feature vectors which conform to similar embedding are similar in some sense. Combining the above concepts, we propose a novel hierarchical matrix factorization method for multi-label classification. Practical multi-label classification problems such as image annotation, text categorization and sentiment analysis can be directly solved by the proposed method. We compare our method with six well-known algorithms on twelve benchmark datasets. Our experimental analysis manifests the superiority of our proposed method over state-of-art algorithm for multi-label learning. © 2017 Elsevier Ltd","Label correlation; Matrix factorization; Multi-label learning","Data mining; Factorization; Learning algorithms; Learning systems; Matrix algebra; Piecewise linear techniques; Text processing; Classification of data; Hierarchical matrix factorizations; Label correlations; Linear parametrization; Machine learning communities; Matrix factorizations; Multi label classification; Multi-label learning; Classification (of information)",2-s2.0-85029144279
"Xu J., Huang Z., Shi M., Jiang M.","Emotion Detection in E-learning Using Expectation-Maximization Deep Spatial-Temporal Inference Network",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029586207&doi=10.1007%2f978-3-319-66939-7_21&partnerID=40&md5=0a6c93c0c91cc8c2c56965b75dd1db02","It is very useful for the E-learning systems to detect the students emotional state accurately, and this can remind the teacher in time to change the teaching rhythm or content to meet the student’s emotional changes for making the teaching effect optimization. In this paper, we propose an emotion detection method based on a deep learning approach, Expectation-maximization Deep Spatial-Temporal Inference Network (EM-DeSTIN). This method takes the student’s facial expression as input and combine with Support Vector Machine (SVM) to implement emotion classification and identification. Experimental results show that the proposed method improves the performance of detecting emotion in a noisy environment compared with other methods. © 2018, Springer International Publishing AG.","Deep learning; E-learning; Emotion detection","Artificial intelligence; Deep learning; E-learning; Maximum principle; Students; Support vector machines; Teaching; Emotion classification; Emotion detection; Emotional change; Expectation - maximizations; Facial Expressions; Learning approach; Noisy environment; Spatial temporals; Education",2-s2.0-85029586207
"Athertya J.S., Saravana Kumar G.","Sensitivity analysis on effect of biomechanical factors for classifying vertebral deformities",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028590997&doi=10.1007%2f978-3-319-60618-7_2&partnerID=40&md5=5c876bec875f3cfd0d8cc30c693c5a7d","Classification of degenerations prevalent in human population is considered to be a crucial task which is performed by a physician or the radiologist. With numerous data being generated and innumerable features getting extracted, identification of normal and pathological case becomes a daunting process. Data learning techniques provide valuable resources in automating the entire procedure easing the burden on the consultant physician. However, since the inception of various machine learning techniques, feasible solution at the cost of computational expense needs to be evaluated. Factors considered for classification play a significant role in defining the accuracy of a system. The current study aims at demonstrating the trade off achieved at the expense of accuracy amongst the number of features and instances. In this article, vertebral column dataset from UCI repository is used for training and testing. Effect of various data pre-processing techniques are presented alongside an extensive study on feature selection method. For validation, breast tissue dataset from the former repository is considered and analyzed. © Springer International Publishing AG 2018.",,"Data handling; Economic and social effects; Learning algorithms; Learning systems; Sensitivity analysis; Soft computing; Statistical tests; Biomechanical factors; Computational expense; Data preprocessing; Feature selection methods; Learning techniques; Machine learning techniques; Pathological case; Training and testing; Pattern recognition",2-s2.0-85028590997
"Rosellini A.J., Dussaillant F., Zubizarreta J.R., Kessler R.C., Rose S.","Predicting posttraumatic stress disorder following a natural disaster",2018,"Journal of Psychiatric Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029673084&doi=10.1016%2fj.jpsychires.2017.09.010&partnerID=40&md5=0eb6c6cb33c9c9b9bf27ffaf6d3abcad","Earthquakes are a common and deadly natural disaster, with roughly one-quarter of survivors subsequently developing posttraumatic stress disorder (PTSD). Despite progress identifying risk factors, limited research has examined how to combine variables into an optimized post-earthquake PTSD prediction tool that could be used to triage survivors to mental health services. The current study developed a post-earthquake PTSD risk score using machine learning methods designed to optimize prediction. The data were from a two-wave survey of Chileans exposed to the 8.8 magnitude earthquake that occurred in February 2010. Respondents (n = 23,907) were interviewed roughly three months prior to and again three months after the earthquake. Probable post-earthquake PTSD was assessed using the Davidson Trauma Scale. We applied super learning, an ensembling machine learning method, to develop the PTSD risk score from 67 risk factors that could be assessed within one week of earthquake occurrence. The super learner algorithm had better cross-validated performance than the 39 individual algorithms from which it was developed, including conventional logistic regression. The super learner also had a better area under the receiver operating characteristic curve (0.79) than existing post-disaster PTSD risk tools. Individuals in the top 5%, 10%, and 20% of the predicted risk distribution accounted for 17.5%, 32.2%, and 51.4% of all probable cases of PTSD, respectively. In addition to developing a risk score that could be implemented in the near future, these results more broadly support the utility of super learning to develop optimized prediction functions for mental health outcomes. © 2017 Elsevier Ltd","Earthquake; Machine learning; Natural disaster; Posttraumatic stress disorder; Risk score","Article; Bayesian learning; Chile; Chilean; Davidson Trauma Scale; decision tree; diagnostic accuracy; diagnostic test accuracy study; earthquake; evaluation study; high risk population; human; independent variable; intermethod comparison; logistic regression analysis; machine learning; major clinical study; natural disaster; posttraumatic stress disorder; predictive value; priority journal; psychotrauma assessment; random forest; receiver operating characteristic; risk assessment; risk factor; structured interview; support vector machine",2-s2.0-85029673084
"Guda V., Sanampudi S.K.","A hybrid method for extraction of events from natural language text",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021236450&doi=10.1007%2f978-981-10-3223-3_28&partnerID=40&md5=8dea5587d7b83a08294c7699a580e8d2","Events extraction is a significant and interesting task in the field of Natural Language Processing (NLP). Basically events are the dynamic occurrences, specific happenings, causes or things. An event plays a vital role in narrative of text and also important for many NLP applications. This paper presents a Hybrid/Composite way of events extraction from natural language text. Earlier work of events extractions were developed with rule based approach or machine learning methods. The Proposed hybrid makes use of both machine learning approaches and hand coded rules to extract the events. Experiments were conducted on SemEval-2010 data set, the results obtained shown better precision and recall when compared with the existing methods. © Springer Nature Singapore Pte Ltd. 2018.","Events; Events extraction; Machine learning techniques; Natural⋅ Language Processing (NLP); Rules based approach","Artificial intelligence; Education; Extraction; Information analysis; Intelligent computing; Learning algorithms; Learning systems; Events; Events extractions; Language processing; Machine learning techniques; Rules based; Natural language processing systems",2-s2.0-85021236450
"Wani M.A., Jabin S.","Big data: Issues, challenges, and techniques in business intelligence",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031400676&doi=10.1007%2f978-981-10-6620-7_59&partnerID=40&md5=96bfe3efb905fe4ca4bf79b1efc27f0d","During the last decade, the most challenging problem the world envisaged was big data problem. The big data problem means that data is growing at a much faster rate than computational speeds. And it is the result of the fact that storage cost is getting cheaper day by day, so people as well as almost all business or scientific organizations are storing more and more data. Social activities, scientific experiments, biological explorations along with the sensor devices are great big data contributors. Big data is beneficial to the society and business but at the same time, it brings challenges to the scientific communities. The existing traditional tools, machine learning algorithms, and techniques are not capable of handling, managing, and analyzing big data, although various scalable machine learning algorithms, techniques, and tools (e.g., Hadoop and Apache Spark open source platforms) are prevalent. In this paper, we have identified the most pertinent issues and challenges related to big data and point out a comprehensive comparison of various techniques for handling big data problem. © 2018, Springer Nature Singapore Pte Ltd.","Apache Spark; Big data; Big data analytics; Business intelligence; Hadoop MapReduce; Online social networks","Artificial intelligence; Competitive intelligence; Data handling; Digital storage; Information analysis; Learning algorithms; Learning systems; Social networking (online); Comprehensive comparisons; Data analytics; Hadoop MapReduce; Issues and challenges; On-line social networks; Open source platforms; Scalable machine learning; Scientific experiments; Big data",2-s2.0-85031400676
"Amasyali K., El-Gohary N.M.","A review of data-driven building energy consumption prediction studies",2018,"Renewable and Sustainable Energy Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029080224&doi=10.1016%2fj.rser.2017.04.095&partnerID=40&md5=f0c2e1a87e727aed8fc24f8a10e07baf","Energy is the lifeblood of modern societies. In the past decades, the world's energy consumption and associated CO2 emissions increased rapidly due to the increases in population and comfort demands of people. Building energy consumption prediction is essential for energy planning, management, and conservation. Data-driven models provide a practical approach to energy consumption prediction. This paper offers a review of the studies that developed data-driven building energy consumption prediction models, with a particular focus on reviewing the scopes of prediction, the data properties and the data preprocessing methods used, the machine learning algorithms utilized for prediction, and the performance measures used for evaluation. Based on this review, existing research gaps are identified and future research directions in the area of data-driven building energy consumption prediction are highlighted. © 2017 Elsevier Ltd","Building energy; Data-driven methods; Energy consumption prediction; Machine learning","Artificial intelligence; Buildings; Energy conservation; Forecasting; Historic preservation; Learning algorithms; Learning systems; Building energy; Building energy consumption prediction; Data preprocessing; Data-driven methods; Data-driven model; Energy consumption prediction; Future research directions; Performance measure; Energy utilization",2-s2.0-85029080224
"Suryanarayana D., Kanakam P., Hussain S.M., Gupta S.","High-performance linguistics scheme for cognitive information processing",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026736387&doi=10.1007%2f978-981-10-3373-5_37&partnerID=40&md5=61f7a986bf3de013f9fbc34353d7a58a","Natural language understanding is a principal segment of natural language processing in semantic analysis to the use of pragmatics to originate meaning from context. Information retrieval (IR) is one of the emerging areas to deal with enormous amounts of data, which are in the form of natural language. Content of the query posed will affect both volume of data and design of IR applications. This paper presents a cognition-applied methodology termed as High-Performance Linguistics (HPL), which is a question-answering system for interpreting a natural language sentence/query. It constitutes three phases of computations: parsing, triplet generation and triplet mapping/matching. The generation of the triplets for the knowledge base is to create new data and compare them with that of stored triplets in the database. Thus, the generation of the cognitive question-answering system can make easy using this machine learning techniques on the generated triplet database. © Springer Nature Singapore Pte Ltd. 2018.","Indexing; Information retrieval; Linguistics; Ontology; Pragmatics; RDF; Semantics; Triplets","Artificial intelligence; Computation theory; Indexing (of information); Information retrieval; Intelligent computing; Knowledge based systems; Learning algorithms; Learning systems; Linguistics; Ontology; Query processing; Semantic Web; Semantics; Syntactics; Cognitive information processing; Machine learning techniques; Natural language understanding; Natural languages; Pragmatics; Question answering systems; Semantic analysis; Triplets; Natural language processing systems",2-s2.0-85026736387
"Alswaitti M., Albughdadi M., Isa N.A.M.","Density-based particle swarm optimization algorithm for data clustering",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028849459&doi=10.1016%2fj.eswa.2017.08.050&partnerID=40&md5=ece85faa884d69ee929e1f99b492b1d4","Particle swarm optimization (PSO) algorithm is widely used in cluster analysis. However, it is a stochastic technique that is vulnerable to premature convergence to sub-optimal clustering solutions. PSO-based clustering algorithms also require tuning of the learning coefficient values to find better solutions. The latter drawbacks can be evaded by setting a proper balance between the exploitation and exploration behaviors of particles while searching the feature space. Moreover, particles must take into account the magnitude of movement in each dimension and search for the optimal solution in the most populated regions in the feature space. This study presents a novel approach for data clustering based on particle swarms. In this proposal, the balance between exploitation and exploration processes is considered using a combination of (i) kernel density estimation technique associated with new bandwidth estimation method to address the premature convergence and (ii) estimated multidimensional gravitational learning coefficients. The proposed algorithm is compared with other state-of-the-art algorithms using 11 benchmark datasets from the UCI Machine Learning Repository in terms of classification accuracy, repeatability represented by the standard deviation of the classification accuracy over different runs, and cluster compactness represented by the average Dunn index values over different runs. The results of Friedman Aligned-Ranks test with Holm's test over the average classification accuracy and Dunn index values indicate that the proposed algorithm achieves better accuracy and compactness when compared with other algorithms. The significance of the proposed algorithm is represented in addressing the limitations of the PSO-based clustering algorithms to push forward clustering as an important technique in the field of expert systems and machine learning. Such application, in turn, enhances the classification accuracy and cluster compactness. In this context, the proposed algorithm achieves better results compared with other state-of-the-art algorithms when applied to high-dimensional datasets (e.g., Landsat and Dermatology). This finding confirms the importance of estimating multidimensional learning coefficients that consider particle movements in all the dimensions of the feature space. The proposed algorithm can likewise be applied in repeatability matters for better decision making, as in medical diagnosis, as proved by the low standard deviation obtained using the proposed algorithm in conducted experiments. © 2017 Elsevier Ltd","Data clustering; Exploitation and exploration balance; Kernel density estimation; Particle swarm optimization; Swarm intelligence; Universal gravity rule","Artificial intelligence; Classification (of information); Cluster analysis; Decision making; Diagnosis; Estimation; Evolutionary algorithms; Expert systems; Learning algorithms; Learning systems; Optimization; Particle swarm optimization (PSO); Statistics; Stochastic systems; Swarm intelligence; Data clustering; Exploitation and explorations; High dimensional datasets; Kernel Density Estimation; Particle swarm optimization algorithm; State-of-the-art algorithms; UCI machine learning repository; Universal gravity rule; Clustering algorithms",2-s2.0-85028849459
"Makde V., Bhavsar J., Jain S., Sharma P.","Deep neural network based classification of tumourous and non-tumorous medical images",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028442098&doi=10.1007%2f978-3-319-63645-0_22&partnerID=40&md5=87f3558dc43678441700aca87044d3d9","Tumor identification and classification from various medical images is a very challenging task. Various image processing and pattern identification techniques can be used for tumor identification and classification process. Deep learning is evolving technique under machine learning that provides the advantage for automatically extracting the features from the images. The computer aided diagnosis system proposed in this research work can assist the radiologists in cancer tumor identification based on various facts and studies done previously. The system can expedite the process of identification even in earlier stages by adding up the facility of a second opinion which makes the process simpler and faster. In this paper, we have proposed a framework of convolution neural network (CNN), that is a technique under Deep Learning. The research work implements the framework on AlexNet and ZFNet architectures and have trained the system for tumor detection in lung nodules and well as brain. The accuracy for classification is more than 97% for both the architectures and both the datasets of lung CT and brain MRI images. © Springer International Publishing AG 2018.","AlexNet; Convolution neural network; Deep learning; Machine learning; Tumor identification and classification; ZFNet","Artificial intelligence; Biological organs; Classification (of information); Computer aided diagnosis; Computerized tomography; Convolution; Deep learning; Deep neural networks; Diagnosis; Image classification; Intelligent systems; Learning systems; Magnetic resonance imaging; Medical imaging; Network architecture; Tumors; AlexNet; Brain mri images; Classification process; Computer aided diagnosis systems; Convolution neural network; Pattern identification; Second opinions; ZFNet; Image processing",2-s2.0-85028442098
"Lee C.S., Cho S.B.","Learning classifier systems for adaptive learning of intrusion detection system",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028680808&doi=10.1007%2f978-3-319-67180-2_54&partnerID=40&md5=c0423f92b3d8090bc33934e9e4184701","Relational databases contain information that must be protected such as personal information, the problem of intrusion detection of relational database is considered important. Also, the pattern of attacks evolves and it is difficult to grasp by rule-based method or general machine learning, so adaptive learning is needed. Learning classifier systems are system that combines supervised learning, reinforcement learning and evolutionary computation. It creates and updates classifiers according to data input. Learning classifier systems can learn adaptive because they generate and evaluate classifiers in real time. In this paper, we apply accuracy based learning classifier systems to relational database and confirm that adaptive learning is possible. Also, we confirmed their practical usability that they close to the best accuracy, though were not the best. © 2018, Springer International Publishing AG.","Anomaly detection; Database; Learning classifier systems; SQL query","Artificial intelligence; Classification (of information); Database systems; Intrusion detection; Learning systems; Mercury (metal); Query processing; Real time systems; Reinforcement learning; Relational database systems; Soft computing; Adaptive learning; Anomaly detection; Intrusion Detection Systems; Learning classifier system; Personal information; Relational Database; Rule-based method; SQL query; Education",2-s2.0-85028680808
"Chaabani Y., Toujani R., Akaichi J.","Sentiment analysis method for tracking touristics reviews in social media network",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020455910&doi=10.1007%2f978-3-319-59480-4_30&partnerID=40&md5=84c3edabe6c576f90215bf2ac10758be","The touristic sector in Tunisia has declined after the “Arabic Spring”. Therefore, the number of comments published by tourists to give their opinions about it has increased. Consequently, this resulted in a high volume of data in the different social networks such as Facebook and Twitter. In this case, the opinion mining plays an important role to more understanding and then ameliorating the situation of tourism in Tunisia. In this paper, the main goal is to select the tourists’ viewpoints in Twitter after the revolution. For this reason, we create a sentiment lexicon based on the emoticons and interjections as well as acronyms. We also use a sentiWordnet to build lexical scales for sentiment analysis of different tourist reviews with reference to a travel agency page on Facebook. Then, we propose a method relying on Support Vector Machine (SVM), Maximum entropy and Naive Bayes. Our approach is efficient as it gives encouraging results. © Springer International Publishing AG 2018.","Machine learning; Medias networks; NP-Complete; Reviews; Sentiment analysis; Text mining","Data mining; Interactive computer systems; Learning systems; Multimedia services; Multimedia systems; Natural language processing systems; Reviews; Social networking (online); Support vector machines; High volumes; NP Complete; Opinion mining; Sentiment analysis; Sentiment lexicons; Social media networks; Text mining; Travel agency; Maximum entropy methods",2-s2.0-85020455910
"Natallia I., Zahar P., Vladimir K.","Art of recognition the electromyographic signals for control of the bionic artificial limb of the hand",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028634606&doi=10.1007%2f978-3-319-67349-3_16&partnerID=40&md5=8fc2e38f6a0c5e11a8eff4c6412f3d60","The LLC Bionic Natali company is a startup and has been engaging in creation of bionic artificial limbs of hands for more than 2 years. From the first steps, the project had been directed on the solution of a problem of development of the domestic bionic functional artificial limb of the hand based on neural network and others algorithms. In the project it had been created the functional system of management, system of tactile feedback which has increased controllability of a functional artificial limb is already realized and integrated, and also the functional bionic artificial limb of the hand. Based on this work it had been done the general representations and practical application of machine training, neural network and others algorithms. The technology of recognition of gestures of electromyographic activity based on neural network or an analog of network is the cornerstone. The bracelet is put on a hand (in case of disabled people, a stump), further noninvasive electrodes remove potential difference of neuromuscular activity; by means of an electric circuit there is data handling and their transmission to the processor where by means of a neural network there is a recognition of a gripper, further data are transferred for control of a bionic hand. Article belongs to the sections Lecture Notes in Artificial Intelligence (LNAI) and Lecture Notes in Bioinformatics (LNBI). © 2018, Springer International Publishing AG.","Artificial intelligence; Bionic Natali; Electromyographic signals; EMG; Machine learning; Neuronal net; Recognition the electromyographic signals; System of control; The bionic artificial limb","Artificial intelligence; Artificial limbs; Biomedical engineering; Biomedical signal processing; Bionics; Data handling; Education; Learning systems; Electromyographic activity; Electromyographic signal; Functional systems; Machine trainings; Neuromuscular activity; Neuronal net; Potential difference; Tactile feedback; Palmprint recognition",2-s2.0-85028634606
"Zielosko B.","Optimization of exact decision rules relative to length",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020405223&doi=10.1007%2f978-3-319-59421-7_14&partnerID=40&md5=6f82f71eb84ba9651de26157e9659167","In the paper, an idea of modified dynamic programming algorithm is used for optimization of exact decision rules relative to length. The aims of the paper are: (i) study a length of decision rules, and (ii) study a size of a directed acyclic graph (the number of nodes and edges). The paper contains experimental results with decision tables from UCI Machine Learning Repository and comparison with results for dynamic programming algorithm. © Springer International Publishing AG 2018.","Decision rules; Dynamic programming; Length; Optimization","Decision tables; Directed graphs; Graph theory; Learning systems; Optimization; Decision rules; Directed acyclic graph (DAG); Dynamic programming algorithm; Length; UCI machine learning repository; Dynamic programming",2-s2.0-85020405223
"Ghahabi O., Hernando J.","Restricted Boltzmann machines for vector representation of speech in speaker recognition",2018,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021886857&doi=10.1016%2fj.csl.2017.06.007&partnerID=40&md5=f391da3dd12a3cb1374b360aea51ac2e","Over the last few years, i-vectors have been the state-of-the-art technique in speaker recognition. Recent advances in Deep Learning (DL) technology have improved the quality of i-vectors but the DL techniques in use are computationally expensive and need phonetically labeled background data. The aim of this work is to develop an efficient alternative vector representation of speech by keeping the computational cost as low as possible and avoiding phonetic labels, which are not always accessible. The proposed vectors will be based on both Gaussian Mixture Models (GMM) and Restricted Boltzmann Machines (RBM) and will be referred to as GMM–RBM vectors. The role of RBM is to learn the total speaker and session variability among background GMM supervectors. This RBM, which will be referred to as Universal RBM (URBM), will then be used to transform unseen supervectors to the proposed low dimensional vectors. The use of different activation functions for training the URBM and different transformation functions for extracting the proposed vectors are investigated. At the end, a variant of Rectified Linear Units (ReLU) which is referred to as variable ReLU (VReLU) is proposed. Experiments on the core test condition 5 of NIST SRE 2010 show that comparable results with conventional i-vectors are achieved with a clearly lower computational load in the vector extraction process. © 2018 The Authors","Deep learning; GMM–RBM vector; i-vector; Restricted Boltzmann machine; Speaker recognition; Variable rectified linear unit","Deep learning; Education; Vectors; Gaussian Mixture Model; I vectors; Linear units; Restricted boltzmann machine; Speaker recognition; State-of-the-art techniques; Transformation functions; Vector representations; Speech recognition",2-s2.0-85021886857
"Crişan N., Andraş I., Coman I.","The role of technology in the implementation and learning of minimally-invasive surgery",2018,"Mechanisms and Machine Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028060171&doi=10.1007%2f978-3-319-59972-4_4&partnerID=40&md5=964aad19991afa7e9ad5992b14df807a","The assimilation of the surgical techniques by the resident doctors should not affect patients’ safety. Practicing certain surgical gestures in a repetitive manner allows a better understanding of the technique and the correct acquisition of the manual skills. The use of simulators as a part of the training programs has considerably reduced the number of surgical errors and has improved the operative time and the quality of robotic and laparoscopic surgical procedures. The latest technologies, like 3D vision, next generation instruments, the use of intraoperative imaging have enabled the development of minimally-invasive surgery, so that a number of laparoscopic and robotic procedures have become the standard of care. Our objective was to evaluate the manner in which the latest technologies influence the development of minimally invasive surgery (laparoscopic and robotic). Also, we assessed the main parameters that influence the learning curve of these two types of minimally invasive approach. We observed that the use of the robotic platform during the learning curve allows the performance of laparoscopic procedures with the same accuracy, but with much lower costs. © Springer International Publishing AG 2018.","Laparoscopic surgery; Learning curve; Robotic surgery; Surgical simulators","Laparoscopy; Machine design; Mobile robots; Personnel training; Robotic surgery; Robotics; Surgery; Transplantation (surgical); Intra-operative imaging; Laparoscopic procedures; Laparoscopic surgery; Learning curves; Minimally invasive surgery; Role of technologies; Surgical simulators; Surgical techniques; Surgical equipment",2-s2.0-85028060171
"Kim J., Lee H., Kang S.","A Trail Detection Using Convolutional Neural Network",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032477643&doi=10.1007%2f978-981-10-6520-0_30&partnerID=40&md5=0bd4c867d70fbfd18f6f562c048bbb64","Small-footprint airborne LiDAR scanning systems are effective in modelling forest structures and can also improve trail detection. We propose a trail detection method through a machine learning method from the LiDAR points. To do that, we analyze features for detecting a trail, digitize each feature and combine the results to distinguish between trail and non-trail areas. Our proposed method shows the feasibility of trail detection by using airborne LiDAR points gathered in dense mixed forest. © 2018, Springer Nature Singapore Pte Ltd.","Forest structure; LiDAR; Neural network; Trail detection; Trail feature","Forestry; Learning systems; Neural networks; Optical radar; Airborne LiDAR; Convolutional neural network; Detection methods; Forest structure; Machine learning methods; Mixed forests; Small footprints; Trail feature; Feature extraction",2-s2.0-85032477643
"Chen M., Lin L., Chen Q., Hu H., Zhang Q., Xu Y., Chen Y.-W.","Computerized features for LI-RADS based computer-aided diagnosis of liver lesions",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019687950&doi=10.1007%2f978-3-319-59397-5_16&partnerID=40&md5=8d839c97a75f72b05c36334befde763a","Liver Imaging Reporting Data System (LI-RADS) aims to standardize liver lesion imaging findings and diagnostic reports, and it is used as an accurate noninvasive diagnosis and staging method of hepatocellular carcinoma (HCC) nowadays. In this study, we proposed several computerized features for LI-RADS based computer-aided diagnosis of liver lesions. We used several popular machining learning approaches for computerized LI-RADS classification (benign and malignant classification) with our proposed features. The performance of each method was evaluated by using ROC curve and the best AUC score was 0.965 reached by the gradient boosting classifier. © Springer International Publishing AG 2018.","Classification; Computerized feature; LI-RADS; Machine learning; Malignancy","Classification (of information); Diagnosis; Health care; Learning systems; Computerized feature; Diagnostic Report; Gradient boosting; Hepatocellular carcinoma; Learning approach; Malignancy; Non-invasive diagnosis; Staging-method; Computer aided diagnosis",2-s2.0-85019687950
"Flath C.M., Stein N.","Towards a data science toolbox for industrial analytics applications",2018,"Computers in Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029686258&doi=10.1016%2fj.compind.2017.09.003&partnerID=40&md5=26b352d1c51e409dacc55194364f5426","Manufacturing companies today have access to a vast number of data sources providing gigantic amounts of process and status data. Consequently, the need for analytical information systems is ever-growing to guide corporate decision-making. However, decision-makers in production environments are still very much focused on static, explanatory modeling provided by business intelligence suites instead of embracing the opportunities offered by predictive analytics. We develop a data science toolbox for manufacturing prediction tasks to bridge the gap between machine learning research and concrete practical needs. We provide guidelines and best practices for modeling, feature engineering and interpretation. To this end, we leverage tools from business information systems as well as machine learning. We illustrate the usage of this toolbox by means of a real-world manufacturing defect prediction case study. Thereby, we seek to enhance the understanding of predictive modeling. In particular, we want to emphasize that simply dumping data into “smart” algorithms is not the silver bullet. Instead, constant refinement and consolidation are required to improve the predictive power of a business analytics solution. © 2017 Elsevier B.V.","Manufacturing; Predictive analytics; Process mining","Artificial intelligence; Decision making; Industrial research; Information systems; Learning systems; Manufacture; Business information systems; Feature engineerings; Machine learning research; Manufacturing companies; Manufacturing defects; Predictive modeling; Process mining; Production environments; Predictive analytics",2-s2.0-85029686258
"Gu K., Li H., Sun G.","Document security identification based on multi-classifier",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032693535&doi=10.1007%2f978-3-319-67071-3_18&partnerID=40&md5=43744c3d794ee639974a53ca6a6d3f1e","Data leakage is a potentially important issue for businesses. Numerous corporate offer data loss prevention (DLP) solutions to monitor information flow, and detect such leakage. Adding a secret label to a document, DLP can use documents label to do securely control, effectively protecting data. With the increasing documents every day, manual labeling is time-consuming. To better solve the difficult task, recently researchers need to start use document security identification by machine learning quickly classify a large number of texts. The contribution of this paper is to explore dimensionality reduction by feature selection and combine two models to avoid the process of weighting different type of features. In contrast to training all features with one algorithm, our experimental results demonstrate that the combination of two models can improve the classification performance. © 2018, Springer International Publishing AG.","Data leakage prevention; Document security identification; Feature selection; Machine learning; Model combination","Artificial intelligence; Learning systems; Classification performance; Data leakage preventions; Dimensionality reduction; Document security; Information flows; Manual labeling; Model combination; Multi-classifier; Feature extraction",2-s2.0-85032693535
"Neubrandt D., Buza K.","Projection-based person identification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019201041&doi=10.1007%2f978-3-319-59162-9_23&partnerID=40&md5=7d9c2c4daaeb07880786cd17180af128","The increasing interest in person identification based on keystroke dynamics can be attributed to several factors. First of all, it is a cheap and widely applicable technique, whereas online services such as internet banking or online tax declaration require reliable person identification methods. Furthermore, there are various attack techniques against the existing identification methods, thus combining the existing methods with new person identification methods could improve the reliability of the identification. Recent research shows that person identification based on machine learning using keystroke dynamics data works surprisingly well. This is because the dynamics of typing is characteristic to users and a user is hardly able to mimic the dynamics of typing of another user. In this paper, we propose to use a projection-based classification technique for the task of person identification based on keystroke dynamics. © Springer International Publishing AG 2018.","Keystroke dynamics; Machine learning; Person identification; PROCESS","Artificial intelligence; Learning systems; Online systems; Processing; Classification technique; Identification method; Internet banking; Keystroke dynamics; On-line service; Person identification; Recent researches; Various attacks; Dynamics",2-s2.0-85019201041
"Nishani L., Biba M.","Randomizing greedy ensemble outlier detection with GRASP",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026288922&doi=10.1007%2f978-3-319-61566-0_92&partnerID=40&md5=ce775056a89f41b025a0bc5907eedf57","Ensemble methods have been recently used in many applications of machine learning in different areas. In this context, outlier detection is an area where recently these methods have received increasing attention. This paper deals with randomization in ensemble methods for outlier detection. We have developed a novel algorithm exploiting stochastic local search heuristics to induce diversity in an ensemble outlier detection algorithm. We exploit the capability of the GRASP heuristic to induce diversity into the search process and to maintain a good balance of exploitation and diversification in building the ensemble. The conducted experiments show interesting improvements over the greedy ensemble method and open the path for novel research in this direction. © Springer International Publishing AG 2018.","Ensemble methods; GRASP; Machine learning; Outlier detection; Stochastic local search","Artificial intelligence; Education; Heuristic algorithms; Intelligent systems; Learning systems; Local search (optimization); Statistics; Stochastic systems; Ensemble methods; GRASP; In-buildings; Novel algorithm; Outlier Detection; Outlier detection algorithm; Search process; Stochastic local searches; Data handling",2-s2.0-85026288922
"Kunimasa S., Seo K., Shimoda H., Ishii H.","A trial of intellectual work performance estimation by using physiological indices",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021820544&doi=10.1007%2f978-3-319-60642-2_29&partnerID=40&md5=8255a365061986c636a3759057f0ed84","In order to evaluate the intellectual productivity quantitatively, most of conventional studies have utilized task performance of cognitive tasks. Meanwhile, more and more studies use physiological indices which reflect cognitive load so as to evaluate the intellectual productivity quantitatively. In this study, the method which estimates task performance of intellectual workers by using several physiological indices (pupil diameter and heart rate variability) has been proposed. As the estimation models of task performance, two machine learning models, Support Vector Regression (SVR) and Random Forests (RF), have been employed. As the result of a subject experiment, it was found that coefficient of determination (R2) of SVR was 0.875 and higher than that of RF (p &lt; 0.01). The result suggested that pupil diameter and heart rate variability were effective as the explanatory variables and SVR estimation was also effective in task performance estimation. © Springer International Publishing AG 2018.","Heart rate variability; Intellectual productivity; Machine learning; Physiological indices; Pupil diameter","Artificial intelligence; Decision trees; Education; Heart; Learning systems; Physiology; Productivity; Coefficient of determination; Explanatory variables; Heart rate variability; Intellectual works; Physiological indices; Pupil diameter; Subject experiment; Support vector regression (SVR); Psychophysiology",2-s2.0-85021820544
"Tusor B., Várkonyi-Kóczy A.R., Tóth J.T.","A fuzzy data structure for variable length data and missing value classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802680&doi=10.1007%2f978-3-319-67459-9_37&partnerID=40&md5=23f35515f80573da2824ed2c52be4759","Variable length data classification is an important field of machine learning. However, while there are plenty of classifiers in literature that can efficiently handle fixed length data, not many can also handle data with varying length samples. In this paper, a structure is proposed for quick and robust classification of such data, as well as data sets with occasionally missing values. It builds on the principle of look-up table classifiers, realizing a direct assignment between the attribute values of the given data samples and their corresponding classes. The proposed data structure solves this problem by decomposing the problem space into a sequence of integer value combinations, thus creating and maintaining a layered structure in the combined form of 1D and 2D arrays. Furthermore, a simple analysis regarding the data structure can reveal functional dependencies considering the attributes of the data set, offering an option to simplify the structure thus reduce its complexity. © Springer International Publishing AG 2018.","Classification; Data mining; Data structure; Machine learning; Missing data; Pattern recognition","Artificial intelligence; Autocorrelation; Data mining; Data structures; Education; Learning systems; Pattern recognition; Table lookup; Attribute values; Data classification; Functional dependency; Layered Structures; Missing data; Robust classification; Simple analysis; Variable length; Classification (of information)",2-s2.0-85029802680
"Radlak K., Radlak N., Smolka B.","Static posed versus genuine smile recognition",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019228860&doi=10.1007%2f978-3-319-59162-9_44&partnerID=40&md5=00511b48f592020ad6d3948e891c0774","Recognition of a posed or fake smile is a vital and challenging research topic and a growing interest has been observed from the computer vision and machine learning community. The state-of-the-art algorithms related to this field focus on the facial expressions dynamics, while several psychologists suggest that the main difference between posed and spontaneous smile should be observed in different muscles contractions in the upper part of the face. Therefore, in this work we evaluate the accuracy of recognition based only on the face appearance using the High-Dimensional Local Binary Patterns. The smile authenticity is analyzed on the set of images extracted at the smile apex phase from the UvA-NEMO database. The obtained results indicate that the analyzed algorithms can spot a fake smile much better than a human, but worse than systems that incorporate the facial dynamics. © Springer International Publishing AG 2018.","Face analysis; Facial expressions; Smile genuineness; Smile recognition","Learning systems; Face analysis; Facial Expressions; High-dimensional; Local binary patterns; Machine learning communities; Smile genuineness; Smile recognition; State-of-the-art algorithms; Computer vision",2-s2.0-85019228860
"Sakouhi T., Akaichi J., Ahmed U.","Computing semantic trajectories: Methods and used techniques",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020422740&doi=10.1007%2f978-3-319-59480-4_39&partnerID=40&md5=3d7b2f68b40155cfad87f658e69b94b9","The widespread use of mobile devices generates huge amount of location data. The generated data is useful for many applications, including location-based services such as outdoor sports forums, routine prediction, location-based activity recognition and location-based social networking. Sharing individuals’ trajectories and annotating them with activities, for example a tourist transportation mode during his trip, helps bringing more semantics to the GPS data. Indeed, this provides a better understanding of the user trajectories, and then more interesting location-based services. To address this issue, diverse range of novel techniques in the literature are explored to enrich this data with semantic information, notably, machine learning and statistical algorithms. In this work, we focused, at a first level, on exploring and classifying the literature works related to semantic trajectory computation. Secondly, we capitalized and discussed the benefits and limitations of each approach. © Springer International Publishing AG 2018.","Activity recognition; Data mining; Machine learning; Mobility data; Ontology; Semantic modeling; Trajectory","Artificial intelligence; Data mining; Interactive computer systems; Learning systems; Location; Mobile devices; Mobile telecommunication systems; Multimedia services; Multimedia systems; Ontology; Pattern recognition; Semantics; Telecommunication services; Trajectories; Activity recognition; Mobility datum; Routine prediction; Semantic information; Semantic Model; Semantic trajectories; Statistical algorithm; Transportation mode; Location based services",2-s2.0-85020422740
"Bagheri A., Bollen M.H.J., Gu I.Y.H.","Improved characterization of multi-stage voltage dips based on the space phasor model",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029393979&doi=10.1016%2fj.epsr.2017.09.004&partnerID=40&md5=f2e2e443ea056f47f5426f8c41d9bda4","This paper proposes a method for characterizing voltage dips based on the space phasor model of the three phase-to-neutral voltages, instead of the individual voltages. This has several advantages. Using a K-means clustering algorithm, a multi-stage dip is separated into its individual event segments directly instead of first detecting the transition segments. The logistic regression algorithm fits the best single-segment characteristics to every individual segment, instead of extreme values being used for this, as in earlier methods. The method is validated by applying it to synthetic and measured dips. It can be generalized for application to both single- and multi-stage dips. © 2017 Elsevier B.V.","Clustering algorithms; Electric power distribution; Electric power transmission; Logistic regression; Machine learning algorithms; Power quality; Voltage dips","Electric power distribution; Electric power transmission; Learning algorithms; Learning systems; Power quality; Regression analysis; Transients; Extreme value; K-Means clustering algorithm; Logistic regression algorithms; Logistic regressions; Neutral voltage; Phasor model; Single segments; Voltage dip; Clustering algorithms",2-s2.0-85029393979
"Li Z., Zhang X., Müller H., Zhang S.","Large-scale retrieval for medical image analytics: A comprehensive review",2018,"Medical Image Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765308&doi=10.1016%2fj.media.2017.09.007&partnerID=40&md5=335200bd8a11a7e0497b257b7a098d85","Over the past decades, medical image analytics was greatly facilitated by the explosion of digital imaging techniques, where huge amounts of medical images were produced with ever-increasing quality and diversity. However, conventional methods for analyzing medical images have achieved limited success, as they are not capable to tackle the huge amount of image data. In this paper, we review state-of-the-art approaches for large-scale medical image analysis, which are mainly based on recent advances in computer vision, machine learning and information retrieval. Specifically, we first present the general pipeline of large-scale retrieval, summarize the challenges/opportunities of medical image analytics on a large-scale. Then, we provide a comprehensive review of algorithms and techniques relevant to major processes in the pipeline, including feature representation, feature indexing, searching, etc. On the basis of existing work, we introduce the evaluation protocols and multiple applications of large-scale medical image retrieval, with a variety of exploratory and diagnostic scenarios. Finally, we discuss future directions of large-scale retrieval, which can further improve the performance of medical image analysis. © 2017 Elsevier B.V.","Computer aided diagnosis; Information retrieval; Large scale; Medical image analysis","Computer aided analysis; Computer aided diagnosis; Computer vision; Diagnosis; Image analysis; Image enhancement; Image retrieval; Imaging techniques; Information retrieval; Learning systems; Pipelines; Conventional methods; Digital imaging techniques; Evaluation protocol; Feature representation; Image data; Large scale; Multiple applications; State-of-the-art approach; Medical imaging; algorithm; compression; data extraction; hashing; histopathology; human; image analysis; image processing; image retrieval; image segmentation; machine learning; mammography; methodology; nerve cell; priority journal; Review",2-s2.0-85031765308
"Cermeño E., Pérez A., Sigüenza J.A.","Intelligent video surveillance beyond robust background modeling",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028862326&doi=10.1016%2fj.eswa.2017.08.052&partnerID=40&md5=d3de1f6860f14a8db7645813a588beb3","The increasing number of video surveillance cameras is challenging video control systems. Monitoring centers require tools to guide the process of supervision. Different video analysis methods have effectively met the main requirements from the industry of perimeter protection. High accuracy detection systems are able to process real time video on affordable hardware. However some problematic environments cause a massive number of false alerts. Many approaches in the literature do not consider this kind of environments while others use metrics that dilute their impact on results. An intelligent video solution for perimeter protection must select and show the cameras which are more likely witnessing a relevant event but systems based only on background modeling tend to give importance to problematic situations no matter if an intrusion is taking place or not. We propose to add a module based on machine learning and global features, bringing adaptability to the video surveillance solution, so that problematic situations can be recognized and given the right priority. Tests with thousands of hours of video show how good an intruder detector can perform but also how a simple fault in a camera can flood a monitoring center with alerts. The new proposal is able to learn and recognize events such that alerts from problematic environments can be properly handled. © 2017 Elsevier Ltd","Event; Global features; Intrusion detection; Machine learning; Recognition; Video; Video surveillance","Artificial intelligence; Cameras; Intrusion detection; Learning systems; Mercury (metal); Monitoring; Real time systems; Event; Global feature; Recognition; Video; Video surveillance; Security systems",2-s2.0-85028862326
"Mosca A., Magoulas G.D.","Distillation of deep learning ensembles as a regularisation method",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032328782&doi=10.1007%2f978-3-319-66790-4_6&partnerID=40&md5=d7aafa7d08e7aad96857969dafab7aa0","Ensemble methods are among the most commonly utilised algorithms that construct a group of models and combine their predictions to provide improved generalisation. They do so by aggregating multiple diverse versions of models learned using machine learning algorithms, and it is this diversity that enables the ensemble to perform better than any of its members taken individually. This approach can be extended to produce ensembles of deep learning methods that combine various good performing models, which are between them very diverse because they have reached different local minima and make different prediction errors. It has been shown that a large, cumbersome deep neural network can be approximated by a smaller network through a process of distillation, and that it is possible to approximate an ensemble of other learning algorithms by using a single neural network, with the help of additional artificially generated pseudo-data. We extend this work to show that an ensemble of deep neural networks can indeed be approximated by a single deep neural network with size and capacity equal to the single ensemble member, and we develop a recipe that shows how this can be achieved without using any artificial training data or any other special provisions, such as using the soft output targets during the distillation process. We also show that, under particular circumstances, the distillation process can be used as a form of regularisation, through its implicit reduction in learning capacity. We corroborate our findings with an experimental analysis on some common benchmark datasets in computer vision and deep learning. © 2018, Springer International Publishing AG.","Convolutional neural networks; Deep learning; Distillation; Ensembles","Artificial intelligence; Deep learning; Deep neural networks; Distillation; Learning systems; Neural networks; Artificial training; Benchmark datasets; Convolutional neural network; Distillation process; Ensembles; Experimental analysis; Learning capacity; Prediction errors; Learning algorithms",2-s2.0-85032328782
"Hatanaka Y., Samo K., Ogohara K., Sunayama W., Muramatsu C., Okumura S., Fujita H.","Automated blood vessel extraction based on high-order local autocorrelation features on retinal images",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350688&doi=10.1007%2f978-3-319-68195-5_87&partnerID=40&md5=acca0eae3473593a23a520a042ddea72","Automated blood vessels detection on retinal images is an important process in the development of pathologies analysis systems. This paper describes about an automated blood vessel extraction using high-order local autocorrelation (HLAC) on retinal images. Although HLAC features are shift-invariant, HLAC features are weak to turned image. Therefore, a method was improved by the addition of HLAC features to a polar transformed image. We have proposed a method using HLAC, pixel-based-features and three filters. However, we have not investigated about feature selection and machine learning method. Therefore, this paper discusses about effective features and machine learning method. We tested eight methods by extension of HLAC features, addition of 4 kinds of pixel-based features, difference of preprocessing techniques, and 3 kinds of machine learning methods. Machine learning methods are general artificial neural network (ANN), a network using two ANNs, and Boosting algorithm. As a result, our already proposed method was the best. When the method was tested by using “Digital Retinal Images for Vessel Extraction” (DRIVE) database, the area under the curve (AUC) based on receiver operating characteristics (ROC) analysis was reached to 0.960. © 2018, Springer International Publishing AG.","Blood vessel extraction; High-order local autocorrelation; Machine learning classifier; Segmentation",,2-s2.0-85032350688
"Zeyringer M., Fais B., Keppo I., Price J.","The potential of marine energy technologies in the UK – Evaluation from a systems perspective",2018,"Renewable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029784432&doi=10.1016%2fj.renene.2017.07.092&partnerID=40&md5=d5aaeae9bff1ed4fee38ee2b4c13d9fc","Accelerated technological change plays a crucial role in enabling the low-carbon energy transition. Quantitative energy modelling exploring alternative long-term decarbonisation pathways can support policy-makers in choosing the most important areas for technology promotion. This study analyses the potential contribution of marine energy in the UK from an energy systems perspective considering the trade-offs between local lead markets and global learning, the uncertainty in the learning potential, competition with alternative technologies and impacts on system balancing. The results indicate that only under very favourable conditions, i.e. with learning rates above 15% and high global deployment, marine energy will have a significant contribution to the UK decarbonisation pathway. Alternatively, marine energy could constitute a hedging strategy against multiple failure in other low-carbon options. The early strategic investments into marine energy lead, in most cases, to a slight rise in societal welfare costs compared to the respective cases without attempts to induce marine learning and brings benefits to the electricity system. Thus, on the whole, we conclude that marine energy has the potential to contribute to the UK energy system, but there is a substantial risk that strategic investments in a national lead market will not directly pay off in the long term. © 2017 The Author(s)","Energy innovation; Energy systems analysis; Marine energy; Sensitivity analysis; Technology learning","Commerce; Decarbonization; Economic and social effects; Sensitivity analysis; Systems analysis; Uncertainty analysis; Alternative technologies; Energy systems analysis; Low carbon energies; Marine energy; Strategic investments; Technological change; Technology learning; Technology promotions; Investments; alternative energy; analytical method; electricity; energy efficiency; energy market; energy resource; innovation; machine learning; marine resource; modeling; policy making; quantitative analysis; sensitivity analysis; technological change; trade-off; United Kingdom",2-s2.0-85029784432
"Chandran A., Anjali T., Mohan N., Soman K.P.","Overlapping group sparsity induced condition monitoring in rotating machineries",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028626140&doi=10.1007%2f978-3-319-60618-7_40&partnerID=40&md5=396bd5952f40c4c78b6614c93675f667","Maintenance of rotating parts in machines is not easy. Prediction of faults in advance reduces the frequency of breakdown and improves the life time of machines. This paper proposes a machine condition monitoring system, which formulates the fault diagnosis problem as a machine learning based pattern classification problem. The vibration signals acquired from rotating machines are initially processed by a group-sparse denoising algorithm namely Overlapping Group Shrinkage (OGS). In OGS, the group sparse signal denoising problem is casted as a convex optimization problem with a group sparsity promoting penalty function. The denoised signals are then processed by Variational Mode Decomposition (VMD), which decomposes the signal into specific frequency modes. For representing the signal in the feature space, energy of each mode is extracted and is classified by LS-SVM classifier. The performance of the proposed condition monitoring system is evaluated in terms of classification accuracies and is compared with statistical features. © Springer International Publishing AG 2018.","Feature extraction; Least square support vector machine (LS-SVM); Machine condition monitoring; Overlapping group shrinkage (OGS); Variational mode decomposition (VMD); Vibration analysis","Computer aided diagnosis; Convex optimization; Fault detection; Feature extraction; Learning systems; Machinery; Monitoring; Optimization; Pattern recognition; Rotating machinery; Shrinkage; Signal denoising; Signal processing; Soft computing; Support vector machines; Vibration analysis; Classification accuracy; Condition monitoring systems; Convex optimization problems; Least square support vector machines; Machine condition monitoring; Mode decomposition; Overlapping groups; Pattern classification problems; Condition monitoring",2-s2.0-85028626140
"Sankaranarayanan H.B., Rathod V.","A novel approach for predicting ancillaries ratings of indian low-cost airlines using clustering techniques",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028379639&doi=10.1007%2f978-3-319-63673-3_24&partnerID=40&md5=74f5401e534fab4257dc479260c94573","In this paper, we will present a novel approach for classifying and predicting airline passenger ratings for ancillaries using unsupervised learning techniques like K-Means and Expectation Maximization clustering. The datasets chosen for this study belong to Indian Low-Cost Airlines. The goal is to perform an empirical study and exploratory analysis for predicting the overall rating with respect to the individual ancillary services ratings. Our results suggest that while there is no clear pattern among the ratings that can lead to the overall rating from passengers, the factors like value for money can largely influence the overall rating. Low-cost airlines aggressively promote competitive fares and choice of ancillary services hence the passenger behavior towards the overall rating varies across the airline datasets. © 2018, Springer International Publishing AG.","Airlines; Clustering; India; LCC; Machine learning; Reviews","Civil aviation; Costs; Forecasting; Intelligent systems; Learning systems; Maximum principle; Rating; Reviews; Transportation; Airline passengers; Clustering; Clustering techniques; Expectation - maximizations; Exploratory analysis; India; Low-cost airlines; Passenger behavior; Air transportation",2-s2.0-85028379639
"LeMoyne R., Mastroianni T.","Homebound Therapy with Wearable and Wireless Systems",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032358727&doi=10.1007%2f978-981-10-5684-0_10&partnerID=40&md5=5e230d38dd1a207cf91da0ae9155a3fd","The context of smartphones and portable media devices as wearable and wireless systems can logically be extrapolated to homebound therapy, especially with regards to of a rehabilitation for hemiparesis from traumatic brain injury and stroke. Four applications are addressed. The portable media device operating as a functionally wireless accelerometer platform can be mounted to a cane for machine learning classification to distinguish appropriate and inappropriate use. An ankle rehabilitation system can apply a smartphone as a wireless gyroscope to differentiate between a hemiplegic ankle and unaffected ankle. Further applications using a portable media device as a wireless gyroscope platform involve the use of a wobble board with machine learning also classifying between a hemiplegic ankle and unaffected ankle. Another scenario applies the smartphone as a wireless gyroscope for Virtual Proprioception as feedback for eccentric training while applying machine learning to classify between Virtual Proprioception feedback and without Virtual Proprioception feedback for eccentric training. These preliminary systems are capable of providing essentially autonomous homebound therapy amendable for Network Centric Therapy. © 2018, Springer Nature Singapore Pte Ltd.","Ankle; Assistive device; Cane gait; Eccentric training; Hemiparesis; Homebound therapy; Machine learning; Portable media device; Rehabilitation; Smartphone; Virtual proprioception; Wireless accelerometer; Wireless gyroscope; Wobble board",,2-s2.0-85032358727
"Ul Haq M.A., Kamboh H.M.A., Akram U., Sohail A., Iram H.","Indoor localization using improved multinomial naïve bayes technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028647972&doi=10.1007%2f978-3-319-60834-1_32&partnerID=40&md5=112101682a37a2822c778193daed6e4c","With the extensive use of mobiles, tablets, laptops and other Wi-Fi carrying handheld devices, indoor localization using Wi-Fi fingerprinting has gained much interest of researchers. Many techniques have been introduced to increase the accuracy of the localization system. Bayesian learning techniques are considered much accurate for localization but still there are some issues including zero probability and good accuracy. In this paper we introduce a unique weighting technique called improved multinomial Naive Bayes technique for localization. For data collection we used a freeware android software, Wi-Fi Analyser. Experiments are conducted in the first floor of my office using HTC One. Our technique which uses the concept of Multinomial Naïve Bayes classifier which is actually not used before in indoor localization. It provides better accuracy, resolves zero probability issue caused due to data incompleteness. It also somehow tackles with naïve Bayes issue of independencies that according to Navies Bayes all the features are independent of each other but in physical circumstances it is not the case as features are dependent sometimes so we have tried to solve this issue as well and is easy to implement as it involves less computations as compared to those weighting techniques that includes non-linear functions. © 2018, Springer International Publishing AG.","Bayes classifiers; Indoor localization; Machine learning","Functions; Learning systems; Warships; Wireless local area networks (WLAN); Android softwares; Bayes Classifier; Indoor localization; Localization system; Multinomial naive bayes; Nonlinear functions; Weighting techniques; Wi-Fi fingerprinting; Indoor positioning systems",2-s2.0-85028647972
"Fu Q., Feng B., Guo D., Li Q.","Combating the evolving spammers in online social networks",2018,"Computers and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029423592&doi=10.1016%2fj.cose.2017.08.014&partnerID=40&md5=b4f740b2c1119f295c43cf6558d142e0","Online social networks, such as Facebook and Sina Weibo, have become the most popular platforms for information sharing and social activities. Spammers have utilized social networks as a new way to spread spam information using fake accounts. Many detection methods have been proposed to solve this problem, and have been proved to be successful to some extent. However, as the spammers' strategies for evading detection evolve, many existing methods lose their efficacy. A major limitation of previous approaches is that they are using the features from a static time point to detect spammers, without considering temporal factors. In this study, we approach the challenge of spammer detection by leveraging the temporal evolution patterns of users. We propose a dynamic metric to measure the change in users' activities and design new features to quantify users' evolution patterns. Then we develop a framework by combining unsupervised and supervised learning to distinguish between spammers and legitimate users. We test our method on a real world dataset with a large number of users. The evaluation results show that our approach can efficiently distinguish the difference between spammers and legitimate users regarding temporal evolution patterns. It also demonstrates the high level of similarity in the spammers' temporal evolution patterns. Compared with other detection methods, our method can achieve better performance. To the best of our knowledge, our study is the first to provide a generic and efficient framework to depict the evolutional pattern of users. It can handle the problem of spammers updating their strategies to evade detection and is a valuable reference for this research field. © 2017 Elsevier Ltd","Classification; Machine learning; Online social networks; Spammer detection; Temporal evolution","Classification (of information); Learning systems; Statistical tests; Detection methods; Evaluation results; Evolution patterns; Information sharing; On-line social networks; Social activities; Spammer detections; Temporal evolution; Social networking (online)",2-s2.0-85029423592
"Laopracha N., Sunat K.","Comparative study of computational time that HOG-based features used for vehicle detection",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022219872&doi=10.1007%2f978-3-319-60663-7_26&partnerID=40&md5=116a0f98eb5d65acc7d78ebae61e96b9","HOG produces a number of redundant and long features so that they affect to the detection rate and computational time. This paper studied the processes that HOG-based features were generated, selected, and used in vehicle detection and find one that takes the shortest time. There were five combinations of feature extractors and classifiers. Time spent in HV step, accuracy of detection and the false positive rate are considered together for making decision of which combination is the best. The experiments were conducted on GIT dataset. The experimental results showed that process which VHOG preceded ELM provided a little less accurate than HOG preceded SVM did. However, it did not only take shortest time in HV step but also provided the lowest false positive rate. Therefore, VHOG preceded ELM should be selected as a method for vehicle detection. © Springer International Publishing AG 2018.","Extreme Learning Machine (ELM); Feature selection method; Histograms of Oriented Gradients (HOG); Support Vector Machine (SVM); Vehicle detection","Support vector machines; Vehicles; Comparative studies; Computational time; Extreme learning machine; False positive rates; Feature extractor; Feature selection methods; Histograms of oriented gradients (HoG); Vehicle detection; Feature extraction",2-s2.0-85022219872
"Malik M.R., Isaac B.J., Coussement A., Smith P.J., Parente A.","Principal component analysis coupled with nonlinear regression for chemistry reduction",2018,"Combustion and Flame",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029696832&doi=10.1016%2fj.combustflame.2017.08.012&partnerID=40&md5=6133584eb88b2ff5b2a0eaf8685d4f44","Large kinetic mechanisms are required in order to accurately model combustion systems. If no parameterization of the thermo-chemical state-space is used, solution of the species transport equations can become computationally prohibitive as the resulting system involves a wide range of time and length scales. Parameterization of the thermo-chemical state-space with an a priori prescription of the dimension of the underlying manifold would lead to a reduced yet accurate description. To this end, the potential offered by Principal Component Analysis (PCA) in identifying low-dimensional manifolds is very appealing. The present work seeks to advance the understanding and application of the PC-transport approach by analyzing the ability to parameterize the thermo-chemical state with the PCA basis using nonlinear regression. In order to demonstrate the accuracy of the method within a numerical solver, unsteady perfectly stirred reactor (PSR) calculations are shown using the PC-transport approach. The PSR analysis extends previous investigations to more complex fuels (methane and propane), showing the ability of the approach to deal with relatively large kinetic mechanisms. The ability to achieve highly accurate mapping through Gaussian Process based nonlinear regression is also shown. In addition, a novel method based on local regression of the PC source terms is also investigated which leads to improved results. © 2017 The Combustion Institute","Combustion; Local regression; Low-dimensional manifolds; Nonlinear regression; Principal component analysis","Chemical analysis; Combustion; Nonlinear analysis; Numerical methods; Regression analysis; Chemistry reductions; Combustion systems; Gaussian Processes; Local regression; Low-dimensional manifolds; Non-linear regression; Perfectly stirred reactor; Priori prescriptions; Principal component analysis; propane; Article; artificial neural network; chemical reaction; combustion; decomposition; gaussian process regression; kernel method; kinetics; linear regression analysis; machine learning; nonlinear regression analysis; perfectly stirred reactor; principal component analysis; priority journal; stirred reactor; support vector machine",2-s2.0-85029696832
"Binias B., Frąckiewicz M., Jaskot K., Palus H.","Pixel Classification for Skin Detection in Color Images",2018,"Studies in Systems, Decision and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029681630&doi=10.1007%2f978-3-319-64674-9_6&partnerID=40&md5=1600c898b9839b2755eda1c73d289805","In this paper a direct, pixel-based skin detection method is proposed and evaluated. Proposed approach discards any spatial information that can be found in digital image and focuses entirely on data-oriented analysis. To ensure the best perfomance two classifiers (Regularized Logistic Regression and Artificial Neural Network with Regularization trained with Backpropagation) were deeply examined, evaluated and compared for this task. The best model achieved the almost perfect accuracy and quality of classification on the used ‘Skin Segmentation Dataset’ provided for the UCI Machine Learning Repository with over 99% accuracy, precision, recall and specificity. © 2018, Springer International Publishing AG.","Classification; Color image; Logistic regression; Machine learning; Neural networks; Skin detection",,2-s2.0-85029681630
"Bawage S., Momin B.","Detection of diseases on crops & design of recommendation engine: A review",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028401184&doi=10.1007%2f978-3-319-63673-3_48&partnerID=40&md5=bdf35367b1ec10013ad28110430d1189","It is very difficult to detect and monitor the diseases manually and also needs the expertise in the field so that process becomes time consuming. Hence, image processing can be used to detect the diseases and further giving the correct recommendation for the detected disease will be the better solution because only detection of disease will not be the helpful. Disease detection process using image processing involves: Image Acquisition from farmers, image pre-processing and enhancement, edge detection and segmentation, feature extraction, classification of extracted features. The process will not stop here a correct recommendation is very necessary to prevent losses which are faced by the farmers. So designing the recommendation system which gives the best doses for detected disease is the good solution to the farmers. © 2018, Springer International Publishing AG.","Classification; Image processing; Machine learning; Recommendation engine","Agriculture; Classification (of information); Edge detection; Engines; Feature extraction; Image acquisition; Image segmentation; Intelligent systems; Learning systems; Recommender systems; Disease detection; Image preprocessing; Image processing",2-s2.0-85028401184
"Wulff-Jensen A., Bruni L.E.","Evaluating ann efficiency in recognizing eeg and eye-tracking evoked potentials in visual-game-events",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021822056&doi=10.1007%2f978-3-319-60642-2_25&partnerID=40&md5=868344a41b863c9e130724eecad1a1d4","EEG and Eye-tracking signals have customarily been analyzed and inspected visually in order to be correlated to the controlled stimuli. This process has proven to yield valid results as long as the stimuli of the experiment are under complete control (e.g.: the order of presentation). In this study, we have recorded the subject’s electroencephalogram and eye-tracking data while they were exposed to a 2D platform game. In the game we had control over the design of each level by choosing the diversity of actions (i.e. events) afforded to the player. However we had no control over the order in which these actions were undertaken. The psychophysiological signals were synchronized to these game events and used to train and test an artificial neural network in order to evaluate how efficiently such a tool can help us in establishing the correlation, and therefore differentiating among the different categories of events. The highest average accuracies were between 60.25%–72.07%, hinting that it is feasible to recognize reactions to complex uncontrolled stimuli, like game events, using artificial neural networks. © Springer International Publishing AG 2018.","Artificial; Electroencephalogram; Eye-tracking; Game events; Games; Machine learning; Neural network; Psychophysiology; Pupillometry","Complex networks; Electrophysiology; Learning systems; Neural networks; Psychophysiology; Artificial; Eye-tracking; Game events; Games; Pupillometry; Electroencephalography",2-s2.0-85021822056
"Buza K., Kis P.B.","Towards privacy-aware keyboards",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019195203&doi=10.1007%2f978-3-319-59162-9_15&partnerID=40&md5=9a1aae86a08bcbca9199296d07e8be9b","As shown by various studies, the dynamics of typing on a keyboard is characteristic to persons. On the one hand, this may allow for person identification based on keystroke dynamics in various applications. On the other hand, in certain situations, such as chat-based anonymous helplines, web search for sensitive topics, etc., users may not want to reveal their identity. In general, there are various methods to increase the protection of personal data. In this paper, we propose the concept of privacy-aware keyboard, i.e., a keyboard which transmits keyboard events (such as pressing or releasing of a key) with small random delays in order to ensure that the identity of the user is difficult to be inferred from her typing dynamics. We use real-world keystroke dynamics data in order to simulate privacy-aware keyboards with uniformly random delay and Gaussian delay. The experimental results indicate that the proposed techniques may have an important contribution to keeping the anonymity of users. © Springer International Publishing AG 2018.","Keystroke dynamics; Machine learning; Privacy; Web search","Dynamics; Information retrieval; Learning systems; Websites; Gaussians; Keystroke dynamics; Person identification; Privacy aware; Random delay; Real-world; Web searches; Data privacy",2-s2.0-85019195203
"Gągolewski M., James S.","Fitting symmetric fuzzy measures for discrete sugeno integration",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029423626&doi=10.1007%2f978-3-319-66824-6_10&partnerID=40&md5=46803838d59202cef7a46724ae4a2bf5","The Sugeno integral has numerous successful applications, including but not limited to the areas of decision making, preference modeling, and bibliometrics. Despite this, the current state of the development of usable algorithms for numerically fitting the underlying discrete fuzzy measure based on a sample of prototypical values – even in the simplest possible case, i.e., assuming the symmetry of the capacity – is yet to reach a satisfactory level. Thus, the aim of this paper is to present some results and observations concerning this class of data approximation problems. © 2018, Springer International Publishing AG.","Aggregation functions; Approximation; Machine learning; Regression; Sugeno integral","Decision making; Fuzzy sets; Fuzzy systems; Learning systems; Pattern matching; Aggregation functions; Approximation; Data approximation; Fuzzy measures; Preference modeling; Regression; Sugeno integrals; Symmetric fuzzy measures; Fuzzy logic",2-s2.0-85029423626
"Strug B., Ślusarczyk G., Grabska E.","Design classification based on matching graph kernels",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030762198&doi=10.1007%2f978-3-319-67792-7_44&partnerID=40&md5=598be59e77b6ac3d66bc9c949b260759","The paper deals with the problem of classification of designs according to their styles. The designs are represented by means of labelled, attributed graphs. The similarity between designs is calculated with the use of a new graph kernel and then used to predict if a given design belongs to a certain style of designs. The prediction process is performed by a classification algorithm. Examples of garden designs are used to present experimental results obtained by means of the presented method. © 2018, Springer International Publishing AG.","Design patterns; Graph classification; Graph kernels; Machine learning","Computer programming; Computer science; Attributed graphs; Classification algorithm; Design Patterns; Graph classification; Graph kernels; Matching graph; Prediction process; Learning systems",2-s2.0-85030762198
"McCutchan M.","Linked data for a digital earth: Spatial forecasting with next generation geographical data",2018,"Lecture Notes in Geoinformation and Cartography",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031308079&doi=10.1007%2f978-3-319-63946-8_19&partnerID=40&md5=e0ccb49221ac8044039462abb04c807b","This document outlines potential research on integrating heterogeneous geographical data for forecasting purposes within the context of Digital Earth. The approach presented in this document relies on Linked Data principles which provide advantages for data integration but also data access. A structure for embedding geographic data into Linked Data is proposed. This structure is then utilized for forecasting spatial phenomena by establishing spatial association rules. The expected outcome of this proposed work is a framework capable of extracting association rules from the Linked Open Data web and an investigation of these rules as well as their potential. © Springer International Publishing AG 2018.","Linked data; Machine learning; Spatial prediction","Association rules; Data handling; Forecasting; Information theory; Learning systems; Forecasting purpose; Geographical data; Linked Data principles; Linked datum; Linked open datum; Potential researches; Spatial association rules; Spatial prediction; Data integration",2-s2.0-85031308079
"Sharma A., Sahay S.K.","An investigation of the classifiers to detect android malicious apps",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031715294&doi=10.1007%2f978-981-10-5508-9_20&partnerID=40&md5=5fbb214625e7e704a9354b9e358080cd","Android devices are growing exponentially and are connected through the Internet accessing billion of online Websites. The popularity of these devices encourages malware developer to penetrate the market with malicious apps to annoy and disrupt the victim. Although for the detection of malicious apps different approaches are discussed. However, proposed approaches are not sufficed to detect the advanced malware to limit/prevent the damages. In this, very few approaches are based on opcode occurrence to classify the malicious apps. Therefore, this paper investigates the five classifiers using opcode occurrence as the prominent features for the detection of malicious apps. For the analysis, we use WEKA tool and found that FT detection accuracy (~79.27%) is best among the investigated classifiers. However, true positives rate, i.e. malware detection rate is highest (~99.91%) by RF and fluctuate least with the different number of prominent features compared to other studied classifiers. The analysis shows that overall accuracy is majorly affected by the false positives of the classifier. © Springer Nature Singapore Pte Ltd. 2018.","Android security; Machine learning; Malware detection; Static analysis","Computer crime; Feature extraction; Learning systems; Malware; Static analysis; Android securities; Detection accuracy; False positive; Malware detection; Overall accuracies; Prominent features; True positive; Weka tool; Android (operating system)",2-s2.0-85031715294
"Mühlbacher C., Gspandl S., Reip M., Steinbauer G.","Estimation of the traversal time for a fleet of industrial transport robots",2018,"Mechanisms and Machine Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028334354&doi=10.1007%2f978-3-319-61276-8_40&partnerID=40&md5=5ca7aa5dda8b140f9d6718e9dcaa3050","Transportation tasks within warehouses are nowadays more and more solved using of fleets of autonomous robots. A fleet allows coordinating the different robots in such a way that it balance the load caused by transportation tasks. This allows the robot fleet to be a cost-efficient solution for moderate and changing loads compared to fixed conveyor belts. To allow such a flexible load balancing it is necessary to estimate the time it may take to perform a certain transportation. This is of interest if different transportation tasks can be assigned to an individual robot and the order may have an impact on the time spent to perform a transportation task. In this paper, we will present a method which can learn to estimate the time spent on certain transportation tasks. The method is evaluated per its prediction accuracy on a different set of data which were obtained from the deployment of a robotic fleet in an industrial environment. © 2018, Springer International Publishing AG.","Estimation; Machine learning; Neural network; Path planning; Temporal pattern; Travel time","Belt conveyors; Estimation; Learning systems; Motion planning; Neural networks; Robotics; Robots; Transportation; Travel time; Conveyor belts; Cost-efficient; Flexible loads; Industrial environments; Prediction accuracy; Temporal pattern; Time spent; Traversal time; Fleet operations",2-s2.0-85028334354
"Vasudevan S.K., Abhishek S.N., Keerthana N.K., Priyanka R., Aravinth A., Divya M.","An interactive and intelligent tool for circuit component recognition through virtual reality",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032683564&doi=10.1007%2f978-3-319-68385-0_31&partnerID=40&md5=5cbc44235b369087e82e740aae3956f3","The electronic products that support the needs in our daily life require a long drawn out process for its development and building. Every step from scratch i.e. requirement collection to integration and testing, involves a lot of time consuming tasks. Generally, a technically sound person with good knowledge in simulation is assigned to carry out these tasks, yet it fritters away time. The idea proposed here is an application for generating NetList for the circuit drawn through hand gesture, powered by virtual reality. The application gets input from the user’s hand gesture using an additional hardware called leap motion sensor. This enables the users to draw any kind of circuit, of any size with any number of components. The completed circuit is captured and further processing is carried out. The circuit is isolated, recognized and segregation of the wires and components takes place in the consecutive phases. The next step involves identification of individual components and the aggregated NetList will be generated in the final step. And also, complex circuit which cannot fit into a limited space can be visualized in the virtual environment. © Springer International Publishing AG 2018.","Component recognition; Electronic circuit; Gesture; Image processing; Machine learning; NetList; Virtual reality","Image processing; Integration testing; Learning systems; Network components; Networks (circuits); Timing circuits; Virtual reality; Circuit components; Component recognition; Electronic product; Gesture; Identification of individuals; Netlist; Number of components; Time-consuming tasks; Intelligent systems",2-s2.0-85032683564
"Rawatlal R.","Application of graph theory to analysing student success through development of progression maps",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025837892&doi=10.1007%2f978-3-319-60937-9_23&partnerID=40&md5=200cd4dcc99b06c4ecdbc3fdba2e53be","Student progression is influenced by a number of factors including the pre- and co-requisite structures. Simply viewing pass/fail rates of individual courses is not sufficient to understand an academic programmes progression profile. In this study, the emphasis was on identifying the major obstacles to progression towards graduation. The approach involved generating progression tree structures that revealed the routes by which students pass through the curriculum. Identifying these routes allows for their support to be implemented in practical ways, e.g. by alternative timetabling so that frequently occurring progression routes would experience fewer scheduling clashes. It was discovered that an large number of progression paths exist even in a structured degree programmes. Further tree analysis was therefore required. The primary approach was to observe the students on the minimum-time-to-graduate route and to determine which events (which courses) caused students to fail out of the minimum time route. The approach was therefore to determine which courses caused the (remaining) minimum time students fail onto longer graduation routes. This result was further distilled to the top five areas obstructing graduation. The methods were applied to an undergraduate Chemical Engineering programme. The system identified three courses in the second year second semester of the programme, with one course in particular, ENCH2TD, causing 45% of all minimum time students in that semester to fail out onto a route requiring an extra year of study. The method therefore consistently identified major obstacles to progression in an academic programme. © Springer International Publishing AG 2018.","Artificial intelligence; Curriculum structure; Machine learning","Artificial intelligence; Curricula; Education; Engineering education; Forestry; Graph theory; Learning systems; Scheduling; Trees (mathematics); Curriculum structure; Minimum time; Number of factors; Student success; Timetabling; Tree structures; Students",2-s2.0-85025837892
"Chadha A., Kaur P.","Handling smurfing through big data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031404531&doi=10.1007%2f978-981-10-6620-7_44&partnerID=40&md5=849518b802fb0398f3a875680cc5e187","Money laundering is a worrying term for every country’s economy these days. Leading economists of all major developed and developing economies are concerned to devise methods to prevent it. The economy of a country is weakened by the impact of money laundering. Networks created between various banks in different countries facilitate online money transfer, which is turning the process of money laundering into digital money laundering. This promotes money launderers to perform wired transactions from anywhere. People involved in the process of money laundering are efficiently using online banking as their weapon. Evading the anti-money laundering agencies is becoming easier for them because of having online bank accounts. Such people are misusing technology. Therefore, it is restricting one’s own country’s economic progress. But with the help of recently developed technologies, we are able to prevent such illegal activities. Scrutinizing all the transactions and investigating them manually at financial intelligence units are cumbersome tasks because petabytes of transactions are taking place each day. Advanced technologies like Big Data enable us to detect the suspicious customers possibly involved in money laundering. In this paper, we have proposed a methodology using big data to detect smurfing; based on which, suspicious people involved in money laundering may be identified and appropriate action can be taken against them. © 2018, Springer Nature Singapore Pte Ltd.","Anti-money laundering; Big data; Counterfeit currency; Cyber threat detection; Economic crisis; Hadoop; Hadoop; Machine learning; MapReduce; Money laundering; Outlier detection; Smurfing; Soft computing; Terrorism","Economics; Laundering; Learning systems; Soft computing; Terrorism; Anti-money laundering; Counterfeit currency; Cyber threats; Economic crisis; Hadoop; Map-reduce; Money laundering; Outlier Detection; Smurfing; Big data",2-s2.0-85031404531
"Żejmo M., Kowal M., Korbicz J., Monczak R.","Nuclei recognition using convolutional neural network and hough transform",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028347021&doi=10.1007%2f978-3-319-64474-5_26&partnerID=40&md5=ac22fd41f288dbd3296669bf60c2f24b","The paper presents method of nuclei segmentation on cytological images based on the Convolutional Neural Network (CNN) and modified Hough Transform method. It approximates nuclei by ellipses fitted to nuclei regions segmented by CNN. As study data set 50 cytological RGB images were used, divided into training set (50 images) and test set (10 images). The first step is to create a CNN model for pixel-wise classification of cytological images. As training set for CNN, patches of size 28 × 28 pixels were created based on images from training set and corresponding ground-truth labels. Using trained model, nuclei regions classification and segmentation from test set images was conducted. The reason of choosing the CNN for segmentation it’s better accuracy in separated overlapping nuclei than conventional methods such as for example Otsu thresholding etc. Subsequently, using Canny algorithm and Euclidean Distance Transform (EDT), edges and centers of segmented regions were extracted. Edges and centers of nuclei were extracted for reduce time computation for next step. Finally, finding nuclei using the modified Hough Transform by fitted ellipses was carried out. © Springer International Publishing AG 2018.","Convolutional neural network; Hough transform; Machine learning; Nuclei segmentation","Classification (of information); Convolution; Fault tolerance; Hough transforms; Image segmentation; Learning systems; Neural networks; Pixels; Statistical tests; Conventional methods; Convolutional neural network; Euclidean distance transforms; Modified hough transform; Nuclei segmentation; Otsu thresholding; Regions Classification; Segmented regions; Feature extraction",2-s2.0-85028347021
"Porto A., Irigoyen E.","Gas consumption prediction based on artificial neural networks for residential sectors",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028681222&doi=10.1007%2f978-3-319-67180-2_10&partnerID=40&md5=18be0cff4bdf607245a4c5ee2e55c678","The objective of this work is to improve gas supply efficiency in residential districts. To achieve this goal, Artificial Neural Networks (ANNs) have been used. In this work, a hybrid model based on ANN has been proposed that obtains total daily gas consumption (in KWh) in residential districts, with a prediction horizon of 7 days. Previous consumption records and meteorological variables have been considered to improve the prediction of future gas consumption. In order to find the best ANN that models the behavior of this consumption variable, a set of experiments has been designed, where the mean square error of each network is measured to rate their reliability and accuracy. A hybrid neural model has been created to determine a horizon of 7 predictions using a median filter of the 5 best predictors per day. © 2018, Springer International Publishing AG.","Artificial neural network; Gas consumption; Gas prediction; Machine learning for prediction","Artificial intelligence; Education; Gases; Housing; Learning systems; Mean square error; Median filters; Neural networks; Soft computing; Gas consumption; Hybrid model; Hybrid neural modeling; Meteorological variables; Prediction horizon; Prediction-based; Residential districts; Residential sectors; Forecasting",2-s2.0-85028681222
"Yu P., Low M.Y., Zhou W.","Development of a partial least squares-artificial neural network (PLS-ANN) hybrid model for the prediction of consumer liking scores of ready-to-drink green tea beverages",2018,"Food Research International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032856159&doi=10.1016%2fj.foodres.2017.10.015&partnerID=40&md5=862b3fdd9cf7f988790b2d86cf79273e","In order to develop products that would be preferred by consumers, the effects of the chemical compositions of ready-to-drink green tea beverages on consumer liking were studied through regression analyses. Green tea model systems were prepared by dosing solutions of 0.1% green tea extract with differing concentrations of eight flavour keys deemed to be important for green tea aroma and taste, based on a D-optimal experimental design, before undergoing commercial sterilisation. Sensory evaluation of the green tea model system was carried out using an untrained consumer panel to obtain hedonic liking scores of the samples. Regression models were subsequently trained to objectively predict the consumer liking scores of the green tea model systems. A linear partial least squares (PLS) regression model was developed to describe the effects of the eight flavour keys on consumer liking, with a coefficient of determination (R2) of 0.733, and a root-mean-square error (RMSE) of 3.53%. The PLS model was further augmented with an artificial neural network (ANN) to establish a PLS-ANN hybrid model. The established hybrid model was found to give a better prediction of consumer liking scores, based on its R2 (0.875) and RMSE (2.41%). © 2017 Elsevier Ltd","Artificial neural network; Consumer liking; Genetic algorithm; Green tea; Hybrid modelling; Machine learning; Partial least squares regression; Regression","Beverages; Chemical analysis; Forecasting; Genetic algorithms; Learning systems; Least squares approximations; Mean square error; Neural networks; Consumer liking; Green tea; Hybrid modelling; Partial least squares regression; Regression; Regression analysis",2-s2.0-85032856159
"Wang W., Brambley M.R., Kim W., Somasundaram S., Stevens A.J.","Automated point mapping for building control systems: Recent advances and future research needs",2018,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031756094&doi=10.1016%2fj.autcon.2017.09.013&partnerID=40&md5=68ba57f47ee380d5d4a1e2774071f35a","This paper presents a review of recent research and development on methodologies relevant to automating mapping of points in building control systems and between building control systems and external or replacement software and hardware. Manual point mapping is labor intensive and costly, presenting a major impediment to innovations in building control (e.g., automated fault detection and diagnostics, self-healing, and automated commissioning for existing building control systems). The methods reviewed focus on classifying building control system points, especially sensor classifications by sensor type. Fewer publications address other important aspects of the point mapping problem, such as discovering spatial and functional relationships among points, relationships between control system points, physical systems, and equipment, and between various equipment and the systems of which they are part, and discovering metadata, normalizing it to a common namespace, and assigning the metadata to control system points. To motivate further development of new automated point mapping approaches, we identify many research questions organized into four key technical needs: 1) a complete solution and underlying problem formulation, 2) alignment of methods with the actual point mapping problem, 3) test cases, data sets for testing, explicit test procedures, and consistent performance metrics for reporting testing and evaluation results, and 4) understanding of the applicable data space to ensure future adaptability of automated BAS point mapping. © 2017","Building automation systems; Building control; Machine learning; Metadata; Point mapping; Semantics","Automation; Control systems; Fault detection; Intelligent buildings; Learning systems; Mapping; Metadata; Semantics; Testing; Automated commissioning; Automated fault detection and diagnostics; Building automation systems; Building control systems; Building controls; Consistent performance; Functional relationship; Testing and evaluation; Buildings",2-s2.0-85031756094
"Dudarin P.V., Yarushkina N.G.","An approach to fuzzy hierarchical clustering of short text fragments based on fuzzy graph clustering",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402952&doi=10.1007%2f978-3-319-68321-8_30&partnerID=40&md5=231819b1de1e3104aee11b73f5737e2d","In this paper a novel approach to fuzzy hierarchical clustering of short text fragments is presented. Nowadays dataset which contains a large and even huge amount of short text fragments becomes quite a common object. Different kinds of short messages, paper or news headers are examples of this kind of objects. Authors have taken another similar object which is a dataset of key process indicators of Strategic Planning System of Russian Federation. In order to reveal structure and thematic variety, fuzzy clustering approach is proposed. Fuzzy graph as a model has been chosen as the most natural view of connected set of words. Finally, hierarchy as a result of clustering obtained as desirable presentation structure of large amount of information. © Springer International Publishing AG 2018.","Clustering; Data mining; Fuzzy clustering; Fuzzy graph; Hierarchical clustering; Machine learning; Soft computing; Text analysis","Data mining; Fuzzy clustering; Fuzzy sets; Fuzzy systems; Graph theory; Learning systems; Soft computing; Clustering; Clustering approach; Connected sets; Fuzzy graph; Hier-archical clustering; Large amounts; Russian federation; Text analysis; Cluster analysis",2-s2.0-85031402952
"Chawky B.S., Elons A.S., Ali A., Shedeed H.A.","A study of action recognition problems: Dataset and architectures perspectives",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032036971&doi=10.1007%2f978-3-319-63754-9_19&partnerID=40&md5=8c8c32c1025c54b89d93f6a99fe0f256","Action recognition field has recently grown dramatically due to its importance in many applications like smart surveillance, human–computer interaction, assisting aged citizens or web-video search and retrieval. Many research trials have tackled action recognition as an open problem. Different datasets are built to evaluate architectures variations. In this survey, different action recognition datasets are explored to highlight their ability to evaluate different models. In addition, for each dataset, a usage is proposed based on the content and format of data it includes, the number of classes and challenges it covers. On other hand, another exploration for different architectures is drawn showing the contribution of each of them to handle different action recognition problem challenges and the scientific explanation behind their results. An overall of 21 datasets is covered with 13 architectures that are shallow and deep models. © Springer International Publishing AG 2018.","Action recognition; Action/activity recognition; Architectures; Computer vision; Deep learning models; Machine learning; Shallow models",,2-s2.0-85032036971
"Zolotin A.A., Malchevskaya E.A., Tulupyev A.L., Sirotkin A.V.","An approach to sensitivity analysis of inference equations in algebraic bayesian networks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031429180&doi=10.1007%2f978-3-319-68321-8_4&partnerID=40&md5=f077bc0e0aaee24c20939fde108784e9","An approach to the sensitivity analysis of local a posterior inference equations in algebraic Bayesian networks is proposed in the paper. Performed a sensitivity analysis of first a posterior inference task for stochastic and deterministic evidences propagated into the knowledge pattern with scalar estimates. For each of the considered cases the necessary metrics are chosen and transformations are carried out, that result into a linear programming problem. In addition, for each type of evidence theorems that postulate upper sensitivity estimates are formulated and proofs are provided. Theoretical results are implemented in CSharp using the module of probabilistic-logical inference software complex. A series of computational experiments is conducted. The results of experiments are visualized using tables and charts. The proposed visualization demonstrates the high sensitivity of the considered models, that confirms the correctness of their use. © Springer International Publishing AG 2018.","Algebraic bayesian network; Evidence propagation; Machine learning; Matrix-vector equations; Posterior inference; Probabilistic graphical model; Sensitivity statistical estimate","Algebra; Bayesian networks; Computation theory; Learning systems; Linear programming; Linear transformations; Mathematical transformations; Stochastic systems; Evidence propagation; Matrix-vector; Posterior inference; Probabilistic graphical models; Sensitivity statistical estimate; Sensitivity analysis",2-s2.0-85031429180
"Amato F., Cozzolino G., Mazzeo A., Vivenzio E.","Using multilayer perceptron in computer security to improve intrusion detection",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020405125&doi=10.1007%2f978-3-319-59480-4_22&partnerID=40&md5=92d5109729f7d144452b1512c3d0449a","Nowadays computer and network security has become a major cause of concern for experts community, due to the growing number of devices connected to the network. For this reason, optimizing the performance of systems able to detect intrusions (IDS - Intrusion Detection System) is a goal of common interest. This paper presents a methodology to classify hacking attacks taking advantage of the generalization property of neural networks. In particular, in this work we adopt the multilayer perceptron (MLP) model with the back-propagation algorithm and the sigmoidal activation function. We analyse the results obtained using different configurations for the neural network, varying the number of hidden layer sand the number of training epochs in order to obtaina low number of false positives. The obtained results will be presented in terms of type of attacks and training epochs and we will show that the best classification is carried out for DOS and Probe attacks. © Springer International Publishing AG 2018.","Intrusion detection; Machine learning; Multilayer perceptron; Network security; Neural networks","Backpropagation; Backpropagation algorithms; Interactive computer systems; Intrusion detection; Learning systems; Mercury (metal); Multilayer neural networks; Multilayers; Multimedia services; Multimedia systems; Neural networks; Personal computing; Security of data; Common interests; Computer and network security; Generalization properties; Intrusion Detection Systems; Multi layer perceptron; Performance of systems; Sigmoidal activation functions; Training epochs; Network security",2-s2.0-85020405125
"Sanwaliya A., Chinnamgari S.K., Desai A., Saha A.","Graph community detection: Normalized compression distance based implementation for text data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028635814&doi=10.1007%2f978-3-319-60834-1_5&partnerID=40&md5=7f3bc31626b87b5c01a50f94c99e4b89","Community detection algorithms are widely used to study the structural properties of real-world networks. In this paper, we experimentally evaluate the qualitative performance of several community detection algorithms based on normalized compression distance (NCD) using diversified datasets like documents, feeds, articles and blogs. We compare the quality of the algorithms based on F-score measure. Text data when given as input to NCD performs better when compare to conventional feed like document term matrix as input. Finally, we reveal that label propagation community detection algorithm is more suitable for clustering text data as compare to other community detection algorithms and it creates more distinct communities on diversified data. © 2018, Springer International Publishing AG.","Graph community detection; Machine learning; NLP; Normalized compression distance; Text data","Learning systems; Population dynamics; Signal detection; Community detection; Community detection algorithms; F-score; Label propagation; Normalized compression distance; Real-world networks; Text data; Clustering algorithms",2-s2.0-85028635814
"Heiberger R.H.","Predicting economic growth with stock networks",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028070940&doi=10.1016%2fj.physa.2017.07.022&partnerID=40&md5=11abd5ce70dc8f126d2f3f078d0320ee","Networks derived from stock prices are often used to model developments on financial markets and are tightly intertwined with crises. Yet, the influence of changing market topologies on the broader economy (i.e. GDP) is unclear. In this paper, we propose a Bayesian approach that utilizes individual-level network measures of companies as lagged probabilistic features to predict national economic growth. We use a comprehensive data set consisting of Standard and Poor's 500 corporations from January 1988 until October 2016. The final model forecasts correctly all major recession and prosperity phases of the U.S. economy up to one year ahead. By employing different network measures on the level of corporations, we can also identify which companies’ stocks possess a key role in a changing economic environment and may be used as indication of critical (and prosperous) developments. More generally, the proposed approach allows to predict probabilities for different overall states of social entities by using local network positions and could be applied on various phenomena. © 2017 Elsevier B.V.","Economic growth; Econophysics; Machine learning; Naïve Bayes classifier; Stock networks","Bayesian networks; Commerce; Financial markets; Forecasting; Learning systems; Bayes Classifier; Bayesian approaches; Economic environment; Economic growths; Econophysicss; Individual levels; Model development; Network measures; Economics",2-s2.0-85028070940
"Kamiran F., Mansha S., Karim A., Zhang X.","Exploiting reject option in classification for social discrimination control",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376541&doi=10.1016%2fj.ins.2017.09.064&partnerID=40&md5=8564aac182aeca8e3baa1b6832f5737e","Social discrimination is said to occur when an unfavorable decision for an individual is influenced by her membership to certain protected groups such as females and minority ethnic groups. Such discriminatory decisions often exist in historical data. Despite recent works in discrimination-aware data mining, there remains the need for robust, yet easily usable, methods for discrimination control. In this paper, we utilize reject option in classification, a general decision theoretic framework for handling instances whose labels are uncertain, for modeling and controlling discriminatory decisions. Specifically, this framework permits a formal treatment of the intuition that instances close to the decision boundary are more likely to be discriminated in a dataset. Based on this framework, we present three different solutions for discrimination-aware classification. The first solution invokes probabilistic rejection in single or multiple probabilistic classifiers while the second solution relies upon ensemble rejection in classifier ensembles. The third solution integrates one of the first two solutions with situation testing which is a procedure commonly used in the court of law. All solutions are easy to use and provide strong justifications for the decisions. We evaluate our solutions extensively on four real-world datasets and compare their performances with previously proposed discrimination-aware classifiers. The results demonstrate the superiority of our solutions in terms of both performance and flexibility of applicability. In particular, our solutions are effective at removing illegal discrimination from the predictions. © 2017","Classification; Decision theory; Discrimination-aware data mining; Fairness in machine learning","Data mining; Decision theory; Learning systems; All solutions; Classifier ensembles; Decision boundary; Decision-theoretic; Ethnic groups; Historical data; Probabilistic classifiers; Real-world datasets; Classification (of information)",2-s2.0-85032376541
"Park A., Conway M., Chen A.T.","Examining thematic similarity, difference, and membership in three online mental health communities from reddit: A text mining and visualization approach",2018,"Computers in Human Behavior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029859953&doi=10.1016%2fj.chb.2017.09.001&partnerID=40&md5=2caece11041fbd2d761de9ee539c6a75","Objectives Social media, including online health communities, have become popular platforms for individuals to discuss health challenges and exchange social support with others. These platforms can provide support for individuals who are concerned about social stigma and discrimination associated with their illness. Although mental health conditions can share similar symptoms and even co-occur, the extent to which discussion topics in online mental health communities are similar, different, or overlapping is unknown. Discovering the topical similarities and differences could potentially inform the design of related mental health communities and patient education programs. This study employs text mining, qualitative analysis, and visualization techniques to compare discussion topics in publicly accessible online mental health communities for three conditions: Anxiety, Depression and Post-Traumatic Stress Disorder. Methods First, online discussion content for the three conditions was collected from three Reddit communities (r/Anxiety, r/Depression, and r/PTSD). Second, content was pre-processed, and then clustered using the k-means algorithm to identify themes that were commonly discussed by members. Third, we qualitatively examined the common themes to better understand them as well as their similarities and differences. Fourth, we employed multiple visualization techniques to form a deeper understanding of the relationships among the identified themes for the three mental health conditions. Results The three mental health communities shared four themes: sharing of positive emotion, gratitude for receiving emotional support, and sleep- and work-related issues. Depression clusters tended to focus on self-expressed contextual aspects of depression, whereas the Anxiety Disorders and Post-Traumatic Stress Disorder clusters addressed more treatment- and medication-related issues. Visualizations showed that discussion topics from the Anxiety Disorders and Post-Traumatic Stress Disorder subreddits shared more similarities to one another than to the depression subreddit. Conclusions We observed that the members of the three communities shared several overlapping concerns (i.e., sleep- and work-related problems) and discussion patterns (i.e., sharing of positive emotion and showing gratitude for receiving emotional support). We also highlighted that the discussions from the r/Anxiety and r/PTSD communities were more similar to one another than to discussions from the r/Depression community. The r/Anxiety and r/PTSD subreddit members are more likely to be individuals whose experiences with a condition are long-term, and who are interested in treatments and medications. The r/Depression subreddit members may be a comparatively diffuse group, many of whom are dealing with transient issues that cause depressed mood. The findings from this study could be used to inform the design of online mental health communities and patient education programs for these conditions. Moreover, we suggest that researchers employ multiple methods to fully understand the subtle differences when comparing similar discussions from online health communities. © 2017","Anxiety disorders; Consumer health information; Consumer health information; Depression; Post-traumatic; Stress disorders; Unsupervised machine learning","Data mining; Education; Learning systems; Social networking (online); Visualization; Anxiety disorders; Consumer health information; Depression; Post-traumatic; Stress disorders; Unsupervised machine learning; Health",2-s2.0-85029859953
"Tsurikov A.N., Guda A.N.","Practical application of the original method for artificial neural network’s training",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031423211&doi=10.1007%2f978-3-319-68321-8_9&partnerID=40&md5=18b2af036533f48f96c5b8652074be41","The article describes practical application of the original method of training artificial neural network based on the poorly formalized expert knowledge. The method allows extending the range of problems to be solved in case of lack of a sufficient number of the observations due to the fact that the training vectors are formed on the basis of the expert knowledge. The expert continuously defines classes of objects that are generated by the pseudorandom number of the training vectors of input signals, and created visual images by computer for clearly describing objects by given training vectors. The method is applied to solve important practical problem for determining of the atmospheric surface layer stability. The problem is formulated as a classification problem. As being the artificial neural network was selected multilayer perceptron. This trained neural network is represented by programming model implementing as DLL-module of dynamic-link library. The research bases on the original computer program that implements the algorithm of author’s training method. The program determines and implements the steps of the author’s research using heuristic training method of the artificial neural network to solve the problems of classification on the basis of poorly formalized experts’ knowledge. Its algorithms are used to generate visual (cognitive) images of possible situations to retrieve the unconscious expert knowledge. As a result, the aim of the study was achieved. The proposed method of training artificial neural network was applied successfully to solve a practical problem and showed its efficiency on an example of the classification problem. The author’s training method is protected by Russian patent for invention; the use of computer software holds a certificate of state registration. © Springer International Publishing AG 2018.","Artificial neural network (ANN); Classification problem; Machine learning; Method of artificial neural network’s training; Multilayer perceptron; Poorly formalized knowledge; Stability of the atmospheric surface layer","Atmospheric thermodynamics; Computer software; Learning systems; Multilayer neural networks; Multilayers; Neural networks; Patents and inventions; Problem solving; Atmospheric surface layers; Dynamic link library; Expert knowledge; Poorly formalized knowledge; Practical problems; Programming models; Pseudo-random numbers; Trained neural networks; Heuristic methods",2-s2.0-85031423211
"Diraco G., Leone A., Siciliano P.","A fall detector based on ultra-wideband radar sensing",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029653655&doi=10.1007%2f978-3-319-55077-0_47&partnerID=40&md5=d536df453747d015fca1a49e2638d337","Falls in the elderly have been recognized worldwide as a major public health problem. Nevertheless, falls cannot be detected efficiently yet, due to open issues on both sensing and processing sides. The most promising sensing approaches raise concerns for privacy issues (e.g., video-based approaches) or low acceptability rate (e.g., wearable approaches); whereas on the processing side, the commonly used methodologies are based on supervised techniques trained with both positive (falls) and negative (ADL-Activity of Daily Living) samples, both simulated by healthy young subjects. As a result of such a training protocol, fall detectors inevitably exhibit lower performance when used in real-world situations, in which monitored subjects are older adults. The aim of this study is to investigate a fully privacy-preserving and high-acceptance sensing technology, i.e. ultra-wideband radar sensor, together with a novelty detection methodology based exclusively on real ADL data from monitored elderly subject. The use of the UWB novelty detection methodology allowed to significantly improve detection performance in comparison to traditional supervised approaches. © Springer International Publishing AG 2018.","Fall detection; Machine learning; Novelty detection; Range sensing; Ultra-wideband radar sensor","Data privacy; Learning systems; Radar; Radar equipment; Video signal processing; Activity of daily livings; Detection performance; Fall detection; Falls in the elderlies; Novelty detection; Range sensing; Real world situations; Ultra wideband radars; Ultra-wideband (UWB)",2-s2.0-85029653655
"Merai M., Yu J.Y.","Joint Energy Demand Prediction and Control",2018,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032687230&doi=10.1007%2f978-3-319-67837-5_9&partnerID=40&md5=9d4bb7fb40767a7aa5e7b1cd04590e10","Joint electricity predictor and controller (JEPAC) is a system that allows energy suppliers to better predict their electricity grid activity and then, optimize their energy production, management and distribution. In fact, the more accurate the prediction is, the lesser its negative impact on the economy and environment. Once the JEPAC system is installed in the energy consumer place, it will collect indoor ambient parameters and energy usage and thus predict the individual future consumption. This prediction will be frequently transmitted to the energy supplier as a formatted commitment then later, the same device will try to respect this commitment by adjusting wisely the user’s appliances and HVAC. As a result, the energy supplier will then crowdsource the global energy demand by aggregating highly detailed individual consumption commitments. This will allow a better prediction and control of the future energy demand. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Energy prediction and control; HVAC; Machine learning; Micro grid; Smart grid","Developing countries; Electric power transmission networks; Energy management; Energy utilization; Forecasting; Learning systems; Energy demand prediction; Energy prediction; Energy productions; Global energy demand; HVAC; Micro grid; Prediction and control; Smart grid; Smart power grids",2-s2.0-85032687230
"Ganesh A., Jadhav A.R., Pragadeesh K.A.C.","Deep learning approach for recognition of handwritten Kannada numerals",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028619871&doi=10.1007%2f978-3-319-60618-7_29&partnerID=40&md5=3b02102923043aef74c926c49580e116","We come across a large volume of handwritten texts in our daily lives and handwritten character recognition has long been an important area of research in pattern recognition. The complexity of the task varies among different languages and it so happens largely due to the similarity between characters, distinct shapes and number of characters which are all language-specific properties. There have been numerous works on character recognition of English alphabets and with laudable success, but regional languages have not been dealt with very frequently and with similar accuracies. In this paper, we explored the performance of Convolutional Neural Networks, and Deep Belief Networks in the classification of Handwritten Kannada numerals, and conclusively compared the results obtained. The proposed method has shown satisfactory recognition accuracy in light of difficulties faced with regional languages such as similarity between characters and minute nuances that differentiate them. We can further extend this to all the Kannada characters. © Springer International Publishing AG 2018.","Convolutional neural networks; Deep belief networks; Deep learning; LeNet; Pattern recognition; Restricted boltzmann machine","Character recognition; Convolution; Deep learning; Neural networks; Soft computing; Convolutional neural network; Deep belief networks; Hand written character recognition; Handwritten kannada numerals; LeNet; Recognition accuracy; Restricted boltzmann machine; Specific properties; Pattern recognition",2-s2.0-85028619871
"Myroniv B., Wu C.-W., Ren Y., Tseng Y.-C.","Analysis of users’ emotions through physiology",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644011&doi=10.1007%2f978-981-10-6487-6_17&partnerID=40&md5=0baed4a02be51e3cf83d79960ae889c2","Most of the existing studies focus on physical activities recognition, such as running, cycling, swimming, etc. But what affects our health, it is not only physical activities, it is also emotional states that we experience throughout the day. These emotional states build our behavior and affect our physical health significantly. Therefore, emotion recognition draws more and more attention of researchers in recent years. In this paper, we propose a system that uses off-the-shelf wearable sensors, including heart rate, galvanic skin response, and body temperature sensors to read physiological signals from the users and applies machine learning techniques to recognize their emotional states. We consider three types of emotional states and conduct experiments on real-life scenarios with ten users. Experimental results show that the proposed system achieves high recognition accuracy. © Springer Nature Singapore Pte Ltd. 2018.","Emotion recognition; Machine learning; Physiological signals; Wearable devices",,2-s2.0-85032644011
"LeMoyne R., Mastroianni T.","Smartphones and portable media devices as wearable and wireless systems for gait and reflex response quantification",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032333497&doi=10.1007%2f978-981-10-5684-0_6&partnerID=40&md5=4f1f4d0dbcd0141b8b65e004859987a0","The smartphone and portable media device are equipped with inertial sensors, such as an accelerometer and gyroscope. With the proper software application they can function as wireless accelerometer and gyroscope platforms. This capability enables the smartphone and portable media device to function as wearable and wireless systems for gait and reflex response. The experimental trial data can be conveyed through wireless connectivity to the Internet as an email attachment for post-processing. The signal data can be further consolidated into a feature set for machine learning classification. Many experimental scenarios pertaining to quantifying the domains of gait and reflex response are presented. The smartphone and portable media device present an insightful perspective of the significant potential of Network Centric Therapy. © 2018, Springer Nature Singapore Pte Ltd.","Gait; Gait analysis; Impact pendulum; Inertial sensor; Machine learning; Portable media device; Smartphone; Tendon reflex; Wireless accelerometer; Wireless gyroscope; Wireless quantified reflex device",,2-s2.0-85032333497
"LeMoyne R., Mastroianni T.","Bluetooth inertial sensors for gait and reflex response quantification with perspectives regarding cloud computing and the internet of things",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376492&doi=10.1007%2f978-981-10-5684-0_7&partnerID=40&md5=de70789c14d74e945c8786db5bd1f94e","Bluetooth wireless enables localized connectivity to a smartphone, portable media device, and tablet. Rather than using these devices as wearable and wireless systems alone, the nature of Bluetooth wireless enables locally situated inertial sensors to be mounted to a subject for quantified evaluation of gait. The smartphone, portable media device, and tablet can then wirelessly transmit the data to a Cloud Computing resource for post-processing. Preliminary demonstration is presented regarding the machine learning classification of gait for Friedreich’s ataxia. A perspective of the application of Bluetooth wireless for reflex quantification is presented. Themes, such as sensor fusion and the Internet of Things, are further discussed. The prevalence of Bluetooth wireless further establishes the realization of Network Centric Therapy. © 2018, Springer Nature Singapore Pte Ltd.","Bluetooth wireless; Cloud computing; Friedreich’s ataxia; Gait; Inertial sensor node; Internet of things; Machine learning; Portable media device; Reflex quantification; Sensor fusion; Smartphone; Table",,2-s2.0-85032376492
"Konishi M., Okubo S., Nishino T., Wakatsuki M.","Decision tree analysis in game informatics",2018,"Studies in Computational Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026643477&doi=10.1007%2f978-3-319-64051-8_2&partnerID=40&md5=8d174bff0a9e6c61d0c591f30cb8b7a8","Computer Daihinmin involves playing Daihinmin, a popular card game in Japan, by using a player program. Because strong player programs of Computer Daihinmin use machine-learning techniques, such as the Monte Carlo method, predicting the program’s behavior is difficult. In this study, we extract the features of the player program through decision tree analysis. The features of programs are extracted by generating decision trees based on three types of viewpoints. To show the validity of our method, computer experiments were conducted. We applied our method to three programs with relatively obvious behaviors, and we confirmed that the extracted features were correct by observing real behaviors of the programs. © Springer International Publishing AG 2018.","Daihinmin; Decision tree analysis; Machine-learning techniques",,2-s2.0-85026643477
"Tan D.S., Leong R.N., Laguna A.F., Ngo C.A., Lao A., Amalin D.M., Alvindia D.G.","AuToDiDAC: Automated Tool for Disease Detection and Assessment for Cacao Black Pod Rot",2018,"Crop Protection",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030702212&doi=10.1016%2fj.cropro.2017.09.017&partnerID=40&md5=8c6a9df572a5fdfd2565676bd6ee25b3","Pest control strategies for crop diseases highly depend on visual inspection to assess the severity of the infection, which usually lead to inconsistencies: either over or under assessment. These inconsistencies could be attributed to the limitations of humans to perceive small differences. A more precise disease assessment is needed for better pest management decision, which will result to a more efficient utilization and allocation of resources for farm inputs. This translates to a better income for cacao farmers. This paper introduces a mobile application named AuToDiDAC or Automated Tool for Disease Detection and Assessment for Cacao Black Pod Rot (BPR). AuToDiDAC automatically detects, separates, and assesses the infection level of BPR in cacao through image processing and machine learning techniques. It gives the farmers the capacity to objectively monitor and report the infection level of the BPR compared to the common visual rating for plant disease level of infection. Pixel-level accuracy test of the tool showed an average of 84% accuracy on an independent test set of ten cacao pod images. © 2017","Black pod rot; Cacao; Decision support; Defect segmentation; Disease management tool; Infection level","cocoa; decision support system; detection method; fungal disease; host plant; image processing; infectious disease; machine learning; pest control; Theobroma cacao",2-s2.0-85030702212
"Zhu X., Skidmore A.K., Darvishzadeh R., Niemann K.O., Liu J., Shi Y., Wang T.","Foliar and woody materials discriminated using terrestrial LiDAR in a mixed natural forest",2018,"International Journal of Applied Earth Observation and Geoinformation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032197039&doi=10.1016%2fj.jag.2017.09.004&partnerID=40&md5=5325722a631b8fd03586bdd4be971f7a","Separation of foliar and woody materials using remotely sensed data is crucial for the accurate estimation of leaf area index (LAI) and woody biomass across forest stands. In this paper, we present a new method to accurately separate foliar and woody materials using terrestrial LiDAR point clouds obtained from ten test sites in a mixed forest in Bavarian Forest National Park, Germany. Firstly, we applied and compared an adaptive radius near-neighbor search algorithm with a fixed radius near-neighbor search method in order to obtain both radiometric and geometric features derived from terrestrial LiDAR point clouds. Secondly, we used a random forest machine learning algorithm to classify foliar and woody materials and examined the impact of understory and slope on the classification accuracy. An average overall accuracy of 84.4% (Kappa = 0.75) was achieved across all experimental plots. The adaptive radius near-neighbor search method outperformed the fixed radius near-neighbor search method. The classification accuracy was significantly higher when the combination of both radiometric and geometric features was utilized. The analysis showed that increasing slope and understory coverage had a significant negative effect on the overall classification accuracy. Our results suggest that the utilization of the adaptive radius near-neighbor search method coupling both radiometric and geometric features has the potential to accurately discriminate foliar and woody materials from terrestrial LiDAR data in a mixed natural forest. © 2017 Elsevier B.V.","Classification; Geometric feature; Radiometric feature; Radius search; Terrestrial laser scanning","algorithm; biomass; image classification; leaf area index; lidar; machine learning; national park; radiometer; scanner; Bavaria; Bavarian Forest National Park; Germany",2-s2.0-85032197039
"Zhang H., Xu R.","Exploring the optimal integration levels between SAR and optical data for better urban land cover mapping in the Pearl River Delta",2018,"International Journal of Applied Earth Observation and Geoinformation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032199129&doi=10.1016%2fj.jag.2017.08.013&partnerID=40&md5=7e2482c4168c139cb17eaaf51969fa40","Integrating synthetic aperture radar (SAR) and optical data to improve urban land cover classification has been identified as a promising approach. However, which integration level is the most suitable remains unclear but important to many researchers and engineers. This study aimed to compare different integration levels for providing a scientific reference for a wide range of studies using optical and SAR data. SAR data from TerraSAR-X and ENVISAT ASAR in both WSM and IMP modes were used to be combined with optical data at pixel level, feature level and decision levels using four typical machine learning methods. The experimental results indicated that: 1) feature level that used both the original images and extracted features achieved a significant improvement of up to 10% compared to that using optical data alone; 2) different levels of fusion required different suitable methods depending on the data distribution and data resolution. For instance, support vector machine was the most stable at both the feature and decision levels, while random forest was suitable at the pixel level but not suitable at the decision level. 3) By examining the distribution of SAR features, some features (e.g., homogeneity) exhibited a close-to-normal distribution, explaining the improvement from the maximum likelihood method at the feature and decision levels. This indicated the benefits of using texture features from SAR data when being combined with optical data for land cover classification. Additionally, the research also shown that combining optical and SAR data does not guarantee improvement compared with using single data source for urban land cover classification, depending on the selection of appropriate fusion levels and fusion methods. © 2017 Elsevier B.V.","Fusion level; Fusion strategies; Optical and SAR fusion; Urban land cover","data processing; Envisat; land classification; land cover; machine learning; mapping; optical method; optimization; synthetic aperture radar; TerraSAR-X; urban planning; China; Guangdong; Zhujiang Delta",2-s2.0-85032199129
"Das U.K., Tey K.S., Seyedmahmoudian M., Mekhilef S., Idris M.Y.I., Van Deventer W., Horan B., Stojcevski A.","Forecasting of photovoltaic power generation and model optimization: A review",2018,"Renewable and Sustainable Energy Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028016495&doi=10.1016%2fj.rser.2017.08.017&partnerID=40&md5=82faed9083400b04955efb667d1eede9","To mitigate the impact of climate change and global warming, the use of renewable energies is increasing day by day significantly. A considerable amount of electricity is generated from renewable energy sources since the last decade. Among the potential renewable energies, photovoltaic (PV) has experienced enormous growth in electricity generation. A large number of PV systems have been installed in on-grid and off-grid systems in the last few years. The number of PV systems will increase rapidly in the future due to the policies of the government and international organizations, and the advantages of PV technology. However, the variability of PV power generation creates different negative impacts on the electric grid system, such as the stability, reliability, and planning of the operation, aside from the economic benefits. Therefore, accurate forecasting of PV power generation is significantly important to stabilize and secure grid operation and promote large-scale PV power integration. A good number of research has been conducted to forecast PV power generation in different perspectives. This paper made a comprehensive and systematic review of the direct forecasting of PV power generation. The importance of the correlation of the input-output data and the preprocessing of model input data are discussed. This review covers the performance analysis of several PV power forecasting models based on different classifications. The critical analysis of recent works, including statistical and machine-learning models based on historical data, is also presented. Moreover, the strengths and weaknesses of the different forecasting models, including hybrid models, and performance matrices in evaluating the forecasting model, are considered in this research. In addition, the potential benefits of model optimization are also discussed. © 2017 Elsevier Ltd","Artificial intelligence; Hybrid model; Machine-learning; Optimization; PV power forecasting",,2-s2.0-85028016495
"Tharwat A., Gabel T., Hassanien A.E.","Parameter optimization of support vector machine using dragonfly algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029530556&doi=10.1007%2f978-3-319-64861-3_29&partnerID=40&md5=c6cfa254556fae368487ebe22e92cc3e","Support Vector Machine (SVM) parameters such as penalty and kernel parameters have a great influence on the complexity and accuracy of the classification model. In this paper, Dragonfly algorithm (DA) has been proposed to optimize the parameters of SVM; thus, the classification error can be decreased. To evaluate the proposed model (DA-SVM), the experiment adopted six standard datasets which are obtained from UCI machine learning data repository. For verification, the results of the DA-SVM algorithm are compared with two well-known optimization algorithms, namely, Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). The experimental results demonstrated that the proposed model is capable to find the optimal values of the SVM parameters and avoids the local optima problem. © 2018, Springer International Publishing AG.","Dragonfly Algorithm (DA); Parameter optimization; Support Vector Machine (SVM)",,2-s2.0-85029530556
"Aoun O., Sarhani M., El Afia A.","Hidden markov model classifier for the adaptive particle swarm optimization",2018,"Operations Research/ Computer Science Interfaces Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644063&doi=10.1007%2f978-3-319-58253-5_1&partnerID=40&md5=ccf3542fc272b263336727c63daee0ef","Particle swarm optimization (PSO) is a stochastic algorithm based population that integrates social interactions of animals in nature. Adaptive Particle swarm optimization (APSO) as an amelioration of the original one, improve the performance of global search and gives better efficiency. The APSO defines four evolutionary states: exploration, exploitation, convergence, and jumping out. According to the state, the inertia weight and acceleration coefficients are controlled. In this paper, we integrate Hidden Markov Model Particle swarm optimization (HMM) in APSO to have a stochastic state classification at each iteration. Furthermore, to tackle the problem of the dynamic environment during iterations, an additional online learning for HMM parameters is integrated into the algorithm using online Expectation-Maximization algorithm. We performed evaluations on ten benchmark functions to test the HMM integration inside APSO. Experimental results show that our proposed scheme outperforms other PSO variants in major cases regarding solution accuracy and specially convergence speed. © Springer International Publishing AG 2018.","Hidden markov model; Machine learning; Metaheuristics control; Parameters adaptation; Particle swarm optimization; Swarm intelligence",,2-s2.0-85032644063
"Narayan R., Rout J.K., Jena S.K.","Review spam detection using opinion mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644680&doi=10.1007%2f978-981-10-3376-6_30&partnerID=40&md5=8554936449bd278aa04c35effff9d6d7","Nowadays with the increasing popularity of Internet, online marketing is going to become more and more popular. This is because, a lot of products and services are easily available online. Hence, reviews about all these products and services are very important for customers as well as organizations. Unfortunately, driven by the will for profit or promotion, fraudsters used to produce fake reviews. These fake reviews written by fraudsters prevent customers and organizations reaching actual conclusions about the products. These fake reviews or review spam must be detected and eliminated so as to prevent deceptive potential customers. In this paper, we have applied supervised learning technique to detect review spam. The proposed work uses different set of features along with sentiment score to build models and their performance were evaluated using different classifiers. © Springer Nature Singapore Pte Ltd. 2018.","Machine learning; Opinion mining; Review spam; Sentiment analysis",,2-s2.0-85032644680
"Singh V., Verma N.K.","Deep learning architecture for high-level feature generation using stacked auto encoder for business intelligence",2018,"Studies in Systems, Decision and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032694589&doi=10.1007%2f978-3-319-69989-9_16&partnerID=40&md5=dbff08c5ca493fc0ec87d2208c5f69f2","In the era of modern world, faster development and wider use of digital technology generates large amount of data in digital space. Handling such large amount of data by conventional machine learning algorithms is difficult because of heterogeneous nature and large size of data. Deep learning strategy, is an advancement in machine learning research to deal with such heterogeneous nature and large size of data and extract high-level representations of data through a hierarchical learning process. This paper proposes novel multi-layer feature selection with conjunction of Stacked Auto-Encoder (SAE) to extract high level features or representations and eliminate the lower level features or representations from data. The proposed approach is validated on the Farm Ads dataset and the result is compared with various conventional machine learning algorithms. The proposed approach has outperformed as compared to conventional machine learning algorithms for the given dataset. © 2018, Springer International Publishing AG.","Business data; Deep learning; Feature selection; Stacked auto-encoder",,2-s2.0-85032694589
"Holmes D.E., Jain L.C.","Advances in Biomedical Informatics: An Introduction",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032017483&doi=10.1007%2f978-3-319-67513-8_1&partnerID=40&md5=329e959dccf9b9f11fa4eb4eb47bd91c","This chapter presents a summary of a sample of research in the field of biomedical informatics. The topics include digital health research, medical decision support systems, Bayesian networks, tele-monitoring, preprocessing in high dimensional datasets. © Springer International Publishing AG 2018.","Bayesian network; Big data; Biomedical informatics; Decision support systems; Digital health; Machine learning; Medical diagnosis; Telemonitoring",,2-s2.0-85032017483
"Ibrahim S., Djemal R., Alsuwailem A.","Electroencephalography (EEG) signal processing for epilepsy and autism spectrum disorder diagnosis",2018,"Biocybernetics and Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806430&doi=10.1016%2fj.bbe.2017.08.006&partnerID=40&md5=541a8a55a40aa0f6cbcdf45a93147b01","Quantification of abnormality in brain signals may reveal brain conditions and pathologies. In this study, we investigate different electroencephalography (EEG) feature extraction and classification techniques to assist in the diagnosis of both epilepsy and autism spectrum disorder (ASD). First, the EEG signal is pre-processed to remove major artifacts before being decomposed into several EEG sub-bands using a discrete-wavelet-transform (DWT). Two nonlinear methods were studied, namely, Shannon entropy and largest Lyapunov exponent, which measure complexity and chaoticity in the EEG recording, in addition to the two conventional methods (namely, standard deviation and band power). We also study the use of a cross-correlation approach to measure synchronization between EEG channels, which may reveal abnormality in communication between brain regions. The extracted features are then classified using several classification methods. Different EEG datasets are used to verify the proposed design exploration techniques: the University of Bonn dataset, the MIT dataset, the King Abdulaziz University dataset, and our own EEG recordings (46 subjects). The combination of DWT, Shannon entropy, and k-nearest neighbor (KNN) techniques produces the most promising classification result, with an overall accuracy of up to 94.6% for the three-class (multi-channel) classification problem. The proposed method obtained better classification accuracy compared to the existing methods and tested using larger and more comprehensive EEG dataset. The proposed method could potentially be used to assist epilepsy and ASD diagnosis therefore improving the speed and the accuracy. © 2017","Autism; Computer aided diagnosis; EEG; Epilepsy; Machine learning; Neurological disorder",,2-s2.0-85032806430
"Swan J., De Causmaecker P., Martin S., Özcan E.","A re-characterization of hyper-heuristics",2018,"Operations Research/ Computer Science Interfaces Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032626387&doi=10.1007%2f978-3-319-58253-5_5&partnerID=40&md5=3c916454ddf37957e4e3075e5743839a","Hyper-heuristics are an optimization methodology which ‘search the space of heuristics’ rather than directly searching the space of the underlying candidate-solution representation. Hyper-heuristic search has traditionally been divided into two layers: a lower problem-domain layer (where domain-specific heuristics are applied) and an upper hyper-heuristic layer, where heuristics are selected or generated. The interface between the two layers is commonly termed the “domain barrier”. Historically this interface has been defined to be highly restrictive, in the belief that this is required for generality. We argue that this prevailing conception of domain barrier is so limiting as to defeat the original motivation for hyper-heuristics. We show how it is possible to make use of domain knowledge without loss of generality and describe generalized hyper-heuristics which can incorporate arbitrary domain knowledge. © Springer International Publishing AG 2018.","Constraint programming; Hyper-heuristics; Machine learning; Metaheuristics; Optimization",,2-s2.0-85032626387
"Shah N.F., Kumar P.","A comparative analysis of various spam classifications",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032635243&doi=10.1007%2f978-981-10-3376-6_29&partnerID=40&md5=6838a34d89b5d00afa1f5080a08db150","Bandwidth, time, and storage space are the major three assets in computational world. Spam emails affect all the three, thus degrade the overall efficiency of the system. Spammers are using new tricks and traps to land these frivolous mails into our inbox. To make mailboxes more intelligent, our effort will be to devise a new algorithm that will help to classify emails in much smarter and efficient way. This paper analyzes various spam classification techniques and thereby put forward a new way of classifying spam emails. This paper thoroughly compares the results that various authors have got while simulating their architectures. Our approach of classification works efficiently and more accurately on varied length and type of datasets during training and testing phases. We tried to minimize the error ratio and increase classifier efficiency by implementing Genetic Algorithm concept. © Springer Nature Singapore Pte Ltd. 2018.","Feature set; Genetic algorithm; Logistic regression; Machine learning; Spam classification; Spam email; Unsolicited",,2-s2.0-85032635243
"Di Luca A., Hamill R., Mullen A.M., Elia G.","Dige analysis of animal tissues",2018,"Methods in Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031405508&doi=10.1007%2f978-1-4939-7268-5_12&partnerID=40&md5=885b1b73e64abda507d81f193318872c","Two-dimensional difference gel electrophoresis (2D-DIGE) is an acrylamide gel electrophoresis-based technique for protein separation and quantification in complex mixtures. The technique addresses some of the drawbacks of conventional 2D polyacrylamide gel electrophoresis (2D-PAGE), offering improved sensitivity, more limited experimental variation and accurate within-gel matching. DIGE is based on direct labeling of proteins with isobaric fluorescent dyes (known as CyDyes: Cy2, Cy3, and Cy5) prior to isoelectric focusing (IEF). Here, up to two samples and a reference pool (internal standard) can be mixed and loaded onto IEF for first dimension prior to SDS-PAGE separation in the second dimension. After the electrophoretic run, the gel is imaged at the specific excitation wavelength for each dye, in sequence, and gel scans are recorded separately. For each individual protein spot, intensities recorded at the different wavelengths are integrated and the ratio between volumes normalized to that of the internal standard. This provides an immediate appreciation of protein amount variations under the different conditions tested. In addition, proteins of interest can still be excised and identified with conventional mass spectrometry techniques and further analyzed by other biochemical methods. In this chapter, we describe the application of this methodology to separation and quantitation of proteins mixtures from porcine muscle exudate, collected following centrifugation of muscle specimens (centrifugal drip) for the characterization of quality parameters of importance in the meat industry. © Springer Science+Business Media LLC 2018.","2D-DIGE; Centrifugal drip; CyDye DIGE fluor; Image analysis; Internal standard; Isoelectric focusing; Machine learning algorithm; Mass spectrometry; Porcine muscle exudate; SDS-PAGE",,2-s2.0-85031405508
"Sarhani M., El Afia A., Faizi R.","Facing the feature selection problem with a binary PSO-GSA approach",2018,"Operations Research/ Computer Science Interfaces Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032615163&doi=10.1007%2f978-3-319-58253-5_26&partnerID=40&md5=45163b8637f55b8af04f902f4f33afc2","Feature selection has become the focus of much research in many areas where we can face the problem of big data or complex relationship among features. Metaheuristics have gained much attention in solving many practical problems, including feature selection. Our contribution in this paper is to propose a binary hybrid metaheuristic to minimize a fitness function representing a trade-off between the classification error of selecting the feature subset and the corresponding number of features. This algorithm combines particle swarm optimization (PSO) and gravitational search algorithm (GSA). Also, a mutation operator is integrated to enhance population diversity. Experimental results on ten benchmark dataset show that our proposed hybrid method for feature selection can achieve high performance when comparing with other metaheuristic algorithms and well-known feature selection approaches. © Springer International Publishing AG 2018.","Feature selection; Gravitational search algorithm; Machine learning; Metaheuristics; Particle swarm optimization",,2-s2.0-85032615163
"Friedrich T., Bochmann A., Dinger J., Teichert S.","Application of the pattern matching approach for EBSD calibration and orientation mapping, utilising dynamical EBSP simulations",2018,"Ultramicroscopy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032288058&doi=10.1016%2fj.ultramic.2017.10.006&partnerID=40&md5=7812a0388f6a840cfb6dddc54d68f128","In this paper an alternative method of off-line Kikuchi pattern centre calibration and orientation mapping, utilising the cross-correlation between entire experimental patterns and dynamical simulated patterns is applied and evaluated. To demonstrate the improvement in angular resolution compared to Hough transform based methods, EBSD datasets of a silicon monocrystal were analysed using both, classical and the presented cross-correlation based method, which revealed significant enhancement of angular resolution for the refined method. The mean misorientation over the monocrystalline sample was found to be up to one order of magnitude lower compared to common methods, with an angular resolution of up to 0.06° indicating a substantial gain in orientation precision. The pattern centres were determined for a number of patterns on the map, using pattern matching refinement as well. Subsequently, a multiple linear regression model was computed to correlate pattern centre positions (XPC, YPC) and detector distances (ZSSD) to x- and y-coordinates on the map by means of plane equations. Employing this method, a reduction of orientation noise was achieved in highly deformed Silicon crystals with large intragranular orientation ranges. Furthermore, it was shown that the cross correlation coefficient CC can be used as a parameter indicating the pattern quality and hence can be utilised to create a pseudo greyscale image of the surface, showing grain boundaries and also depicting lattice distortions. © 2017 Elsevier B.V.","Angular resolution; Calibration; Cross-correlation; Dynamical simulation; EBSD; Pattern matching","Calibration; Grain boundaries; Hough transforms; Linear regression; Mapping; Pattern matching; Regression analysis; Angular resolution; Cross correlations; Cross-correlation coefficient; Dynamical simulation; EBSD; Multiple linear regression models; Orientation mapping; Transform-based methods; Crystal orientation; silicon; angular resolution; Article; calibration; controlled study; correlation coefficient; crystal structure; dynamics; electron backscatter diffraction; electron backscatter pattern; Hough transform based method; machine learning; multiple linear regression analysis; orientation mapping; physical parameters; simulation; structure analysis",2-s2.0-85032288058
"Fomin I., Gromoshinskii D., Bakhshiev A.","Object Detection on Images in Docking Tasks Using Deep Neural Networks",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029223844&doi=10.1007%2f978-3-319-66604-4_12&partnerID=40&md5=9b89b7afa0f605aeb0c966d46124d24f","In process of docking of automated apparatus there is a problem of determining of them relative position. This problem may be effectively solved with algorithms for relative position calculation, based on television picture formed by camera, installed on one apparatus and observing another one, or docking position. Apparatus position and orientation calculates using visual landmarks positions and information about 3D configuration of observing object and visual landmarks’ relative positions. Visual landmarks detection algorithm is the crucial part of such solution. Study of ability of application of object detection system based on deep convolutional neural network to task of visual landmark detection will be discussed in this article. As an example, detection of visual landmarks on space docking images will be discussed. Neural network based detection system learned using images of International Space Station received in process of docking of cargo spacecrafts will be represented. © 2018, Springer International Publishing AG.","Computer vision; Convolutional neural networks; Deep neural networks; Faster R-CNN; Machine learning; Object detection",,2-s2.0-85029223844
"Verhoeven T., Coito A., Plomp G., Thomschewski A., Pittau F., Trinka E., Wiest R., Schaller K., Michel C., Seeck M., Dambre J., Vulliemoz S., van Mierlo P.","Automated diagnosis of temporal lobe epilepsy in the absence of interictal spikes",2018,"NeuroImage: Clinical",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030110166&doi=10.1016%2fj.nicl.2017.09.021&partnerID=40&md5=e9810611f963556b0edaf4410e3bc8f3","Objective To diagnose and lateralise temporal lobe epilepsy (TLE) by building a classification system that uses directed functional connectivity patterns estimated during EEG periods without visible pathological activity. Methods Resting-state high-density EEG recording data from 20 left TLE patients, 20 right TLE patients and 35 healthy controls was used. Epochs without interictal spikes were selected. The cortical source activity was obtained for 82 regions of interest and whole-brain directed functional connectivity was estimated in the theta, alpha and beta frequency bands. These connectivity values were then used to build a classification system based on two two-class Random Forests classifiers: TLE vs healthy controls and left vs right TLE. Feature selection and classifier training were done in a leave-one-out procedure to compute the mean classification accuracy. Results The diagnosis and lateralization classifiers achieved a high accuracy (90.7% and 90.0% respectively), sensitivity (95.0% and 90.0% respectively) and specificity (85.7% and 90.0% respectively). The most important features for diagnosis were the outflows from left and right medial temporal lobe, and for lateralization the right anterior cingulate cortex. The interaction between features was important to achieve correct classification. Significance This is the first study to automatically diagnose and lateralise TLE based on EEG. The high accuracy achieved demonstrates the potential of directed functional connectivity estimated from EEG periods without visible pathological activity for helping in the diagnosis and lateralization of TLE. © 2017","Diagnosis; EEG; Lateralization; Machine learning; Temporal lobe epilepsy","accuracy; anterior cingulate; Article; clinical article; comparative study; controlled study; electroencephalography; functional connectivity; human; left hippocampus; medial temporal lobe; priority journal; random forest; retrospective study; right hippocampus; sensitivity and specificity; temporal lobe epilepsy",2-s2.0-85030110166
"Jakubik J.","Evaluation of gated recurrent neural networks in music classification tasks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029501887&doi=10.1007%2f978-3-319-67220-5_3&partnerID=40&md5=b2999dab5ed13fcd27a314a663d2f0c5","In this paper, we evaluate two popular Recurrent Neural Network (RNN) architectures employing the mechanism of gating: Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU), in music classification tasks. We examine the performance on four datasets concerning genre, emotion and dance style recognition. Our key result is a significant improvement of classification accuracy achieved by training the recurrent network on random short subsequences of the vector sequences in the training set. We examine the effect of this training approach on both architectures and discuss the implications for the potential use of RNN in music information retrieval. © 2018, Springer International Publishing AG.","Artificial intelligence; Machine learning; Music Information Retrieval; Recurrent Neural Network",,2-s2.0-85029501887
"LeMoyne R., Mastroianni T.","Quantification systems appropriate for a clinical setting",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032341808&doi=10.1007%2f978-981-10-5684-0_3&partnerID=40&md5=0266fbd9b500ece8a2d5ad58015492c2","Conventional gait quantification is provided in a highly structured clinical setting. These devices represent a metaphorical second wave encompassing clinically standard quantification techniques. Traditional gait quantification systems, such as force plates, EMG, foot-switches, and motion capture systems are described in the chapter for gait analysis. Their relevance for objectively quantifying the status of a patient’s rehabilitation progress is advocated. Regarding reflex quantification the application of motion capture systems, EMG, and strain/force sensors are covered in the chapter. There are drawbacks of these devices, such as expense, complexity, and limitations to a clinical setting. By contrast, wearable and wireless systems are projected to transcend the capabilities of these traditional quantification systems with expanded autonomy for subject evaluation in the context of Network Centric Therapy. © 2018, Springer Nature Singapore Pte Ltd.","Electrogoniometers; Electromyogram (EMG); Foot switches; Force plates; Gait analysis; Machine learning; Metabolic analysis; Optical motion cameras; Quantification; Reflex response; Tendon reflex",,2-s2.0-85032341808
"Ferrarelli P., Lázaro M.T., Iocchi L.","Design of robot teaching assistants through multi-modal human-robot interactions",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029404175&doi=10.1007%2f978-3-319-62875-2_25&partnerID=40&md5=0d2a8b6744f4e358a90976b492867e77","The interest on introducing robots in schools has increased significantly in recent years. Robots in these environments are managed by educators who design teaching activities where the students can consolidate the knowledge acquired in the classroom by interacting with the robot. In this context, the use of multiple modalities of communication can become a determining factor to achieve the success of the interaction and a better learning experience. However, the design of such multi-modal interactions can be a complex and time-consuming process, specially for teachers lacking of technical expertise. In this paper, we propose a formalism for the description of multi-modal interactions based on the use of interaction templates which facilitates the design and management of the multi-modal behaviour by non-expert users (i.e., teachers). We provide an example of application of our approach on the educational context, using an autonomous robot as a teaching assistant, showing the usability and extendability of our system. © 2018, Springer International Publishing AG.","Human-robot interaction; Interaction design; Learning with robots; Multi-modal interaction; Teaching with robots","Education; Human computer interaction; Machine design; Man machine systems; Robotics; Robots; Students; Teaching; Design teaching; Educational context; Interaction design; Learning experiences; Multi-Modal Interactions; Multiple modalities; Teaching assistants; Technical expertise; Human robot interaction",2-s2.0-85029404175
"Qian P., Xi C., Xu M., Jiang Y., Su K.-H., Wang S., Muzic R.F., Jr.","SSC-EKE: Semi-supervised classification with extensive knowledge exploitation",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028954207&doi=10.1016%2fj.ins.2017.08.093&partnerID=40&md5=cde837f6ffcd3557407f75dd02b0d4fd","We introduce a new, semi-supervised classification method that extensively exploits knowledge. The method has three steps. First, the manifold regularization mechanism, adapted from the Laplacian support vector machine (LapSVM), is adopted to mine the manifold structure embedded in all training data, especially in numerous label-unknown data. Meanwhile, by converting the labels into pairwise constraints, the pairwise constraint regularization formula (PCRF) is designed to compensate for the few but valuable labelled data. Second, by further combining the PCRF with the manifold regularization, the precise manifold and pairwise constraint jointly regularized formula (MPCJRF) is achieved. Third, by incorporating the MPCJRF into the framework of the conventional SVM, our approach, referred to as semi-supervised classification with extensive knowledge exploitation (SSC-EKE), is developed. The significance of our research is fourfold: 1) The MPCJRF is an underlying adjustment, with respect to the pairwise constraints, to the graph Laplacian enlisted for approximating the potential data manifold. This type of adjustment plays the correction role, as an unbiased estimation of the data manifold is difficult to obtain, whereas the pairwise constraints, converted from the given labels, have an overall high confidence level. 2) By transforming the values of the two terms in the MPCJRF such that they have the same range, with a trade-off factor varying within the invariant interval [0, 1), the appropriate impact of the pairwise constraints to the graph Laplacian can be self-adaptively determined. 3) The implication regarding extensive knowledge exploitation is embodied in SSC-EKE. That is, the labelled examples are used not only to control the empirical risk but also to constitute the MPCJRF. Moreover, all data, both labelled and unlabelled, are recruited for the model smoothness and manifold regularization. 4) The complete framework of SSC-EKE organically incorporates multiple theories, such as joint manifold and pairwise constraint-based regularization, smoothness in the reproducing kernel Hilbert space, empirical risk minimization, and spectral methods, which facilitates the preferable classification accuracy as well as the generalizability of SSC-EKE. © 2017","Graph Laplacian; Knowledge; Manifold learning; Reproducing kernel Hilbert space (RKHS); Semi-supervised classification; Support vector machine (SVM)","Economic and social effects; Hilbert spaces; Laplace transforms; Support vector machines; Vector spaces; Graph Laplacian; Knowledge; Manifold learning; Reproducing Kernel Hilbert spaces; Semi-supervised classification; Supervised learning",2-s2.0-85028954207
"Wiercioch M.","Feature selection in texts",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019213117&doi=10.1007%2f978-3-319-59162-9_35&partnerID=40&md5=7a8b2df79eb5e7bf52ca3c03c577e1b4","Feature selection is used in many application areas relevant to expert and intelligent systems, such as machine learning, data mining, cheminformatics and natural language processing. In this study we propose methods for feature selection and features analysis based on Support Vector Machines (SVM) with linear kernels. We explore how these techniques can be used to obtain some interesting information for further exploration of text data. The results provide satisfactory observations which may lead to progress in feature selection field. © Springer International Publishing AG 2018.","Dimension reduction; Feature selection; Support vector machines; Text classification","Classification (of information); Data mining; Intelligent systems; Learning algorithms; Learning systems; Natural language processing systems; Support vector machines; Text processing; Application area; Cheminformatics; Dimension reduction; Interesting information; Linear kernel; NAtural language processing; Text classification; Text data; Feature extraction",2-s2.0-85019213117
"Ferreira R., Cavalcanti G.D.C., Freitas F., Lins R.D., Simske S.J., Riss M.","Combining sentence similarities measures to identify paraphrases",2018,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025124168&doi=10.1016%2fj.csl.2017.07.002&partnerID=40&md5=4bef24cc43644845eb40dd87def061ab","Paraphrase identification consists in the process of verifying if two sentences are semantically equivalent or not. It is applied in many natural language tasks, such as text summarization, information retrieval, text categorization, and machine translation. In general, methods for assessing paraphrase identification perform three steps. First, they represent sentences as vectors using bag of words or syntactic information of the words present the sentence. Next, this representation is used to measure different similarities between two sentences. In the third step, these similarities are given as input to a machine learning algorithm that classifies these two sentences as paraphrase or not. However, two important problems in the area of paraphrase identification are not handled: (i) the meaning problem: two sentences sharing the same meaning, composed of different words; and (ii) the word order problem: the order of the words in the sentences may change the meaning of the text. This paper proposes a paraphrase identification system that represents each pair of sentence as a combination of different similarity measures. These measures extract lexical, syntactic and semantic components of the sentences encompassed in a graph. The proposed method was benchmarked using the Microsoft Paraphrase Corpus, which is the publicly available standard dataset for the task. Different machine learning algorithms were applied to classify a sentence pair as paraphrase or not. The results show that the proposed method outperforms state-of-the-art systems. © 2018 Elsevier Ltd","Graph-based model; Paraphrase identification; Sentence similarity; Sentence simplification","Artificial intelligence; Education; Learning systems; Natural language processing systems; Semantics; Syntactics; Text processing; Graph-based modeling; Machine translations; Paraphrase identifications; Semantic components; Sentence similarity; Sentence simplification; State-of-the-art system; Syntactic information; Learning algorithms",2-s2.0-85025124168
"Prabhu G., Ahmadi A., O’Connor N.E., Moran K.","Activity Recognition of Local Muscular Endurance (LME) Exercises Using an Inertial Sensor",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029595481&doi=10.1007%2f978-3-319-67846-7_4&partnerID=40&md5=13faae44e57bd0309d27f28bfece2e57","In this paper, we propose an algorithmic approach for a motion analysis framework to automatically recognize local muscular endurance (LME) exercises and to count their repetitions using a wrist-worn inertial sensor. LME exercises are prescribed for cardiovascular disease rehabilitation. As a technical solution, we propose activity recognition based on machine learning. We developed an algorithm to automatically segment the captured data from all participants. Relevant time and frequency domain features were extracted using a sliding window technique. Principal component analysis (PCA) was applied for dimensionality reduction of the extracted features. We trained 15 binary classifiers using support vector machine (SVM) to recognize individual LME exercises, achieving overall accuracy of more than 98%. We applied grid search technique to obtain the optimal SVM hyperplane parameters. The learning curves (mean ± stdev) for each model is investigated to verify that the models were not over-fitted and performed well on any new test data. Also, we devised a method to count the repetitions of the upper body exercises. © 2018, Springer International Publishing AG.","Cardiovascular disease; Human activity recognition; Local muscular endurance; Principle component analysis; Support vector machine","Cardiology; Diseases; Frequency domain analysis; Inertial navigation systems; Learning systems; Pattern recognition; Sports; Support vector machines; Activity recognition; Algorithmic approach; Cardio-vascular disease; Dimensionality reduction; Human activity recognition; Principle component analysis; Sliding window techniques; Time and frequency domains; Principal component analysis",2-s2.0-85029595481
"Zhao J., Hei X., Shi Z., Dong L., Liu Y., Yan R., Li X.","Regression learning based on incomplete relationships between attributes",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029445554&doi=10.1016%2fj.ins.2017.09.023&partnerID=40&md5=68bc8ba9680a10683af2b1ca642f64e4","In recent years, machine learning researchers have focused on methods to construct flexible and interpretable regression models. However, the method of obtaining complete knowledge from incomplete and fuzzy prior knowledge and the trade-off between the generalization performance and the interpretability of the model are very important factors to consider. In this paper, we propose a new regression learning method. Complete relationships are obtained from the incomplete fuzzy relationships between attributes by using Markov logic networks [29]. The complete relationships are then applied to constrain the shape of the regression model in the optimization procedure to solve the trade-off problem. Finally, the benefits of our approach are illustrated on benchmark data sets and in real-world experiments. © 2017 Elsevier Inc.","Attribute relationships; First-order predicate; Interpretability; Markov logic network; Regression problem","Computer circuits; Economic and social effects; Learning systems; Markov processes; Probabilistic logics; Attribute relationships; First order; Interpretability; Markov logic networks; Regression problem; Regression analysis",2-s2.0-85029445554
"Prabhakar S.K., Rajaguru H.","Expectation maximization based PCA and hessian LLE with suitable post classifiers for epilepsy classification from EEG signals",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606303&doi=10.1007%2f978-3-319-60618-7_36&partnerID=40&md5=546d1d5f33c18c95ce93dc779cae2f29","Brain is a highly complicated structure which contains billions of neurons which help in maintaining the electrical charges of the system. Due to the sudden and unexpected electrical discharges occurring in the brain, epilepsy occurs and is a most commonly occurring neurological disorder next to stroke. For the easy monitoring of the activities of the brain, Electroencephalography (EEG) is widely used. For the primary purpose of assessment of brain activities, EEG serves as a valuable treasure and an indispensable tool. The various information related to the physiological state of the brain can be bought out by EEG. For the analysis of the EEG records in a visual manner, expert neurologists consume more time and to detect the epileptic seizures, only EEG signals are used widely as it contains vital information. The EEG recording contains a lot of noise and so it is difficult to isolate the seizures from other artifacts with resembles more or less similar time-frequency patterns. Various automatic detection and machine learning algorithms have been used to predict the risk of epileptic seizures in raw EEG signals. In this study, the dimensions of the EEG signals are reduced initially with the help of two approaches, namely, Expectation Maximization Based Principal Component Analysis (EM-PCA) approach and Hessian Local Linear Embedding (HLLE) approach. The dimensionally reduced values are then classified to predict the risk of epilepsy from EEG signals with the help of two post classifiers namely Weighted K Nearest Neighbour (WKNN) and Linear Support Vector Machine (L-SVM). The performance metrics are analyzed in terms of various measures like Performance Index, Accuracy, Time Delay, Specificity and Sensitivity. The study shows that the best result is obtained when HLLE is used as a dimensionality reduction technique and classified with WKNN as it gives the highest perfect classification rate with an average accuracy of 97.743%. © Springer International Publishing AG 2018.","EEG; EM-PCA; Epilepsy; HLLE; L-SVM; WKNN","Brain; Electric discharges; Electroencephalography; Electrophysiology; Learning algorithms; Learning systems; Maximum principle; Nearest neighbor search; Neurology; Neurophysiology; Pattern recognition; Principal component analysis; Soft computing; Support vector machines; Time delay; EM-PCA; Epilepsy; HLLE; L-SVM; WKNN; Biomedical signal processing",2-s2.0-85028606303
"Du Y., Wang H., Qiu S., Zhang J., Xie P.","Continuous prediction of joint angle of lower limbs from sEMG signals",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030839510&doi=10.1007%2f978-981-10-6496-8_61&partnerID=40&md5=0b6ef2489ff827263d7e4a0072ed6c57","In order to realize the rehabilitation training of mirror movement in stroke patients, a new motion analysis method of EMG signal is proposed. First, surface electromyography (sEMG), hip joint and knee joint angles of 6 lower limb muscles are collected synchronously. Then, by introducing the coherence analysis and calculating the significant area index, the coupling relationship between the sEMG and the joint angle is quantitatively described, and the muscles of the most coupling relationship are set to the input channels of the model. Next, we introduce the least squares extreme learning machine algorithm based on golden section (GS-LSELM), and establish a nonlinear prediction model between sEMG and joint angle. Finally, the experimental results show that the proposed method can quickly build the model under different motion periods, and it could be used in the tracking control of the rehabilitation robot. © 2018, Springer Nature Singapore Pte Ltd.","Angle prediction; Coherence analysis; GS-LSELM; Motion analysis; sEMG","Biological organs; Electromyography; Forecasting; Intelligent systems; Learning systems; Motion analysis; Muscle; Patient rehabilitation; Coherence analysis; Coupling relationships; Extreme learning machine; GS-LSELM; Nonlinear prediction model; Rehabilitation training; sEMG; Surface electromyography; Joints (anatomy)",2-s2.0-85030839510
"Yousefi B., Loo C.K.","A dual fast and slow feature interaction in biologically inspired visual recognition of human action",2018,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032451331&doi=10.1016%2fj.asoc.2017.10.021&partnerID=40&md5=c373e90fa2f1b785cce5faa7be6a2875","Computational neuroscience studies have examined the human visual system through functional magnetic resonance imaging (fMRI) and identified a model where the mammalian brain pursues two independent pathways for recognizing biological movement tasks. On the one hand, the dorsal stream analyzes the motion information by applying optical flow, which considers the fast features. On the other hand, the ventral stream analyzes the form information with slow features. The proposed approach suggests that the motion perception of the human visual system comprises fast and slow feature interactions to identify biological movements. The form features in the visual system follow the application of the active basis model (ABM) with incremental slow feature analysis (IncSFA). Episodic observation is required to extract the slowest features, whereas the fast features update the processing of motion information in every frame. Applying IncSFA provides an opportunity to abstract human actions and use action prototypes. However, the fast features are obtained from the optical flow division, which gives an opportunity to interact with the system as the final recognition is performed through a combination of the optical flow and ABM-IncSFA information and through the application of kernel extreme learning machine. Applying IncSFA into the ventral stream and involving slow and fast features in the recognition mechanism are the major contributions of this research. The two human action datasets for benchmarking (KTH and Weizmann) and the results highlight the promising performance of this approach in model modification. © 2017 Elsevier B.V.","Biologically inspired model; Dual processing pathways; Extreme learning machine; Human action recognition; Incremental slow feature analysis; Interaction between ventral and dorsal stream","Benchmarking; Image processing; Image recognition; Knowledge acquisition; Learning systems; Magnetic resonance imaging; Mammals; Neural networks; Optical flows; Biologically inspired models; Dorsal stream; Dual processing pathways; Extreme learning machine; Human-action recognition; Incremental slow feature analysis; Functional neuroimaging",2-s2.0-85032451331
"Agarwal M., Srivastava G.M.S.","A cuckoo search algorithm-based task scheduling in cloud computing",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031407915&doi=10.1007%2f978-981-10-3773-3_29&partnerID=40&md5=0e59ccf0cafd61e1446ac6d6cbdc8ba7","Recently, Cloud computing emerges out as a latest technology which enables an organization to use the computing resources like hardware, applications, and software, etc., to perform the computation over the internet. Cloud computing gain so much attention because of advance technology, availability, and cost reduction. Task scheduling in cloud computing emerges out as new area of research which attracts the attention of lots researchers. An effective task scheduling is always required for optimum or efficient utilization of the computing resources to avoid the situation of over or under-utilization of such resources. Through this paper, we are going to proposed the cuckoo search-based task scheduling approach which helps in distributing the tasks efficiently among the available virtual machines (VM’s) and also keeps the overall response time (QoS) minimum. This algorithm assigns the tasks among the virtual machines on the basis of their processing power, i.e., million instructions per seconds (MIPS) and length of the tasks. A comparison of cuckoo search algorithm is done with the first—in first—out (FIFO) and greedy-based scheduling algorithm which is performed using the CloudSim simulator, the results clearly shows that cuckoo search outperforms the other algorithms. © Springer Nature Singapore Pte Ltd. 2018.","Cloud computing; Cuckoo search; Genetic algorithm; Makespan; QoS; Task scheduling; Virtual machines","Application programs; Cloud computing; Cost reduction; Genetic algorithms; Learning algorithms; Multitasking; Network function virtualization; Network security; Optimization; Quality of service; Virtual machine; Computing resource; Cuckoo search algorithms; Cuckoo searches; Latest technology; Makespan; Million instructions per seconds; Processing power; Task-scheduling; Scheduling algorithms",2-s2.0-85031407915
"Pacheco-Sánchez J.H., Vera-Torres R.D., Alejo R.","Bayesian Learning on Discrete Systems of Two Classes",2018,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028777595&doi=10.1142%2fS0218001418600133&partnerID=40&md5=d27bd63e723487f8c4c3918c65433d27","Bayesian learning is applied on two class systems. Partitioning a big sample made up of many elements of two classes of indistinguishable objects, we indistinctly pursue from 2 to 5 training sets called hypotheses in the probability field, with a plausible rate of object from each hypothesis. Objects are taken one by one from the sample. The basic aim faced is to predict one type of objects in the following occasion in which an agent takes one of them from the original sample to test it. We obtain the graph of a posteriori probability for each hypothesis of one of the objects. A prediction that the following object is specifically one of them is acquired in one probability curve by means of training previously accomplished. This methodology is applied on manufacture of glass bottles of two classes: good or crash. The main interest is to predict which machine produced one detected crash bottle because bottles turn to be indistinguishable when they are reviewed. This is solved by fixing a priori probabilities and taking into account all possible probability distribution combinations in the classes. © 2018 World Scientific Publishing Company.","Bayesian learning; prediction methods; probabilistic inference","Bottles; Forecasting; Glass bottles; Probability; A-posteriori probabilities; A-priori probabilities; Bayesian learning; Discrete systems; Original sample; Prediction methods; Probabilistic inference; Probability curves; Probability distributions",2-s2.0-85028777595
"Weßel G., Schreck C., Altendorf E., Canpolat Y., Flemisch F.","Learning from the best – naturalistic arbitration for cooperative driving",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022346884&doi=10.1007%2f978-3-319-60441-1_61&partnerID=40&md5=83aeee68d6811e3fff33889c16d6ee0f","In cooperative automated driving, the task of lateral and longitudinal vehicle control can be shared by driver and automation. However, conflicting action intentions of the two partners could arise, which need to be resolved within limited time. This can be achieved through structured multimodal negotiation, called arbitration. In order to explore intuitive interaction patterns for arbitration situations, insights from human-human interaction might be transferred. Accordingly, in a field study, couples holding hands or walking arm in arm were videotaped and interviewed when a conflict concerning motion control has been observed. The analysis of the data shows that conflict situations concerning velocity and/or direction of movement occur in natural human-human interaction and that these types of conflict can be dependent on each other. Furthermore, partners use different interaction resources to successfully solve these situations. Results are transferred to cooperative automated driving and an example of an interaction pattern is presented. © Springer International Publishing AG 2018.","Arbitration; Automated driving; Cooperative guidance and control; Cooperative movement; Human systems integration; Human-machine cooperation","Automation; Control system synthesis; Human engineering; Video recording; Arbitration; Automated driving; Cooperative movement; Guidance and control; Human systems integration; Human-machine cooperation; Human computer interaction",2-s2.0-85022346884
"Dobriborsci D., Bazylev D., Margun A.","Teaching students the basics of control theory using NI ELVIS ii",2018,"Smart Innovation, Systems and Technologies",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020390955&doi=10.1007%2f978-3-319-59451-4_42&partnerID=40&md5=2fa9d0a311f0b005a1517e9886205304","This paper describes a remote lab, currently available on the Department of Control Systems and Computer Science in ITMO University. The remote lab allows students with disabilities to gain knowledge in Electrical Engineering and Electronics, Control Theory, Systems Identification and etc. In this paper the developed laboratory setup for DC motor control is described. The paper shows the model of DC motor and designing the proportional-integral-differential controller tuned using Ziegler-Nichols and modal control approaches. © Springer International Publishing AG 2018.","Active learning; DC motor; Education; Modal control; NI ELVIS; Remote lab; Ziegler-Nichols","Computer control systems; Control theory; DC motors; Distance education; E-learning; Education; Education computing; Laboratories; Students; Two term control systems; Active Learning; Modal control; Ni elvis; Remote-labs; Ziegler Nichols; Electric machine control",2-s2.0-85020390955
"Pachalag V., Malhotra A.","Internet of emotions: Emotion management using affective computing",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028416772&doi=10.1007%2f978-3-319-63645-0_63&partnerID=40&md5=6a28ac4d05b198c907a8c8fae5644458","The many advantages of increase in Human Machine Interaction are obvious but it has also led to issues such as emotional imbalance, depression, reduction in interpersonal communication etc. Internet of Emotions can be broadly categorized as internet based technologies which aim to mitigate these problems and facilitate better Human to Human interaction in real world. IoE can be defined as an ecosystem where emotion packets travel via internet to manage user’s real time experience. We propose a system which will detect emotional state of the user, categorize it and actuate outer net elements to manage the emotion of the user. Detailed algorithm is given which includes use of the passive sensors, smartphone, big data analytics and machine learning. The framework is further explained with example of stress management. The proposed system based on affective computing will play a vital role in development of products and platforms which emphasises user involvement. © Springer International Publishing AG 2018.","Affective computing; Algorithm; Design; Emotion; Internet of Things; Sensors; Stress; System","Algorithms; Big data; Design; Intelligent systems; Internet; Internet of things; Learning systems; Sensors; Stresses; Affective Computing; Emotion; Emotion managements; Human machine interaction; Human-to-human interactions; Inter-personal communications; Internet based technology; System; Human computer interaction",2-s2.0-85028416772
"Zaroor A., Maree M., Sabha M.","A hybrid approach to conceptual classification and ranking of resumes and their corresponding job posts",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020411425&doi=10.1007%2f978-3-319-59421-7_10&partnerID=40&md5=23532552f159fe7086ef45abfd7c212f","Due to the constant growth in online recruitment, job portals are starting to receive thousands of resumes in diverse styles and formats from job seekers who have different fields of expertise and specialize in various domains. Accordingly, automatically extracting structured information from such resumes is needed not only to support the automatic matching between candidate resumes and their corresponding job offers, but also to efficiently route them to their appropriate occupational categories to minimize the effort required for managing and organizing them. As a result, instead of searching globally in the entire space of resumes and job posts, resumes that fall under a certain occupational category are only those that will be matched to their relevant job post. In this research work, we present a hybrid approach that employs conceptual-based classification of resumes and job postings and automatically ranks candidate resumes (that fall under each category) to their corresponding job offers. In this context, we exploit an integrated knowledge base for carrying out the classification task and experimentally demonstrate - using a real-world recruitment dataset- achieving promising precision results compared to conventional machine learning based resume classification approaches. © Springer International Publishing AG 2018.","Concept-based classification; Job matching; Online recruitment","Knowledge based systems; Learning systems; Classification approach; Classification tasks; Concept-based; Conventional machines; Job matching; Occupational categories; Online recruitment; Structured information; Classification (of information)",2-s2.0-85020411425
"Mohan G., Subashini M.M.","MRI based medical image analysis: Survey on brain tumor grade classification",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026870163&doi=10.1016%2fj.bspc.2017.07.007&partnerID=40&md5=f40dea82fb9ee5c914b9a4395693b658","A review on the recent segmentation and tumor grade classification techniques of brain Magnetic Resonance (MR) Images is the objective of this paper. The requisite for early detection of a brain tumor and its grade is the motivation for this study. In Magnetic Resonance Imaging (MRI), the tumor might appear clear but physicians need quantification of the tumor area for further treatment. This is where the digital image processing methodologies along with machine learning aid further diagnosis, treatment, prior and post-surgical procedures, synergizing between the radiologist and computer. These hybrid techniques provide a second opinion and assistance to radiologists in understanding medical images hence improving diagnostic accuracy. This article aims to retrospect the current trends in segmentation and classification relevant to tumor infected human brain MR images with a target on gliomas which include astrocytoma. The methodologies used for extraction and grading of tumors which can be integrated into the standard clinical imaging protocols are elucidated. Lastly, a crucial assessment of the state of the art, future developments and trends are dissertated. © 2017 Elsevier Ltd","Astrocytoma; Brain tumor grade; Classification; MRI; Neural networks; Segmentation","Brain; Classification (of information); Grading; Image classification; Image processing; Image segmentation; Learning systems; Magnetic resonance imaging; Medical imaging; Neural networks; Tumors; Astrocytoma; Brain tumors; Classification technique; Diagnostic accuracy; Further treatments; Hybrid techniques; Imaging protocol; Surgical procedures; Diagnosis; artificial neural network; astrocytoma; brain tumor; cancer grading; fuzzy logic; fuzzy system; human; image analysis; image processing; image segmentation; neuroimaging; nuclear magnetic resonance imaging; priority journal; radiologist; random forest; retrospective study; Review; support vector machine",2-s2.0-85026870163
"Mousavi S., Mosavi A., Varkonyi-Koczy A.R.","A load balancing algorithm for resource allocation in cloud computing",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029796640&doi=10.1007%2f978-3-319-67459-9_36&partnerID=40&md5=eda892f2535e5e681eadf03dcfaeced6","Utilizing dynamic resource allocation for load balancing is considered as an important optimization process of task scheduling in cloud computing. A poor scheduling policy may overload certain virtual machines while remaining virtual machines are idle. Accordingly, this paper proposes a hybrid load balancing algorithm with combination of Teaching-Learning-Based Optimization (TLBO) and Grey Wolves Optimization algorithms (GWO), which can well contribute in maximizing the throughput using well balanced load across virtual machines and overcome the problem of trap into local optimum. The hybrid algorithm is benchmarked on eleven test functions and a comparative study is conducted to verify the results with particle swarm optimization (PSO), Biogeography-based optimization (BBO), and GWO. To evaluate the performance of the proposed algorithm for load balancing, the hybrid algorithm is simulated and the experimental results are presented. © Springer International Publishing AG 2018.","Cloud computing; Optimization; Resource allocation","Cloud computing; Education; Heuristic algorithms; Network security; Particle swarm optimization (PSO); Resource allocation; Virtual machine; Biogeographybased optimizations (BBO); Comparative studies; Dynamic resource allocations; Hybrid algorithms; Load balancing algorithms; Optimization algorithms; Scheduling policies; Teaching-learning-based optimizations; Optimization",2-s2.0-85029796640
"Li K., Zou C., Bu S., Liang Y., Zhang J., Gong M.","Multi-modal feature fusion for geographic image annotation",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028468404&doi=10.1016%2fj.patcog.2017.06.036&partnerID=40&md5=eca31a1145333ba67fd17e0bc801a003","This paper presents a multi-modal feature fusion based framework to improve the geographic image annotation. To achieve effective representations of geographic images, the method leverages a low-to-high learning flow for both the deep and shallow modality features. It first extracts low-level features for each input image pixel, such as shallow modality features (SIFT, Color, and LBP) and deep modality features (CNNs). It then constructs mid-level features for each superpixel from low-level features. Finally it harvests high-level features from mid-level features by using deep belief networks (DBN). It uses a restricted Boltzmann machine (RBM) to mine deep correlations between high-level features from both shallow and deep modalities to achieve a final representation for geographic images. Comprehensive experiments show that this feature fusion based method achieves much better performances compared to traditional methods. © 2017","Convolutional neural networks (CNNs); Deep learning; Geographic image annotation; Multi-modal feature fusion","Deep learning; Image analysis; Neural networks; Pixels; Video streaming; Convolutional neural network; Deep belief network (DBN); Feature fusion; Geographic images; High-level features; Low-level features; Mid-level features; Restricted boltzmann machine; Image fusion",2-s2.0-85028468404
"Svetsky S., Moravcik O.","The empirical research on human knowledge processing in natural language within engineering education",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025826926&doi=10.1007%2f978-3-319-60937-9_1&partnerID=40&md5=7584ae2b87ac1ca085571e9287fc4ea7","The state-of-the art of the computer supported or enhanced teaching and learning processes is mostly focused on content acquisition, processing, and management; virtual learning environments; learning analytics; educational data mining. There is an absence of approaches based on knowledge exchange between machines and humans in knowledge based processes. This paper describes such an approach. Within an empirical participatory action research on technology enhanced learning the issue of knowledge is understood as a key element of any automation of human knowledge processing. Teaching is thus considered as a typical knowledge based process. In this context, the utility model is also used, which implemented “virtual knowledge” as a universal construct, which “inter-disciplinarilly” bridges the mental processes of humans and the physical processes performed by computers. Examples of such solutions are briefly mentioned. It is also emphasized that a complex computer supported collaborative learning requires, in the real practice, to execute a combined pedagogic (didactic) and informatics research. © Springer International Publishing AG 2018.","Computer supported collaborative learning; Engineering education; Human computer interactions; Human knowledge processing; Technology enhanced learning","Computer aided instruction; Education; Human computer interaction; Knowledge based systems; Knowledge management; Teaching; Virtual reality; Computer Supported Collaborative Learning; Educational data mining; Human knowledge; Knowledge-based process; Participatory action research; Teaching and learning; Technology enhanced learning; Virtual learning environments; Engineering education",2-s2.0-85025826926
"Han Z., Wang J.","A Fault Diagnosis Method Based on Active Example Selection",2018,"Journal of Circuits, Systems and Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020484759&doi=10.1142%2fS0218126618500135&partnerID=40&md5=ae7cc568453fee3ade5bb923cd57113b","The fault diagnosis in the real world is often complicated. It is due to the fact that not all relevant fault information is available directly. In many fault diagnosis situations, it is impossible or inconvenient to find all fault information before establishing a fault diagnosis model. To deal with this issue, a method named active example selection (AES) is proposed for the fault diagnosis. AES could actively discover unseen faults and choose useful samples to improve the fault detection accuracy. AES consists of three key components: (1) a fusion model of combining the advantage of the unsupervised and supervised fault diagnosis methods, where the unsupervised fault diagnosis methods could discover unseen faults and the supervised fault diagnosis methods could provide better fault detection accuracy on seen faults, (2) an active learning algorithm to help the supervised fault diagnosis methods actively discover unseen faults and choose useful samples to improve the fault detection accuracy, and (3) an incremental learning scheme to speed up the iterative training procedure for AES. The proposed method was evaluated on the benchmark Tennessee Eastman Process data. The proposed method performed better on both unseen and seen faults than the stand-alone unsupervised, supervised fault diagnosis methods, their joint and referenced support vector machines based on active learning. © 2018 World Scientific Publishing Company.","active learning; data drive; Fault diagnosis; TEP","Artificial intelligence; Digital storage; Failure analysis; Iterative methods; Learning algorithms; Active Learning; Active-learning algorithm; Data drives; Fault diagnosis method; Fault diagnosis model; Incremental learning; Tennessee Eastman process; Training procedures; Fault detection",2-s2.0-85020484759
"Singh M., Zeng Z., Kalaw E.M., Giron D.M., Chong K.-T., Lee H.K.","A study of nuclei classification methods in histopathological images",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019652481&doi=10.1007%2f978-3-319-59397-5_9&partnerID=40&md5=eaf117f0e800bebd82f5a63d077904cb","Cancer is a group of diseases involving abnormal cell growth with varying malignancy and extent across different patients. Cytological features like prominent nucleoli, nuclear enlargement, and hyperchro-masia are important to the tumor pathologist in assessment of cancer malignancy from tissue biopsies. In a recent study, Yap et al. [21] proposed effective prominent nucleoli detectors in histopathological images and developed different feature generation methods. These methods were based on polar gradients and were used along with support vector machine (SVM) and AdaBoost for detection purposes. In this study, we benchmark the performance of these methods along with convolutional and fully connected networks for the task of distinguishing between nuclei with and without prominent nucleolus. © Springer International Publishing AG 2018.","AdaBoost; Auto-encoder networks; Convolutional neural networks; Deep learning; Nuclei classification","Adaptive boosting; Benchmarking; Convolution; Deep learning; Diseases; Health care; Neural networks; Support vector machines; Auto encoders; Classification methods; Convolutional neural network; Feature generation; Fully connected networks; Histopathological images; Tissue biopsies; Image classification",2-s2.0-85019652481
"Wang W., Li Y., Wang X., Liu J., Zhang X.","Detecting Android malicious apps and categorizing benign apps with ensemble of classifiers",2018,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011337840&doi=10.1016%2fj.future.2017.01.019&partnerID=40&md5=6200f25a74a02a07085783fd6e2fdcb9","Android platform has dominated the markets of smart mobile devices in recent years. The number of Android applications (apps) has seen a massive surge. Unsurprisingly, Android platform has also become the primary target of attackers. The management of the explosively expansive app markets has thus become an important issue. On the one hand, it requires effectively detecting malicious applications (malapps) in order to keep the malapps out of the app market. On the other hand, it needs to automatically categorize a big number of benign apps so as to ease the management, such as correcting an app's category falsely designated by the app developer. In this work, we propose a framework to effectively and efficiently manage a big app market in terms of detecting malapps and categorizing benign apps. We extract 11 types of static features from each app to characterize the behaviors of the app, and employ the ensemble of multiple classifiers, namely, Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Naive Bayes (NB), Classification and Regression Tree (CART) and Random Forest (RF), to detect malapps and to categorize benign apps. An alarm will be triggered if an app is identified as malicious. Otherwise, the benign app will be identified as a specific category. We evaluate the framework on a large app set consisting of 107,327 benign apps as well as 8,701 malapps. The experimental results show that our method achieves the accuracy of 99.39% in the detection of malapps and achieves the best accuracy of 82.93% in the categorization of benign apps. © 2017 Elsevier B.V.","Android security; Classification; Ensemble learning; Intrusion detection; Malware detection; Static analysis","Classification (of information); Classifiers; Commerce; Decision trees; Intrusion detection; Malware; Nearest neighbor search; Static analysis; Support vector machines; Android applications; Android securities; Classification and regression tree; Ensemble learning; Ensemble of classifiers; K nearest neighbor (KNN); Malware detection; Multiple classifiers; Android (operating system)",2-s2.0-85011337840
"Mazaheri A., Segaert K., Olichney J., Yang J.-C., Niu Y.-Q., Shapiro K., Bowman H.","EEG oscillations during word processing predict MCI conversion to Alzheimer's disease",2018,"NeuroImage: Clinical",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032199455&doi=10.1016%2fj.nicl.2017.10.009&partnerID=40&md5=d74fb5590abb0206add53eb89bd171a7","Only a subset of mild cognitive impairment (MCI) patients progress to develop a form of dementia. A prominent feature of Alzheimer's disease (AD) is a progressive decline in language. We investigated if subtle anomalies in EEG activity of MCI patients during a word comprehension task could provide insight into the likelihood of conversion to AD. We studied 25 amnestic MCI patients, a subset of whom developed AD within 3-years, and 11 elderly controls. In the task, auditory category descriptions (e.g., ‘a type of wood’) were followed by a single visual target word either semantically congruent (i.e., oak) or incongruent with the preceding category. We found that the MCI convertors group (i.e. patients that would go on to convert to AD in 3-years) had a diminished early posterior-parietal theta (3–5 Hz) activity induced by first presentation of the target word (i.e., access to lexico-syntactic properties of the word), compared to MCI non-convertors and controls. Moreover, MCI convertors exhibited oscillatory signatures for processing the semantically congruent words that were different from non-convertors and controls. MCI convertors thus showed basic anomalies for lexical and meaning processing. In addition, both MCI groups showed anomalous oscillatory signatures for the verbal learning/memory of repeated words: later alpha suppression (9–11 Hz), which followed first presentation of the target word, was attenuated for the second and third repetition in controls, but not in either MCI group. Our findings suggest that a subtle breakdown in the brain network subserving language comprehension can be foretelling of conversion to AD. © 2017 The Authors",,"adult; aged; alpha rhythm; Alzheimer disease; Article; beta rhythm; clinical article; controlled study; electroencephalogram; human; inferior frontal gyrus; learning; mild cognitive impairment; neuromodulation; oscillation; perception; priority journal; semantics; support vector machine; temporal lobe; theta rhythm; verbal memory; word processing",2-s2.0-85032199455
"Agrawal R.","Integrated effect of nearest neighbors and distance measures in k-nn algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031411767&doi=10.1007%2f978-981-10-6620-7_74&partnerID=40&md5=fe57f0cb618373fa54fc7213dd12953f","Supervised learning or classification is the cornerstone of Data Mining. A well-known, simple, and effective algorithm for supervised classification is k-Nearest Neighbor (k-NN). A distance measure provides significant support in the process of classification and the correct choice of distance measure is the most influential process in the classification technique. Also, the choice of k in k-Nearest Neighbor algorithm plays an effective role in the accuracy of the classifier. The aim of this paper is to analyze the integrated effect of various distance measures on different values of k in k-Nearest Neighbor algorithm on different data sets taken from UCI machine learning repository. © 2018, Springer Nature Singapore Pte Ltd.","Cityblock; Classification; Cosine; Data sets; Distance measure; Euclidean; K-Nearest neighbor; Mahalanobis","Big data; Data mining; Learning algorithms; Learning systems; Motion compensation; Nearest neighbor search; Pattern recognition; Supervised learning; Cityblock; Cosine; Data sets; Distance measure; Euclidean; K-nearest neighbors; Mahalanobis; Classification (of information)",2-s2.0-85031411767
"Walkowiak T.","Language processing modelling notation – Orchestration of NLP microservices",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020785890&doi=10.1007%2f978-3-319-59415-6_44&partnerID=40&md5=a429f935ac40248556817e8d5b077409","The paper presents Language Processing Modelling Notation (LPMN). It is a formal language used to orchestrate a set of NLP microservices. The LPMN allows modeling and running complex workflows of language and machine learning tools. The scalability of the solution was achieved by a usage of message-oriented middleware. LPMN is used for developing text mining application with web-based interface and performing research experiments that requires a usage of NLP and machine learning tools. © Springer International Publishing AG 2018.","Microservices; Natural language processing; Orchestration; Text mining; Web-based application","Artificial intelligence; Data mining; Formal languages; Learning algorithms; Learning systems; Middleware; Multimedia systems; Natural language processing systems; Websites; Complex workflows; Language processing; Message oriented middleware; Microservices; Orchestration; Text mining; Web-based applications; Web-based interface; Modeling languages",2-s2.0-85020785890
"Toh K.-A., Lin Z., Sun L., Li Z.","Stretchy binary classification",2018,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350826&doi=10.1016%2fj.neunet.2017.09.015&partnerID=40&md5=215232a3b158b6a504bf5cc5dc7fcc3f","In this article, we introduce an analytic formulation for compressive binary classification. The formulation seeks to solve the least ℓp-norm of the parameter vector subject to a classification error constraint. An analytic and stretchable estimation is conjectured where the estimation can be viewed as an extension of the pseudoinverse with left and right constructions. Our variance analysis indicates that the estimation based on the left pseudoinverse is unbiased and the estimation based on the right pseudoinverse is biased. Sparseness can be obtained for the biased estimation under certain mild conditions. The proposed estimation is investigated numerically using both synthetic and real-world data. © 2017 Elsevier Ltd","Parameter learning; Pattern classification; Sparse estimation","Artificial intelligence; Cognitive systems; Pattern recognition; Biased estimation; Binary classification; Classification errors; Parameter learning; Parameter vectors; Pseudo-inverses; Sparse estimation; Variance analysis; Bins; analytical error; Arabidopsis thaliana; Article; breast cancer; classification algorithm; gene expression; hereditary tumor syndrome; leukemia; priority journal; statistical model; support vector machine",2-s2.0-85032350826
"Deshmukh V., Khaparde A., Shaikh S.","Multi-focus image fusion using deep belief network",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028369281&doi=10.1007%2f978-3-319-63673-3_28&partnerID=40&md5=bcc385402ddfbd41c97a164c18c48e7f","Multi-focus images may be fused to get the relevant information of a particular scene. Due to the limited depth of field of a convex lens of a camera, some objects in the image may not be focused. These images are fused to get all-in-focus image. This paper proposes an innovative way to fuse multi-focus images. The proposed algorithm calculates weights indicating the sharp regions of input images with the help of Deep Belief Network (DBN) and then fuses input images using weighted superimposition fusion rule. The proposed algorithm is analyzed and examined using various parameters like entropy, mutual information, SSIM, IQI etc. © 2018, Springer International Publishing AG.","Deep learning; Image fusion; Restricted boltzman machine; Superimposed","Deep learning; Intelligent systems; All-in-focus image; Deep belief network (DBN); Deep belief networks; Depth of field; Multifocus image fusion; Multifocus images; Mutual informations; Superimposed; Image fusion",2-s2.0-85028369281
"Cevzar M., Petrič T., Babič J.","Open source EMG device for controlling a robotic hand",2018,"Mechanisms and Machine Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028360583&doi=10.1007%2f978-3-319-61276-8_84&partnerID=40&md5=92fe3dd9711c505f646113421d6f6d88","Off-the-shelf electronic market is large, diverse and easily accessible by many. Credit card size computers (example: Raspberry Pi) or micro-controller boards (example: Arduino) can be used for learning how to code and how to control embedded systems. Nevertheless, there is a lack of off-the-shelf, open source devices that would enable us to learn about and make use of human signal processing. An example of such a device is an electromyograph (EMG). In this paper we investigated, if an EMG device could fulfill the aforementioned gap. EMG device we used for conducting our experiment was a five channel open source EMG Arduino shield. The performance of the device was evaluated on three healthy male subjects. They were instructed to perform basic finger movements which we classified and executed on the robotic hand. The EMG signal classification was performed using a Support Vector Machine (SVM) algorithm. In our experimental setup the average EMG signal classification accuracy was 78.29%. This we believe demonstrates there are EMG devices on the market today that provide access to cost effective prototyping and learning about EMG signals. © 2018, Springer International Publishing AG.","Education; Electromyograph (EMG); Open source; Robotic hand; Support vector machine (SVM)","Commerce; Cost effectiveness; Education; Electromyography; Embedded systems; End effectors; Robotic arms; Robotics; Signal processing; Support vector machines; Cost effective; Credit cards; Electromyograph (EMG); Electronic market; Emg signal classifications; Finger movements; Open sources; Support vector machine algorithm; Biomedical signal processing",2-s2.0-85028360583
"Manuel J., Cordeiro R., Silva C.","Between Data Mining and Predictive Analytics Techniques to Cybersecurity Protection on eLearning Environments",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029599904&doi=10.1007%2f978-3-319-67621-0_17&partnerID=40&md5=c2d6825e018575bcd7ed8c375b500dc5","This paper aims to present a hypothetic theory of intelligent security system. In society the threat of cyber-attacks is getting louder and the use of computers, criminal activity has also changed from physical to cybernetic intrusion. There had been many cyber security solutions used to counteract these attacks, however we highlight the importance of self-protected systems in defense and in a correct analysis of cyber attacks. The internet is vulnerable to cyber-attacks as well as the information found in data systems and through a form of recognition and extraction of relevant information, we can represent data as shared data and integrated to intelligent system. What was used us a static firewall is now intended to be dynamic and self-critical. By techniques of data analysis, statistics, machine learning, data mining, the cybersecurity and privacy challenges are within our reach. This paper examines data mining techniques in order to predict pathways of Internet security and which considerations are involved in the theoretical solutions presented for the privacy systems such as the e-Learning environments. © 2018, Springer International Publishing AG.","Cybersecurity; Data mining; Intelligent firewall; Intrusion detection systems; Predictive models","Artificial intelligence; Computation theory; Computational methods; Computer aided instruction; Computer crime; Computer system firewalls; Crime; Data privacy; Distributed computer systems; E-learning; Intelligent systems; Intrusion detection; Learning systems; Mercury (metal); Network security; Predictive analytics; Criminal activities; Cyber security; E-learning environment; Intelligent firewall; Intelligent security systems; Intrusion Detection Systems; Predictive models; Theoretical solutions; Data mining",2-s2.0-85029599904
"Hoogendoorn M., Funk B.","Mathematical foundations for supervised learning",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030716950&doi=10.1007%2f978-3-319-66308-1_6&partnerID=40&md5=e56bbd0d7faf45294212c67d28b0cbdb","In this chapter the theoretical underpinning of supervised learning are discussed. The whole supervised machine learning process is explained from a more formal perspective as well as some underlying theories. The theories discussed include concepts such as PAC learnability and VC dimensions. The implications of these theories are discussed. © Springer International Publishing AG 2018.",,,2-s2.0-85030716950
[No author name available],"5th International Conference on Man-Machine Interactions, ICMMI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030781966&partnerID=40&md5=4df79cf9a65856cd567664b343f5dc2e","The proceedings contain 56 papers. The special focus in this conference is on Man-Machine Interactions. The topics include: Deep learning with dense random neural networks; a perceptually inspired method for enhancing contrast in uneven lighting images.; advances in hand-eye robot interactions; human perception of the pattern strength measure; typing braille code in the air with the leap motion controller; touchless virtual keyboard controlled by eye blinking and EEG signals; an alternative virtual keyboard for blind people; how increasing machine agency affects human agency; eye movement traits in differentiating experts and laymen; mobile application using embedded sensors as a three dimensional motion registration method; virtual reality application to study the visual influences on human balance; improvements in DNA reads correction; semantic-based clustering of gene ontology terms on the biotest platform; comparative analysis of MicroRNA-target gene interaction prediction algorithms based on integrated P-value calculation; searching through scientific PDF files supported by bi-clustering of key terms matrices.; searching for cancer signatures using data mining techniques; consensus approach for detection of cancer somatic mutations; cancer clonal evolution simulation program; image denoising using backward stochastic differential equations; gabor filters generalization based on ateb-functions for information security; hierarchical agglomerative clustering of time-warped series; averaging of nonlinearly aligned evoked potentials in impulsive noise environment; linguistically described covariance matrix estimation; DBpedia and YAGO as knowledge base for natural language based question answering - the evaluation and expressing the notion of a mathematical structure in the formal language of mizar.",,,2-s2.0-85030781966
"Solanki P., Gopal G.","Image categorization using improved data mining technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031405458&doi=10.1007%2f978-981-10-6620-7_19&partnerID=40&md5=884aad2d0dd8c6dbd78481b324203d8b","Image categorization is one of the important branches of artificial intelligence. Categorization of images is a way of grouping images according to their similarity. Image categorization uses various features of images like texture, color component, shape, edge, etc. Categorization process has various steps like image preprocessing, object detection, object segmentation, feature extraction, and object classification. For the past few years, researchers have been contributing different algorithms in the two most common machine learning categories to either cluster or classify images. The goal of this paper is to discuss two of the most popular machine learning algorithms: Nearest Neighbor (k-NN) for image classification and Means clustering algorithm. After that, a Hybrid model of both the above algorithms is proposed. These algorithms are implemented in MATLAB; finally, the experimental results of each algorithm are presented and discussed. © 2018, Springer Nature Singapore Pte Ltd.","Image categorization; Means algorithm; Nearest neighbor (NN) algorithm","Artificial intelligence; Big data; Data mining; Feature extraction; Image classification; Image enhancement; Image segmentation; Imaging systems; Learning algorithms; Learning systems; Nearest neighbor search; Object detection; Color component; Image Categorization; Image preprocessing; Means clustering algorithm; Nearest neighbor algorithm; Nearest neighbors; Object classification; Object segmentation; Clustering algorithms",2-s2.0-85031405458
"Coronato A., Paragliola G.","Towards a cognitive system for the identification of sleep disorders",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020400474&doi=10.1007%2f978-3-319-59480-4_10&partnerID=40&md5=8e8166e22a7001b7e06a3a8aeb932845","Alzheimer’s disease (AD) is the most common type of dementia. Patients with AD may show anomalous behaviors such as sleeping disorders. Due to the increasing attention focused on these kinds of behaviors, activities like monitoring and identification are becoming critical. In order to meet these requirements, we propose a cognitive approach based on a combination of machine learning algorithms and a prior knowledge base for the identification of anomalous behaviors during sleep. The results show an improvement in the identification of sleeping disorders of more than 10%. © Springer International Publishing AG 2018.",,"Cognitive systems; Interactive computer systems; Knowledge based systems; Learning algorithms; Learning systems; Multimedia services; Multimedia systems; Alzheimer; Anomalous behavior; Cognitive approaches; Prior knowledge; Sleep disorders; Sleep research",2-s2.0-85020400474
"Błaszczyński J., Stefanowski J.","Local data characteristics in learning classifiers from imbalanced data",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030102225&doi=10.1007%2f978-3-319-67946-4_2&partnerID=40&md5=3c40621a15ea4fa2fc419bfa65ead934","Learning classifiers from imbalanced data is still one of challenging tasks in machine learning and data mining. Data difficulty factors referring to internal and local characteristics of class distributions deteriorate performance of standard classifiers. Many of these factors may be approximated by analyzing the neighbourhood of the learning examples and identifying different types of examples from the minority class. In this paper, we follow recent research on developing such methods for assessing the types of examples which exploit either k-nearest neighbours or kernels. We discuss the approaches to tune the size of both kinds of neighborhoods depending on the data set characteristics and evaluate their usefulness in series of experiments with real-world and synthetic data sets. Furthermore, we claim that the proper analysis of these neighborhoods could be the basis for developing new specialized algorithms for imbalanced data. To illustrate it, we study generalizations of over-sampling in pre-processing methods and neighbourhood based ensembles. © Springer International Publishing AG 2018.",,,2-s2.0-85030102225
"Aggarwal V., Jajoria S., Sood A.","Text retrieval from scanned forms using optical character recognition",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031398822&doi=10.1007%2f978-981-10-6614-6_21&partnerID=40&md5=06214aadc474343f6d5566a1dc7cbddb","This paper investigates the use of image processing techniques and machine learning algorithm of logistic regression to extract text from scanned forms. Conversion of printed or handwritten documents into digital modifiable text is a tedious task and requires a lot of human effort. In order to automate this task, we apply the machine learning algorithm of logistic regression. The main components of this system are (i) text detection from the scanned document and (ii) character recognition of the individual characters in the detected text. In order to complete these tasks, we firstly use the image processing techniques to do line segmentation, character segmentation, and then ultimately character recognition. The character recognition is done by a one-vs-all classifier which is trained using the training data set and learns the parameters with the help of this data set. Once the classifier has learned the parameters, it could identify a total of 39 characters which include capital English alphabets, numerals, and a few symbols. © 2018, Springer Nature Singapore Pte Ltd.","Classifier; Logistic regression; OCR; Segmentation","Artificial intelligence; Classification (of information); Classifiers; Electronic document exchange; Image processing; Image segmentation; Learning algorithms; Learning systems; Optical character recognition; Optical data processing; Regression analysis; Character segmentation; Handwritten document; Image processing technique; Line segmentation; Logistic regressions; Text detection; Text retrieval; Training data sets; Character recognition",2-s2.0-85031398822
"Stapor K.","Evaluating and comparing classifiers: Review, some recommendations and limitations",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019245137&doi=10.1007%2f978-3-319-59162-9_2&partnerID=40&md5=667bb84c6f8500f41ef91e43eaba70ab","Performance evaluation of supervised classification learning method related to its prediction ability on independent data is very important in machine learning. It is also almost unthinkable to carry out any research work without the comparison of the new, proposed classifier with other already existing ones. This paper aims to review the most important aspects of the classifier evaluation process including the choice of evaluating metrics (scores) as well as the statistical comparison of classifiers. Critical view, recommendations and limitations of the reviewed methods are presented. The article provides a quick guide to understand the complexity of the classifier evaluation process and tries to warn the reader about the wrong habits. © Springer International Publishing AG 2018.","Classifier evaluation; Performance metrics; Statistical classifier comparison; Supervised classification","Classification (of information); Learning systems; Classifier evaluation; Comparing classifiers; Evaluating metrics; Performance metrics; Statistical classifier; Statistical comparisons; Supervised classification; Supervised learning",2-s2.0-85019245137
"Zhang Y., Godaliyadda G.M.D., Ferrier N., Gulsoy E.B., Bouman C.A., Phatak C.","Reduced electron exposure for energy-dispersive spectroscopy using dynamic sampling",2018,"Ultramicroscopy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032744359&doi=10.1016%2fj.ultramic.2017.10.015&partnerID=40&md5=68d1591e76d93f5a67cf693932323b6d","Analytical electron microscopy and spectroscopy of biological specimens, polymers, and other beam sensitive materials has been a challenging area due to irradiation damage. There is a pressing need to develop novel imaging and spectroscopic imaging methods that will minimize such sample damage as well as reduce the data acquisition time. The latter is useful for high-throughput analysis of materials structure and chemistry. In this work, we present a novel machine learning based method for dynamic sparse sampling of EDS data using a scanning electron microscope. Our method, based on the supervised learning approach for dynamic sampling algorithm and neural networks based classification of EDS data, allows a dramatic reduction in the total sampling of up to 90%, while maintaining the fidelity of the reconstructed elemental maps and spectroscopic data. We believe this approach will enable imaging and elemental mapping of materials that would otherwise be inaccessible to these analysis techniques. © 2017 Elsevier B.V.","Dose reduction; Dynamic sampling; Energy dispersive spectroscopy (EDS); Neural networks; Scanning electron microscopy (SEM); SLADS","Biological materials; Chemical analysis; Data acquisition; Electron energy loss spectroscopy; Electron microscopy; Electrons; Energy dispersive spectroscopy; Learning systems; Neural networks; Scanning electron microscopy; Spectrum analysis; Analytical electron microscopy; Dose reduction; Dynamic sampling; Energy dispersive spectroscopies (EDS); High-throughput analysis; SLADS; Spectroscopic imaging; Supervised learning approaches; Data reduction",2-s2.0-85032744359
"Hrabuska R., Cedivodova V., Prauzek M., Hlavica J., Konecny J.","Electrical impedance distribution in human torax: A modeling framework",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031411597&doi=10.1007%2f978-3-319-68321-8_53&partnerID=40&md5=84592ecb76627f91b7d96299322e426f","Electrical impedance tomography (EIT) is an imaging system suitable for long-term monitoring. To extend current uses of EIT, improvements in the image reconstruction algorithms are essential. New image reconstruction methods for EIT can be tested on an impedance model of human body. Moreover, accurate anatomical impedance distribution models of human body are used to generate training data used in machine learning algorithms. Simulation framework, introduced in this paper, is capable of autonomous conversion of Computed tomography (CT) scans from DICOM format into 2D MESH human thorax impedance distribution model. Developed impedance models of large thorax structures achieve accurate results through segmentation of CT images and Fourier Fitting. Framework is developed in MATLAB as an extension to EIDORS and NETGEN frameworks. © Springer International Publishing AG 2018.","EIDORS; Electrical impedance tomography; Mesh framework; Thorax impedance model","Electric impedance; Electric impedance measurement; Electric impedance tomography; Image enhancement; Image processing; Image reconstruction; Image segmentation; Learning algorithms; Learning systems; MATLAB; Mesh generation; Network function virtualization; Tomography; Computed tomography scan; EIDORS; Electrical impe dance tomography (EIT); Electrical impedance tomography; Image reconstruction algorithm; Image reconstruction methods; Impedance modeling; Mesh framework; Computerized tomography",2-s2.0-85031411597
"Vu L., Kim H., Benson E., Amonette W., Hanson A., Perera J., Rajulu S.","Development of a depth camera-based instructional tool for resistive exercise during spaceflight",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021772018&doi=10.1007%2f978-3-319-60822-8_9&partnerID=40&md5=05c35c474671b067abff9e39691c56aa","Resistive exercise is essential to maintaining proper musculoskeletal health during spaceflight. Therefore crewmembers receive instruction from strength, conditioning, and rehabilitation specialists on proper exercise technique to maximize exercise effectiveness and prevent injuries. However, long duration missions can make real-time exercise instruction and feedback problematic. A depth camera-based software tool was developed to provide exercise instruction and feedback during the deadlift exercise. The software tool uses a machine learning algorithm to identify 5 common deadlift technique mistakes. A subset containing 2 subjects with no deadlift training experience were coached on the deadlift exercise and separated into 2 groups: experimental group using the software tool or a control group without the tool. Motion-capture data were collected to evaluate the kinematic characteristics between the test and control group. It was found that the software tool assisted with increased torso, hip, and knee joint angle consistency and improved form during deadlifts. © Springer International Publishing AG 2018.","Biomechanics; Exercise performance assessment; Motion learning; Virtual coaching","Biomechanics; Cameras; Computer software; Education; Feedback; Human computer interaction; Human engineering; Joints (anatomy); Sports; Experimental groups; Kinematic characteristics; Long duration missions; Motion capture data; Motion learning; Performance assessment; Training experiences; Virtual coaching; Learning algorithms",2-s2.0-85021772018
"De Vito S., Fattoruso G., Esposito E., Salvato M., Agresta A., Panico M., Leopardi A., Formisano F., Buonanno A., Delli Veneri P., Di Francia G.","A distributed sensor network for waste water management plant protection",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029624585&doi=10.1007%2f978-3-319-55077-0_39&partnerID=40&md5=d623d65360d9f49d1b4254d9a683dab5","Waste water management process has a significant role in guarantee sea and surface water bodies water quality with direct impact on tourism based economy and public health. Protection of this critical infrastructure form illicit discharges is hence paramount for the whole society. Here, We propose a pervasive monitoring centered approach to the protection of wastewater management plant. An hybrid sensor network is actually deployed along the wastewater network including several different transducers. Incepted data are harmonized and processed with an integrated SWMM model and machine learning based approach in order to forecast water qualitative and quantitative aspects, detect and localize anomalies. An advanced WEBGIS-SOS based interface conveys relevant information to the management entity allowing it to take appropriate actions in a timely way, reducing and mitigating the impacts of illicit discharges. © Springer International Publishing AG 2018.","Distributed sensor network; Plant protection; Wastewater management; Water quality monitoring","Hybrid sensors; Learning algorithms; Learning systems; Sensor networks; Surface waters; Water management; Water quality; Direct impact; Distributed sensor networks; Pervasive monitoring; Plant protection; Surface water body; Wastewater management; Wastewater network; Water quality monitoring; Waste management",2-s2.0-85029624585
"Kamanksha D.P., Sanjay A.","A critical analysis of twitter data for movie reviews through ‘random forest’ approach",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028391349&doi=10.1007%2f978-3-319-63645-0_52&partnerID=40&md5=f6a23e307d84998c9fade0e754715dcd","Using Sentiment analysis one can understand interaction of a user with the movies through their feedback. Here analysis is done based on the movie reviews that can be collected from many sources. Twitter is one among the foremost frequent on-line social media and micro blogging services. Due to the popularity of twitter it has become a useful resource for collecting sentiments through API or other data mining techniques. Our work here presents an examination on the evaluation of the machine learning algorithms (Random Forest, bagging, SVM and Naïve Bayes) in R together the public opinion for example opinion about ‘Civil War’ Movie. Here we have used ‘Random Forest’ to show its better performance in the analysis of movie reviews. © Springer International Publishing AG 2018.","Natural language processing; Opinion mining; Sentiment analysis; Sentiment classification; Twitter","Decision trees; Intelligent systems; Learning algorithms; Learning systems; Motion pictures; Natural language processing systems; Social aspects; Social networking (online); Critical analysis; Micro-blogging services; Opinion mining; Public opinions; Random forests; Sentiment analysis; Sentiment classification; Twitter; Data mining",2-s2.0-85028391349
"Muthukumaran K., Srinivas S., Malapati A., Neti L.B.M.","Software defect prediction using augmented bayesian networks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593555&doi=10.1007%2f978-3-319-60618-7_28&partnerID=40&md5=d68689127586210ca132411e9c8f3e87","Prediction models are built with various machine learning algorithms to identify defects prior to release to facilitate software testing, and save testing costs. Naïve Bayes classifier is one of the best performing classification techniques in defect prediction. It assumes conditional independence of features and for defect prediction problem some of the features are not actually conditionally independent. The interesting problem is to relax these conditional independence assumptions and to check whether there is any improvement in performance of classifiers. We have built Bayesian Network structures using different classes of algorithms namely score-based, constraint-based and hybrid algorithms. We propose an approach to augment these Bayesian Network structures with class node. Bayesian Network classifiers along with Random Forests, Logistic Regression and Naïve Bayes classifiers are then evaluated using measures like AUC and H-measure. We observe that RSMAX2 and Grow-Shrink classifiers (after augmentation) perform consistently better in defect prediction. © Springer International Publishing AG 2018.","Bayesian networks; Classification; Software defect prediction","Classification (of information); Decision trees; Defects; Evolutionary algorithms; Forecasting; Learning algorithms; Learning systems; Pattern recognition; Sodium; Soft computing; Software testing; Structures (built objects); Bayesian network classifiers; Bayesian network structure; Classification technique; Conditional independence assumption; Conditional independences; Logistic regressions; Performance of classifier; Software defect prediction; Bayesian networks",2-s2.0-85028593555
"Shi J., Ren X., Liu Z., Chen Z., Duan F.","The development of a wheelchair control method based on sEMG signals",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030839812&doi=10.1007%2f978-981-10-6496-8_40&partnerID=40&md5=e008cfd58c8c73d38bc5b0c81d104800","This paper proposed a control method of the electric wheelchair based on surface electromyography (sEMG) signals. In this method, a mapping between hand motions and control commands was established. When a certain kind of hand motion was recognized from sEMG signals, corresponding control would be applied in the wheelchair. The sEMG signals was as raw material for the pattern recognition type of classifier, which promoted the accuracy rate and robustness. The fusion features of Autoregressive (AR) model coefficient and root mean square ratio (RMSR) were used as features of data of hand motions. Support vector machine (SVM) as one of state-of-the-art supervised learning models, was used as classifier. Furthermore, comprehensive real-time simulation and control experiment were implemented. The accuracy rate of hand motions recognition in real-time reached 95% and the success rate of control experiment was up to 88%, which showed the proposed method was feasible and practical. © 2018, Springer Nature Singapore Pte Ltd.","Electric wheelchair; Hand motion recognition; sEMG; Support vector machine (SVM)","Biomedical signal processing; Electromyography; Intelligent systems; Palmprint recognition; Pattern recognition; Support vector machines; Wheelchairs; Autoregressive model coefficients; Control experiments; Electric wheelchair; Hand motion; Real time simulations; sEMG; Surface electromyography; Wheelchair control; Motion estimation",2-s2.0-85030839812
"Muthukumaran K., Dasgupta A., Abhidnya S., Neti L.B.M.","On the effectiveness of cost sensitive neural networks for software defect prediction",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028598796&doi=10.1007%2f978-3-319-60618-7_55&partnerID=40&md5=7d2b78371d8de6737b1e4f4011576a1d","The cost of fixing a software defect varies with the phase in which it is uncovered. Defect found during post-release phase costs much more than the defect that is uncovered in pre-release phase. Hence defect prediction models have been proposed to predict bugs in pre-release phase. For any prediction model, there are two kinds of misclassification errors - Type I and Type II errors. Type II errors are found to be more costly than Type I errors for defect prediction problem. However there have been only few studies that have considered misclassifications costs while building or evaluating defect predictions models. We have built classification models using three cost-sensitive boosting Neural Network methods, namely, CSBNN-TM, CSBNN-WU1 and CSBNN-WU2. We have compared the performance of these cost sensitive Neural Networks with the traditional machine learning algorithms like Logistic Regression, Naive Bayes, Random Forest, Bayesian Network, Neural Networks, k-Nearest Neighbors and Decision Tree. We have compared the performance of the resultant models using cost centric measure - Normalized Expected Cost of Misclassification (NECM). © Springer International Publishing AG 2018.","Cost-sensitive neural networks; Misclassification cost; Software defect prediction","Bayesian networks; Data mining; Decision trees; Defects; Errors; Forecasting; Learning algorithms; Learning systems; Nearest neighbor search; Pattern recognition; Soft computing; Classification models; Cost-sensitive neural networks; Defect prediction models; Expected cost of misclassification; Misclassification costs; Misclassification error; Software defect prediction; Type I and type II errors; Costs",2-s2.0-85028598796
"Feicán C., Cabrera J., Arévalo J., Ayala E., Guerrero F., Pinos E.","Sign language trainer using leap motion",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022187693&doi=10.1007%2f978-3-319-60018-5_25&partnerID=40&md5=ccc9e6e72e7f9c73b083df809d5d6350","Learning a sign language can be a long process, but necessary to improve the communication of people, nowadays you need face-to-face courses. To facilitate the learning of Ecuadorian Sign Language and make it more interactive, a software trainer is proposed using the Leap Motion® device. The system is based on an SVM classifier, the parameters were calculated to obtain greater efficiency in the classifier. It is able to predict signs from A to Z including mobile signs, the Ecuadorian typographic alphabet has 30 letters and 5 require movement. The system was trained with a database of 40 people resulting in 1200 signs. © Springer International Publishing AG 2018.","Ecuadorian Sign Language; Human factors in education; Human factors in training; Leap motion; Support vector machine","Human engineering; Support vector machines; Face to face; Leap motion; Sign language; SVM classifiers; Education",2-s2.0-85022187693
"Štrba R., Štrbová K., Vondrák I., Ježek D., Štolfa S.","Exploratory data analysis of software requirements using statistics and kohonen’s self-organizing map",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028653420&doi=10.1007%2f978-3-319-60834-1_9&partnerID=40&md5=97a26da10d86ce029ddf5221098e5d54","Many factors affect the success of prediction using Machine-Learning on given task. The quality of provided data is one of the key factors which influence accuracy of Machine-Learning (ML) and Artificial Intelligence (AI) algorithms. The main goal of this research is to explore data, choose the right parameters and remove noisy items before usage of ML or AI. This research provides results of exploratory data analysis of software requirements collected from Software Company. Presented results help identify general patterns in the dataset of software requirements for future prediction purposes. © 2018, Springer International Publishing AG.","Clustering; Data analysis; Kohonen’s neural network; MatLab; R programme; Self-organizing map","Artificial intelligence; Conformal mapping; Data handling; Data reduction; Information analysis; Learning systems; Requirements engineering; Self organizing maps; Clustering; Exploratory data analysis; Future predictions; General patterns; Kohonen; R programme; Software company; Software requirements; MATLAB",2-s2.0-85028653420
"Abuelenin S., Elmougy S., Naguib E.","Twitter sentiment analysis for arabic tweets",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029536167&doi=10.1007%2f978-3-319-64861-3_44&partnerID=40&md5=1b28fbcf0a54cfc763d9a6fb26b2e3bf","Recently, increasing attention has been attracted to social networking sentiment analysis. Twitter is an online real-time social network and microblogging service that allows certified participants to distribute short posts called tweets. Twitter plays a major role in showing how consumers discover, research, and share information about brands and products. Sentiment analysis can be considered as a basic classification problem between three classes (Positive, Negative, and Neutral). Much work had been done on sentiment analysis in English while less work had been done on other languages like Arabic. Social media and blogs used by individuals are typically in Dialect Arabic. This work is focused on exploring efficient ways to increase the accuracy of sentiment analysis in Egyptian Arabic. The proposed system is based on semantic orientation (Cosine similarity and ISRI Arabic stemmer) and machine learning techniques. Experimental results showed that it achieves an overall accuracy of 92.98% using Linear Support Vector Machine. © 2018, Springer International Publishing AG.","Arabic NLP; Cosine algorithm; Naive bayes classifier; Support vector machine; TFIDF; Twitter sentiment analysis",,2-s2.0-85029536167
"Gorunescu F., Belciug S.","Intelligent Decision Support Systems in Automated Medical Diagnosis",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032019210&doi=10.1007%2f978-3-319-67513-8_8&partnerID=40&md5=43568784a87e928d8986f3af16bc2cde","The Intelligent Decision Support Systems (IDSSs) represent an interdisciplinary research domain bringing together Artificial Intelligence/Machine Learning (AI/ML), Decision Science (DS), and Information Systems (IS). IDSS refers to the use of AI/ML techniques in decision support systems. In this context, it should be emphasized the special role of statistical learning (SL) in the process of training algorithms from data. The purpose of this chapter is to provide a short review of some of the state-of-the-art AI/ML algorithms, seen as intelligent tools used in the medical decision-making, along with some important applications in the automated medical diagnosis of some major chronic diseases (MCDs). In addition, we aim to present an interesting approach to develop novel IDSS inspired by the evolutionary paradigm. © Springer International Publishing AG 2018.","Computer-aided medical diagnosis; Evolutionary computation; Intelligent decision support system; Neural networks; Support vector machines",,2-s2.0-85032019210
"Troya-Galvis A., Gançarski P., Berti-Équille L.","Remote sensing image analysis by aggregation of segmentation-classification collaborative agents",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028728912&doi=10.1016%2fj.patcog.2017.08.030&partnerID=40&md5=799e4d7745506e95b478f4077a8e0bf4","In this article we present two different approaches for automatic remote sensing image interpretation which are based on a multi-paradigm collaborative framework which uses classification in order to guide the segmentation process. The first approach applies sequentially many one-vs-all class extractors in a manner inspired by cascading techniques in machine learning. The second approach applies many collaborating one-vs-all class extractors in parallel. We show that the collaboration of the segmentation and classification paradigms result in a remarkable reduction of segmentation errors but also in better object classification in comparison to a hybrid pixel-object approach as well as a deep learning approach. © 2017 Elsevier Ltd","Classification; Collaborative approaches; Remote sensing; Segmentation","Classification (of information); Image analysis; Image classification; Image reconstruction; Image segmentation; Learning systems; Cascading technique; Collaborative agents; Collaborative approach; Collaborative framework; Object classification; Remote sensing image interpretations; Remote sensing images; Segmentation process; Remote sensing",2-s2.0-85028728912
"Mosavi A., Bathla Y., Varkonyi-Koczy A.","Predicting the future using web knowledge: State of the art survey",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029799290&doi=10.1007%2f978-3-319-67459-9_42&partnerID=40&md5=36d0224c895b338f1a8820dfa95695d1","Accurate prediction models can potentially transform businesses, organizations, governments, and industries. Data-driven prediction methods and applications have recently become very popular. One of the novel method of building prediction models is to use data-driven methods and knowledge discovery on the web contents. This includes the news and media as well as social networks contents. This method uses advanced technologies of big data, machine learning, deep learning and intelligent optimization for finding patterns in big data to build prediction models. This article presents a state of the art survey on the latest technological advancements, novel methods, and applications in developing prediction models. © Springer International Publishing AG 2018.","Predictive analytics; Predictive decision models; Web dynamics","Education; Forecasting; Learning systems; Predictive analytics; Surveys; Accurate prediction; Advanced technology; Data-driven methods; Decision models; Intelligent optimization; Prediction methods; Technological advancement; Web dynamics; Big data",2-s2.0-85029799290
"Yamamoto M., Umemura N., Kawano H.","Automated essay scoring system based on rubric",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026676025&doi=10.1007%2f978-3-319-64051-8_11&partnerID=40&md5=33d8dd9927059243d7f84f87097b7690","In this paper, we propose an architecture of automated essay scoring system based on rubric, which combines automated scoring with human scoring. Rubrics are valid criteria for grading students’ essays. Our proposed rubric has five evaluation viewpoints “Contents, Structure, Evidence, Style, and Skill” and 25 evaluation items which are subdivided viewpoints. The system is cloud-based application and consists of several tools such as Moodle, R, MeCab, and RedPen. At first, the system automatically scores 11 items included in the Style and Skill such as sentence style, syntax, usage, readability, lexical richness, and so on. Then it predicts scores of Style and Skill from these items’ scores by multiple regression model. It also predicts Contents’ score by the cosine similarity between topics and descriptions. Moreover, our system classifies into five grades “A+, A, B, C, D” as useful information for teachers, by using machine learning techniques such as support vector machine. We try to improve automated scoring algorithms and a variety of input essays in order to improve accuracy of classification over 90%. © Springer International Publishing AG 2018.","Automated scoring; Cosine similarity; Essay evaluation; Multiple regression model; Rubric; Support vector machine",,2-s2.0-85026676025
"Park S.J., Subramaniyam M., Hong S., Kim D.","Service based healthcare monitoring system for the elderly - Physical activity and exercise",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023174480&doi=10.1007%2f978-3-319-60483-1_34&partnerID=40&md5=6794a74d084128474f40278471ed424d","The chances of surviving a fall, heart attack, and stroke are six times greater if the elderly get emergency assistance within an hour of incidence. Therefore, the elderly real-time healthcare monitoring system is necessary to reduce the anxiety of them and the risk of accidents. The purpose of this study is to successfully detect and generate alarms in cases of sudden stroke onset while doing physical activity and exercise. The purpose would be done by the development of an elderly health monitoring system, which is controlled by hyper-connected self-machine learning engine. The components of the system are a knowledge base, real-time data monitoring, network security, and self-learning engine. The knowledge base would have risk factors, medical health records, psychological factors, gait and motion patterns, and bio-signals. The old peoples’ activities are monitored in real-time through wearable sensors. The method mentioned above and its frameworks will be discussed in this paper. © Springer International Publishing AG 2018.","Disease prediction; Elderly; Stroke onset; Wearable sensors","Biomedical equipment; Education; Engines; Ergonomics; Health care; Health risks; Human engineering; Knowledge based systems; Learning systems; Network security; Wearable sensors; Wearable technology; Elderly; Health monitoring system; Healthcare monitoring; Psychological factors; Real-time data monitoring; Real-time healthcares; Risk of accidents; Stroke onset; Monitoring",2-s2.0-85023174480
"Patel R., Thakkar A., Makwana K., Patel J.","Comprehensive and evolution study focusing on comparative analysis of automatic text summarization",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028373351&doi=10.1007%2f978-3-319-63645-0_43&partnerID=40&md5=111ea60701e532cafd49760ae1908d1d","In the escalating trend of atomization and online information, text summarization bolster in perceiving textual information in the form of summary. It’s highly tedious for human beings to manually summarize large documents of text. In this paper, a study on abstractive and extractive content rundown strategies has been displayed. In Extractive Text Summarization it talk about TF-IDF, Cluster based, Graph theory, Machine learning, Latent Semantic Analysis (LSA) and Fuzzy logic approaches. Abstractive rundown techniques are ordered into two classes i.e. Structured based approach and Semantic based approach. In Structure Based approach it talk about Tree based, Template based, Ontology based, Lead & Phase based and Rule based method. In Semantic Based Approach it talks about Multimodal semantic, Informative item based and Semantic graph based method. The central idea of this method has been elaborated further, apart from idea, the advantages and disadvantages of these methods have been procured. © Springer International Publishing AG 2018.","Abstractive text summarization; Extractive text summarization","Fuzzy logic; Graph theory; Graphic methods; Intelligent systems; Learning systems; Ontology; Text processing; Automatic text summarization; Comparative analysis; Fuzzy logic approach; Latent Semantic Analysis; On-line information; Rule-based method; Text summarization; Textual information; Semantics",2-s2.0-85028373351
"Pal M., Saha S., Konar A.","Probability-induced distance-based gesture matching for health care using microsoft’s kinect sensor",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026763760&doi=10.1007%2f978-981-10-3373-5_28&partnerID=40&md5=86bab43a76eda0fc0417b89534918c10","Detection of 14 healthcare-related gestures due to pain at different body parts is the target area of this work using Kinect sensor. The novelty of our work lies in suppressing the problem of compensation by the use of probability while using similarity matching technique for gesture recognition. The adopted method enhances the matching accuracy for all the similarity measures. A shared probability and similarity measure-based metric has been defined as the matching index. This unique technique contributes to field of health care under static gesture recognition as an application of machine learning with a high accuracy of 99.1071% in 0.0126 s using probability-induced city-block distance. © Springer Nature Singapore Pte Ltd. 2018.","Distance matching; Health care; Kinect sensor; Probability","Computation theory; Health care; Intelligent computing; Learning systems; Probability; City-block distances; Distance matching; Distance-based; Kinect sensors; Matching index; Similarity measure; Similarity-matching; Using probabilities; Gesture recognition",2-s2.0-85026763760
"Patsadu O., Watanapa B., Nukoolkit C.","A multiple-stage classification of fall motions using kinect camera",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022188800&doi=10.1007%2f978-3-319-60663-7_11&partnerID=40&md5=190415f79f27a2b45bf4487c391a46bd","This paper proposes a model of fall detection using hybrid classification methods in video streaming. In particular, we are interested in a stream of data representing time sequential frames of fifteen body joint positions capturable by Kinect camera. A set of features is then extracted and fed into the designated multiple-stage classification. The first stage classifies a fall as a different event from normal activities of daily living (ADLs). The second stage is to classify types of fall once the fall was detected in the first stage, for aiding the diagnosis and treatment of a fall by a physician. We selected a number of reliable machine learning algorithms (MLP, SVM, and decision tree) in forming a hybrid model. Experimental results show that the first stage classifier can differentiate falls and ADLs with 99.98% accuracy and the second stage classifier can identify type of fall with 99.35% accuracy. © Springer International Publishing AG 2018.","Fall detection; Hybrid classification methods; Kinect camera; Multiple-stage classifier; Smart home system","Automation; Cameras; Decision trees; Intelligent buildings; Learning algorithms; Motion analysis; Video streaming; Fall detection; Hybrid classification; Kinect cameras; Multiple stages; Smart-home system; Data mining",2-s2.0-85022188800
"Novakovic J., Veljovic A., Ilic S.S., Veljovic V.","Improving the accuracy of SVM algorithm in classification problems with PCA method",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031427093&doi=10.1007%2f978-3-319-68321-8_7&partnerID=40&md5=fa79201797cbf8f2dda29501379e63bd","This paper investigates the use of SVM algorithm with PCA method in classification, which is one of the most common task of machine learning. Classification is the problem of identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. SVM algorithm can produce accurate and robust classification results on a sound theoretical basis, even when input data are non-monotone and non-linearly separable. So they can help to evaluate more relevant information in a convenient way. PCA method reduces the dimensionality and the maximum number of new variables that can be obtained is equal to the original, with new variables are not correlated with each other. Experimental studies have shown that it is possible to improve the accuracy of SVM classification algorithm using PCA method. © Springer International Publishing AG 2018.","Classification problem; Dimensionality reduction; PCA; SVM","Learning systems; Dimensionality reduction; Input datas; Linearly separable; PCA method; Robust classification; SVM algorithm; SVM classification; Training sets; Classification (of information)",2-s2.0-85031427093
"Keshwani K., Agarwal P., Kumar D., Ranvijay","Prediction of market movement of gold, silver and crude oil using sentiment analysis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410637&doi=10.1007%2f978-981-10-3773-3_11&partnerID=40&md5=c79889467c6cec38181aba8438dd087f","Prediction of stock movements and share market has always remained an area of great curiosity and concern for investors. It has already been established that the movement of market shares a big correlation with the sentiments about it. In this paper, we have applied sentiments analysis techniques and machine learning principles to foretell the stock market trends of three major commodities, Gold, Silver and Crude oil. We have used the SentiWordNet library to quantify the emotions expressed in the text. Further neural network has been trained over the calculated readings. Thereafter, the trained neural network is used to forecast the future values. The efficacy of the proposed model is measured on the basis of mean absolute percentage error. The results clearly reflect that there in fact lies a strong correlation between public mood and stock market variations. © Springer Nature Singapore Pte Ltd. 2018.","Microblogging data; Neural network; Sentiment analysis; SentiWordNet; Stock market","Commerce; Competition; Crude oil; Data mining; Electronic trading; Finance; Financial markets; Forecasting; Gold; Learning systems; Neural networks; Petroleum analysis; Silver; Analysis techniques; Market variations; Mean absolute percentage error; Microblogging; Sentiment analysis; SentiWordNet; Strong correlation; Trained neural networks; Investments",2-s2.0-85031410637
"Wang X., Zuo M., Song L.","A feature selection method based on information gain and BP neural network",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030834457&doi=10.1007%2f978-981-10-6496-8_3&partnerID=40&md5=6634e51afb5306b905d91af51a1f5d3c","Data mining and machine learning fields are facing with a great challenge of mass data with high dimensionality. Feature selection can contribute a lot to address this issue with the concept of reducing the number of features by eliminating the redundant and irrelevant ones while preserving the information of original features maximally. This paper analyzes and compares two common feature selection methods, then puts forward a novel method for feature selection based on information gain and BP neural network (IGBP). The experimental result shows that IGBP method can reduce the time cost and improve the accuracy of the model at the meantime. The scientificity and superiority of IGBP are demonstrated in this paper, making it an efficient approach to deal with high-dimensional data. © 2018, Springer Nature Singapore Pte Ltd.","BP neural network; Data mining; Feature selection; IGBP method; Information gain","Clustering algorithms; Feature extraction; Intelligent systems; Learning systems; Neural networks; BP neural networks; Common features; Feature selection methods; High dimensional data; High dimensionality; IGBP method; Information gain; Time cost; Data mining",2-s2.0-85030834457
"Min S.N., Lee K.-S., Park S.J., Subramaniyam M., Kim D.J.","Development of stroke diagnosis algorithm through logistic regression analysis with national health insurance database",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023168858&doi=10.1007%2f978-3-319-60483-1_37&partnerID=40&md5=90164361db460b53a632277378fa1e06","This study purpose is to derive a model equation for developing a stroke pre-diagnosis algorithm with the potentially modifiable risk factors. In this study, logistic regression analysis technique was used for model derivation. It is one of the methods employed in the machine-learning field of statistics. Korea’s National Health Insurance Service (NHIS), one of the largest administrative health care databases around the world, has been used widely in academic studies. From the NHIS Corporation, 500,000 enrollees’ databases were collected. For the regression analysis, 367 stroke patients’ data were selected from the NHIS database. The control group consisted of 500 patients who were followed up for two consecutive years and who had no history of stroke. As a result, the separation accuracy with the modifiable risk factors was 64.7%. The results of this study are expected to be useful for the development of stroke pre-diagnosis algorithms. © Springer International Publishing AG 2018.","Logistic regression analysis; Modifiable risk factors; Stroke diagnosis","Biomedical equipment; Database systems; Diagnosis; Ergonomics; Health care; Health insurance; Human engineering; Insurance; Learning algorithms; Risk assessment; Control groups; Diagnosis algorithms; Logistic regression analysis; Model derivations; Model equations; Risk factors; Stroke patients; Regression analysis",2-s2.0-85023168858
"Srivastava D.K., Nair P.","Employee attrition analysis using predictive techniques",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028428954&doi=10.1007%2f978-3-319-63673-3_35&partnerID=40&md5=d4363b592d85e8869b4c9778510afe86","Employee churn is an unsolicited aftermath of our blooming economy. Attrition may be defined as voluntary or involuntary resignation of a serving employee from an organization. Employee churn can incur a colossal cost to the firm. However, furtherance to prediction and control over attrition can give quality results. Earmarking the risk of attrition, the management can take required steps to retain the high valued talent. Workforce Analytics can be applied to reduce the overall business risk by predicting the employee churn. Predictive Analytics is the field of study that employs statistical analysis, data mining techniques and machine learning to predict the future events with accuracy based on past and current situation. The paper presents a framework for predicting the employee attrition with respect to voluntary termination employing predictive analytics. © 2018, Springer International Publishing AG.","Data mining; Employee attrition; Predictive algorithms; Predictive analytics; Turnover prediction","Data mining; Forecasting; Intelligent systems; Learning systems; Quality control; Business risks; Current situation; Prediction and control; Predictive algorithms; Predictive techniques; Risk of attritions; Predictive analytics",2-s2.0-85028428954
"Kumar H., Kaur P.","Social media user ranking based on temporal trust",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028427706&doi=10.1007%2f978-3-319-63673-3_72&partnerID=40&md5=cb8c8e376001d592a04584a7b6101934","This paper proposes a methodology to compute trust and rank user on the basis of time, number and nature of interactions. Technique models trust based on two main factors: Engagement of user, temporal/behavioural factor based on difference between time of user reaction to that of any social media activity. It is evaluated using verified Facebook page of ‘Panjab University’ with 160,000+ users. Validation uses 985 active users. On analyzing the result it is found that the top scorers are from ‘Public Relation Department’ of university. Naïve Bayesian machine learning technique has classified the data more accurately as compared to SVM and Logistic regression. Accuracy of Naïve Bayesian is 77.54%, as the condition of independence of dataset is satisfied and degree of overlapping is null. This result states that proposed model provide efficient technique to rank users on social networks. © 2018, Springer International Publishing AG.","Behavioral trust; Engagement trust; Propagational trust; Social capital; Social influence; Social networks; Social trust","Intelligent systems; Learning systems; Public relations; Social networking (online); Sodium; Behavioral trust; Engagement trust; Propagational trust; Social capitals; Social influence; Social trust; Economic and social effects",2-s2.0-85028427706
"Sharma D.","Study of sentiment analysis using hadoop",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031424428&doi=10.1007%2f978-981-10-6620-7_35&partnerID=40&md5=89ddd40d565f2a809a17feb3e1f89067","In the current world of Internet people express themselves, present their views and feelings about specific topics or entities using various social media application. These posts from users present a huge opportunity for the organizations to increase their market value by analyzing the posts and using information in decision making. These posts can be studied using various machine learning and lexicon-based approaches for extracting its sentiments. With more and more people moving to internet, huge data is being produced every second and challenge is to store this large data and process it efficiently in real time to infer knowledge from this data. This paper presents different approaches for real-time and scalable ways of performing sentiment analysis using Hadoop in a time efficient manner. Hadoop and its component tools like MapReduce, Mahout, and Hive are being surveyed in different scholar articles for this paper. © 2018, Springer Nature Singapore Pte Ltd.","Hadoop; Hive; Mahout; MapReduce; Sentiment analysis; Twitter","Behavioral research; Data mining; Decision making; Learning systems; Social networking (online); Hadoop; Hive; Mahout; Map-reduce; Sentiment analysis; Twitter; Big data",2-s2.0-85031424428
"Settouti N., El Habib Daho M., Bechar M.E.A., Lazouni M.A., Chikh M.A.","Semi-automated method for the glaucoma monitoring",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031999059&doi=10.1007%2f978-3-319-63754-9_11&partnerID=40&md5=8330f1fc58a12fa0d65511e50bc593ab","The current trend of computer vision and image processing systems in biomedical field is the application of the Computational Intelligence (CI) approaches, which include the use of tools as machine learning and soft computing. The CI approaches bring a new solution to automatic feature extraction for a particular task. Based on that techniques, we have proposed in this work a semi-automated method for the glaucoma monitoring through retinal images. Glaucoma is a disease caused by neuro-degeneration of the optic nerve leading to blindness. It can be assessed by monitoring Intraocular Pressure (IOP), by the visual field and the aspect of the optic disc (ratio cup/disc). Glaucoma increases the rate of cup/disc (CDR), which affects the loss of peripheral vision. In this work, a segmentation method of cups and discs regions is proposed in a semi-supervised pixel-based classification paradigm to automate the cup/disc ratio calculation for the concrete medical supervision of the glaucoma disease. The idea is to canvas the medical expert for labeling the regions of interest (ROI) (three retinal images) and automate the segmentation by intelligent region growing based on machine learning. A comparative study of semi-supervised and supervised methods is carried out in this proposal, by mono approaches (decision tree and SETRED) and multi-classifiers (Random Forest and co-Forest). Our proposition is evaluated on real images of normal and glaucoma cases. The obtained results are very promising and demonstrate the efficacy and potency of segmentation by the multi-classifier systems in semi-automatic segmentation. © Springer International Publishing AG 2018.","Co-forest; Fuzzy C-means; Glaucoma monitoring; Pixel-based classification; Semi-automatic segmentation; Semi-supervised learning",,2-s2.0-85031999059
"Lee H., Upright C., Eliuk S., Kobsa A.","Personalized visual recognition via wearables: A first step toward personal perception enhancement",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028944661&doi=10.1007%2f978-3-319-62530-0_6&partnerID=40&md5=2f01a8e2c3ee10f6400cfaae2f69cb5e","During the last few years, deep learning has led to an astonishing advancement in visual recognition. Computers now reach near-human accuracy in visually recognizing characters, physical objects and human faces. This will certainly allow us to build more intelligent personal assistants that can help users better understand their surrounding environments. However, most visual recognition systems have been designed for user-independent recognition (e.g., Google reverse image search), and not for an individual user. We believe this practice is restricting the technology from helping people who have individual needs. For example, a person with memory problems may want to have a computer that accurately recognizes a few close friends, rather than hundreds of celebrities. To address this issue, we propose a novel wearable system that enables users to create their own visual recognition system with minimal effort. A client running on Google Glass collects images of objects a user is interested in, and sends them to the server with a request for a specific machine learning task: training or classification. The server performs deep learning according to the request and returns the result to Glass. Regarding the training task, our system not only aims to build deep learning models with user generated image data, but also to update the models whenever new data is added by the user. Experiments show that our system is able to train the custom deep learning models in an efficient manner, in terms of the required amount of computing power and training data. Based on the customized deep learning model, the system classifies an image into one of 10 different user-defined categories with 97% accuracy. © Springer International Publishing AG 2018.","Deep convolutional neural networks; Finetuning; Google glass; Object recognition; Person identification; Personalization; Transfer learning",,2-s2.0-85028944661
"Maniak T., Iqbal R., Doctor F.","Traffic modelling, visualisation and prediction for urban mobility management",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032360117&doi=10.1007%2f978-3-319-66790-4_4&partnerID=40&md5=b637f58ed0c2a0deb88722baec291722","Smart city combines connected services from different disciplines offering a promise of increased efficiency in transport and mobility in urban environment. This has been enabled through many important advancements in fields like machine learning, big data analytics, hardware manufacturing and communication technology. Especially important in this context is big data which is fueling the digital revolution in an increasingly knowledge driven society by offering intelligence solutions for the smart city. In this paper, we discuss the importance of big data analytics and computational intelligence techniques for the problem of taxi traffic modelling, visualisation and prediction. This work provides a comprehensive survey of computational intelligence techniques appropriate for the effective processing and analysis of big data. A brief description of many smart city projects, initiatives and challenges in the UK is also presented. We present a hybrid data modelling approach used for the modelling and prediction of taxi usage. The approach introduces a novel biologically inspired universal generative modelling technique called Hierarchical Spatial-Temporal State Machine (HSTSM). The HSTSM modelling approach incorporates many soft computing techniques including: deep belief networks, auto-encoders, agglomerative hierarchical clustering and temporal sequence processing. A case study for the modelling and prediction of traffic based on taxi movements is described, where HSTSM is used to address the computational challenges arising from analysing and processing large volumes of varied data. © 2018, Springer International Publishing AG.",,"Artificial intelligence; Engineering education; Forecasting; Learning systems; Smart city; Soft computing; Taxicabs; Urban transportation; Visualization; Agglomerative hierarchical clustering; Communication technologies; Computational challenges; Computational intelligence techniques; Modelling and predictions; Softcomputing techniques; Transport and mobilities; Urban mobility managements; Big data",2-s2.0-85032360117
"Coroiu A.M.","Model evaluation as approach to predict a diagnosis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031406667&doi=10.1007%2f978-3-319-62524-9_1&partnerID=40&md5=9be5bb361267e6485b75dacda942a0a5","The paper presents an approach to a relevant issue of the supervised learning: classification. Creating the models that are able to generalize to new data, tuning the model so that the performance can be increased and models evaluation are relevant task of this sub-field of machine learning. In this paper, we have chosen to treat the evaluation of a model. The paper consists in two experiments, each of them with a particular purpose. First experiment outlines different model evaluation methods using specific performance metrics. This paper analyzes two models, logistic regression and the k-nearest neighbor with three methods of evaluation: train-test approach, test-set approach and the k-cross validation approach. On our analyzed data set, we achieved reasonable results using logistic regression as model with k-cross validation approach as evaluating method of the model. The second experiment determines the rank of the observations by predicting the probabilities depending on the response vector. Based on the predicted probability we try to improve the metric performance. For our particular task, one metric that allows us to extract relevant information from data and can be improved is the sensitivity metric. © Springer International Publishing AG 2018.",,"Learning systems; Nearest neighbor search; Regression analysis; Cross validation; Evaluating method; K-nearest neighbors; Logistic regressions; Model evaluation; Performance metrics; Sub fields; Test sets; Soft computing",2-s2.0-85031406667
"Bach M., Werner A.","Cost-sensitive feature selection for class imbalance problem",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029508811&doi=10.1007%2f978-3-319-67220-5_17&partnerID=40&md5=354b63ab3ed9063f3fa70c5c0b86e844","The class imbalance problem is encountered in real-world applications of machine learning and results in suboptimal performance during data classification. This is especially true when data is not only imbalanced but also high dimensional. The class imbalance is very often accompanied by a high dimensionality of datasets and in such aÂ case these problems should be considered together. Traditional feature selection methods usually assign the same weighting to samples from different classes when the samples are used to evaluate each feature. Therefore, they do not work good enough with imbalanced data. In situation when the costs of misclassification of different classes are diverse, cost-sensitive learning methods are often applied. These methods are usually used in the classification phase, but we propose to take the cost factors into consideration during the feature selection. In this study we analyse whether the use of cost-sensitive feature selection followed by resampling can give good results for mentioned problems. To evaluate tested methods three imbalanced and multidimensional datasets are considered and the performance of chosen feature selection methods and classifiers are analysed. © 2018, Springer International Publishing AG.","Class imbalance problem; Classification; Cost sensitive learning; Feature selection",,2-s2.0-85029508811
"Li J., Fong S., Wong R.K., Chu V.W.","Adaptive multi-objective swarm fusion for imbalanced data classification",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016392535&doi=10.1016%2fj.inffus.2017.03.007&partnerID=40&md5=6b582a17fdabebe7881a6f2f7424cbf8","Learning a classifier from an imbalanced dataset is an important problem in data mining and machine learning. Since there is more information from the majority classes than the minorities in an imbalanced dataset, the classifier would become over-fitted to the former and under-fitted to the latter classes. Previous attempts to address the problem have been focusing on increasing the learning sensitivity to the minorities and/or rebalancing sample sizes among classes before learning. However, how to efficiently identify their optimal mix in rebalancing is still an unresolved problem. Due to non-linear relationships between attributes and class labels, merely to rebalance sample sizes rarely comes up with optimal results. Moreover, brute-force search for the perfect combination is known to be NP-hard and hence a smarter heuristic is required. In this paper, we propose a notion of swarm fusion to address the problem – using stochastic swarm heuristics to cooperatively optimize the mixtures. Comparing with conventional rebalancing methods, e.g., linear search, our novel fusion approach is able to find a close to optimal mix with improved accuracy and reliability. Most importantly, it has found to be with higher computational speed than other coupled swarm optimization techniques and iteration methods. In our experiments, we first compared our proposed solution with traditional methods on thirty publicly available imbalanced datasets. Using neural network as base learner, our proposed method is found to outperform other traditional methods by up to 69% in terms of the credibility of the learned classifiers. Secondly, we wrapped our proposed swarm fusion method with decision tree. Notably, it defeated six state-of-the-art methods on ten imbalanced datasets in all evolution metrics that we considered. © 2017 Elsevier B.V.","Crossover rebalancing; Imbalanced data classification; Multi-objective; Swarm fusion; Swarm intelligence algorithm","Artificial intelligence; Data mining; Decision trees; Iterative methods; Learning systems; Optimization; Stochastic systems; Swarm intelligence; Computational speed; Imbalanced data; Imbalanced Data-sets; Multi objective; Non-linear relationships; Rebalancing; State-of-the-art methods; Swarm intelligence algorithms; Classification (of information)",2-s2.0-85016392535
"Iguchi N.","Virtual IP network practice system with software agent",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026312543&doi=10.1007%2f978-3-319-61566-0_66&partnerID=40&md5=1ff9d858ed40cbeccdf4e22210528ea5","It is important for beginning network engineering students to frequently practice setting up fundamental IP networks according to predetermined procedures. We have been developing NetPowerLab, a hands-on IP network practice system that uses virtual machines to provide an IP network learning environment. In this paper, we report the new development of IP network practice system with software agents. Software agents operate as a teacher or a cooperative student. The new system has three mode. The tutorial mode is used by beginner students with software agent operating as teacher. The practice mode is used by intermediate students with software agent operating as intermediate cooperative student. The troubleshooting mode is used by advanced students with software agent operating as advanced cooperative student. We describe outline of the system and software agent for IP network practice in this paper. © Springer International Publishing AG 2018.",,"Complex networks; Computer aided instruction; Education; Software agents; Students; Teaching; Virtual machine; IP networks; Network engineering; Internet protocols",2-s2.0-85026312543
"Itani S., Lecron F., Fortemps P.","A multi-level classification framework for multi-site medical data: Application to the ADHD-200 collection",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028500199&doi=10.1016%2fj.eswa.2017.08.044&partnerID=40&md5=7cdfa8fdc386b982509d1b16c624de5d","Recently, the culture of sharing medical data has emerged impressively, reducing significantly the barrier to the development of medical research accordingly. As open-access large datasets result from this significant initiative, data mining techniques can be considered for the development of interpretable expert systems to help in diagnosis. However, the collaborative effort of information gathering yields heterogeneous databases because of technical and geographical factors. Indeed, on the one hand, the harmonization of protocols for data collection is still missing. On the other hand, cultural and social factors impact locally both the epidemiology and etiology of a given disease. Ignoring these factors could weaken the credibility of studies based on multi-site data. Thereby, our work tackles the development of computer-aided diagnosis systems relying on heterogeneous data. For such a purpose, we propose a multi-level approach (inspired by multi-level statistical modeling) based on decision trees (in the sense of machine learning). This framework is applied on the public ADHD-200 collection for the study of Attention Deficit Hyperactivity Disorder (ADHD). © 2017 Elsevier Ltd","Attention Deficit Hyperactivity Disorder (ADHD); Clinical decision support systems; Decision trees; Multi-level approach","Artificial intelligence; Computer aided diagnosis; Data mining; Decision support systems; Decision trees; Diseases; Distributed computer systems; Expert systems; Forestry; Learning systems; Attention deficit hyperactivity disorder; Clinical decision support systems; Computer aided diagnosis systems; Heterogeneous database; Information gathering; Multi-level classifications; Multilevels; Statistical modeling; Diagnosis",2-s2.0-85028500199
"Nastuta A.V., Agheorghiesei C.","Monitoring hand gesture and effort using a low-cost open-source microcontroller system coupled with force sensitive resistors and electromyography sensors",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029818949&doi=10.1007%2f978-3-319-67459-9_33&partnerID=40&md5=bbd4fca75538b8f76367cd472fcb2492","In this study, we consider a low-cost open-source environment, where users interact with several computing devices and platforms. Thus, the specific usage of any tool requires a specific configuration process in order to meet the end user’s needs. The aim is to compare the effectiveness of hand gesture recognition using electromyography (EMG) electrodes when using sensors located on the forearm in comparison to force-sensitive resistor (FSR) array located over the fingers of the hand. Our study involves monitoring the movement of the fingers in a single (angular) direction corresponding to gestures of gripping and releasing objects (a single degree of freedom). Our interest is in how the relocation of sensors would affect the classification rates of finger gestures. Our study confirmed that by including EMG along the FSR sensors the classification rate for different kinds of gesture (including all fingers and wrist) increased, providing a better understanding of the complex hand dynamics. These findings can be used in machine learning systems for developing versatile hand prosthesis or in rehabilitation. © Springer International Publishing AG 2018.","Biomedical-based sensors; Hand gestures","Degrees of freedom (mechanics); Education; Electromyography; Learning systems; Microcontrollers; Resistors; Classification rates; Configuration process; Electromyography (EMG) electrode; Force sensitive resistors; Hand gesture; Hand-gesture recognition; Microcontroller systems; Single degree of freedoms; Open systems",2-s2.0-85029818949
"Anjana K., Radhika K., Darshana P.","Imbalanced data stream classification: Analysis and solution",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028400475&doi=10.1007%2f978-3-319-63645-0_35&partnerID=40&md5=47334b1f0688a8f77ae0c1486daa26f2","Through the progress in each hardware and software system technologies, automatic data creation and storage have become quicker than ever. Such data is called as a data stream. Streaming information is present everywhere and it’s usually a difficult problem to visualize, collect and examine such huge volumes of information. Data stream mining has become a unique experimental area in information finding because of the large size and rapid speed of data in the data stream, due to this reason conventional classification methods are not effective. In today`s a substantial amount of analysis has been done on this issue whose main aim is to efficiently solve the difficulty of information stream mining with concept drift. Class imbalance is one of the problems of machine learning and data processing fields. Imbalance data sets reduce the performance as well as the overall accuracy of data mining methods. Decision making towards the majority class, which lead to misclassifying the minority class examples or moreover considered them as noise. © Springer International Publishing AG 2018.","Data stream; Ensemble method; Hoeffding tree; K-nearest neighbor; MSMOTE; SMOTE","Classification (of information); Data communication systems; Data handling; Decision making; Digital storage; Intelligent systems; Learning systems; Nearest neighbor search; Trees (mathematics); Data stream; Ensemble methods; Hoeffding tree; K-nearest neighbors; MSMOTE; SMOTE; Data mining",2-s2.0-85028400475
"Monroy-Tenorio F., Batyrshin I., Gelbukh A., Solovyev V., Kubysheva N., Rudas I.","Correlation measures for bipolar rating profiles",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669431&doi=10.1007%2f978-3-319-67137-6_3&partnerID=40&md5=5148531bc4c84a8b441cdd4484aee28f","We introduce new correlation measures for measuring similarity and association of rating profiles obtained from bipolar rating scales. Instead of the measurement based approach when the user’s rating is considered as a number measured in ordinal, interval or ratio scales we use model based approach when user’s rating is modeled by bipolar score function that can be nonlinear. This approach can use different models of preferences for different users. The values of utility function can be adjusted in machine learning procedure to obtain better solutions on the output of recommender or decision making system. We show that Pearson’s correlation coefficient often used for measuring similarity between bipolar rating profiles in recommender systems has some drawbacks. New correlation measures proposed in the paper have not these drawbacks. These measures are obtained using general methods of construction of association measures from similarity measures on sets with involutive operation. Proposed measures can be used in recommender systems, in opinion mining and in sociological research for analysis of possible relationships between opinions of users and ratings of items. © Springer International Publishing AG 2018.","Association measure; Bipolar scale; Correlation; Opinion mining; Rating scale; Recommender system; Sentient analysis","Correlation methods; Data mining; Decision making; Learning systems; Recommender systems; Association measures; Bipolar scale; Opinion mining; Rating scale; Sentient analysis; Rating",2-s2.0-85030669431
"Khawaja S.G., Khan A.M., Akram M.U., Khan S.A.","A novel architecture for k-means clustering algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028652714&doi=10.1007%2f978-3-319-60834-1_31&partnerID=40&md5=119624485bc57603b0d9a9d76a0b4749","Technological advancements in today information age has helped the researchers to capture digital footprints of humans with regards to their daily activities. These logs of information posses valuable information for the data analytics who process it to find hidden pattern and unique behavior. Among the many algorithms k-means clustering is one of the very popular and widely used algorithm in the field of data mining and machine learning. k-means provides natural segments of dataset provided for clustering. It uses proximity to assign data points to a specific cluster, here the criteria of allocation is the minimum distance from the cluster center. Unfortunately, the rate of data growth has not been met by the speed of the algorithms. A number of hardware based solutions have been proposed to increase the processing power of different algorithms. In this paper, we present a novel algorithm for k-mean clustering which exploits the data redundancy occurring in the dataset. The proposed algorithm performs computations for the available unique items in the dataset and uses its frequency to finalize the results. Furthermore, FPGA based hardware architecture for the proposed algorithm is also presented in the paper. The performance of the proposed algorithm and its hardware implementation is evaluated using execution time, speedup and throughput. The proposed architecture provides speedup of 23 times and 2600 times against sequential hardware architecture and software implementation with a very small area requirement. © 2018, Springer International Publishing AG.",,"Data mining; Hardware; Learning systems; Hardware architecture; Hardware implementations; K-means clustering; K-Means clustering algorithm; Novel architecture; Proposed architectures; Software implementation; Technological advancement; Clustering algorithms",2-s2.0-85028652714
"Kaing D., Medsker L.","Competitive hybrid ensemble using neural network and decision tree",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669103&doi=10.1007%2f978-3-319-67137-6_16&partnerID=40&md5=ecaac6d6d425069629dccdc972c9ef51","A group of experts can offer a more-informed opinion than any individual expert. In machine learning, the ensemble algorithm mirrors this real-world approach by combining predictions of multiple models, yielding higher performance than any individual model. However, having many models does not ensure optimal performance, the challenge is to choose the best set of models that are both diverse and accurate. In this paper, we propose an ensemble model selection algorithm for a hybrid ensemble, called competitive hybrid ensemble (CHE). CHE first creates a population of models, and then ranks the performance of each model on the validation set. From this ranking, CHE assembles the ensemble candidates and evaluates them on the training set. Finally, the best performing candidate is selected as the final hybrid ensemble. We tested our algorithm using neural network and decision tree as the base models. We compared CHE with random forest, a simple hybrid ensemble without the proposed method, and four types of neural network ensembles. Results show that CHE significantly outperforms or is on-par with most of the other methods. © Springer International Publishing AG 2018.","Decision tree; Ensemble; Hybrid; Neural network","Decision trees; Learning systems; Neural networks; Ensemble; Ensemble algorithms; Ensemble modeling; Hybrid; Individual modeling; Neural network ensembles; Optimal performance; Random forests; Data mining",2-s2.0-85030669103
"Niemiec R.M., Asner G.P., Brodrick P.G., Gaertner J.A., Ardoin N.M.","Scale-dependence of environmental and socioeconomic drivers of albizia invasion in Hawaii",2018,"Landscape and Urban Planning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028561787&doi=10.1016%2fj.landurbplan.2017.08.008&partnerID=40&md5=d3720421aa26f10f189a3557244564b3","To reduce the spread and impacts of invasive species in human-dominated landscapes, numerous and diverse residents often need to engage in individual as well as collective invasive species control efforts on their property and in their community. Combating invasion thus requires an understanding of what socioeconomic factors, in addition to ecological factors, may slow an invasive species or facilitate its spread. However, few studies have examined associations between invasive species and multi-scale socioeconomic and environmental factors, which may influence residents’ decisions to control invaders. We combine spatially explicit social and environmental datasets, and we apply gradient boosting regression to examine the socioeconomic, land use, and environmental factors associated with the distribution of albizia (Falcataria moluccana), an invasive tree species, in Hawaii. We find that environmental factors are the dominant controls on albizia cover at the landscape scale, but socioeconomic variables lead to a modest improvement in the ability to predict albizia distribution at the housing subdivision scale. At the subdivision scale, albizia is more common on properties with absentee and/or less-wealthy landowners. Albizia is also more common on smaller properties and non-agricultural land. Our study provides policy recommendations for reducing the spread of invaders and outlines an approach using computation machine learning for examining multiple socioeconomic and environmental factors associated with biological invasion in complex social landscapes. © 2017 Elsevier B.V.","Boosted regression trees; Conservation behavior; Invasive species; Private lands conservation; Species distribution models","Agricultural machinery; Conservation; Economics; Forestry; Land use; Learning systems; Boosted regression trees; Invasive species; Invasive species controls; Non-agricultural lands; Policy recommendations; Private land; Social and environmental; Species distribution models; Population distribution; agricultural land; anthropogenic effect; biological invasion; conservation management; ecological impact; environmental conditions; environmental modeling; invasive species; land management; legume; regression analysis; scale effect; socioeconomic conditions; Hawaii [United States]; United States; Albizia; Falcataria moluccana",2-s2.0-85028561787
"Qian C., Yang X.","An integrated method for atherosclerotic carotid plaque segmentation in ultrasound image",2018,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030683977&doi=10.1016%2fj.cmpb.2017.10.002&partnerID=40&md5=7af8ab6f589384dc4313abdd748f00a7","Background and objective Carotid artery atherosclerosis is an important cause of stroke. Ultrasound imaging has been widely used in the diagnosis of atherosclerosis. Therefore, segmenting atherosclerotic carotid plaque in ultrasound image is an important task. Accurate plaque segmentation is helpful for the measurement of carotid plaque burden. In this paper, we propose and evaluate a novel learning-based integrated framework for plaque segmentation. Methods In our study, four different classification algorithms, along with the auto-context iterative algorithm, were employed to effectively integrate features from ultrasound images and later also the iteratively estimated and refined probability maps together for pixel-wise classification. The four classification algorithms were support vector machine with linear kernel, support vector machine with radial basis function kernel, AdaBoost and random forest. The plaque segmentation was implemented in the generated probability map. The performance of the four different learning-based plaque segmentation methods was tested on 29 B-mode ultrasound images. The evaluation indices for our proposed methods were consisted of sensitivity, specificity, Dice similarity coefficient, overlap index, error of area, absolute error of area, point-to-point distance, and Hausdorff point-to-point distance, along with the area under the ROC curve. Results The segmentation method integrated the random forest and an auto-context model obtained the best results (sensitivity 80.4 ± 8.4%, specificity 96.5 ± 2.0%, Dice similarity coefficient 81.0 ± 4.1%, overlap index 68.3 ± 5.8%, error of area -1.02 ± 18.3%, absolute error of area 14.7 ± 10.9%, point-to-point distance 0.34 ± 0.10 mm, Hausdorff point-to-point distance 1.75 ± 1.02 mm, and area under the ROC curve 0.897), which were almost the best, compared with that from the existed methods. Conclusions Our proposed learning-based integrated framework investigated in this study could be useful for atherosclerotic carotid plaque segmentation, which will be helpful for the measurement of carotid plaque burden. © 2017 Elsevier B.V.","Atherosclerotic carotid plaque; Auto-context model; Random forest; Semi-automatic plaque segmentation; Ultrasound Image","Adaptive boosting; Decision trees; Diseases; Errors; Iterative methods; Radial basis function networks; Support vector machines; Ultrasonic imaging; Carotid plaques; Context modeling; Random forests; Semi-automatics; Ultrasound images; Image segmentation",2-s2.0-85030683977
"Bauer W., Schlund S., Vocke C.","Working life within a hybrid world – How digital transformation and agile structures affect human functions and increase quality of work and business performance",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031300069&doi=10.1007%2f978-3-319-60372-8_1&partnerID=40&md5=123529b16d107fb56f9ae64d95f32cf3","Digitization is dramatically changing economy and society. With current developments in the field of e.g. artificial intelligence and machine learning, big data and data analytics, cloud computing, conversational systems and adaptive architectures, robotics as well as virtual and augmented reality work life is facing huge challenges. On the other side the networking over the internet, more effective handling and sharing of data and new forms of human-machine-collaboration offer a great variety of potentials for designing even more flexible business processes, agile working structures and even smarter working setups and environments. Technique, organizational aspects and humans in the future are going to be within a new triad. Instead of taking the role of a “dominator” or “captain” as in former times, humans now more and more have to fulfill tasks as a “conductor”. The role of building up and interacting within new hybrid networks and holistic systems is gaining higher importance – leading to massive changes with reference to all dimensions of work. Total new requirements concerning work objectives, working tasks, work equipment, workspace as well as new challenges for organization, qualification, employment and leadership arise. Work is becoming more and more digitally and going to look quite different than expected today. Combining the physical and virtual world is representing the key success factor for future work. The study examines how digitization is going to penetrate working life further on displaying central measures and selected solutions for resulting organizational structures, human qualification needs and optimized working conditions in a hybrid world. © Springer International Publishing AG 2018.","Agility; Collaboration; Data; Digitization; Industry 4.0; Organization; Performance; Qualifications; Requirements; Technology","Agile manufacturing systems; Analog to digital conversion; Artificial intelligence; Augmented reality; Distributed computer systems; Human engineering; Learning systems; Societies and institutions; Technology; Virtual reality; Agility; Collaboration; Data; Performance; Qualifications; Requirements; Big data",2-s2.0-85031300069
"Reda A., Fakharany E., Hazman M.","Early prediction of wheat diseases using SVM multiclass",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029532692&doi=10.1007%2f978-3-319-64861-3_24&partnerID=40&md5=85270fa931412aee5831e3a6419609ba","The early prediction of Plant diseases based on learning algorithms is one of promising research areas. Several types of classification techniques can be utilized on such data to early predict the different kinds of wheat diseases. However, the high dimension of the dataset in our case study and how selecting of the best data mining classifiers is one of the challenges. For that, Principle Component Analysis (PCA) technique was carried out for reducing the dimension by combining a set of correlated features as preprocessing step. Then, the Support Vector Machine (SVM) classifier with different multiclass techniques has been applied to predict of wheat diseases. The results have been combined with different voting methods in conjunction with PCA. The proposed system evaluated by several measurements and the classification accuracy reached to 96%. © 2018, Springer International Publishing AG.","Data mining; Data preprocessing; Principle component analysis; Support vector machine; Wheat diseases",,2-s2.0-85029532692
"Sánchez-Silva D.M., Acosta-Mesa H.G., Romo-González T.","Semi-Automatic Analysis for Unidimensional Immunoblot Images to Discriminate Breast Cancer Cases Using Time Series Data Mining",2018,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026352050&doi=10.1142%2fS0218001418600042&partnerID=40&md5=b052b03e133d9175781c358d56064a26","Breast cancer (BC) is one of the leading causes of death in adult women worldwide and the best way to reduce mortality and improve prognosis is through early diagnosis. Thus, it is necessary to optimize diagnostic methods; one option could be the automatic detection of patterns in 1D-II. In that respect, through recent analysis of unidimensional Immunoblot Images (1D-II), it was possible to distinguish between women with and without breast disease using as a discrimination criterion the presence of autoantibodies (bands) in their blood. However, the analysis of 1D-II is a difficult task even for an expert, generating great subjectivity and complexity in the process of interpretation. In the present study, a semi-automatic methodology for the bands' analysis contained in the 1D-II's was implemented and evaluated, the bands were extracted using digital image processing techniques. This was possible through the recognition of banding patterns represented as time series to distinguish between three classes: women with breast cancer (BC), women with benign breast pathology (BBP) and women without breast pathology (H). The classification was performed using the machine learning algorithm k-nearest neighbors (KNN) with different parameters over the time series representation. The semi-automatic method here presented was able to reduce the time, complexity and subjectivity of the image analysis with the performance metrics compared, obtaining similar percentages for both representations. With the traditional analysis, binary representation [Accuracy 72.8%, Precision 73.42% for three classes (BC, BBP and H) and Accuracy 90.91% Accuracy 92.55% Sensitivity 93.57% and Specificity 92.99% for two classes (BC and H)], versus Time series representation [Accuracy 66.4%, Precision 67.07% for three classes (BC, BBP and H) and Accuracy 86.36% Accuracy 87.31% Sensitivity 95.86% and Specificity 85.56% for two classes (BC and H)]. © 2018 World Scientific Publishing Company.","Breast cancer; digital image processing; protein bands; semi-automatic method; time series data mining; unidimensional immunoblot","Automation; Data mining; Diagnosis; Diseases; Image analysis; Image processing; Learning algorithms; Medical imaging; Nearest neighbor search; Pathology; Pattern recognition; Time series; Breast Cancer; Immunoblots; Protein bands; Semiautomatic methods; Time series data mining; Time series analysis",2-s2.0-85026352050
"Rubin S.H., Bouabana-Tebibel T., Grefe W.K.","On the tractable acquisition of heuristics for software synthesis demonstrating that P~NP",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027889352&doi=10.1007%2f978-3-319-56157-8_1&partnerID=40&md5=e159467a552fee1995e6ac482b687bb6","The Traveling Salesman Problem (TSP) was first formulated in 1930 and is one of the most studied problems in optimization. If the optimal solution to the TSP can be found in polynomial time, it would then follow that every NP-hard problem could be solved in polynomial time, proving P = NP. It will be shown that our algorithm finds P~NP with scale. Using a δ - ε proof, it is straightforward to show that as the number of cities goes to infinity, P goes to NP (i.e., δ > 0). This was demonstrated using a quadratic number of parallel processors because that speedup, by definition, is polynomial. A fastest parallel algorithm is defined. Six distinct 3-D charts of empirical results are supplied. For example, using an arbitrary run of 5,000 cities, we obtained a tour within 0.00001063 percent of the optimal using 4,166,667 virtual processors (Intel Xenon E5-1603 @ 2.8 GHz). To save the calculated extra 209 miles would take a quantum computer, the fastest possible computer, over (5,000!/(2**4,978 * 22!)) * 267,782 centuries. Clearly, the automated acquisition of heuristics and the associated P~NP solutions are an important problem warranting attention. Machine learning through self-randomization is demonstrated in the solution of the TSP. It is also shown, in the small using property lists, for an inductive logic of abduction. Finally, it is argued that self-randomizing knowledge bases will lead to the creation of a synthetic intelligence, which enables cyber-secure software automation. © 2018, Springer International Publishing AG.","Heuristics; NP-Hard; P~NP; Randomization; Software automation; Synthetic intelligence; TSP","Automation; Computational complexity; Computer software; Learning systems; Parallel processing systems; Polynomial approximation; Polynomials; Quantum computers; Random processes; Traveling salesman problem; Heuristics; NP-hard; Randomization; Software automation; Synthetic intelligence; Optimization",2-s2.0-85027889352
"Hassan G., Abdelbaki N.","Gesture recognition for improved user experience in augmented biology lab",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029492016&doi=10.1007%2f978-3-319-64861-3_28&partnerID=40&md5=06c9c5502099772d6417654d6d9352ab","The Learning process in education systems is one of the most important issues that affect all societies. Advances in technology have influenced how people communicate and learn. Gaming Techniques (GT) and Augmented Reality (AR) technologies provide new opportunities for a learning process. They transform the student’s role from passive to active in the learning process. It can provide a realistic, authentic, engaging and interesting learning environment. Hand Gesture Recognition (HGR) is a major driver in the field of Augmented Reality (AR). In this paper, we propose an initiative Augmented Biology Lab (ABL) which mix between Augmented Reality and Gaming Techniques to make the learning process more effective in biology learning. Our contribution in this paper focuses on the integration of hand gesture recognition technique for the use within the proposed ABL to reduce the gap between biology lessons, especially in body anatomy and understanding in an interactive and collaborative way. Furthermore, we present a reliable and robust hand gesture recognition system (ABL-HGR). © 2018, Springer International Publishing AG.","Augmented reality; Gaming techniques; Hand gesture recognition; Support vector machine",,2-s2.0-85029492016
"Silva S., Costa P., Gouvea M., Lacerda A., Alves F., Leite D.","High impedance fault detection in power distribution systems using wavelet transform and evolving neural network",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029810257&doi=10.1016%2fj.epsr.2017.08.039&partnerID=40&md5=5b57771efe5f0b87f9ac75dc8f9ef5a6","This paper concerns how to apply an incremental learning algorithm based on data streams to detect high impedance faults in power distribution systems. A feature extraction method, based on a discrete wavelet transform that is combined with an evolving neural network, is used to recognize spatial–temporal patterns of electrical current data. Different wavelet families, such as Haar, Symlet, Daubechie, Coiflet and Biorthogonal, and different decomposition levels, were investigated in order to provide the most discriminative features for fault detection. The use of an evolving neural network was shown to be a quite appropriate approach to fault detection since high impedance faults is a time-varying problem. The performance of the proposed evolving system for detecting and classifying faults was compared with those of well-established computational intelligence methods: multilayer perceptron neural network, probabilistic neural network, and support vector machine. The results showed that the proposed system is efficient and robust to changes. A classification performance in the order of 99% is exhibited by all classifiers in situations where the fault patterns do not significantly change during tests. However, a performance drop of about 13–24% is exhibited by non-evolving classifiers when fault patterns suffer from gradual or abrupt change in their behavior. The evolving system is capable, after incremental learning, of maintaining its detection and classification performance even in such situations. © 2017 Elsevier B.V.","Evolving neural network; High impedance fault detection; Pattern recognition; Power distribution system; Wavelet transform","Data mining; Discrete wavelet transforms; Electric fault currents; Electric fault location; Electric power system protection; Feature extraction; Learning algorithms; Neural networks; Pattern recognition; Pattern recognition systems; Wavelet decomposition; Wavelet transforms; Classification performance; Computational intelligence methods; Evolving neural network; Feature extraction methods; High impedance fault detection; Multi-layer perceptron neural networks; Power distribution system; Probabilistic neural networks; Fault detection",2-s2.0-85029810257
"Soto R., Crawford B., Olivares R., Escárate F.","A Harmony Search Algorithm to Solve the Manufacturing Cell Design Problem",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029592852&doi=10.1007%2f978-3-319-67618-0_3&partnerID=40&md5=5aa9a8ef98d657d823f8cb0f224486e1","This paper focuses on modeling and solving the Manufacturing Cell Design Problem (MCDP) by using the Harmony Search (HS) metaheuristic. The MDCP consists on grouping machines and parts that they process, into groups called cells. So, the idea is to identify an organization of cells such that the number of times that a piece is transported between these cells is minimized. To this end, we use the HS optimization algorithm, which is based on the process of improvisation performed by musicians to find a perfect musical harmony. The experimental results demonstrate the efficiency of the proposed approach which is able to reach all global optimums for a set of 90 well-known MDCP instances. © 2018, Springer International Publishing AG.","Harmony search algorithm; Manufacturing cell design problem; Metaheuristics","Cells; Computational methods; Cytology; Flexible manufacturing systems; Intelligent systems; Learning algorithms; Manufacture; Cell design; Global optimum; Harmony search; Harmony search algorithms; Meta heuristics; Metaheuristic; Optimization algorithms; Optimization",2-s2.0-85029592852
"Alvear-Sandoval R.F., Figueiras-Vidal A.R.","On building ensembles of stacked denoising auto-encoding classifiers and their further improvement",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017032718&doi=10.1016%2fj.inffus.2017.03.008&partnerID=40&md5=e56b17e85abb084e3fb5b19746cc1cd6","To aggregate diverse learners and to train deep architectures are the two principal avenues towards increasing the expressive capabilities of neural networks. Therefore, their combinations merit attention. In this contribution, we study how to apply some conventional diversity methods –bagging and label switching– to a general deep machine, the stacked denoising auto-encoding classifier, in order to solve a number of appropriately selected image recognition problems. The main conclusion of our work is that binarizing multi-class problems is the key to obtain benefit from those diversity methods. Additionally, we check that adding other kinds of performance improvement procedures, such as pre-emphasizing training samples and elastic distortion mechanisms, further increases the quality of the results. In particular, an appropriate combination of all the above methods leads us to reach a new absolute record in classifying MNIST handwritten digits. These facts reveal that there are clear opportunities for designing more powerful classifiers by means of combining different improvement techniques. © 2017 Elsevier B.V.","Augmentation; Classification; Deep; Diversity; Learning; Pre-emphasis","Classification (of information); Image recognition; Augmentation; Deep; Diversity; Learning; Pre-emphasis; Encoding (symbols)",2-s2.0-85017032718
"Wang Y., Huang Y.","Research on education and teaching resources management system based on ASP.NET",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028421226&doi=10.1007%2f978-3-319-60744-3_46&partnerID=40&md5=3269311d4c8fdb2739c29a8d442e8a45","The system using popular B/S (Browser/server) of the three layer structure of software architecture, to Microsoft Windows Server 2003 network operating system, based on C# ASP.Net application development environment, for SQL Server 2005 for the background number according to the database while the completion of the development. The system in theory and technology is advanced, it is education and teaching staff provides an editable and can check the background of teaching management platform and guarantee the teaching information resources sharing and effective use. The system is oriented to non-computer professionals, so the man-machine interface design is simple and intuitive, easy to operate, clear, functional and practical. In this paper, we first introduce education and teaching management platform of the development background and significance, followed by the introduction of the development environment of education and teaching management platform and implementation techniques, a detailed description of the system design principle and system components and modules of the database structure, and then expounds the platform realization in three layers of structure solution and test, finally has made the summary and the prospect. © 2018, Springer International Publishing AG.","ASP.NET technology; B/S framework; Database technology; Intelligent learning; System design","Application programs; Database systems; Education; Engineering education; Human resource management; Intelligent systems; Real time systems; Systems analysis; Windows operating system; Application development environment; ASP.NEt; Database technology; Development environment; Implementation techniques; Intelligent learning; Network operating system; Three-layer structures; Information management",2-s2.0-85028421226
"Yudin D., Zeno B.","Event recognition on images by fine-tuning of deep neural networks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410330&doi=10.1007%2f978-3-319-68321-8_49&partnerID=40&md5=051ae3712dfdcba937d670fb78c2ab80","The paper considers usage of fine-tuning of the deep neural network ensemble for recognition of 60 event types in the set of 60,000 images from WIDER database. The applied ensemble consists of two deep convolutional neural networks (CNN) using the GoogLeNet architecture, previously trained on other image bases: ImageNet and Places. Separately the accuracy of recognition of 10 events was analyzed: “Car Racing”, “Ceremony”, “Concert”, “Demonstration”, “Football”, “Meeting”, “Picnic”, “Swimming”, “Tennis” and “Traffic”. During the ensemble training output layer in the each of deep CNN is replaced to the layer with respectively 10 and 60 neurons and we tune only weights which connect output layer with previous one. The classification accuracy of 10 event classes from the WIDER image database averages 83.22%, for 60 event classes accuracy is 50.4%. In addition, the approach based on the automatic features formation using deep CNN provided a much better recognition quality of social events compared to the choice of features manually (LBP, LDP or HOG) and their further classification by support vector machine. The testing time of the developed ensemble provides the possibility of using the classifier in practical applications of event recognition with a processing speed up to 20 frames per second. © Springer International Publishing AG 2018.","Deep learning; Fine-tuning; Image recognition; Neural network; Social event","Classification (of information); Deep learning; Image recognition; Image retrieval; Neural networks; Sports; Classification accuracy; Convolutional neural network; Event recognition; Fine tuning; Frames per seconds; Neural network ensembles; Processing speed; Social events; Deep neural networks",2-s2.0-85031410330
"Vo H.V., Nguyen B.N., Le T.T., McMahan C.T., O’Brien E.M., Kunz R.K.","The novel design of the mercer universal prosthesis",2018,"IFMBE Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030872841&doi=10.1007%2f978-981-10-4361-1_33&partnerID=40&md5=0bd06b57e1dbb953c6b6db1225d88c4a","The Mercer On Mission (MOM) Orthopedic and Prosthetic projects are addressing a worldwide problem, which is predominantly critical in Vietnam and Southeast Asian countries. More than 2000 Vietnamese are injured each year by landmines and unexploded bombs left over from the Vietnam War. An estimated 100,000 amputees live in Vietnam today, and there are more than 18 million amputees around the world, with more than 80% of those living in developing countries. The prosthetic device was invented by one of the authors. The innovative design features a “V-cut” in the back of the socket, allowing the amputee patient to adjust the volume of the socket opening as the size and shape of patients’ stump changes. This adjustability is a principal advantage, as standard prostheses generally need to be replaced every two to three years because they no longer fit. This is not financially possible for many impoverished Vietnamese amputees. The prosthetic sockets and feet are made using injection molds. Knee joints, pylons and flanges are made using CNC and milling machines. They are made of polypropylene and aluminum instead of carbon fiber and stainless steel or titanium alloys. Therefore, the universal below the knee (BK) and above the knee (AK) prostheses are much lighter than the customized ones. At a cost of only $100 USD per BK and $150 USD per AK prosthesis to manufacture, coupled with grants and donations to cover material costs, the program is able to provide prostheses to Vietnamese amputees free of charge. Since 2009, authors with faculty and students from Mercer University have traveled to Ho Chi Minh City, Can Tho, Phung Hiep, Ben Tre, Phong Dien, Dong Thap, Ca Mau, Vinh Long, Vung Tau, and Quang Tri numerous times, bringing a total of 4816 Mercer prosthetic devices to fit for the local patients. © Springer Nature Singapore Pte Ltd. 2018.","F-Scan; Gait analysis; Mercer low-cost prostheses; Mercer On Mission; Service learning; Universal design","Alloy steel; Artificial limbs; Biomedical engineering; Bombs (ordnance); Carbon; Carbon fibers; Cost benefit analysis; Costs; Design; Developing countries; Education; Gait analysis; Joints (anatomy); Polypropylenes; Stainless steel; Steel fibers; Amputee patients; Innovative design; Low costs; Mercer On Mission; Prosthetic devices; Prosthetic sockets; Service learning; Universal Design; Prosthetics",2-s2.0-85030872841
"Hoogendoorn M., Funk B.","Discussion",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030658879&doi=10.1007%2f978-3-319-66308-1_10&partnerID=40&md5=197b3bb17b4d6a6564af187cf3aade15","In this chapter, we list some of the important challenges we see in the field of machine learning for the quantified self. © Springer International Publishing AG 2018.",,,2-s2.0-85030658879
"Félix C., Soares C., Jorge A., Ferreira H.","Using metalearning for parameter tuning in neural networks",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032381272&doi=10.1007%2f978-3-319-68195-5_120&partnerID=40&md5=fcb516d1def837d86e70f803ab5a936f","Neural networks have been applied as a machine learning tool in many different areas. Recently, they have gained increased attention with what is now called deep learning. Neural networks algorithms have several parameters that need to be tuned in order to maximize performance. The definition of these parameters can be a difficult, extensive and time consuming task, even for expert users. One approach that has been successfully used for algorithm and parameter selection is metalearning. Metalearning consists in using machine learning algorithm on (meta)data from machine learning experiments to map the characteristics of the data with the performance of the algorithms. In this paper we study how a metalearning approach can be used to obtain a good set of parameters to learn a neural network for a given new dataset. Our results indicate that with metalearning we can successfully learn classifiers from past learning tasks that are able to define appropriate parameters. © 2018, Springer International Publishing AG.",,,2-s2.0-85032381272
"Hoogendoorn M., Funk B.","Introduction",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030685562&doi=10.1007%2f978-3-319-66308-1_1&partnerID=40&md5=e90eaca91382845ac770a9774aae95a8","This chapter provides a basic introduction into the two domains that are pivotal in this book: the quantified self and machine learning. It explains the need and purpose of the book and introduces the basic definitions and notations. On top of that, an overview of the book is given and a reader’s guide is provided. © Springer International Publishing AG 2018.",,,2-s2.0-85030685562
"Hoogendoorn M., Funk B.","Basics of sensory data",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030726257&doi=10.1007%2f978-3-319-66308-1_2&partnerID=40&md5=615a025c9f8d248e01fef678ada14791","A typical quantified self/sensory dataset is introduced in this chapter. Popular sensors are briefly highlighted and a procedure is provided to transform raw sensory datasets into a suitable format to enable the application of machine learning techniques. The dataset introduced in this chapter is used to illustrate the approaches explained in the remainder of the chapters. © Springer International Publishing AG 2018.",,,2-s2.0-85030726257
"Cheriguene S., Azizi N., Dey N., Ashour A.S., Mnerie C.A., Olariu T., Shi F.","Classifier ensemble selection based on mRMR algorithm and diversity measures: An application of medical data classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029528458&doi=10.1007%2f978-3-319-62521-8_32&partnerID=40&md5=de8667d9b8c66a8a57a1c7c46efec469","Classifier selection is a significant problem in machine learning to reduce the computational time and the number of ensemble members. Over the past decade, multiple classifier systems (MCS) have been actively exploited to enhance the classification accuracy. Finding a pertinent objective function for measuring the competence of base classifier is a critical issue to select the appropriate subset from a pool of classifiers. Along with the accuracy, diversity measures are designed as objective functions for ensemble selection. This current work proposed a new selection method based on accuracy and diversity in order to achieve better medical data classification performance. The classifiers correlation was calculated using Minimum Redundancy Maximum Relevance (mRMR) method based on relevance and diversity measures. Experiments were carried out on five data sets from UCI Machine Learning Repository and LudmilaKuncheva Collection. The experimental results proved the superiority of the proposed classifiers selection method. © 2018, Springer International Publishing AG.","Classifiers selection; Diversity measures; Medical data classification; Minimum Redundancy Maximum Relevance Method; Relevance",,2-s2.0-85029528458
"Cooper J.N., Minneci P.C., Deans K.J.","Postoperative neonatal mortality prediction using superlearning",2018,"Journal of Surgical Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031011093&doi=10.1016%2fj.jss.2017.09.002&partnerID=40&md5=d6d996d7dac382a99600e518b2f40627","Background The variable risks associated with neonatal surgery present a challenge to accurate mortality prediction. We aimed to apply superlearning, an ensemble machine learning method, to the prediction of 30-day neonatal postoperative mortality. Materials and methods We included neonates in the 2012-2014 National Surgical Quality Improvement Program Pediatric. Patients treated in 2012-13 were used in model development (n = 6499), and patients treated in 2014 formed the validation sample (n = 3552). Our superlearner algorithm included 14 regression and machine learning algorithms and included all preoperative patient demographic and clinical characteristics, including indicator variables for surgical procedures. Performance was evaluated using mean squared error and measures of discrimination and calibration. Results The superlearner out-performed all individual algorithms with regard to cross-validated mean squared error. It showed excellent discrimination, with an area under the receiver-operating characteristic curve of 0.91 in development and 0.87 in validation. The superlearner showed good calibration in development but not in validation (Cox calibration test P = 0.06 and P < 0.001, respectively). Performance was improved when the superlearner was fit using only variables strongly associated with mortality in bivariate analysis (area under the receiver-operating characteristic curve 0.89, calibration test P = 0.63 in validation). Conclusions Superlearning provided improved or equivalent performance compared with individual regression and machine learning algorithms for predicting neonatal surgical mortality. This method should be considered for prediction in large data sets whenever complex mechanisms make parametric modeling assumptions unrealistic. © 2017 Elsevier Inc.","Neonates; Postoperative mortality; Prediction; Superlearning",,2-s2.0-85031011093
"Abdulrahman S.M., Cachada M.V., Brazdil P.","Impact of feature selection on average ranking method via metalearning",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364887&doi=10.1007%2f978-3-319-68195-5_121&partnerID=40&md5=c400f2c4868237280d7dcd3b707d0485","Selecting appropriate classification algorithms for a given dataset is crucial and useful in practice but is also full of challenges. In order to maximize performance, users of machine learning algorithms need methods that can help them identify the most relevant features in datasets, select algorithms and determine their appropriate hyperparameter settings. In this paper, a method of recommending classification algorithms is proposed. It is oriented towards the average ranking method, combining algorithm rankings observed on prior datasets to identify the best algorithms for a new dataset. Our method uses a special case of data mining workflow that combines algorithm selection preceded by a feature selection method (CFS). © 2018, Springer International Publishing AG.",,,2-s2.0-85032364887
"Castillo J.C., Castro-González Á., Alonso-Martín F., Fernández-Caballero A., Salichs M.Á.","Emotion detection and regulation from personal assistant robot in smart environment",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028917436&doi=10.1007%2f978-3-319-62530-0_10&partnerID=40&md5=61021916a544e8d74fcae10b0dc980eb","This paper introduces a proposal for integrating personal assistant robots with social capacities in smart environments. The personal robot will be a fundamental element for the detection and healthy regulation of the affect of the environment’s inhabitants. A full description of the main features of the proposed personal assistant robot are introduced. Also, the multi-modal emotion detection and emotion regulation modules are fully described. Machine learning techniques are employed for emotion recognition from voice and images and both outputs are merged to achieve the detected emotion. © Springer International Publishing AG 2018.",,,2-s2.0-85028917436
"Miyamoto S., Koshizen T., Matsumoto T., Kawase H., Higuchi M., Torimoto Y., Uno K., Sato F.","An application using a BLE beacon model combined with fully autonomous wheelchair control",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026312993&doi=10.1007%2f978-3-319-61566-0_30&partnerID=40&md5=4d794fa68af177e55154addb415fe0b5","This paper describes a novel technology for controlling a fully autonomous wheelchair, using a Bluetooth Low Energy (BLE) beacon. Our framework, in particular, enables the beacon substrate (Received Signal Strength Indicator) to allow machine learning for fully autonomous wheelchair control, namely for the beacon model. Location awareness is computed for the smooth locomotion control of a motorized wheelchair. As a result, low current consumption is achieved to allow lasting battery life. In this paper, we provide several results relevant to this, and future remarks. © Springer International Publishing AG 2018.",,"Intelligent systems; Autonomous wheelchair; Beacon models; Bluetooth low energies (BLE); Location awareness; Locomotion control; Low currents; Motorized wheelchairs; Received signal strength indicators; Wheelchairs",2-s2.0-85026312993
"Baral C., Fuentes O., Kreinovich V.","Why deep neural networks: A possible theoretical explanation",2018,"Studies in Systems, Decision and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029219978&doi=10.1007%2f978-3-319-61753-4_1&partnerID=40&md5=4e7d5d4f0605a75f6bdb329b90a590fb","In the past, the most widely used neural networks were 3-layer ones. These networks were preferred, since one of the main advantages of the biological neural networks—which motivated the use of neural networks in computing—is their parallelism, and 3-layer networks provide the largest degree of parallelism. Recently, however, it was empirically shown that, in spite of this argument, multi-layer (“deep”) neural networks leads to a much more efficient machine learning. In this paper, we provide a possible theoretical explanation for the somewhat surprising empirical success of deep networks. © Springer International Publishing AG 2018.",,,2-s2.0-85029219978
"Ali N., Amer E., Zayed H.","Understanding medical text related to breast cancer: A review",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539660&doi=10.1007%2f978-3-319-64861-3_26&partnerID=40&md5=4c9af8f732c3774c15aea2d7edd3e4f1","Breast Cancer is a harmful disease that has caused millions of women deaths. There are a huge number of publications on breast cancer research which offers a good source of information. Identifying breast cancer biomarkers is not a trivial task. There are many approaches used to identify and extract the needed information more efficiently from structured/unstructured text, uncover relationships and hidden rules from the huge amount of information such as text mining, machine learning and data mining. This paper reviews some of research literature on breast cancer using these approaches. © 2018, Springer International Publishing AG.","Breast cancer; Data mining; NLP; Text mining",,2-s2.0-85029539660
"Chavolla E., Zaldivar D., Cuevas E., Perez M.A.","Color spaces advantages and disadvantages in image color clustering segmentation",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032027151&doi=10.1007%2f978-3-319-63754-9_1&partnerID=40&md5=2aad97d2e583d48900bede4e5b932bc5","Machine learning has been widely used in image analysis and processing for the purpose of letting the computer recognize specifics aspects like color, shape, textures, size, and position. Such procedures allow to have algorithms capable of identifying objects, find relationships, and perform tracking. The present chapter executes an analysis of the one image attribute that is listed among the basic aspects of an image and the color. The color is chosen due to the importance given by humans to this attribute. Also the color is used main filter criteria in object recognition and tracking. In this section the effect of the color is studied from the point of view of the most popular color spaces available in image processing. It will be tested the effect of the selection of a given color space one of the most common clustering machine learning algorithms. The chosen algorithm is K-means ++, a variation of the popular K-means, which allows a more fair evaluation since it mitigates some of the randomness in the final cluster configuration. Every advantage or weakness will be exposed, so it can be known what color spaces are the right choice depending on the desired objective in the image processing. © Springer International Publishing AG 2018.",,,2-s2.0-85032027151
"Milazzo F., Gentile V., Gentile A., Sorce S.","Real-time body gestures recognition using training set constrained reduction",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026303683&doi=10.1007%2f978-3-319-61566-0_21&partnerID=40&md5=ee3540f4a781e0835041aa4bbc4a6ea1","Gesture recognition is an emerging cross-discipline research field, which aims at interpreting human gestures and associating them to a well-defined meaning. It has been used as a mean for supporting human to machine interaction in several applications of robotics, artificial intelligence, and machine learning. In this paper, we propose a system able to recognize human body gestures which implements a constrained training set reduction technique. This allows the system for a real-time execution. The system has been tested on a publicly available dataset of 7,000 gestures, and experimental results have highlighted that at the cost of a little decrease in the maximum achievable recognition accuracy, the required time for recognition can be dramatically reduced. © Springer International Publishing AG 2018.","Constrained optimization; Gesture recognition; Real-time systems","Artificial intelligence; Constrained optimization; Human robot interaction; Intelligent systems; Real time systems; Cross-disciplines; Human gestures; Real time execution; Recognition accuracy; Reduction techniques; Required time; Research fields; Training sets; Gesture recognition",2-s2.0-85026303683
"Montebello M.","User profiling and personalisation",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032568268&doi=10.1007%2f978-3-319-67928-0_4&partnerID=40&md5=8ce9c14b539cc7d1df286adf0d519b5c","Personalisation, user profiling and the use of machine learning techniques from the computer science arena fall under the umbrella of Artificial Intelligence or AI. Rather then going through all the technical details of machine learning and AI we will be looking into the conceptual application of such techniques, as well as the educational undertones of doing so. Personalisation features as a main component in this chapter due to its exceptional and remarkable property of improving a service or a product. We shall be looking into how such a widely employed technique in industry can be similarly applied to education that promises to alleviate and add-value to e-learning as we know them. The main concept behind such a technique is the capturing and representation of the specific user model or profile. This user representation is a living model that evolves over time and requires constant updating to ensure the profile realistically embodies the user or the learner in our case. As we shall investigate in the next sections the user profile is generally generated and trained using the user patterns and trends but also the interests, needs and choices that all indicate something specific about the user in isolation as well as in combination together. In another section we will also take an in-depth analysis of how user profiling can be optimised in the case of education in a similar attempt to encapsulate the specific and characteristic learner profile. We close this chapter with a look at recommender systems and how all the different parts mentioned above come together to the cause of enhancing education and the e-learning medium. © 2018, Springer International Publishing AG.",,,2-s2.0-85032568268
[No author name available],"17th UK Workshop on Computational Intelligence, UKCI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029598294&partnerID=40&md5=3708023c22b5a7cc5c7fcea8c59e7049","The proceedings contain 32 papers. The special focus in this conference is on Computational Intelligence. The topics include: Integrating association rules mined from health-care data with ontological information for automated knowledge generation; sentiment analysis model based on structure attention mechanism; fuzzy representation for flexible requirement satisfaction; a multidisciplinary method for constructing and validating word similarity datasets; fuzzy connected-triple for predicting inter-variable correlation; data integration with self-organising neural network reveals chemical structure and therapeutic effects of drug atc codes; a modified approach to inferring animal social networks from spatiotemporal data streams; a heuristic approach for the dynamic frequency assignment problem; applying ACO to large scale TSP instances; a new steady-state MOEA/D for sparse optimization; a multiobjective evolutionary algorithm approach for map sketch generation; a reference-inspired evolutionary algorithm with subregion decomposition for many-objective optimization; generation of reducts and threshold functions using discernibility and indiscernibility matrices for classification; adaptive noise cancelation using fuzzy brain emotional learning network; artificial neural network analysis of volatile organic compounds for the detection of lung cancer; predicting the occurrence of world news events using recurrent neural networks and auto-regressive moving average models; a comparison study on flush+reload and prime+probe attacks on AES using machine learning approaches; classifying and recommending using gradient boosted machines and vector space models; unsupervised automatic keyphrase extraction using affinity propagation; towards low-cost P300-based BCI using emotiv epoc headset and emotion detection in e-learning using expectation-maximization deep spatial-temporal inference network.",,,2-s2.0-85029598294
"Paulano-Godino F., Jiménez-Pérez J.R., Jiménez-Delgado J.J.","Issues on the simulation of geometric fractures of bone models",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032344529&doi=10.1007%2f978-3-319-68195-5_51&partnerID=40&md5=cb55c904d28dfb0cb9da292487e01079","The simulation of realistic fracture cases on geometric models representing bone structures is almost an unexplored field of research. These fractured models have many applications in computer-assisted methods that support specialist in fracture reduction interventions. For instance, the generation of specific fracture patterns can provide uncommon cases for training simulators or even can be used to improve machine-learning applications. This paper focuses on the issues to be considered in the generation of fractures on geometric models that represent bone structures. The main recent contributions for fracturing geometric models are examined and the challenges in terms of the application of real bone fracture patterns on geometric models are presented. Moreover, different alternatives for the evaluation of the results obtained by the geometric fracture generation algorithms when applied to bone structures are showed. Finally, the potential applications of the virtual generation of specific bone fractures are described. © 2018, Springer International Publishing AG.",,,2-s2.0-85032344529
"Hernández-Torruco J., Canul-Reich J., Román D.L.","Rule based classifiers for diagnosis of mechanical ventilation in Guillain-Barré syndrome",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022178988&doi=10.1007%2f978-3-319-62410-5_22&partnerID=40&md5=83dc3fdfd2ac7cd2e5d4f068bd292bfe","Breathing difficulty is a complication present in almost a third of Guillain-Barré Syndrome (GBS) patients. To alleviate this condition a mechanical respiratory device is needed. Anticipating this need is crucial for patients’ recovery. This can be achieved by means of machine learning predictive models. We investigated whether clinical, serological, and nerve conduction features separately can predict the need of mechanical ventilation with high accuracy. In this work, three rule based classifiers are applied to create a diagnostic model for this necessity. JRip, OneR and PART algorithms are analyzed using a real dataset. We performed classification experiments using train-test evaluation scheme. Clinical features were found as the best predictors. © Springer International Publishing AG 2018.","Data mining and processing; JRip; OneR; PART; Performance evaluation; Train-test","Artificial intelligence; Data handling; Distributed computer systems; Patient rehabilitation; Ventilation; Clinical features; JRip; Mechanical ventilation; OneR; PART; Performance evaluation; Predictive models; Rule-based classifier; Diagnosis",2-s2.0-85022178988
"Tao J., Huang J., Yu L., Li Z., Liu H., Yuan B., Zeng D.","A new methodology combining microscopy observation with Artificial Neural Networks for the study of starch gelatinization",2018,"Food Hydrocolloids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027572736&doi=10.1016%2fj.foodhyd.2017.07.037&partnerID=40&md5=06090a7a64b7fb5287f4e5fadc17c20f","A novel methodology combining microscopy observation with Artificial Neural Networks (ANNs) and realized by machine learning algorithms for the study of starch gelatinization was developed. As the most critical part during object detection, an improved starch single shot multi-box detector (starch-SSD) originated from ANNs was purposely designed and applied in monitoring the morphological changes of starch with increasing temperature. In the case, the birefringences were automatically identified by computer vision and then the relative birefringence number of the image was calculated. Basing on such number change, the temperature of phase transition was detected and consequently the degree of gelatinization (DG) at specific temperature was quickly calculated. Compared with traditional methods that mainly performed by manual operation, experimental results confirmed that the proposed method has competitive accuracy and is much faster. It also provides a unified standard for microscopy observation without subjective uncertainty. © 2017 Elsevier Ltd","Artificial Neural Networks; Degree of gelatinization; Shot multi-box detector; Starch gelatinization",,2-s2.0-85027572736
"Cover G.S., Herrera W.G., Bento M.P., Rittner L.","Corpus Callosum 2D segmentation on diffusion tensor imaging using growing neural gas network",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032337468&doi=10.1007%2f978-3-319-68195-5_23&partnerID=40&md5=b9e9c4f99c5a8e799b2dec0a76051599","The Corpus Callosum (CC) segmentation on Magnetic Resonance Images (MRI) is of utmost importance for the study of neurodegenerative diseases, since it is the largest white matter brain structure, interconnecting the two cerebral hemispheres. Operator-independent segmentation methods are desirable, even though such task is complex due to shape and intensity variation among subjects, especially on low resolution images such as Diffusion-MRI. This paper proposes an automatic CC segmentation approach on Diffusion Tensor imaging (DTI). The method uses Growing Neural Gas (GNG) network, an unsupervised machine learning algorithm, on the fractional anisotropy map. The proposed method obtained a Dice coefficient of 0.88 in experiments using DTI of fifty human subjects, while other segmentation approaches obtained Dice results below 0.73. Although the GNG network had five parameters to be set, it requires no user intervention and was the only method that successfully detected and segmented the CC on all experimented dataset. © 2018, Springer International Publishing AG.",,,2-s2.0-85032337468
"Alonso-Betanzos A., Bolón-Canedo V., Eiras-Franco C., Morán-Fernández L., Seijo-Pardo B.","Preprocessing in High Dimensional Datasets",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032031620&doi=10.1007%2f978-3-319-67513-8_11&partnerID=40&md5=7b3063142e75bfeb83253e7161142b76","In the last few years, we have witnessed the advent of Big Data and, more specifically, Big Dimensionality, which refers to the unprecedented number of features that are rendering existing machine learning inadequate. To be able to deal with these high-dimensional spaces, a common solution is to use data preprocessing techniques which might help to reduce the dimensionality of the problem. Feature selection is one of the most popular dimensionality reduction techniques. It can be defined as the process of detecting the relevant features and discarding the irrelevant and redundant ones. Moreover, discretization can help to reduce the size and complexity of a problem in Big Data settings, by diminishing data from a large domain of numeric values to a subset of categorical values. This chapter describes in detail these preprocessing techniques as well as providing examples of new implementations developed to deal with Big Data. © Springer International Publishing AG 2018.","Big data; Big dimensionality; Discretization; Feature selection; Preprocessing",,2-s2.0-85032031620
"Faleiros M.C., Ferreira Junior J.R., Jens E.Z., Dalto V.F., Nogueira-Barbosa M.H., de Azevedo-Marques P.M.","Pattern recognition of inflammatory sacroiliitis in magnetic resonance imaging",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032387124&doi=10.1007%2f978-3-319-68195-5_69&partnerID=40&md5=9d1e58824b3639a6b76a970a960b7ba8","The standard reference to evaluate active inflammation of sacroiliac joints in spondyloarthritis is magnetic resonance imaging (MRI). However, visual evaluation may be challenging to specialists due to clinical variability. In order to improve the diagnosis of inflammatory sacroiliitis we have used image processing and machine learning technics to recognize inflammatory patterns in sacroiliac joints in spectral attenuated inversion recovery (SPAIR) T2-weighted MRI using gray-level, texture and spectral features. Pattern recognition was performed by the ReliefF method for attribute selection and the classifiers K nearest neighbors (with 5 values for K), Multilayer Perceptron artificial neural network, Naive Bayes, Random Forest, and Decision Tree J48. Classification was assessed by the area under the ROC (receiver operating characteristic) curve (AUC), Sensitivity and Specificity, with a 10-fold cross validation. The K nearest neighbors with K = 5 obtained the best performance with AUC up to 0.96. © 2018, Springer International Publishing AG.","Inflammatory sacroiliitis; Magnetic resonance imaging; Pattern recognition; Sacroiliac joints; Spondyloarthritis",,2-s2.0-85032387124
[No author name available],"14th International Symposium on Distributed Computing and Artificial Intelligence, DCAI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022209944&partnerID=40&md5=81508107ebc6721ead3e7a1dc4922276","The proceedings contain 29 papers. The special focus in this conference is on Distributed Computing and Artificial Intelligence. The topics include: Optimization of urban freight distribution with different time constraints; artificial bee colony algorithms for two-sided assembly line worker assignment and balancing problem; cyclic steady state behaviour subject to grid-like network constraints; application of fuzzy logic and genetic algorithms in automated works transport organization; quality assessment of implementation of strategy design pattern; minimizing energy consumption in a straight robotic assembly line using differential evolution algorithm; statistics-based approach to enable consumer profile definition for demand response programs; feature extraction-based method for voltage sag source location in the context of smart grids; a multi-agent system for energy trading between prosumers; smart grids data management; data mining for prosumers aggregation considering the self- generation; control of accuracy of forming elastic deformable shafts with low rigidity; a negotiation algorithm for decision-making in the construction domain; deep neural networks and transfer learning applied to multimedia web mining; predicting the risk of suffering chronic social exclusion with machine learning; semantic profiling and destination recommendation based on crowd-sourced tourist reviews; robustness of coordination mechanisms in distributed problem solving against erroneous communication; a sentiment analysis model to analyze students reviews of teacher performance using support vector machines; proposal of wearable sensor-based system for foot temperature monitoring and energy analyzer emulation for energy management simulators.",,,2-s2.0-85022209944
[No author name available],"1st International Conference on Intelligent Systems in Production Engineering and Maintenance, ISPEM 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028616058&partnerID=40&md5=5d145217dcabf3b9227b141729dd2481","The proceedings contain 44 papers. The special focus in this conference is on Intelligent Systems in Production Engineering and Maintenance. The topics include: A concept of an IT tool for supporting knowledge transfer among facility maintenance employees as part of intelligent organization; an intelligent system supporting a forklifts maintenance process; an intelligent system supporting a maintenance process of specialised medical equipment; incident detection in industrial processes utilizing machine learning techniques; intelligent systems of forecasting the failure of machinery park and supporting fulfilment of orders of spare parts; smartmaintenance - the concept of supporting the exploitation decision-making process in the selected technical network system; use of intelligent informatics module for registration and assessment of causes of breaks in selected mining machines; using a simulation method for intelligent maintenance management; a heuristic and simulation hybrid approach for mixed and multi model assembly line balancing; a new petri nets based approach for modeling of discrete manufacturing system; algorithm of autonomic shaping of workflow; an intelligent system for core-competence identification for industry 4.0 based on research results from german and polish manufacturing companies; an optimization approach for scheduling and lot sizing problems in electromechanical industry using GA-based method; application of neural-fuzzy system in prediction of methane hazard; application of support vector machine for determination of impact of traffic-induced vibrations on buildings and artificial neural networks as a means for making process control charts user friendly.",,,2-s2.0-85028616058
"Chaudhuri S., Bhardwaj A.","Task dependence of perceptual deadzone",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032577146&doi=10.1007%2f978-981-10-6692-4_7&partnerID=40&md5=a638e9fa80b48c4401a943cd603c379f","In this chapter, we study whether the perceptual deadzone depends on the task to be performed during the psychophysical experiments. In order to study this, we design a psychophysical experiment where we define two specific tasks: discriminative and comparative. In the discriminative task, the user must discriminate if the successive stimulus is different from the reference force, be it increasing or decreasing in magnitude. On the other hand, in case of the comparative task, the user has to discriminate the stimulus only along one direction, either increasing or decreasing in magnitude. Responses are recorded for both the tasks for several users. Support vector machine (SVM), a machine learning approach, is applied to the recorded responses to estimate the perceptual deadzone for each task. Our results demonstrate that comparative deadzone is significantly smaller than the discriminative deadzone in terms of their width and the just noticeable difference, suggesting that the task of discriminating two forces is more difficult for a user than to compare which force is greater (or smaller). Hence taking inference of this study, we demonstrate that the perceptual deadzone does depend on the task being performed. © 2018, Springer Nature Singapore Pte Ltd.",,,2-s2.0-85032577146
[No author name available],"16th International Conference on Global Research and Education Inter-Academia, 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029817252&partnerID=40&md5=796856dcaa5a573290f00658226ce09e","The proceedings contain 42 papers. The special focus in this conference is on Global Research and Education Inter-Academia. The topics include: Hydrothermal synthesis and characterization of mixed fluoride based nanophosphors; phase composition, structure and mechanical properties of carbon coatings doped by carbide-forming metals; ellipsometric control of laser welded materials; comparison of euler-bernoulli and timoshenko beam equations for railway system dynamics; interaction of hydrogen isotopes with radiation damaged tungsten; reviewing the novel machine learning tools for materials design; development of functional coatings by electron-beam sputtering of sol-gel targets; omega-structured substrate-supported metamaterial for the transformation of wave polarization in THz frequency range; core-shell powders with titanium coating; plasma-chemical synthesis of plasmon metal-polymer coatings, their structure and properties; automatic evaluation of surface nanostructuring using image processing; friction and adhesion forces at a nanoscopic contact between titanium dioxide thin film surfaces; nanostructure and ferroelectric properties of sol-gel SBTN-films for electronic devices; sol-gel synthesis of ZnO nanorods for MEMS; flow control by dielectric barrier discharge microplasma; pulsed magnetron sputtering; generation and evaluation of surrogate to design fuel supply system; optimal design of electrical machines: state of the art survey; review on the usage of the multiobjective optimization package of modefrontier in the energy sector; calculation method of chemical kinetics to search the optimal experimental conditions in a micro flow reactor; ontology extension for personalized accessible indoor navigation and a load balancing algorithm for resource allocation in cloud computing.",,,2-s2.0-85029817252
[No author name available],"9th KES International Conference on Intelligent Decision Technologies, KES-IDT 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020387681&partnerID=40&md5=69bf3362fb461d501cdd83184cc836c6","The proceedings contain 63 papers. The special focus in this conference is on Intelligent Decision Technologies. The topics include: The Shapley value and consistency axioms of cooperative games under incomplete information; decremental subset construction; using alloy for verifying the integration of OLAP preferences in a hybrid what-if scenario application; electrohydrodynamic effect simulation and method of its optimization; specialized decision techniques for data mining, transportation and project management; applying the intelligent decision heuristic to solve large scale technician and task scheduling problems; manipulability of majority relation-based collective decision rules; stacking-based integrated machine learning with data reduction; a hybrid approach to conceptual classification and ranking of resumes and their corresponding job posts; chaotic nature of eye movement signal; generational feature elimination to find all relevant feature subset; optimization of exact decision rules relative to length; evaluating importance for numbers of bins in discretised learning and test sets; assessing the similarity of situations and developments by using metrics; interval-valued intuitionistic fuzzy cognitive maps for supplier selection; heuristic method of air defense planning for an area object with the use of very short range air defense; an optimization problem of air defense planning for an area object; forecasting social security revenues in Jordan using fuzzy cognitive maps; fuzzy cognitive maps employing ARIMA components for time series forecasting; adjustment from inconsistent comparisons in AHP to perfect consistency and managerial decisions modelling for the company development strategy.",,,2-s2.0-85020387681
[No author name available],"50th Annual Convention of Computer Society of India: ICT Based Innovations, CSI 2015",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031407324&partnerID=40&md5=c70a438c4f8e24c58797cf374ee1ba50","The proceedings contain 25 papers. The special focus in this conference is on of Computer Society of India: ICT Based Innovations. The topics include: Minimax (Maximin) with special approach of gamification in higher education; comparing the behavior of oversampling and undersampling approach of class imbalance learning by combining class imbalance problem with noise; proposed ICT-based transportation model; multi-criteria rating using fuzzy ranking for improving soil recommendation system; a way to connect farmer community to agriculture market for betterment of rural development; a methodical study on behavior of different seeds using an iterative technique with evaluation of cluster validity; bharatanatyam dance classification with rough set tools; effective and efficient digital advertisement algorithms; automation of patient information in healthcare system; recommendation for selecting smart village in india through opinion mining using big data analytics; analyzing online groups or the communities in social media networks by algorithmic approach; open source EJBCA public key infrastructure for e-governance enabled software systems in RRCAT; anticipation of gross domestic product using world development indicators; an efficacious matching of finger knuckle print images using gabor feature; an ensemble-based decision support system for the students' academic performance prediction; content-based social network aggregation; collaborative filtering-based recommender system; classifying exoplanets as potentially habitable using machine learning; towards understanding preference of use of emoticons for effective online communication and promotion; computer simulation using GPSC package MATLAB, simulink for bioinformatics professional.",,,2-s2.0-85031407324
[No author name available],"13th International Conference on Intelligent Information Hiding and Multimedia Signal Processing, IIH-MSP 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026635564&partnerID=40&md5=458bdf43364c1ec49e205d435b91c7e7","The proceedings contain 103 papers. The special focus in this conference is on Intelligent Information Hiding and Multimedia Signal Processing. The topics include: Extraction of EEG components based on time - frequency blind source separation; an algorithm for asymmetric clipping detection based on parameter optimization; automatic facial age estimate based on convolution neural network; the application of eye tracking in education; adaptive multiple description depth image coding based on wavelet sub-band coefficients; adaptive histogram shifting based reversible data hiding; design and implementation of network video encryption system based on STM32 and AES algorithm; implementation of a drone-based video streamer; response selection of interview-based dialog system using user focus and semantic orientation; development and evaluation of Julius-compatible interface for kaldi ASR; voice conversion from arbitrary speakers based on deep neural networks with adversarial learning; evaluation of nonlinear tempo modification methods based on sinusoidal modeling; towards an interrogation speech manipulation detection method using speech fingerprinting; detection of singing mistakes from singing voice; a study of audio watermarking method using non-negative matrix factorization for a duet of different instruments; a wind noise detection algorithm for monitoring infrasound using Smartphone as a sensor device; study on speech representation based on spikegram for speech fingerprints; embedding multiple audio data using information misreading technique; an automatic detection method for Morse signal based on machine learning and building of a practical monitoring system for the small wind turbine.",,,2-s2.0-85026635564
"Liu Y.L., Gomide F.","Participatory search in evolutionary fuzzy modeling",2018,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027247116&doi=10.1007%2f978-3-319-64286-4_11&partnerID=40&md5=e69c88fe47307c2ce69f3e3c7e297741","Search is one of the most useful procedures employed in numerous situations such as optimization, machine learning, information processing and retrieval. This chapter introduces participatory search, a class of population-based search algorithms constructed upon the participatory learning paradigm. Participatory search relies on search mechanisms that progress forming pools of compatible individuals. The individual that is the most compatible with the best individual is always kept in the current population. Random immigrants are added to complete the population at each algorithm step. Different types of recombination are possible. The first is a convex combination, arithmetic-like recombination modulated by the compatibility between individuals. The second is a recombination mechanism based on selective transfer. Mutation is an instance of differential variation modulated by compatibility between selected and recombined individuals. Applications concerning development of fuzzy rule-based models from actual data illustrate the potential of the algorithms. The performance of the models produced by participatory search algorithms are compared with a state of the art genetic fuzzy system. Experimental results show that the participatory search algorithm with arithmetic-like recombination performs better than the remaining ones. © Springer International Publishing AG 2018.",,,2-s2.0-85027247116
"Hoogendoorn M., Funk B.","Predictive modeling without notion of time",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030663789&doi=10.1007%2f978-3-319-66308-1_7&partnerID=40&md5=736294a0c2acd8a706fb9c3cc35f04bf","Supervised learning approaches that do not explicitly take the time component into account are briefly discussed in this chapter. The approaches explained include feedforward neural networks, support vector machines, k-nearest neighbor, decision trees, naïve bayes and ensembles. Guidelines are provided on how to apply these algorithms to quantified self data, including the learning setup (e.g. learning for single users or across multiple users) and other practical considerations such as feature selection and regularization. Data stream mining approaches for predictive modeling are also briefly discussed. © Springer International Publishing AG 2018.",,,2-s2.0-85030663789
"Martins B., Rei J., Braga M., Abelha A., Vicente H., Neves J., Neves J.","Kidney care—a personal assistant assessment",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028949295&doi=10.1007%2f978-3-319-62530-0_3&partnerID=40&md5=9187b41835f9acb55aab34b4c846a055","A cognitive disability is a medical condition that, despite all technological progress, still does not have a cure, i.e., there are cases where the physician may use medication, but the only purpose is to decrease the progression of the disease, not its cure. This is the case in many situations, and in particular in kidney illnesses, which have a dominant impact on a person well being, i.e., the assistance to an individual to whom was diagnosed cognitive disabilities is essential, where the location of the individual is not decisive or important. Hence, the presence of a Personal Assistance Service can become a cornerstone in achieving independence and quality of life. Therefore, the objective of this work is to present an intelligent system aimed at an endless individuals monitoring and alerting system, based on a Logical Programming approach to Knowledge Representation and Reasoning, and centre on RapidMiner, a software platform that provides an integrated environment for machine learning, predictive analysis or application development and deployment. It undergoes a Case Based approach to computing that tracks patient’s performance, learn and deliver content when it is needed, and assures that patient’s key information is changed into the indispensable ongoing knowledge. © Springer International Publishing AG 2018.","Case based reasoning; Knowledge representation and reasoning; Logic programming; Personal assistants artificial intelligence intelligent systems",,2-s2.0-85028949295
[No author name available],"10th KES International Conference on Intelligent Interactive Multimedia Systems and Services, IIMSS 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020406138&partnerID=40&md5=7b48e5ef0e03f76353475ebaa62bb4e3","The proceedings contain 57 papers. The special focus in this conference is on Intelligent Interactive Multimedia Systems and Services. The topics include: Images selection and best descriptor combination for multi-shot person re-identification; dimensionality reduction strategies for CNN-based classification of histopathological images; optimizing multiresolution segmentation for extracting plastic greenhouses from worldview-3 imagery; greenhouse detection using aerial orthophoto and digital surface model; comparison of mesh simplification tools in a 3D watermarking framework; a smart-ca architecture for opencast matterhorn; an effective corpus-based question answering pipeline for Italian; towards a cognitive system for the identification of sleep disorders; an ensemble classifiers approach for emotion classification; sign languages recognition based on neural network architecture; medical entity and relation extraction from narrative clinical records in Italian language; detection of indoor actions through probabilistic induction model; a ROS driven platform for radiomap management optimization in fingerprinting based indoor positioning; improving spatial reasoning by interacting with a humanoid robot; an artificial pain model for a humanoid robot; interaction capabilities of a robotic receptionist; artificial pleasure and pain antagonism mechanism in a social robot; using multilayer perceptron in computer security to improve intrusion detection; a composite methodology for supporting early-detection of handwriting dysgraphia via big data analysis techniques; autonomous vehicle design for predator proof fence monitoring; sentiment analysis method for tracking touristics reviews in social media network and mobility based machine learning modeling for event mining in social networks.",,,2-s2.0-85020406138
[No author name available],"3rd International Symposium on Signal Processing and Intelligent Recognition Systems, SIRS 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030181543&partnerID=40&md5=5921f44588de119aae58b4dcbebbc42a","The proceedings contain 41 papers. The special focus in this conference is on Signal Processing and Intelligent Recognition Systems. The topics include: Removal of BW and respiration noise in abdECG for f ECG extraction; early stage detection of diabetic retinopathy using an optimal feature set; exploring cepstral coefficient based sleep stage scoring method for single-channel EEG signal using machine learning technique; non linear tracking using unscented kalman filter; an analysis on the influence that the position and number of control points have on MLS registration of medical images; component characterization of western and indian classical music; an experimental setup of DSA algorithm suitable for high bandwidth data transfer using USRP and GNU radio companion; influence of filter bank structure on the statistical significance of coefficients in cepstral analysis for acoustic signals; particle filtering technique for fast fading shadow power estimation in wireless communication; object detection and localization using compressed sensing; an improved approach for securing document images using dual cover; dependency of various color and intensity planes on CNN based image classification; epigraphic document image enhancement using retinex method; improved microaneurysm detection in fundus images for diagnosis of diabetic retinopathy; swarm robots in a closed loop visual odometry system by using visible light communication; an adaptive neuro-fuzzy inference system based situation awareness assessment in VLC enabled connected cars; identifying issues in estimating parameters from speech under lombard effect and a model for an emotional respondent robot.",,,2-s2.0-85030181543
"Ferreira Junior J.R., Cipriano F.E.G., Fabro A.T., Koenigkam-Santos M., de Azevedo-Marques P.M.","Radiomics-based recognition of metastatic and histopathological patterns of lung cancer",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032389428&doi=10.1007%2f978-3-319-68195-5_66&partnerID=40&md5=3312a98d2c0ebcd75532390376c1c370","Lung cancer is the leading cause of cancer-related deaths in the world and its poor prognosis varies markedly according to the tumor staging. Tumor histopathology and computed tomography (CT) features have been used as prognostic factors, but they still present challenges. This work addressess the problem of lung cancer pattern recognition in terms of histopathology and nodal and distant metastasis, using radiomic CT image features and machine learning classifiers. We retrospectively analyzed 52 tumors and semiautomaticaly segmented the CT images. Tumors were characterized by clinical factors and quantitative image attributes of gray level, histogram, texture, shape, and volume. Three classifiers used relevant selected features to perform the analysis. An artificial neural network presented stabled performances on pattern recognition, obtaining areas under the receiver operating characteristic curve of 0.90 for histopathology, 0.88 for nodal metastasis, and 0.98 for distant metastasis. The radiomic pattern recognition presented high performance and great potential to aid the lung cancer diagnosis and prognosis. © 2018, Springer International Publishing AG.","Lung cancer; Pattern recognition; Quantitative imaging; Radiomics",,2-s2.0-85032389428
"Abdelhade N., Soliman T.H.A., Ibrahim H.M.","Detecting twitter users’ opinions of arabic comments during various time episodes via deep neural network",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029504404&doi=10.1007%2f978-3-319-64861-3_22&partnerID=40&md5=201694403c2e79ed7878370334f2ec43","Due to the revolution of web 2.0, the amount of opinionated data has been extremely increased, produced by online users through sharing comments, videos, pictures, reviews, news and opinions. Although Twitter is one of the most prevalent social networking, the gathered data from Twitter is highly disorganized. However, extracting useful information from tweets is considered a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. There has been a lot of work on sentiment analysis in English texts. However, the datasets and the publications of Arabic tweets analysis are still somewhat limited. In addition, one of the main important issues is that users can change their opinions on different subjects over time. In this work, two main points are discussed. First, a deep neural network (DNN) approach (back propagation algorithm) is applied to Arabic tweets to two different domains: Egyptian stock exchange and sports’ tweets. Second, DNN is implemented to detect users’ attitude in a time period of two years for each dataset (2014 and 2015) and (2012 and 2013). The datasets are manually annotated via constructing a lexicon from the two already existing ones. When DNN performance is evaluated an average value of accuracy 90.22%, precision 90.56%, recall 90.90%, and F-measure of 90.68%, when compared to other three machine learning algorithms Naïve Bayes (NB), Decision Tree, and K-Nearest. © 2018, Springer International Publishing AG.","Arabic tweets; Back propagation algorithm; Deep neural network; Opinion mining and time episodes; Sentiment analysis",,2-s2.0-85029504404
"Kang J., Lim J.","Storytelling-based hand gesture interaction in a virtual reality environment",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021703380&doi=10.1007%2f978-3-319-60639-2_17&partnerID=40&md5=880d5e8a53847d5d6592f5b351f17393","This paper focuses on emotional effects of storytelling-based hand gesture interaction in a VR (Virtual Reality) environment. Unlike using depth cameras like Kinect sensor, a wearable band is proposed to detect users’ hand gestures to create a virtual reality film. The VR film is created using a storytelling-based hand gesture recognition system and focuses on the users’ emotional effects. For the design, the hand gestures suitable for the story in the film are derived from research on users and are applied to the VR film titled ‘Not Alone’. In order to recognize the hand gestures of the user, the data collected from the wearable band is analyzed to classify movements of the hand through machine learning, which helps to solve problems of the existing hand gestures. This study proposes hand gesture interaction that is most suitable for users with two free hands in the head mounted display HMD (Head Mounted Display) based VR environment and tries to maximize users’ emotional responses in narrative-based film content by developing storytelling-based hand gesture interaction. © Springer International Publishing AG 2018.","Hand gesture; Interaction; Storytelling; Virtual reality; Wearable band","Gesture recognition; Helmet mounted displays; Human computer interaction; Human engineering; Palmprint recognition; Virtual reality; Emotional response; Hand gesture; Hand-gesture recognition; Head mounted displays; Interaction; Storytelling; Virtual-reality environment; Wearable band; Wearable technology",2-s2.0-85021703380
"Thongkam P., Leesutthipornchai P.","Ensemble features selection algorithm by considering features ranking priority",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022225003&doi=10.1007%2f978-3-319-60663-7_5&partnerID=40&md5=3a86adaed6870f950cfd9e54c83b5e57","Feature selection is a pre-processing for choosing relevant features and ignores features that tend to have no predictive information. Feature selection is applied to improve the accuracy of classification process. High relevant features have a tendency to get high classification performance. This paper proposed the ensemble of multiple feature ranking techniques by considering ranker priority for feature selection. Five individual feature ranking algorithms (information gain, gain ratio, symmetrical uncertainty, reliefF and oneR) are investigated and considered together as ensemble, based on ranking priority. The lung cancer, lymphoma, breast cancer, ovarian cancer and leukemia datasets were gathered from Kent Ridge bio-medical data and Machine Learning data repository. The datasets are applied to ensemble features selection algorithm. The obtained results are compared to results from individual feature ranking algorithms and the existing ensemble algorithm. The selected features are applied to classification algorithms. Area under the curve (AUC), precision and recall values from six classification algorithms are used to evaluate the obtained features. The experimental results show that the selected features from proposed ensemble features selection algorithm are greater than those of individual feature ranking techniques and the existing ensemble features selection algorithm. © Springer International Publishing AG 2018.","Ensemble; Feature selection; Ranker","Diseases; Feature extraction; Accuracy of classifications; Area under the curves; Classification algorithm; Classification performance; Ensemble; Precision and recall; Predictive information; Ranker; Classification (of information)",2-s2.0-85022225003
"Zalasiński M., Cpałka K., Rutkowski L.","Fuzzy-genetic approach to identity verification using a handwritten signature",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030093105&doi=10.1007%2f978-3-319-67946-4_17&partnerID=40&md5=a11e542732ea545be85d7c67ae0a666f","Verification of the dynamic signature is an important issue of biometrics. There are many methods for the signature verification using dynamics of the signing process. Many of these methods are based on the so-called global features. In this paper we propose a new approach to the signature verification using global features. The proposed approach can be characterized as follows: (a) Classification of the signature is performed using a fuzzy-genetic system. (b) We select an individual set of features for each signer. (c) In the procedure of features selection we use a genetic algorithm with appropriately designed evaluation function. It works without access to the signatures called skilled forgeries (this is a major advantage of the proposed approach). (d) We determine weights of importance for evolutionarily selected features. (e) The weights are taken into account in the classification process. (f) An additional advantage of the proposed classifier is the possibility of its work interpretation and possibility of an analytical determination of its parameters without machine learning. In this paper we present the simulation results for the BioSecure signature database, distributed by the BioSecure Association. © Springer International Publishing AG 2018.",,,2-s2.0-85030093105
"Rupérez M.J., Martínez-Martínez F., Martínez-Sober M., Lago M.A., Lorente D., Bakic P.R., Serrano-López A.J., Martínez-Sanchis S., Monserrat C., Martín-Guerrero J.D.","Modeling the mechanical behavior of the breast tissues under compression in real time",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032331348&doi=10.1007%2f978-3-319-68195-5_63&partnerID=40&md5=070f185960659d1625bb67da4487d0d6","This work presents a data-driven model to simulate the mechanical behavior of the breast tissues in real time. The aim of this model is to speed up some multimodal registration algorithms, as well as some image-guided interventions. Ten virtual breast phantoms were used in this work. Their deformation during a mammography was performed off-line using the finite element method. Three machine learning models were trained with the data from those simulations. Then, they were used to predict the deformation of the breast tissues. The models were a decision tree and two ensemble methods (extremely randomized trees and random forest). Four experiments were designed to assess the performance of these models. The mean 3D euclidean distance between the nodal displacements predicted by the models and those extracted from the FE simulations were used for the assessment. The mean error committed by the three models were under 3 mm for all the experiments, although extremely randomized trees performed better than the other two models. Breast compression prediction takes on average 0.05 s, 0.33 s and 0.43 s with decision tree, random forest and extremely randomized trees respectively, thus proving the suitability of the three models for clinical practice. © 2018, Springer International Publishing AG.",,,2-s2.0-85032331348
"LeMoyne R., Mastroianni T.","Wearable and wireless systems for gait analysis and reflex quantification",2018,"Smart Sensors, Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032370700&doi=10.1007%2f978-981-10-5684-0_1&partnerID=40&md5=9e55119ec5839659e63f9d974d26f686","The capacity to quantify the movement features of a person undergoing the rehabilitation process enables therapists and clinicians to proactively optimize the therapy strategy. Wearable and wireless systems, such as the smartphone and portable media device, are equipped with accelerometers and gyroscopes that can readily quantify aspects of human movement pertinent to rehabilitation, such as gait and reflex response. The smartphone and portable media device can measure gait and reflex response through their inertial sensors, and the acquired data can be conveyed by wireless transmission to the Internet as an email attachment. This capability enables the experimental site and post-processing resources to be remotely situated. Three phases of the evolution of quantification techniques for the rehabilitation process are observed, which are characterized as a first, second, and third wave. The first wave pertains to the traditional ordinal scale approach used by expert clinicians. The second wave emphasizes the role of quantification systems that are generally constrained to a clinical setting. The third wave envisions the development of Network Centric Therapy through the application of wearable and wireless systems, such as smartphones and portable media devices, for quantifying movement characteristics, such as gait and reflex response. Network Centric Therapy encompasses a quantum leap in rehabilitation capability through Cloud Computing amalgamated with machine learning with patient and therapy team situated remotely anywhere in the world. A summary of each chapter is further presented. © 2018, Springer Nature Singapore Pte Ltd.","Accelerometer; Gait; Gait analysis; Gyroscope; Network centric therapy; Ordinal scale; Portable media device; Quantification apparatus; Reflex response; Reflex response quantification; Smartphone; Wearable and wireless systems",,2-s2.0-85032370700
[No author name available],"10th International Conference on Computer Recognition Systems, CORES 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019267008&partnerID=40&md5=4d7345a72a3aa8aaca39058524f03984","The proceedings contain 53 papers. The special focus in this conference is on Computer Recognition Systems. The topics include: Recognition of fuzzy or incompletely described objects; multi-aspect assessment and classification of porous materials designed for tissue engineering; enhancing English-Japanese translation using syntactic pattern recognition methods; travel time prediction for trams in Warsaw; diagnostic rule extraction using the dempster-shafer theory extended for fuzzy focal elements; gait recognition using motion trajectory analysis; determining of an estimate of the equivalence relation on the basis of pairwise comparisons; semi-automatic segmentation of scattered and distributed objects; a vision-based method for automatic crack detection in railway sleepers; towards privacy-aware keyboards; saliency-based optimization for the histogram of oriented gradients-based detection methods; efficient sketch recognition based on shape features and multidimensional indexing; performance evaluation of selected thermal imaging-based human face detectors; on a new method of dynamic integration of fuzzy linear regression models; ensemble machine learning approach for android malware classification using hybrid features; an ensemble of weak classifiers for pattern recognition in motion capture clouds of points; portable dynamic malware analysis with an improved scalability and automatisation; projection-based person identification; an algorithm for selective preprocessing of multi-class imbalanced data; the method of person verification by use of finger knuckle images; recent advances in image pre-processing methods for palmprint biometrics; some properties of consensus based classification; knowledge based active partition approach for heart ventricle recognition; raster maps search using text queries and reasoning.",,,2-s2.0-85019267008
[No author name available],"3rd International Symposium on Intelligent Systems Technologies and Applications, ISTA 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032707507&partnerID=40&md5=9a82a9919f79312cdcb884d843a43f8f","The proceedings contain 34 papers. The special focus in this conference is on Intelligent Systems Technologies and Applications. The topics include: Markov Chain monte carlo methods and evolutionary algorithms for automatic feature selection from legal documents; an adaptive soft set based diagnostic risk prediction system; weighted bipartite graph model for recommender system using entropy based similarity measure; automated quiz generator; temporal modelling of bug numbers of open source software applications using LSTM; direct demodulator for amplitude modulated signals using artificial neural network; real-time detection of atrial fibrillation from short time single lead ECG traces using recurrent neural networks; styloLIT: Stylometry and location indicative terms based geographic location estimation using convolutional neural networks; an energy-efficient fuzzy based data fusion and tree based clustering algorithm for wireless sensor networks; eMG pattern classification using neural networks; crime against women: A state level analysis using a hierarchical and k-means clustering techniques; zero pronouns and their resolution in Sanskrit texts; semantic analysis using pairwise sentence comparison with word embeddings; illuminant color inconsistency as a powerful clue for detecting digital image forgery: A survey; a fast, block based, copy-move forgery detection approach using image gradient and modified K-means; oR operation based deterministic extended visual cryptography using complementary cover images; empirical comparison of different key frame extraction approaches with differential evolution based algorithms; breast cancer diagnosis and prognosis using machine learning techniques; performance assessment framework for computational models of visual attention; biologically-inspired foraging decision making in distributed cognitive radio networks; pAM4-based RADAR counter-measures in hostile environments.",,,2-s2.0-85032707507
[No author name available],"2nd International Congress on Information and Communication Technology, ICICT 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031762172&partnerID=40&md5=72bfa7acab12addc84d0aba86622d5ca","The proceedings contain 32 papers. The special focus in this conference is on Information and Communication Technology. The topics include: Preface; automation of railway engine pilot security system using multimodal biometric identification; restricted turn model fault tolerant routing techniques for 3D mesh network-on-chip: An evaluation; power issues of MANET; social media for enhanced e-education at Namibian schools; fifth-level second-generation wavelet-based image fusion algorithm for visual quality enhancement of digital image data; advanced teaching materials of inverted pendulum system by the PLC sequence method; intelligent locker system; precision agriculture system design using wireless sensor network; optimal tree search by a swarm of mobile robots; achieving guaranteed service with fault-tolerant resources in grid; a smart air pollution analytics framework; extracting hidden patterns within road accident data using machine learning techniques; an investigation of the classifiers to detect android malicious apps; a case-based reasoning framework for prediction of stroke; cognitive radio: A network structure perspective; comparative study of N-tier and cloud-based web application using automated load testing tool; the design of ultra-high frequency (UHF) radio frequency identification (RFID) reader antenna; provisioning of healthcare service in cloud; academic analytics implemented for students performance in terms of canonical correlation analysis and chi-square analysis; a pairwise alignment algorithm for long sequences of high similarity; information extraction approaches: A survey; the role of IoT-based devices for the better world; implementation of smart job first dynamic round robin (SJFDRR) scheduling algorithm with smart time quantum in multi-core processing system; feature extraction of protein contact maps from protein 3D-coordinates.",,,2-s2.0-85031762172
[No author name available],"7th International Workshop Soft Computing Applications, SOFA 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031395543&partnerID=40&md5=4f608c918a9b2e84d94ae471a50b8e5b","The proceedings contain 91 papers. The special focus in this conference is on Soft Computing Applications. The topics include: Model evaluation as approach to predict a diagnosis; the basics of implementing intelligent social networks; the generation of tests of knowledge check using genetic algorithms; matrix-valued bidirectional associative memories; enhanced gradient descent algorithms for quaternion-valued neural networks; a reinforcement learning based approach to multiple sequence alignment; supervised learning techniques for body mass estimation in bioarchaeology; a fuzzy approach of sensitivity for multiple colonies on ant colony optimization; nature inspired partitioning clustering algorithms; parametric prediction model using expert system and fuzzy harmonic system; prediction of customer satisfaction using naive bayes, multiclass classifier, K-star and IBK; spam email detection using deep support vector machine, support vector machine and artificial neural network; collaborative business process solution considering an ontological dimension of process models; logistics scenario for wind turbine assembly based on ERP; relieving supply chain through collaboration for wind turbine assembly based on AHP and DBR; integrating E-learning platforms in teaching/learning. a critical review upon the virtual campus usage in UPT; application of the fuzzy-pay-off method in the valuation of a financial instrument; fuzzy logic control strategy for a three-phase electric arc furnace; fuzzy-expert system for investments in winemaking; closed loop blood glucose regulation of type 1 diabetic patient using takagi-sugeno fuzzy logic control; distributive fairness in educational assessment and iris segmentation in the last decade - a survey.",,,2-s2.0-85031395543
"Konar A., Saha S.","Probabilistic neural network based dance gesture recognition",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022018546&doi=10.1007%2f978-3-319-62212-5_6&partnerID=40&md5=e2b8d6b72176c5fe5c2db9858c495c33","With the growing interest in the domain of human computer interaction these days, budding research professionals are coming up with novel ideas of developing more versatile and flexible modes of communication between a man and a machine. Using the attributes of internet, the scientists have been able to create a web based social platform for learning any desired art by the subject himself/herself, and this particular procedure is termed as electronic learning or e-learning. In this chapter, we propose a novel application of gesture dependent e-learning of dance. This e-learning procedure may provide help to many dance enthusiasts who cannot learn the art because of scarcity of resources despite having great zeal. The chapter mainly deals with recognition of different dance gestures of a trained user such that after detecting the discrepancies between the gestures shown and actually performed by a novice; the user can rectify his faults. The elementary knowledge of geometry has been employed to introduce the concept of planes in the feature extraction stage. Actually, five planes have been constructed to signify major body parts while keeping the synchronous parts in one unit. Then four distances and four angular features have been obtained to provide entire positional information of the different body joints. Finally, using a probabilistic neural network the dance gestures have been classified after training the said network with sufficient amount of data recorded from numerous subjects to maintain generality. To check the capability of the discussed method, it has been compared with various standard classifiers in terms of performance indices and in each case the proposed framework has surpassed or provided nearly equal performance as given by the other networks. © 2018, Springer International Publishing AG.",,,2-s2.0-85022018546
"Sreekara Reddy M.B.S., Ratnam C., Rajyalakshmi G., Manupati V.K.","An effective hybrid multi objective evolutionary algorithm for solving real time event in flexible job shop scheduling problem",2018,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029506357&doi=10.1016%2fj.measurement.2017.09.022&partnerID=40&md5=621c700b45a4fe84a6c2735b59216916","This paper addresses the multi-objective model for a flexible job shop scheduling problem (FJSSP) to improve the system performance under the condition of machines break down as a real time event. It is important to identify the relevant performance measures to the mentioned problem for examining the system performance. Therefore, minimization of make span and minimization of total machine load variation is considered as two performance measures. Generally, it is very difficult to develop a mathematical model for the real-time situations in FJSSP. Hence, in this paper we divided the research work into two folds: Primarily, a mixed-integer non-linear programming (MINLP) model has been developed to represent the above-mentioned multi-objectives that subjected to constraints without considering machines break down. Secondarily, by incorporating the machines break down as the real-time event the performance of the system is examined. Solving conflicting objectives simultaneously for finding the optimal/near optimal solutions in a reasonable time is a challenge. In this paper, we proposed a new evolutionary based multi-objective teacher learning-based optimization algorithm (MOTLBO) to solve the above-mentioned complex problem. Moreover, to improve the obtained solutions a local search technique has been incorporated in the MOTLBO and comparisons has been made with existing multi-objective particle swarm optimization (MOPSO) and conventional non-dominated sorting genetic algorithm (CNSGA-II). Results found that the proposed multi-objective-based hybrid meta-heuristic algorithm produced high-quality solutions as proved by the tests we performed over a number of randomly generated test problems. Finally, comparisons also made with how the machines break down can affect the proposed systems performance. © 2017 Elsevier Ltd","Flexible job shop; Multi-objective evolutionary algorithm; NP-hard; Optimization","Education; Evolutionary algorithms; Genetic algorithms; Heuristic algorithms; Integer programming; Multiobjective optimization; Nonlinear programming; Optimization; Particle swarm optimization (PSO); Scheduling; Screening; Teaching; Flexible job shops; Flexible job-shop scheduling problem; Mixed-integer nonlinear programming; Multi objective evolutionary algorithms; Multi objective particle swarm optimization; Multi-objective modeling; Non- dominated sorting genetic algorithms; NP-hard; Job shop scheduling",2-s2.0-85029506357
"Peng Z., Lai J.","Commonsense-knowledge based inference engine",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032642063&doi=10.1007%2f978-981-10-6487-6_16&partnerID=40&md5=3aa38be95c49b62d0eaca195e212a040","Nowadays, more and more industries achieve automatic large-scale production, and also more and more robots undertake the responsibilities of domestic chores. When machines run in industries or robots work in home, they should have abilities to make judgement and abilities to learn from experience, if they want to do their jobs well. In this study, a commonsense knowledge base (CKB) and an inference engine that can support decision making with the commonsense knowledge are built. They can understand human languages, and equip with inferencing abilities, and learning abilities. © Springer Nature Singapore Pte Ltd. 2018.","Artificial intelligence; Commonsense base; Inference engine",,2-s2.0-85032642063
"Manring N.D., Muhi L., Fales R.C., Mehta V.S., Kuehn J., Peterson J.","Using Feedback Linearization to Improve the Tracking Performance of a Linear Hydraulic-Actuator",2018,"Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029176403&doi=10.1115%2f1.4037285&partnerID=40&md5=f0f872e3c8c65f694d62d2b1b17c9ae1","In this paper, a simple feedback linearization method is used to improve the tracking performance of a linear hydraulic-actuator. This research uses an open-centered four-way valve to control the displacement of the hydraulic actuator, based upon an input command from the operator. In this research, the operator is modeled as a first-order system with a bandwidth frequency of 2 Hz. The feedback linearization method is used to adjust the operator input based on the measurement of fluid pressure on only one side of the actuator and the pump pressure that supplies the valve. No other sensing is needed. Using this approach, the R-squared value for tracking a sinusoidal displacement of the actuator and the bandwidth frequency of the actuator are increased. Furthermore, it is shown that the feedback linearization method reduces and nearly eliminates the load dependence of the tracking response, which means that operators should have less difficulty learning how to operate the machine over a wide range of conditions, and the overall productivity of the machine should go up. In summary, the elegance of this model is found in the fact that it is very simple to implement and that the alterations in output performance are greatly enhanced. Copyright © 2018 by ASME.",,"Actuators; Bandwidth; Feedback; Hydraulic actuators; Hydraulic machinery; Linear actuators; Linearization; Bandwidth frequency; Feedback linearization methods; First order systems; Fluid pressures; Load dependence; Output performance; Tracking performance; Tracking response; Feedback linearization",2-s2.0-85029176403
[No author name available],"Chinese Intelligent Systems Conference, CISC 2017",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030863915&partnerID=40&md5=acec316e8f9df847d8bb62853b75a62a","The proceedings contain 144 papers. The special focus in this conference is on Chinese Intelligent Systems. The topics include: Recursive state estimation for discrete-time nonlinear complex networks; adaptive observer design for quasi-one-sided lipschitz nonlinear systems; a feature selection method based on information gain and bp neural network; fault diagnosis of hoist braking system based on improved particle swarm optimization algorithm; attitude control of a quad-rotor based on LADRC; quasi-synchronization of chaotic systems with parameter mismatches via aperiodically intermittent control; interactive touch control method based on image denoising technology; EEG multi-fractal de-trended fluctuation mental stress analysis; the simulation of neural oscillations during propofol anesthesia based on the FPGA platform.; decision making in multi-agent systems based on the evolutionary game with switching probabilities; fault diagnosis of rolling bearing based on wavelet packet and extreme learning machine; event-triggered control for multi-agent system with a smart leader; visual based abnormal target annotation with recurrent neural networks; analysis of the fluid approximation of stochastic process algebra models; a wall interactive system based on infrared electronic pen; the design of new energy-saving DC brake device; data missing process by extended kalman filter with equality constraints; adaptive controller for flexible-joint robot; dynamic modeling and analysis of the micro-spacecraft ejection separation system; neural network adaptive control for hysteresis hammerstein system; microblog query expansion based on ontology expansion and borda count rank; microblogging event search based on LSTM model and microblog search method based on neural network language model.",,,2-s2.0-85030863915
[No author name available],"3rd International Afro-European Conference for Industrial Advancement, AECIA 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028639052&partnerID=40&md5=5adcca636eaf0ddfbe16e90e79bf5b92","The proceedings contain 37 papers. The special focus in this conference is on Industrial Advancement. The topics include: Integrated autopilot complex system proposal for aircraft model application; maximizing the delivery rate for DTN networks; application to an automated manufacturing system; normalized compression distance based implementation for text data; adaptive methods of process state evaluation: the development of an application for engineering purposes; detection of finger flexions based on decision tree; a new fuzzy clustering algorithm to enhance lifetime of wireless sensor networks; exploratory data analysis of software requirements using statistics and kohonen’s self-organizing map; method for estimation of software requirements using neural network based classification technique; fall detection for elderly based on background subtraction and key points matching; active and reactive power robust control of doubly fed induction generator wind turbine to satisfy new grid codes; scalable addressing mechanism for structured P2P networks; improving the speed and quality of extreme learning machine by conjugate gradient method; new multi-criteria decision-making based on fuzzy similarity, distance and ranking; application in EOLES european project; analysis of air pollution in vertical profile using self-organizing maps; an enhanced delay time model to support evidence based maintenance in healthcare domain; a novel ant colony optimization based cryptanalysis of substitution cipher; software defined stochastic model for moving target defense; adaptive educational games using game metrics and the future of education.",,,2-s2.0-85028639052
[No author name available],"5th International Conference on Frontier Computing, FC 2016",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410159&partnerID=40&md5=f082c7ef5a7d0fcf3bae1335c7bc451b","The proceedings contain 98 papers. The special focus in this conference is on Frontier Computing. The topics include: Building the profile of web events based on website measurement; building domain keywords using cognitive based sentences framework; the study on vehicle detection based on DPM in traffic scenes; the research on video security carving using secure outsourcing approach; community semantics recommender based on association link network; research and implementation of FM-DCSK chaotic communication system based on GNU radio platform; a survey of techniques for the representation of very large access control matrices; a settling time model for testing potential induced degradation of solar cells; multi-stage dictionary learning for image super-resolution based on sparse representation; research and practice of genetic algorithm theory; micro-blog friend recommendation algorithms based on content and social relationship; a hybrid methodologies for intrusion detection based deep neural network with support vector machine and clustering technique; PET/CT imaging automated classification, structure testing and inspection of the human spinal cord; the improved gaussian mixture model for real-time detection system of moving object; music similarity evaluation based on onsets; human activity recognition with smart watch based on H-SVM; integrating augmented reality technology into subject teaching; influence of inclined angles on the stability of inclined granular flows down rough bottoms; evaluation of influences of frictions in hopper flows through GPU simulations; the requirement analysis and initial design of a cloud and crowd supported mathematics learning environment for computer science students.",,,2-s2.0-85031410159
"Pudaruth S., Soyjaudah K.M.S., Gunputh R.P.","Markov Chain monte carlo methods and evolutionary algorithms for automatic feature selection from legal documents",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032705319&doi=10.1007%2f978-3-319-68385-0_12&partnerID=40&md5=023f8dfa60faf8bbec4c6dad9cdfc584","In this paper, we present three different approaches for feature selection, starting from a naïve Markov Chain Monte Carlo random walk algorithm to more refined methods like simulated annealing and genetic algorithms. It is typical for textual data to have thousands of dimensions in their feature space which makes feature selection a crucial phase before the final classification. Classification of legal documents into eight categories was performed via a simple document similarity measure based on term frequency and the nearest neighbour concept. With an average success rate of 76.4%, the random walk algorithm not only performed better than the simulated annealing and genetic algorithms but also matched the accuracy of support vector machines. Although these methods have commonly been used for selecting appropriate features in other fields, their use in text categorisation have not been satisfactorily investigated. And, to our knowledge, this is the first work which investigates their use in the legal domain. This generic text classification framework can further be enhanced by using an active learning methodology for the selection of training samples rather than following a passive learning approach. © Springer International Publishing AG 2018.","Court judgements; Genetic algorithm; Legal text categorisation; Monte carlo; Random walk; Simulated annealing","Authentication; Chains; Classification (of information); Evolutionary algorithms; Feature extraction; Genetic algorithms; Information retrieval systems; Intelligent systems; Markov processes; Random processes; Simulated annealing; Text processing; Automatic feature selection; Court judgement; Legal texts; Markov chain Monte Carlo method; Markov Chain Monte-Carlo; Random Walk; Random walk algorithms; Text classification; Monte Carlo methods",2-s2.0-85032705319
[No author name available],"3rd International Conference on Advanced Intelligent Systems and Informatics, AISI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029500347&partnerID=40&md5=31d50d254e3f21c089dda1394141a8f7","The proceedings contain 85 papers. The special focus in this conference is on Advanced Intelligent Systems and Informatics. The topics include: BP-MPSO algorithm for PUMA robot vacumn path planning; modeling and performance analysis of planar parallel manipulators; design and simulation of fuzzy water monitoring system using WSN for fishing; passivity based decoupling of lagrangian systems; control of new type of fractional chaos synchronization; control of a two link planar electrically-driven rigid robotic manipulator using fractional order SOFC; wavefront and a-star algorithms for mobile robot path planning; novel multi-input multi-output brain emotional learning based intelligent controller for PUMA 560 robotic arm; a computational offloading framework for object detection in mobile devices; a hybrid EEG signals classification approach based on grey wolf optimizer enhanced svms for epileptic detection; a model of electrokinetic platform for separation of different sizes of biological particles; breast cancer detection using polynomial fitting for intensity spreading within ROIs; low complexity intra-prediction algorithm for video coding standards; autonomic self-healing approach to eliminate hardware faults in wireless sensor networks; classification of toxicity effects of biotransformed hepatic drugs using optimized support vector machine; automating requirements elicitation of cloud-based ERPs; an experimental evaluation of binary feature descriptors; developing an efficient clique-based algorithm for community detection in large graphs; stock exchange threat modeling, EGX as a case study; stock exchange threat modeling, EGX as a case study and semantic cloud community framework for services provision.",,,2-s2.0-85029500347
"Minorowicz B., Palubicki M., Stec N., Bartoszek J., Antczak L., Matyszczak J.","Survey on design and development of hexapod walking robot, automated guided vehicle and drone",2018,"Lecture Notes in Mechanical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032677442&doi=10.1007%2f978-3-319-68619-6_38&partnerID=40&md5=f16dbe6dc9c3389b0675b44ee56da63f","This paper presents scope of work performed by Students in R&D Group “Mechatron”, where student can develop own ideas under supervision of experienced tutors. The work carried out mostly on autonomous machines and issues related with their control. In recent years, few interesting designs have been developed by Students, three chosen are presented in this paper. In the first section, two designs were presented and it means automated guided vehicle and drone. Presented line follower and its designer participate successfully in nationwide competitions. This vehicle is a result of three years of learning and gaining experience, which allowed for fabrication such complex design. Next described project is a drone, where students prepared 3D printed reinforced lightweight frame. The purpose of this drone is to carriage, e.g., first aid or rescue equipment like ropes. The following and the biggest part of the paper is focused on walking robot. Authors analyzed different kinematics for movement generation and finally, it was decided to perform legs like a cockroach. Based on a kinematic chain, inverse kinematic approach and oscillators Authors successfully evaluated control algorithm which was used for walking generation. © Springer International Publishing AG 2018.","Design; Hexapod walking robot; Line follower; Modeling; UAV; UGV",,2-s2.0-85032677442
[No author name available],"8th International Conference on Soft Computing and Pattern Recognition, SoCPaR 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028586241&partnerID=40&md5=50a17fe310a0338b25c3ccc69a1a0259","The proceedings contain 70 papers. The special focus in this conference is on Soft Computing and Pattern Recognition. The topics include: Toward real-time high-frequency stock monitoring system using node.js; sensitivity analysis on effect of biomechanical factors for classifying vertebral deformities; soft feature based personal recognition; a new form of fuzzy reasoning tool to ensure both accuracy and readability; a novel edge based image steganography technique; RDE - reconstructed mutation strategy for differential evolution algorithm; predicting mobile application ratings using artificial neural network; dimensionality reduction of sift descriptor using vector decomposition for image classification; a comparative analysis of the different data mining tools by using supervised learning algorithms; a well organized phrase-based document clustering using ASCII values and adjacency list; reconstruction of 3-dimensional scenes using depth from defocus and artificial neural networks trained on fractals; performance comparison between apache hive and oracle SQL for big data analytics; design of wide beam hexagonal shaped circularly polarized patch antenna for WLAN application; an experimental comparison with time-series methods; advanced deep neural networks for pattern recognition; security enabled cluster head selection for wireless sensor network using improved firefly optimization; chaperoning the optimization of symmetric finFET circuits; solving machine part cell formation problem using genetic algorithm based evolutionary computing; DWT based source localization using microphone array; prevention of illegal content sharing in peer to peer systems; towards designing a framework for practical keystroke dynamics based authentication and exudates in detection and classification of diabetic retinopathy.",,,2-s2.0-85028586241
"Lam Pham T., Kino H., Terakura K., Miyake T., Tsuda K., Takigawa I., Chi Dam H.","Machine learning reveals orbital interaction in materials",2017,"Science and Technology of Advanced Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032589753&doi=10.1080%2f14686996.2017.1378060&partnerID=40&md5=5b5486dcfa03bb0b054d078ac305b8b5","We propose a novel representation of materials named an ‘orbital-field matrix (OFM)’, which is based on the distribution of valence shell electrons. We demonstrate that this new representation can be highly useful in mining material data. Experimental investigation shows that the formation energies of crystalline materials, atomization energies of molecular materials, and local magnetic moments of the constituent atoms in bimetal alloys of lanthanide metal and transition-metal can be predicted with high accuracy using the OFM. Knowledge regarding the role of the coordination numbers of the transition-metal and lanthanide elements in determining the local magnetic moments of the transition-metal sites can be acquired directly from decision tree regression analyses using the OFM. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","data mining; machine learning; magnetic materials; Material descriptor; material informatics","Artificial intelligence; Crystalline materials; Data mining; Decision trees; Learning systems; Magnetic materials; Magnetic moments; Magnetism; Metals; Rare earth elements; Regression analysis; Transition metals; Atomization energies; Decision tree regression; Descriptors; Experimental investigations; Local magnetic moments; Material Informatics; Molecular materials; Transition metal site; Transition metal alloys",2-s2.0-85032589753
"Shin D., Lee S., Shyam A., Haynes J.A.","Petascale supercomputing to accelerate the design of high-temperature alloys",2017,"Science and Technology of Advanced Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032332635&doi=10.1080%2f14686996.2017.1371559&partnerID=40&md5=88dcc669c774d58a2175e1b562c4af7e","Recent progress in high-performance computing and data informatics has opened up numerous opportunities to aid the design of advanced materials. Herein, we demonstrate a computational workflow that includes rapid population of high-fidelity materials datasets via petascale computing and subsequent analyses with modern data science techniques. We use a first-principles approach based on density functional theory to derive the segregation energies of 34 microalloying elements at the coherent and semi-coherent interfaces between the aluminium matrix and the θ′-Al2Cu precipitate, which requires several hundred supercell calculations. We also perform extensive correlation analyses to identify materials descriptors that affect the segregation behaviour of solutes at the interfaces. Finally, we show an example of leveraging machine learning techniques to predict segregation energies without performing computationally expensive physics-based simulations. The approach demonstrated in the present work can be applied to any high-temperature alloy system for which key materials data can be obtained using high-performance computing. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","alloys; correlation analysis; density functional theory; first-principles calculations; machine learning; Supercomputing","Alloying; Artificial intelligence; Calculations; Computation theory; Correlation methods; Interfaces (materials); Learning systems; Population statistics; Correlation analysis; First-principles approaches; First-principles calculation; High performance computing; Machine learning techniques; Physics-based Simulation; Semi-coherent interfaces; Supercomputing; Density functional theory",2-s2.0-85032332635
"Rial M., Martínez Cortizas A., Rodríguez-Lado L.","Understanding the spatial distribution of factors controlling topsoil organic carbon content in European soils",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026733722&doi=10.1016%2fj.scitotenv.2017.08.012&partnerID=40&md5=d24c85abe24e8003b1cc21d22706ffd7","Soil Organic Carbon (SOC) constitutes the largest terrestrial carbon pool. The understanding of its dynamics and the environmental factors that influence its behaviour as sink or source of atmospheric CO2 is crucial to quantify the carbon budget at the global scale. At the European scale, most of the existing studies to account for SOC stocks are centred in the fitting of predictive model to ascertain the distribution of SOC. However, the development of methodologies for monitoring and identifying the environmental factors that control SOC storage in Europe remains a key research challenge. Here we present a modelling procedure for mapping and monitoring SOC contents that uses Visible-Near Infrared (VNIR) spectroscopic measurements and a series of environmental covariates to ascertain the key environmental processes that have a major contribution into SOC sequestration processes. Our results show that it follows a geographically non-stationary process in which the influencing environmental factors have different weights depending on the spatial location. This implies that SOC stock modelling should not rely on a single model but on a combination of different statistical models depending on the environmental characteristics of each area. A cluster classification of European soils in relation to those factors resulted in the determination of four groups for which specific models have been obtained. Differences in climate, soil pH, content of coarse fragments or land cover type are the main factors explaining the differences in SOC in topsoil from Europe. We found that climatic conditions are the main driver of SOC storage at the continental scale, but we also found that parameters like land cover type influence SOC content found at the local scales in certain areas. Our methodology developed at continental scale could be used in future research aimed to improve the predictive performance of SOC assessments at European scale. © 2017 Elsevier B.V.","Infrared spectroscopy; Machine learning; Soil organic carbon; Spatial statistics","Budget control; Infrared devices; Infrared spectroscopy; Learning systems; Soils; Environmental characteristic; Nonstationary process; Organic carbon contents; Predictive performance; Soil organic carbon; Spatial statistics; Spectroscopic measurements; Visible near-infrared; Organic carbon; organic carbon; environmental factor; infrared spectroscopy; machine learning; organic carbon; soil organic matter; spatial distribution; statistical analysis; topsoil; Article; biomass; carbon sequestration; carbon storage; climate; environmental factor; Europe; geographic distribution; geographic mapping; geostatistical analysis; land use; near infrared spectroscopy; priority journal; soil acidity; soil analysis; soil chemistry; spatial soil variability; surface soil; Europe",2-s2.0-85026733722
"Singh B.K., Verma K., Panigrahi L., Thoke A.S.","Integrating radiologist feedback with computer aided diagnostic systems for breast cancer risk prediction in ultrasonic images: An experimental investigation in machine learning paradigm",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027710107&doi=10.1016%2fj.eswa.2017.08.020&partnerID=40&md5=26a4ced0f77e8b0b3eb9b3592611e7c4","With advancements in machine learning algorithms and computer aided diagnostic (CAD) systems, the performance of automated analysis of radiological images has improved substantially in recent times. However, the lack of integration between the radiologist and CAD systems restrains the rate of progress as well as the reach of such advancements in clinical use. This article aims to improve the clinical efficiency of ultrasound based CAD systems for classification of breast lesions by integrating back-propagation artificial neural network (BPANN), support vector machine (SVM) and radiologist feedback. The acquired breast ultrasound images were subjected to wavelet based filtering in order to reduce speckle noise followed by feature extraction, feature selection and classification. Experiments on a database of 178 ultrasound images of breast anomalies (88 benign and 90 malignant) show that the proposed methodology achieves classification accuracy of 98.621% and 98.276%, respectively, when all 457 and 19 most relevant features selected by multi-criteria feature selection method were used for classification. The accuracy achieved is significantly higher than that using conventional classifiers based on BPANN and SVM. Further, it is found that integrating expert opinion in CAD systems improves its overall performance. The quantitative results obtained are discussed in light of some recently reported studies. © 2017 Elsevier Ltd","Breast tumor classification; Machine learning; Neural network; Radiologist opinion; Support vector machine; Ultrasound","Artificial intelligence; Backpropagation; Computer aided analysis; Computer aided diagnosis; Computer aided instruction; Diagnosis; Feature extraction; Learning algorithms; Learning systems; Medical imaging; Neural networks; Support vector machines; Ultrasonic imaging; Ultrasonics; Wavelet analysis; Back propagation artificial neural network (BPANN); Breast tumor classifications; Breast ultrasound images; Computer aided diagnostics; Experimental investigations; Feature selection and classification; Feature selection methods; Radiologist opinion; Classification (of information)",2-s2.0-85027710107
"Stallard R., Rejc E., Conn Welch K.","Wavelet-derived features as indicators of physiological changes induced by bed rest",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028019283&doi=10.1016%2fj.eswa.2017.08.024&partnerID=40&md5=622f89a9f9af0ca20939ab3e002a5374","Objective: Bed rest studies are employed to simulate microgravity situations as encountered in spaceflight. Current methods of assessing muscle function impairment due to microgravity exposure include techniques such as maximum voluntary contraction assessments using force measurements. Such techniques involve impractical long-feedback loops for applications involving rehabilitation or otherwise detecting physiological changes. Recent studies have made use of the discrete wavelet transform in combination with machine learning methods to classify hand gestures and detect pathologies. In this paper, we demonstrate models capable of discriminating between the before and after bed rest states by extracting features from surface electromyography measurements. Methods: A previously conducted and studied bed rest experiment is examined by discrete wavelet transform for tractable feature sets for the purpose of k-nearest neighbor and Support Vector Machine classification. Forward feature selection is used with k-nearest neighbor or Support Vector Machine selection criteria. Classifiers are evaluated on non-wavelet-derived features for sake of comparison. Results: Wavelet-derived features perform well for both classifiers with classification accuracies as high as 95%. Models without wavelet-derived features do not perform as well overall. Conclusion: These high-accuracy results are promising for future efforts in neuromuscular monitoring and further investigations with larger sample sizes. Significance: Classification algorithms utilizing features derived by wavelet transforms provide a method toward development of short-feedback loop measurements of the physiological effects of prolonged disuse. © 2017 Elsevier Ltd","Classification algorithms; Electromyography; Wavelets","Discrete wavelet transforms; Electromyography; Feedback; Learning systems; Microgravity; Microgravity processing; Motion compensation; Muscle; Nearest neighbor search; Physiology; Support vector machines; Wavelet transforms; Classification accuracy; Classification algorithm; Forward feature selections; Machine learning methods; Maximum voluntary contraction; Support vector machine classification; Surface electromyography; Wavelets; Classification (of information)",2-s2.0-85028019283
"Gorriz J.M., Ramirez J., Suckling J., Martinez-Murcia F.J., Illán I.A., Segovia F., Ortiz A., Salas-González D., Castillo-Barnés D., Puntonet C.G.","A semi-supervised learning approach for model selection based on class-hypothesis testing",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026919588&doi=10.1016%2fj.eswa.2017.08.006&partnerID=40&md5=971a94d45ba5f1d7e3d243fa6a0063d0","This paper deals with the topic of learning from unlabeled or noisy-labeled data in the context of a classification problem. In the classification problem the outcome yields one of a discrete set of values thus, assumptions on them could be established to obtain the most likely prediction model at the training stage. In this paper, a novel case-based model selection method is proposed, which combines hypothesis testing from a discrete set of expected outcomes and feature extraction within a cross-validated classification stage. This wrapper-type procedure acts on fully-observable variables under hypothesis-testing and improves the classification accuracy on the test set, or keeps its performance at least at the level of the statistical classifier. The model selection strategy in the cross validation loop allows building an ensemble classifier that could improve the performance of any expert and intelligence system, particularly on small sample-size datasets. Experiments were carried out on several databases yielding a clear improvement on the baseline, i.e., SPECT dataset Acc=86.35±1.51, with Sen=91.10±2.77, and Spe=81.11±1.61. In addition, the CV error estimate for the classifier under our approach was found to be an almost unbiased estimate (as the baseline approach) of the true error that the classifier would incur on independent data. © 2017 Elsevier Ltd","Hypothesis testing; Partial least squares; Semi-supervised learning; Statistical learning and decision theory; Support vector machines (SVM)",,2-s2.0-85026919588
"Petropoulos A., Chatzis S.P., Siakoulis V., Vlachogiannakis N.","A stacked generalization system for automated FOREX portfolio trading",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027974046&doi=10.1016%2fj.eswa.2017.08.011&partnerID=40&md5=f7ea56c9e0b2962bcc9a36b15225e28c","Multiple FOREX time series forecasting is a hot research topic in the literature of portfolio trading. To this end, a large variety of machine learning algorithms have been examined. However, it is now widely understood that, in real-world trading settings, no single machine learning model can consistently outperform the alternatives. In this work, we examine the efficacy and the feasibility of developing a stacked generalization system, intelligently combining the predictions of diverse machine learning models. Our approach establishes a novel inferential framework that comprises the following levels of data processing: (i) We model the dependence patterns between major currency pairs via a diverse set of commonly used machine learning algorithms, namely support vector machines (SVMs), random forests (RFs), Bayesian autoregressive trees (BART), dense-layer neural networks (NNs), and naïve Bayes (NB) classifiers. (ii) We generate implied signals of exchange rate fluctuation, based on the output of these models, as well as appropriate side information obtained by analyzing the correlations across currency pairs in our training datasets. (iii) We finally combine these implied signals into an aggregate predictive waveform, by leveraging majority voting, genetic algorithm optimization, and regression weighting techniques. We thoroughly test our framework in real-world trading scenarios; we show that our system leads to significantly better trading performance than the considered benchmarks. Thus, it represents an attractive solution for financial firms and corporations that perform foreign exchange portfolio management and daily trading. Our system can be used as an integrated part in international commercial trade activities or in a quantitative investing framework for algorithmic trading and carry-trade speculation. © 2017 Elsevier Ltd","Algorithmic trading; Forex forecasting; Machine learning; Portfolio management; Stacked generalization",,2-s2.0-85027974046
"Alphonse A.S., Dharma D.","Enhanced Gabor (E-Gabor), Hypersphere-based normalization and Pearson General Kernel-based discriminant analysis for dimension reduction and classification of facial emotions",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027501922&doi=10.1016%2fj.eswa.2017.08.013&partnerID=40&md5=b55857645fde1d92996b99782b6188ad","This paper puts forward an Enhanced Gabor feature descriptor termed as E-Gabor for obtaining high classification accuracy of emotions with low dimension. Two methods have been used for further classification. In the first method, E-Gabor is used as a stand-alone feature for classification. Hypersphere-based normalization has been used for normalizing the E-Gabor features, thereby improving the efficiency in the classification of emotions. In the second method, the E-Gabor feature descriptor is fused with Pyramid Histogram of Gradient (PHOG) feature descriptor and projected to a common subspace of six dimensions using the proposed Pearson General Kernel-based Discriminant Analysis (PGK-DA) before classification. In both the methods, Pearson General Kernel-based Extreme Learning Machine (PGK-ELM) is used for classification. Experiments conducted on Japanese Female Facial Expression (JAFFE), Cohn Kanade (CK+), Multimedia Understanding Group (MUG), Static Facial Expressions in the Wild (SFEW), Oulu-Chinese Academy of Science, Institute of Automation (Oulu-CASIA) and Man–Machine Interaction (MMI) datasets report a classification accuracy of 97.6%, 97.9%, 95.7%, 35.4%, 87.7% and 82.7% with method I and 95.7%, 97.2%, 94.9%, 35.2%, 87.1% and 82.1% with method II, respectively, for seven class emotion detection, which is high when compared to other state-of-the-art methods. © 2017 Elsevier Ltd","Discriminant analysis; ELM; Emotion; Facial expression; Gabor","Discriminant analysis; Learning systems; Chinese Academy of Sciences; Classification accuracy; Classification of emotions; Emotion; Extreme learning machine; Facial Expressions; Gabor; State-of-the-art methods; Classification (of information)",2-s2.0-85027501922
"Abd Elaziz M., Oliva D., Xiong S.","An improved Opposition-Based Sine Cosine Algorithm for global optimization",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028517439&doi=10.1016%2fj.eswa.2017.07.043&partnerID=40&md5=c6e65cb4966d7ccbab759ff154355341","Real life optimization problems require techniques that properly explore the search spaces to obtain the best solutions. In this sense, it is common that traditional optimization algorithms fail in local optimal values. The Sine Cosine Algorithms (SCA) has been recently proposed; it is a global optimization approach based on two trigonometric functions. SCA uses the sine and cosine functions to modify a set of candidate solutions; such operators create a balance between exploration and exploitation of the search space. However, like other similar approaches, SCA tends to be stuck into sub-optimal regions that it is reflected in the computational effort required to find the best values. This situation occurs due that the operators used for exploration do not work well to analyze the search space. This paper presents an improved version of SCA that considers the opposition based learning (OBL) as a mechanism for a better exploration of the search space generating more accurate solutions. OBL is a machine learning strategy commonly used to increase the performance of metaheuristic algorithms. OBL considers the opposite position of a solution in the search space. Based on the objective function value, the OBL selects the best element between the original solution and its opposite position; this task increases the accuracy of the optimization process. The hybridization of concepts from different fields is crucial in intelligent and expert systems; it helps to combine the advantages of algorithms to generate more efficient approaches. The proposed method is an example of this combination; it has been tested over several benchmark functions and engineering problems. Such results support the efficacy of the proposed approach to find the optimal solutions in complex search spaces. © 2017 Elsevier Ltd","Engineering problems; Metaheuristic (MH); Opposition-Based Learning (OBL); Sine Cosine Algorithms (SCA)","Cosine transforms; Expert systems; Functions; Global optimization; Learning algorithms; Learning systems; Optimal systems; Engineering problems; Exploration and exploitation; Meta heuristic algorithm; Metaheuristic; Objective function values; Opposition-based learning; Optimization algorithms; Sine-cosine algorithm; Optimization",2-s2.0-85028517439
"Mohamed N.S., Zainudin S., Ali Othman Z.","Metaheuristic approach for an enhanced mRMR filter method for classification using drug response microarray data",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028024950&doi=10.1016%2fj.eswa.2017.08.026&partnerID=40&md5=1a79033a4d6a8e7497f4308265a22bb8","Quality data mining analysis based on microarray gene expression data is a good approach for disease classification and other fields, such as pharmacology, as well as a useful tool for medical innovation. One of the challenges in classification is that microarrays involve high dimensionality and a large number of redundant and irrelevant features. Feature selection is the most popular method for determining the optimal number of features that will be used for classification. Feature selection is important to accelerate learning, which is represented only by the optimal feature subset. The current approach for microarray feature selection for the filter method is to simply select the top-ranked genes, i.e., keeping the 50 or 100 best-ranked genes. However, the current approach is determined by human intuition; it requires trial and error, and thus, is time-consuming. Accordingly, this study aims to propose a metaheuristic approach for selecting the top n relevant genes in drug microarray data to enhance the minimum redundancy–maximum relevance (mRMR) filter method. Three metaheuristics are applied, namely, particle swarm optimization (PSO), cuckoo search (CS), and artificial bee colony (ABC). Subsequently, k-nearest neighbor and support vector machine are used as classifiers to evaluate classification performance. The experiment used a microarray gene dataset of liver xenobiotic and pharmacological responses. Experimental results show that meta-heuristic is more efficient approaches that have reduced the complexity of the classifier. Furthermore, the results show that mRMR-CS exhibits the best performance compared with mRMR-PSO and mRMR-ABC. © 2017 Elsevier Ltd","Classification; Data mining; Feature selection; Filter; Microarray","Bandpass filters; Bioassay; Data mining; Evolutionary algorithms; Feature extraction; Filtration; Gene expression; Genes; Heuristic algorithms; Medical computing; Microarrays; Nearest neighbor search; Optimization; Particle swarm optimization (PSO); Quality control; Support vector machines; Artificial bee colonies (ABC); Classification performance; Disease classification; Filter; K-nearest neighbors; Meta-heuristic approach; Microarray gene expression data; Pharmacological response; Classification (of information)",2-s2.0-85028024950
"Kamkarhaghighi M., Makrehchi M.","Content Tree Word Embedding for document representation",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028017103&doi=10.1016%2fj.eswa.2017.08.021&partnerID=40&md5=7a633253e22f49d65359fac091308787","Only humans can understand and comprehend the actual meaning that underlies natural written language, whereas machines can form semantic relationships only after humans have provided the parameters that are necessary to model the meaning. To enable computer models to access the underlying meaning in written language, accurate and sufficient document representation is crucial. Recently, word embedding approaches have drawn much attention in text mining research. One of the main benefits of such approaches is the use of global corpuses with the generation of pre-trained word vectors. Although very effective, these approaches have their disadvantages. Relying only on pre-trained word vectors may neglect the local context and increase word ambiguity. In this study, a new approach, Content Tree Word Embedding (CTWE), is introduced to mitigate the risk of word ambiguity and inject a local context into globally pre-trained word vectors. CTWE is basically a framework for document representation while using word embedding feature learning. The CTWE structure is locally learned from training data and ultimately represents the local context. While CTWE is constructed, each word vector is updated based on its location in the content tree. For the task of classification, the results show an improvement in F-score and accuracy measures when using two deep learning-based word embedding approaches, namely GloVe and Word2Vec. © 2017 Elsevier Ltd","Content tree; Deep learning; GloVe; Sentiment analysis; Word embedding; Word2Vec","Data mining; Deep learning; Natural language processing systems; Semantics; Vectors; Content tree; GloVe; Sentiment analysis; Word embedding; Word2Vec; Forestry",2-s2.0-85028017103
"Efremenko D.S., Loyola R D.G., Hedelt P., Spurr R.J.D.","Volcanic SO2 plume height retrieval from UV sensors using a full-physics inverse learning machine algorithm",2017,"International Journal of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028676439&doi=10.1080%2f01431161.2017.1348644&partnerID=40&md5=51a8910b2b4c1f7324e55e0b04a5208a","Precise knowledge of the location and height of the volcanic sulphur dioxide (SO2) plume is essential for accurate determination of SO2 emitted by volcanic eruptions. Current SO2 plume height retrieval algorithms based on ultraviolet (UV) satellite measurements are very time-consuming and therefore not suitable for near-real-time applications. In this work we present a novel method called the full-physics inverse learning machine (FP-ILM) algorithm for extremely fast and accurate retrieval of the SO2 plume height. FP-ILM creates a mapping between the spectral radiance and the geophysical parameters of interest using supervised learning methods. The FP-ILM combines smart sampling methods, dimensionality reduction techniques, and various linear and non-linear regression analysis schemes based on principal component analysis and neural networks. The computationally expensive operations in FP-ILM are the radiative transfer model computations of a training dataset and the determination of the inversion operator–these operations are performed off-line. The application of the resulting inversion operator to real measurements is extremely fast since it is based on calculations of simple regression functions. Retrieval of the SO2 plume height is demonstrated for the volcanic eruptions of Mt. Kasatochi (in 2008) and Eyjafjallajökull (in 2010), measured by the GOME-2 (Global Ozone Monitoring Instrument–2) UV instrument on-board MetOp-A. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",,"Inverse problems; Learning algorithms; Personnel training; Photomapping; Principal component analysis; Radiative transfer; Regression analysis; Sulfur dioxide; Volcanoes; Dimensionality reduction techniques; Geophysical parameters; Non-linear regression analysis; Ozone monitoring instruments; Radiative transfer model; Regression function; Satellite measurements; Supervised learning methods; Learning systems; algorithm; inverse problem; machine learning; plume; satellite sensor; sensor; sulfur dioxide; volcanic eruption",2-s2.0-85028676439
"Ding J., Dong Y., Gao T., Zhang Z., Liu Y.","Sentiment Analysis of Chinese Micro-Blog Based on Classification and Rich Features",2017,"Proceedings - 13th Web Information Systems and Applications Conference, WISA 2016 - In conjunction with 1st Symposium on Big Data Processing and Analysis, BDPA 2016 and 1st Workshop on Information System Security, ISS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027224067&doi=10.1109%2fWISA.2016.22&partnerID=40&md5=5ba0c42f5f9f084a22f0ffca015b0677","With the development of the Web2.0, micro-blogs gradually become a common essential part of the public life. The reviews in the micro-blogs have huge hidden value. Many machine learning approaches have been used to solve sentiment analysis. However, the features used in existing researches are still not enough. To improve the accuracy of sentiment analysis, in this paper, we use a classification approach to solve two tasks of sentiment analysis: identifying opinion sentence and judging sentiment polarity of the emotional sentence. And we incorporate five kinds of features: sentiment lexicons-based features, N-POS(part of speech combination)-based features, pattern-based features, special symbols-based features and length-based features to train seven classifiers and compare their performance. Experimental result shows that Random Forest classifier achieves the best performance. © 2016 IEEE.","Chinese micro-blog; classification; multiple features; sentiment analysis","Big data; Blogs; Data handling; Data mining; Decision trees; Information systems; Learning systems; Classification approach; Machine learning approaches; Micro-blog; Multiple features; Random forest classifier; Sentiment analysis; Sentiment lexicons; Special symbols; Classification (of information)",2-s2.0-85027224067
"Niu J., Wang S., Niu W., Atiquzzaman M.","User-aware partitioning algorithm for mobile cloud computing based on maximum graph cuts",2017,"Computer Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030686800&doi=10.1016%2fj.comnet.2017.09.011&partnerID=40&md5=e55fee284e2f8782419cce8cca29136d","Partitioning and offloading of mobile applications have been demonstrated as a promising approach that not only enhances the performance but also extends the battery life of Smart Mobile Devices (SMDs) effectively. Researchers have proposed four factors that affect the partitioning decision: device (hardware), application (software), developer and user. However, most existing research efforts focus on the first three factors and pay little attention to the influence of user preferences on the partitioning decision. Among these factors, user preference usually affects user experience most. Moreover, previous work which took into account the other factors cannot generate adaptive partitioning results. In this work, we propose a user-aware partitioning algorithm to offer a personalized and precise partitioning plan for better user experience. Based on machine learning methods, we first propose a user profile model for characterizing the preferences of different phone users. In addition, a novel cost evaluation model (called CMET model) is proposed to evaluate the comprehensive offloading costs in terms of CPU & memory utilization, time cost and energy consumption. Finally, we propose a Max-Cuts partitioning algorithm based on Branch-and-Bound search to obtain the optimal partitioning plan. Experimental results demonstrate that for different types of phone users, our partitioning algorithm could effectively improve corresponding performances that they concern about, and achieve the most satisfactory user experience compared with state-of-the-art approaches. © 2017 Elsevier B.V.","Graph-based model; Maximum graph cuts; Mobile cloud computing; Partitioning algorithm; User profile","Application programs; Axial flow; Cloud computing; Costs; Energy utilization; Graphic methods; Learning systems; Mobile telecommunication systems; Telephone sets; User interfaces; Adaptive partitioning; Branch and bound search; Graph-based modeling; Maximum graphs; Optimal partitioning; Partitioning algorithms; State-of-the-art approach; User profile; Mobile cloud computing",2-s2.0-85030686800
"Sankari E.S., Manimegalai D.","Predicting membrane protein types using various decision tree classifiers based on various modes of general PseAAC for imbalanced datasets",2017,"Journal of Theoretical Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029784094&doi=10.1016%2fj.jtbi.2017.09.018&partnerID=40&md5=7cdba187c1465b92226d7cefa7303ae4","Predicting membrane protein types is an important and challenging research area in bioinformatics and proteomics. Traditional biophysical methods are used to classify membrane protein types. Due to large exploration of uncharacterized protein sequences in databases, traditional methods are very time consuming, expensive and susceptible to errors. Hence, it is highly desirable to develop a robust, reliable, and efficient method to predict membrane protein types. Imbalanced datasets and large datasets are often handled well by decision tree classifiers. Since imbalanced datasets are taken, the performance of various decision tree classifiers such as Decision Tree (DT), Classification And Regression Tree (CART), C4.5, Random tree, REP (Reduced Error Pruning) tree, ensemble methods such as Adaboost, RUS (Random Under Sampling) boost, Rotation forest and Random forest are analysed. Among the various decision tree classifiers Random forest performs well in less time with good accuracy of 96.35%. Another inference is RUS boost decision tree classifier is able to classify one or two samples in the class with very less samples while the other classifiers such as DT, Adaboost, Rotation forest and Random forest are not sensitive for the classes with fewer samples. Also the performance of decision tree classifiers is compared with SVM (Support Vector Machine) and Naive Bayes classifier. © 2017 Elsevier Ltd","Decision tree classifiers; Membrane protein types; Prediction","membrane protein; algorithm; amino acid; data set; membrane; prediction; protein; tree; adaboost; amino acid composition; Article; Bayesian learning; classification and regression tree; classifier; controlled study; decision tree; intermethod comparison; prediction; priority journal; pseudo amino acid composition; random forest; random tree; random under sampling boost; reduced error pruning tree; rotation forest; support vector machine",2-s2.0-85029784094
"Khan M., Hayat M., Khan S.A., Ahmad S., Iqbal N.","Bi-PSSM: Position specific scoring matrix based intelligent computational model for identification of mycobacterial membrane proteins",2017,"Journal of Theoretical Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029694086&doi=10.1016%2fj.jtbi.2017.09.013&partnerID=40&md5=42f8fba6b589570fe2c56e7800dcf19a","Mycobacterium is a pathogenic bacterium, which is a causative agent of tuberculosis (TB) and leprosy. These diseases are very crucial and become the cause of death of millions of people every year in the world. So, the characterize structure of membrane proteins of the protozoan play a vital role in the field of drug discovery because, without any knowledge about this Mycobacterium's membrane protein and their types, the scientists are unable to treat this pathogenic protozoan. So, an accurate and competitive computational model is needed to characterize this uncharacterized structure of mycobacterium. Series of attempts were carried out in this connection. Split amino acid compositions, Unbiased-Dipeptide peptide compositions (Unb-DPC), Over-represented tri-peptide compositions, compositions & translation were the few recent encoding techniques followed by different researchers in their publications. Although considerable results have been achieved by these models, still there is a gap which is filled in this study. In this study, an evolutionary feature extraction technique position specific scoring matrix (PSSM) is applied in order to extract evolutionary information from protein sequences. Consequently, 99.6% accuracy was achieved by the learning algorithms. The experimental results demonstrated that the proposed computational model will lead to develop a powerful tool for anti-mycobacterium drugs as well as play a promising rule in proteomic and bioinformatics. © 2017 Elsevier Ltd","Mycobacterium protein; PSSM; SVM","membrane protein; algorithm; amino acid; bacterium; membrane; pattern recognition; protein; amino acid sequence; Article; classification algorithm; jackknife test; k nearest neighbor; learning algorithm; Mycobacterium; nonhuman; position weight matrix; prediction; principal component analysis; priority journal; random forest; sensitivity and specificity; support vector machine; Bacteria (microorganisms); Mycobacterium; Protozoa",2-s2.0-85029694086
"Wang J., Gu Z., Yu Z., Li Y.","An online semi-supervised P300 speller based on extreme learning machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020653452&doi=10.1016%2fj.neucom.2016.12.098&partnerID=40&md5=a4789273221a24d95721a1ce8db6a889","Semi-supervised learning has been applied in brain–computer interfaces (BCIs) to reduce calibration time for user. For example, a sequential updated self-training least squares support vector machine (SUST-LSSVM) was devised for online semi-supervised P300 speller. Despite its good performance, the computational complexity becomes too high after several updates, which hinders its practical online application. In this paper, we present a self-training regularized weighted online sequential extreme learning machine (ST-RWOS-ELM) for P300 speller. It achieves much lower complexity compared to SUST-LSSVM without affecting the spelling accuracy performance. The experimental results validate its effectiveness in the P300 system. © 2017 Elsevier B.V.","Brain–computer interface; Extreme learning machine; Semi-supervised learning","Brain computer interface; Interfaces (computer); Knowledge acquisition; Learning systems; Supervised learning; Brain computer interfaces (BCIs); Calibration time; Extreme learning machine; Least squares support vector machines; Lower complexity; Online sequential extreme learning machine; Practical online; Semi- supervised learning; E-learning; Article; brain computer interface; calibration; computer prediction; female; human; human experiment; male; measurement accuracy; online system; priority journal; self training least square support vector machine; self training regularized weighted online sequential extreme learning machine; sequential analysis; supervised machine learning; support vector machine",2-s2.0-85020653452
"Yang M., Chang H., Luo W., Yang J.","Fisher discrimination dictionary pair learning for image classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020404410&doi=10.1016%2fj.neucom.2016.08.146&partnerID=40&md5=d75b0b7aeeff68538603f2c75f526b12","Dictionary learning has played an important role in the success of sparse representation. Although several dictionary learning approaches have been developed for image classification, discriminative dictionary pair learning, i.e., jointly learning a synthesis dictionary and an analysis dictionary, is still in its infant stage. In this paper, we proposed a novel model of Fisher discrimination dictionary pair learning (FDDPL), in which Fisher discrimination information is embedded into analysis representation, analysis dictionary, and synthesis dictionary representation. With the proposed Fisher-like discrimination term, discrimination of both synthesis dictionary representation and analysis dictionary representation is introduced into the dictionary pair learning model. An iterative algorithm to efficiently solve the proposed FDDPL and a FDDPL based classifier are also presented in this paper. The experiments on face recognition, scene categorization, gender classification, and action recognition clearly show the advantage of the proposed FDDPL. © 2017","Dictionary pair learning; Fisher discrimination; Image classification","Face recognition; Iterative methods; Action recognition; Dictionary learning; Discriminative dictionaries; Fisher discrimination; Gender classification; Iterative algorithm; Scene categorization; Sparse representation; Image classification; Article; classification; controlled study; discrimination learning; facial recognition; Fisher discrimination dictionary pair learning; human; image analysis; image processing; intermethod comparison; learning algorithm; machine learning; mathematical computing; pattern recognition; priority journal; sex difference",2-s2.0-85020404410
"Zheng W., Li M.","The best answer prediction by exploiting heterogeneous data on software development Q&A forum",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020742468&doi=10.1016%2fj.neucom.2016.12.097&partnerID=40&md5=864e598885198faf3e5fb718650cb7b3","Recently, Questions and Answers (Q&A) forum for software development (e.g. Stack Overflow) becomes popular. Identifying the best answer to a raised question is important for Q&A forum since the best answer which provides an excellent solution to the raised question may guide the developers to solve their problems in practice. However, the best answers are often not explicitly tagged by question owners. It would be time-consuming for other developers with the same question to check all candidate answers to find the appropriate one. In this paper, we propose a novel approach to predict the best answers to the questions raised on Stack Overflow by exploiting heterogeneous data sources on the forum. We extract different groups features from multiple data sources and combine them for final prediction via multi-view learning. Experimental results indicate that the proposed method is effective in identifying the best answers to questions raised on Stack Overflow. © 2017 Elsevier B.V.","Best answer recommendation; Multi-view learning consistency; Stack Overflow","Forecasting; Best answer recommendation; Heterogeneous data; Heterogeneous data sources; Multi-view learning; Multiple data sources; Stack overflow; Software design; Article; controlled study; data analysis; data extraction; internal consistency; machine learning; multiview learning; prediction; priority journal; software design",2-s2.0-85020742468
"Zheng W.-S., Gao X., Li Y., Lai J.","Special issue on 2016 International Conference on Intelligence Science and Big Data Engineering",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021072369&doi=10.1016%2fj.neucom.2017.05.079&partnerID=40&md5=2d831da1f2524c4ee2873d8819883e4a",[No abstract available],,"behavioral science; brain computer interface; cluster analysis; conference paper; Conference Paper; connectome; data mining; facial recognition; functional magnetic resonance imaging; image analysis; image processing; intelligence science; learning algorithm; machine learning; priority journal; science; software",2-s2.0-85021072369
"Yan Q., Sun J., Li H., Zhu Y., Zhang Y.","High dynamic range imaging by sparse representation",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020406965&doi=10.1016%2fj.neucom.2017.03.083&partnerID=40&md5=22b0a9bb0bb0646340bc406cb5634243","High dynamic range (HDR) imaging technology is becoming increasingly popular in various applications. A common approach to get an HDR image is the multiple exposed images fusion. However, the phenomenon of ghosting artifacts is brought in for the scene with non-static objects. This paper proposes a ghost-free HDR image synthesis algorithm that utilizes a sparse representation framework. Based on the dependency among adjacent low dynamic range (LDR) images and the sparsity of the moving object that leads to the ghost artifacts, we formulate the problem into two steps: moving object detection and ghost free HDR generation. In the moving object detection step, we formulate the problem as sparse representation due to the sparsity and instantaneous of the moving objects. In the HDR generation step, joint weighting is proposed to generate a ghost-free HDR image from the reference image. Experiments show that the proposed algorithm outperforms the state-of-the-art methods favorably on the textures and colors. © 2017 Elsevier B.V.","Exposures fusion; Ghost removal; High dynamic range imaging; Sparse representation","Imaging techniques; Object recognition; Exposures fusions; Ghost removals; Ghosting artifacts; High dynamic range; High dynamic range imaging; Moving-object detection; Sparse representation; State-of-the-art methods; Object detection; algorithm; Article; artifact; color; digital imaging; high dynamic range imaging; image enhancement; image processing; image quality; machine learning; movement perception; priority journal",2-s2.0-85020406965
"Shan S., Cai D., Deng C., Chang H.","Special issue on selected and extended papers from the 2015 International Conference on Intelligence Science and Big Data Engineering (IScIDE 2015)",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020617514&doi=10.1016%2fj.neucom.2017.05.077&partnerID=40&md5=73f6e6dbeb8e2b54446f324adacbfd6e",[No abstract available],,"automated pattern recognition; Conference Paper; data processing; human; image processing; medical research; movement perception; natural language processing; nuclear magnetic resonance imaging; priority journal; problem solving; publication; quality control; scientific literature; social media; unsupervised machine learning; videorecording",2-s2.0-85020617514
"Jiang K.-M., Chen Y.-J., Lv J.-X., Lu B.-L., Xu L.","Bootstrapping integrative hypothesis test for identifying biomarkers that differentiates lung cancer and chronic obstructive pulmonary disease",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020426382&doi=10.1016%2fj.neucom.2016.10.092&partnerID=40&md5=02002a03f8c6c29e83bbd4d935c38210","Different from the common approaches that use either hypothesis test or classifier for biomarker discovery, we applied the integrative hypothesis test (IHT) that combined both to identifying miRNAs for differentiation between lung cancer and Chronic Obstructive Pulmonary Disease (shortly L-C differentiation) on GEO data set GSE24709, and further extended IHT implementation by bootstrapping aided ranking and mean-variance based reliability check, which outputs a list of the top-15 differentially expressed miRNAs that confirmed the previously reported 14 miRNAs for L-C differentiation from a very different perspective plus an additional one. Moreover, we conducted a literature survey for a further explanation via dividing the 15 miRNAs into subclasses based on known relevances to the two diseases. Also, every pair of 15 miRNAs is exhaustively examined on their joint effect via p-value, misclassification, and correlation, which identifies core pairs and linked cliques as joint miRNAs biomarkers. © 2017 Elsevier B.V.","Bootstrapping; Differential gene expression; Integrative hypothesis test; Rank reliability","Biological organs; Classification (of information); Diseases; Gene expression; Pulmonary diseases; RNA; Statistical tests; Bio-marker discovery; Bootstrapping; Chronic obstructive pulmonary disease; Differential gene expressions; Hypothesis tests; Literature survey; Mean variance; Misclassifications; Biomarkers; biological marker; microRNA; microRNA 1224 3p; microRNA 1299; microRNA 1308; microRNA 130b; microRNA 232 3p; microRNA 26a; microRNA 328; microRNA 369 5p; microRNA 383; microRNA 513b; microRNA 636; microRNA 641; microRNA 662; microRNA 668; microRNA 675; microRNA 767 3p; microRNA 875 3p; microRNA 888; microRNA 93; microRNA 940; microRNA 95; unclassified drug; Article; bootstrapping; chronic obstructive lung disease; controlled study; gene expression; gene identification; genetic analysis; human; lung cancer; machine learning; major clinical study; priority journal; quantitative analysis; reliability; reverse transcription polymerase chain reaction",2-s2.0-85020426382
"Chen C., Lin K.-Y., Wang C.-D., Liu J.-B., Huang D.","CCMS: A nonlinear clustering method based on crowd movement and selection",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020402589&doi=10.1016%2fj.neucom.2016.12.101&partnerID=40&md5=5a556b87d76a1eb5a57464a2b0aee3fb","Nonlinear clustering has attracted an increasing amount of attention recently. In this paper, we propose a new nonlinear clustering method based on crowd movement and selection (CCMS). Different from most of the existing clustering methods that concentrate on cluster centers, our method focuses on the data points themselves and the data distribution of their neighborhood. Based on this novel idea, some useful rules are designed to transform the original nonlinearly separable datasets into linearly separable datasets. Finally, a new clustering phase is designed to partition the transformed data. Extensive experiments have been conducted to evaluate the effectiveness of the proposed CCMS method. © 2017 Elsevier B.V.","Clustering; Crowd movement; Crowd selection; Nonlinear clustering","Cluster analysis; Lattice constants; Cluster centers; Clustering; Clustering methods; Crowd movements; Crowd selection; Data distribution; Linearly separable; Nonlinear clustering; Mathematical transformations; affinity propagation algorithm; Article; attraction vector; clinical effectiveness; cluster analysis; Clustering method based on Crowd Movement and Selection; conscience; density based method; direction vector; hierarchical clustering; kernel based clustering; learning; learning algorithm; mathematical analysis; moving distance; multi exemplar affinity propagation; neighbor vector; nonlinear system; normalized mutual information; position regularized support vector domain description; priority journal; support vector clustering; support vector domain description; support vector machine",2-s2.0-85020402589
"Azad M., Moshkov M.","Multi-stage optimization of decision and inhibitory trees for decision tables with many-valued decisions",2017,"European Journal of Operational Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021200212&doi=10.1016%2fj.ejor.2017.06.026&partnerID=40&md5=a00ea91d5802979d0ff1c92e6b176b00","We study problems of optimization of decision and inhibitory trees for decision tables with many-valued decisions. As cost functions, we consider depth, average depth, number of nodes, and number of terminal/nonterminal nodes in trees. Decision tables with many-valued decisions (multi-label decision tables) are often more accurate models for real-life data sets than usual decision tables with single-valued decisions. Inhibitory trees can sometimes capture more information from decision tables than decision trees. In this paper, we create dynamic programming algorithms for multi-stage optimization of trees relative to a sequence of cost functions. We apply these algorithms to prove the existence of totally optimal (simultaneously optimal relative to a number of cost functions) decision and inhibitory trees for some modified decision tables from the UCI Machine Learning Repository. © 2017 Elsevier B.V.","Decision trees; Dynamic programming; Inhibitory trees; Multiple criteria analysis; Totally optimal trees","Cost functions; Costs; Decision trees; Dynamic programming; Forestry; Optimization; Dynamic programming algorithm; Inhibitory trees; Multi-label; Multi-stage optimization; Multiple criteria analysis; Real life datasets; Totally optimal trees; UCI machine learning repository; Decision tables",2-s2.0-85021200212
"Robinson C., Dilkina B., Hubbs J., Zhang W., Guhathakurta S., Brown M.A., Pendyala R.M.","Machine learning approaches for estimating commercial building energy consumption",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030639721&doi=10.1016%2fj.apenergy.2017.09.060&partnerID=40&md5=c495bdb7225d0636c0ffa7d6c6473996","Building energy consumption makes up 40% of the total energy consumption in the United States. Given that energy consumption in buildings is influenced by aspects of urban form such as density and floor-area-ratios (FAR), understanding the distribution of energy intensities is critical for city planners. This paper presents a novel technique for estimating commercial building energy consumption from a small number of building features by training machine learning models on national data from the Commercial Buildings Energy Consumption Survey (CBECS). Our results show that gradient boosting regression models perform the best at predicting commercial building energy consumption, and can make predictions that are on average within a factor of 2 from the true energy consumption values (with an r2 score of 0.82). We validate our models using the New York City Local Law 84 energy consumption dataset, then apply them to the city of Atlanta to create aggregate energy consumption estimates. In general, the models developed only depend on five commonly accessible building and climate features, and can therefore be applied to diverse metropolitan areas in the United States and to other countries through replication of our methodology. © 2017 Elsevier Ltd","CBECS; Commercial building energy consumption; Machine learning; Modeling","Artificial intelligence; Buildings; Climate models; Energy conservation; Learning systems; Models; Office buildings; Regression analysis; 00-01; 99-00; Building and climates; Building energy consumption; CBECS; Commercial building; Machine learning approaches; Total energy consumption; Energy utilization",2-s2.0-85030639721
"Semaan R.","Optimal sensor placement using machine learning",2017,"Computers and Fluids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030661129&doi=10.1016%2fj.compfluid.2017.10.002&partnerID=40&md5=864d8c955afe34ad2e73c5ceb9fa1936","A new method for optimal sensor placement based on input variables importance of machine learned models is proposed. With its simplicity, adaptivity, and low computational cost, the method offers many advantages over existing approaches. The method is implemented on flow over an airfoil equipped with a Coanda actuator. The analysis is based on flow field data obtained from two-dimensional unsteady Reynolds averaged Navier–Stokes (URANS) simulations with different actuation conditions. The optimal sensor locations are compared against the current de-facto standard of maximum POD modal amplitude location, and against a brute force approach that scans all possible sensor combinations. The results show that both the flow conditions and the type of sensor have an effect on the optimal sensor placement, whereas the choice of the response function appears to have limited influence. © 2017 Elsevier Ltd","Flow control; Machine learning; Optimal sensor placement","Artificial intelligence; Flow control; Reynolds equation; Brute-force approach; Computational costs; De facto standard; Optimal sensor locations; Optimal sensor placement; Response functions; Reynolds averaged; Sensor combinations; Learning systems",2-s2.0-85030661129
"Milošević P., Petrović B., Jeremić V.","IFS-IBA similarity measure in machine learning algorithms",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026648469&doi=10.1016%2fj.eswa.2017.07.048&partnerID=40&md5=1550c5523f7bd9e7d588b0ca2001fd90","The purpose of this paper is to introduce a novel similarity measure of intuitionistic fuzzy sets (IFSs). The proposed measure is based on the equivalence relation in the IFS-IBA approach. Due to the logic-based background, this measure compares IFS from a different viewpoint than the standard measures, emphasizing comprehension of intuitionism. The IFS-IBA similarity measure has a solid mathematical background and can be combined with various IF aggregation operators. Additionally, we define IFS-IBA distance function as a complement of IFS-IBA similarity. Both IFS-IBA similarity and distance functions may have different realizations that are easy to interpret. Hence, the measures are offering great descriptive power and the ability to model various problems. The benefits of the proposed measure are illustrated on the problem of pattern recognition and classification within k-NN algorithm. Finally, we show that the proposed measure is appropriate for IF hierarchical clustering on the problem of clustering Serbian medium-sized companies according to their financial ratios. Results obtained using the IFS-IBA measure are clear-cut and more meaningful compared to a standard IF distances regardless of the I-fuzzification method used. © 2017 Elsevier Ltd","Classification; Clustering; IFS-IBA approach; Interpolative Boolean algebra; Intuitionistic fuzzy sets; Similarity measure","Boolean algebra; Classification (of information); Learning algorithms; Learning systems; Mathematical operators; Nearest neighbor search; Pattern recognition; Clustering; IFS-IBA approach; Interpolative boolean algebras; Intuitionistic fuzzy sets; Similarity measure; Fuzzy sets",2-s2.0-85026648469
"Sebayang A.H., Masjuki H.H., Ong H.C., Dharma S., Silitonga A.S., Kusumo F., Milano J.","Prediction of engine performance and emissions with Manihot glaziovii bioethanol − Gasoline blended using extreme learning machine",2017,"Fuel",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029417416&doi=10.1016%2fj.fuel.2017.08.102&partnerID=40&md5=c3590aab07d2a68136ff5e84fd3ab8a3","Bioethanol can potentially replace gasoline because of its lower exhaust emissions. The purpose of this study was to investigate the engine performance and exhaust emissions of Manihot glaziovii bioethanol–gasoline blends at different blend ratios (5%, 10%, 15%, and 20%). Tests were performed on a single-cylinder, four-stroke spark-ignition engine with engine speed was varied from 1600 to 3400 rpm, and the properties of the Manihot glaziovii bioethanol–gasoline blends were measured and analysed. The vapour pressure increased for fuel blends with low concentrations of bioethanol due to the oxygen within the bioethanol molecules and the contribution of the flame speed which can enhance the combustion and improved the engine efficiency. In addition, the engine torque, brake power, and brake-specific fuel consumption (BSFC) were measured, as well as the carbon monoxide (CO), hydrocarbon (HC), and nitrogen oxide emissions. For a fuel blend containing 20% bioethanol at an engine speed of 3200 rpm, the BSFC decreased, with maximum values of 270.7 g/kWh. The CO and HC emissions were lower for the Manihot glaziovii bioethanol–gasoline blends. In addition, an extreme learning machine (ELM) model was developed for application in the automotive and industrial sectors. This tool reduces the cost, time, and effort associated with experimental data. The blend ratio of the bioethanol–gasoline blends and the engine speed were used as the input data of the model, and the engine performance and exhaust emissions parameters were used as the output data. The coefficient of determination (R2) was within a range of 0.980–1.000, and the mean absolute percentage error was within a range of 0.411%−2.782% for all the parameters. The results indicate that the ELM model is capable of predicting the engine performance and exhaust emissions of bioethanol–gasoline fuel blends. © 2017 Elsevier Ltd","Alternative fuel; Engine performance; Exhaust emission; Extreme learning machine; Manihot glaziovii bioethanol","Alternative fuels; Brakes; Carbon; Carbon monoxide; Diesel engines; Engine cylinders; Engines; Ethanol; Fuel consumption; Fuels; Gasoline; Knowledge acquisition; Learning systems; Nitrogen oxides; Speed; Brake specific fuel consumption; Coefficient of determination; Engine performance; Exhaust emission; Extreme learning machine; Manihot glaziovii; Mean absolute percentage error; Nitrogen oxide emissions; Bioethanol",2-s2.0-85029417416
"Zheng W., Gao X., Liu Y., Wang L., Yang J., Gao Z.","Industrial Mooney viscosity prediction using fast semi-supervised empirical model",2017,"Chemometrics and Intelligent Laboratory Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032431887&doi=10.1016%2fj.chemolab.2017.10.009&partnerID=40&md5=03efe4af272d58e3f44870a2b44d5e0e","In industrial rubber mixing processes, the quality index (i.e., Mooney viscosity) cannot be online measured directly. Traditional data-driven empirical models for online prediction of the Mooney viscosity have not utilized the information hidden in lots of unlabeled data (e.g., process input variables during each mixing batch). A simple semi-supervised nonlinear soft sensor method for the Mooney viscosity prediction is developed. It integrates extreme learning machine (ELM) and the graph Laplacian regularization into a unified modeling framework. The useful information in unlabeled data can be explored and introduced into the prediction model. Furthermore, a bagging-based ensemble strategy is combined into semi-supervised ELM (SELM) to obtain more accurate predictions. The Mooney viscosity prediction in an industrial internal mixer exhibits its promising prediction performance of the proposed method by incorporating the information in unlabeled data efficiently. © 2017 Elsevier B.V.","Extreme learning machine; Mooney viscosity; Rubber mixing process; Semi-supervised model; Soft sensor","Article; calculation; data analysis; empiricism; industry; machine learning; mathematical computing; mathematical model; Mooney viscosity; prediction; priority journal; process development; viscosity",2-s2.0-85032431887
"Garciarena U., Santana R.","An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025602749&doi=10.1016%2fj.eswa.2017.07.026&partnerID=40&md5=15deff76ab7e64d1fcc6be9266e20583","When applying data-mining techniques to real-world data, we often find ourselves facing observations that have no value recorded for some attributes. This can be caused by several phenomena, such as a machine's incapability to record certain characteristics or a person refusing to answer a question in a poll. Depending on that motivation, values gone missing may follow one kind of pattern or another, or describe no regularity at all. One approach to palliate the effect of missing data on machine learning tasks is to replace the missing observations. Imputation algorithms attempt to calculate a value for a missing gap, using information associated with it, i.e., the attribute and/or other values in the same observation. While several imputation methods have been proposed in the literature, few works have addressed the question of the relationship between the type of missing data, the choice of the imputation method, and the effectiveness of classification algorithms that used the imputed data. In this paper we address the relationship among these three factors. By constructing a benchmark of hundreds of databases containing different types of missing data, and applying several imputation methods and classification algorithms, we empirically show that an interaction between imputation methods and supervised classification can be deduced. Besides, differences in terms of classification performance for the same imputation method in different missing data patterns have been found. This points to the convenience of considering the combined choice of the imputation method and the classifier algorithm according to the missing data type. © 2017 Elsevier Ltd","Imputation methods; Machine learning; Missing data; Supervised classifiers","Artificial intelligence; Data mining; Education; Learning systems; Supervised learning; Classification algorithm; Classification performance; Classifier algorithms; Imputation algorithm; Imputation methods; Missing data; Supervised classification; Supervised classifiers; Classification (of information)",2-s2.0-85025602749
"Adnan M.N., Islam M.Z.","Forest PA: Constructing a decision forest by penalizing attributes used in previous trees",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026815936&doi=10.1016%2fj.eswa.2017.08.002&partnerID=40&md5=5110ebdc4ef86900e16f7c3660c473f1","In this paper, we propose a new decision forest algorithm that builds a set of highly accurate decision trees by exploiting the strength of all non-class attributes available in a data set, unlike some existing algorithms that use a subset of the non-class attributes. At the same time to promote strong diversity, the proposed algorithm imposes penalties (disadvantageous weights) to those attributes that participated in the latest tree in order to generate the subsequent trees. Besides, some other weight-related concerns are taken into account so that the trees generated by the proposed algorithm remain individually accurate and retain strong diversity. In order to show the worthiness of the proposed algorithm, we carry out experiments on 20 well known data sets that are publicly available from the UCI Machine Learning Repository. The experimental results indicate that the proposed algorithm is effective in generating highly accurate and more balanced decision forests compared to other prominent decision forest algorithms. Accordingly, the proposed algorithm is expected to be very effective in the domain of expert and intelligent systems. © 2017 Elsevier Ltd","Classification; Decision Forest; Decision Tree; Ensemble Accuracy; Random Forest","Classification (of information); Decision trees; Intelligent systems; Learning systems; Data set; Decision forest; Ensemble Accuracy; Highly accurate; Random forests; UCI machine learning repository; Learning algorithms",2-s2.0-85026815936
"Chatzakou D., Vakali A., Kafetsios K.","Detecting variation of emotions in online activities",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026741003&doi=10.1016%2fj.eswa.2017.07.044&partnerID=40&md5=a569e26987ecb355d859e86872f1da72","Online text sources form evolving large scale data repositories out of which valuable knowledge about human emotions can be derived. Beyond the primary emotions which refer to the global emotional signals, deeper understanding of a wider spectrum of emotions is important to detect online public views and attitudes. The present work is motivated by the need to test and provide a system that categorizes emotion in online activities. Such a system can be beneficial for online services, companies recommendations, and social support communities. The main contributions of this work are to: (a) detect primary emotions, social ones, and those that characterize general affective states from online text sources, (b) compare and validate different emotional analysis processes to highlight those that are most efficient, and (c) provide a proof of concept case study to monitor and validate online activity, both explicitly and implicitly. The proposed approaches are tested on three datasets collected from different sources, i.e., news agencies, Twitter, and Facebook, and on different languages, i.e., English and Greek. Study results demonstrate that the methodologies at hand succeed to detect a wider spectrum of emotions out of text sources. © 2017 Elsevier Ltd","Emotion detection; Hybrid process; Lexicon-based approach; Machine learning","Learning systems; Affective state; Emotion detection; Emotional analysis; Hybrid process; Large scale data; Lexicon-based; Online activities; Proof of concept; Social networking (online)",2-s2.0-85026741003
"Albuquerque M.T.D., Gerassis S., Sierra C., Taboada J., Martín J.E., Antunes I.M.H.R., Gallego J.R.","Developing a new Bayesian Risk Index for risk evaluation of soil contamination",2017,"Science of the Total Environment",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020830356&doi=10.1016%2fj.scitotenv.2017.06.068&partnerID=40&md5=02c65595505fd086cfc09f8a3cbf83d1","Industrial and agricultural activities heavily constrain soil quality. Potentially Toxic Elements (PTEs) are a threat to public health and the environment alike. In this regard, the identification of areas that require remediation is crucial. In the herein research a geochemical dataset (230 samples) comprising 14 elements (Cu, Pb, Zn, Ag, Ni, Mn, Fe, As, Cd, V, Cr, Ti, Al and S) was gathered throughout eight different zones distinguished by their main activity, namely, recreational, agriculture/livestock and heavy industry in the Avilés Estuary (North of Spain). Then a stratified systematic sampling method was used at short, medium, and long distances from each zone to obtain a representative picture of the total variability of the selected attributes. The information was then combined in four risk classes (Low, Moderate, High, Remediation) following reference values from several sediment quality guidelines (SQGs). A Bayesian analysis, inferred for each zone, allowed the characterization of PTEs correlations, the unsupervised learning network technique proving to be the best fit. Based on the Bayesian network structure obtained, Pb, As and Mn were selected as key contamination parameters. For these 3 elements, the conditional probability obtained was allocated to each observed point, and a simple, direct index (Bayesian Risk Index-BRI) was constructed as a linear rating of the pre-defined risk classes weighted by the previously obtained probability. Finally, the BRI underwent geostatistical modeling. One hundred Sequential Gaussian Simulations (SGS) were computed. The Mean Image and the Standard Deviation maps were obtained, allowing the definition of High/Low risk clusters (Local G clustering) and the computation of spatial uncertainty. High-risk clusters are mainly distributed within the area with the highest altitude (agriculture/livestock) showing an associated low spatial uncertainty, clearly indicating the need for remediation. Atmospheric emissions, mainly derived from the metallurgical industry, contribute to soil contamination by PTEs. © 2017 Elsevier B.V.","Bayesian networks; Local G clustering; Potentially toxic elements; Sequential Gaussian simulation","Agriculture; Bayesian networks; Health risks; Manganese; Metallurgy; Pollution; Public health; Risks; Soils; Titanium; Agricultural activities; Bayesian network structure; Conditional probabilities; Geostatistical modeling; Local G clustering; Potentially toxic elements; Sediment quality guideline (SQGs); Sequential Gaussian simulation; Soil pollution; aluminum; arsenic; cadmium; chromium; copper; iron; lead; manganese; nickel; silver; sulfur; titanium; vanadium; zinc; agricultural production; Bayesian analysis; human activity; industrial production; soil pollution; soil quality; soil remediation; toxicity; uncertainty analysis; agriculture; air pollution; Article; Bayesian Risk Index; concentration (parameters); ecotoxicity; geostatistical analysis; livestock; metallurgy; priority journal; recreation; risk assessment; soil pollution; statistical model; uncertainty; unsupervised machine learning",2-s2.0-85020830356
"Dong J., Wang N.-N., Liu K.-Y., Zhu M.-F., Yun Y.-H., Zeng W.-B., Chen A.F., Cao D.-S.","ChemBCPP: A freely available web server for calculating commonly used physicochemical properties",2017,"Chemometrics and Intelligent Laboratory Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031763774&doi=10.1016%2fj.chemolab.2017.10.006&partnerID=40&md5=65fef6f1fe44dd4eb5340c7307c8745b","The behavior of a chemical in human or environment mostly depends on its several key physicochemical properties, such as aqueous solubility, octanol-water partition coefficient (logP), boiling point (BP), density, flash point (FP), viscosity, surface tension (ST), vapor pressure (VP) and melting point (MP). Commonly, these properties are important for the environmental sciences and drug discovery, such as the absorption, distribution, metabolism, excretion, and toxicity (ADMET) for medicinal compounds and the common risk assessment for problematic chemicals. At present, the quantitative structure-property relationship (QSPR) model was widely applied to save time and money investment in the early stage of chemical research. Although some satisfactory models were already obtained, most of them are not available for the public researchers and thus cannot be directly applied to practical research projects. Herein, in this study, we developed a user-friendly web server named ChemBCPP that can be used to predict aforementioned 8 important physicochemical properties and calculate several other commonly used properties just by uploading a molecular structure or file. In addition, for a new chemical entity, users can not only get its predicted value but also obtain a leverage value (h value) which can be used to evaluate the reliability of predictive result. We believe that ChemBCPP could be widely applied in environmental science, chemical synthesis and drug ADMET fields with the demand for high quality of chemical properties. ChemBCPP could be freely available via http://chembcpp.scbdd.com. © 2017","Cheminformatics; Machine learning; Physiochemical property prediction; QSPR; Web server","octanol; water; aqueous solution; Article; boiling point; chemical structure; cheminformatics; controlled study; density; information science; machine learning; melting point; physical chemistry; priority journal; quantitative structure property relation; software; solubility; surface tension; synthesis; temperature; vapor pressure; viscosity; web server",2-s2.0-85031763774
"Kardas K., Cicekli N.K.","SVAS: Surveillance Video Analysis System",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026764328&doi=10.1016%2fj.eswa.2017.07.051&partnerID=40&md5=939512d863628d224f8ae8bb5757bff5","This paper introduces a Surveillance Video Analysis System, called SVAS, for surveillance domain, in which the semantic rules and the definition of event models can be learned or defined by the user for automatic detection and inference of complex video events. In the scope of SVAS, an event model method named Interval-Based Spatio-Temporal Model (IBSTM) is proposed. SVAS can learn action models and event models without any predefined threshold values and generates understandable and manageable IBSTM event models. Hybrid machine learning methods are proposed and used. A set of feature models named Threshold Model, which reflects the spatio-temporal motion analysis of an event, is kept as the first model. As the second model, Bag of Actions (BoA) model is used in order to reduce the search space in the detection phase. Markov Logic Network (MLN) model, which provides understandable and manageable logic predicates for users, is kept as the third model. SVAS has high performance event detection capability due to its interval-based hierarchical manner. It determines related candidate intervals for each main model of IBSTM and uses the related main model when needed rather than using all models as a whole. The main contribution of this study is to fill the semantic gap between humans and video computer systems such that, on the one hand it decreases human intervention through its learning capabilities, but on the other hand it also enables human intervention when necessary through its manageable event model method. The study achieves all of them in the most efficient way through its machine learning methods. The proposed system is applied to different event datasets from CAVIAR, BEHAVE and our synthetic datasets. The experimental results show that our approach improves the event recognition performance and precision as compared to the current state-of-the-art approaches. © 2017 Elsevier Ltd","Event detection; Event inference; Event model learning; Markov logic networks; Video surveillance","Artificial intelligence; Computer circuits; Learning systems; Markov processes; Monitoring; Probabilistic logics; Semantics; Event detection; Event inference; Event model; Markov logic networks; Video surveillance; Security systems",2-s2.0-85026764328
"Mazzutti T., Roisenberg M., de Freitas Filho P.J.","INFGMN – Incremental Neuro-Fuzzy Gaussian mixture network",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026429533&doi=10.1016%2fj.eswa.2017.07.032&partnerID=40&md5=b21326a24bfac2296b3729d9ffb0274a","Accuracy and interpretability are contradictory objectives that conflict in all machine learning techniques and achieving a satisfactory balance between these two criteria is a major challenge. The objective is not only to maximize interpretability, but also to guarantee a high degree of accuracy. This challenge is even greater when it is considered that the model will have to evolve and adapt itself to the dynamics of the underlying environment, i.e. it will have to learn incrementally. Little research has been published about incremental learning using Mamdani–Larsen (ML) fuzzy models under these conditions. This article presents a novel proposal for a Neuro-Fuzzy System (NFS) with an incremental learning capability, the Incremental Neuro-Fuzzy Gaussian Mixture Network (INFGMN), that attempts to generate incremental models that are highly interpretable and precise. The principal characteristics of the INFGMN are as follows: (i) the INFGMN learns incrementally using a single sweep of the training data (each training pattern can be immediately used and discarded); (ii) it is capable of producing reasonable estimates based on few training data; (iii) the learning process can proceed in perpetuity as new training data become available (learning and recalling phases are not separate); (iv) the INFGMN can deal with the Stability-Plasticity dilemma and is unaffected by catastrophic interference (rules are added or removed whenever necessary); (v) the fuzzy rule base is defined automatically and incrementally (new rules are added whenever necessary); and (vi) the INFGMN maintains an ML-type fuzzy rule base that attempts to provide the best trade-off between accuracy and interpretability, thereby dealing with the Accuracy-Interpretability dilemma. The INFGMNs performance in terms of learning and modelling is assessed using a variety of benchmark applications and the results are promising. © 2017 Elsevier Ltd","Accuracy-Interpretability dilemma; Incremental learning; Mamdani–Larsen-type fuzzy; Neuro-Fuzzy system; Stability-Plasticity dilemma","Benchmarking; Education; Fuzzy neural networks; Fuzzy rules; Fuzzy systems; Knowledge engineering; Learning systems; Incremental learning; Interpretability; Lars-en; Neurofuzzy system; Stability-plasticity dilemma; Fuzzy inference",2-s2.0-85026429533
"Gallego A.-J., Calvo-Zaragoza J.","Staff-line removal with selectional auto-encoders",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026254841&doi=10.1016%2fj.eswa.2017.07.002&partnerID=40&md5=aa826a4d3f4fedce79326a74a3cd8e5c","Staff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images. © 2017 Elsevier Ltd","Auto-encoders; Convolutional networks; Optical music recognition; Staff-line removal","Image processing; Optical data processing; Auto encoders; Convolutional networks; Image processing technique; Input features; Line removal; Musical symbols; New approaches; Optical music recognition; Learning systems",2-s2.0-85026254841
"Shiju S.S., Sumitra S.","Multiple kernel learning using single stage function approximation for binary classification problems",2017,"International Journal of Systems Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708295&doi=10.1080%2f00207721.2017.1381892&partnerID=40&md5=add9d04b80e07a87c8a13106b1001474","In this paper, the multiple kernel learning (MKL) is formulated as a supervised classification problem. We dealt with binary classification data and hence the data modelling problem involves the computation of two decision boundaries of which one related with that of kernel learning and the other with that of input data. In our approach, they are found with the aid of a single cost function by constructing a global reproducing kernel Hilbert space (RKHS) as the direct sum of the RKHSs corresponding to the decision boundaries of kernel learning and input data and searching that function from the global RKHS, which can be represented as the direct sum of the decision boundaries under consideration. In our experimental analysis, the proposed model had shown superior performance in comparison with that of existing two stage function approximation formulation of MKL, where the decision functions of kernel learning and input data are found separately using two different cost functions. This is due to the fact that single stage representation helps the knowledge transfer between the computation procedures for finding the decision boundaries of kernel learning and input data, which inturn boosts the generalisation capacity of the model. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","classification; function approximation; Multiple kernel learning; online learning; support vector machine","Bins; Cost benefit analysis; Cost functions; Input output programs; Knowledge management; Support vector machines; Binary classification; Binary classification problems; Experimental analysis; Function approximation; Multiple Kernel Learning; Online learning; Reproducing Kernel Hilbert spaces; Supervised classification; Classification (of information)",2-s2.0-85030708295
"de Ávila M.B., Xavier M.M., Pintro V.O., de Azevedo W.F., Jr.","Supervised machine learning techniques to predict binding affinity. A study for cyclin-dependent kinase 2",2017,"Biochemical and Biophysical Research Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031327838&doi=10.1016%2fj.bbrc.2017.10.035&partnerID=40&md5=861d7ab44ece15c751feaa8967cea088","Here we report the development of a machine-learning model to predict binding affinity based on the crystallographic structures of protein-ligand complexes. We used an ensemble of crystallographic structures (resolution better than 1.5 Å resolution) for which half-maximal inhibitory concentration (IC50) data is available. Polynomial scoring functions were built using as explanatory variables the energy terms present in the MolDock and PLANTS scoring functions. Prediction performance was tested and the supervised machine learning models showed improvement in the prediction power, when compared with PLANTS and MolDock scoring functions. In addition, the machine-learning model was applied to predict binding affinity of CDK2, which showed a better performance when compared with AutoDock4, AutoDock Vina, MolDock, and PLANTS scores. © 2017 Elsevier Inc.","Bioinformatics; CDK2; Docking; Drug design; Kinase; Machine learning","cyclin dependent kinase 2; Article; binding affinity; crystal structure; crystallography; IC50; intermethod comparison; mathematical analysis; molecular docking; prediction; priority journal; scoring system; supervised machine learning",2-s2.0-85031327838
"Wang M., Chen H., Yang B., Zhao X., Hu L., Cai Z., Huang H., Tong C.","Toward an optimal kernel extreme learning machine using a chaotic moth-flame optimization strategy with applications in medical diagnoses",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019950299&doi=10.1016%2fj.neucom.2017.04.060&partnerID=40&md5=dc915e0ac380f5972c46e6743dda132b","This study proposes a novel learning scheme for the kernel extreme learning machine (KELM) based on the chaotic moth-flame optimization (CMFO) strategy. In the proposed scheme, CMFO simultaneously performs parameter optimization and feature selection. The proposed methodology is rigorously compared to several other competitive KELM models that are based on the original moth-flame optimization, particle swarm optimization, and genetic algorithms. The comparison is made using the medical diagnosis problems of Parkinson's disease and breast cancer. And the proposed method has successfully been applied to practical medical diagnosis cases. The experimental results demonstrate that, compared to the alternative methods, the proposed method offers significantly better classification performance and also obtains a smaller feature subset. Promisingly, the proposed CMFOFS-KELM, can serve as an effective and efficient computer aided tool for medical diagnosis in the field of medical decision making. © 2017 Elsevier B.V.","Feature selection; Improved moth-flame optimization; Kernel extreme learning machine; Medical diagnosis; Parameter optimization","Computer aided diagnosis; Decision making; Feature extraction; Genetic algorithms; Knowledge acquisition; Learning systems; Medical problems; Neural networks; Particle swarm optimization (PSO); Classification performance; Computer aided tools; Diagnosis problem; Extreme learning machine; Medical decision making; Optimization strategy; Parameter optimization; Parkinson's disease; Diagnosis; alanine; alanine aminotransferase; albumin; arabinitol; aspartate aminotransferase; butyric acid; creatinine; glucose; glutamine; glycerol; inositol; leucine; linoleic acid; phenylalanine; phosphate; piperazine; proline; purine; sorbitol; threonine; triacylglycerol; tyrosine; uric acid; valine; xylitol; area under the curve; Article; artificial neural network; breast cancer; chaotic moth-flame optimization; classifier; computer aided design; extreme learning machine; genetic algorithm; learning algorithm; machine learning; mass fragmentography; medical decision making; nonhuman; optimal kernel extreme learning machine; Parkinson disease; priority journal; process optimization; radial basis function; sensitivity and specificity; support vector machine",2-s2.0-85019950299
"Lee E.Y., Lee M.W., Fulan B.M., Ferguson A.L., Wong G.C.L.","What can machine learning do for antimicrobial peptides, and what can antimicrobial peptides do for machine learning?",2017,"Interface Focus",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032030361&doi=10.1098%2frsfs.2016.0153&partnerID=40&md5=d97b08f2c287dbaba03780d7c6ae89b4","Antimicrobial peptides (AMPs) are a diverse class of well-studied membranepermeating peptides with important functions in innate host defense. In this short review, we provide a historical overview of AMPs, summarize previous applications of machine learning to AMPs, and discuss the results of our studies in the context of the latest AMP literature. Much work has been recently done in leveraging computational tools to design new AMP candidates with high therapeutic efficacies for drug-resistant infections. We show that machine learning on AMPs can be used to identify essential physicochemical determinants of AMP functionality, and identify and design peptide sequences to generate membrane curvature. In a broader scope, we discuss the implications of our findings for the discovery of membrane-active peptides in general, and uncovering membrane activity in new and existing peptide taxonomies. © 2017 The Author(s) Published by the Royal Society. All rights reserved.","Amphiphilic peptides; Antimicrobial peptides; Machine learning; Membrane curvature",,2-s2.0-85032030361
"Salaken S.M., Khosravi A., Nguyen T., Nahavandi S.","Extreme learning machine based transfer learning algorithms: A survey",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022083603&doi=10.1016%2fj.neucom.2017.06.037&partnerID=40&md5=96065ac032abd5a45fcb72c09c8dbcd9","Extreme learning machine (ELM) has been increasingly popular in the field of transfer learning (TL) due to its simplicity, training speed and ease of use in online sequential learning process. This paper critically examines transfer learning algorithms formulated with ELM technique and provides state of the art knowledge to expedite the learning process ELM based TL algorithms. As this article discusses available ELM based TL algorithm in detail, it provides a holistic overview of current literature, serves as a starting point for new researchers in ELM based TL algorithms and facilitates identification of future research direction in concise manner. © 2017 Elsevier B.V.","Extreme learning machine; Transfer learning","Education; Knowledge acquisition; Learning systems; Ease-of-use; Extreme learning machine; Future research directions; Learning process; Sequential learning; State of the art; Training speed; Transfer learning; Learning algorithms; Article; benchmarking; extreme learning machine; information processing; kernel method; learning algorithm; online system; priority journal; process optimization; remote sensing; transfer learning algorithm",2-s2.0-85022083603
"Chan P.P.K., Lin Z., Hu X., Tsang E.C.C., Yeung D.S.","Sensitivity based robust learning for stacked autoencoder against evasion attack",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021955741&doi=10.1016%2fj.neucom.2017.06.032&partnerID=40&md5=689d9ae9da10bb6a4b9945cf91c1458f","Although deep learning has achieved excellent performance in many applications, some studies indicate that deep learning algorithms are vulnerable in an adversarial environment. A small distortion on a sample leads to misclassification easily. Until now, the vulnerability issue of stacked autoencoder, which is one of the most popular deep learning algorithms, has not been investigated. In this paper, we firstly investigate the existing evasion attack to stacked autoencoder in an effort to understand whether, and to what extent, they can work efficiently. A robust learning algorithm which minimizes both its error and sensitivity is then proposed for stacked autoencoder. The sensitivity is defined as the change of the output due to a small fluctuation on the input. As the proposed algorithm considers not only accuracy but also stability, a more robust stacked autoencoder against evasion attack is expected. The performance of our methods is then evaluated and compared with conventional stacked autoencoder and denoising autoencoder experimentally in terms of accuracy, robustness and time complexity. Moreover, the experimental results also suggest that the proposed learning method is more robust than others when a training set is contaminated. © 2017 Elsevier B.V.","Adversarial learning; Deep learning; Evasion attack; Robustness; Sensitivity","Deep learning; Education; Learning systems; Robustness (control systems); Adversarial environments; Adversarial learning; Evasion attack; Learning methods; Misclassifications; Robust learning algorithm; Sensitivity; Small fluctuation; Learning algorithms; Article; autoencoder; classifier; computer security; data processing; learning algorithm; machine learning; priority journal",2-s2.0-85021955741
"Kang Z., Peng C., Cheng Q.","Kernel-driven similarity learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020776356&doi=10.1016%2fj.neucom.2017.06.005&partnerID=40&md5=11ac5f93771154c23589f19026180f17","Similarity measure is fundamental to many machine learning and data mining algorithms. Predefined similarity metrics are often data-dependent and sensitive to noise. Recently, data-driven approach which learns similarity information from data has drawn significant attention. The idea is to represent a data point by a linear combination of all (other) data points. However, it is often the case that more complex relationships beyond linear dependencies exist in the data. Based on the well known fact that kernel trick can capture the nonlinear structure information, we extend this idea to kernel spaces. Nevertheless, such an extension brings up another issue: its algorithm performance is largely determined by the choice of kernel, which is often unknown in advance. Therefore, we further propose a multiple kernel-based learning method. By doing so, our model can learn both linear and nonlinear similarity information, and automatically choose the most suitable kernel. As a result, our model is capable of learning complete similarity information hidden in data set. Comprehensive experimental evaluations of our algorithms on clustering and recommender systems demonstrate its superior performance compared to other state-of-the-art methods. This performance also shows the great potential of our proposed algorithm for other possible applications. © 2017 Elsevier B.V.","Clustering; Kernel method; Multiple kernel learning; Nonlinear relation; Recommender systems; Similarity measure; Sparse representation","Data mining; Learning systems; Recommender systems; Clustering; Kernel methods; Multiple Kernel Learning; Nonlinear relations; Similarity measure; Sparse representation; Clustering algorithms; algorithm; Article; cluster analysis; controlled study; kernel method; linear system; machine learning; nonlinear system; priority journal; similarity learning",2-s2.0-85020776356
"Xu Z., Skorheim S., Tu M., Berisha V., Yu S., Seo J.-S., Bazhenov M., Cao Y.","Improving efficiency in sparse learning with the feedforward inhibitory motif",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019640545&doi=10.1016%2fj.neucom.2017.05.016&partnerID=40&md5=687bc85699552f3ee828db9795df2544","Neuro-inspired computing has made significant progress in recent years. However, its computation efficiency and hardware cost still lag behind the biological nervous system, especially during the training stage. This work targets to understand this gap from a neural motif perspective, particularly the feedforward inhibitory motif. Such a motif has been found in many cortical systems, presenting a vital role in sparse learning. This work first establishes a neural network model that emulates the insect's olfactory system, and then systematically studies various effects of the feedforward inhibitory motif. The performance and efficiency of the neural network models are evaluated through the handwritten digits recognition task, with and without the feedforward inhibitory motif. As demonstrated in the results, the utilization of the feedforward inhibitory motif is able to reduce the network size by > 3X at the same accuracy of 95% in handwritten digits recognition. Further simulation experiments reveal that the feedforward inhibition not only dynamically regulates the firing rate of excitatory neurons, promotes and stabilizes the sparsity, but also provides a coarse categorization of the inputs, which improves the final accuracy with a smaller, cascade structure. These results differentiate the feedforward inhibition path from previous understanding of the feedback inhibition, illustrating its functional importance for high computation and structure efficiency. © 2017 Elsevier B.V.","Feedforward inhibition; Handwritten recognition; Hebbian learning; Neural motif; Sparse learning; Spiking neural network","Character recognition; Efficiency; Neural networks; Feedforward inhibition; Handwritten recognition; Hebbian learning; Neural motif; Sparse learning; Spiking neural networks; Computational efficiency; Article; artificial neural network; firing rate; insect; machine learning; mathematical model; measurement accuracy; olfactory system; positive feedback; prediction; priority journal; simulation; sparse learning; synapse",2-s2.0-85019640545
"Luo S., Zhu L., Althoefer K., Liu H.","Knock-Knock: Acoustic object recognition by using stacked denoising autoencoders",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017121577&doi=10.1016%2fj.neucom.2017.03.014&partnerID=40&md5=fe0df667aa151966cb7eac6142b95fa2","This paper presents a successful application of deep learning for object recognition based on acoustic data. The shortcomings of previously employed approaches where handcrafted features describing the acoustic data are being used, include limiting the capability of the found representation to be widely applicable and facing the risk of capturing only insignificant characteristics for a task. In contrast, there is no need to define the feature representation format when using multilayer/deep learning architecture methods: features can be learned from raw sensor data without defining discriminative characteristics a-priori. In this paper, stacked denoising autoencoders are applied to train a deep learning model. Knocking each object in our test set 120 times with a marker pen to obtain the auditory data, thirty different objects were successfully classified in our experiment and each object was knocked 120 times by a marker pen to obtain the auditory data. By employing the proposed deep learning framework, a high accuracy of 91.50% was achieved. A traditional method using handcrafted features with a shallow classifier was taken as a benchmark and the attained recognition rate was only 58.22%. Interestingly, a recognition rate of 82.00% was achieved when using a shallow classifier with raw acoustic data as input. In addition, we could show that the time taken to classify one object using deep learning was far less (by a factor of more than 6) than utilizing the traditional method. It was also explored how different model parameters in our deep architecture affect the recognition performance. © 2017 Elsevier B.V.","Acoustic data analysis; Deep networks; Object recognition","Deep learning; Learning systems; Network architecture; Object recognition; Acoustic data; Deep architectures; Feature representation; High-accuracy; Learning architectures; Learning frameworks; Learning models; Model parameters; Classification (of information); acoustics; Article; automated pattern recognition; classifier; deep learning; intermethod comparison; machine learning; priority journal; sound; stacked denoising autoencoder; support vector machine",2-s2.0-85017121577
"Yang M., Liu Y., You Z.","The Euclidean embedding learning based on convolutional neural network for stereo matching",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020801527&doi=10.1016%2fj.neucom.2017.06.007&partnerID=40&md5=a8fa8622ca9ae8e2606cd0685c976a71","Stereo matching is one of the most important and fundamental topics in computer vision. The calculation of matching cost plays a very important role for stereo matching algorithms. The stereo matching algorithm proposed by Zbontar and LeCun focusing on the training of the matching cost has showed the good performance of the convolutional neural network. Unfortunately, computing a convolutional neural network for matching cost is computationally very expensive. This paper proposes a method based on learning a Euclidean embedding using a convolutional neural network with a triplet-based loss function, where the matching cost is directly computed by the squared L2 distances between two vectors in the embedding space. The cost is refined by Semiglobal Matching with an adaptive smoothness constraint based on multi-scale segmentations. The proposed method has a comparable performance with the state-of-the-art algorithms, and it overcomes a problem of heavy computation. The proposed method takes only about 5 s for predicting a single image pair, where the computing of convolutional neural networks needs less than 2 s with CPU, that is much faster than the algorithm by Zbontar and LeCun where the computing of convolutional neural network takes 67 s with GPU. © 2017 Elsevier B.V.","Convolutional neural network; Semiglobal Matching; Stereo matching","Convolution; Costs; Neural networks; Stereo vision; Vector spaces; Convolutional neural network; Euclidean embedding; Multiscale segmentation; Semi-global matching; Smoothness constraints; State-of-the-art algorithms; Stereo matching; Stereo matching algorithm; Stereo image processing; algorithm; Article; artificial neural network; computer graphics; computer vision; convolutional neural network; Euclidean embedding learning; image processing; machine learning; mathematical computing; priority journal; stereo matching",2-s2.0-85020801527
"Lu Y., Yi S., Zeng N., Liu Y., Zhang Y.","Identification of rice diseases using deep convolutional neural networks",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021853536&doi=10.1016%2fj.neucom.2017.06.023&partnerID=40&md5=795a293f86605df2cfe256d3a7837492","The automatic identification and diagnosis of rice diseases are highly desired in the field of agricultural information. Deep learning is a hot research topic in pattern recognition and machine learning at present, it can effectively solve these problems in vegetable pathology. In this study, we propose a novel rice diseases identification method based on deep convolutional neural networks (CNNs) techniques. Using a dataset of 500 natural images of diseased and healthy rice leaves and stems captured from rice experimental field, CNNs are trained to identify 10 common rice diseases. Under the 10-fold cross-validation strategy, the proposed CNNs-based model achieves an accuracy of 95.48%. This accuracy is much higher than conventional machine learning model. The simulation results for the identification of rice diseases show the feasibility and effectiveness of the proposed method. © 2017 Elsevier B.V.","Convolutional neural networks; Deep learning; Identification of rice diseases; Image recognition","Agricultural machinery; Artificial intelligence; Automation; Convolution; Deep neural networks; Diagnosis; Education; Image recognition; Learning systems; Neural networks; Pattern recognition; 10-fold cross-validation; Agricultural informations; Automatic identification; Conventional machines; Convolutional neural network; Hot research topics; Identification method; Natural images; Deep learning; Article; artificial neural network; controlled study; convolutional neural network; machine learning; measurement accuracy; nonhuman; plant disease; plant leaf; plant stem; priority journal; rice; simulation",2-s2.0-85021853536
"Qi T., Xu Y., Quan Y., Wang Y., Ling H.","Image-based action recognition using hint-enhanced deep neural networks",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026394713&doi=10.1016%2fj.neucom.2017.06.041&partnerID=40&md5=acf6b85af2f7495e40e3387364b499bb","While human action recognition from still images finds wide applications in computer vision, it remains a very challenging problem. Compared with video-based ones, image-based action representation and recognition are impossible to access the motion cues of action, which largely increases the difficulties in dealing with pose variances and cluttered backgrounds. Motivated by the recent success of convolutional neural networks (CNN) in learning discriminative features from objects in the presence of variations and backgrounds, in this paper, we investigate the potentials of CNN in image-based action recognition. A new action recognition method is proposed by implicitly integrating pose hints into the CNN framework, i.e., we use a CNN originally learned for object recognition as a base network and then transfer it to action recognition by training the base network jointly with inference of poses. Such a joint training scheme can guide the network towards pose inference and meanwhile prevent the unrelated knowledge inherited from the base network. For further performance improvement, the training data is augmented by enriching the pose-related samples. The experimental results on three benchmark datasets have demonstrated the effectiveness of our method. © 2017","Action recognition; Convolutional neural networks; Pose hints","Convolution; Neural networks; Object recognition; Action recognition; Action representations; Benchmark datasets; Cluttered backgrounds; Convolutional neural network; Discriminative features; Human-action recognition; Pose hints; Deep neural networks; accuracy; algorithm; Article; calculation; convolutional neural network; illumination; image analysis; image quality; linear system; machine learning; mathematical analysis; mathematical model; nonlinear system; priority journal; support vector machine",2-s2.0-85026394713
"Liu Y., Zhang X., Zhu X., Guan Q., Zhao X.","ListNet-based object proposals ranking",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020936262&doi=10.1016%2fj.neucom.2017.06.008&partnerID=40&md5=dc2d67d59dca289e6988229ae004f8f0","In object detection, object proposal methods have been widely used to generate candidate regions which may contain objects. Object proposal based on superpixel merging is one kind of object proposal methods, and the merging strategies of superpixels have been extensively explored. However, the ranking of generated candidate proposals still remains to be further studied. In this paper, we formulate the ranking of object proposals as a learning to rank problem, and propose a novel object proposals ranking method based on ListNet. In the proposed method, Selective Search, which is one of the state-of-the-art object proposal methods based on superpixel merging, is adopted to generate the candidate proposals. During the superpixel merging process, five discriminative objectness features are extracted from superpixel sets and the corresponding bounding boxes. Then, to weight each feature, a linear neural network is learned based on ListNet. Consequently, objectness scores can be computed for final candidate proposals ranking. Extensive experiments demonstrate the effectiveness and robustness of the proposed method. © 2017","ListNet; Object detection; Object proposal; Objectness feature; Superpixel merging","Merging; Object recognition; Pixels; Learning to rank; Linear neural network; ListNet; Merging strategy; Object proposal; Objectness feature; State of the art; Super pixels; Object detection; art; Article; artificial neural network; classification; clinical effectiveness; computer analysis; conceptual framework; entropy; information processing; intermethod comparison; ListNet method; machine learning; mathematical analysis; object proposal; pattern recognition; priority journal",2-s2.0-85020936262
"Zhou F., Wu F., Zhang Z., Dong M.","Node-level parallelization for deep neural networks with conditional independent graph",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020842908&doi=10.1016%2fj.neucom.2017.06.002&partnerID=40&md5=b5581e54b0e2d8ad83bae5269362322c","Deep neural networks require high performance computing and highly effective implementation to constrain the running time into a reasonable range. We proposed a novel node-level parallelization, conditional independent parallelization, of the forward and backward propagations to improve the level of concurrency. The propagations exploit a conditional independent graph (CIG) built in O(N) times, which consists of conditional independent sets of nodes. Each set in the CIG is sequentially visited, while the nodes in the set are calculated concurrently. Besides, we analyze the properties of the CIG and prove the correctness of the propagations with the CIG, then study the theoretical speedup ratios of the parallelization. Moreover, this parallelism can be applied to arbitrary structures of neural networks without influencing convergence, which only needs a conditional independent graph. It can be further integrated into other frameworks with batch-level and data-level parallelism to improve the level of concurrency. Since modern GPU supports concurrent kernels, the parallelization can also be implemented on GPU directly. To verify the parallelization in experiments, we implement an autoencoder, a dependency parser and an image recognizer with the parallelization and test them on a 4-core CPU I7 4790K with 32 GB memory. The results demonstrate that it can achieve maximum speedups of 3.965 × for the autoencoder, of 3.106 × for the parsing and of 2.966 × for the recognizer. © 2017 Elsevier B.V.","Concurrent kernels; Conditional independent graph; Deep neural networks; Node-level parallelization; OpenMP","Application programming interfaces (API); Graph theory; Learning systems; Arbitrary structures; Concurrent kernels; Conditional independent graph; Data-level parallelism; Forward-and-backward; High performance computing; OpenMP; Parallelizations; Deep neural networks; Article; artificial neural network; automation; back propagation; conditional independent graph; deep neural network; image processing; kernel method; machine learning; node level parallelization; nonlinear system; pattern recognition; priority journal; process optimization",2-s2.0-85020842908
"Wang J., Liu W., Xing W., Zhang S.","Two-level superpixel and feedback based visual object tracking",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021854508&doi=10.1016%2fj.neucom.2017.06.031&partnerID=40&md5=32ea88073f8a32dc7063ca666c690579","While numerous superpixel-based tracking algorithms have been proposed and demonstrated successfully, there still remain some challenges, such as determining the number of superpixels, mining and exploiting the structural information of superpixels and handling the drifts. In this paper, we propose a tracking method with two-level superpixels and a novel update strategy based on feedback to deal with the challenges mentioned above. Firstly, Bilateral filter is introduced to filter out outliers and improve the boundary capability of object as well as segmentation of superpixels. Then two-level superpixel is proposed to determine superpixel number automatically through iterating instead of setting superpixel number empirically which affects the robustness of tracking algorithm. Moreover, a novel measuring method which considers color similarity and relative positions of superpixels is proposed to make a better use of structural information of superpixels and improve tracking performance by adding relative position of superpixels into the appearance model. Finally, a feedback based update strategy is presented to handle drifts existing in tracking by calculating the adaptation of appearance model and updating the parameters like superpixel number and relative position of superpixels. Experiments on challenging sequences and comparisons to state-of-the-art methods demonstrate the feasibility and effectiveness of the proposed tracking algorithm. © 2017","Feedback; Superpixel; Two-level superpixel; Visual tracking","Feedback; Filtration; Tracking (position); Appearance modeling; Novel measuring method; State-of-the-art methods; Structural information; Super pixels; Tracking performance; Visual object tracking; Visual Tracking; Pixels; Article; automated pattern recognition; Bayes theorem; data mining; feedback system; illumination; image display; image segmentation; machine learning; online system; priority journal; process optimization; superpixel based tracking algorithm; visual object tracking",2-s2.0-85021854508
"Yan J., Xia Y., Wen C.","Quantized control for NCSs with communication constraints",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023609558&doi=10.1016%2fj.neucom.2017.06.040&partnerID=40&md5=96a5de7b00ecc77f3796b9648ef3ba42","This paper studies the stabilization problem for networked control systems (NCSs) affected by data quantization, time-varying transmission intervals, time-varying transmission delays and communication constraints. Time-varying transmission intervals and delays, by limiting the upper and lower bounds of which, can be described by a two-dimensional convex region. Combined with the coupling role of data quantization, communication constraints mean that only one node can occupy the network and send its quantized values in each transmission. The order in which node transmits its quantized values is determined by a given periodic network protocol. By introducing a variable called proportionality coefficient of saturation value in well-known zoom strategy to deal with the complex coupling relationship between system states and quantized variables, some sufficient conditions are derived for reaching asymptotic stability of NCSs under properly designed quantizer parameters. A simulation example is given to illustrate the effectiveness of the theoretical analysis. © 2017 Elsevier B.V.","Asymptotic stability; Data quantization; Periodic protocol; Time-varying systems","Asymptotic stability; Complex networks; Network protocols; System stability; Time varying systems; Communication constraints; Data quantizations; Networked Control Systems (NCSs); Periodic protocols; Stabilization problems; Transmission delays; Transmission intervals; Upper and lower bounds; Networked control systems; algorithm; Article; calculation; communication protocol; data analysis; linear system; machine learning; mathematical analysis; mathematical model; networked control system; nonlinear system; priority journal; quantitative analysis; simulation",2-s2.0-85023609558
"Sun W., Zhao H., Jin Z.","An efficient unconstrained facial expression recognition algorithm based on Stack Binarized Auto-encoders and Binarized Neural Networks",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021789014&doi=10.1016%2fj.neucom.2017.06.050&partnerID=40&md5=d786e703a217b984cee1857c6387a10e","Although deep learning has achieved good performances in many pattern recognition tasks, the over-fitting problem is still a serious issue for training deep networks containing large sets of parameters with limited labeled data. In this work, Binarized Auto-encoders (BAEs) and Stacked Binarized Auto-encoders (Stacked BAEs) are proposed to learn a kind of domain knowledge from a large-scale unlabeled facial dataset. By transferring the knowledge to another Binarized Neural Networks (BNNs) based supervised learning task with limited labeled data, the performance of the BNNs can be improved. A real-world facial expression recognition system is constructed by combining an unconstrained face normalization method, a variant of LBP descriptor, BAEs and BNNs. The experiment result shows that the whole system achieves good performance on the Static Facial Expressions in the Wild (SFEW) benchmark with minimal hardware requirements and lower memory and computation costs. © 2017 Elsevier B.V.","Binarized Auto-encoder; Facial expression recognition; Unconstrained face","Benchmarking; Bins; Deep learning; Education; Learning systems; Pattern recognition; Auto encoders; Computation costs; Domain knowledge; Face normalization; Facial expression recognition; Facial Expressions; Over fitting problem; Unconstrained face; Face recognition; algorithm; Article; Binarized Neural Network; classifier; controlled study; data augmentation; facial expression recognition algorithm; information processing; intermethod comparison; multi scale dense local binary patterns; priority journal; Stack Binarized Auto encoders; static facial expressions in the wild; support vector machine",2-s2.0-85021789014
"Wu J., Zhu Y., Wang Z., Song Z., Liu X., Wang W., Zhang Z., Yu Y., Xu Z., Zhang T., Zhou J.","A novel ship classification approach for high resolution SAR images based on the BDA-KELM classification model",2017,"International Journal of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026222647&doi=10.1080%2f01431161.2017.1356487&partnerID=40&md5=15d216777b3f66e66f94ab373dad99e9","Ship classification based on synthetic aperture radar (SAR) images is a crucial component in maritime surveillance. In this article, the feature selection and the classifier design, as two key essential factors for traditional ship classification, are jointed together, and a novel ship classification model combining kernel extreme learning machine (KELM) and dragonfly algorithm in binary space (BDA), named BDA-KELM, is proposed which conducts the automatic feature selection and searches for optimal parameter sets (including the kernel parameter and the penalty factor) for classifier at the same time. Finally, a series of ship classification experiments are carried out based on high resolution TerraSAR-X SAR imagery. Other four widely used classification models, namely k-Nearest Neighbour (k-NN), Bayes, Back Propagation neural network (BP neural network), Support Vector Machine (SVM), are also tested on the same dataset. The experimental results shows that the proposed model can achieve a better classification performance than these four widely used models with an classification accuracy as high as 97% and encouraging results of other three multi-class classification evaluation metrics. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Backpropagation; Image classification; Learning systems; Nearest neighbor search; Neural networks; Radar imaging; Ships; Support vector machines; Synthetic aperture radar; Automatic feature selection; Back propagation neural networks; Classification accuracy; Classification performance; Extreme learning machine; K nearest neighbours (k-NN); Multi-class classification; Synthetic aperture radar (SAR) images; Classification (of information); accuracy assessment; classification; machine learning; remote sensing; satellite imagery; ship design; support vector machine; synthetic aperture radar; TerraSAR-X; Anisoptera (dragonflies)",2-s2.0-85026222647
"Chen Y., Liu X., Li X.","Calibrating a Land Parcel Cellular Automaton (LP-CA) for urban growth simulation based on ensemble learning",2017,"International Journal of Geographical Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028546274&doi=10.1080%2f13658816.2017.1367004&partnerID=40&md5=8dd66bf17a20c9a2eef98dd8eba69253","The reliability of raster cellular automaton (CA) models for fine-scale land change simulations has been increasingly questioned, because regular pixels/grids cannot precisely represent irregular geographical entities and their interactions. Vector CA models can address these deficiencies due to the ability of the vector data structure to represent realistic urban entities. This study presents a new land parcel cellular automaton (LP-CA) model for simulating urban land changes. The innovation of this model is the use of ensemble learning method for automatic calibration. The proposed model is applied in Shenzhen, China. The experimental results indicate that bagging-Naïve Bayes yields the highest calibration accuracy among a set of selected classifiers. The assessment of neighborhood sensitivity suggests that the LP-CA model achieves the highest simulation accuracy with neighbor radius r = 2. The calibrated LP-CA is used to project future urban land use changes in Shenzhen, and the results are found to be consistent with those specified in the official city plan. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Cellular automata; ensemble learning; irregular cells; land parcels","calibration; cellular automaton; computer simulation; land use change; machine learning; urban growth; China; Guangdong; Shenzhen",2-s2.0-85028546274
"Terziyan V.","Social Distance metric: from coordinates to neighborhoods",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028531521&doi=10.1080%2f13658816.2017.1367796&partnerID=40&md5=aba4e6406349a67032ba39e8555e2eb3","Choice of a distance metric is a key for the success in many machine learning and data processing tasks. The distance between two data samples traditionally depends on the values of their attributes (coordinates) in a data space. Some metrics also take into account the distribution of samples within the space (e.g. local densities) aiming to improve potential classification or clustering performance. In this paper, we suggest the Social Distance metric that can be used on top of any traditional metric. For a pair of samples x and y, it averages the two numbers: the place (rank), which sample y holds in the list of ordered nearest neighbors of x; and vice versa, the rank of x in the list of the nearest neighbors of y. Average is a contraharmonic Lehmer mean, which penalizes the difference between the numbers by giving values greater than the Arithmetic mean for the unequal arguments. We consider normalized average as a distance function and we prove it to be a metric. We present several modifications of such metric and show that their properties are useful for a variety of classification and clustering tasks in data spaces or graphs in a Geographic Information Systems context and beyond. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","classification; clustering; data mining; density; distance function; graphs; Lehmer mean; Metric; social neighborhood",,2-s2.0-85028531521
"Hu Y., Ye X., Shaw S.-L.","Extracting and analyzing semantic relatedness between cities using news articles",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028556133&doi=10.1080%2f13658816.2017.1367797&partnerID=40&md5=745211ac4ba975d47bb9c0fd7e8646f2","News articles capture a variety of topics about our society. They reflect not only the socioeconomic activities that happened in our physical world, but also some of the cultures, human interests, and public concerns that exist only in the perceptions of people. Cities are frequently mentioned in news articles, and two or more cities may co-occur in the same article. Such co-occurrence often suggests certain relatedness between the mentioned cities, and the relatedness may be under different topics depending on the contents of the news articles. We consider the relatedness under different topics as semantic relatedness. By reading news articles, one can grasp the general semantic relatedness between cities; yet, given hundreds of thousands of news articles, it is very difficult, if not impossible, for anyone to manually read them. This paper proposes a computational framework which can ‘read’ a large number of news articles and extract the semantic relatedness between cities. This framework is based on a natural language processing model and employs a machine learning process to identify the main topics of news articles. We describe the overall structure of this framework and its individual modules, and then apply it to an experimental dataset with more than 500,000 news articles covering the top 100 US cities spanning a 10-year period. We perform exploratory visualizations of the extracted semantic relatedness under different topics and over multiple years. We also analyze the impact of geographic distance on semantic relatedness and find varied distance decay effects. The proposed framework can be used to support large-scale content analysis in city network research. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","city network; geographic knowledge discovery; geospatial semantics; Place relatedness; semantic analysis; spatial data mining",,2-s2.0-85028556133
"Lynch C.M., Abdollahi B., Fuqua J.D., de Carlo A.R., Bartholomai J.A., Balgemann R.N., van Berkel V.H., Frieboes H.B.","Prediction of lung cancer patient survival via supervised machine learning classification techniques",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030106984&doi=10.1016%2fj.ijmedinf.2017.09.013&partnerID=40&md5=4d35fe5facbda6a4cc27c63bee0ea079","Outcomes for cancer patients have been previously estimated by applying various machine learning techniques to large datasets such as the Surveillance, Epidemiology, and End Results (SEER) program database. In particular for lung cancer, it is not well understood which types of techniques would yield more predictive information, and which data attributes should be used in order to determine this information. In this study, a number of supervised learning techniques is applied to the SEER database to classify lung cancer patients in terms of survival, including linear regression, Decision Trees, Gradient Boosting Machines (GBM), Support Vector Machines (SVM), and a custom ensemble. Key data attributes in applying these methods include tumor grade, tumor size, gender, age, stage, and number of primaries, with the goal to enable comparison of predictive power between the various methods The prediction is treated like a continuous target, rather than a classification into categories, as a first step towards improving survival prediction. The results show that the predicted values agree with actual values for low to moderate survival times, which constitute the majority of the data. The best performing technique was the custom ensemble with a Root Mean Square Error (RMSE) value of 15.05. The most influential model within the custom ensemble was GBM, while Decision Trees may be inapplicable as it had too few discrete outputs. The results further show that among the five individual models generated, the most accurate was GBM with an RMSE value of 15.32. Although SVM underperformed with an RMSE value of 15.82, statistical analysis singles the SVM as the only model that generated a distinctive output. The results of the models are consistent with a classical Cox proportional hazards model used as a reference technique. We conclude that application of these supervised learning techniques to lung cancer data in the SEER database may be of use to estimate patient survival time with the ultimate goal to inform patient care decisions, and that the performance of these techniques with this particular dataset may be on par with that of classical methods. © 2017 Elsevier B.V.","Biomedical big data; Data classification; Lung cancer; Machine learning; SEER database; Supervised classification","Artificial intelligence; Big data; Biological organs; Classification (of information); Database systems; Decision trees; Diseases; Forecasting; Forestry; Learning algorithms; Learning systems; Mean square error; Support vector machines; Trees (mathematics); Tumors; Cox proportional hazards models; Data classification; Lung Cancer; Machine learning techniques; Root mean square errors; Seer database; Supervised classification; Supervised machine learning; Supervised learning",2-s2.0-85030106984
"Regan T., Beale C., Inalpolat M.","Wind Turbine Blade Damage Detection Using Supervised Machine Learning Algorithms",2017,"Journal of Vibration and Acoustics, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026894926&doi=10.1115%2f1.4036951&partnerID=40&md5=c15171ae07d89af05f228d02c7a6d3fb","Wind turbine blades undergo high operational loads, experience variable environmental conditions, and are susceptible to failure due to defects, fatigue, and weather-induced damage. These large-scale composite structures are fundamentally enclosed acoustic cavities and currently have limited, if any, structural health monitoring (SHM) in place. A novel acoustics-based structural sensing and health monitoring technique is developed, requiring efficient algorithms for operational damage detection of cavity structures. This paper describes the selection of a set of statistical features for acoustics-based damage detection of enclosed cavities, such as wind turbine blades, as well as a systematic approach used in the identification of competent machine learning algorithms. Logistic regression (LR) and support vector machine (SVM) methods are identified and used with optimal feature selection for decision-making via binary classification algorithms. A laboratory-scale wind turbine with hollow composite blades was built for damage detection studies. This test rig allows for testing of stationary or rotating blades, of which time and frequency domain information can be collected to establish baseline characteristics. The test rig can then be used to observe any deviations from the baseline characteristics. An external microphone attached to the tower will be utilized to monitor blade health while blades are internally ensonified by wireless speakers. An initial test campaign with healthy and damaged blade specimens is carried out to arrive at several conclusions on the detectability and feature extraction capabilities required for damage detection. © 2017 by ASME.","health monitoring; logistic regression; machine learning; support vector machine; Wind turbine blades","Acoustic fields; Artificial intelligence; Damage detection; Decision making; Fatigue damage; Feature extraction; Frequency domain analysis; Learning systems; Regression analysis; Structural health monitoring; Supervised learning; Support vector machines; Turbine components; Turbomachine blades; Wind turbines; Health monitoring; Health monitoring technique; Logistic regressions; Optimal feature selections; Structural health monitoring (SHM); Supervised machine learning; Time and frequency domains; Wind turbine blades; Learning algorithms",2-s2.0-85026894926
"Ransom K.M., Nolan B.T., A. Traum J., Faunt C.C., Bell A.M., Gronberg J.A.M., Wheeler D.C., Z. Rosecrans C., Jurgens B., Schwarz G.E., Belitz K., M. Eberts S., Kourakos G., Harter T.","A hybrid machine learning model to predict and visualize nitrate concentration throughout the Central Valley aquifer, California, USA",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020189979&doi=10.1016%2fj.scitotenv.2017.05.192&partnerID=40&md5=8c3c01172828d55d48f1c3611a02c7aa","Intense demand for water in the Central Valley of California and related increases in groundwater nitrate concentration threaten the sustainability of the groundwater resource. To assess contamination risk in the region, we developed a hybrid, non-linear, machine learning model within a statistical learning framework to predict nitrate contamination of groundwater to depths of approximately 500 m below ground surface. A database of 145 predictor variables representing well characteristics, historical and current field and landscape-scale nitrogen mass balances, historical and current land use, oxidation/reduction conditions, groundwater flow, climate, soil characteristics, depth to groundwater, and groundwater age were assigned to over 6000 private supply and public supply wells measured previously for nitrate and located throughout the study area. The boosted regression tree (BRT) method was used to screen and rank variables to predict nitrate concentration at the depths of domestic and public well supplies. The novel approach included as predictor variables outputs from existing physically based models of the Central Valley. The top five most important predictor variables included two oxidation/reduction variables (probability of manganese concentration to exceed 50 ppb and probability of dissolved oxygen concentration to be below 0.5 ppm), field-scale adjusted unsaturated zone nitrogen input for the 1975 time period, average difference between precipitation and evapotranspiration during the years 1971–2000, and 1992 total landscape nitrogen input. Twenty-five variables were selected for the final model for log-transformed nitrate. In general, increasing probability of anoxic conditions and increasing precipitation relative to potential evapotranspiration had a corresponding decrease in nitrate concentration predictions. Conversely, increasing 1975 unsaturated zone nitrogen leaching flux and 1992 total landscape nitrogen input had an increasing relative impact on nitrate predictions. Three-dimensional visualization indicates that nitrate predictions depend on the probability of anoxic conditions and other factors, and that nitrate predictions generally decreased with increasing groundwater age. © 2017 The Authors","Boosted regression trees; Groundwater; Machine learning; Modeling; Nitrate","Aquifers; Artificial intelligence; Dissolved oxygen; Evapotranspiration; Forecasting; Forestry; Groundwater; Groundwater flow; Groundwater pollution; Land use; Landforms; Learning systems; Models; Nitrates; Nitrogen; Probability; Risk assessment; Three dimensional computer graphics; Water pollution; Boosted regression trees; Dissolved oxygen concentrations; Hybrid machine learning; Machine learning models; Manganese concentration; Physically based models; Potential evapotranspiration; Three dimensional visualization; Groundwater resources; dissolved oxygen; ground water; nitrate; nitrogen; anoxic conditions; aquifer; concentration (composition); groundwater pollution; machine learning; nitrate; numerical model; prediction; regression analysis; visualization; aquifer; Article; California; concentration (parameters); evapotranspiration; land use; machine learning; nitrate leaching; nitrogen balance; oxidation reduction reaction; priority journal; soil property; water contamination; water flow; water supply; California; Central Valley [California]; United States",2-s2.0-85020189979
"Ertuğrul Ö.F., Tağluk M.E.","A novel machine learning method based on generalized behavioral learning theory",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964067914&doi=10.1007%2fs00521-016-2314-8&partnerID=40&md5=6e03f7dbc349bfe714465989ffebbc4e","Learning is an important talent for understanding the nature and accordingly controlling behavioral characteristics. Behavioral learning theories are one of the popular learning theories which are built on experimental findings. These theories are widely applied in psychotherapy, psychology, neurology as well as in advertisements and robotics. There is an abundant literature associated with understanding learning mechanism, and various models have been proposed for the realization of learning theories. Nevertheless, none of those models are able to satisfactorily simulate the concept of classical conditioning. In this study, popular behavioral learning theories were firstly simplified and the contentious issues with them were clarified by conducting intuitive experiments. The experimental results and information available in the literature were evaluated, and behavioral learning theories were jointly generalized accordingly. The proposed model, to our knowledge, is the first one that possesses not only modeling all features of classical conditioning but also including all features with behavioral theories such as Pavlov, Watson, Guthrie, Thorndike and Skinner. Also, a microcontroller card (Arduino Mega 2560) was used to validate the applicability of the proposed model in robotics. Obtained results showed that this generalized model has a high capacity for modeling human learning. Then, the proposed learning model was further improved to be utilized as a machine learning method that can continuously learn similar to human being. The result obtained from the use of this method, in terms of computational cost and accuracy, showed that the proposed method can be successfully employed in machine learning, especially for time ordered datasets. © 2016, The Natural Computing Applications Forum.","Behavioral human learning; Classification; Computational model; Endless learning; Machine learning; Regression","Artificial intelligence; Behavioral research; Classification (of information); Computation theory; Robotics; Behavioral characteristics; Classical conditioning; Computational costs; Computational model; Endless learning; Human learning; Machine learning methods; Regression; Learning systems",2-s2.0-84964067914
"Alvarez A.M., Louveaux Q., Wehenkel L.","A machine learning-based approximation of strong branching",2017,"INFORMS Journal on Computing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011320141&doi=10.1287%2fijoc.2016.0723&partnerID=40&md5=0adc024d62483811a5dd6ea9d5a859be","We present in this paper a new generic approach to variable branching in branch and bound for mixedinteger linear problems. Our approach consists in imitating the decisions taken by a good branching strategy, namely strong branching, with a fast approximation. This approximated function is created by a machine learning technique from a set of observed branching decisions taken by strong branching. The philosophy of the approach is similar to reliability branching. However, our approach can catch more complex aspects of observed previous branchings to take a branching decision. The experiments performed on randomly generated and MIPLIB problems show promising results. © 2017 INFORMS.","Branch and bound; Strong branching; Supervised machine learning; Variable branching","Artificial intelligence; Branch and bound method; Supervised learning; Approximated functions; Fast approximation; Generic approach; Linear problems; Machine learning techniques; Strong branching; Supervised machine learning; Variable branching; Learning systems",2-s2.0-85011320141
"Gomez Fernandez M., Tokuhiro A., Welter K., Wu Q.","Nuclear energy system's behavior and decision making using machine learning",2017,"Nuclear Engineering and Design",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028890475&doi=10.1016%2fj.nucengdes.2017.08.020&partnerID=40&md5=f641ed758d96a0930ffc430c4709e8b2","Early versions of artificial neural networks’ ability to learn from data based on multivariable statistics and optimization demanded high computational performance as multiple training iterations are necessary to find an optimal local minimum. The rapid advancements in computational performance, storage capacity, and big data management have allowed machine-learning techniques to improve in the areas of learning speed, non-linear data handling, and complex features identification. Machine-learning techniques have proven successful and been used in the areas of autonomous machines, speech recognition, and natural language processing. Though the application of artificial intelligence in the nuclear engineering domain has been limited, it has accurately predicted desired outcomes in some instances and has proven to be a worthwhile area of research. The objectives of this study are to create neural networks topologies to use Oregon State University's Multi-Application Small Light Water Reactor integrated test facility's data and evaluate its capability of predicting the systems behavior during various core power inputs and a loss of flow accident. This study uses data from multiple sensors, focusing primarily on the reactor pressure vessel and its internal components. As a result, the artificial neural networks are able to predict the behavior of the system with good accuracy in each scenario. Its ability to provide technical data can help decision makers to take actions more rapidly, identify safety issues, or provide an intelligent system with the potential of using pattern recognition for reactor accident identification and classification. Overall, the development and application of neural networks can be promising in the nuclear industry and any product processes that can benefit from utilizing a quick data analysis tool. © 2017 The Authors","Decision-making optimization; Machine learning; Nuclear energy systems; Small modular reactors","Accidents; Artificial intelligence; Big data; Data handling; Decision making; Digital storage; Intelligent systems; Learning algorithms; Learning systems; Light water reactors; Natural language processing systems; Neural networks; Nuclear energy; Nuclear industry; Nuclear reactor accidents; Pattern recognition; Pattern recognition systems; Pressure vessels; Speech recognition; Computational performance; Decision-making optimization; Development and applications; Features identification; Machine learning techniques; Nuclear energy systems; Reactor Pressure Vessel; Small modular reactors; Information management",2-s2.0-85028890475
"Tahmasebi P., Javadpour F., Sahimi M.","Data mining and machine learning for identifying sweet spots in shale reservoirs",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025811616&doi=10.1016%2fj.eswa.2017.07.015&partnerID=40&md5=0d7b0d7131c8874d7204448db331b48a","Due to its complex structure, production form a shale-gas formation requires more drillings than those for the traditional reservoirs. Modeling of such reservoirs and making predictions for their production also require highly extensive datasets. Both are very costly. In-situ measurements, such as well-logging, are one of most indispensable tools for providing considerable amount of information and data for such unconventional reservoirs. Production from shale reservoirs involves the so-called fracking, i.e. injection of water and chemicals into the formation in order to open up flow paths for the hydrocarbons. The measurements and any other types of data are utilized for making critical decisions regarding development of a potential shale reservoir, as it requires hundreds of millions of dollar initial investment. The questions that must be addressed include, does the region under study can be used economically for producing hydrocarbons? If the response to the first question is affirmative, then, where are the best places to carry out hydro-fracking? Through the answers to such questions one identifies the sweet spots of shale reservoirs, which are the regions that contain high total organic carbon (TOC) and brittle rocks that can be fractured. In this paper, two methods from data mining and machine learning techniques are used to aid identifying such regions. The first method is based on a stepwise algorithm that determines the best combination of the variables (well-log data) to predict the target parameters. However, in order to incorporate more training, and efficiently use the available datasets, a hybrid machine-learning algorithm is also presented that models more accurately the complex spatial correlations between the input and target parameters. Then, statistical comparisons between the estimated variables and the available data are made, which indicate very good agreement between the two. The proposed model can be used effectively to estimate the probability of targeting the sweet spots. In the light of an automatic input and parameter selection, the algorithm does not require any further adjustment and can continuously evaluate the target parameters, as more data become available. Furthermore, the method is able to optimally identify the necessary logs that must be run, which significantly reduces data acquisition operations. © 2017 Elsevier Ltd","Big data; Brittleness index; Fracable zones; Genetic algorithm; Neural networks; Shale formation","Artificial intelligence; Carbon; Complex networks; Data mining; Education; Fracture mechanics; Genetic algorithms; Hydraulic fracturing; Hydrocarbons; Learning algorithms; Learning systems; Neural networks; Organic carbon; Parameter estimation; Shale; Water injection; Well logging; Amount of information; Brittleness index; Hybrid machine learning; Machine learning techniques; Shale formation; Spatial correlations; Statistical comparisons; Unconventional reservoirs; Big data",2-s2.0-85025811616
"Amrit C., Paauw T., Aly R., Lavric M.","Identifying child abuse through text mining and machine learning",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024891722&doi=10.1016%2fj.eswa.2017.06.035&partnerID=40&md5=586459c9e8239e9ebdf04cdff2f299fd","In this paper, we describe how we used text mining and analysis to identify and predict cases of child abuse in a public health institution. Such institutions in the Netherlands try to identify and prevent different kinds of abuse. A significant part of the medical data that the institutions have on children is unstructured, found in the form of free text notes. We explore whether these consultation data contain meaningful patterns to determine abuse. Then we train machine learning models on cases of abuse as determined by over 500 child specialists from a municipality in The Netherlands. The resulting model achieves a high score in classifying cases of possible abuse. We methodologically evaluate and compare the performance of the classifiers. We then describe our implementation of the decision support API at a municipality in the Netherlands. © 2017 Elsevier Ltd","Child abuse; Decision support; Machine learning; Text mining","Artificial intelligence; Data mining; Decision support systems; Learning systems; Child abuse; Decision supports; Free texts; Machine learning models; Medical data; Netherlands; Text mining; Education",2-s2.0-85024891722
"Gao W., Kwong S., Jia Y.","Joint Machine Learning and Game Theory for Rate Control in High Efficiency Video Coding",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028705788&doi=10.1109%2fTIP.2017.2745099&partnerID=40&md5=e421a83a0b48455869e96ef294409a90","In this paper, a joint machine learning and game theory modeling (MLGT) framework is proposed for inter frame coding tree unit (CTU) level bit allocation and rate control (RC) optimization in high efficiency video coding (HEVC). First, a support vector machine-based multi-classification scheme is proposed to improve the prediction accuracy of CTU-level rate-distortion (R-D) model. The legacy 'chicken-And-egg' dilemma in video coding is proposed to be overcome by the learning-based R-D model. Second, a mixed R-D model-based cooperative bargaining game theory is proposed for bit allocation optimization, where the convexity of the mixed R-D model-based utility function is proved, and Nash bargaining solution is achieved by the proposed iterative solution search method. The minimum utility is adjusted by the reference coding distortion and frame-level quantization parameter (QP) change. Finally, intra frame QP and inter frame adaptive bit ratios are adjusted to make inter frames have more bit resources to maintain smooth quality and bit consumption in the bargaining game optimization. Experimental results demonstrate that the proposed MLGT-based RC method can achieve much better R-D performances, quality smoothness, bit rate accuracy, buffer control results, and subjective visual quality than the other state-of-The-Art one-pass RC methods, and the achieved R-D performances are very close to the performance limits from the FixedQP method. © 1992-2012 IEEE.","bit allocation; game theory; H.265/HEVC; iterative search; machine learning; Nash bargaining solution (NBS); non-numerical solution; R-D model classification; rate control; support vector machine (SVM); video coding","Artificial intelligence; Codes (symbols); Efficiency; Electric distortion; Encoding (symbols); Game theory; Iterative methods; Learning systems; Motion Picture Experts Group standards; Optimization; Quality control; Signal distortion; Signal receivers; Support vector machines; Video signal processing; Bit allocation; Bit rates; H.265/HEVC; iterative search; Nash bargaining solution; Numerical solution; Predictive models; R-D model; Rate controls; Image coding",2-s2.0-85028705788
"Zhang H., Wang Y., Chen H., Zhao Y., Zhang J.","Exploring machine-learning-based control plane intrusion detection techniques in software defined optical networks",2017,"Optical Fiber Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030676395&doi=10.1016%2fj.yofte.2017.09.023&partnerID=40&md5=5c7ec6335e3553390141b8f4261f00c8","In software defined optical networks (SDON), the centralized control plane may encounter numerous intrusion threatens which compromise the security level of provisioned services. In this paper, the issue of control plane security is studied and two machine-learning-based control plane intrusion detection techniques are proposed for SDON with properly selected features such as bandwidth, route length, etc. We validate the feasibility and efficiency of the proposed techniques by simulations. Results show an accuracy of 83% for intrusion detection can be achieved with the proposed machine-learning-based control plane intrusion detection techniques. © 2017","Control plane; Intrusion detection; Machine learning; Optical networks","Artificial intelligence; Fiber optic networks; Learning systems; Mercury (metal); Centralized control; Control planes; Route length; Security level; Two machines; Intrusion detection",2-s2.0-85030676395
"Jahani E., Sundsøy P., Bjelland J., Bengtsson L., Pentland A.S., de Montjoye Y.-A.","Improving official statistics in emerging markets using machine learning and mobile phone data",2017,"EPJ Data Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019244546&doi=10.1140%2fepjds%2fs13688-017-0099-3&partnerID=40&md5=92bfe87a64fa1df50fc511201fab3381","Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated everytime phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis. We developed a framework to extract more than 1400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost. © 2017, The Author(s).","data-driven development; machine learning; mobile phone metadata; vulnerable populations","Artificial intelligence; Cellular telephones; Clock and data recovery circuits (CDR circuits); Developing countries; E-learning; Economic and social effects; Economics; Forecasting; Learning algorithms; Learning systems; Metadata; Mobile phones; Network function virtualization; Population statistics; Social sciences; Telephone sets; Cross-country studies; Data driven; Demographic variables; Developed countries; Individual characteristics; Science and development; Socio-economic status; vulnerable populations; Cellular telephone systems",2-s2.0-85019244546
"Maniruzzaman M., Kumar N., Menhazul Abedin M., Shaykhul Islam M., Suri H.S., El-Baz A.S., Suri J.S.","Comparative approaches for classification of diabetes mellitus data: Machine learning paradigm",2017,"Computer Methods and Programs in Biomedicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029347697&doi=10.1016%2fj.cmpb.2017.09.004&partnerID=40&md5=4d311f8e18597728f5a64ed1775293c3","Background and objective Diabetes is a silent killer. The main cause of this disease is the presence of excessive amounts of metabolites such as glucose. There were about 387 million diabetic people all over the world in 2014. The financial burden of this disease has been calculated to be about $13,700 per year. According to the World Health Organization (WHO), these figures will more than double by the year 2030. This cost will be reduced dramatically if someone can predict diabetes statistically on the basis of some covariates. Although several classification techniques are available, it is very difficult to classify diabetes. The main objectives of this paper are as follows: (i) Gaussian process classification (GPC), (ii) comparative classifier for diabetes data classification, (iii) data analysis using the cross-validation approach, (iv) interpretation of the data analysis and (v) benchmarking our method against others. Methods To classify diabetes, several classification techniques are used such as linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), and Naive Bayes (NB). However, most of the medical data show non-normality, non-linearity and inherent correlation structure. So in this paper we adapted Gaussian process (GP)-based classification technique using three kernels namely: linear, polynomial and radial basis kernel. We also investigate the performance of a GP-based classification technique in comparison to existing techniques such as LDA, QDA and NB. Performances are evaluated by using the accuracy (ACC), sensitivity (SE), specificity (SP), positive predictive value (PPV), negative predictive value (NPV) and receiver-operating characteristic (ROC) curves. Results Pima Indian diabetes dataset is taken as part of the study. This consists of 768 patients, of which 268 patients are diabetic and 500 patients are controls. Our machine learning system shows the performance of GP-based model as: ACC 81.97%, SE 91.79%, SP 63.33%, PPV 84.91% and NPV 62.50% which are larger compared to other methods. © 2017","Diabetes; GPC; LDA; Machine learning; NB; QDA","Artificial intelligence; Data handling; Discriminant analysis; Gaussian distribution; Gaussian noise (electronic); Information analysis; Learning algorithms; Learning systems; Medical problems; Niobium; Classification technique; Gaussian process classifications; Linear discriminant analysis; Negative predictive value; Positive predictive values; Quadratic discriminant analysis; Receiver operating characteristic curves; World Health Organization; Classification (of information); accuracy; Article; Bayesian learning; benchmarking; classification algorithm; controlled study; data analysis; diabetes mellitus; discriminant analysis; disease classification; gaussian process classification; human; Indian; intermethod comparison; jackknife test; kernel method; machine learning; major clinical study; mathematical computing; performance measurement system; predictive value; receiver operating characteristic; sensitivity and specificity",2-s2.0-85029347697
"Maimaitijiang M., Ghulam A., Sidike P., Hartling S., Maimaitiyiming M., Peterson K., Shavers E., Fishman J., Peterson J., Kadam S., Burken J., Fritschi F.","Unmanned Aerial System (UAS)-based phenotyping of soybean using multi-sensor data fusion and extreme learning machine",2017,"ISPRS Journal of Photogrammetry and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032392082&doi=10.1016%2fj.isprsjprs.2017.10.011&partnerID=40&md5=f0f40c55bdbbeb05bd1a8b73179711c0","Estimating crop biophysical and biochemical parameters with high accuracy at low-cost is imperative for high-throughput phenotyping in precision agriculture. Although fusion of data from multiple sensors is a common application in remote sensing, less is known on the contribution of low-cost RGB, multispectral and thermal sensors to rapid crop phenotyping. This is due to the fact that (1) simultaneous collection of multi-sensor data using satellites are rare and (2) multi-sensor data collected during a single flight have not been accessible until recent developments in Unmanned Aerial Systems (UASs) and UAS-friendly sensors that allow efficient information fusion. The objective of this study was to evaluate the power of high spatial resolution RGB, multispectral and thermal data fusion to estimate soybean (Glycine max) biochemical parameters including chlorophyll content and nitrogen concentration, and biophysical parameters including Leaf Area Index (LAI), above ground fresh and dry biomass. Multiple low-cost sensors integrated on UASs were used to collect RGB, multispectral, and thermal images throughout the growing season at a site established near Columbia, Missouri, USA. From these images, vegetation indices were extracted, a Crop Surface Model (CSM) was advanced, and a model to extract the vegetation fraction was developed. Then, spectral indices/features were combined to model and predict crop biophysical and biochemical parameters using Partial Least Squares Regression (PLSR), Support Vector Regression (SVR), and Extreme Learning Machine based Regression (ELR) techniques. Results showed that: (1) For biochemical variable estimation, multispectral and thermal data fusion provided the best estimate for nitrogen concentration and chlorophyll (Chl) a content (RMSE of 9.9% and 17.1%, respectively) and RGB color information based indices and multispectral data fusion exhibited the largest RMSE 22.6%; the highest accuracy for Chl a + b content estimation was obtained by fusion of information from all three sensors with an RMSE of 11.6%. (2) Among the plant biophysical variables, LAI was best predicted by RGB and thermal data fusion while multispectral and thermal data fusion was found to be best for biomass estimation. (3) For estimation of the above mentioned plant traits of soybean from multi-sensor data fusion, ELR yields promising results compared to PLSR and SVR in this study. This research indicates that fusion of low-cost multiple sensor data within a machine learning framework can provide relatively accurate estimation of plant traits and provide valuable insight for high spatial precision in agriculture and plant stress assessment. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Data Fusion; Extreme Learning Machine (ELM); Extreme Learning Machine based Regression (ELR); Phenotyping; Remote sensing; Unmanned Aerial System (UAS)","Agriculture; Amino acids; Biophysics; Chlorophyll; Cost estimating; Costs; Crops; Data fusion; Knowledge acquisition; Learning systems; Least squares approximations; Nitrogen; Nitrogen fixation; Regression analysis; Remote sensing; Unmanned aerial vehicles (UAV); Vegetation; Extreme learning machine; High-throughput phenotyping; Multisensor data fusion; Nitrogen concentrations; Partial least squares regressions (PLSR); Phenotyping; Support vector regression (SVR); Unmanned aerial systems; Sensor data fusion; agricultural modeling; biochemistry; biomass; biophysics; data assimilation; leaf area index; machine learning; phenotype; regression analysis; remote sensing; satellite imagery; sensor; soybean; spatial resolution; statistical analysis; unmanned vehicle; Columbia [Missouri]; Missouri; United States; Glycine max",2-s2.0-85032392082
"Wang H., Zhou Z., Li Y., Chen Z., Lu P., Wang W., Liu W., Yu L.","Comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from 18F-FDG PET/CT images",2017,"EJNMMI Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010894651&doi=10.1186%2fs13550-017-0260-9&partnerID=40&md5=f993083a0662d75b12c0585d861e7ad9","Background: This study aimed to compare one state-of-the-art deep learning method and four classical machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer (NSCLC) from 18F-FDG PET/CT images. Another objective was to compare the discriminative power of the recently popular PET/CT texture features with the widely used diagnostic features such as tumor size, CT value, SUV, image contrast, and intensity standard deviation. The four classical machine learning methods included random forests, support vector machines, adaptive boosting, and artificial neural network. The deep learning method was the convolutional neural networks (CNN). The five methods were evaluated using 1397 lymph nodes collected from PET/CT images of 168 patients, with corresponding pathology analysis results as gold standard. The comparison was conducted using 10 times 10-fold cross-validation based on the criterion of sensitivity, specificity, accuracy (ACC), and area under the ROC curve (AUC). For each classical method, different input features were compared to select the optimal feature set. Based on the optimal feature set, the classical methods were compared with CNN, as well as with human doctors from our institute. Results: For the classical methods, the diagnostic features resulted in 81~85% ACC and 0.87~0.92 AUC, which were significantly higher than the results of texture features. CNN’s sensitivity, specificity, ACC, and AUC were 84, 88, 86, and 0.91, respectively. There was no significant difference between the results of CNN and the best classical method. The sensitivity, specificity, and ACC of human doctors were 73, 90, and 82, respectively. All the five machine learning methods had higher sensitivities but lower specificities than human doctors. Conclusions: The present study shows that the performance of CNN is not significantly different from the best classical methods and human doctors for classifying mediastinal lymph node metastasis of NSCLC from PET/CT images. Because CNN does not need tumor segmentation or feature calculation, it is more convenient and more objective than the classical methods. However, CNN does not make use of the import diagnostic features, which have been proved more discriminative than the texture features for classifying small-sized lymph nodes. Therefore, incorporating the diagnostic features into CNN is a promising direction for future research. © 2017, The Author(s).","Computer-aided diagnosis; Deep learning; Machine learning; Non-small cell lung cancer; Positron-emission tomography","fluorodeoxyglucose f 18; adult; aged; area under the curve; Article; comparative study; computer assisted tomography; controlled study; diagnostic test accuracy study; female; human; image analysis; intermethod comparison; lymph node metastasis; machine learning; major clinical study; male; mediastinum lymph node; non small cell lung cancer; positron emission tomography; priority journal; sensitivity and specificity; tumor classification; validation therapy",2-s2.0-85010894651
"Sohrabi B., Manian A., Arman M.","Evaluation of most visited news websites in Iran based on machine learning",2017,"Iranian Journal of Information Processing Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014448708&partnerID=40&md5=9c3df8f71cf395a839d547ed3efcab06","Success and effectiveness of websites are largely dependent on their quality. The biggest share of the quality of new concept is that technical aspects of products and services combine with customers usage and understanding. Therefore, websites evaluation based on the maximum usage and perception of the customers is considered an important issue to announce to the related organizations. This ranking and evaluation should be performed in a special activity domain so that the first place of website rank determines among its other competitors. In this article achieving the information of websites is automatic and without the intervention of human so that the instant evaluation could be possible. In this study, one of the multi criteria decision-making methods called TOPSIS is used and the weights of the criteria have been achieved on entropy method in the mentioned method. Eventually, according to the Alexa ranking report, 791 ranking news website have been obtained which have most visitors of Iranian users, but on the other hand, just a numerical rank as a final output of websites evaluation can not be very inconsistent with the purpose of competition between websites, so these numerical ranking from TOPSIS method used as output in machine learing method for separating websites from excellent to very poor in six categories as lables for training dataset in classification, instead of using manual lables achieved from experts and users' opinion. For this classification, machine learning techniques, including artificial neural network and support vector machine were used.","Automation; Machine learning; Neural network; News websites; Support vector machine; TOPSIS; Websites evaluation",,2-s2.0-85014448708
"Tripathy A., Anand A., Rath S.K.","Document-level sentiment classification using hybrid machine learning approach",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019070167&doi=10.1007%2fs10115-017-1055-z&partnerID=40&md5=4e0664f2097c6bcdf09dc7c073658702","It is a practice that users or customers intend to share their comments or reviews about any product in different social networking sites. An analyst usually processes to reviews properly to obtain any meaningful information from it. Classification of sentiments associated with reviews is one of these processing steps. The reviews framed are often made in text format. While processing the text reviews, each word of the review is considered as a feature. Thus, selection of right kind of features needs to be carried out to select the best feature from the set of all features. In this paper, the machine learning algorithm, i.e., support vector machine, is used to select the best features from the training data. These features are then given input to artificial neural network method, to process further. Different performance evaluation parameters such as precision, recall, f-measure, accuracy have been considered to evaluate the performance of the proposed approach on two different datasets, i.e., IMDb dataset and polarity dataset. © 2017, Springer-Verlag London.","Artificial neural network (ANN); Document-level sentiment analysis; Machine learning algorithm; Performance evaluation parameter; Support vector machine (SVM)",,2-s2.0-85019070167
"Skoraczyñski G., DIttwald P., Miasojedow B., Szymkuc S., Gajewska E.P., Grzybowski B.A., Gambin A.","Predicting the outcomes of organic reactions via machine learning: Are current descriptors sufficient?",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020943110&doi=10.1038%2fs41598-017-02303-0&partnerID=40&md5=88caf08f297416beb1cf30a628a2e6a0","As machine learning/artificial intelligence algorithms are defeating chess masters and, most recently, GO champions, there is interest- And hope-that they will prove equally useful in assisting chemists in predicting outcomes of organic reactions. This paper demonstrates, however, that the applicability of machine learning to the problems of chemical reactivity over diverse types of chemistries remains limited-in particular, with the currently available chemical descriptors, fundamental mathematical theorems impose upper bounds on the accuracy with which raction yields and times can be predicted. Improving the performance of machine-learning methods calls for the development of fundamentally new chemical descriptors. © 2017 The Author(s).",,"chemical binding; chemistry; machine learning",2-s2.0-85020943110
"Kolb B., Lentz L.C., Kolpak A.M.","Discovering charge density functionals and structure-property relationships with PROPhet: A general framework for coupling machine learning and first-principles methods",2017,"Scientific Reports",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018942583&doi=10.1038%2fs41598-017-01251-z&partnerID=40&md5=df6a773f68014dc893a0cb6a59263718","Modern ab initio methods have rapidly increased our understanding of solid state materials properties, chemical reactions, and the quantum interactions between atoms. However, poor scaling often renders direct ab initio calculations intractable for large or complex systems. There are two obvious avenues through which to remedy this problem: (i) develop new, less expensive methods to calculate system properties, or (ii) make existing methods faster. This paper describes an open source framework designed to pursue both of these avenues. PROPhet (short for PROPerty Prophet) utilizes machine learning techniques to find complex, non-linear mappings between sets of material or system properties. The result is a single code capable of learning analytical potentials, non-linear density functionals, and other structure-property or property-property relationships. These capabilities enable highly accurate mesoscopic simulations, facilitate computation of expensive properties, and enable the development of predictive models for systematic materials design and optimization. This work explores the coupling of machine learning to ab initio methods through means both familiar (e.g., the creation of various potentials and energy functionals) and less familiar (e.g., the creation of density functionals for arbitrary properties), serving both to demonstrate PROPhet's ability to create exciting post-processing analysis tools and to open the door to improving ab initio methods themselves with these powerful machine learning techniques. © 2017 The Author(s).",,"ab initio calculation; machine learning; simulation",2-s2.0-85018942583
"Broecker P., Carrasquilla J., Melko R.G., Trebst S.","Machine learning quantum phases of matter beyond the fermion sign problem",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027722916&doi=10.1038%2fs41598-017-09098-0&partnerID=40&md5=9f6691fb3e02456b309c411e24026345","State-of-the-art machine learning techniques promise to become a powerful tool in statistical mechanics via their capacity to distinguish different phases of matter in an automated way. Here we demonstrate that convolutional neural networks (CNN) can be optimized for quantum many-fermion systems such that they correctly identify and locate quantum phase transitions in such systems. Using auxiliary-field quantum Monte Carlo (QMC) simulations to sample the many-fermion system, we show that the Green's function holds sufficient information to allow for the distinction of different fermionic phases via a CNN. We demonstrate that this QMC + machine learning approach works even for systems exhibiting a severe fermion sign problem where conventional approaches to extract information from the Green's function, e.g. in the form of equal-time correlation functions, fail. © 2017 The Author(s).",,"correlation function; extract; fermion; machine learning; Monte Carlo method; nervous system; phase transition",2-s2.0-85027722916
"Das S.P., Padhy S.","A new hybrid parametric and machine learning model with homogeneity hint for European-style index option pricing",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963642541&doi=10.1007%2fs00521-016-2303-y&partnerID=40&md5=4dc44cb4a28ab8e4ab7ce691e5f19ba9","Here, we propose and investigate a hybrid model that combines parametric option pricing models such as Black–Scholes (BS) option pricing model, Monte Carlo option pricing model, and finite difference method with nonparametric machine learning techniques such as support vector regression (SVR) and extreme learning machine-based regression models. The purpose of this model is to support better investment decisions by forecasting the option price with high predictive accuracy. To further reduce the forecasting error, we incorporate a homogeneity hint (i.e., training the model by categorizing the options data based on moneyness and time-to-maturity of the option contract) into the model. We examine the feasibility and effectiveness of this model using a case study to predict the one-day-ahead price of index options traded in the National Stock Exchange of India Limited. Our experimental results show that the proposed new hybrid model is viable and effective and provides better predictive performance as compared with our benchmark models (standard BS Model, standard Monte Carlo, standard finite difference model, and standard SVR Model). For example, the proposed hybrid model using SVR improved, respectively, the root-mean-square error and mean absolute error by 83.66 and 85.46 % (D1 dataset), 78.02 and 76.0 % (D2 dataset), 91.86 and 90.62 % (D3 dataset), and 87.7 and 90.29 % (D4 dataset), when compared with the benchmarked BS model. We observe similar improvements over the other benchmarked models. Therefore, the proposed new hybrid model is a suitable alternative model for option pricing when higher predictive accuracy is desired. © 2016, The Natural Computing Applications Forum.","Extreme learning machines (ELMs); Homogeneity hint; Nonparametric methods; Option pricing; Parametric methods; Support vector regression (SVR)","Artificial intelligence; Benchmarking; Costs; Errors; Financial markets; Finite difference method; Forecasting; Investments; Knowledge acquisition; Learning systems; Mean square error; Monte Carlo methods; Regression analysis; Extreme learning machine; Homogeneity hint; Nonparametric methods; Option pricing; Parametric method; Support vector regression (SVR); Economics",2-s2.0-84963642541
"Wang X., Zhang F., Ding J.","Evaluation of water quality based on a machine learning algorithm and water quality index for the Ebinur Lake Watershed, China",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031020713&doi=10.1038%2fs41598-017-12853-y&partnerID=40&md5=5601fbe638d05f19b19790bf5fce53b6","The water quality index (WQI) has been used to identify threats to water quality and to support better water resource management. This study combines a machine learning algorithm, WQI, and remote sensing spectral indices (difference index, DI; ratio index, RI; and normalized difference index, NDI) through fractional derivatives methods and in turn establishes a model for estimating and assessing the WQI. The results show that the calculated WQI values range between 56.61 and 2,886.51. We also explore the relationship between reflectance data and the WQI. The number of bands with correlation coefficients passing a significance test at 0.01 first increases and then decreases with a peak appearing after 1.6 orders. WQI and DI as well as RI and NDI correlation coefficients between optimal band combinations of the peak also appear after 1.6 orders with R2 values of 0.92, 0.58 and 0.92. Finally, 22 WQI estimation models were established by POS-SVR to compare the predictive effects of these models. The models based on a spectral index of 1.6 were found to perform much better than the others, with an R2 of 0.92, an RMSE of 58.4, and an RPD of 2.81 and a slope of curve fitting of 0.97. © 2017 The Author(s).",,"China; correlation coefficient; curve fitting; DNA polymorphism; machine learning; remote sensing; statistical significance; water quality; watershed",2-s2.0-85031020713
"O'Connell G.C., Chantler P.D., Barr T.L.","Stroke-associated pattern of gene expression previously identified by machine-learning is diagnostically robust in an independent patient population",2017,"Genomics Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028758648&doi=10.1016%2fj.gdata.2017.08.006&partnerID=40&md5=7bb231a921f3e2b7fd2af391c7cd20c8","Our group recently employed genome-wide transcriptional profiling in tandem with machine-learning based analysis to identify a ten-gene pattern of differential expression in peripheral blood which may have utility for detection of stroke. The objective of this study was to assess the diagnostic capacity and temporal stability of this stroke-associated transcriptional signature in an independent patient population. Publicly available whole blood microarray data generated from 23 ischemic stroke patients at 3, 5, and 24 h post-symptom onset, as well from 23 cardiovascular disease controls, were obtained via the National Center for Biotechnology Information Gene Expression Omnibus. Expression levels of the ten candidate genes (ANTXR2, STK3, PDK4, CD163, MAL, GRAP, ID3, CTSZ, KIF1B, and PLXDC2) were extracted, compared between groups, and evaluated for their discriminatory ability at each time point. We observed a largely identical pattern of differential expression between stroke patients and controls across the ten candidate genes as reported in our prior work. Furthermore, the coordinate expression levels of the ten candidate genes were able to discriminate between stroke patients and controls with levels of sensitivity and specificity upwards of 90% across all three time points. These findings confirm the diagnostic robustness of the previously identified pattern of differential expression in an independent patient population, and further suggest that it is temporally stable over the first 24 h of stroke pathology. © 2017","Biomarker; Brain injury; Cerebrovascular disease; GA/kNN; Genetic algorithm; Immunology; Pattern recognition; Triage","CD163 antigen; inhibitor of differentiation 3; pyruvate dehydrogenase kinase 4; tissue plasminogen activator; adult; ANTXR2 gene; Article; CD163 gene; cerebrovascular accident; clinical article; controlled study; CTSZ gene; diagnostic test accuracy study; diagnostic value; DNA microarray; dyslipidemia; female; gene; gene expression; gene expression profiling; genetic transcription; GRAP gene; human; ID3 gene; KIF1B gene; machine learning; MAL gene; male; PDK4 gene; PLXDC2 gene; priority journal; receiver operating characteristic; sensitivity and specificity; STK3 gene",2-s2.0-85028758648
"Han X., Guo Y., Mi C., Huettmann F., Wen L.","Machine Learning Model Analysis of Breeding Habitats for the Black-necked Crane in Central Asian Uplands under Anthropogenic Pressures",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025833189&doi=10.1038%2fs41598-017-06167-2&partnerID=40&md5=83608b68ed2b459473a324beb38e70c8","The black-necked crane (Grus nigricollis) is the only alpine crane species and is endemic to the Tibetan Plateau. The breeding habitats of this species are poorly understood, which greatly hampers practical research and conservation work. Using machine learning methods and the best-available data from our 7,000-kilometer mega-transect survey and open access data, we built the first species distribution model (SDM) to analyze the black-necked crane's breeding habitats. Our model showed that current conservation gaps account for 26.7% of its predicted breeding habitats. Specifically, the northern parts of the Hengduan Mountains and the southeastern Tibet Valley, the northern side of the middle Kunlun Mountains, parts of the Pamir Plateau, the northern Pakistan Highlands and the western Hindu Kush should be considered as its main potential breeding areas. Additionally, our model suggested that the crane prefers to breed in alpine meadows at an elevation over 2,800 m, a maximum temperature of the warmest month below 20.5 °C, and a temperature seasonality above 7,800 units. The identified conservation gaps and potential breeding areas can aid in clearly prioritizing future conservation and research, but more attention and study should be directed to the unassessed Western Development of China to secure this endangered crane lineage and other wildlife on the Tibetan Plateau. © 2017 The Author(s).",,"attention; breed; breeding; habitat; Hindu; human; machine learning; model; Pakistan; seasonal variation; species distribution; Tibet; wildlife",2-s2.0-85025833189
"Zhao Y.-P., Song F.-Q., Pan Y.-T., Li B.","Retargeting extreme learning machines for classification and their applications to fault diagnosis of aircraft engine",2017,"Aerospace Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032856626&doi=10.1016%2fj.ast.2017.10.004&partnerID=40&md5=113d40c1afd026d523f55944d992bc05","Since the original extreme learning machine (ELM) generates the hidden nodes randomly, it usually needs more hidden nodes to reach the good classification performance. However, more hidden nodes will jeopardize the real time, which limits its applications to the testing time sensitive scenarios. To this end, the commonly-used methods tend to compact its structure via optimizing the number of hidden nodes. Different from this viewpoint of network structure, in this paper two algorithms are proposed to improve the real time performance of ELM from a viewpoint of data structure. Specially, they improve the ELM classification performance by retargeting its label vectors. As thus, they need fewer hidden nodes to reach the same classification performance, which means the better real time. Finally, experimental results on the benchmark data sets validate the effectiveness and feasibility of the presented two algorithms. To be more important, they are applied to the fault diagnosis of aircraft engine and can be developed as its candidate techniques. © 2017 Elsevier Masson SAS","Aircraft engine; Extreme learning machine; Fault diagnosis; Machine learning algorithm","Aircraft engines; Computer aided diagnosis; Engines; Failure analysis; Knowledge acquisition; Learning algorithms; Learning systems; Benchmark data; Classification performance; Extreme learning machine; Hidden nodes; ITS applications; Network structures; Real time performance; Testing time; Fault detection",2-s2.0-85032856626
"Alwasel A., Sabet A., Nahangi M., Haas C.T., Abdel-Rahman E.","Identifying poses of safe and productive masons using machine learning",2017,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030539003&doi=10.1016%2fj.autcon.2017.09.022&partnerID=40&md5=bfbc662e1b4f6b18eec55ff25609ce90","This paper presents a framework to classify work poses among groups of masons during the building of a standard wall of concrete masonry units. The experience of the group composed of masonry instructors and master masons averaged five times that of the other groups, their productivity was highest, and the loads on their joints were the lowest. Thus, they were deemed experts in this paper. Inertial measurement units (IMU) and video cameras were used to collect kinematic data of the masons, from which pose clusters were identified. A Support Vector Machine (SVM) algorithm was used to classify masons' poses into expert and inexpert classes based on the relative frequency of poses in the motions used to lay each of 945 masonry units. Two classification scenarios were tested. While both scenarios achieved similar levels of accuracy, 91.23% and 92.04% respectively, the processing time for binary classification was only 13 s compared to 523 s for inter-group multiclass SVM. Like characteristic vibration frequencies in machine diagnostics and system identification, the characteristic poses identified provide insight into differing methods between expert and less experienced masons. For example, results show that experts utilize fewer and more ergonomicaly safe poses, while being more productive, which indicates lower energy expenditure (less wasted motions). The classification method and the poses identified contribute knowledge to help develop affordable mason training systems that utilize IMU and video feedback to improve health and productivity of apprentice masons. © 2017 Elsevier B.V.","Classification; Data analytics; Efficiency; Masonry; Musculoskeletal injuries; Safety; Support vector machine; Training","Accident prevention; Classification (of information); Efficiency; Learning systems; Masonry materials; Personnel training; Productivity; Support vector machines; Units of measurement; Video cameras; Binary classification; Classification methods; Concrete masonry units; Data analytics; Inertial measurement unit; Masonry; Musculo-skeletal injuries; Support vector machine algorithm; Gesture recognition",2-s2.0-85030539003
"Lu F., Jiang J., Huang J., Qiu X.","Dual reduced kernel extreme learning machine for aero-engine fault diagnosis",2017,"Aerospace Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441426&doi=10.1016%2fj.ast.2017.10.024&partnerID=40&md5=e522bfe42adaed0f5304ae6a93f62afd","In order to improve the sparsity of kernel-based extreme learning machine (KELM), this paper proposed a novel method named dual reduced kernel extreme learning machine (DR-KELM). The proposed algorithm incorporates traditional greedy forward learning algorithm into backward learning algorithm to gain more sparsity and enhance testing time further. Compared to original KELM, the proposed method produces satisfactory performance of pattern recognition with fewer nodes, and reduces diagnostic consuming time from the tests on benchmark dataset. The DR-KELM application to aero-engine fault diagnosis also demonstrates its superior performance with more sparse structure. © 2017 Elsevier Masson SAS","Aero-engine; Backward learning; Fault diagnosis; Forward learning; Kernel extreme learning machine; Sparsity","Aircraft engines; Benchmarking; Engines; Failure analysis; Knowledge acquisition; Learning algorithms; Learning systems; Neural networks; Pattern recognition; Statistical tests; Aero-engine; Backward learning; Extreme learning machine; Forward learning; Sparsity; Fault detection",2-s2.0-85032441426
"Guo W., Xu T., Tang K.","M-estimator-based online sequential extreme learning machine for predicting chaotic time series with outliers",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963642591&doi=10.1007%2fs00521-016-2301-0&partnerID=40&md5=63fdd24330f6ce33f6ef0d2d8773f1af","An M-estimator-based online sequential extreme learning machine (M-OSELM) is proposed to predict chaotic time series with outliers. The M-OSELM develops from the online sequential extreme learning machine (OSELM) algorithm and retains the same excellent sequential learning ability as OSELM, but replaces the conventional least-squares cost function with a robust M-estimator-based cost function to enhance the robustness of the model to outliers. By minimizing the M-estimator-based cost function, the possible outliers are prevented from entering the model’s output weights updating scheme. Meanwhile, in the sequential learning process of M-OSELM, a sequential parameter estimation approach based on error sliding window is introduced to estimate the threshold value of the M-estimator function for online outlier detection. Thanks to the built-in median operation and sliding window strategy, this approach is efficient to provide a stable estimator continuously without high computational costs, and then the potential outliers can be effectively detected. Simulation results show that the proposed M-OSELM has an excellent immunity to outliers and can always achieve better performance than its counterparts for prediction of chaotic time series when the training dataset contains outliers, ensuring at the same time all benefits of an online sequential approach. © 2016, The Natural Computing Applications Forum.","Chaotic time series prediction; Extreme learning machine; M-estimator; Online sequential learning; Outliers","Cost estimating; Cost functions; Costs; Forecasting; Knowledge acquisition; Learning systems; Statistics; Time series; Chaotic time series prediction; Extreme learning machine; M-estimators; Outliers; Sequential learning; E-learning",2-s2.0-84963642591
"Zhan T., Fang L., Xu Y.","Prediction of thermal boundary resistance by the machine learning method",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026853036&doi=10.1038%2fs41598-017-07150-7&partnerID=40&md5=77189f4b53e4f327fba1f12582619f60","Thermal boundary resistance (TBR) is a key property for the thermal management of high power micro- and opto-electronic devices and for the development of high efficiency thermal barrier coatings and thermoelectric materials. Prediction of TBR is important for guiding the discovery of interfaces with very low or very high TBR. In this study, we report the prediction of TBR by the machine learning method. We trained machine learning models using the collected experimental TBR data as training data and materials properties that might affect TBR as descriptors. We found that the machine learning models have much better predictive accuracy than the commonly used acoustic mismatch model and diffuse mismatch model. Among the trained models, the Gaussian process regression and the support vector regression models have better predictive accuracy. Also, by comparing the prediction results using different descriptor sets, we found that the film thickness is an important descriptor in the prediction of TBR. These results indicate that machine learning is an accurate and cost-effective method for the prediction of TBR. © 2017 The Author(s).",,"experimental model; prediction; support vector machine; thickness",2-s2.0-85026853036
"Guo D., Juan J., Chang L., Zhang J., Huang D.","Discrimination of plant root zone water status in greenhouse production based on phenotyping and machine learning techniques",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027500070&doi=10.1038%2fs41598-017-08235-z&partnerID=40&md5=7e7503115982692f5149fedab9574df5","Plant-based sensing on water stress can provide sensitive and direct reference for precision irrigation system in greenhouse. However, plant information acquisition, interpretation, and systematical application remain insufficient. This study developed a discrimination method for plant root zone water status in greenhouse by integrating phenotyping and machine learning techniques. Pakchoi plants were used and treated by three root zone moisture levels, 40%, 60%, and 80% relative water content. Three classification models, Random Forest (RF), Neural Network (NN), and Support Vector Machine (SVM) were developed and validated in different scenarios with overall accuracy over 90% for all. SVM model had the highest value, but it required the longest training time. All models had accuracy over 85% in all scenarios, and more stable performance was observed in RF model. Simplified SVM model developed by the top five most contributing traits had the largest accuracy reduction as 29.5%, while simplified RF and NN model still maintained approximately 80%. For real case application, factors such as operation cost, precision requirement, and system reaction time should be synthetically considered in model selection. Our work shows it is promising to discriminate plant root zone water status by implementing phenotyping and machine learning techniques for precision irrigation management. © 2017 The Author(s).",,"greenhouse; model; moisture; nervous system; phenotype; plant root; random forest; reaction time; support vector machine; water availability; water content",2-s2.0-85027500070
"Hou Y., Aldrich C., Lepkova K., Machuca L.L., Kinsella B.","Analysis of electrochemical noise data by use of recurrence quantification analysis and machine learning methods",2017,"Electrochimica Acta",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031115621&doi=10.1016%2fj.electacta.2017.09.169&partnerID=40&md5=fbcffa16efe66ec6a7cd9c42511b0a46","By use of recurrence quantification analysis (RQA), twelve features were extracted from the electrochemical noise signals generated by three types of corrosion: uniform, pitting and passivation. Machine learning methods, i.e. linear discriminant analysis (LDA) and random forests (RF), were used to identify the different corrosion types from those features. Both models gave satisfactory performance, but the RF model showed better prediction accuracy of 93% than the LDA model (88%). Furthermore, an estimation of the importance of the variables by use of the RF model suggested the RQA variables laminarity (LAM) and determinism (DET) played the most significant role with regard to identification of corrosion types. In addition, the comparison of noise resistance with the resistance obtained from EIS measurement showed that the noise resistance can be used for monitoring corrosion rate variations not only for uniform corrosion and passivation, but also for pitting. © 2017","Corrosion type identification; Electrochemical noise; Linear discriminant analysis; Random forest; Recurrence quantification analysis","Artificial intelligence; Corrosion; Corrosion rate; Decision trees; Discriminant analysis; Multivariable control systems; Passivation; Pitting; Corrosion types; Electrochemical noise; Linear discriminant analysis; Random forests; Recurrence quantification analysis; Learning systems",2-s2.0-85031115621
"Banchhor S.K., Londhe N.D., Araki T., Saba L., Radeva P., Laird J.R., Suri J.S.","Wall-based measurement features provides an improved IVUS coronary artery risk assessment when fused with plaque texture-based features during machine learning paradigm",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032272005&doi=10.1016%2fj.compbiomed.2017.10.019&partnerID=40&md5=8152204852258aa0c642daa1cd3db527","Background Planning of percutaneous interventional procedures involves a pre-screening and risk stratification of the coronary artery disease. Current screening tools use stand-alone plaque texture-based features and therefore lack the ability to stratify the risk. Method This IRB approved study presents a novel strategy for coronary artery disease risk stratification using an amalgamation of IVUS plaque texture-based and wall-based measurement features. Due to common genetic plaque makeup, carotid plaque burden was chosen as a gold standard for risk labels during training-phase of machine learning (ML) paradigm. Cross-validation protocol was adopted to compute the accuracy of the ML framework. A set of 59 plaque texture-based features was padded with six wall-based measurement features to show the improvement in stratification accuracy. The ML system was executed using principle component analysis-based framework for dimensionality reduction and uses support vector machine classifier for training and testing-phases. Results The ML system produced a stratification accuracy of 91.28%, demonstrating an improvement of 5.69% when wall-based measurement features were combined with plaque texture-based features. The fused system showed an improvement in mean sensitivity, specificity, positive predictive value, and area under the curve by: 6.39%, 4.59%, 3.31% and 5.48%, respectively when compared to the stand-alone system. While meeting the stability criteria of 5%, the ML system also showed a high average feature retaining power and mean reliability of 89.32% and 98.24%, respectively. Conclusions The ML system showed an improvement in risk stratification accuracy when the wall-based measurement features were fused with the plaque texture-based features. © 2017","Atherosclerosis; Cardiovascular disease; Carotid artery; Coronary arteries; Ultrasound imaging","Artificial intelligence; Blood vessels; Diagnosis; Diseases; Heart; Learning systems; Metals; Principal component analysis; Printing machinery; Stability criteria; Ultrasonic imaging; Atherosclerosis; Cardio-vascular disease; Carotid artery; Coronary arteries; Ultrasound imaging; Risk assessment",2-s2.0-85032272005
"Choi Y., Jimenez H., Mavris D.N.","Two-layer obstacle collision avoidance with machine learning for more energy-efficient unmanned aircraft trajectories",2017,"Robotics and Autonomous Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032258621&doi=10.1016%2fj.robot.2017.09.004&partnerID=40&md5=d33db7bb72f17da46fe448294618b178","This paper proposes a new two-layer obstacle avoidance algorithm that allows an unmanned aircraft system to avoid multiple obstacles with minimal effort. The algorithm includes a global-path optimization that identifies the number of obstacles resulting from a clustering technique based on obstacle information from an airborne sensor, and specifies a potential threat. A local-path trajectory optimization employs a model predictive control structure based on a multi-phase optimal trajectory resulting from approximated dynamics, vehicle constraints, and the result of the global-path optimization. Numerical flight simulations are conducted with a conventional one-layer obstacle avoidance algorithm and the two-layer obstacle avoidance algorithm. The results of the numerical simulation show that the proposed two-layer optimal obstacle avoidance algorithm generates more energy-efficient avoidance trajectories when an unmanned aircraft meets multiple obstacles. © 2017 Elsevier B.V.","Clustering algorithm; Model predictive control; Obstacle avoidance; Optimal trajectory; Path-planning; UAV","Aircraft accidents; Collision avoidance; Energy efficiency; Flight simulators; Learning systems; Model predictive control; Motion planning; Optimization; Trajectories; Unmanned aerial vehicles (UAV); Clustering techniques; Obstacle avoidance algorithms; Obstacle collision; Optimal trajectories; Path optimizations; Trajectory optimization; Unmanned aircraft system; Unmanned aircrafts; Clustering algorithms",2-s2.0-85032258621
"Parodi S., Dosi C., Zambon A., Ferrari E., Muselli M.","Identifying Environmental and Social Factors Predisposing to Pathological Gambling Combining Standard Logistic Regression and Logic Learning Machine",2017,"Journal of Gambling Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032567253&doi=10.1007%2fs10899-017-9679-1&partnerID=40&md5=4c999c8342eded444b07dc6deefd0b31","Identifying potential risk factors for problem gambling (PG) is of primary importance for planning preventive and therapeutic interventions. We illustrate a new approach based on the combination of standard logistic regression and an innovative method of supervised data mining (Logic Learning Machine or LLM). Data were taken from a pilot cross-sectional study to identify subjects with PG behaviour, assessed by two internationally validated scales (SOGS and Lie/Bet). Information was obtained from 251 gamblers recruited in six betting establishments. Data on socio-demographic characteristics, lifestyle and cognitive-related factors, and type, place and frequency of preferred gambling were obtained by a self-administered questionnaire. The following variables associated with PG were identified: instant gratification games, alcohol abuse, cognitive distortion, illegal behaviours and having started gambling with a relative or a friend. Furthermore, the combination of LLM and LR indicated the presence of two different types of PG, namely: (a) daily gamblers, more prone to illegal behaviour, with poor money management skills and who started gambling at an early age, and (b) non-daily gamblers, characterised by superstitious beliefs and a higher preference for immediate reward games. Finally, instant gratification games were strongly associated with the number of games usually played. Studies on gamblers habitually frequently betting shops are rare. The finding of different types of PG by habitual gamblers deserves further analysis in larger studies. Advanced data mining algorithms, like LLM, are powerful tools and potentially useful in identifying risk factors for PG. © 2017, Springer Science+Business Media New York.","Logic Learning Machine; Logistic regression; Problem gambling; ROC analysis",,2-s2.0-85032567253
"Kruglov I., Sergeev O., Yanilkin A., Oganov A.R.","Energy-free machine learning force field for aluminum",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027873825&doi=10.1038%2fs41598-017-08455-3&partnerID=40&md5=1a327ec37bc8da8afb7c870c162e4fcb","We used the machine learning technique of Li et al. (PRL 114, 2015) for molecular dynamics simulations. Atomic configurations were described by feature matrix based on internal vectors, and linear regression was used as a learning technique. We implemented this approach in the LAMMPS code. The method was applied to crystalline and liquid aluminum and uranium at different temperatures and densities, and showed the highest accuracy among different published potentials. Phonon density of states, entropy and melting temperature of aluminum were calculated using this machine learning potential. The results are in excellent agreement with experimental data and results of full ab initio calculations. © 2017 The Author(s).",,,2-s2.0-85027873825
"González-Serrano F.-J., Navia-Vázquez Á., Amor-Martín A.","Training Support Vector Machines with privacy-protected data",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027508986&doi=10.1016%2fj.patcog.2017.06.016&partnerID=40&md5=a6bbbbe83104576e18266c96fcc97c3a","In this paper, we address a machine learning task using encrypted training data. Our basic scenario has three parties: Data Owners, who own private data; an Application, which wants to train and use an arbitrary machine learning model on the Users’ data; and an Authorization Server, which provides Data Owners with public and secret keys of a partial homomorphic cryptosystem (that protects the privacy of their data), authorizes the Application to get access to the encrypted data, and assists it in those computations not supported by the partial homomorphism. As machine learning model, we have selected the Support Vector Machine (SVM) due to its excellent performance in supervised classification tasks. We evaluate two well known SVM algorithms, and we also propose a new semiparametric SVM scheme better suited for the privacy-protected scenario. At the end of the paper, a performance analysis regarding the accuracy and the complexity of the developed algorithms and protocols is presented. © 2017 Elsevier Ltd","Homomorphic encryption; Machine learning; Privacy protection; Support Vector Machines","Artificial intelligence; Cryptography; Data privacy; Learning systems; Supervised learning; Algorithms and protocols; Authorization Servers; Ho-momorphic encryptions; Homomorphic cryptosystem; Machine learning models; Privacy protection; Supervised classification; Training support vector machines; Support vector machines",2-s2.0-85027508986
"Brockherde F., Vogt L., Li L., Tuckerman M.E., Burke K., Müller K.-R.","Bypassing the Kohn-Sham equations with machine learning",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031128428&doi=10.1038%2fs41467-017-00839-3&partnerID=40&md5=35f1414eb7040e96735cd3d85c997809","Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields. Machine learning holds the promise of learning the energy functional via examples, bypassing the need to solve the Kohn-Sham equations. This should yield substantial savings in computer time, allowing larger systems and/or longer time-scales to be tackled, but attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty by directly learning the density-potential and energy-density maps for test systems and various molecules. We perform the first molecular dynamics simulation with a machine-learned density functional on malonaldehyde and are able to capture the intramolecular proton transfer process. Learning density models now allows the construction of accurate density functionals for realistic molecular systems. © 2017 The Author(s).",,,2-s2.0-85031128428
"Paré G., Mao S., Deng W.Q.","A machine-learning heuristic to improve gene score prediction of polygenic traits",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030656412&doi=10.1038%2fs41598-017-13056-1&partnerID=40&md5=5947f48411c7b85188b962cf3997c06d","Machine-learning techniques have helped solve a broad range of prediction problems, yet are not widely used to build polygenic risk scores for the prediction of complex traits. We propose a novel heuristic based on machine-learning techniques (GraBLD) to boost the predictive performance of polygenic risk scores. Gradient boosted regression trees were first used to optimize the weights of SNPs included in the score, followed by a novel regional adjustment for linkage disequilibrium. A calibration set with sample size of ~200 individuals was sufficient for optimal performance. GraBLD yielded prediction R 2 of 0.239 and 0.082 using GIANT summary association statistics for height and BMI in the UK Biobank study (N = 130 K; 1.98 M SNPs), explaining 46.9% and 32.7% of the overall polygenic variance, respectively. For diabetes status, the area under the receiver operating characteristic curve was 0.602 in the UK Biobank study using summary-level association statistics from the DIAGRAM consortium. GraBLD outperformed other polygenic score heuristics for the prediction of height (p &lt; 2.2 × 10-16) and BMI (p &lt; 1.57 × 10-4), and was equivalent to LDpred for diabetes. Results were independently validated in the Health and Retirement Study (N = 8,292; 688,398 SNPs). Our report demonstrates the use of machine-learning techniques, coupled with summary-level data from large genome-wide meta-analyses to improve the prediction of polygenic traits. © 2017 The Author(s).",,,2-s2.0-85030656412
"Leger S., Zwanenburg A., Pilz K., Lohaus F., Linge A., Zöphel K., Kotzerke J., Schreiber A., Tinhofer I., Budach V., Sak A., Stuschke M., Balermpas P., Rödel C., Ganswindt U., Belka C., Pigorsch S., Combs S.E., Mönnich D., Zips D., Krause M., Baumann M., Troost E.G.C., Löck S., Richter C.","A comparative study of machine learning methods for time-To-event survival data for radiomics risk modelling",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031807357&doi=10.1038%2fs41598-017-13448-3&partnerID=40&md5=2ea8621d04bc1af5bc10c2cb230bdb7d","Radiomics applies machine learning algorithms to quantitative imaging data to characterise the tumour phenotype and predict clinical outcome. For the development of radiomics risk models, a variety of different algorithms is available and it is not clear which one gives optimal results. Therefore, we assessed the performance of 11 machine learning algorithms combined with 12 feature selection methods by the concordance index (C-Index), to predict loco-regional tumour control (LRC) and overall survival for patients with head and neck squamous cell carcinoma. The considered algorithms are able to deal with continuous time-To-event survival data. Feature selection and model building were performed on a multicentre cohort (213 patients) and validated using an independent cohort (80 patients). We found several combinations of machine learning algorithms and feature selection methods which achieve similar results, e.g., MSR-RF: C-Index = 0.71 and BT-COX: C-Index = 0.70 in combination with Spearman feature selection. Using the best performing models, patients were stratified into groups of low and high risk of recurrence. Significant differences in LRC were obtained between both groups on the validation cohort. Based on the presented analysis, we identified a subset of algorithms which should be considered in future radiomics studies to develop stable and clinically relevant predictive models for time-To-event endpoints. © 2017 The Author(s).",,,2-s2.0-85031807357
"Zhu H., Chu B., Zhang C., Liu F., Jiang L., He Y.","Hyperspectral Imaging for Presymptomatic Detection of Tobacco Disease with Successive Projections Algorithm and Machine-learning Classifiers",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021430611&doi=10.1038%2fs41598-017-04501-2&partnerID=40&md5=0762f4ebc7c7daa0c6ec6ebd7cc07ba0","We investigated the feasibility and potentiality of presymptomatic detection of tobacco disease using hyperspectral imaging, combined with the variable selection method and machine-learning classifiers. Images from healthy and TMV-infected leaves with 2, 4, and 6 days post infection were acquired by a pushbroom hyperspectral reflectance imaging system covering the spectral range of 380-1023 nm. Successive projections algorithm was evaluated for effective wavelengths (EWs) selection. Four texture features, including contrast, correlation, entropy, and homogeneity were extracted according to grey-level co-occurrence matrix (GLCM). Additionally, different machine-learning algorithms were developed and compared to detect and classify disease stages with EWs, texture features and data fusion respectively. The performance of chemometric models with data fusion manifested better results with classification accuracies of calibration and prediction all above 80% than those only using EWs or texture features; the accuracies were up to 95% employing back propagation neural network (BPNN), extreme learning machine (ELM), and least squares support vector machine (LS-SVM) models. Hence, hyperspectral imaging has the potential as a fast and non-invasive method to identify infected leaves in a short period of time (i.e. 48 h) in comparison to the reference images (5 days for visible symptoms of infection, 11 days for typical symptoms). © 2017 The Author(s).",,,2-s2.0-85021430611
"Ghanat Bari M., Ung C.Y., Zhang C., Zhu S., Li H.","Machine Learning-Assisted Network Inference Approach to Identify a New Class of Genes that Coordinate the Functionality of Cancer Networks",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026788304&doi=10.1038%2fs41598-017-07481-5&partnerID=40&md5=d06189ed09fcff563f298aa66f7480ae","Emerging evidence indicates the existence of a new class of cancer genes that act as ""signal linkers"" coordinating oncogenic signals between mutated and differentially expressed genes. While frequently mutated oncogenes and differentially expressed genes, which we term Class I cancer genes, are readily detected by most analytical tools, the new class of cancer-related genes, i.e., Class II, escape detection because they are neither mutated nor differentially expressed. Given this hypothesis, we developed a Machine Learning-Assisted Network Inference (MALANI) algorithm, which assesses all genes regardless of expression or mutational status in the context of cancer etiology. We used 8807 expression arrays, corresponding to 9 cancer types, to build more than 2 × 108 Support Vector Machine (SVM) models for reconstructing a cancer network. We found that ~3% of ~19,000 not differentially expressed genes are Class II cancer gene candidates. Some Class II genes that we found, such as SLC19A1 and ATAD3B, have been recently reported to associate with cancer outcomes. To our knowledge, this is the first study that utilizes both machine learning and network biology approaches to uncover Class II cancer genes in coordinating functionality in cancer networks and will illuminate our understanding of how genes are modulated in a tissue-specific network contribute to tumorigenesis and therapy development. © 2017 The Author(s).",,,2-s2.0-85026788304
"Yoo K.D., Noh J., Lee H., Kim D.K., Lim C.S., Kim Y.H., Lee J.P., Kim G., Kim Y.S.","A Machine Learning Approach Using Survival Statistics to Predict Graft Survival in Kidney Transplant Recipients: A Multicenter Cohort Study",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027872451&doi=10.1038%2fs41598-017-08008-8&partnerID=40&md5=e71f712e4d58275234d196bcb83761df","Accurate prediction of graft survival after kidney transplant is limited by the complexity and heterogeneity of risk factors influencing allograft survival. In this study, we applied machine learning methods, in combination with survival statistics, to build new prediction models of graft survival that included immunological factors, as well as known recipient and donor variables. Graft survival was estimated from a retrospective analysis of the data from a multicenter cohort of 3,117 kidney transplant recipients. We evaluated the predictive power of ensemble learning algorithms (survival decision tree, bagging, random forest, and ridge and lasso) and compared outcomes to those of conventional models (decision tree and Cox regression). Using a conventional decision tree model, the 3-month serum creatinine level post-transplant (cut-off, 1.65 mg/dl) predicted a graft failure rate of 77.8% (index of concordance, 0.71). Using a survival decision tree model increased the index of concordance to 0.80, with the episode of acute rejection during the first year post-transplant being associated with a 4.27-fold increase in the risk of graft failure. Our study revealed that early acute rejection in the first year is associated with a substantially increased risk of graft failure. Machine learning methods may provide versatile and feasible tools for forecasting graft survival. © 2017 The Author(s).",,,2-s2.0-85027872451
"Pan L., Liu G., Lin F., Zhong S., Xia H., Sun X., Liang H.","Machine learning applications for prediction of relapse in childhood acute lymphoblastic leukemia",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027240620&doi=10.1038%2fs41598-017-07408-0&partnerID=40&md5=52f6b555bdb8fa8b4c8a0422b1c5aea0","The prediction of relapse in childhood acute lymphoblastic leukemia (ALL) is a critical factor for successful treatment and follow-up planning. Our goal was to construct an ALL relapse prediction model based on machine learning algorithms. Monte Carlo cross-validation nested by 10-fold cross-validation was used to rank clinical variables on the randomly split training sets of 336 newly diagnosed ALL children, and a forward feature selection algorithm was employed to find the shortest list of most discriminatory variables. To enable an unbiased estimation of the prediction model to new patients, besides the split test sets of 150 patients, we introduced another independent data set of 84 patients to evaluate the model. The Random Forest model with 14 features achieved a cross-validation accuracy of 0.827 ± 0.031 on one set and an accuracy of 0.798 on the other, with the area under the curve of 0.902 ± 0.027 and 0.904, respectively. The model performed well across different risk-level groups, with the best accuracy of 0.829 in the standard-risk group. To our knowledge, this is the first study to use machine learning models to predict childhood ALL relapse based on medical data from Electronic Medical Record, which will further facilitate stratification treatments. © 2017 The Author(s).",,,2-s2.0-85027240620
"Kobayashi H., Lei C., Wu Y., Mao A., Jiang Y., Guo B., Ozeki Y., Goda K.","Label-free detection of cellular drug responses by high-throughput bright-field imaging and machine learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030317641&doi=10.1038%2fs41598-017-12378-4&partnerID=40&md5=975199f71631c5ffb6cc1b3960b4758a","In the last decade, high-content screening based on multivariate single-cell imaging has been proven effective in drug discovery to evaluate drug-induced phenotypic variations. Unfortunately, this method inherently requires fluorescent labeling which has several drawbacks. Here we present a label-free method for evaluating cellular drug responses only by high-throughput bright-field imaging with the aid of machine learning algorithms. Specifically, we performed high-throughput bright-field imaging of numerous drug-treated and -untreated cells (N = ∼240,000) by optofluidic time-stretch microscopy with high throughput up to 10,000 cells/s and applied machine learning to the cell images to identify their morphological variations which are too subtle for human eyes to detect. Consequently, we achieved a high accuracy of 92% in distinguishing drug-treated and -untreated cells without the need for labeling. Furthermore, we also demonstrated that dose-dependent, drug-induced morphological change from different experiments can be inferred from the classification accuracy of a single classification model. Our work lays the groundwork for label-free drug screening in pharmaceutical science and industry. © 2017 The Author(s).",,,2-s2.0-85030317641
"Yoon J., Jo Y., Kim M.-H., Kim K., Lee S., Kang S.-J., Park Y.","Identification of non-activated lymphocytes using three-dimensional refractive index tomography and machine learning",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026410993&doi=10.1038%2fs41598-017-06311-y&partnerID=40&md5=6521ad6809aa2080791e7283f2cff9b9","Identification of lymphocyte cell types are crucial for understanding their pathophysiological roles in human diseases. Current methods for discriminating lymphocyte cell types primarily rely on labelling techniques with magnetic beads or fluorescence agents, which take time and have costs for sample preparation and may also have a potential risk of altering cellular functions. Here, we present the identification of non-activated lymphocyte cell types at the single-cell level using refractive index (RI) tomography and machine learning. From the measurements of three-dimensional RI maps of individual lymphocytes, the morphological and biochemical properties of the cells are quantitatively retrieved. To construct cell type classification models, various statistical classification algorithms are compared, and the k-NN (k = 4) algorithm was selected. The algorithm combines multiple quantitative characteristics of the lymphocyte to construct the cell type classifiers. After optimizing the feature sets via cross-validation, the trained classifiers enable identification of three lymphocyte cell types (B, CD4+ T, and CD8+ T cells) with high sensitivity and specificity. The present method, which combines RI tomography and machine learning for the first time to our knowledge, could be a versatile tool for investigating the pathophysiological roles of lymphocytes in various diseases including cancers, autoimmune diseases, and virus infections. © 2017 The Author(s).",,,2-s2.0-85026410993
"Ing N., Huang F., Conley A., You S., Ma Z., Klimov S., Ohe C., Yuan X., Amin M.B., Figlin R., Gertych A., Knudsen B.S.","A novel machine learning approach reveals latent vascular phenotypes predictive of renal cancer outcome",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031716982&doi=10.1038%2fs41598-017-13196-4&partnerID=40&md5=5abe590059a2019685633916210704b4","Gene expression signatures are commonly used as predictive biomarkers, but do not capture structural features within the tissue architecture. Here we apply a 2-step machine learning framework for quantitative imaging of tumor vasculature to derive a spatially informed, prognostic gene signature. The trained algorithms classify endothelial cells and generate a vascular area mask (VAM) in H&E micrographs of clear cell renal cell carcinoma (ccRCC) cases from The Cancer Genome Atlas (TCGA). Quantification of VAMs led to the discovery of 9 vascular features (9VF) that predicted disease-free-survival in a discovery cohort (n = 64, HR = 2.3). Correlation analysis and information gain identified a 14 gene expression signature related to the 9VF's. Two generalized linear models with elastic net regularization (14VF and 14GT), based on the 14 genes, separated independent cohorts of up to 301 cases into good and poor disease-free survival groups (14VF HR = 2.4, 14GT HR = 3.33). For the first time, we successfully applied digital image analysis and targeted machine learning to develop prognostic, morphology-based, gene expression signatures from the vascular architecture. This novel morphogenomic approach has the potential to improve previous methods for biomarker development. © 2017 The Author(s).",,,2-s2.0-85031716982
"Waldstein S.M., Montuoro A., Podkowinski D., Philip A.-M., Gerendas B.S., Bogunovic H., Schmidt-Erfurth U.","Evaluating the impact of vitreomacular adhesion on anti-VEGF therapy for retinal vein occlusion using machine learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020401396&doi=10.1038%2fs41598-017-02971-y&partnerID=40&md5=9716ebc86401999eade22dfc5fc6e5f3","Vitreomacular adhesion (VMA) represents a prognostic biomarker in the management of exudative macular disease using anti-vascular endothelial growth factor (VEGF) agents. However, manual evaluation of VMA in 3D optical coherence tomography (OCT) is laborious and data on its impact on therapy of retinal vein occlusion (RVO) are limited. The aim of this study was to (1) develop a fully automated segmentation algorithm for the posterior vitreous boundary and (2) to study the effect of VMA on anti-VEGF therapy for RVO. A combined machine learning/graph cut segmentation algorithm for the posterior vitreous boundary was designed and evaluated. 391 patients with central/branch RVO under standardized ranibizumab treatment for 6/12 months were included in a systematic post-hoc analysis. VMA (70%) was automatically differentiated from non-VMA (30%) using the developed method combined with unsupervised clustering. In this proof-of-principle study, eyes with VMA showed larger BCVA gains than non-VMA eyes (BRVO: 15 ± 12 vs. 11 ± 11 letters, p = 0.02; CRVO: 18 ± 14 vs. 9 ± 13 letters, p < 0.01) and received a similar number of retreatments. However, this association diminished after adjustment for baseline BCVA, also when using more fine-grained VMA classes. Our study illustrates that machine learning represents a promising path to assess imaging biomarkers in OCT. © 2017 The Author(s).",,,2-s2.0-85020401396
"Feng Z., Yang Z., Jin L., Huang S., Sun J.","Robust shared feature learning for script and handwritten/machine-printed identification",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029715661&doi=10.1016%2fj.patrec.2017.09.016&partnerID=40&md5=754a0273da95ac37248db10c2d4857c0","In this paper, we focus on the problem of script and handwritten/machine-printed identification of texts. We simultaneously identify the script (Chinese, English, Japanese, Korean, or Russian) and whether it is handwritten or machine-printed text by designing a dual-branch structured deep convolutional neural network (CNN). For the training stage, we propose a two-stage multi-task learning strategy to learn robust shared features for script and handwritten/machine-printed identification. Accordingly, we can implement two identification tasks using the proposed single CNN model. We compare the effects of using different length of input to train CNN. The experimental results show that text-line input is a suitable choice for the two identification tasks, as it can effectively capture more discriminative content for both script and handwritten/machine-printed identification. Furthermore, we evaluate three CNN networks of different scales (small, medium, and large) to determine the best CNN architecture for script and handwritten/machine-printed identification. As shown by our experimental validation, integrating the text-line input with larger architecture significantly improves performance. The accuracies achieved by the two-stage multi-task CNN for handwritten/machine-printed and script identification are 99% and 95%, respectively. © 2017 Elsevier B.V.",,"Deep neural networks; Learning systems; Neural networks; CNN models; CNN network; Convolutional neural network; Experimental validations; Feature learning; Multitask learning; Printed texts; Script identification; Network architecture",2-s2.0-85029715661
"Simpraga S., Alvarez-Jimenez R., Mansvelder H.D., Van Gerven J.M.A., Groeneveld G.J., Poil S.-S., Linkenkaer-Hansen K.","EEG machine learning for accurate detection of cholinergic intervention and Alzheimer's disease",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024914732&doi=10.1038%2fs41598-017-06165-4&partnerID=40&md5=00071998bcc2efe183fd094c13702c30","Monitoring effects of disease or therapeutic intervention on brain function is increasingly important for clinical trials, albeit hampered by inter-individual variability and subtle effects. Here, we apply complementary biomarker algorithms to electroencephalography (EEG) recordings to capture the brain's multi-faceted signature of disease or pharmacological intervention and use machine learning to improve classification performance. Using data from healthy subjects receiving scopolamine we developed an index of the muscarinic acetylcholine receptor antagonist (mAChR) consisting of 14 EEG biomarkers. This mAChR index yielded higher classification performance than any single EEG biomarker with cross-validated accuracy, sensitivity, specificity and precision ranging from 88-92%. The mAChR index also discriminated healthy elderly from patients with Alzheimer's disease (AD); however, an index optimized for AD pathophysiology provided a better classification. We conclude that integrating multiple EEG biomarkers can enhance the accuracy of identifying disease or drug interventions, which is essential for clinical trials. © 2017 The Author(s).",,,2-s2.0-85024914732
"Abbot J., Marohasy J.","The application of machine learning for evaluating anthropogenic versus natural climate change",2017,"GeoResJ",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028312816&doi=10.1016%2fj.grj.2017.08.001&partnerID=40&md5=b27050fe5aae1032086e4fa47e3af3a1","Time-series profiles derived from temperature proxies such as tree rings can provide information about past climate. Signal analysis was undertaken of six such datasets, and the resulting component sine waves used as input to an artificial neural network (ANN), a form of machine learning. By optimizing spectral features of the component sine waves, such as periodicity, amplitude and phase, the original temperature profiles were approximately simulated for the late Holocene period to 1830 CE. The ANN models were then used to generate projections of temperatures through the 20th century. The largest deviation between the ANN projections and measured temperatures for six geographically distinct regions was approximately 0.2 °C, and from this an Equilibrium Climate Sensitivity (ECS) of approximately 0.6 °C was estimated. This is considerably less than estimates from the General Circulation Models (GCMs) used by the Intergovernmental Panel on Climate Change (IPCC), and similar to estimates from spectroscopic methods. © 2017 Elsevier Ltd",,,2-s2.0-85028312816
"Schubach M., Re M., Robinson P.N., Valentini G.","Imbalance-Aware Machine Learning for Predicting Rare and Common Disease-Associated Non-Coding Variants",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020406342&doi=10.1038%2fs41598-017-03011-5&partnerID=40&md5=41db8ce71ba4218c081294158b59cceb","Disease and trait-associated variants represent a tiny minority of all known genetic variation, and therefore there is necessarily an imbalance between the small set of available disease-associated and the much larger set of non-deleterious genomic variation, especially in non-coding regulatory regions of human genome. Machine Learning (ML) methods for predicting disease-associated non-coding variants are faced with a chicken and egg problem - such variants cannot be easily found without ML, but ML cannot begin to be effective until a sufficient number of instances have been found. Most of state-of-the-art ML-based methods do not adopt specific imbalance-aware learning techniques to deal with imbalanced data that naturally arise in several genome-wide variant scoring problems, thus resulting in a significant reduction of sensitivity and precision. We present a novel method that adopts imbalance-aware learning strategies based on resampling techniques and a hyper-ensemble approach that outperforms state-of-the-art methods in two different contexts: the prediction of non-coding variants associated with Mendelian and with complex diseases. We show that imbalance-aware ML is a key issue for the design of robust and accurate prediction algorithms and we provide a method and an easy-to-use software tool that can be effectively applied to this challenging prediction task. © 2017 The Author(s).",,,2-s2.0-85020406342
"Ohsugi H., Tabuchi H., Enno H., Ishitobi N.","Accuracy of deep learning, a machine-learning technology, using ultra-wide-field fundus ophthalmoscopy for detecting rhegmatogenous retinal detachment",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028359586&doi=10.1038%2fs41598-017-09891-x&partnerID=40&md5=4a65da8ea5695cf3e93a9f737e78fcdd","Rhegmatogenous retinal detachment (RRD) is a serious condition that can lead to blindness; however, it is highly treatable with timely and appropriate treatment. Thus, early diagnosis and treatment of RRD is crucial. In this study, we applied deep learning, a machine-learning technology, to detect RRD using ultra-wide-field fundus images and investigated its performance. In total, 411 images (329 for training and 82 for grading) from 407 RRD patients and 420 images (336 for training and 84 for grading) from 238 non-RRD patients were used in this study. The deep learning model demonstrated a high sensitivity of 97.6% [95% confidence interval (CI), 94.2-100%] and a high specificity of 96.5% (95% CI, 90.2-100%), and the area under the curve was 0.988 (95% CI, 0.981-0.995). This model can improve medical care in remote areas where eye clinics are not available by using ultra-wide-field fundus ophthalmoscopy for the accurate diagnosis of RRD. Early diagnosis of RRD can prevent blindness. © 2017 The Author(s).",,,2-s2.0-85028359586
"Lezcano-Valverde J.M., Salazar F., León L., Toledano E., Jover J.A., Fernandez-Gutierrez B., Soudah E., González-Álvaro I., Abasolo L., Rodriguez-Rodriguez L.","Development and validation of a multivariate predictive model for rheumatoid arthritis mortality using a machine learning approach",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028651814&doi=10.1038%2fs41598-017-10558-w&partnerID=40&md5=783a989cdbf793c633a4a023708f0081","We developed and independently validated a rheumatoid arthritis (RA) mortality prediction model using the machine learning method Random Survival Forests (RSF). Two independent cohorts from Madrid (Spain) were used: the Hospital Clínico San Carlos RA Cohort (HCSC-RAC; training; 1,461 patients), and the Hospital Universitario de La Princesa Early Arthritis Register Longitudinal study (PEARL; validation; 280 patients). Demographic and clinical-related variables collected during the first two years after disease diagnosis were used. 148 and 21 patients from HCSC-RAC and PEARL died during a median follow-up time of 4.3 and 5.0 years, respectively. Age at diagnosis, median erythrocyte sedimentation rate, and number of hospital admissions showed the higher predictive capacity. Prediction errors in the training and validation cohorts were 0.187 and 0.233, respectively. A survival tree identified five mortality risk groups using the predicted ensemble mortality. After 1 and 7 years of follow-up, time-dependent specificity and sensitivity in the validation cohort were 0.79-0.80 and 0.43-0.48, respectively, using the cut-off value dividing the two lower risk categories. Calibration curves showed overestimation of the mortality risk in the validation cohort. In conclusion, we were able to develop a clinical prediction model for RA mortality using RSF, providing evidence for further work on external validation. © 2017 The Author(s).",,,2-s2.0-85028651814
"Karamzadeh R., Karimi-Jafari M.H., Sharifi-Zarchi A., Chitsaz H., Salekdeh G.H., Moosavi-Movahedi A.A.","Machine Learning and Network Analysis of Molecular Dynamics Trajectories Reveal Two Chains of Red/Ox-specific Residue Interactions in Human Protein Disulfide Isomerase",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020897876&doi=10.1038%2fs41598-017-03966-5&partnerID=40&md5=43fad26e845e84f5dbba64f0a8a7b21d","The human protein disulfide isomerase (hPDI), is an essential four-domain multifunctional enzyme. As a result of disulfide shuffling in its terminal domains, hPDI exists in two oxidation states with different conformational preferences which are important for substrate binding and functional activities. Here, we address the redox-dependent conformational dynamics of hPDI through molecular dynamics (MD) simulations. Collective domain motions are identified by the principal component analysis of MD trajectories and redox-dependent opening-closing structure variations are highlighted on projected free energy landscapes. Then, important structural features that exhibit considerable differences in dynamics of redox states are extracted by statistical machine learning methods. Mapping the structural variations to time series of residue interaction networks also provides a holistic representation of the dynamical redox differences. With emphasizing on persistent long-lasting interactions, an approach is proposed that compiled these time series networks to a single dynamic residue interaction network (DRIN). Differential comparison of DRIN in oxidized and reduced states reveals chains of residue interactions that represent potential allosteric paths between catalytic and ligand binding sites of hPDI. © 2017 The Author(s).",,,2-s2.0-85020897876
"Mossotto E., Ashton J.J., Coelho T., Beattie R.M., MacArthur B.D., Ennis S.","Classification of Paediatric Inflammatory Bowel Disease using Machine Learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019713721&doi=10.1038%2fs41598-017-02606-2&partnerID=40&md5=354524ed4af3860e4b26f592dbf7e03c","Paediatric inflammatory bowel disease (PIBD), comprising Crohn's disease (CD), ulcerative colitis (UC) and inflammatory bowel disease unclassified (IBDU) is a complex and multifactorial condition with increasing incidence. An accurate diagnosis of PIBD is necessary for a prompt and effective treatment. This study utilises machine learning (ML) to classify disease using endoscopic and histological data for 287 children diagnosed with PIBD. Data were used to develop, train, test and validate a ML model to classify disease subtype. Unsupervised models revealed overlap of CD/UC with broad clustering but no clear subtype delineation, whereas hierarchical clustering identified four novel subgroups characterised by differing colonic involvement. Three supervised ML models were developed utilising endoscopic data only, histological only and combined endoscopic/histological data yielding classification accuracy of 71.0%, 76.9% and 82.7% respectively. The optimal combined model was tested on a statistically independent cohort of 48 PIBD patients from the same clinic, accurately classifying 83.3% of patients. This study employs mathematical modelling of endoscopic and histological data to aid diagnostic accuracy. While unsupervised modelling categorises patients into four subgroups, supervised approaches confirm the need of both endoscopic and histological evidence for an accurate diagnosis. Overall, this paper provides a blueprint for ML use with clinical data. © 2017 The Author(s).",,,2-s2.0-85019713721
"Fan K., Zhang S., Zhang Y., Lu J., Holcombe M., Zhang X.","A Machine Learning Assisted, Label-free, Non-invasive Approach for Somatic Reprogramming in Induced Pluripotent Stem Cell Colony Formation Detection and Prediction",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031912437&doi=10.1038%2fs41598-017-13680-x&partnerID=40&md5=f577e08058f7ff2cb05df65083aad38c","During cellular reprogramming, the mesenchymal-to-epithelial transition is accompanied by changes in morphology, which occur prior to iPSC colony formation. The current approach for detecting morphological changes associated with reprogramming purely relies on human experiences, which involve intensive amounts of upfront training, human error with limited quality control and batch-to-batch variations. Here, we report a time-lapse-based bright-field imaging analysis system that allows us to implement a label-free, non-invasive approach to measure morphological dynamics. To automatically analyse and determine iPSC colony formation, a machine learning-based classification, segmentation, and statistical modelling system was developed to guide colony selection. The system can detect and monitor the earliest cellular texture changes after the induction of reprogramming in human somatic cells on day 7 from the 20-24 day process. Moreover, after determining the reprogramming process and iPSC colony formation quantitatively, a mathematical model was developed to statistically predict the best iPSC selection phase independent of any other resources. All the computational detection and prediction experiments were evaluated using a validation dataset, and biological verification was performed. These algorithm-detected colonies show no significant differences (Pearson Coefficient) in terms of their biological features compared to the manually processed colonies using standard molecular approaches. © 2017 The Author(s).",,,2-s2.0-85031912437
"Rostam H.M., Reynolds P.M., Alexander M.R., Gadegaard N., Ghaemmaghami A.M.","Image based Machine Learning for identification of macrophage subsets",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020693852&doi=10.1038%2fs41598-017-03780-z&partnerID=40&md5=622622828c38a254c3482493480f7b3a","Macrophages play a crucial rule in orchestrating immune responses against pathogens and foreign materials. Macrophages have remarkable plasticity in response to environmental cues and are able to acquire a spectrum of activation status, best exemplified by pro-inflammatory (M1) and anti-inflammatory (M2) phenotypes at the two ends of the spectrum. Characterisation of M1 and M2 subsets is usually carried out by quantification of multiple cell surface markers, transcription factors and cytokine profiles. These approaches are time-consuming, require large numbers of cells and are resource intensive. In this study, we used machine learning algorithms to develop a simple and fast imaging-based approach that enables automated identification of different macrophage functional phenotypes using their cell size and morphology. Fluorescent microscopy was used to assess cell morphology of different cell types which were stained for nucleus and actin distribution using DAPI and phalloidin respectively. By only analysing their morphology we were able to identify M1 and M2 phenotypes effectively and could distinguish them from naïve macrophages and monocytes with an average accuracy of 90%. Thus we suggest high-content and automated image analysis can be used for fast phenotyping of functionally diverse cell populations with reasonable accuracy and without the need for using multiple markers. © 2017 The Author(s).",,,2-s2.0-85020693852
"Sohn K.-S., Chung J., Cho M.-Y., Timilsina S., Park W.B., Pyo M., Shin N., Sohn K., Kim J.S.","An extremely simple macroscale electronic skin realized by deep machine learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029230488&doi=10.1038%2fs41598-017-11663-6&partnerID=40&md5=9bb5e5ee518493dcf5ee5f7fddd95e33","Complicated structures consisting of multi-layers with a multi-modal array of device components, i.e., so-called patterned multi-layers, and their corresponding circuit designs for signal readout and addressing are used to achieve a macroscale electronic skin (e-skin). In contrast to this common approach, we realized an extremely simple macroscale e-skin only by employing a single-layered piezoresistive MWCNT-PDMS composite film with neither nano-, micro-, nor macro-patterns. It is the deep machine learning that made it possible to let such a simple bulky material play the role of a smart sensory device. A deep neural network (DNN) enabled us to process electrical resistance change induced by applied pressure and thereby to instantaneously evaluate the pressure level and the exact position under pressure. The great potential of this revolutionary concept for the attainment of pressure-distribution sensing on a macroscale area could expand its use to not only e-skin applications but to other high-end applications such as touch panels, portable flexible keyboard, sign language interpreting globes, safety diagnosis of social infrastructures, and the diagnosis of motility and peristalsis disorders in the gastrointestinal tract. © 2017 The Author(s).",,,2-s2.0-85029230488
"Jamal S., Goyal S., Shanker A., Grover A.","Predicting neurological Adverse Drug Reactions based on biological, chemical and phenotypic properties of drugs using machine learning models",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018186464&doi=10.1038%2fs41598-017-00908-z&partnerID=40&md5=05810748d571721a06f3a3bcadea0fdf","Adverse drug reactions (ADRs) have become one of the primary reasons for the failure of drugs and a leading cause of deaths. Owing to the severe effects of ADRs, there is an urgent need for the generation of effective models which can accurately predict ADRs during early stages of drug development based on integration of various features of drugs. In the current study, we have focused on neurological ADRs and have used various properties of drugs that include biological properties (targets, transporters and enzymes), chemical properties (substructure fingerprints), phenotypic properties (side effects (SE) and therapeutic indications) and a combinations of the two and three levels of features. We employed relief-based feature selection technique to identify relevant properties and used machine learning approach to generated learned model systems which would predict neurological ADRs prior to preclinical testing. Additionally, in order to explain the efficiency and applicability of the models, we tested them to predict the ADRs for already existing anti-Alzheimer drugs and uncharacterized drugs, respectively in side effect resource (SIDER) database. The generated models were highly accurate and our results showed that the models based on chemical (accuracy 93.20%), phenotypic (accuracy 92.41%) and combination of three properties (accuracy 94.18%) were highly accurate while the models based on biological properties (accuracy 82.11%) were highly informative. © The Author(s) 2017.",,,2-s2.0-85018186464
"Zhang B., Wan X., Ouyang F.-S., Dong Y.-H., Luo D.-H., Liu J., Liang L., Chen W.-B., Luo X.-N., Mo X.-K., Zhang L., Huang W.-H., Pei S.-F., Guo B.-L., Liang C.-H., Lian Z.-Y., Zhang S.-X.","Machine Learning Algorithms for Risk Prediction of Severe Hand-Foot-Mouth Disease in Children",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024390108&doi=10.1038%2fs41598-017-05505-8&partnerID=40&md5=767e12e5fdd9136edd892e9a6d19740b","The identification of indicators for severe HFMD is critical for early prevention and control of the disease. With this goal in mind, 185 severe and 345 mild HFMD cases were assessed. Patient demographics, clinical features, MRI findings, and laboratory test results were collected. Gradient boosting tree (GBT) was then used to determine the relative importance (RI) and interaction effects of the variables. Results indicated that elevated white blood cell (WBC) count &gt; 15 × 109/L (RI: 49.47, p &lt; 0.001) was the top predictor of severe HFMD, followed by spinal cord involvement (RI: 26.62, p &lt; 0.001), spinal nerve roots involvement (RI: 10.34, p &lt; 0.001), hyperglycemia (RI: 3.40, p &lt; 0.001), and brain or spinal meninges involvement (RI: 2.45, p = 0.003). Interactions between elevated WBC count and hyperglycemia (H statistic: 0.231, 95% CI: 0-0.262, p = 0.031), between spinal cord involvement and duration of fever ≥3 days (H statistic: 0.291, 95% CI: 0.035-0.326, p = 0.035), and between brainstem involvement and body temperature (H statistic: 0.313, 95% CI: 0-0.273, p = 0.017) were observed. Therefore, GBT is capable to identify the predictors for severe HFMD and their interaction effects, outperforming conventional regression methods. © 2017 The Author(s).",,,2-s2.0-85024390108
"Fernandes R., Dsouza G.L R.","A New Approach to Predict user Mobility Using Semantic Analysis and Machine Learning",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032001303&doi=10.1007%2fs10916-017-0837-x&partnerID=40&md5=87f490912bd00ffaddfda7d0b169d044","Mobility prediction is a technique in which the future location of a user is identified in a given network. Mobility prediction provides solutions to many day-to-day life problems. It helps in seamless handovers in wireless networks to provide better location based services and to recalculate paths in Mobile Ad hoc Networks (MANET). In the present study, a framework is presented which predicts user mobility in presence and absence of mobility history. Naïve Bayesian classification algorithm and Markov Model are used to predict user future location when user mobility history is available. An attempt is made to predict user future location by using Short Message Service (SMS) and instantaneous Geological coordinates in the absence of mobility patterns. The proposed technique compares the performance metrics with commonly used Markov Chain model. From the experimental results it is evident that the techniques used in this work gives better results when considering both spatial and temporal information. The proposed method predicts user’s future location in the absence of mobility history quite fairly. The proposed work is applied to predict the mobility of medical rescue vehicles and social security systems. © 2017, Springer Science+Business Media, LLC.","Global Positioning System (GPS) coordinates; Instantaneous prediction; Markov chain model; Mobility prediction; Naïve Bayesian classifier; Short Message Service (SMS)","classification algorithm; classifier; global positioning system; human; Markov chain; prediction; social security",2-s2.0-85032001303
"Jahani E., Sundsøy P., Bjelland J., Bengtsson L., Pentland A.S., de Montjoye Y.-A.","Erratum to: Improving official statistics in emerging markets using machine learning and mobile phone data(EPJ Data Science, 10.1140/epjds/s13688-017-0099-3)",2017,"EPJ Data Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020424627&doi=10.1140%2fepjds%2fs13688-017-0106-8&partnerID=40&md5=df24193dcb83cb71d6644065de725cb2","Upon publication of the original article [1], it was noticed that in the Availability of data materials section, the link to the code ‘https://github.com/eamanj/demographics_ prediction’ was incorrectly given as ‘https://github.edu/eamanj/demographics_ prediction’. This has now been acknowledged and corrected in this erratum. © 2017, The Author(s).",,,2-s2.0-85020424627
"Boland M.R., Polubriaginof F., Tatonetti N.P.","Development of A Machine Learning Algorithm to Classify Drugs of Unknown Fetal Effect",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031047977&doi=10.1038%2fs41598-017-12943-x&partnerID=40&md5=d6750680e0387587ac2d72c5c90021d0","Many drugs commonly prescribed during pregnancy lack a fetal safety recommendation-called FDA 'category C' drugs. This study aims to classify these drugs into harmful and safe categories using knowledge gained from chemoinformatics (i.e., pharmacological similarity with drugs of known fetal effect) and empirical data (i.e., derived from Electronic Health Records). Our fetal loss cohort contains 14,922 affected and 33,043 unaffected pregnancies and our congenital anomalies cohort contains 5,658 affected and 31,240 unaffected infants. We trained a random forest to classify drugs of unknown pregnancy class into harmful or safe categories, focusing on two distinct outcomes: fetal loss and congenital anomalies. Our models achieved an out-of-bag accuracy of 91% for fetal loss and 87% for congenital anomalies outperforming null models. Fifty-seven 'category C' medications were classified as harmful for fetal loss and eleven for congenital anomalies. This includes medications with documented harmful effects, including naproxen, ibuprofen and rubella live vaccine. We also identified several novel drugs, e.g., haloperidol, that increased the risk of fetal loss. Our approach provides important information on the harmfulness of 'category C' drugs. This is needed, as no FDA recommendation exists for these drugs' fetal safety. © 2017 The Author(s).",,,2-s2.0-85031047977
"Myers P.D., Scirica B.M., Stultz C.M.","Machine Learning Improves Risk Stratification after Acute Coronary Syndrome",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030665286&doi=10.1038%2fs41598-017-12951-x&partnerID=40&md5=e00de290c76d5f87e0b37cb336acdd50","The accurate assessment of a patient's risk of adverse events remains a mainstay of clinical care. Commonly used risk metrics have been based on logistic regression models that incorporate aspects of the medical history, presenting signs and symptoms, and lab values. More sophisticated methods, such as Artificial Neural Networks (ANN), form an attractive platform to build risk metrics because they can easily incorporate disparate pieces of data, yielding classifiers with improved performance. Using two cohorts consisting of patients admitted with a non-ST-segment elevation acute coronary syndrome, we constructed an ANN that identifies patients at high risk of cardiovascular death (CVD). The ANN was trained and tested using patient subsets derived from a cohort containing 4395 patients (Area Under the Curve (AUC) 0.743) and validated on an independent holdout set containing 861 patients (AUC 0.767). The ANN 1-year Hazard Ratio for CVD was 3.72 (95% confidence interval 1.04-14.3) after adjusting for the TIMI Risk Score, left ventricular ejection fraction, and B-type natriuretic peptide. A unique feature of our approach is that it captures small changes in the ST segment over time that cannot be detected by visual inspection. These findings highlight the important role that ANNs can play in risk stratification. © 2017 The Author(s).",,,2-s2.0-85030665286
"Raja K., Patrick M., Elder J.T., Tsoi L.C.","Machine learning workflow to enhance predictions of Adverse Drug Reactions (ADRs) through drug-gene interactions: Application to drugs for cutaneous diseases",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020853750&doi=10.1038%2fs41598-017-03914-3&partnerID=40&md5=8dca8f516406ed3ce58acfe9f372361f","Adverse drug reactions (ADRs) pose critical public health issues, affecting over 6% of hospitalized patients. While knowledge of potential drug-drug interactions (DDI) is necessary to prevent ADR, the rapid pace of drug discovery makes it challenging to maintain a strong insight into DDIs. In this study, we present a novel literature-mining framework for enhancing the predictions of DDIs and ADR types by integrating drug-gene interactions (DGIs). The ADR types were adapted from a DDI corpus, including i) adverse effect; ii) effect at molecular level; iii) effect related to pharmacokinetics; and iv) DDIs without known ADRs. By using random forest classifier our approach achieves an F-score of 0.87 across the ADRs classification using only the DDI features. We then enhanced the performance of the classifier by including DGIs (F-score = 0.90), and applied the classification model trained with the DDI corpus to identify the drugs that might interact with the drugs for cutaneous diseases. We successfully predict previously known ADRs for drugs prescribed to cutaneous diseases, and are also able to identify promising new ADRs. © 2017 The Author(s).",,,2-s2.0-85020853750
"Ali A., Yangyu F.","Unsupervised feature learning and automatic modulation classification using deep learning model",2017,"Physical Communication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029702698&doi=10.1016%2fj.phycom.2017.09.004&partnerID=40&md5=eb28fac643b88b62f088af77196db581","Recently, deep learning has received a lot of attention in many machine learning applications for its superior classification performance in speech recognition, natural language understanding and image processing. However, it still lacks attention in automatic modulation classification (AMC) until now. Here, we introduce the application of deep learning in AMC. We propose a fully connected 2 layer feed-forward deep neural network (DNN) with layerwise unsupervised pretraining for the classification of digitally modulated signals in various channel conditions. The system uses independent autoencoders (AEs) for feature learning with multiple hidden nodes. Signal information from the received samples is extracted and preprocessed via I and Q components, and formed into training input to 1st AE layer. A probabilistic based method is employed at the output layer to detect the correct modulation signal. Simulation results show that a significant improvement can be achieved compared to the other conventional machine learning methods in the literature. Moreover, we also show that our proposed method can extract the features from cyclic-stationary data samples. A good classification accuracy was achieved, even when the proposed deep network is trained and tested at different SNRs. This shows the future potential of the deep learning model for application to AMC. © 2017 Elsevier B.V.","Autoencoders; Automatic modulation classification; Deep learning networks; Digital modulation","Artificial intelligence; Classification (of information); Deep neural networks; Extraction; Image processing; Learning systems; Modulation; Speech recognition; Support vector machines; Autoencoders; Automatic modulation classification; Automatic modulation classification (AMC); Digital modulations; Learning network; Machine learning applications; Natural language understanding; Unsupervised feature learning; Deep learning",2-s2.0-85029702698
"Liu Q., Gu Q., Wu Z.","Feature selection method based on support vector machine and shape analysis for high-throughput medical data",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031756607&doi=10.1016%2fj.compbiomed.2017.10.008&partnerID=40&md5=112b449f1d1c089082d33c2c0604d1bb","Proteomics data analysis based on the mass-spectrometry technique can provide a powerful tool for early diagnosis of tumors and other diseases. It can be used for exploring the features that reflect the difference between samples from high-throughput mass spectrometry data, which are important for the identification of tumor markers. Proteomics mass spectrometry data have the characteristics of too few samples, too many features and noise interference, which pose a great challenge to traditional machine learning methods. Traditional unsupervised dimensionality reduction methods do not utilize the label information effectively, so the subspaces they find may not be the most separable ones of the data. To overcome the shortcomings of traditional methods, in this paper, we present a novel feature selection method based on support vector machine (SVM) and shape analysis. In the process of feature selection, our method considers not only the interaction between features but also the relationship between features and class labels, which improves the classification performance. The experimental results obtained from four groups of proteomics data show that, compared with traditional unsupervised feature extraction methods (i.e., Principal Component Analysis - Procrustes Analysis, PCA-PA), our method not only ensures that fewer features are selected but also ensures a high recognition rate. In addition, compared with the two kinds of multivariate filter methods, i.e., Max-Relevance Min-Redundancy (MRMR) and Fast Correlation-Based Filter (FCBF), our method has a higher recognition rate. © 2017 Elsevier Ltd","Feature selection; High-throughput medical data; Shape analysis; Support vector machine","Diagnosis; Learning systems; Mass spectrometry; Molecular biology; Principal component analysis; Spectrometry; Support vector machines; Throughput; Tumors; Classification performance; Dimensionality reduction method; Feature extraction methods; Feature selection methods; Machine learning methods; Medical data; Proteomics data analysis; Shape analysis; Feature extraction; Article; comparative study; high throughput screening; mass spectrometry; mathematical analysis; molecular recognition; multivariate analysis; principal component analysis; priority journal; proteomics; structure analysis; support vector machine",2-s2.0-85031756607
"Nguyen B., Morell C., De Baets B.","Distance metric learning with the Universum",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030313689&doi=10.1016%2fj.patrec.2017.09.031&partnerID=40&md5=ffd0f467b047eaf57fdbc31cd4688be3","Universum, a set of examples that do not belong to any class of interest for a classification problem, has been playing an important role in improving the performance of many machine learning methods. Since Universum examples are not required to have the same distribution as the training data, they can contain prior information for the possible classifiers. In this paper, we propose a novel distance metric learning method for nearest-neighbor (NN) classification, namely U-LMNN, that exploits prior information contained in the available Universum examples. Based on the large-margin nearest neighbor (LMNN) method, U-LMNN maximizes, for each training example, the margin between its nearest neighbor of the same class and the neighbors of different classes, while controlling the generalization capacity through the number of contradictions on Universum examples. Experimental results on synthetic as well as real-world data sets demonstrate a good performance of U-LMNN compared to the conventional LMNN method. © 2017 Elsevier B.V.","Metric learning; Nearest neighbor; Universum learning","Learning systems; Distance Metric Learning; Generalization capacity; Large margin nearest neighbors; Machine learning methods; Metric learning; Nearest neighbor classification; Nearest neighbors; Universum learning; Classification (of information)",2-s2.0-85030313689
"Kingston J.","Using artificial intelligence to support compliance with the general data protection regulation",2017,"Artificial Intelligence and Law",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028774071&doi=10.1007%2fs10506-017-9206-9&partnerID=40&md5=4dcf090c8eab4a1af7cab83a4f0b8b71","The General Data Protection Regulation (GDPR) is a European Union regulation that will replace the existing Data Protection Directive on 25 May 2018. The most significant change is a huge increase in the maximum fine that can be levied for breaches of the regulation. Yet fewer than half of UK companies are fully aware of GDPR—and a number of those who were preparing for it stopped doing so when the Brexit vote was announced. A last-minute rush to become compliant is therefore expected, and numerous companies are starting to offer advice, checklists and consultancy on how to comply with GDPR. In such an environment, artificial intelligence technologies ought to be able to assist by providing best advice; asking all and only the relevant questions; monitoring activities; and carrying out assessments. The paper considers four areas of GDPR compliance where rule based technologies and/or machine learning techniques may be relevant: Following compliance checklists and codes of conduct; Supporting risk assessments; Complying with the new regulations regarding technologies that perform automatic profiling; Complying with the new regulations concerning recognising and reporting breaches of security. It concludes that AI technology can support each of these four areas. The requirements that GDPR (or organisations that need to comply with GDPR) state for explanation and justification of reasoning imply that rule-based approaches are likely to be more helpful than machine learning approaches. However, there may be good business reasons to take a different approach in some circumstances. © 2017, Springer Science+Business Media B.V.","Artificial intelligence; Compliance; Data protection; GDPR; Machine learning; Rule-based systems","Data privacy; Engineering education; Knowledge based systems; Learning systems; Risk assessment; Security of data; Artificial intelligence technologies; Compliance; Data Protection Directive; European Union regulations; GDPR; General data protection regulations; Machine learning approaches; Machine learning techniques; Artificial intelligence",2-s2.0-85028774071
"Pei H., Wang K., Zhong P.","Semi-supervised matrixized least squares support vector machine",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030839207&doi=10.1016%2fj.asoc.2017.07.040&partnerID=40&md5=e1e44166df80b72737ee90def3aed7bf","The matrix learning, which studies how to design algorithms based on matrix patterns, is proven to have some significant advantages over the vector learning such as the improved classification performance and the low computational complexity. However, most of the traditional matrix learning algorithms are supervised ones which require labels of all patterns. In practice, the difficult acquisition of labeled patterns is a major challenge for supervised algorithms. An effective approach to handle this problem is the manifold regularization, which is known as one of the most elegant frameworks for the semi-supervised learning (SSL). The Laplacian regularized least squares (LapRLS) is a classical vector learning algorithm following this framework. Inspired by the advantages of the matrix learning and the SSL, in this paper, we propose a novel semi-supervised matrix learning algorithm by incorporating the manifold regularization into the matrixized least squares support vector machine (MatLSSVM), termed as Laplacian matrixized LSSVM, or LapMatLSSVM for short. MatLSSVM, which has been built by combining the merits of the matrix learning and LSSVM, is a promising supervised algorithm. As an extension of MatLSSVM to the SSL, LapMatLSSVM can not only directly operate on matrix patterns, but also effectively exploit the geometric information embedded in unlabeled matrix patterns. Moreover, its generalization risk bound is tighter than that of LapRLS in terms of the Rademacher complexity. For the implementation, LapMatLSSVM learns in an iterative manner, and solves a least squares optimization problem at each iteration. Extensive experiments have been conducted across two kinds of datasets: image datasets and UCI datasets. Experimental results confirm the benefits of the proposed algorithm. © 2017 Elsevier B.V.","LapRLS; Manifold regularization; MatLSSVM; Matrix learning; Semi-supervised learning","Iterative methods; Laplace transforms; Optimization; Supervised learning; Support vector machines; Vectors; Classification performance; LapRLS; Least squares support vector machines; Low computational complexity; Manifold regularizations; MatLSSVM; Semi- supervised learning; Semi-supervised learning (SSL); Learning algorithms",2-s2.0-85030839207
"Podryabinkin E.V., Shapeev A.V.","Active learning of linearly parametrized interatomic potentials",2017,"Computational Materials Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028916984&doi=10.1016%2fj.commatsci.2017.08.031&partnerID=40&md5=aecc10c5d95e989feb7a65c904283f71","This paper introduces an active learning approach to the fitting of machine learning interatomic potentials. Our approach is based on the D-optimality criterion for selecting atomic configurations on which the potential is fitted. It is shown that the proposed active learning approach is highly efficient in training potentials on the fly, ensuring that no extrapolation is attempted and leading to a completely reliable atomistic simulation without any significant decrease in accuracy. We apply our approach to molecular dynamics and structure relaxation, and we argue that it can be applied, in principle, to any other type of atomistic simulation. The software, test cases, and examples of usage are published at http://gitlab.skoltech.ru/shapeev/mlip/. © 2017 Elsevier B.V.","Active learning; Atomistic simulation; Interatomic potential; Learning on the fly; Machine learning; Moment tensor potentials","Learning systems; Molecular dynamics; Software testing; Active Learning; Atomistic simulations; Interatomic potential; Moment tensors; On the flies; Artificial intelligence",2-s2.0-85028916984
"Parisi G.I., Tani J., Weber C., Wermter S.","Lifelong learning of human actions with deep neural network self-organization",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030761514&doi=10.1016%2fj.neunet.2017.09.001&partnerID=40&md5=7e27de4e52e03c9d872e5b8ae1566bb7","Lifelong learning is fundamental in autonomous robotics for the acquisition and fine-tuning of knowledge through experience. However, conventional deep neural models for action recognition from videos do not account for lifelong learning but rather learn a batch of training data with a predefined number of action classes and samples. Thus, there is the need to develop learning systems with the ability to incrementally process available perceptual cues and to adapt their responses over time. We propose a self-organizing neural architecture for incrementally learning to classify human actions from video sequences. The architecture comprises growing self-organizing networks equipped with recurrent neurons for processing time-varying patterns. We use a set of hierarchically arranged recurrent networks for the unsupervised learning of action representations with increasingly large spatiotemporal receptive fields. Lifelong learning is achieved in terms of prediction-driven neural dynamics in which the growth and the adaptation of the recurrent networks are driven by their capability to reconstruct temporally ordered input sequences. Experimental results on a classification task using two action benchmark datasets show that our model is competitive with state-of-the-art methods for batch learning also when a significant number of sample labels are missing or corrupted during training sessions. Additional experiments show the ability of our model to adapt to non-stationary input avoiding catastrophic interference. © 2017 The Author(s)","Action recognition; Lifelong learning; Self-organizing neural networks; Unsupervised deep learning","Classification (of information); Deep neural networks; Image recognition; Network architecture; Neural networks; Recurrent neural networks; Action recognition; Action representations; Additional experiments; Growing self-organizing networks; Life long learning; Self-organizing neural network; Spatiotemporal receptive field; State-of-the-art methods; Deep learning; Article; artificial neural network; classification algorithm; human activities; measurement accuracy; prediction; priority journal; unsupervised machine learning; videorecording",2-s2.0-85030761514
"Litjens G., Kooi T., Bejnordi B.E., Setio A.A.A., Ciompi F., Ghafoorian M., van der Laak J.A.W.M., van Ginneken B., Sánchez C.I.","A survey on deep learning in medical image analysis",2017,"Medical Image Analysis",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026529300&doi=10.1016%2fj.media.2017.07.005&partnerID=40&md5=7a8f8ab63ff9d2cdaab2acbbea2e9c2f","Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research. © 2017 Elsevier B.V.","Convolutional neural networks; Deep learning; Medical imaging; Survey","Convolution; Deep learning; Image analysis; Image segmentation; Learning algorithms; Neural networks; Object detection; Surveying; Surveys; Application area; Convolutional networks; Convolutional neural network; Critical discussions; Digital pathologies; State of the art; Medical imaging; abdomen; anatomic landmark; artificial neural network; brain; breast; classification; computer; diagnostic imaging; digital imaging; eye; human; image analysis; image enhancement; image retrieval; image segmentation; learning; learning algorithm; priority journal; registration; Review; software; thorax; unsupervised machine learning",2-s2.0-85026529300
"Öztoprak H., Toycan M., Alp Y.K., Arıkan O., Doğutepe E., Karakaş S.","Machine-based classification of ADHD and nonADHD participants using time/frequency features of event-related neuroelectric activity",2017,"Clinical Neurophysiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032391582&doi=10.1016%2fj.clinph.2017.09.105&partnerID=40&md5=a724d8ba953ecc4f172478a805662b6b","Objective Attention-deficit/hyperactivity disorder (ADHD) is the most frequent diagnosis among children who are referred to psychiatry departments. Although ADHD was discovered at the beginning of the 20th century, its diagnosis is still confronted with many problems. Method A novel classification approach that discriminates ADHD and nonADHD groups over the time-frequency domain features of event-related potential (ERP) recordings that are taken during Stroop task is presented. Time-Frequency Hermite-Atomizer (TFHA) technique is used for the extraction of high resolution time-frequency domain features that are highly localized in time-frequency domain. Based on an extensive investigation, Support Vector Machine-Recursive Feature Elimination (SVM-RFE) was used to obtain the best discriminating features. Results When the best three features were used, the classification accuracy for the training dataset reached 98%, and the use of five features further improved the accuracy to 99.5%. The accuracy was 100% for the testing dataset. Based on extensive experiments, the delta band emerged as the most contributing frequency band and statistical parameters emerged as the most contributing feature group. Conclusion The classification performance of this study suggests that TFHA can be employed as an auxiliary component of the diagnostic and prognostic procedures for ADHD. Significance The features obtained in this study can potentially contribute to the neuroelectrical understanding and clinical diagnosis of ADHD. © 2017 International Federation of Clinical Neurophysiology","Attention-deficit/hyperactivity disorder (ADHD); Classification; Feature selection; Machine learning; Support vector machine-recursive feature elimination (SVM-RFE); Time-frequency Hermite atomizer","Article; attention deficit disorder; child; clinical article; clinical effectiveness; controlled study; data mining; diagnostic accuracy; disease classification; DSM-IV; electric activity; event related potential; human; information processing; machine learning; male; methodology; priority journal; sensitivity and specificity; statistical parameters; Stroop test; support vector machine",2-s2.0-85032391582
"Jersakova R., Allen R.J., Booth J., Souchay C., O'Connor A.R.","Understanding metacognitive confidence: Insights from judgment-of-learning justifications",2017,"Journal of Memory and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028467183&doi=10.1016%2fj.jml.2017.08.002&partnerID=40&md5=bda1e4d3962e128ec9269a18c5e2e786","This study employed the delayed judgment-of-learning (JOL) paradigm to investigate the content of metacognitive judgments; after studying cue-target word-pairs, participants predicted their ability to remember targets on a future memory test (cued recognition in Experiments 1 and 2 and cued recall in Experiment 3). In Experiment 1 and the confidence JOL group of Experiment 3, participants used a commonly employed 6-point numeric confidence JOL scale (0–20–40–60–80–100%). In Experiment 2 and the binary JOL group of Experiment 3 participants first made a binary yes/no JOL prediction followed by a 3-point verbal confidence judgment (sure-maybe-guess). In all experiments, on a subset of trials, participants gave a written justification of why they gave that specific JOL response. We used natural language processing techniques (latent semantic analysis and word frequency [n-gram] analysis) to characterize the content of the written justifications and to capture what types of evidence evaluation uniquely separate one JOL response type from others. We also used a machine learning classification algorithm (support vector machine [SVM]) to quantify the extent to which any two JOL responses differed from each other. We found that: (i) participants can justify and explain their JOLs; (ii) these justifications reference cue familiarity and target accessibility and so are particularly consistent with the two-stage metacognitive model; and (iii) JOL confidence judgements do not correspond to yes/no responses in the manner typically assumed within the literature (i.e. 0–40% interpreted as no predictions). © 2017 Elsevier Inc.","Confidence; Episodic memory; Judgments-of-learning; Linguistics; Metacognition","case report; classification algorithm; decision making; episodic memory; female; human; linguistics; machine learning; male; memory test; metacognition; model; natural language processing; prediction; quantitative study; recall; support vector machine",2-s2.0-85028467183
"Pise N., Kulkarni P.","Evolving learners’ behavior in data mining",2017,"Evolving Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032575216&doi=10.1007%2fs12530-016-9167-3&partnerID=40&md5=2cbdfbeb449f9479b99f42ab7fc81b6c","An evaluation and choice of learning algorithms is a current research area in data mining, artificial intelligence and pattern recognition, etc. Supervised learning is one of the tasks most frequently used in data mining. There are several learning algorithms available in machine learning field and new algorithms are being added in machine learning literature. There is a need for selecting the best suitable learning algorithm for a given data. With the information explosion of different learning algorithms and the changing data scenarios, there is a need of smart learning system. The paper shows one approach where past experiences learned are used to suggest the best suitable learner using 3 meta-features namely simple, statistical and information theoretic features. The system tests 38 UCI benchmark datasets from various domains using nine classifiers from various categories. It is observed that for 29 datasets, i.e., 76 % of datasets, both the predicted and actual accuracies directly match. The proposed approach is found to be correct for algorithm selection of these datasets. New proposed equation of finding classifier accuracy based on meta-features is determined and validated. The study compares various supervised learning algorithms by performing tenfold cross-validation paired t test. The work helps in a critical step in data mining for selecting the suitable data mining algorithm. © 2016, Springer-Verlag Berlin Heidelberg.","Classification; Data characteristics; Data mining techniques; Intelligent data analysis; Learning algorithms; Machine learning","Artificial intelligence; Classification (of information); Data mining; Information theory; Learning systems; Pattern recognition; Supervised learning; Algorithm selection; Benchmark datasets; Cross validation; Data characteristics; Data mining algorithm; Information explosion; Intelligent data analysis; Machine learning literature; Learning algorithms",2-s2.0-85032575216
"Zhang J., Yin J., Zhang Q., Shi J., Li Y.","Robust sound event classification with bilinear multi-column ELM-AE and two-stage ensemble learning",2017,"Eurasip Journal on Audio, Speech, and Music Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019613735&doi=10.1186%2fs13636-017-0109-1&partnerID=40&md5=ef6e9e665cd0715a0365b3a9491685c1","The automatic sound event classification (SEC) has attracted a growing attention in recent years. Feature extraction is a critical factor in SEC system, and the deep neural network (DNN) algorithms have achieved the state-of-the-art performance for SEC. The extreme learning machine-based auto-encoder (ELM-AE) is a new deep learning algorithm, which has both an excellent representation performance and very fast training procedure. However, ELM-AE suffers from the problem of unstability. In this work, a bilinear multi-column ELM-AE (B-MC-ELM-AE) algorithm is proposed to improve the robustness, stability, and feature representation of the original ELM-AE, which is then applied to learn feature representation of sound signals. Moreover, a B-MC-ELM-AE and two-stage ensemble learning (TsEL)-based feature learning and classification framework is then developed to perform the robust and effective SEC. The experimental results on the Real World Computing Partnership Sound Scene Database show that the proposed SEC framework outperforms the state-of-the-art DNN algorithm. © 2017, The Author(s).","Bilinear; Ensemble learning; Extreme learning machine auto-encoder; Multi-column; Sound event classification","Deep neural networks; Feature extraction; Knowledge acquisition; Learning algorithms; Learning systems; Signal encoding; Auto encoders; Bilinear; Classification framework; Ensemble learning; Extreme learning machine; Feature representation; Sound event classification; State-of-the-art performance; Deep learning",2-s2.0-85019613735
"Chen S.-G., Wu X.-J.","Multiple birth least squares support vector machine for multi-class classification",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032353453&doi=10.1007%2fs13042-016-0554-7&partnerID=40&md5=f4d75fe306c09da67c92909b466956dd","Least squares twin support vector machine (LSTSVM) was initially designed for binary classification. However, practical problems often require the discrimination more than two categories. To tackle multi-class classification problem, a novel algorithm, called multiple birth least squares support vector machine (MBLSSVM), is proposed. Our MBLSSVM solves K quadratic programming problems (QPPs) to obtain K hyperplanes, each problem is similar to binary LSTSVM. Comparison against the Multi-LSTSVM, Multi-TWSVM, MBSVM and our MBLSSVM on both UCI datasets and ORL, YALE face datasets illustrates the effectiveness of the proposed method. © 2016, Springer-Verlag Berlin Heidelberg.","Least squares; Multi-class classification; Multiple birth least squares support vector machine; Twin support vector machine","Bins; Classification (of information); Classifiers; Quadratic programming; Support vector machines; Vectors; Binary classification; Least Square; Least squares support vector machines; Least squares twin support vector machines; Multi-class classification; Multiclass classification problems; Quadratic programming problems; Twin support vector machines; Learning systems",2-s2.0-85032353453
"Ziemke T., Schaefer K.E., Endsley M.","Situation awareness in human-machine interactive systems",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023624612&doi=10.1016%2fj.cogsys.2017.06.004&partnerID=40&md5=8a6d554a89548d1aadfc02d4127e2164","This special issue brings together six papers on situation awareness in human-machine interactive systems, in particular in teams of collaborating humans and artificial agents. The editorial provides a brief introduction and overviews the contributions, addressing issues such as team and shared situation awareness, trust, transparency, timing, engagement, and ethical aspects. © 2017",,"Cognitive systems; Artificial agents; Ethical aspects; Human-Machine Interactive; Situation awareness; Artificial intelligence; artificial intelligence; awareness; cognition; decision making; Editorial; emotion; ethics; human; human computer interaction; machine learning; motivation; priority journal; psychology; simulation; teamwork; time; trust; verbal behavior",2-s2.0-85023624612
"Hoang N.-D., Chen C.-T., Liao K.-W.","Prediction of chloride diffusion in cement mortar using Multi-Gene Genetic Programming and Multivariate Adaptive Regression Splines",2017,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412290&doi=10.1016%2fj.measurement.2017.08.031&partnerID=40&md5=43423aa3d0de1f8380a417146defc34a","Chloride-induced damage of coastal concrete structure leads to serious structural deterioration. Thus, chloride content in concrete is a crucial parameter for determining the corrosion state. This study aims at establishing machine learning models for chloride diffusion prediction with the utilizations of the Multi-Gene Genetic Programming (MGGP) and Multivariate Adaptive Regression Splines (MARS). MGGP and MARS are well-established methods to construct predictive modeling equations from experimental data. These modeling equations can be used to express the relationship between the chloride ion diffusion in concrete and its influencing factors. Moreover, a data set, which contains 132 cement mortar specimens, has been collected for this study to train and verify the machine learning approaches. The prediction results of MGGP and MARS are compared with those of the Artificial Neural Network and Least Squares Support Vector Regression. Notably, MARS demonstrates the best prediction performance with the Root Mean Squared Error (RMSE) = 0.70 and the coefficient of determination (R2) = 0.91. © 2017 Elsevier Ltd","Cement mortar; Chloride diffusion; Construction material; Machine learning; Modeling equation","Artificial intelligence; Building materials; Cements; Chlorine compounds; Concretes; Diffusion; Forecasting; Genes; Genetic algorithms; Learning algorithms; Learning systems; Mean square error; Mortar; Neural networks; Regression analysis; Splines; Cement mortars; Chloride diffusion; Coefficient of determination; Least squares support vector regression; Machine learning approaches; Model equations; Multi-gene genetic programming; Multivariate adaptive regression splines; Genetic programming",2-s2.0-85028412290
"Liu Y., Cong M., Zheng H., Liu D.","Porcine automation: Robotic abdomen cutting trajectory planning using machine vision techniques based on global optimization algorithm",2017,"Computers and Electronics in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032438005&doi=10.1016%2fj.compag.2017.10.009&partnerID=40&md5=f59eb35389a285cc3cebea71dd8ca061","The purpose of this paper is to provide details on implementation of accurate and intelligent automation solution for porcine abdomen cutting while a pig is hung up by rear legs. The system developed utilized a 6-DOF industrial manipulators, customized tools, 2D camera and PC. Eye-to-hand calibrations built coordinate transformation relations of units in Cartesian space. The porcine abdomen curve was identified and fitted into quintic spline curve from image. Under cavum peritonaei constrains, optimal sectional trajectory was planned based on genetic algorithm (GA) by comparing several kinds of optimization algorithms. The results of experimental replications show that the system was successful both in following the varied position carcass and cutting open abdominal cavity without haslet damage. The system can enhance the quality, hygienic standard and efficiency of the process. © 2017 Elsevier B.V.","Global optimization algorithm; Industrial robot; Machine vision; Porcine automation; Trajectory planning","Automation; Genetic algorithms; Global optimization; Industrial manipulators; Industrial robots; Optimization; Robot programming; Trajectories; Abdominal cavity; Coordinate transformation relations; Experimental replications; Global optimization algorithm; Hygienic standards; Intelligent automation; Optimization algorithms; Trajectory Planning; Computer vision; algorithm; automation; calibration; machine learning; optimization; robotics; Suidae; Sus",2-s2.0-85032438005
"Hahne J.M., Markovic M., Farina D.","User adaptation in Myoelectric Man-Machine Interfaces",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021637978&doi=10.1038%2fs41598-017-04255-x&partnerID=40&md5=f696822b081c0d0d5af193a6773bed0b","State of the art clinical hand prostheses are controlled in a simple and limited way that allows the activation of one function at a time. More advanced laboratory approaches, based on machine learning, offer a significant increase in functionality, but their clinical impact is limited, mainly due to lack of reliability. In this study, we analyse two conceptually different machine learning approaches, focusing on their robustness and performance in a closed loop application. A classification (finite number of classes) and a regression (continuous mapping) based projection of EMG into external commands were applied while artificially introducing non-stationarities in the EMG signals. When tested on ten able-bodied individuals and one transradial amputee, the two methods were similarly influenced by non-stationarities when tested offline. However, in online tests, where the user could adapt his muscle activation patterns to the changed conditions, the regression-based approach was significantly less influenced by the changes in signal features than the classification approach. This observation demonstrates, on the one hand, the importance of online tests with users in the loop for assessing the performance of myocontrol approaches. On the other hand, it also demonstrates that regression allows for a better user correction of control commands than classification. © 2017 The Author(s).",,"amputee; bionics; classification; clinical article; human; machine learning; male; muscle contraction",2-s2.0-85021637978
"Alvarez-Rodriguez U., Lamata L., Escandell-Montero P., Martín-Guerrero J.D., Solano E.","Supervised Quantum Learning without Measurements",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031999700&doi=10.1038%2fs41598-017-13378-0&partnerID=40&md5=a27a26a16aa8bad1626ee293140c456d","We propose a quantum machine learning algorithm for efficiently solving a class of problems encoded in quantum controlled unitary operations. The central physical mechanism of the protocol is the iteration of a quantum time-delayed equation that introduces feedback in the dynamics and eliminates the necessity of intermediate measurements. The performance of the quantum algorithm is analyzed by comparing the results obtained in numerical simulations with the outcome of classical machine learning methods for the same problem. The use of time-delayed equations enhances the toolbox of the field of quantum machine learning, which may enable unprecedented applications in quantum technologies. © 2017 The Author(s).",,"computer simulation; machine learning",2-s2.0-85031999700
"Zielinski F., Maxwell P.I., Fletcher T.L., Davie S.J., Di Pasquale N., Cardamone S., Mills M.J.L., Popelier P.L.A.","Geometry Optimization with Machine Trained Topological Atoms",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031000206&doi=10.1038%2fs41598-017-12600-3&partnerID=40&md5=3f2c95fae2a58f52628952673b659cbf","The geometry optimization of a water molecule with a novel type of energy function called FFLUX is presented, which bypasses the traditional bonded potentials. Instead, topologically-partitioned atomic energies are trained by the machine learning method kriging to predict their IQA atomic energies for a previously unseen molecular geometry. Proof-of-concept that FFLUX's architecture is suitable for geometry optimization is rigorously demonstrated. It is found that accurate kriging models can optimize 2000 distorted geometries to within 0.28 kJ mol-1 of the corresponding ab initio energy, and 50% of those to within 0.05 kJ mol-1. Kriging models are robust enough to optimize the molecular geometry to sub-noise accuracy, when two thirds of the geometric inputs are outside the training range of that model. Finally, the individual components of the potential energy are analyzed, and chemical intuition is reflected in the independent behavior of the three energy terms EA E intra A (intra-atomic), VAA V cl AA ' (electrostatic) and VAA V x AA ' (exchange), in contrast to standard force fields. © 2017 The Author(s).",,"ab initio calculation; behavior; geometry; human; human experiment; intuition; kriging; machine learning; noise",2-s2.0-85031000206
"Cole J.H., Poudel R.P.K., Tsagkrasoulis D., Caan M.W.A., Steves C., Spector T.D., Montana G.","Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker",2017,"NeuroImage",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029673464&doi=10.1016%2fj.neuroimage.2017.07.059&partnerID=40&md5=02424eca162f6a1fbda1fc7deecf8e81","Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people. Deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of ‘brain-predicted age’ as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. CNN accurately predicted chronological age using GM (correlation between brain-predicted age and chronological age r = 0.96, mean absolute error [MAE] = 4.16 years) and raw (r = 0.94, MAE = 4.65 years) data. This was comparable to GPR brain-predicted age using GM data (r = 0.95, MAE = 4.66 years). Brain-predicted age was a heritable phenotype for all models and input data (h2 ≥ 0.5). Brain-predicted age showed high test-retest reliability (intraclass correlation coefficient [ICC] = 0.90–0.99). Multi-centre reliability was more variable within high ICCs for GM (0.83–0.96) and poor-moderate levels for WM and raw data (0.51–0.77). Brain-predicted age represents an accurate, highly reliable and genetically-influenced phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings. © 2017","Biomarker; Brain ageing; Convolutional neural networks; Deep learning; Gaussian processes; Heritability; Neuroimaging; Reliability","adult; age determination; aged; aging; Article; artificial neural network; brain; brain aging; convolutional neural network; deep learning; dizygotic twins; female; gray matter; heritability; human; machine learning; male; monozygotic twins; neuroimaging; normal human; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; priority journal; test retest reliability; white matter",2-s2.0-85029673464
"Reece A.G., Danforth C.M.","Instagram photos reveal predictive markers of depression",2017,"EPJ Data Science",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027011738&doi=10.1140%2fepjds%2fs13688-017-0110-z&partnerID=40&md5=12591ab9e3de234208ab344a688f4cec","Using Instagram data from 166 individuals, we applied machine learning tools to successfully identify markers of depression. Statistical features were computationally extracted from 43,950 participant Instagram photos, using color analysis, metadata components, and algorithmic face detection. Resulting models outperformed general practitioners’ average unassisted diagnostic success rate for depression. These results held even when the analysis was restricted to posts made before depressed individuals were first diagnosed. Human ratings of photo attributes (happy, sad, etc.) were weaker predictors of depression, and were uncorrelated with computationally-generated features. These results suggest new avenues for early screening and detection of mental illness. © 2017, The Author(s).","computational social science; depression; machine learning; psychology; social media","Artificial intelligence; Diseases; Learning systems; Applied machine learning; Computational social science; depression; General practitioners; Mental illness; psychology; Social media; Statistical features; Face recognition",2-s2.0-85027011738
"Yousef M., Nigatu D., Levy D., Allmer J., Henkel W.","Categorization of species based on their microRNAs employing sequence motifs, information-theoretic sequence feature extraction, and k-mers",2017,"Eurasip Journal on Advances in Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032857843&doi=10.1186%2fs13634-017-0506-8&partnerID=40&md5=74d7234f9d5c4f882085168926e0fc07","Background: Diseases like cancer can manifest themselves through changes in protein abundance, and microRNAs (miRNAs) play a key role in the modulation of protein quantity. MicroRNAs are used throughout all kingdoms and have been shown to be exploited by viruses to modulate their host environment. Since the experimental detection of miRNAs is difficult, computational methods have been developed. Many such tools employ machine learning for pre-miRNA detection, and many features for miRNA parameterization have been proposed. To train machine learning models, negative data is of importance yet hard to come by; therefore, we recently started to employ pre-miRNAs from one species as positive data versus another species’ pre-miRNAs as negative examples based on sequence motifs and k-mers. Here, we introduce the additional usage of information-theoretic (IT) features. Results: Pre-miRNAs from one species were used as positive and another species’ pre-miRNAs as negative training data for machine learning. The categorization capability of IT and k-mer features was investigated. Both feature sets and their combinations yielded a very high accuracy, which is as good as the previously suggested sequence motif and k-mer based method. However, for obtaining a high performance, a sufficiently large phylogenetic distance between the species and sufficiently high number of pre-miRNAs in the training set is required. To examine the contribution of the IT and k-mer features, an information gain-based feature ranking was performed. Although the top 3 are IT features, 80% of the top 100 features are k-mers. The comparison of all three individual approaches (motifs, IT, and k-mers) shows that the distinction of species based on their pre-miRNAs k-mers are sufficient. Conclusions: IT sequence feature extraction enables the distinction among species and is less computationally expensive than motif calculations. However, since IT features need larger amounts of data to have enough statistics for producing highly accurate results, future categorization into species can be effectively done using k-mers only. The biological reasoning for this is the existence of a codon bias between species which can, at least, be observed in exonic miRNAs. Future work in this direction will be the ab initio detection of pre-miRNA. In addition, prediction of pre-miRNA from RNA-seq can be done. © 2017, The Author(s).","Differentiate miRNAs among species; Information theory; k-mer; Machine learning; MicroRNA; miRNA categorization; Pre-microRNA; Sequence motifs","Artificial intelligence; Calculations; Computation theory; Extraction; Feature extraction; Information theory; Learning systems; Proteins; Viruses; Differentiate miRNAs among species; Information gain; Machine learning models; MicroRNAs; miRNA categorization; Mirna detections; Negative examples; Sequence motifs; RNA",2-s2.0-85032857843
"Ijjina E.P., Chalavadi K.M.","Human action recognition in RGB-D videos using motion sequence information and deep learning",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023621585&doi=10.1016%2fj.patcog.2017.07.013&partnerID=40&md5=c6f83ab10f679d413548d91da0d34518","In this paper, we propose an approach for recognizing human actions based on motion sequence information in RGB-D video using deep learning. A new representation that gives emphasis to the key poses associated with each action is presented. The features obtained from motion in RGB and depth video streams are given as input to the convolutional neural network to learn the discriminative features. The efficacy of the proposed approach is demonstrated on MIVIA action, NATOPS gesture, SBU Kinect interaction, and Weizmann datasets. © 2017 Elsevier Ltd","Deep learning; Extreme learning machines; Motion information; Multi-modal action recognition","Education; Image recognition; Learning systems; Neural networks; Video streaming; Action recognition; Convolutional neural network; Discriminative features; Extreme learning machine; Human actions; Human-action recognition; Motion information; Motion sequences; Deep learning",2-s2.0-85023621585
"Aykanat M., Kılıç Ö., Kurt B., Saryal S.","Classification of lung sounds using convolutional neural networks",2017,"Eurasip Journal on Image and Video Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029531080&doi=10.1186%2fs13640-017-0213-2&partnerID=40&md5=00f24cccf4d70cf745f6b22fd08da906","In the field of medicine, with the introduction of computer systems that can collect and analyze massive amounts of data, many non-invasive diagnostic methods are being developed for a variety of conditions. In this study, our aim is to develop a non-invasive method of classifying respiratory sounds that are recorded by an electronic stethoscope and the audio recording software that uses various machine learning algorithms. In order to store respiratory sounds on a computer, we developed a cost-effective and easy-to-use electronic stethoscope that can be used with any device. Using this device, we recorded 17,930 lung sounds from 1630 subjects. We employed two types of machine learning algorithms; mel frequency cepstral coefficient (MFCC) features in a support vector machine (SVM) and spectrogram images in the convolutional neural network (CNN). Since using MFCC features with a SVM algorithm is a generally accepted classification method for audio, we utilized its results to benchmark the CNN algorithm. We prepared four data sets for each CNN and SVM algorithm to classify respiratory audio: (1) healthy versus pathological classification; (2) rale, rhonchus, and normal sound classification; (3) singular respiratory sound type classification; and (4) audio type classification with all sound types. Accuracy results of the experiments were; (1) CNN 86%, SVM 86%, (2) CNN 76%, SVM 75%, (3) CNN 80%, SVM 80%, and (4) CNN 62%, SVM 62%, respectively. As a result, we found out that spectrogram image classification with CNN algorithm works as well as the SVM algorithm, and given the large amount of data, CNN and SVM machine learning algorithms can accurately classify and pre-diagnose respiratory audio. © 2017, The Author(s).","Convolutional neural networks; Deep learning; Lung sounds; Machine learning","Artificial intelligence; Audio acoustics; Biological organs; Classification (of information); Convolution; Cost effectiveness; Deep learning; Diagnosis; Learning systems; Neural networks; Noninvasive medical procedures; Spectrographs; Speech recognition; Support vector machines; Classification methods; Convolutional neural network; Electronic stethoscope; Lung sounds; Mel-frequency cepstral coefficients; Non-invasive diagnostics; Sound classification; Type classifications; Learning algorithms",2-s2.0-85029531080
"Portman N., Tamblyn I.","Sampling algorithms for validation of supervised learning models for Ising-like systems",2017,"Journal of Computational Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029502646&doi=10.1016%2fj.jcp.2017.06.045&partnerID=40&md5=529e78a2d97d31dc497b049df838fc54","In this paper, we build and explore supervised learning models of ferromagnetic system behavior, using Monte-Carlo sampling of the spin configuration space generated by the 2D Ising model. Given the enormous size of the space of all possible Ising model realizations, the question arises as to how to choose a reasonable number of samples that will form physically meaningful and non-intersecting training and testing datasets. Here, we propose a sampling technique called “ID-MH” that uses the Metropolis–Hastings algorithm creating Markov process across energy levels within the predefined configuration subspace. We show that application of this method retains phase transitions in both training and testing datasets and serves the purpose of validation of a machine learning algorithm. For larger lattice dimensions, ID-MH is not feasible as it requires knowledge of the complete configuration space. As such, we develop a new “block-ID” sampling strategy: it decomposes the given structure into square blocks with lattice dimension N≤5 and uses ID-MH sampling of candidate blocks. Further comparison of the performance of commonly used machine learning methods such as random forests, decision trees, k nearest neighbors and artificial neural networks shows that the PCA-based Decision Tree regressor is the most accurate predictor of magnetizations of the Ising model. For energies, however, the accuracy of prediction is not satisfactory, highlighting the need to consider more algorithmically complex methods (e.g., deep learning). © 2017 Elsevier Inc.","Ising model; Machine learning; Monte-Carlo sampling",,2-s2.0-85029502646
"Liu P., Choo K.-K.R., Wang L., Huang F.","SVM or deep learning? A comparative study on remote sensing image classification",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978054052&doi=10.1007%2fs00500-016-2247-2&partnerID=40&md5=1753c5e766218705c4396734b348ad6f","With constant advancements in remote sensing technologies resulting in higher image resolution, there is a corresponding need to be able to mine useful data and information from remote sensing images. In this paper, we study auto-encoder (SAE) and support vector machine (SVM), and to examine their sensitivity, we include additional umber of training samples using the active learning frame. We then conduct a comparative evaluation. When classifying remote sensing images, SVM can also perform better than SAE in some circumstances, and active learning schemes can be used to achieve high classification accuracy in both methods. © 2016, Springer-Verlag Berlin Heidelberg.","Active learning; Remote sensing; Sparse auto-encoder; Spatial big data; Support vector machine","Artificial intelligence; Big data; Image reconstruction; Image resolution; Learning systems; Remote sensing; Support vector machines; Active Learning; Active learning scheme; Auto encoders; Classification accuracy; Comparative evaluations; Remote sensing image classification; Remote sensing images; Remote sensing technology; Image classification",2-s2.0-84978054052
"Rastegar-Mojarad M., Sohn S., Wang L., Shen F., Bleeker T.C., Cliby W.A., Liu H.","Need of informatics in designing interoperable clinical registries",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030853243&doi=10.1016%2fj.ijmedinf.2017.10.004&partnerID=40&md5=5d85cc67daa15b4db308197997537d9e","Clinical registries are designed to collect information relating to a particular condition for research or quality improvement. Intuitively, informatics in the area of data management and extraction plays a central role in clinical registries. Due to various reasons such as lack of informatics awareness or expertise, there may be little informatics involvement in designing clinical registries. In this paper, we studied a clinical registry from two critical perspectives, data quality and interoperability, where informatics can play a role. We evaluated these two aspects of an existing registry, Gynecology Surgery Registry, by mapping data elements and value sets, used in the registry, to a standardized terminology, SNOMED-CT. The results showed that majority of the values are ad-hoc and only 6 of 91 procedures in the registry could be mapped to the SNOMED-CT. To tackle this issue, we assessed the feasibility of automated data abstraction process, by training machine learning classifiers, based on existing manually extracted data. These classifiers achieved a reasonable average F-measure of 0.94. We concluded that more informatics engagement is needed to improve the interoperability, reusability, and quality of the registry. © 2017 Elsevier B.V.","Clinical registry; Informatics; Machine learning; Natural language processing","Artificial intelligence; Information management; Interoperability; Learning algorithms; Natural language processing systems; Reusability; Automated data; Clinical registry; Critical perspectives; Informatics; Particular condition; Quality improvement; Standardized terminologies; Training machines; Learning systems; abdominal hysterectomy; analytical error; Article; evaluation study; false positive result; female; gynecologic surgery; human; hysterectomy; machine learning; medical informatics; omentectomy; ovariectomy; pelvis lymphadenectomy; priority journal; random forest; register; salpingectomy; salpingooophorectomy; support vector machine; Systematized Nomenclature of Medicine",2-s2.0-85030853243
"KV S., Pillai G.N., Peethambaran B.","Prediction of landslide displacement with controlling factors using extreme learning adaptive neuro-fuzzy inference system (ELANFIS)",2017,"Applied Soft Computing Journal",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029406904&doi=10.1016%2fj.asoc.2017.09.001&partnerID=40&md5=321895f4c9b108024e8be8386786ec0e","Landslide is a major geo-environmental hazard which imparts serious threat to lives and properties. The slope failures are due to adverse inherent geological conditions triggered by an external factor. This paper proposes a new method for the prediction of displacement of step-like landslides, by accounting the controlling factors, using recently proposed extreme learning adaptive neuro-fuzzy inference system (ELANFIS) with empirical mode decomposition (EMD) technique. ELANFIS reduces the computational complexity of conventional ANFIS by incorporating the theoretical idea of extreme learning machines (ELM). The rainfall data and reservoir level elevation data are also integrated into the study. The nonlinear original landslide displacement series, rainfall data, and reservoir level elevation data are first converted into a limited number of intrinsic mode functions (IMF) and one residue. Then decomposed displacement data are predicted by using appropriate ELANFIS model. Final prediction is obtained by the summation of outputs of all ELANFIS sub models. The performance of proposed the technique is tested for the prediction Baishuihe and Shiliushubao landslides. The results show that ELANFIS with EMD model outperforms other methods in terms of generalization performance. © 2017 Elsevier B.V.","Empirical mode decomposition; Extreme learning machines (ELM); Landslide displacement; Neuro-fuzzy systems","Adaptive control systems; Forecasting; Fuzzy neural networks; Fuzzy systems; Knowledge acquisition; Landslides; Learning systems; Rain; Signal processing; Adaptive neuro-fuzzy inference system; Empirical Mode Decomposition; Environmental hazards; Extreme learning machine; Generalization performance; Intrinsic Mode functions; Neurofuzzy system; Shiliushubao Landslide; Fuzzy inference",2-s2.0-85029406904
"Bertacchini F., Bilotta E., Pantano P.","Shopping with a robotic companion",2017,"Computers in Human Behavior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015734591&doi=10.1016%2fj.chb.2017.02.064&partnerID=40&md5=19b73e077038527a9d8f8a05c5a9c86d","In this paper, we present a robotic shopping assistant, designed with a cognitive architecture, grounded in machine learning systems, in order to study how the human-robot interaction (HRI) is changing the shopping behavior in smart technological stores. In the software environment of the NAO robot, connected to the Internet with cloud services, we designed a social-like interaction where the robot carries out actions with the customer. In particular, we focused our design on two main skills the robot has to learn: the first is the ability to acquire social input communicated by relevant clues that humans provide about their emotional state (emotions, emotional speech), or collected in the Social Media (such as, information on the customer's tastes, cultural background, etc.). The second is the skill to express in turn its own emotional state, so that it can affect the customer buying decision, refining in the user the sense of interacting with a human-like companion. By combining social robotics and machine learning systems the potential of robotics to assist people in real life situations will increase, providing a gentle customers' acceptance of advanced technologies. © 2017 Elsevier Ltd","Emotion and Gesture Recognition; Human Robot Interaction (HRI); Machine learning; Smart retail settings; Social robotics","Artificial intelligence; Cognitive systems; Deep neural networks; Learning systems; Machine design; Man machine systems; Robotics; Robots; Sales; Web services; Advanced technology; Cognitive architectures; Cultural backgrounds; Human robot Interaction (HRI); Retail settings; Shopping behavior; Social robotics; Software environments; Human robot interaction; emotion; gesture; human; human experiment; Internet; life; machine learning; robotics; shopping; skill; social media; software; speech",2-s2.0-85015734591
"Lin S.-J.","Integrated artificial intelligence-based resizing strategy and multiple criteria decision making technique to form a management decision in an imbalanced environment",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032382134&doi=10.1007%2fs13042-016-0574-3&partnerID=40&md5=45963fb89da6a52e08bd2ec640f0776a","Classification in an imbalanced dataset is a current challenge in machine learning communities, as the class-imbalanced problem deteriorates the performance of numerous classifiers. This study introduces a two-stage intelligent data preprocessing approach to tackle the class-imbalanced problem. By modifying the penalty parameter of the support vector machine (SVM), the discriminating boundary will move toward the majority class and in turn misclassify the majority class examples as minority class examples. That is, more misclassifications for the majority class examples are equivalent to a greater number of minority class examples. Executing the SVM as a preprocessor can be used to overcome the class imbalanced problem. Sequentially, the modified dataset undergoes the random forest to defy the curse of dimensionality. Finally, the preprocessed data are fed into a rule-based classifier to generate comprehensive decision rules. According to the empirical results, the presented architecture is a promising alternative for the class-imbalanced problem. © 2016, Springer-Verlag Berlin Heidelberg.","Decision making; Imbalance data; Multiple criteria decision making; Support vector machine","Artificial intelligence; Classification (of information); Decision trees; Equivalence classes; Learning systems; Support vector machines; Curse of dimensionality; Imbalance datum; Imbalanced dataset; Machine learning communities; Management decisions; Multiple criteria decision making; Pre-processed data; Rule-based classifier; Decision making",2-s2.0-85032382134
"Fang Y., Liu Z.-H., Min F.","A PSO algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978173215&doi=10.1007%2fs00500-016-2260-5&partnerID=40&md5=ab1758405d86a43433f87ea35dfca991","Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values wik, and a designated non-positive exponent λ. We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the λ-weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms. © 2016, Springer-Verlag Berlin Heidelberg.","Attribute reduction; Cost-sensitive learning; Particle swarm optimization; Rough sets","Algorithms; Artificial intelligence; Cost reduction; Costs; Heuristic algorithms; Learning systems; Optimization; Pareto principle; Particle swarm optimization (PSO); Rough set theory; Supervised learning; Attribute reduction; Cost-sensitive learning; Learning strategy; Parameter setting; Pareto optimal solutions; Supervised machine learning; Test cost reduction; University of California; Data reduction",2-s2.0-84978173215
"Eulenberg P., Köhler N., Blasi T., Filby A., Carpenter A.E., Rees P., Theis F.J., Wolf F.A.","Reconstructing cell cycle and disease progression using deep learning",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028914209&doi=10.1038%2fs41467-017-00623-3&partnerID=40&md5=4aa164f0a373ca82cf53ce9baeb73d9b","We show that deep convolutional neural networks combined with nonlinear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a sixfold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer. © 2017 The Author(s).",,"artificial neural network; cells and cell components; disease; flow cytometry; image; subpopulation; anaphase; Article; cell cycle; cell cycle progression; diabetic retinopathy; disease course; disease severity; DNA content; flow cytometer; human; human cell; interphase; Jurkat cell line; machine learning; metaphase; mitosis; phenotype; prediction; prophase; telophase",2-s2.0-85028914209
"Singh A., Dutta M.K., Jennane R., Lespessailles E.","Classification of the trabecular bone structure of osteoporotic patients using machine vision",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031789173&doi=10.1016%2fj.compbiomed.2017.10.011&partnerID=40&md5=a168c9d6efda1388d0712b1fa641facb","Osteoporosis is a common bone disease which often leads to fractures. Clinically, the major challenge for the automatic diagnosis of osteoporosis is the complex architecture of bones. The clinical diagnosis of osteoporosis is conventionally done using Dual-energy X-ray Absorptiometry (DXA). This method has specific limitations, however, such as the large size of the instrument, a relatively high cost and limited availability. The method proposed here is based on the automatic processing of X-ray images. The bone X-ray image was statistically processed and strategically reformed to extract discriminatory statistical features of different orders. These features were used for machine learning for the classification of two populations composed of osteoporotic and healthy subjects. Four classifiers - support vector machine (SVM), k-nearest neighbors, Naïve Bayes and artificial neural network - were used to test the performance of the proposed method. Tests were performed on X-ray images of the calcaneus bone collected from the hospital of Orleans. The results are significant in terms of accuracy and time complexity. Experimental results indicate a classification rate of 98% using an SVM classifier which is encouraging for automatic osteoporosis diagnosis using bone X-ray images. The low time complexity of the proposed method makes it suitable for real time applications. © 2017 Elsevier Ltd","Bone X-ray images; Feature extraction; Image analysis; Osteoporosis; Supervised classification","Complex networks; Computer aided diagnosis; Computer vision; Diagnosis; Diseases; Feature extraction; Image analysis; Image processing; Learning systems; Nearest neighbor search; Neural networks; Supervised learning; Support vector machines; X ray analysis; Automatic processing; Complex architectures; Dual energy x ray absorptiometry (DXA); Osteoporosis; Real-time application; Supervised classification; Trabecular bone structures; X-ray image; Bone",2-s2.0-85031789173
"Izetta J., Verdes P.F., Granitto P.M.","Improved multiclass feature selection via list combination",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021896145&doi=10.1016%2fj.eswa.2017.06.043&partnerID=40&md5=d1d72ac3e4efa685a5ab3e4ee4289650","Feature selection is a crucial machine learning technique aimed at reducing the dimensionality of the input space. By discarding useless or redundant variables, not only it improves model performance but also facilitates its interpretability. The well-known Support Vector Machines–Recursive Feature Elimination (SVM-RFE) algorithm provides good performance with moderate computational efforts, in particular for wide datasets. When using SVM-RFE on a multiclass classification problem, the usual strategy is to decompose it into a series of binary ones, and to generate an importance statistics for each feature on each binary problem. These importances are then averaged over the set of binary problems to synthesize a single value for feature ranking. In some cases, however, this procedure can lead to poor selection. In this paper we discuss six new strategies, based on list combination, designed to yield improved selections starting from the importances given by the binary problems. We evaluate them on artificial and real-world datasets, using both One–Vs–One (OVO) and One–Vs–All (OVA) strategies. Our results suggest that the OVO decomposition is most effective for feature selection on multiclass problems. We also find that in most situations the new K-First strategy can find better subsets of features than the traditional weight average approach. © 2017 Elsevier Ltd","Feature selection; Multiclass problems; Support vector machine","Learning systems; Support vector machines; Computational effort; Machine learning techniques; Model performance; Multi-class problems; Multiclass classification problems; Real-world datasets; Recursive feature elimination; Redundant variables; Feature extraction",2-s2.0-85021896145
"Shahvari O., Logendran R.","A bi-objective batch processing problem with dual-resources on unrelated-parallel machines",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027543968&doi=10.1016%2fj.asoc.2017.08.014&partnerID=40&md5=e4efc81aa33049fc595deafdf381074c","In this paper, a bi-objective batch processing problem with dual-resources on unrelated-parallel machines is addressed. Typically, both machines and operators as dual-resources are required to process the setup and run of any batch on any machine. The pursuit of this research is motivated by the adoption of the just-in-time philosophy on unrelated-parallel machines, where both the production time and cost are considered. A mathematical programming model is proposed in four layers for simultaneously minimizing the production cost including total cost of tardy and early jobs along with total batch processing cost as well as the makespan with dual-resources. Four bi-objective particle swarm optimization based search algorithms are proposed to efficiently solve the optimization problem for medium- and large-size instances. To reflect the real industry requirements, dynamic machine and operator availability times, dynamic job release times, machine eligibility and capability for processing jobs, batch capacity limitations, machine-dependent setup time, and different skill levels of operators are considered. Several numerical examples are generated by a comprehensive data generation mechanism to compare the search algorithms with respect to four performance indicators. A comparison of small-size instances between performance indicators of the best search algorithm and optimization method shows the same performance to find non-dominated solutions in the final Pareto optimal solutions, while the best search algorithm spreads and extends more in the entire Pareto optimal region and shows that it is less capable in converging to the true Pareto optimal front. To the best of our knowledge, this research is the first of its kind to provide a comprehensive mathematical model for bi-objective batch processing problem with dual-resources. © 2017 Elsevier B.V.","Batch processing; Bi-objective; Dual-resources; Just-in-time; Particle swarm optimization; Unrelated-parallel machines","Batch data processing; Benchmarking; Costs; Learning algorithms; Mathematical programming; Optimization; Pareto principle; Particle swarm optimization (PSO); Problem solving; Bi objectives; Dual resource; Just in time; Mathematical programming models; Nondominated solutions; Pareto optimal solutions; Performance indicators; Unrelated parallel machines; Just in time production",2-s2.0-85027543968
"Barzegar R., Fijani E., Asghari Moghaddam A., Tziritis E.","Forecasting of groundwater level fluctuations using ensemble hybrid multi-wavelet neural network-based models",2017,"Science of the Total Environment",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018268569&doi=10.1016%2fj.scitotenv.2017.04.189&partnerID=40&md5=ed3dc5873acca1debad4a7c07e0bb173","Accurate prediction of groundwater level (GWL) fluctuations can play an important role in water resources management. The aims of the research are to evaluate the performance of different hybrid wavelet-group method of data handling (WA-GMDH) and wavelet-extreme learning machine (WA-ELM) models and to combine different wavelet based models for forecasting the GWL for one, two and three months step-ahead in the Maragheh–Bonab plain, NW Iran, as a case study. The research used totally 367 monthly GWLs (m) datasets (Sep 1985–Mar 2016) which were split into two subsets; the first 312 datasets (85% of total) were used for model development (training) and the remaining 55 ones (15% of total) for model evaluation (testing). The stepwise selection was used to select appropriate lag times as the inputs of the proposed models. The performance criteria such as coefficient of determination (R2), root mean square error (RMSE) and Nash-Sutcliffe efficiency coefficient (NSC) were used for assessing the efficiency of the models. The results indicated that the ELM models outperformed GMDH models. To construct the hybrid wavelet based models, the inputs and outputs were decomposed into sub-time series employing different maximal overlap discrete wavelet transform (MODWT) functions, namely Daubechies, Symlet, Haar and Dmeyer of different orders at level two. Subsequently, these sub-time series were served in the GMDH and ELM models as an input dataset to forecast the multi-step-ahead GWL. The wavelet based models improved the performances of GMDH and ELM models for multi-step-ahead GWL forecasting. To combine the advantages of different wavelets, a least squares boosting (LSBoost) algorithm was applied. The use of the boosting multi-WA-neural network models provided the best performances for GWL forecasts in comparison with single WA-neural network-based models. © 2017 Elsevier B.V.","ELM; Forecast; GMDH; Groundwater level; Iran; MODWT","Data handling; Efficiency; Forecasting; Groundwater; Groundwater resources; Learning systems; Mean square error; Time series; Water resources; Wavelet analysis; Wavelet transforms; Coefficient of determination; GMDH; Groundwater level fluctuation; Group method of data handling; Iran; Maximal overlap discrete wavelet transforms; MODWT; Water resources management; Discrete wavelet transforms; ground water; artificial neural network; ensemble forecasting; groundwater; numerical model; transform; water level; wavelet analysis; Article; artificial neural network; comparative study; ensemble model; extreme learning machine; information processing; Iran; least square analysis; machine learning; mathematical model; maximal overlap discrete wavelet transform; prediction and forecasting; priority journal; validation process; water management; water table; wavelet analysis; Iran",2-s2.0-85018268569
"Sublime J., Matei B., Cabanes G., Grozavu N., Bennani Y., Cornuéjols A.","Entropy based probabilistic collaborative clustering",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027515200&doi=10.1016%2fj.patcog.2017.07.014&partnerID=40&md5=179608bc7ceb0b1ccde55e7b8ed91475","Unsupervised machine learning approaches involving several clustering algorithms working together to tackle difficult data sets are a recent area of research with a large number of applications such as clustering of distributed data, multi-expert clustering, multi-scale clustering analysis or multi-view clustering. Most of these frameworks can be regrouped under the umbrella of collaborative clustering, the aim of which is to reveal the common underlying structures found by the different algorithms while analyzing the data. Within this context, the purpose of this article is to propose a collaborative framework lifting the limitations of many of the previously proposed methods: Our proposed collaborative learning method makes possible for a wide range of clustering algorithms from different families to work together based solely on their clustering solutions, thus lifting previous limitation requiring identical prototypes between the different collaborators. Our proposed framework uses a variational EM as its theoretical basis for the collaboration process and can be applied to any of the previously mentioned collaborative contexts. In this article, we give the main ideas and theoretical foundations of our method, and we demonstrate its effectiveness in a series of experiments on real data sets as well as data sets from the literature. © 2017 Elsevier Ltd","Collaborative clustering; EM algorithms; Entropy based methods","Cluster analysis; Entropy; Learning algorithms; Learning systems; Collaborative clustering; Collaborative framework; Collaborative learning; EM algorithms; Entropy-based methods; Multi-scale clustering; Theoretical foundations; Unsupervised machine learning; Clustering algorithms",2-s2.0-85027515200
"Dussaut J.S., Gallo C.A., Cravero F., Martínez M.J., Carballido J.A., Ponzoni I.","GeRNet: a gene regulatory network tool",2017,"BioSystems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029418178&doi=10.1016%2fj.biosystems.2017.08.006&partnerID=40&md5=bca71b80eadfec4a2d7b124d939e7549","Gene regulatory networks (GRNs) are crucial in every process of life since they govern the majority of the molecular processes. Therefore, the task of assembling these networks is highly important. In particular, the so called model-free approaches have an advantage modeling the complexities of dynamic molecular networks, since most of the gene networks are hard to be mapped with accuracy by any other mathematical model. A highly abstract model-free approach, called rule-based approach, offers several advantages performing data-driven analysis; such as the requirement of the least amount of data. They also have an important ability to perform inferences: its simplicity allows the inference of large size models with a higher speed of analysis. However, regarding these techniques, the reconstruction of the relational structure of the network is partial, hence incomplete, for an effective biological analysis. This situation motivated us to explore the possibility of hybridizing with other approaches, such as biclustering techniques. This led to incorporate a biclustering tool that finds new relations between the nodes of the GRN. In this work we present a new software, called GeRNeT that integrates the algorithms of GRNCOP2 and BiHEA along a set of tools for interactive visualization, statistical analysis and ontological enrichment of the resulting GRNs. In this regard, results associated with Alzheimer disease datasets are presented that show the usefulness of integrating both bioinformatics tools. © 2017 Elsevier B.V.","Biclustering; Gene regulatory network; Machine-learning; Microarray","quinolinic acid; bioinformatics; complexity; genetic analysis; genetic marker; machine learning; molecular analysis; statistical analysis; visualization; Alzheimer disease; Article; bioinformatics; clinical article; controlled study; evolutionary algorithm; gene regulatory network; genetic algorithm; human; hybridization; immunoreactivity; population size; protein phosphorylation; regulator gene; software; statistical analysis",2-s2.0-85029418178
"Wang X., Hu J., Huang Z.","Minimum class variance support vector ordinal regression",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032390254&doi=10.1007%2fs13042-016-0582-3&partnerID=40&md5=f9fb78a801edde63febc8a8929208045","The support vector ordinal regression (SVOR) method is derived from support vector machine and developed to tackle the ordinal regression problems. However, it ignores the distribution characteristics of the data. In this paper, we propose a novel method to handle the ordinal regression problems. This method is referred to as minimum class variance support vector ordinal regression (MCVSVOR). In contrast with SVOR, MCVSVOR explicitly takes into account the distribution of the categories and achieves better generalization performance. Moreover, the problem of MCVSVOR can be transformed into one of SVOR. Thus, the existing software of SVOR can be used to solve the problem of MCVSVOR. In the paper, we first discuss the linear case of MCVSVOR and then develop the nonlinear MCVSVOR through using the kernelization trick. The comprehensive experiment results show that the proposed method is effective and can achieve better generalization performance in contrast with SVOR. © 2016, Springer-Verlag Berlin Heidelberg.","Machine learning; Ordinal regression; Support vector machine; Support vector ordinal regression","Learning systems; Regression analysis; Support vector machines; Vectors; Distribution characteristics; Generalization performance; Kernelization; Ordinal regression; Support vector; Problem solving",2-s2.0-85032390254
"Camalan M., Çavur M., Hoşten Ç.","Assessment of chromite liberation spectrum on microscopic images by means of a supervised image classification",2017,"Powder Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029393587&doi=10.1016%2fj.powtec.2017.08.063&partnerID=40&md5=11c5c164d8c7b0c413845c0801704517","Assessment of mineral liberation spectrum with all its aspects is essential for plant control and optimization. This paper aims to estimate 2D mineral map and its associated liberation spectrum of a particular chromite sample from optical micrographs by using Random Forest Classification, a powerful machine-learning algorithm implemented on a user-friendly and an open-source software. This supervised classification method can be used to accurately generate 2D mineral map of this chromite sample. The variation of the measured spectra with the sample size is studied, showing that images of 200 particles randomly selected from the optical micrographs are sufficient to reproduce liberation spectrum of this sample. In addition, the 2D spectrum obtained with this classification method is compared with the one obtained from the Mineral Liberation Analyzer (MLA). Although 2D mineralogical compositions obtained by the two methods are quite similar, microscopic analysis estimates poorer liberation than MLA due to the residual noise (misclassified gangue) generated by the classification. Nevertheless, we cannot compare the reliabilities of the two methods, as there is not a standard produce yet to quantify the accuracy of MLA analysis. © 2017 Elsevier B.V.","Image classification; Mineral liberation; Mineral liberation analyzer; Optical microscope; Random forest tree","Chromite; Chromite deposits; Decision trees; Learning algorithms; Learning systems; Minerals; Open source software; Open systems; Optical microscopy; Software engineering; Spectrum analysis; Supervised learning; Classification methods; Microscopic analysis; Mineral liberation; Mineralogical compositions; Random forest classification; Random forests; Supervised classification; Supervised image classifications; Image classification; chromite; mineral; unclassified drug; Article; classification algorithm; image analysis; image processing; image quality; imaging software; microscopy; particle size; software design; supervised machine learning; surface property",2-s2.0-85029393587
"Wang X., Thome N., Cord M.","Gaze latent support vector machine for image classification improved by weakly supervised region selection",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027531875&doi=10.1016%2fj.patcog.2017.07.001&partnerID=40&md5=2be2e788b089e5c4aa5dd5ae30577b57","This paper deals with Weakly Supervised Learning (WSL), i.e. performing image classification by leveraging local information with models trained from global image labels. We propose a new WSL method which incorporates gaze features collected by an eye-tracker to guide the region selection policy. Our approach presents two appealing advantages: gaze features are cheap to collect, e.g. one order of magnitude faster than bounding boxes, and our method only requires gaze features during training, while being gaze free at test time. For this purpose, the training objective is enriched with a gaze loss, from which we derive a concave-convex upper bound, leading to an off-the-shelf CCCP optimization scheme. Extensive experiments are conducted to validate the effectiveness of the approach for WSL image classification on two public datasets with gaze annotation, i.e. PASCAL VOC 2012 action and POET. In addition, we publicly release a new food-related dataset, the Gaze-based UPMC Food dataset (UPMC-G20), which covers 20 food categories and 2,000 images. This dataset intends to promote the research in the food-related computer vision community. © 2017 Elsevier Ltd","Human gaze; image classification; Weakly supervised learning","Classification (of information); Supervised learning; Convex upper bound; Human gaze; Latent support vector machines; Local information; Optimization scheme; Region selections; Vision communities; Weakly supervised learning; Image classification",2-s2.0-85027531875
"Peska L., Buza K., Koller J.","Drug-target interaction prediction: A Bayesian ranking approach",2017,"Computer Methods and Programs in Biomedicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029180108&doi=10.1016%2fj.cmpb.2017.09.003&partnerID=40&md5=afbe978c3c69d30371ec60e3643e9168","Background and objective In silico prediction of drug-target interactions (DTI) could provide valuable information and speed-up the process of drug repositioning – finding novel usage for existing drugs. In our work, we focus on machine learning algorithms supporting drug-centric repositioning approach, which aims to find novel usage for existing or abandoned drugs. We aim at proposing a per-drug ranking-based method, which reflects the needs of drug-centric repositioning research better than conventional drug-target prediction approaches. Methods We propose Bayesian Ranking Prediction of Drug-Target Interactions (BRDTI). The method is based on Bayesian Personalized Ranking matrix factorization (BPR) which has been shown to be an excellent approach for various preference learning tasks, however, it has not been used for DTI prediction previously. In order to successfully deal with DTI challenges, we extended BPR by proposing: (i) the incorporation of target bias, (ii) a technique to handle new drugs and (iii) content alignment to take structural similarities of drugs and targets into account. Results Evaluation on five benchmark datasets shows that BRDTI outperforms several state-of-the-art approaches in terms of per-drug nDCG and AUC. BRDTI results w.r.t. nDCG are 0.929, 0.953, 0.948, 0.897 and 0.690 for G-Protein Coupled Receptors (GPCR), Ion Channels (IC), Nuclear Receptors (NR), Enzymes (E) and Kinase (K) datasets respectively. Additionally, BRDTI significantly outperformed other methods (BLM-NII, WNN-GIP, NetLapRLS and CMF) w.r.t. nDCG in 17 out of 20 cases. Furthermore, BRDTI was also shown to be able to predict novel drug-target interactions not contained in the original datasets. The average recall at top-10 predicted targets for each drug was 0.762, 0.560, 1.000 and 0.404 for GPCR, IC, NR, and E datasets respectively. Conclusions Based on the evaluation, we can conclude that BRDTI is an appropriate choice for researchers looking for an in silico DTI prediction technique to be used in drug-centric repositioning scenarios. BRDTI Software and supplementary materials are available online at www.ksi.mff.cuni.cz/∼peska/BRDTI. © 2017 Elsevier B.V.","Bayesian personalized ranking; Drug repositioning; Drug-target interactions; Machine learning","Artificial intelligence; Enzymes; Factorization; Forecasting; Integrated circuits; Learning algorithms; Learning systems; Tensors; Bayesian; Drug repositioning; Drug-target interactions; G-protein coupled receptors; Matrix factorizations; Prediction techniques; State-of-the-art approach; Structural similarity; Drug interactions; cell nucleus receptor; enzyme; G protein coupled receptor; ion channel; phosphotransferase; Article; Bayesian learning; drug repositioning; drug targeting; prediction; receiver operating characteristic",2-s2.0-85029180108
"Pham B.T., Tien Bui D., Prakash I.","Landslide Susceptibility Assessment Using Bagging Ensemble Based Alternating Decision Trees, Logistic Regression and J48 Decision Trees Methods: A Comparative Study",2017,"Geotechnical and Geological Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019911590&doi=10.1007%2fs10706-017-0264-2&partnerID=40&md5=92a603e13dee069098c7f9daf10dff16","In this study, we have evaluated and compared prediction capability of Bagging Ensemble Based Alternating Decision Trees (BADT), Logistic Regression (LR), and J48 Decision Trees (J48DT) for landslide susceptibility mapping at part of the Uttarakhand State (India). The BADT method has been proposed in the present study which is a novel hybrid machine learning ensemble approach of bagging ensemble and alternating decision trees. The J48DT is a relative new machine learning technique which has been applied only in few landslide studies, and the LR is known as a popular landslide susceptibility model. For the model studies, a spatial database of 930 historical landslide events and 15 landslide affecting factors have been collected and analyzed. This database has been used to build and validate the landslide models namely BADT, LR and J48DT Predictive capability of these models has been validated and compared using statistical analyzing methods and Receiver Operating Characteristic (ROC) curve. Results show that these three landslide models (BADT, LR and J48DT) performed well with the training dataset. However, using the validation dataset the BADT model has the highest prediction capability, followed by the LR model, and the J48DT model, respectively. This indicates that the BADT is a promising method which can be used for landslide susceptibility assessment also for other landslide prone areas. © 2017, Springer International Publishing Switzerland.","Alternating decision trees; Bagging ensemble; Decision trees; GIS; India; Landslides; Logistic regression","Artificial intelligence; Forestry; Geographic information systems; Landslides; Learning algorithms; Learning systems; Regression analysis; Alternating decision trees; Bagging ensemble; India; Landslide susceptibility assessments; Landslide susceptibility mapping; Logistic regressions; Machine learning techniques; Receiver operating characteristic curves; Decision trees",2-s2.0-85019911590
"Lins A.J.C.C., Muniz M.T.C., Garcia A.N.M., Gomes A.V., Cabral R.M., Bastos-Filho C.J.A.","Using artificial neural networks to select the parameters for the prognostic of mild cognitive impairment and dementia in elderly individuals",2017,"Computer Methods and Programs in Biomedicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029670932&doi=10.1016%2fj.cmpb.2017.09.013&partnerID=40&md5=b55b17cabcd607e33eaa0788501a7553","Background and Objectives: A huge number of solutions based on computational systems have been recently developed for the classification of cognitive abnormalities in older people, so that individuals at high risk of developing neurodegenerative diseases, such as Cognitive Impairment and Alzheimer?s disease, can be identified before the manifestation of the diseases. Several factors are related to these pathologies, making the diagnostic process a hard problem to solve. This paper proposes a computational model based on the artificial neural network to classify data patterns of older adults. Methods: The proposal takes into account the several parameters as diagnostic factors as gender, age, the level of education, study time, and scores from cognitive tests (Mini-Mental State Examination, Semantic Verbal Fluency Test, Clinical Dementia Rating and Ascertaining Dementia). This non-linear regression model is designed to classify healthy and pathological aging with machine learning techniques such as neural networks, random forest, SVM, and stochastic gradient boosting. We deployed a simple linear regression model for the sake of comparison. The primary objective is to use a regression model to analyze the data set aiming to check which parameters are necessary to achieve high accuracy in the diagnosis of neurodegenerative disorders. Results: The analysis demonstrated that the usage of cognitive tests produces median values for the accuracy greater than 90%. The ROC analysis shows that the best sensitivity performance is above 98% and specificity of 96% when the configurations have only cognitive tests. Conclusions: The presented approach is a valuable tool for identifying patients with dementia or MCI and for supporting the clinician in the diagnostic process, by providing an outstanding support decision tool in the diagnostics of neurodegenerative diseases. © 2017 Elsevier B.V.","Aging; Alzheimer's disease; Artificial neural networks; Dementia; Mild cognitive impairment; Regression","Adaptive boosting; Aging of materials; Cognitive systems; Decision trees; Diagnosis; Education; Learning systems; Linear regression; Neural networks; Regression analysis; Semantics; Stochastic models; Stochastic systems; Alzheimer's disease; Dementia; Machine learning techniques; Mild cognitive impairments; Mini-Mental State Examination; Neurodegenerative disorders; Regression; Stochastic gradient boosting; Neurodegenerative diseases; adult; age; aged; Alzheimer disease; artificial neural network; Ascertaining Dementia; classification algorithm; Clinical Dementia Rating; comparative study; computer assisted diagnosis; controlled study; dementia assessment; diagnostic accuracy; educational status; female; gender; human; major clinical study; male; mild cognitive impairment; Mini Mental State Examination; prognosis; random forest; receiver operating characteristic; Review; Semantic Verbal Fluency Test; sensitivity and specificity; stochastic model; support vector machine",2-s2.0-85029670932
"Soleimani H., Tomasin S., Alizadeh T., Shojafar M.","Cluster-head based feedback for simplified time reversal prefiltering in ultra-wideband systems",2017,"Physical Communication",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030175017&doi=10.1016%2fj.phycom.2017.09.005&partnerID=40&md5=39b699e0878f07d6a3f8fcae9d3d4d7b","Time-reversal prefiltering (TRP) technique for impulse radio (IR) ultra wide-band (UWB) systems requires a large amount of feedback to transmit the channel impulse response from the receiver to the transmitter. In this paper, we propose a new feedback design based on vector quantization. We use a machine learning algorithm to cluster the estimated channels into several groups and to select the channel cluster heads (CCHs) for feedback. In particular, CCHs and their labels are recorded at both side of the UWB transceivers and the label of the most similar CCH to the estimated channel is fed back to the transmitter. Finally, the TRP is applied using the feedback CCH. The proposed digital feedback provides three main advantages: (1) it significantly reduces the dedicated bandwidth required for feedback; (2) it considerably improves the speed of transceivers; and, (3) it is robust to noise in the feedback channel since few bytes are required to send the codes that can be heavily error protected. Numerical results on standard UWB channel models are discussed, showing the advantage of the proposed solution. © 2017 Elsevier B.V.","Clustering channels; Digital feedback; IEEE 802.15.SG3 channel model; Machine learning; Prefiltering; Ultra wide-band (UWB)","Artificial intelligence; Broadband networks; Impulse response; Learning algorithms; Learning systems; Radio transceivers; Radio transmission; Transceivers; Transmitters; Channel impulse response; Channel model; Clustering channels; Digital feedback; Estimated channels; Pre-filtering; Ultra wide band systems; UWB channel models; Ultra-wideband (UWB)",2-s2.0-85030175017
"Yilmaz L., Franco-Watkins A., Kroecker T.S.","Computational models of ethical decision-making: A coherence-driven reflective equilibrium model",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016088149&doi=10.1016%2fj.cogsys.2017.02.005&partnerID=40&md5=31db33f25e5f066265bbbaf5c3b27841","There are scientific and technical challenges that must be addressed in developing systems that interact with humans and work along with other agents in complex, dynamic, and uncertain environments where ethical concerns may arise. In such systems relationships between users and autonomous components will be driven as much by issues such as trust, responsibility, and acceptability, as technical ones such as planning and coordination. This paper provides a comprehensive review and classification of existing methods in machine ethics, resulting in delineation of specific challenges and issues. To address the identified challenges, we introduce a method that leverages the method of reflective equilibrium and the multi-coherence theory as a unifying constraint satisfaction framework to simultaneously assess multiple ethical principles and manage ethical conflicts in a context-sensitive manner. © 2017 Elsevier B.V.","Cognitive agent; Cognitive coherence; Decision-making; Machine ethics; Reflective equilibrium","Computation theory; Decision making; Autonomous components; Cognitive agents; Cognitive coherence; Constraint Satisfaction; Equilibrium modeling; Ethical decision making; Technical challenges; Uncertain environments; Philosophical aspects; Article; computer simulation; conceptual framework; conflict of interest; engineering; ethical decision making; goal attainment; job satisfaction; learning theory; machine learning; mathematical model; medical ethics; priority journal; sense of coherence",2-s2.0-85016088149
"Chen H., Gao P., Tan S., Tang J., Yuan H.","Online sequential condition prediction method of natural circulation systems based on EOS-ELM and phase space reconstruction",2017,"Annals of Nuclear Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027876373&doi=10.1016%2fj.anucene.2017.08.035&partnerID=40&md5=33c3084a6b1530dde0c86fa32e10c06c","Natural circulation design is widely used in the passive safety systems of advanced nuclear power reactors. The irregular and chaotic flow oscillations are often observed in boiling natural circulation systems so it is difficult for operators to monitor and predict the condition of these systems. An online condition forecasting method for natural circulation system is proposed in this study as an assisting technique for plant operators. The proposed prediction approach was developed based on Ensemble of Online Sequential Extreme Learning Machine (EOS-ELM) and phase space reconstruction. Online Sequential Extreme Learning Machine (OS-ELM) is an online sequential learning neural network algorithm and EOS-ELM is the ensemble method of it. The proposed condition prediction method can be initiated by a small chunk of monitoring data and it can be updated by newly arrived data at very fast speed during the online prediction. Simulation experiments were conducted on the data of two natural circulation loops to validate the performance of the proposed method. The simulation results show that the proposed predication model can successfully recognize different types of flow oscillations and accurately forecast the trend of monitored plant variables. The influence of the number of hidden nodes and neural network inputs on prediction performance was studied and the proposed model can achieve good accuracy in a wide parameter range. Moreover, the comparison results show that the proposed condition prediction method has much faster online learning speed and better prediction accuracy than conventional neural network model. © 2017","Extreme learning machine; Flow oscillation; Natural circulation; Nuclear power plant condition monitoring; Online sequential prediction","Condition monitoring; Forecasting; Knowledge acquisition; Learning systems; Natural convection; Neural networks; Nuclear energy; Nuclear fuels; Nuclear power plants; Online systems; Oscillating flow; Phase space methods; Extreme learning machine; Flow oscillations; Natural circulation; Natural circulation system; Online sequential extreme learning machine; Online sequential prediction; Phase space reconstruction; Sequential learning neural networks; E-learning",2-s2.0-85027876373
"Rastogi A., Sampath S.","Multi-task learning via linear functional strategy",2017,"Journal of Complexity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028306635&doi=10.1016%2fj.jco.2017.08.001&partnerID=40&md5=fc50416cbacfbe1e139aca25590a5e94","In machine learning, the multi-task learning is a natural approach that exploits the relations among different tasks to improve the performance. We develop a theoretical analysis of multi-penalty least-square regularization scheme on the reproducing kernel Hilbert space in vector-valued setting. The results hold for general framework of vector-valued functions; therefore they can be applied to multi-task learning problems. We study an approach for multi-penalty regularization scheme which is based on the idea of linear combination of different regularized solutions. We estimate the coefficients of the linear combination by means of the so-called linear functional strategy. We discuss a theoretical justification of the linear functional strategy which particularly provides the optimal convergence rates for multi-penalty regularization scheme. Finally, we provide an empirical analysis of the multi-view manifold regularization scheme based on the linear functional strategy for the challenging multi-class image classification and species recognition with attributes. © 2017 Elsevier Inc.","Error estimate; Linear functional strategy; Manifold learning; Multi-penalty regularization; Multi-task learning; Vector-valued RKHS","Learning algorithms; Vector spaces; Vectors; Error estimates; Linear functional; Manifold learning; Multi-penalty regularization; Multitask learning; Vector valued; Learning systems",2-s2.0-85028306635
"Ooi S.Y., Tan S.C., Cheah W.P.","Temporal sampling forest (TS-F): an ensemble temporal learner",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978153695&doi=10.1007%2fs00500-016-2242-7&partnerID=40&md5=3f4d36e6cdebd480e8b8d6e114c0960e","Ensemble learning is in favour of machine learning community due to its tolerance in handling divergence and biasness issues faced by a single learner. In this work, an ensemble temporal learner, namely temporal sampling forest (TS-F), is proposed. Building on the random forest, we consider its limitations in handling temporal classification tasks. Temporal data classification is an important area of machine learning and data mining, where it fills the gap of ordinary data classification when the observed datasets are temporally related across sequential and time domains. TS-F incorporated the temporal sampling (bagging) and temporal randomization procedures in the classical random forest, hence extending its ability to handle temporal data. TS-F was tested on 11 public sequential and temporal datasets from different domains. Experiments demonstrate that TS-F could provide promising results with average classification accuracy of 98 %, substantiating its ability to escalate the random forest performance in the application of temporal classification. © 2016, Springer-Verlag Berlin Heidelberg.","Ensemble learner; Random forest; Temporal application; Temporal classification","Artificial intelligence; Data mining; Decision trees; Learning systems; Classification accuracy; Data classification; Ensemble learner; Machine learning communities; Random forests; Randomization procedure; Temporal applications; Temporal classification; Classification (of information)",2-s2.0-84978153695
"Venkatesan R., Er M.J., Dave M., Pratama M., Wu S.","A novel online multi-label classifier for high-speed streaming data applications",2017,"Evolving Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032579461&doi=10.1007%2fs12530-016-9162-8&partnerID=40&md5=080d27330af8878496f3daa71d92aef6","In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classification problems are far more complex than binary and multi-class classification problems, as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. The proposed work exploits the high-speed nature of the extreme learning machines to achieve real-time multi-label classification of streaming data. A new threshold-based online sequential learning algorithm is proposed for high speed and streaming data classification of multi-label problems. The proposed method is experimented with six different datasets from different application domains such as multimedia, text, and biology. The hamming loss, accuracy, training time and testing time of the proposed technique is compared with nine different state-of-the-art methods. Experimental studies shows that the proposed technique outperforms the existing multi-label classifiers in terms of performance and speed. © 2016, Springer-Verlag Berlin Heidelberg.","Classification; Extreme learning machines; High speed; Multi-label; Real-time","Bins; Classifiers; Knowledge acquisition; Learning algorithms; Learning systems; Speed; Extreme learning machine; High Speed; Multi label classification; Multi-class classification; Multi-label; Multiclass classification problems; Real time; Sequential learning algorithm; Classification (of information)",2-s2.0-85032579461
"Zhang L., Yang J., Zhang D.","Domain class consistency based transfer learning for image classification across domains",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026919224&doi=10.1016%2fj.ins.2017.08.034&partnerID=40&md5=503f14855dcd6eb2e1edbdaddb0faed0","Distribution mismatch between the modeling data and the query data is a known domain adaptation issue in machine learning. To this end, in this paper, we propose a l2,1-norm based discriminative robust kernel transfer learning (DKTL) method for high-level recognition tasks. The key idea is to realize robust domain transfer by simultaneously integrating domain-class-consistency (DCC) metric based discriminative subspace learning, kernel learning in reproduced kernel Hilbert space, and representation learning between source and target domain. The DCC metric includes two properties: domain-consistency used to measure the between-domain distribution discrepancy and class-consistency used to measure the within-domain class separability. The essential objective of the proposed transfer learning method is to maximize the DCC metric, which is equivalently to minimize the domain-class-inconsistency (DCIC), such that domain distribution mismatch and class inseparability are well formulated and unified simultaneously. The merits of the proposed method include (1) the robust sparse coding selects a few valuable source data with noises (outliers) removed during knowledge transfer, and (2) the proposed DCC metric can pursue more discriminative subspaces of different domains. As a result, the maximum class-separability is also well guaranteed. Extensive experiments on a number of visual datasets demonstrate the superiority of the proposed method over other state-of-the-art domain adaptation and transfer learning methods. © 2017 Elsevier Inc.","Domain adaptation; Kernel learning; Representation learning; Subspace learning; Transfer learning","Knowledge management; Learning systems; Domain adaptation; Kernel learning; Representation learning; Subspace learning; Transfer learning; Image classification",2-s2.0-85026919224
"Du J., Vong C.-M., Pun C.-M., Wong P.-K., Ip W.-F.","Post-boosting of classification boundary for imbalanced data using geometric mean",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030687939&doi=10.1016%2fj.neunet.2017.09.004&partnerID=40&md5=ba09e0f548290121ff6f6a53aff6d73d","In this paper, a novel imbalance learning method for binary classes is proposed, named as Post-Boosting of classification boundary for Imbalanced data (PBI), which can significantly improve the performance of any trained neural networks (NN) classification boundary. The procedure of PBI simply consists of two steps: an (imbalanced) NN learning method is first applied to produce a classification boundary, which is then adjusted by PBI under the geometric mean (G-mean). For data imbalance, the geometric mean of the accuracies of both minority and majority classes is considered, that is statistically more suitable than the common metric accuracy. PBI also has the following advantages over traditional imbalance methods: (i) PBI can significantly improve the classification accuracy on minority class while improving or keeping that on majority class as well; (ii) PBI is suitable for large data even with high imbalance ratio (up to 0.001). For evaluation of (i), a new metric called Majority loss/Minority advance ratio (MMR) is proposed that evaluates the loss ratio of majority class to minority class. Experiments have been conducted for PBI and several imbalance learning methods over benchmark datasets of different sizes, different imbalance ratios, and different dimensionalities. By analyzing the experimental results, PBI is shown to outperform other imbalance learning methods on almost all datasets. © 2017 Elsevier Ltd","Boosting; Imbalance learning; SMOTE; Weighted ELM","Geometry; Learning systems; Benchmark datasets; Boosting; Classification accuracy; Classification boundary; Imbalance learning; SMOTE; Trained neural networks; Weighted ELM; Classification (of information); Article; artificial neural network; classification boundary; classifier; data processing; image processing; imbalance learning; machine learning; priority journal; sensitivity and specificity",2-s2.0-85030687939
"Christensen K., Liland K.H., Kvaal K., Risvik E., Biancolillo A., Scholderer J., Nørskov S., Næs T.","Mining online community data: The nature of ideas in online communities",2017,"Food Quality and Preference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020649599&doi=10.1016%2fj.foodqual.2017.06.001&partnerID=40&md5=8e93edf4abcb6556eb8290d1693f9a7e","Ideas are essential for innovation and for the continuous renewal of a firm's product offerings. Previous research has argued that online communities contain such ideas. Therefore, online communities such as forums, Facebook groups, blogs etc. are potential gold mines for innovative ideas that can be used for boosting the innovation performance of the firm. However, the nature of online community data makes idea detection labor intensive. As an answer to this problem, research has shown that it might be possible to detect ideas from online communities, automatically. Research is however, yet to provide an answer to what is it that makes such automatic idea detection possible? Our study is based on two datasets from dialogue between members of two distinct online communities. The first community is related to beer. The second is related to Lego. We generate machine learning classifiers based on Support Vector Machines and Partial Least Squares that can detect ideas from each respective online community. We use partial least squares to investigate what are the words and expressions that allows for automatic classification of ideas. We conclude that ideas from the two online communities, contains suggestion/solution words and expressions and it is these that make automatic idea detection possible. In addition we conclude that the nature of the ideas in the beer community seems to be related to the brewing process. The nature of the ideas in the Lego community seems to be related to new products that consumers would want. © 2017","Machine learning; Natural language processing; Online communities, ideas; Partial least squares; Support vector machines; Text mining",,2-s2.0-85020649599
"Stegmann P.G., Yang P.","A regional, size-dependent, and causal effective medium model for Asian and Saharan mineral dust refractive index spectra",2017,"Journal of Aerosol Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031302768&doi=10.1016%2fj.jaerosci.2017.10.003&partnerID=40&md5=c02321a9c9e6d2cc790179aab8168d29","A model is developed for the refractive index spectra of desert mineral dust. This model is applicable regionally to both Asian and Saharan dust as the largest global aerosol sources. The capability of the model further aims at representing important local features through a subdivision into northern and southern Sahara, as well as western and eastern Asia. Machine learning techniques for accelerated literature data acquisition are presented. Available refractive index spectra for individual minerals and chemical species are combined based on the Bruggeman effective medium formula. A numerical procedure for effectively solving the resulting higher-order polynomial expression is presented. The present results of the effective refractive indices are validated through the Kramers-Kronig relation; in particular, a Hilbert transform is applied to the imaginary part of the refractive index spectra. © 2017 Elsevier Ltd","Effective medium; Gobi; Index of refraction; Kramers-Kronig; Mineral dust; Sahara","Data acquisition; Dust; Kramers-Kronig relations; Learning systems; Mathematical transformations; Minerals; Effective medium; Gobi; Index of refraction; Kramers-Kronig; Mineral dust; Sahara; Refractive index; aerosol; atmospheric modeling; data acquisition; dust; machine learning; refractive index; size structure; spectral analysis; aerosol; Article; Asia; boundary layer; chemical composition; Fourier transformation; information processing; machine learning; mineral dust; particle size; plume; priority journal; refraction index; Asia; Gobi Desert; Sahara",2-s2.0-85031302768
"Volkova S., Charles L.E., Harrison J., Corley C.D.","Uncovering the relationships between military community health and affects expressed in social media",2017,"EPJ Data Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020618360&doi=10.1140%2fepjds%2fs13688-017-0102-z&partnerID=40&md5=f4a3431628e0ef96141cae35a7f19235","Military populations present a small, unique community whose mental and physical health impacts the security of the nation. Recent literature has explored social media’s ability to enhance disease surveillance and characterize distinct communities with encouraging results. We present a novel analysis of the relationships between influenza-like illnesses (ILI) clinical data and affects (i.e., emotions and sentiments) extracted from social media around military facilities. Our analyses examine (1) differences in affects expressed by military and control populations, (2) affect changes over time by users, (3) differences in affects expressed during high and low ILI seasons, and (4) correlations and cross-correlations between ILI clinical visits and affects from an unprecedented scale - 171M geo-tagged tweets across 31 global geolocations. Key findings include: Military and control populations differ in the way they express affects in social media over space and time. Control populations express more positive and less negative sentiments and less sadness, fear, disgust, and anger emotions than military. However, affects expressed in social media by both populations within the same area correlate similarly with ILI visits to military health facilities. We have identified potential responsible cofactors leading to location variability, e.g., region or state locale, military service type and/or the ratio of military to civilian populations. For most locations, ILI proportions positively correlate with sadness and neutral sentiment, which are the affects most often expressed during high ILI season. The ILI proportions negatively correlate with fear, disgust, surprise, and positive sentiment. These results are similar to the low ILI season where anger, surprise, and positive sentiment are highest. Finally, cross-correlation analysis shows that most affects lead ILI clinical visits, i.e. are predictive of ILI data, with affect-ILI leading intervals dependent on geolocation and affect type. Overall, information gained in this study exemplifies a usage of social media data to understand the correlation between psychological behavior and health in the military population and the potential for use of social media affects for prediction of ILI cases. © 2017, The Author(s).","biosurveillance; emotion detection; influenza; machine learning; natural language processing; sentiment analysis; social media analytics","Behavioral research; Health; Inspection; Learning algorithms; Learning systems; Natural language processing systems; Population statistics; Social networking (online); Biosurveillance; Emotion detection; influenza; Sentiment analysis; Social media analytics; Economic and social effects",2-s2.0-85020618360
"Mishra A., Chandra P., Ghose U., Sodhi S.S.","Bi-modal derivative adaptive activation function sigmoidal feedforward artificial neural networks",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029701430&doi=10.1016%2fj.asoc.2017.09.002&partnerID=40&md5=2e64b43118b5d39edf6211b562f005db","In this work an adaptive mechanism for choosing the activation function is proposed and described. Four bi-modal derivative sigmoidal adaptive activation function is used as the activation function at the hidden layer of a single hidden layer sigmoidal feedforward artificial neural networks. These four bi-modal derivative activation functions are grouped as asymmetric and anti-symmetric activation functions (in groups of two each). For the purpose of comparison, the logistic function (an asymmetric function) and the function obtained by subtracting 0.5 from it (an anti-symmetric) function is also used as activation function for the hidden layer nodes’. The resilient backpropagation algorithm with improved weight-tracking (iRprop+) is used to adapt the parameter of the activation functions and also the weights and/or biases of the sigmoidal feedforward artificial neural networks. The learning tasks used to demonstrate the efficacy and efficiency of the proposed mechanism are 10 function approximation tasks and four real benchmark problems taken from the UCI machine learning repository. The obtained results demonstrate that both for asymmetric as well as anti-symmetric activation usage, the proposed/used adaptive activation functions are demonstratively as good as if not better than the sigmoidal function without any adaptive parameter when used as activation function of the hidden layer nodes. © 2017 Elsevier B.V.","Activation function; Activation function adaptation; Bi-modal derivative activation function; Resilient backpropagation algorithm; Sigmoidal feed-forward artificial neural network","Backpropagation; Backpropagation algorithms; Bismuth compounds; Feedforward neural networks; Learning systems; Neural networks; Activation functions; Adaptive activation function; Bench-mark problems; Feed-forward artificial neural networks; Function approximation; Resilient backpropagation; Sigmoidal functions; UCI machine learning repository; Chemical activation",2-s2.0-85029701430
"Zhu X., Pedrycz W., Li Z.","Fuzzy clustering with nonlinearly transformed data",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029186806&doi=10.1016%2fj.asoc.2017.07.026&partnerID=40&md5=ee61576fcad37b1c0ca67f4d800e45cd","The Fuzzy C-Means (FCM) algorithm is a widely used objective function-based clustering method exploited in numerous applications. In order to improve the quality of clustering algorithms, this study develops a novel approach, in which a transformed data-based FCM is developed. Two data transformation methods are proposed, using which the original data are projected in a nonlinear fashion onto a new space of the same dimensionality as the original one. Next, clustering is carried out on the transformed data. Two optimization criteria, namely a classification error and a reconstruction error, are introduced and utilized to guide the optimization of the performance of the new clustering algorithm and a transformation of the original data space. Unlike other data transformation methods that require some prior knowledge, in this study, Particle Swarm Optimization (PSO) is used to determine the optimal transformation realized on a basis of a certain performance index. Experimental studies completed for a synthetic data set and a number of data sets coming from the Machine Learning Repository demonstrate the performance of the FCM with transformed data. The experiments show that the proposed fuzzy clustering method achieves better performance (in terms of the clustering accuracy and the reconstruction error) in comparison with the outcomes produced by the generic version of the FCM algorithm. © 2017 Elsevier B.V.","Classification error rate; Data transformation; Fuzzy clustering; Particle Swarm Optimization (PSO); Reconstruction error","Cluster analysis; Errors; Evolutionary algorithms; Fuzzy clustering; Fuzzy systems; Learning systems; Mathematical transformations; Metadata; Optimization; Particle swarm optimization (PSO); Classification error rate; Data transformation; Fuzzy C-means algorithms; Fuzzy clustering method; Machine learning repository; New clustering algorithms; Objective function-based clustering; Reconstruction error; Clustering algorithms",2-s2.0-85029186806
"Faiz M., Anuar N.B., Wahab A.W.A., Shamshirband S., Chronopoulos A.T.","Source camera identification: a distributed computing approach using Hadoop",2017,"Journal of Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027711546&doi=10.1186%2fs13677-017-0088-x&partnerID=40&md5=65eb566a754bea86c2b9344f6acec9b0","The widespread use of digital images has led to a new challenge in digital image forensics. These images can be used in court as evidence of criminal cases. However, digital images are easily manipulated which brings up the need of a method to verify the authenticity of the image. One of the methods is by identifying the source camera. In spite of that, it takes a large amount of time to be completed by using traditional desktop computers. To tackle the problem, we aim to increase the performance of the process by implementing it in a distributed computing environment. We evaluate the camera identification process using conditional probability features and Apache Hadoop. The evaluation process used 6000 images from six different mobile phones of the different models and classified them using Apache Mahout, a scalable machine learning tool which runs on Hadoop. We ran the source camera identification process in a cluster of up to 19 computing nodes. The experimental results demonstrate exponential decrease in processing times and slight decrease in accuracies as the processes are distributed across the cluster. Our prediction accuracies are recorded between 85 to 95% across varying number of mappers. © 2017, The Author(s).","Distributed computing; Hadoop; Mahout; Source camera identification","Cameras; Computer software; Digital forensics; Distributed computer systems; Image processing; Learning systems; Personal computers; Camera identifications; Conditional probabilities; Digital image forensics; Distributed computing environment; Hadoop; Mahout; Scalable machine learning; Source camera identifications; Cluster computing",2-s2.0-85027711546
"Chaib Draa I., Niar S., Tayeb J., Grislin E., Desertot M.","Sensing user context and habits for run-time energy optimization",2017,"Eurasip Journal on Embedded Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978708913&doi=10.1186%2fs13639-016-0036-8&partnerID=40&md5=55ee6bd6ea966304a527d52c62e16495","Optimizing energy consumption in modern mobile handheld devices plays a very important role as lowering energy consumption impacts battery life and system reliability. With next-generation smartphones and tablets, the number of sensors and communication tools will increase and more and more communication interfaces and protocols such as Wi-Fi, Bluetooth, GPRS, UMTS, and LTE will be incorporated. Consequently, the fraction of energy consumed by these components will be larger. Nevertheless, the use of the large amount of data from the different sensors can be beneficial to detect the changing user context, to understand habits, and to detect running application needs. All these information, when used properly, may lead to an efficient energy consumption control. This paper proposes a tool to analyze user/application interaction to understand how the different hardware components are used at run-time and optimize them. The idea here is to use machine learning methods to identify and classify user behaviors and habit information. Using this tool, a software has been developed to control at run-time system component activities that have high impacts on the energy consumption. The tool allows also to predict future applications usages. By this way, screen brightness, CPU frequency, Wi-Fi connectivity, and playback sound level can be optimized while meeting the applications and the user requirements. Our experimental results show that the proposed solution can lower the energy consumption by up to 30 % versus the out-of-the-box power governor, while maintaining a negligible system overhead. © 2016, Chaib Draa et al.","Applications sequences prediction; Device’s context; Energy consumption; Run-time user and application analysis","Artificial intelligence; Classification (of information); Energy utilization; Learning systems; Wi-Fi; Wireless local area networks (WLAN); Wireless telecommunication systems; Application analysis; Communication interface; Communication tools; Future applications; Hardware components; Machine learning methods; Mobile handheld devices; Running applications; Behavioral research",2-s2.0-84978708913
"Malmir B., Amini M., Chang S.I.","A medical decision support system for disease diagnosis under uncertainty",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021661144&doi=10.1016%2fj.eswa.2017.06.031&partnerID=40&md5=e3e03ad394da8d12005ff599b8c37885","This paper presents a decision support system (DSS) modeled by a fuzzy expert system (FES) for medical diagnosis to help physicians make better decisions. The proposed system collects comprehensive information about a disease from a group of experts. To this aim, a cross-sectional study is conducted by asking physicians’ expertise on all symptoms relevant to a disease. A fuzzy rule based system is then formed based on this information, which contains a set of significant symptoms relevant to the suspected disease. Linguistic fuzzy values are assigned to model each symptom. The input of the system is the severity level of each symptom reported by patients. The proposed FES considers two approaches to account for uncertain inputs from patients. Two case studies on kidney stone and kidney infection were conducted to demonstrate how the proposed method could be used. A group of patients were used to validate the effectiveness of the proposed expert system. The results show that the proposed fuzzy expert system is capable of diagnosing diseases with a high degree of accuracy and precision comparing to a couple of machine learning methods. © 2017","Decision support system; Disease diagnosis; Fuzzy expert system; Fuzzy rule based systems; Medical diagnosis problems","Artificial intelligence; Biological organs; Decision support systems; Expert systems; Fuzzy inference; Fuzzy rules; Knowledge based systems; Learning systems; Medical problems; Comprehensive information; Decision support system (dss); Diagnosis problem; Disease diagnosis; Fuzzy expert systems; High degree of accuracy; Machine learning methods; Medical decision support system; Diagnosis",2-s2.0-85021661144
"Schaefer K.E., Straub E.R., Chen J.Y.C., Putney J., Evans A.W., III","Communicating intent to develop shared situation awareness and engender trust in human-agent teams",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014752866&doi=10.1016%2fj.cogsys.2017.02.002&partnerID=40&md5=e6a67c42277d30e23853c340296e717d","This paper addresses issues related to integrating autonomy-enabled, intelligent agents into collaborative, human-machine teams. Interaction with intelligent machine agents capable of making independent, goal-directed decisions in human-machine teaming operations constitutes a major change from traditional human-machine interaction involving teleoperation. Communicating the machine agent's intent to human counterparts becomes increasingly important as independent machine decisions become subject to human trust and mental models. The authors present findings from their research that suggest existing user display technologies, tailored with context-specific information and the human's knowledge level of the machine agent's decision process, can mitigate misperceptions of the appropriateness of agent behavioral responses. This is important because misperceptions on the part of human team members increases the likelihood of trust degradation and unnecessary interventions, ultimately leading to disuse of the agent. Examples of possible issues associated with communicating agent intent, as well as potential implications for trust calibration are provided. © 2017","Human-agent teaming; Intent; Shared situation awareness; Trust; User displays","Human computer interaction; Intelligent agents; Communicating agents; Display technologies; Human machine interaction; Human-agent teaming; Intent; Situation awareness; Specific information; Trust; Autonomous agents; awareness; behavior; decision making; human; human computer interaction; independent variable; information processing; interpersonal communication; knowledge; machine learning; peer group; personal autonomy; priority journal; Review; teamwork; trust",2-s2.0-85014752866
"Shao X., Li H., Lin H., Kang X., Lu T.","Ship Detection in Optical Satellite Image Based on RX Method and PCAnet",2017,"Sensing and Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020528608&doi=10.1007%2fs11220-017-0167-6&partnerID=40&md5=9959e9901ebe308dce9ed85dfc9a8dd3","In this paper, we present a novel method for ship detection in optical satellite image based on the ReedXiaoli (RX) method and the principal component analysis network (PCAnet). The proposed method consists of the following three steps. First, the spatially adjacent pixels in optical image are arranged into a vector, transforming the optical image into a 3D cube image. By taking this process, the contextual information of the spatially adjacent pixels can be integrated to magnify the discrimination between ship and background. Second, the RX anomaly detection method is adopted to preliminarily extract ship candidates from the produced 3D cube image. Finally, real ships are further confirmed among ship candidates by applying the PCAnet and the support vector machine (SVM). Specifically, the PCAnet is a simple deep learning network which is exploited to perform feature extraction, and the SVM is applied to achieve feature pooling and decision making. Experimental results demonstrate that our approach is effective in discriminating between ships and false alarms, and has a good ship detection performance. © 2017, Springer Science+Business Media New York.","Deep learning network; Optical satellite image; PCAnet; RX method; Ship detection; Support vector machine","Decision making; Deep learning; Feature extraction; Geometrical optics; Learning systems; Pixels; Principal component analysis; Satellites; Ships; Software architecture; Support vector machines; Learning network; Optical satellite images; PCAnet; RX method; Ship detection; Image processing",2-s2.0-85020528608
"Shao W., Pi D., Shao Z.","An extended teaching-learning based optimization algorithm for solving no-wait flow shop scheduling problem",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027559730&doi=10.1016%2fj.asoc.2017.08.020&partnerID=40&md5=d585f2c0d09d200f1f5188baefe83394","The no-wait flow shop scheduling problem (NWFSSP) performs an important function in the manufacturing industry. Inspired by the overall process of teaching-learning, an extended framework of meta-heuristic based on the teaching-learning process is proposed, which consists of four parts, i.e. previewing before class, teaching phase, learning phase, reviewing after class. This paper implements a hybrid meta-heuristic based on probabilistic teaching-learning mechanism (mPTLM) to solve the NWFSSP with the makespan criterion. In previewing before class, an initial method that combines a modified Nawaz-Enscore-Ham (NEH) heuristic and the opposition-based learning (OBL) is introduced. In teaching phase, the Gaussian distribution is employed as the teacher to guide learners to search more promising areas. In learning phase, this paper presents a new means of communication with crossover. In reviewing after class, an improved speed-up random insert local search based on simulated annealing (SA) is developed to enhance the local searching ability. The computational results and comparisons based on Reeves, Taillard and VRF's benchmarks demonstrate the effectiveness of mPTLM for solving the NWFSSP. © 2017 Elsevier B.V.","Learning phase; Local search; Minimizing makespan; No-wait flow shop scheduling; Probabilistic teaching phase","Heuristic algorithms; Heuristic methods; Local search (optimization); Machine shop practice; Optimization; Scheduling; Simulated annealing; Teaching; Learning phase; Local search; Minimizing makespan; No-wait flow-shop scheduling; Probabilistic teaching phase; Education",2-s2.0-85027559730
"Luo Y., Zhao X., Zhou J., Yang J., Zhang Y., Kuang W., Peng J., Chen L., Zeng J.","A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029583555&doi=10.1038%2fs41467-017-00680-8&partnerID=40&md5=0d7a9811a21ad515f7dae1990bc6fffd","The emergence of large-scale genomic, chemical and pharmacological data provides new opportunities for drug discovery and repositioning. In this work, we develop a computational pipeline, called DTINet, to predict novel drug-target interactions from a constructed heterogeneous network, which integrates diverse drug-related information. DTINet focuses on learning a low-dimensional vector representation of features, which accurately explains the topological properties of individual nodes in the heterogeneous network, and then makes prediction based on these representations via a vector space projection scheme. DTINet achieves substantial performance improvement over other state-of-the-art methods for drug-target interaction prediction. Moreover, we experimentally validate the novel interactions between three drugs and the cyclooxygenase proteins predicted by DTINet, and demonstrate the new potential applications of these identified cyclooxygenase inhibitors in preventing inflammatory diseases. These results indicate that DTINet can provide a practically useful tool for integrating heterogeneous information to predict new drug-target interactions and repurpose existing drugs. © 2017 The Author(s).",,"alendronic acid; alfuzosin; bisoprolol; carteolol; chlorpromazine; chlorpropamide; clonidine; darifenacin; doxepin; fluoxetine; imipramine; isoprenaline; labetalol; metoprolol; midodrine; nicardipine; pindolol; prazosin; prostaglandin synthase; prostaglandin synthase inhibitor; reserpine; risperidone; tadalafil; telmisartan; terazosin; tetrabenazine; thioridazine; timolol; tolterodine; unindexed drug; accuracy assessment; computer simulation; drug; experimental study; genomics; heterogeneity; performance assessment; prediction; animal cell; animal tissue; Article; comparative study; drug information; drug protein binding; drug repositioning; drug targeting; heterogeneous network; hydrogen bond; IC50; inflammatory disease; learning algorithm; machine learning; measurement accuracy; measurement precision; molecular docking; mouse; nonhuman; peritoneum macrophage; prediction; Protein Data Bank; receiver operating characteristic; validation process",2-s2.0-85029583555
"Chamorro-Premuzic T., Akhtar R., Winsborough D., Sherman R.A.","The datafication of talent: how technology is advancing the science of human potential at work",2017,"Current Opinion in Behavioral Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020276945&doi=10.1016%2fj.cobeha.2017.04.007&partnerID=40&md5=d3758252bb7f7c9d91b833c88d7dabfd","This article reviews three innovations that not only have the potential to revolutionize the way organizations identify, develop and engage talent, but are also emerging as tools used by practitioners and firms. Specifically, we discuss (a) machine-learning algorithms that can evaluate digital footprints, (b) social sensing technology that can automatically decode verbal and nonverbal behavior to infer personality and emotional states, and (c) gamified assessment tools that focus on enhancing the user-experience in personnel selection. The strengths and limitations of each of these approaches are discussed, and practical and theoretical implications are considered. © 2017 Elsevier Ltd",,"algorithm; human; leadership; machine learning; nonverbal communication; organization; personality; priority journal; Review; science; social media; social network; technology",2-s2.0-85020276945
"Moreira I.S., Koukos P.I., Melo R., Almeida J.G., Preto A.J., Schaarschmidt J., Trellet M., Gümüş Z.H., Costa J., Bonvin A.M.J.J.","SpotOn: High Accuracy Identification of Protein-Protein Interface Hot-Spots",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027463377&doi=10.1038%2fs41598-017-08321-2&partnerID=40&md5=883fbf9216c69bb2a467d7fd7e41be8d","We present SpotOn, a web server to identify and classify interfacial residues as Hot-Spots (HS) and Null-Spots (NS). SpotON implements a robust algorithm with a demonstrated accuracy of 0.95 and sensitivity of 0.98 on an independent test set. The predictor was developed using an ensemble machine learning approach with up-sampling of the minor class. It was trained on 53 complexes using various features, based on both protein 3D structure and sequence. The SpotOn web interface is freely available at: http://milou.science.uu.nl/services/SPOTON/. © 2017 The Author(s).",,"computer interface; DNA structure; machine learning; sampling",2-s2.0-85027463377
"Taloni A., Font-Clos F., Guidetti L., Milan S., Ascagni M., Vasco C., Pasini M.E., Gioria M.R., Ciusani E., Zapperi S., La Porta C.A.M.","Probing spermiogenesis: A digital strategy for mouse acrosome classification",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020849612&doi=10.1038%2fs41598-017-03867-7&partnerID=40&md5=161b61003442807e02db6ecb12c2573c","Classification of morphological features in biological samples is usually performed by a trained eye but the increasing amount of available digital images calls for semi-automatic classification techniques. Here we explore this possibility in the context of acrosome morphological analysis during spermiogenesis. Our method combines feature extraction from three dimensional reconstruction of confocal images with principal component analysis and machine learning. The method could be particularly useful in cases where the amount of data does not allow for a direct inspection by trained eye. © 2017 The Author(s).",,"acrosome; classification; extraction; machine learning; principal component analysis; spermatogenesis; three dimensional imaging",2-s2.0-85020849612
"Zhao H., Zhang X., Li K.","A Sentiment Classification Model Using Group Characteristics of Writing Style Features",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020708058&doi=10.1142%2fS021800141756016X&partnerID=40&md5=927f551ff05469ffa7162da09b52a0f7","Sentiment analysis is becoming increasingly important mainly because of the growth of web comments. Sentiment polarity classification is a popular process in this field. Writing style features, such as lexical and word-based features, are often used in the authorship identification and gender classification of online messages. However, writing style features were only used in feature selection for sentiment classification. This research presents an exploratory study of the group characteristics of writing style features on the Internet Movie Database (IMDb) movie sentiment data set. Furthermore, this study utilizes the specific group characteristics of writing style in improving the performance of sentiment classification. We determine the optimum clustering number of user reviews based on writing style features distribution. According to the classification model trained on a training subset with specific writing style clustering tags, we determine that the model trained on the data set of a specific writing style group has an optimal effect on the classification accuracy, which is better than the model trained on the entire data set in a particular positive or negative polarity. Through the polarity characteristics of specific writing style groups, we propose a general model in improving the performance of the existing classification approach. Results of the experiments on sentiment classification using the IMDb data set demonstrate that the proposed model improves the performance in terms of classification accuracy. © 2017 World Scientific Publishing Company.","IMDb; machine learning; sentiment classification; writing style","Learning systems; Authorship identification; Classification accuracy; Classification approach; IMDb; Internet movie database; Polarity classification; Sentiment classification; Writing style; Classification (of information)",2-s2.0-85020708058
"Sangogboye F.C., Arendt K., Singh A., Veje C.T., Kjærgaard M.B., Jørgensen B.N.","Performance comparison of occupancy count estimation and prediction with common versus dedicated sensors for building model predictive control",2017,"Building Simulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030870394&doi=10.1007%2fs12273-017-0397-5&partnerID=40&md5=f7d49daea419fa09c44f2a7b3bd36928","Model predictive control is a promising approach to optimize the operation of building systems and provide demand-response functionalities without compromising indoor comfort. The performance of model predictive control relies, among other things, on the quality of weather forecasts and building occupancy predictions. The present study compares the accuracy and computational demand of two occupancy estimation and prediction approaches suitable for building model predictive control: (1) count prediction based on indoor climate modeling and parameter estimation “using common sensors”, (2) count prediction based on data from 3D stereovision camera. The performance of the two approaches was tested in two rooms of a case study building. The results show that the method with dedicated sensors outperforms common sensors. However, if a building is not equipped with dedicated sensors, the present study shows that the common sensor method can be a satisfactory alternative to be used in model predictive control. © 2017, Tsinghua University Press and Springer-Verlag GmbH Germany.","machine learning; model predictive control; occupancy estimation; occupancy prediction","Buildings; Climate models; Forecasting; Learning systems; Quality control; Weather forecasting; Building occupancy; Building systems; Computational demands; Count estimation; Estimation and predictions; Occupancy predictions; Performance comparison; Prediction-based; Model predictive control",2-s2.0-85030870394
"Gupta B.B., Tewari A., Jain A.K., Agrawal D.P.","Fighting against phishing attacks: state of the art and future challenges",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961211997&doi=10.1007%2fs00521-016-2275-y&partnerID=40&md5=c9be65bfd15ee4b02b0dce84b3df9616","In the last few years, phishing scams have rapidly grown posing huge threat to global Internet security. Today, phishing attack is one of the most common and serious threats over Internet where cyber attackers try to steal user’s personal or financial credentials by using either malwares or social engineering. Detection of phishing attacks with high accuracy has always been an issue of great interest. Recent developments in phishing detection techniques have led to various new techniques, specially designed for phishing detection where accuracy is extremely important. Phishing problem is widely present as there are several ways to carry out such an attack, which implies that one solution is not adequate to address it. Two main issues are addressed in our paper. First, we discuss in detail phishing attacks, history of phishing attacks and motivation of attacker behind performing this attack. In addition, we also provide taxonomy of various types of phishing attacks. Second, we provide taxonomy of various solutions proposed in the literature to detect and defend from phishing attacks. In addition, we also discuss various issues and challenges faced in dealing with phishing attacks and spear phishing and how phishing is now targeting the emerging domain of IoT. We discuss various tools and datasets that are used by the researchers for the evaluation of their approaches. This provides better understanding of the problem, current solution space and future research scope to efficiently deal with such attacks. © 2016, The Natural Computing Applications Forum.","Bag-of-word; Data mining; Key logger; Machine learning; Malware; Phishing; Social engineering; Soft computing; Spam; Visual similarity","Artificial intelligence; Data mining; Internet; Learning systems; Malware; Soft computing; Taxonomies; Bag of words; Phishing; Social engineering; Spam; Visual similarity; Computer crime",2-s2.0-84961211997
"Ghorpade V.K., Checchin P., Malaterre L., Trassoudaine L.","3D shape representation with spatial probabilistic distribution of intrinsic shape keypoints",2017,"Eurasip Journal on Advances in Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023764428&doi=10.1186%2fs13634-017-0483-y&partnerID=40&md5=ee85bb817af16ef787b8d20e3ef02b7a","The accelerated advancement in modeling, digitizing, and visualizing techniques for 3D shapes has led to an increasing amount of 3D models creation and usage, thanks to the 3D sensors which are readily available and easy to utilize. As a result, determining the similarity between 3D shapes has become consequential and is a fundamental task in shape-based recognition, retrieval, clustering, and classification. Several decades of research in Content-Based Information Retrieval (CBIR) has resulted in diverse techniques for 2D and 3D shape or object classification/retrieval and many benchmark data sets. In this article, a novel technique for 3D shape representation and object classification has been proposed based on analyses of spatial, geometric distributions of 3D keypoints. These distributions capture the intrinsic geometric structure of 3D objects. The result of the approach is a probability distribution function (PDF) produced from spatial disposition of 3D keypoints, keypoints which are stable on object surface and invariant to pose changes. Each class/instance of an object can be uniquely represented by a PDF. This shape representation is robust yet with a simple idea, easy to implement but fast enough to compute. Both Euclidean and topological space on object’s surface are considered to build the PDFs. Topology-based geodesic distances between keypoints exploit the non-planar surface properties of the object. The performance of the novel shape signature is tested with object classification accuracy. The classification efficacy of the new shape analysis method is evaluated on a new dataset acquired with a Time-of-Flight camera, and also, a comparative evaluation on a standard benchmark dataset with state-of-the-art methods is performed. Experimental results demonstrate superior classification performance of the new approach on RGB-D dataset and depth data. © 2017, The Author(s).","3D descriptors; Classification; Geodesics; Machine learning; Neural networks; Object recognition; Shape signature; Weighted graphs","Content based retrieval; Distribution functions; Learning systems; Neural networks; Object recognition; Probability density function; Probability distributions; Spatial distribution; Three dimensional computer graphics; Topology; Classification performance; Content-based information retrieval; Descriptors; Geodesics; Probabilistic distribution; Shape signatures; State-of-the-art methods; Weighted graph; Classification (of information)",2-s2.0-85023764428
"Mustaqeem A., Anwar S.M., Khan A.R., Majid M.","A statistical analysis based recommender model for heart disease patients",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031723741&doi=10.1016%2fj.ijmedinf.2017.10.008&partnerID=40&md5=ca0e748031291deb9d60ae155383a2b8","Objectives An intelligent information technology based system could have a positive impact on the life-style of patients suffering from chronic diseases by providing useful health recommendations. In this paper, we have proposed a hybrid model that provides disease prediction and medical recommendations to cardiac patients. The first part aims at implementing a prediction model, that can identify the disease of a patient and classify it into one of the four output classes i.e., non-cardiac chest pain, silent ischemia, angina, and myocardial infarction. Following the disease prediction, the second part of the model provides general medical recommendations to patients. Methods The recommendations are generated by assessing the severity of clinical features of patients, estimating the risk associated with clinical features and disease, and calculating the probability of occurrence of disease. The purpose of this model is to build an intelligent and adaptive recommender system for heart disease patients. The experiments for the proposed recommender system are conducted on a clinical data set collected and labelled in consultation with medical experts from a known hospital. Results The performance of the proposed prediction model is evaluated using accuracy and kappa statistics as evaluation measures. The medical recommendations are generated based on information collected from a knowledge base created with the help of physicians. The results of the recommendation model are evaluated using confusion matrix and gives an accuracy of 97.8%. Conclusion The proposed system exhibits good prediction and recommendation accuracies and promises to be a useful contribution in the field of e-health and medical informatics. © 2017 Elsevier B.V.","E-health; Heart disease; Machine learning; Medical recommendations; Risk analysis","Cardiology; Diseases; Forecasting; Health; Health risks; Heart; Knowledge based systems; Learning systems; Risk analysis; Risk assessment; Risk perception; E health; Evaluation measures; Heart disease; Intelligent information; Medical recommendations; Myocardial Infarction; Probability of occurrence; Recommendation accuracy; Recommender systems",2-s2.0-85031723741
"Filho J.B.O.S., Diniz P.S.R.","A Fixed-Point Online Kernel Principal Component Extraction Algorithm",2017,"IEEE Transactions on Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029173322&doi=10.1109%2fTSP.2017.2750119&partnerID=40&md5=9f37d3467dab61fd36dc94eb46da4275","Kernel principal component analysis (KPCA) is a powerful and widely applied nonlinear feature extraction technique. However, as originally proposed, KPCA may be cumbersome or infeasible in large-scale datasets, which motivated the development of low-complexity iterative extraction algorithms, mainly aiming image processing applications. Recently, some online KPCA extraction algorithms were proposed, but most of them suffer from low-convergence speed. This paper proposes a new algorithm based on fixed-point iterative equations for KPCA extraction, expanding kernel components using a compact dictionary, dynamically built from data, according to a user-defined accuracy parameter. The algorithm relies on simple equations, can track nonstationary environments, and requires reduced storage, enabling its use in real-time applications operating in low-cost embedded hardware. Results involving open-access image datasets show improved accuracy and convergence speed, as well as permitted effective improvements in practical image applications, as compared to state-of-art online KPCA techniques. © 2017 IEEE.","generalized Hebbian algorithm; kernel methods; machine learning; online kernel algorithms; Principal component analysis","Computational complexity; Data reduction; Digital storage; Extraction; Image processing; Iterative methods; Learning systems; Generalized Hebbian algorithm; Image processing applications; Kernel algorithms; Kernel methods; Kernel principal component; Kernel principal component analyses (KPCA); Non-stationary environment; Nonlinear feature extraction; Principal component analysis",2-s2.0-85029173322
"Pampouchidou A., Pediaditis M., Maridaki A., Awais M., Vazakopoulou C.-M., Sfakianakis S., Tsiknakis M., Simos P., Marias K., Yang F., Meriaudeau F.","Quantitative comparison of motion history image variants for video-based depression assessment",2017,"Eurasip Journal on Image and Video Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029085354&doi=10.1186%2fs13640-017-0212-3&partnerID=40&md5=dcb9ebb1cff424e3d8e6d1668ae60349","Depression is the most prevalent mood disorder and a leading cause of disability worldwide. Automated video-based analyses may afford objective measures to support clinical judgments. In the present paper, categorical depression assessment is addressed by proposing a novel variant of the Motion History Image (MHI) which considers Gabor-inhibited filtered data instead of the original image. Classification results obtained with this method on the AVEC’14 dataset are compared to those derived using (a) an earlier MHI variant, the Landmark Motion History Image (LMHI), and (b) the original MHI. The different motion representations were tested in several combinations of appearance-based descriptors, as well as with the use of convolutional neural networks. The F1 score of 87.4% achieved in the proposed work outperformed previously reported approaches. © 2017, The Author(s).","Affective computing; Depression assessment; Facial image analysis; Facial landmarks; Gabor inhibition; Image processing; Machine learning; Motion history image","Classification (of information); Image processing; Learning systems; Neural networks; Affective Computing; Depression assessment; Facial images; Facial landmark; Motion history images; Video streaming",2-s2.0-85029085354
"D'Amato D., Droste N., Allen B., Kettunen M., Lähtinen K., Korhonen J., Leskinen P., Matthies B.D., Toppinen A.","Green, circular, bio economy: A comparative analysis of sustainability avenues",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030721949&doi=10.1016%2fj.jclepro.2017.09.053&partnerID=40&md5=2676156cc2ea7d27af16c2ab86ab61d5","Despite their evidently different assumptions and operationalization strategies, the concepts of Circular Economy, Green Economy and Bioeconomy are joined by the common ideal to reconcile economic, environmental and social goals. The three concepts are currently mainstreamed in academia and policy making as key sustainability avenues, but a comparative analysis of such concepts is missing. The aim of this article is thus to comprehensively analyse the diversity within and between such concepts. The results are drawn from a bibliometric review of almost two thousand scientific articles published within the last three decades, coupled with a conceptual analysis. We find that, for what concerns environmental sustainability, Green Economy acts as an ‘umbrella' concept, including elements from Circular Economy and Bioeconomy concepts (e.g. eco-efficiency; renewables), as well as additional ideas, e.g. nature-based solutions. In particular, Circular Economy and Bioeconomy are resource-focused, whereas in principle Green Economy acknowledges the underpinning role of all ecological processes. Regarding the social dimension, Green Economy is more inclusive of some aspects at local level (e.g. eco-tourism, education), while there is an emerging discussion in Bioeconomy literature around local processes in terms of biosecurity and rural policies. When considering weak/strong sustainability visions, all concepts remain limited in questioning economic growth. By comparing the different sustainability strategies promoted by these concepts we do not advocate for their substitutability, but for their clarification and reciprocal integration. The findings are discussed in light of the concepts' synergies and limits, with the purpose to inform research and policy implementation. © 2017 The Authors","Bioeconomy; Circular economy; Green economy; Latent dirichlet allocation; Machine learning; Sustainability","Economics; Education; Learning systems; Statistics; Bioeconomy; Circular economy; Comparative analysis; Environmental sustainability; Green economies; Latent Dirichlet allocation; Policy implementations; Sustainability strategies; Sustainable development",2-s2.0-85030721949
"Uddin M.F., Lee J.","Proposing stochastic probability-based math model and algorithms utilizing social networking and academic data for good fit students prediction",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023753518&doi=10.1007%2fs13278-017-0448-z&partnerID=40&md5=429df37de405be6db236962cf42506ec","The research progress presented in this paper comes under the areas of data science. The authors propose enhanced machine learning (supervised learning) framework for the prediction of the students through stochastic probability-based math constructs/model and an algorithm [Good Fit Student (GFS)], along with the enhanced quantification of target variables and algorithmic metrics. Academia in today’s modern world sees the problem of dropouts, low retention, poor student performances, lack of motivation, and unnecessary change of study majors and re-admissions. The authors consider this challenge as a research problem and attempt to solve it by utilizing social networking-based personality traits, relevant data and features to improve the predictive modeling approach. The authors recognize that admission choices are often governed by family trends, affordability, basic motivation, market trends, and natural instincts. However, natural gifts and talents are minimally used to select such directions in the academics. The authors based on literature review identify this a research gap and improves with a unique blend of algorithms/methods, an improved modeling of performance metrics, built upon cross-validation to improve fitness, and enhance the process of feature engineering and tuning for reduced errors and optimum fitness, at the end. The authors present the latest progress of their research in this paper. The included results show the progress of the work and ongoing improvements. The authors use machine learning techniques, Microsoft SQL Server, Excel data mining, R and Python to develop and test their model. The authors provide related work and conclude with final remarks and future work. © 2017, Springer-Verlag GmbH Austria.","Algorithm tuning and blending; Data analytics and mining; Feature engineering; Good Fit Student Algorithm; Machine learning; Personality features correlation with academic data; Social networking features correlation; Stochastic probability distribution-based modeling",,2-s2.0-85023753518
"Hejna M., Jorapur A., Song J.S., Judson R.L.","High accuracy label-free classification of single-cell kinetic states from holographic cytometry of human melanoma cells",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029623097&doi=10.1038%2fs41598-017-12165-1&partnerID=40&md5=16f60c76d474497ca560c7788a2f134c","Digital holographic cytometry (DHC) permits label-free visualization of adherent cells. Dozens of cellular features can be derived from segmentation of hologram-derived images. However, the accuracy of single cell classification by these features remains limited for most applications, and lack of standardization metrics has hindered independent experimental comparison and validation. Here we identify twenty-six DHC-derived features that provide biologically independent information across a variety of mammalian cell state transitions. When trained on these features, machine-learning algorithms achieve blind single cell classification with up to 95% accuracy. Using classification accuracy to guide platform optimization, we develop methods to standardize holograms for the purpose of kinetic single cell cytometry. Applying our approach to human melanoma cells treated with a panel of cancer therapeutics, we track dynamic changes in cellular behavior and cell state over time. We provide the methods and computational tools for optimizing DHC for kinetic single adherent cell classification. © 2017 The Author(s).",,"adherent cell; cancer therapy; cell function; classification; conformational transition; cytometry; human; human cell; machine learning; mammal cell; melanoma cell",2-s2.0-85029623097
"Gillan C.M., Whelan R.","What big data can do for treatment in psychiatry",2017,"Current Opinion in Behavioral Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025595859&doi=10.1016%2fj.cobeha.2017.07.003&partnerID=40&md5=504e8dc4fbd33ea5f804bcfde74ce505","Treatments for psychiatric disorders are only as effective as the precision with which we administer them. We have treatments that work; we just cannot always accurately predict who they are going to work for and why. In this article, we discuss how big data can help identify robust, reproducible and generalizable predictors of treatment response in psychiatry. Specifically, we focus on how machine-learning approaches can facilitate a move beyond discovery studies and toward model validation. We will highlight some recent exemplary studies in this area, describe how one can assess the merits of studies reporting treatment biomarkers, and discuss what we consider to be best practice for prediction research in psychiatry. © 2017 Elsevier Ltd",,"electroconvulsive therapy; electroencephalogram; human; machine learning; medical research; mental disease; methodology; neuroimaging; nuclear magnetic resonance imaging; predictive value; priority journal; psychotherapy; Review; treatment response",2-s2.0-85025595859
"Parast L., McCaffrey D.F., Burgette L.F., de la Guardia F.H., Golinelli D., Miles J.N.V., Griffin B.A.","Optimizing variance-bias trade-off in the TWANG package for estimation of propensity scores",2017,"Health Services and Outcomes Research Methodology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007162535&doi=10.1007%2fs10742-016-0168-2&partnerID=40&md5=d72ab4bb6cbe3a3731885ae7c27b9fd4","While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique [generalized boosted models (GBM)] to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner’s general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth. © 2016, Springer Science+Business Media New York.","Causal inference; Machine learning; Propensity score","Article; controlled study; correlation coefficient; covariance; generalized boosted model; human; logistic regression analysis; machine learning; priority journal; probability; propensity score; sample size; sampling error; selection bias; simulation; variance",2-s2.0-85007162535
"Wu C., Chen T., Jiang R., Ning L., Jiang Z.","A novel approach to wavelet selection and tree kernel construction for diagnosis of rolling element bearing fault",2017,"Journal of Intelligent Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925617962&doi=10.1007%2fs10845-015-1070-4&partnerID=40&md5=ae3308385635c05bfc42a2ce237592e0","A novel methodology for early diagnosis of rolling element bearing fault is employed based on continuous wavelet transform (CWT) and support vector machine (SVM). CWT is especially suited for analyzing non-stationary signals in time–frequency domain where time information is retained as well as frequency content. To better approximate non-stationary vibration signals from rolling element bearing, a wavelet choice criterion is established to select an appropriate mother wavelet for feature extraction. The Shannon wavelet is picked out of several considered wavelets. The classification tree kernels (CTK) are constructed to address nonlinear classification of the characteristic samples derived from the wavelet coefficients. By using Fuzzy pruning strategy, a large variety of classification trees are generated. The trees with diverse structures can effectively explore intrinsic information among samples. Then, the tree kernel matrices can be acquired through ensemble statistical learning, which eventually reveal the similarity of samples objectively and stably. Under such architecture of kernel methods, a classification tree kernel based support vector machine (CTKSVM) is proposed to identify bearing fault. The performance of the methodology involving CWT and CTKSVM (CWT–CTKSVM) is evaluated by cross validation and independent test. The results show that the CWT–CTKSVM totally is superior to other SVM methods with common kernels. Therefore, it is a prospective technique for detection and identification of rolling element bearing fault. © 2015, Springer Science+Business Media New York.","Classification tree kernel; Continuous wavelet transform; Fuzzy pruning strategy; Support vector machine; Tree ensemble statistical learning","Bearings (machine parts); Diagnosis; Fault detection; Feature extraction; Frequency domain analysis; Roller bearings; Support vector machines; Classification trees; Continuous Wavelet Transform; Continuous wavelet transforms; Detection and identifications; Non-stationary vibration signals; Nonlinear classification; Pruning strategy; Statistical learning; Wavelet transforms",2-s2.0-84925617962
"Magana-Mora A., Bajic V.B.","OmniGA: Optimized Omnivariate Decision Trees for Generalizable Classification Models",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021120213&doi=10.1038%2fs41598-017-04281-9&partnerID=40&md5=147ce70393711d3e77d6b416d0be117c","Classification problems from different domains vary in complexity, size, and imbalance of the number of samples from different classes. Although several classification models have been proposed, selecting the right model and parameters for a given classification task to achieve good performance is not trivial. Therefore, there is a constant interest in developing novel robust and efficient models suitable for a great variety of data. Here, we propose OmniGA, a framework for the optimization of omnivariate decision trees based on a parallel genetic algorithm, coupled with deep learning structure and ensemble learning methods. The performance of the OmniGA framework is evaluated on 12 different datasets taken mainly from biomedical problems and compared with the results obtained by several robust and commonly used machine-learning models with optimized parameters. The results show that OmniGA systematically outperformed these models for all the considered datasets, reducing the F1 score error in the range from 100% to 2.25%, compared to the best performing model. This demonstrates that OmniGA produces robust models with improved performance. OmniGA code and datasets are available at www.cbrc.kaust.edu.sa/omniga/. © The Author(s) 2017.",,"classification; decision tree; genetic algorithm; machine learning; model",2-s2.0-85021120213
"Tong H., Wu Q.","Learning performance of regularized moving least square regression",2017,"Journal of Computational and Applied Mathematics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019227981&doi=10.1016%2fj.cam.2017.04.046&partnerID=40&md5=dc83d906804d81a4771a3152e281b3d2","Moving least square regression is an important local learning algorithm. In this paper, we consider a regularized moving least square regression algorithm in reproducing kernel Hilbert space. The localized representer theorem is different from the classical representer theorems for regularized kernel machines. It shows that, regularization not only ensures the computational stability, it is also necessary for the algorithm to preserve localization property. We also studied the learning performance of the regularized moving least square algorithm and conducted a rigorous error analysis. Compared with the unregularized method, convergence analysis of regularized moving least square regression requires more natural and much simpler conditions and achieves fast rates. © 2017 Elsevier B.V.","Error bounds; Learning rate; Moving least square regression; Regularization; Reproducing kernel Hilbert space","Computation theory; Error analysis; Hilbert spaces; Least squares approximations; Regression analysis; Vector spaces; Error bound; Learning rates; Moving least squares; Regularization; Reproducing Kernel Hilbert spaces; Learning algorithms",2-s2.0-85019227981
"Tian J., Yan Y., Yue Q., Liu X., Chu X., Wu N., Fan Y.","Predicting synonymous codon usage and optimizing the heterologous gene for expression in E. coli",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028597790&doi=10.1038%2fs41598-017-10546-0&partnerID=40&md5=e17f6f4d0b514882f774960d346b0cb6","Of the 20 common amino acids, 18 are encoded by multiple synonymous codons. These synonymous codons are not redundant; in fact, all of codons contribute substantially to protein expression, structure and function. In this study, the codon usage pattern of genes in the E. coli was learned from the sequenced genomes of E. coli. A machine learning based method, Presyncodon was proposed to predict synonymous codon selection in E. coli based on the learned codon usage patterns of the residue in the context of the specific fragment. The predicting results indicate that Presycoden could be used to predict synonymous codon selection of the gene in the E. coli with the high accuracy. Two reporter genes (egfp and mApple) were designed with a combination of low- and high-frequency-usage codons by the method. The fluorescence intensity of eGFP and mApple expressed by the (egfp and mApple) designed by this method was about 2.3- or 1.7-folds greater than that from the genes with only high-frequency-usage codons in E. coli. Therefore, both low- and high-frequency-usage codons make positive contributions to the functional expression of the heterologous proteins. This method could be used to design synthetic genes for heterologous gene expression in biotechnology. © 2017 The Author(s).",,"biotechnology; codon usage; fluorescence; gene expression; gene frequency; heterologous expression; machine learning; reporter gene",2-s2.0-85028597790
"Hossu C.A., Ioja I.C., Nita M.R., Hartel T., Badiu D.L., Hersperger A.M.","Need for a cross-sector approach in protected area management",2017,"Land Use Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031009765&doi=10.1016%2fj.landusepol.2017.10.012&partnerID=40&md5=2590fa5f79ce07379b398a05810a52ce","The need for various stakeholders to harmonize their policies and practices has emerged as a dominant paradigm for 21st century natural resource management. Cross-sector coordination is promising because it can enhance policy consistency, enable the realization of synergies and resolve conflicts among sectors regarding resource management. The extent to which ministries and their main stakeholders make efforts to achieve integrated policies for nature conservation requires further research. Therefore, the aim of this study was to explore the consultation reports of ministries from relevant fields (i.e., environmental protection, agriculture, spatial planning, and security) regarding the management plans for Romania's protected areas. We analysed and visualized 152 consultation reports (2013–2016) covering 15% of Romania's protected areas using self-organizing maps (SOMs), an unsupervised machine-learning method. Our results showed that considerable attention was paid to formal issues in these reports. The cross-sector issues that emerged as the most important were those related to forest landowner consultation, and the harmonization of agricultural and forestry practices, as well as spatial plans for conservation. The resulting SOMs could be used as a tool to strengthen protected area management in the future because they can (i) guide managers of protected areas to develop plans that ensure that resources will be used in the best way according to the visions of multiple sectors and (ii) help the relevant ministries to improve future consultation reports. © 2017 Elsevier Ltd","Consultation; Nature conservation; Protected area; Romania; Self-organizing map","environmental management; machine learning; nature conservation; protected area; spatial planning; stakeholder; Romania",2-s2.0-85031009765
"Tsakanikas P., Sigalas C., Rigas P., Skaliora I.","High-Throughput Analysis of in-vitro LFP Electrophysiological Signals: A validated workflow/software package",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020400113&doi=10.1038%2fs41598-017-03269-9&partnerID=40&md5=0156fb2be6266dba0be3510dec0451a8","Synchronized brain activity in the form of alternating epochs of massive persistent network activity and periods of generalized neural silence, has been extensively studied as a fundamental form of circuit dynamics, important for many cognitive functions including short-term memory, memory consolidation, or attentional modulation. A key element in such studies is the accurate determination of the timing and duration of those network events. The local field potential (LFP) is a particularly attractive method for recording network activity, because it allows for long and stable recordings from multiple sites, allowing researchers to estimate the functional connectivity of local networks. Here, we present a computational method for the automatic detection and quantification of in-vitro LFP events, aiming to overcome the limitations of current approaches (e.g. slow analysis speed, arbitrary threshold-based detection and lack of reproducibility across and within experiments). The developed method is based on the implementation of established signal processing and machine learning approaches, is fully automated and depends solely on the data. In addition, it is fast, highly efficient and reproducible. The performance of the software is compared against semi-manual analysis and validated by verification of prior biological knowledge. © The Author(s) 2017.",,"electric potential; human; in vitro study; machine learning; quantitative study; reproducibility; signal processing; software; velocity; workflow",2-s2.0-85020400113
"Karim M.N., Reid C.M., Cochrane A., Tran L., Alramadan M., Hossain M.N., Billah B.","Mortality risk prediction models for coronary artery bypass graft surgery: Current scenario and future direction",2017,"Journal of Cardiovascular Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029694114&doi=10.23736%2fS0021-9509.17.09965-7&partnerID=40&md5=3694f961e5d928d70c43265660d76cc7","INTRODUCTION? Many risk prediction models are currently in use for predicting short-Term mortality following coronary artery bypass graft (CABG) surgery. This review critically appraised the methods that were used for developing these models to assess their applicability in current practice setting as well as for the necessity of up-gradation. EVIDENCE ACQUISITION? Medline via Ovid was searched for articles published between 1946 and 2016 and EMBASE via Ovid between 1974 and 2016 to identify risk prediction models for CABG. Article selection and data extraction was conducted using the CHARMS checklist for review of prediction model studies. Association between model development methods and model's discrimination was assessed using Kruskal-Wallis one-way analysis of variance and Mann-Whitney U-Test. EVIDENCE SYNTHESIS? A total of 53 risk prediction models for short-Term mortality following CABG were identified. The review found a wide variation in development methodology of risk prediction models in the field. Ambiguous predictor and outcome definition, sub-optimum sample size, inappropriate handling of missing data and inefficient predictor selection technique are major issues identified in the review. Quantitative synthesis in the review showed ""missing value imputation"" and ""adopting machine learning algorithms"" may result in better discrimination power of the models. CONCLUSIONS? There are aspects in current risk modeling, where there is room for improvement to reflect current clinical practice. Future risk modelling needs to adopt a standardized approach to defining both outcome and predictor variables, rational treatment of missing data and robust statistical techniques to enhance performance of the mortality risk prediction. © 2016 EDIZIONI MINERVA MEDICA.","Cardiac surgical procedures; Coronary artery bypass; Mortality; Myocardial revascularization; Risk","analysis of variance; checklist; clinical practice; coronary artery bypass graft; data extraction; Embase; heart muscle revascularization; human; intermethod comparison; machine learning; Medline; mortality risk; prediction; predictor variable; rank sum test; sample size; synthesis; systematic review; adverse effects; clinical decision making; coronary artery bypass graft; coronary artery disease; decision support system; diagnostic imaging; mortality; patient selection; predictive value; risk assessment; risk factor; time factor; treatment outcome; Analysis of Variance; Clinical Decision-Making; Coronary Artery Bypass; Coronary Artery Disease; Decision Support Techniques; Humans; Patient Selection; Predictive Value of Tests; Risk Assessment; Risk Factors; Time Factors; Treatment Outcome",2-s2.0-85029694114
"Mi C.R., Zu Q., He L., Huettmann F., Jin N., Li J.","Climate change would enlarge suitable planting areas of sugarcanes in China",2017,"International Journal of Plant Production",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010038972&partnerID=40&md5=49b329f5e9082f3a200cff7104a8f751","China’s sugar production and consumption continues to increase. This process is already ongoing for over 15 years and over 90% of the sugar production comes from sugarcane (Saccharum officinarum). Most of the sugarcane is planted in the south (e.g. the Chinese provinces of Yunnan, Guangxi, Guangdong and Hainan) and it represents there a major economic crop in these landscapes. As found virtually worldwide, climate change is generally expected to influence such suitable planting areas. Here we started a first empirical assessment how climate change would influence the spatial distribution of those current and future suitable planting areas of this strategic crop in China. We employed an ensemble machine learning algorithm (Random Forest; bagging) and increasingly used and robust species distribution models (SDMs). These are based on our compiled and best publicly available crop data sampled from the Chinese sugarcane industry map. They were linked with bioclimate variables from the Worldclim database. This powerful concept allowed us to project sugarcane’s current and future (2070) suitable distributions based on the climate niche. Our results were extrapolated to three Global Circulation Models (GCMs; BCC-CSM1-1, CNRM-CM5 and MIROC-ESM) under three representative concentration pathways (RCPs of 2.6, 4.5 and 8.5). The evaluations of these models indicated that our results had a powerful performance (AUC=0.97, TSS=0.96) for robust inference. Bioclimatic variables related to temperature were the most important predictors for sugarcane planting. All models showed similar increasing spatial trends in suitable distribution area and just a few original suitable areas would be lost. Our finding puts emphasize on new growing areas, their soil and management. It is the first to provide the necessary background in the future to safely cultivate sugarcane in climate-suitable areas and to obtain more sugar production for farmers and the industry; it is of large and strategic importance for food security and national autonomy of this central commodity. © 2017, Gorgan Univ Agricultural Sciences and Natural Resources. All rights reserved.","China; Climate change; Food security; Random forest (bagging) and machine learning; Species distribution model (SDMs); Sugarcane",,2-s2.0-85010038972
"Dewan P., Kumaraguru P.","Facebook Inspector (FbI): Towards automatic real-time detection of malicious content on Facebook",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018571396&doi=10.1007%2fs13278-017-0434-5&partnerID=40&md5=6059cc233ba8503039b984cdc561ae38","Online Social Networks witness a rise in user activity whenever a major event makes news. Cyber criminals exploit this spur in user engagement levels to spread malicious content that compromises system reputation, causes financial losses and degrades user experience. In this paper, we collect and characterize a dataset of 4.4 million public posts generated on Facebook during 17 news-making events (natural calamities, sports, terror attacks, etc.) over a 16-month time period. From this dataset, we filter out two sets of malicious posts, one using URL blacklists and another using human annotations. Our observations reveal some characteristic differences between malicious posts obtained from the two methodologies, thus demanding a twofold filtering process for a more complete and robust filtering system. We empirically confirm the need for this twofold filtering approach by cross-validating supervised learning models obtained from the two sets of malicious posts. These supervised learning models include Naive Bayesian, Decision Trees, Random Forest, and Support Vector Machine-based models. Based on this learning, we implement Facebook Inspector, a REST API-based browser plug-in for identifying malicious Facebook posts in real time. Facebook Inspector uses class probabilities obtained from two independent supervised learning models based on a Random Forest classifier to identify malicious posts in real time. These supervised learning models are based on a feature set comprising of 44 features and achieve an accuracy of over 80% each, using only publicly available features. During the first 9 months of its public deployment (August 2015–May 2016), Facebook Inspector processed 0.97 million posts at an average response time of 2.6 s per post and was downloaded over 2500 times. We also evaluate Facebook Inspector in terms of performance and usability to identify further scope for improvement. © 2017, Springer-Verlag Wien.","Facebook; Machine learning; Malicious content; Real-time system",,2-s2.0-85018571396
"Ali A., Yangyu F., Liu S.","Automatic modulation classification of digital modulation signals with stacked autoencoders",2017,"Digital Signal Processing: A Review Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029713852&doi=10.1016%2fj.dsp.2017.09.005&partnerID=40&md5=db950fdceccac281d096318139ac0c67","Modulation identification of the transmitted signals remain a challenging area in modern intelligent communication systems like cognitive radios. The computation of the distinct features from input data set and applying machine learning algorithms has been a well-known method in the classification of such signals. However, recently, deep neural networks, a branch of machine learning, have gained significant attention in the pattern recognition of complex data due to its superior performance. Here, we test the application of deep neural networks to the automatic modulation classification in AWGN and flat-fading channel. Three training inputs were used; mainly 1) In-phase and quadrature (I-Q) constellation points, 2) the centroids of constellation points employing the fuzzy C-means algorithm to I-Q diagrams, and 3) the high-order cumulants of received samples. The unsupervised learning from these data sets was done using the sparse autoencoders and a supervised softmax classifier was employed for the classification. The designing parameters for training single and 2-layer sparse autoencoders are proposed and their performance compared with each other. The results show that a very good classification rate is achieved at a low SNR of 0 dB. This shows the potential of the deep learning model for the application of modulation classification. © 2017 Elsevier Inc.","Automatic modulation classification; Deep neural networks; Digital modulation signals; I-Q diagrams; Pattern recognition","Artificial intelligence; Classification (of information); Clustering algorithms; Cognitive radio; Cognitive systems; Copying; Deep learning; Deep neural networks; Fading (radio); Fading channels; Fuzzy clustering; Learning algorithms; Learning systems; Pattern recognition; Signal to noise ratio; Support vector machines; Automatic modulation classification; Classification rates; Digital modulation signals; Fuzzy C-means algorithms; High order cumulants; Intelligent communication; Modulation classification; Modulation identification; Modulation",2-s2.0-85029713852
"Cao P., Liu X., Yang J., Zhao D., Huang M., Zhang J., Zaiane O.","Nonlinearity-aware based dimensionality reduction and over-sampling for AD/MCI classification from MRI measures",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030978756&doi=10.1016%2fj.compbiomed.2017.10.002&partnerID=40&md5=12c822788b9b62fdd2c7d076b47c75a9","Alzheimer's disease (AD) has been not only a substantial financial burden to the health care system but also an emotional burden to patients and their families. Making accurate diagnosis of AD based on brain magnetic resonance imaging (MRI) is becoming more and more critical and emphasized at the earliest stages. However, the high dimensionality and imbalanced data issues are two major challenges in the study of computer aided AD diagnosis. The greatest limitations of existing dimensionality reduction and over-sampling methods are that they assume a linear relationship between the MRI features (predictor) and the disease status (response). To better capture the complicated but more flexible relationship, we propose a multi-kernel based dimensionality reduction and over-sampling approaches. We combined Marginal Fisher Analysis with ℓ2,1-norm based multi-kernel learning (MKMFA) to achieve the sparsity of region-of-interest (ROI), which leads to simultaneously selecting a subset of the relevant brain regions and learning a dimensionality transformation. Meanwhile, a multi-kernel over-sampling (MKOS) was developed to generate synthetic instances in the optimal kernel space induced by MKMFA, so as to compensate for the class imbalanced distribution. We comprehensively evaluate the proposed models for the diagnostic classification (binary class and multi-class classification) including all subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The experimental results not only demonstrate the proposed method has superior performance over multiple comparable methods, but also identifies relevant imaging biomarkers that are consistent with prior medical knowledge. © 2017 Elsevier Ltd","Alzheimer's disease; Feature selection; Manifold learning; Multi-kernel learning; Over-sampling","Brain; Computer aided diagnosis; Diagnosis; Feature extraction; Image segmentation; Magnetic resonance imaging; Medical imaging; Neurodegenerative diseases; Neuroimaging; Alzheimer's disease; Dimensionality reduction; Linear relationships; Manifold learning; Marginal fisher analysis; Multi-class classification; Multi-kernel learning; Over sampling; Classification (of information); Alzheimer disease; Article; brain region; disease classification; human; k nearest neighbor; kernel method; learning; learning algorithm; mild cognitive impairment; nonlinear system; nuclear magnetic resonance imaging; prediction; priority journal; support vector machine",2-s2.0-85030978756
"Martin R., Boisvert J.B.","Iterative refinement of implicit boundary models for improved geological feature reproduction",2017,"Computers and Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026832155&doi=10.1016%2fj.cageo.2017.07.003&partnerID=40&md5=5de67e92bb251eeb8fdbb14cbce3be49","Geological domains contain non-stationary features that cannot be described by a single direction of continuity. Non-stationary estimation frameworks generate more realistic curvilinear interpretations of subsurface geometries. A radial basis function (RBF) based implicit modeling framework using domain decomposition is developed that permits introduction of locally varying orientations and magnitudes of anisotropy for boundary models to better account for the local variability of complex geological deposits. The interpolation framework is paired with a method to automatically infer the locally predominant orientations, which results in a rapid and robust iterative non-stationary boundary modeling technique that can refine locally anisotropic geological shapes automatically from the sample data. The method also permits quantification of the volumetric uncertainty associated with the boundary modeling. The methodology is demonstrated on a porphyry dataset and shows improved local geological features. © 2017 Elsevier Ltd","Geological modeling; Geostatistics; Non-stationarity; Partition of unity; Radial basis functions","Anisotropy; Domain decomposition methods; Functions; Geology; Heat conduction; Image segmentation; Radial basis function networks; Uncertainty analysis; Geo-statistics; Geological modeling; Non-stationarities; Partition of unity; Radial basis functions; Iterative methods; geostatistics; machine learning; modeling; porphyry",2-s2.0-85026832155
"Aswani R., Ghrera S.P., Kar A.K., Chandra S.","Identifying buzz in social media: a hybrid approach using artificial bee colony and k-nearest neighbors for outlier detection",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027856858&doi=10.1007%2fs13278-017-0461-2&partnerID=40&md5=b288aaa60151ba17c05bc0469e51db35","The exponential growth in the use of social media has not only impacted the way individuals communicate and interact but has also opened new avenues for various domains including health care, marketing, e-commerce, e-governance and politics to name a few. It has been further seen that such engagements result in huge amount of user-generated content (UGC) from both individuals and organizations combined. This UGC can be analyzed in multiple ways to mine useful information. One such popular domain that uses this information is content buzz/popularity. The content shared on social media platforms becomes popular and subsequently viral when shared and propagated by a larger audience at a faster pace. Organizations are leveraging this power of social media in the domain of content buzz and virality by employing various buzz monitoring techniques to boost the reach of their content. This study thus proposes a hybrid artificial bee colony approach integrated with k-nearest neighbors to identify and segregate buzz in Twitter. A set of metrics comprising of created discussions, increase in authors, attention level, burstiness level, contribution sparseness, author interaction, author count and average length of discussions are used to model the buzz. The proposed approach considers the buzz discussions as outliers deviating from the normal discussions and identifies the same using the proposed hybrid bio-inspired approach. Findings may be useful in domains like e-commerce, digital and influencer marketing to explore the factors that might create buzz along with the difference between the impact of buzz and normal discussions on the consumers. © 2017, Springer-Verlag GmbH Austria.","Artificial bee colony; Bio-inspired computing; Machine learning; Outlier detection; Social media analytics; Twitter analytics",,2-s2.0-85027856858
"Jayanthi N., Babu B.V., Rao N.S.","Survey on clinical prediction models for diabetes prediction",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028314925&doi=10.1186%2fs40537-017-0082-7&partnerID=40&md5=4ef312725e979355da1dc2a3137ffbb4","Predictive analytics has gained a lot of reputation in the emerging technology Big data. Predictive analytics is an advanced form of analytics. Predictive analytics goes beyond data mining. A huge amount of medical data is available today regarding the disease, their symptoms, reasons for illness, and their effects on health. But this data is not analysed properly to predict or to study a disease. The aim of this paper is to give a detailed version of predictive models from base to state-of-art, describing various types of predictive models, steps to develop a predictive model, their applications in health care in a broader way and particularly in diabetes. © 2017, The Author(s).","Clinical prediction models; Diabetes; Hybrid model; Machine learning; Predictive analytics; Traditional model",,2-s2.0-85028314925
"Schillaci C., Acutis M., Lombardo L., Lipani A., Fantappiè M., Märker M., Saia S.","Spatio-temporal topsoil organic carbon mapping of a semi-arid Mediterranean region: The role of land use, soil texture, topographic indices and the influence of remote sensing data to modelling",2017,"Science of the Total Environment",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019998646&doi=10.1016%2fj.scitotenv.2017.05.239&partnerID=40&md5=2370e7394541b628b89c8d3f8b01fab4","SOC is the most important indicator of soil fertility and monitoring its space-time changes is a prerequisite to establish strategies to reduce soil loss and preserve its quality. Here we modelled the topsoil (0–0.3 m) SOC concentration of the cultivated area of Sicily in 1993 and 2008. Sicily is an extremely variable region with a high number of ecosystems, soils, and microclimates. We studied the role of time and land use in the modelling of SOC, and assessed the role of remote sensing (RS) covariates in the boosted regression trees modelling. The models obtained showed a high pseudo-R2 (0.63–0.69) and low uncertainty (s.d. &lt; 0.76 g C kg− 1 with RS, and &lt; 1.25 g C kg− 1 without RS). These outputs allowed depicting a time variation of SOC at 1 arcsec. SOC estimation strongly depended on the soil texture, land use, rainfall and topographic indices related to erosion and deposition. RS indices captured one fifth of the total variance explained, slightly changed the ranking of variance explained by the non-RS predictors, and reduced the variability of the model replicates. During the study period, SOC decreased in the areas with relatively high initial SOC, and increased in the area with high temperature and low rainfall, dominated by arables. This was likely due to the compulsory application of some Good Agricultural and Environmental practices. These results confirm that the importance of texture and land use in short-term SOC variation is comparable to climate. The present results call for agronomic and policy intervention at the district level to maintain fertility and yield potential. In addition, the present results suggest that the application of RS covariates enhanced the modelling performance. © 2017 Elsevier B.V.","Agro-ecosystems; Digital soil mapping; Legacy dataset; R programming; SOC mapping; Space-time SOC variation","Carbon; Ecology; Ecosystems; Mapping; Organic carbon; Rain; Remote sensing; Soil surveys; Soils; Space optics; Agro ecosystems; Digital soil mappings; Legacy dataset; R programming; Space time; Land use; organic carbon; soil organic matter; agricultural ecosystem; data set; land use; mapping method; Mediterranean environment; modeling; organic carbon; remote sensing; semiarid region; soil organic matter; soil texture; spatiotemporal analysis; topsoil; agronomic trait; arable land; Article; boosted regression tree; climate change; geographic distribution; high temperature; land use; machine learning; priority journal; remote sensing; seasonal variation; semiarid climate; Sicily; soil; soil erosion; soil fertility; soil pollution; soil texture; spatiotemporal analysis; time factor; topography; topsoil; Italy; Sicily",2-s2.0-85019998646
"Oliveira J.J.","Global exponential stability of nonautonomous neural network models with unbounded delays",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030665005&doi=10.1016%2fj.neunet.2017.09.006&partnerID=40&md5=5d3d33bcbae83ab04513b3e8a773e0a4","For a nonautonomous class of n-dimensional differential system with infinite delays, we give sufficient conditions for its global exponential stability, without showing the existence of an equilibrium point, or a periodic solution, or an almost periodic solution. We apply our main result to several concrete neural network models, studied in the literature, and a comparison of results is given. Contrary to usual in the literature about neural networks, the assumption of bounded coefficients is not required to obtain the global exponential stability. Finally, we present numerical examples to illustrate the effectiveness of our results. © 2017 Elsevier Ltd","Cohen–Grossberg neural networks; Global exponential stability; Infinite discrete delays; Infinite distributed delays; Unbounded coefficients","Artificial intelligence; Cognitive systems; Almost periodic solutions; Discrete delay; Existence of an equilibrium points; Global exponential stability; Infinite-distributed delays; Neural network model; Non-autonomous neural networks; Unbounded coefficients; Stability; Article; comparative effectiveness; computer simulation; equilibrium constant; machine learning; mathematical computing; mathematical model; nonautonomous neural network; priority journal",2-s2.0-85030665005
"Houlihan P., Creamer G.G.","Can Sentiment Analysis and Options Volume Anticipate Future Returns?",2017,"Computational Economics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019544023&doi=10.1007%2fs10614-017-9694-4&partnerID=40&md5=848a69a685311cba4ff160fc7782f53b","This paper evaluates the question of whether sentiment extracted from social media and options volume anticipates future asset return. The research utilized both textual based data and a particular market data derived call-put ratio, collected between July 2009 and September 2012. It shows that: (1) features derived from market data and a call-put ratio can improve model performance, (2) sentiment derived from StockTwits, a social media platform for the financial community, further enhances model performance, (3) aggregating all features together also facilitates performance, and (4) sentiment from social media and market data can be used as risk factors in an asset pricing framework. © 2017, Springer Science+Business Media New York.","Behavioral finance; Investor sentiment; Machine learning; Social media",,2-s2.0-85019544023
"Shi X., Xing F., Xu K., Xie Y., Su H., Yang L.","Supervised graph hashing for histopathology image retrieval and classification",2017,"Medical Image Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026733987&doi=10.1016%2fj.media.2017.07.009&partnerID=40&md5=9b3d56bcaba14363c07d836762b9fce9","In pathology image analysis, morphological characteristics of cells are critical to grade many diseases. With the development of cell detection and segmentation techniques, it is possible to extract cell-level information for further analysis in pathology images. However, it is challenging to conduct efficient analysis of cell-level information on a large-scale image dataset because each image usually contains hundreds or thousands of cells. In this paper, we propose a novel image retrieval based framework for large-scale pathology image analysis. For each image, we encode each cell into binary codes to generate image representation using a novel graph based hashing model and then conduct image retrieval by applying a group-to-group matching method to similarity measurement. In order to improve both computational efficiency and memory requirement, we further introduce matrix factorization into the hashing model for scalable image retrieval. The proposed framework is extensively validated with thousands of lung cancer images, and it achieves 97.98% classification accuracy and 97.50% retrieval precision with all cells of each query image used. © 2017 Elsevier B.V.","Hashing; Histopathology image analysis; Image retrieval; Large-scale images","Cells; Computational efficiency; Cytology; Factorization; Graphic methods; Image analysis; Image classification; Image segmentation; Pathology; Classification accuracy; Hashing; Image representations; Large-scale images; Matrix factorizations; Morphological characteristic; Segmentation techniques; Similarity measurements; Image retrieval; algorithm; analytical parameters; Article; cell level; clinical classification; clinical examination; conceptual framework; diagnostic accuracy; extracellular matrix; histopathology; image analysis; image retrieval; large scale production; lung cancer; mathematical analysis; memory; morphological trait; priority journal; single cell analysis; supervised graph hashing; supervised machine learning",2-s2.0-85026733987
"Brunton S.L., Brunton B.W., Proctor J.L., Kaiser E., Nathan Kutz J.","Chaos as an intermittently forced linear system",2017,"Nature Communications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019971668&doi=10.1038%2fs41467-017-00030-8&partnerID=40&md5=63efefd1bbff0c2ea9153f733df1779a","Understanding the interplay of order and disorder in chaos is a central challenge in modern quantitative science. Approximate linear representations of nonlinear dynamics have long been sought, driving considerable interest in Koopman theory. We present a universal, data-driven decomposition of chaos as an intermittently forced linear system. This work combines delay embedding and Koopman theory to decompose chaotic dynamics into a linear model in the leading delay coordinates with forcing by low-energy delay coordinates; this is called the Hankel alternative view of Koopman (HAVOK) analysis. This analysis is applied to the Lorenz system and real-world examples including Earth's magnetic field reversal and measles outbreaks. In each case, forcing statistics are non-Gaussian, with long tails corresponding to rare intermittent forcing that precedes switching and bursting phenomena. The forcing activity demarcates coherent phase space regions where the dynamics are approximately linear from those that are s rongly nonlinear. © The Author(s) 2017.",,"chaotic dynamics; decomposition analysis; Gaussian method; magnetic field; measles; nonlinearity; Article; chaotic dynamics; energy; linear system; machine learning; space",2-s2.0-85019971668
"Selkowitz A.R., Lakhmani S.G., Chen J.Y.C.","Using agent transparency to support situation awareness of the Autonomous Squad Member",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017417829&doi=10.1016%2fj.cogsys.2017.02.003&partnerID=40&md5=726645b47344ee5f9e09dbf02db08bea","Agent transparency has been proposed as a solution to the problem of facilitating operators’ situation awareness in human-robot teams. Sixty participants performed a dual monitoring task, monitoring both an intelligent, autonomous robot teammate and performing threat detection in a virtual environment. The robot displayed four different interfaces, corresponding to information from the Situation awareness-based Agent Transparency (SAT) model. Participants’ situation awareness of the robot, confidence in their situation awareness, trust in the robot, workload, cognitive processing, and perceived usability of the robot displays were assessed. Results indicate that participants using interfaces corresponding to higher SAT level had greater situation awareness, cognitive processing, and trust in the robot than when they viewed lower level SAT interfaces. No differences in workload or perceived usability of the display were detected. Based on these findings, we observed that transparency has a significant effect on situation awareness, trust, and cognitive processing. © 2017 Elsevier B.V.","Human-Robot interaction; Situation awareness; Transparency; Trust","Autonomous agents; Intelligent robots; Robots; Transparency; Virtual reality; Cognitive processing; Human-robot-team; Monitoring tasks; Perceived usability; Situation awareness; Threat detection; Trust; Human robot interaction; adult; Article; awareness; cognition; computer model; controlled study; human; human computer interaction; human experiment; intelligence test; machine learning; priority journal; robotics; situation awareness based agent transparency model; teamwork; trust; virtual reality; workload; young adult",2-s2.0-85017417829
"Xie K., Ozbay K., Zhu Y., Yang H.","Evacuation zone modeling under climate change: A data-driven method",2017,"Journal of Infrastructure Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018741518&doi=10.1061%2f%28ASCE%29IS.1943-555X.0000369&partnerID=40&md5=cfc4b4646daf3cdb6b7e828070349d67","Predetermined evacuation zones can be used to estimate the demand of evacuees, which is helpful in assessing the resilience of transportation systems in the presence of natural disasters. Evacuation zones defined based on current road networks and environmental and demo-economic characteristics of a region cannot remain the same in the future because long-term climate change such as the rise of sea level would have major impacts on hurricane-related risks. Traditional methods for the prediction of future evacuation zones rely heavily on the storm surge models and could be time-consuming and costly to use. This study develops a novel grid cell-based data-driven method that can predict future evacuation zones under climate change without running the expensive storm surge models. The map of Manhattan, which is the central area of New York City, was uniformly split into 45 × 45 m2 grid cells as the basic geographical units of analysis. A decision tree and a random forest were used to capture the relationship between grid cell-specific features, such as geographical features, evacuation mobility, and demo-economic features, and current zone categories that could reflect the risk levels during hurricanes. Tenfold cross validation was used to evaluate model performance and it was found that the random forest outperformed the decision tree in term of the accuracy and kappa statistic. The random forest was used to predict the delineation of evacuation zones in the 2050s and 2090s based on the predicted sea-level rises and changes of demo-economic features. Compared with the current zoning, the areas with a need for evacuation are expected to expand in the future. The proposed method can be used to promptly estimate the future evacuation zones under different sea-level rise scenarios and can provide the convenience to assess transportation system resilience in the context of climate change. © 2017 American Society of Civil Engineers.","Climate change; Emergency management; Evacuation zone; Hurricane; Random forest; Resilience","Climate models; Decision trees; Disasters; Floods; Forecasting; Hurricanes; Risk management; Sea level; Storms; Transportation; Data-driven methods; Economic characteristics; Emergency management; Geographical features; Random forests; Resilience; Sea-level rise scenarios; Transportation system; Climate change; climate change; hurricane; machine learning; modeling; natural disaster; prediction; storm surge; transportation system; New York [New York (STT)]; New York [United States]; United States",2-s2.0-85018741518
"Kitani A., Kimura T., Nakatani T.","Toward the reduction of incorrect drawn ink retrieval",2017,"Human-centric Computing and Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020890788&doi=10.1186%2fs13673-017-0099-0&partnerID=40&md5=6509889b92e786dc3eb6b21496f4e180","As tablet devices become popular, various handwriting applications are used. Some of applications incorporate a specific function, which is generally called palm rejection. Palm rejection enables application users to put the palm of a writing hand onto a touch display. It classifies intended touches and unintended touches so that it prevents accidental inking, which has been known to occur under the writing hand. Though some of palm rejections can remove accidental inking afterward, this function occasionally does not execute correctly as it may remove rather correct ink strokes as well. We call this interaction Incorrect Drawn Ink Retrieval (IDIR). In this paper, we propose a software algorithm that is a combination of two palm rejection logics that reduces IDIR with precision and without latency. That algorithm does not depend on specific hardware, such as an active stylus pen. Our data provides 98.98% correctness and the algorithm takes less than 10 ms for the distinction. We confirm that our experimental application reduced the occurrences of IDIR throughout an experiment. © 2017, The Author(s).","Drawn Ink Retrieval; Handwriting; Incorrect Drawn Ink Retrieval; Machine learning; Usability; User experience",,2-s2.0-85020890788
"Malinowski M., Rohrbach M., Fritz M.","Ask Your Neurons: A Deep Learning Approach to Visual Question Answering",2017,"International Journal of Computer Vision",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028770526&doi=10.1007%2fs11263-017-1038-2&partnerID=40&md5=5eb37eca94bacef97accb7ff67c62d61","We propose a Deep Learning approach to the visual question answering task, where machines answer to questions about real-world images. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We evaluate our approaches on the DAQUAR as well as the VQA dataset where we also report various baselines, including an analysis how much information is contained in the language part only. To study human consensus, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus. Finally, we evaluate a rich set of design choices how to encode, combine and decode information in our proposed Deep Learning formulation. © 2017, Springer Science+Business Media, LLC.","Computer vision; Deep learning; Natural language processing; Scene understanding; Visual question answering; Visual turing test","Computer vision; Deep learning; Natural language processing systems; Image representations; Learning formulation; Multimodal problems; Natural languages; Question Answering; Question Answering Task; Scene understanding; Turing tests; Visual languages",2-s2.0-85028770526
"Ding Y., Tang J., Guo F.","Identification of drug-target interactions via multiple information integration",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027405304&doi=10.1016%2fj.ins.2017.08.045&partnerID=40&md5=8c4e9867f6244cfe26f0be0701e38484","Identifying Drug-Target Interactions (DTIs) is an important process in drug discovery. Traditional experimental methods are expensive and time-consuming for detecting DTIs. Therefore, computational approaches provide many effective strategies to deal with this issue. In recent years, most of computational methods only use the information of drug-drug similarity or target-target similarity, which cannot perfectly capture all characteristics to identify DTIs. In this paper, we propose a novel computational model of DTIs prediction, based on machine learning methods. To improve the performance of prediction, we further use molecular substructure fingerprints, Multivariate Mutual Information (MMI) of proteins and network topology to represent drugs, targets and relationship between them. Moreover, we employ Support Vector Machine (SVM) and Feature Selection (FS) to construct model for predicting DTIs. Experiments of evaluation show that proposed approach achieves better results than other outstanding methods for feature-based DTIs prediction. The proposed approach achieves AUPRs of 0.899, 0.929, 0.821 and 0.655 on Enzyme, Ion Channel (IC), GPCR and Nuclear Receptor datasets, respectively. Compared with existing best methods, AUPRs are increased by 0.016 on Ion Channel datasets. In addition, our method obtains the second best performance on GPCR and Enzyme datasets. The source code and all datasets are available at https://figshare.com/s/53bf5a6065f3911d46f6. © 2017","Drug-target interaction; Feature selection; Molecular fingerprint; Multivariate mutual information; Support vector machine","Computational methods; Enzymes; Feature extraction; Forecasting; Learning systems; Support vector machines; Computational approach; Computational model; Drug-target interactions; Experimental methods; Information integration; Molecular fingerprint; Multivariate mutual information; Nuclear receptors; Drug interactions",2-s2.0-85027405304
"Adiga N., Khonglah B.K., Mahadeva Prasanna S.R.","Improved voicing decision using glottal activity features for statistical parametric speech synthesis",2017,"Digital Signal Processing: A Review Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030315382&doi=10.1016%2fj.dsp.2017.09.007&partnerID=40&md5=aed3d22d4b048e52117d981902ea5306","A method to improve voicing decision using glottal activity features proposed for statistical parametric speech synthesis. In existing methods, voicing decision relies mostly on fundamental frequency F0, which may result in errors when the prediction is inaccurate. Even though F0 is a glottal activity feature, other features that characterize this activity may help in improving the voicing decision. The glottal activity features used in this work are the strength of excitation (SoE), normalized autocorrelation peak strength (NAPS), and higher-order statistics (HOS). These features obtained from approximated source signals like zero-frequency filtered signal and integrated linear prediction residual. To improve voicing decision and to avoid heuristic threshold for classification, glottal activity features are trained using different statistical learning methods such as the k-nearest neighbor, support vector machine (SVM), and deep belief network. The voicing decision works best with SVM classifier, and its effectiveness is tested using the statistical parametric speech synthesis. The glottal activity features SoE, NAPS, and HOS modeled along with F0 and Mel-cepstral coefficients in Hidden Markov model and deep neural network to get the voicing decision. The objective and subjective evaluations demonstrate that the proposed method improves the naturalness of synthetic speech. © 2017 Elsevier Inc.","Glottal activity features; Statistical parametric speech synthesis; Support vector machine; Voicing decision","Deep neural networks; Hidden Markov models; Higher order statistics; Markov processes; Nearest neighbor search; Signal processing; Speech; Speech synthesis; Statistics; Support vector machines; Cepstral coefficients; Fundamental frequencies; Glottal activity features; Higherorder statistic (HOS); Objective and subjective evaluations; Statistical learning methods; Statistical parametric speech synthesis; Voicing decision; Heuristic methods",2-s2.0-85030315382
"Md Nor N., Hussain M.A., Che Hassan C.R.","Fault diagnosis and classification framework using multi-scale classification based on kernel Fisher discriminant analysis for chemical process system",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029700889&doi=10.1016%2fj.asoc.2017.09.019&partnerID=40&md5=114cc5d3cec9c9ca9ee3b4e8d96ffb2b","Fault detection and diagnosis (FDD) in chemical process systems is an important tool for effective process monitoring to ensure the safety of a process. Multi-scale classification offers various advantages for monitoring chemical processes generally driven by events in different time and frequency domains. However, there are issues when dealing with highly interrelated, complex, and noisy databases with large dimensionality. Therefore, a new method for the FDD framework is proposed based on wavelet analysis, kernel Fisher discriminant analysis (KFDA), and support vector machine (SVM) classifiers. The main objective of this work was to combine the advantages of these tools to enhance the performance of the diagnosis on a chemical process system. Initially, a discrete wavelet transform (DWT) was applied to extract the dynamics of the process at different scales. The wavelet coefficients obtained during the analysis were reconstructed using the inverse discrete wavelet transform (IDWT) method, which were then fed into the KFDA to produce discriminant vectors. Finally, the discriminant vectors were used as inputs for the SVM classification task. The SVM classifiers were utilized to classify the feature sets extracted by the proposed method. The performance of the proposed multi-scale KFDA-SVM method for fault classification and diagnosis was analysed and compared using a simulated Tennessee Eastman process as a benchmark. The results showed the improvements of the proposed multiscale KFDA-SVM framework with an average 96.79% of classification accuracy over the multi-scale KFDA-GMM (84.94%), and the established independent component analysis-SVM method (95.78%) of the faults in the Tennessee Eastman process. © 2017 Elsevier B.V.","Fault classification; Fault diagnosis; Kernel Fisher discriminant analysis; Support vector machine; Wavelet analysis","Benchmarking; Chemical analysis; Chemical detection; Classification (of information); Computer aided diagnosis; Discrete wavelet transforms; Discriminant analysis; Failure analysis; Fisher information matrix; Independent component analysis; Inverse problems; Learning algorithms; Process monitoring; Support vector machines; Vectors; Wavelet analysis; Wavelet transforms; Chemical process systems; Classification framework; Fault classification; Fault detection and diagnosis; Inverse discrete wavelet transforms; Kernel fisher discriminant analysis; Tennessee Eastman process; Time and frequency domains; Fault detection",2-s2.0-85029700889
"Gao X., Duan L.-M.","Efficient representation of quantum many-body states with deep neural networks",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029746340&doi=10.1038%2fs41467-017-00705-2&partnerID=40&md5=26a42407b045381c5603fdcb06927dda","Part of the challenge for quantum many-body problems comes from the difficulty of representing large-scale quantum states, which in general requires an exponentially large number of parameters. Neural networks provide a powerful tool to represent quantum many-body states. An important open question is what characterizes the representational power of deep and shallow neural networks, which is of fundamental interest due to the popularity of deep learning methods. Here, we give a proof that, assuming a widely believed computational complexity conjecture, a deep neural network can efficiently represent most physical states, including the ground states of many-body Hamiltonians and states generated by quantum dynamics, while a shallow network representation with a restricted Boltzmann machine cannot efficiently represent some of those states. © 2017 The Author(s).",,"artificial neural network; complexity; computer simulation; learning; machinery; parameterization; quantum mechanics; machine; nervous system",2-s2.0-85029746340
"Tiwari A.K., Nath A., Subbiah K., Shukla K.K.","Enhanced Prediction for Observed Peptide Count in Protein Mass Spectrometry Data by Optimally Balancing the Training Dataset",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020757089&doi=10.1142%2fS0218001417500409&partnerID=40&md5=a93a2fd8f98910f9f6d314b6a353e059","Imbalanced dataset affects the learning of classifiers. This imbalance problem is almost ubiquitous in biological datasets. Resampling is one of the common methods to deal with the imbalanced dataset problem. In this study, we explore the learning performance by varying the balancing ratios of training datasets, consisting of the observed peptides and absent peptides in the Mass Spectrometry experiment on the different machine learning algorithms. It has been observed that the ideal balancing ratio has yielded better performance than the imbalanced dataset, but it was not the best as compared to some intermediate ratio. By experimenting using Synthetic Minority Oversampling Technique (SMOTE) at different balancing ratios, we obtained the best results by achieving sensitivity of 92.1%, specificity value of 94.7%, overall accuracy of 93.4%, MCC of 0.869, and AUC of 0.982 with boosted random forest algorithm. This study also identifies the most discriminating features by applying the feature ranking algorithm. From the results of current experiments, it can be inferred that the performance of machine learning algorithms for the classification tasks can be enhanced by selecting optimally balanced training dataset, which can be obtained by suitably modifying the class distribution. © 2017 World Scientific Publishing Company.","Imbalanced dataset; mass spectrometry; optimal balancing; random forest; real adaboost; SMOTE","Adaptive boosting; Artificial intelligence; Bioinformatics; Classification (of information); Decision trees; Learning systems; Mass spectrometry; Peptides; Spectrometry; Imbalanced dataset; Imbalanced dataset problems; Learning performance; Mass spectrometry data; Random forest algorithm; Random forests; SMOTE; Synthetic minority over-sampling techniques; Learning algorithms",2-s2.0-85020757089
"Esteban S., Rodríguez Tablado M., Peper F.E., Mahumud Y.S., Ricci R.I., Kopitowski K.S., Terrasa S.A.","Development and validation of various phenotyping algorithms for Diabetes Mellitus using data from electronic health records",2017,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029576985&doi=10.1016%2fj.cmpb.2017.09.009&partnerID=40&md5=0f3c3dd12109877e37da0c9e0ae7a902","Background and Objective Recent progression towards precision medicine has encouraged the use of electronic health records (EHRs) as a source for large amounts of data, which is required for studying the effect of treatments or risk factors in more specific subpopulations. Phenotyping algorithms allow to automatically classify patients according to their particular electronic phenotype thus facilitating the setup of retrospective cohorts. Our objective is to compare the performance of different classification strategies (only using standardized problems, rule-based algorithms, statistical learning algorithms (six learners) and stacked generalization (five versions)), for the categorization of patients according to their diabetic status (diabetics, not diabetics and inconclusive; Diabetes of any type) using information extracted from EHRs. Methods Patient information was extracted from the EHR at Hospital Italiano de Buenos Aires, Buenos Aires, Argentina. For the derivation and validation datasets, two probabilistic samples of patients from different years (2005: n = 1663; 2015: n = 800) were extracted. The only inclusion criterion was age (≥40 & <80 years). Four researchers manually reviewed all records and classified patients according to their diabetic status (diabetic: diabetes registered as a health problem or fulfilling the ADA criteria; non-diabetic: not fulfilling the ADA criteria and having at least one fasting glycemia below 126 mg/dL; inconclusive: no data regarding their diabetic status or only one abnormal value). The best performing algorithms within each strategy were tested on the validation set. Results The standardized codes algorithm achieved a Kappa coefficient value of 0.59 (95% CI 0.49, 0.59) in the validation set. The Boolean logic algorithm reached 0.82 (95% CI 0.76, 0.88). A slightly higher value was achieved by the Feedforward Neural Network (0.9, 95% CI 0.85, 0.94). The best performing learner was the stacked generalization meta-learner that reached a Kappa coefficient value of 0.95 (95% CI 0.91, 0.98). Conclusions The stacked generalization strategy and the feedforward neural network showed the best classification metrics in the validation set. The implementation of these algorithms enables the exploitation of the data of thousands of patients accurately. © 2017 Elsevier B.V.","Diabetes Mellitus; Electronic health records; Electronic phenotyping algorithms; Stacked generalization","Classification (of information); Feedforward neural networks; Health; Records management; Buenos Aires , Argentina; Diabetes mellitus; Electronic health record; Electronic health record (EHRs); Large amounts of data; Phenotyping; Rule based algorithms; Stacked generalization; Health risks; glucose; adult; aged; algorithm; Article; diabetes mellitus; diet restriction; electronic health record; glucose blood level; human; learning algorithm; major clinical study; middle aged; patient coding; patient information; phenotyping algorithm; random forest; support vector machine",2-s2.0-85029576985
"Sun T., Jiang H., Cheng L.","Convergence of proximal iteratively reweighted nuclear norm algorithm for image processing",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028692729&doi=10.1109%2fTIP.2017.2745200&partnerID=40&md5=2c4cd2aab24f0a015702a74ca3c781ff","The nonsmooth and nonconvex regularization has many applications in imaging science and machine learning research due to its excellent recovery performance. A proximal iteratively reweighted nuclear norm algorithm has been proposed for the nonsmooth and nonconvex matrix minimizations. In this paper, we aim to investigate the convergence of the algorithm. With the Kurdyka-Łojasiewicz property, we prove the algorithm globally converges to a critical point of the objective function. The numerical results presented in this paper coincide with our theoretical findings. © 1992-2012 IEEE.","Convergence analysis; Iteratively reweighted nuclear norm algorithm; Kurdyka-Łojasiewicz property; Nonconvex regularization","Artificial intelligence; Image processing; Learning algorithms; Learning systems; Linear programming; Optimization; Signal processing; Algorithm design and analysis; Convergence; Convergence analysis; Nonconvex; Signal processing algorithms; Sparse matrices; Iterative methods",2-s2.0-85028692729
"Lamata L.","Basic protocols in quantum reinforcement learning with superconducting circuits",2017,"Scientific Reports",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018925423&doi=10.1038%2fs41598-017-01711-6&partnerID=40&md5=ce9ef4df72afb8df23cbfdc6828aa28a","Superconducting circuit technologies have recently achieved quantum protocols involving closed feedback loops. Quantum artificial intelligence and quantum machine learning are emerging fields inside quantum technologies which may enable quantum devices to acquire information from the outer world and improve themselves via a learning process. Here we propose the implementation of basic protocols in quantum reinforcement learning, with superconducting circuits employing feedback-loop control. We introduce diverse scenarios for proof-of-principle experiments with state-of-the-art superconducting circuit technologies and analyze their feasibility in presence of imperfections. The field of quantum artificial intelligence implemented with superconducting circuits paves the way for enhanced quantum control and quantum computation protocols. © 2017 The Author(s).",,"artificial intelligence; feasibility study; feedback system; reinforcement",2-s2.0-85018925423
"Zhang L., Ai H., Chen W., Yin Z., Hu H., Zhu J., Zhao J., Zhao Q., Liu H.","CarcinoPred-EL: Novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and ensemble learning methods",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019953043&doi=10.1038%2fs41598-017-02365-0&partnerID=40&md5=602ca794dfbb93f854cfe1fd1f573960","Carcinogenicity refers to a highly toxic end point of certain chemicals, and has become an important issue in the drug development process. In this study, three novel ensemble classification models, namely Ensemble SVM, Ensemble RF, and Ensemble XGBoost, were developed to predict carcinogenicity of chemicals using seven types of molecular fingerprints and three machine learning methods based on a dataset containing 1003 diverse compounds with rat carcinogenicity. Among these three models, Ensemble XGBoost is found to be the best, giving an average accuracy of 70.1 ± 2.9%, sensitivity of 67.0 ± 5.0%, and specificity of 73.1 ± 4.4% in five-fold cross-validation and an accuracy of 70.0%, sensitivity of 65.2%, and specificity of 76.5% in external validation. In comparison with some recent methods, the ensemble models outperform some machine learning-based approaches and yield equal accuracy and higher specificity but lower sensitivity than rule-based expert systems. It is also found that the ensemble models could be further improved if more data were available. As an application, the ensemble models are employed to discover potential carcinogens in the DrugBank database. The results indicate that the proposed models are helpful in predicting the carcinogenicity of chemicals. A web server called CarcinoPred-EL has been built for these models (http://ccsipb.lnu.edu.cn/toxicity/CarcinoPred-EL/). © 2017 The Author(s).",,,2-s2.0-85019953043
"Saha M., Chakraborty C., Arun I., Ahmed R., Chatterjee S.","An Advanced Deep Learning Approach for Ki-67 Stained Hotspot Detection and Proliferation Rate Scoring for Prognostic Evaluation of Breast Cancer",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020745784&doi=10.1038%2fs41598-017-03405-5&partnerID=40&md5=65021d353e58cbaf337a4f42782f9e57","Being a non-histone protein, Ki-67 is one of the essential biomarkers for the immunohistochemical assessment of proliferation rate in breast cancer screening and grading. The Ki-67 signature is always sensitive to radiotherapy and chemotherapy. Due to random morphological, color and intensity variations of cell nuclei (immunopositive and immunonegative), manual/subjective assessment of Ki-67 scoring is error-prone and time-consuming. Hence, several machine learning approaches have been reported; nevertheless, none of them had worked on deep learning based hotspots detection and proliferation scoring. In this article, we suggest an advanced deep learning model for computerized recognition of candidate hotspots and subsequent proliferation rate scoring by quantifying Ki-67 appearance in breast cancer immunohistochemical images. Unlike existing Ki-67 scoring techniques, our methodology uses Gamma mixture model (GMM) with Expectation-Maximization for seed point detection and patch selection and deep learning, comprises with decision layer, for hotspots detection and proliferation scoring. Experimental results provide 93% precision, 0.88% recall and 0.91% F-score value. The model performance has also been compared with the pathologists' manual annotations and recently published articles. In future, the proposed deep learning framework will be highly reliable and beneficial to the junior and senior pathologists for fast and efficient Ki-67 scoring. © 2017 The Author(s).",,,2-s2.0-85020745784
"Ponzoni I., Sebastián-Pérez V., Requena-Triguero C., Roca C., Martínez M.J., Cravero F., Díaz M.F., Páez J.A., Arrayás R.G., Adrio J., Campillo N.E.","Hybridizing Feature Selection and Feature Learning Approaches in QSAR Modeling for Drug Discovery /631/114/2248 /631/154/309 /639/638/563/606 /119/118 article",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019743419&doi=10.1038%2fs41598-017-02114-3&partnerID=40&md5=f642a98621fb3e483621649dd61815ac","Quantitative structure-activity relationship modeling using machine learning techniques constitutes a complex computational problem, where the identification of the most informative molecular descriptors for predicting a specific target property plays a critical role. Two main general approaches can be used for this modeling procedure: feature selection and feature learning. In this paper, a performance comparative study of two state-of-art methods related to these two approaches is carried out. In particular, regression and classification models for three different issues are inferred using both methods under different experimental scenarios: two drug-like properties, such as blood-brain-barrier and human intestinal absorption, and enantiomeric excess, as a measurement of purity used for chiral substances. Beyond the contrastive analysis of feature selection and feature learning methods as competitive approaches, the hybridization of these strategies is also evaluated based on previous results obtained in material sciences. From the experimental results, it can be concluded that there is not a clear winner between both approaches because the performance depends on the characteristics of the compound databases used for modeling. Nevertheless, in several cases, it was observed that the accuracy of the models can be improved by combining both approaches when the molecular descriptor sets provided by feature selection and feature learning contain complementary information. © 2017 The Author(s).",,,2-s2.0-85019743419
"Wang Y., Song J., Marquez-Lago T.T., Leier A., Li C., Lithgow T., Webb G.I., Shen H.-B.","Knowledge-Transfer learning for prediction of matrix metalloprotease substrate-cleavage sites",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024909398&doi=10.1038%2fs41598-017-06219-7&partnerID=40&md5=eba10773d1084d24844c7fa29e19e16e","Matrix Metalloproteases (MMPs) are an important family of proteases that play crucial roles in key cellular and disease processes. Therefore, MMPs constitute important targets for drug design, development and delivery. Advanced proteomic technologies have identified type-specific target substrates; however, the complete repertoire of MMP substrates remains uncharacterized. Indeed, computational prediction of substrate-cleavage sites associated with MMPs is a challenging problem. This holds especially true when considering MMPs with few experimentally verified cleavage sites, such as for MMP-2,-3,-7, and-8. To fill this gap, we propose a new knowledge-Transfer computational framework which effectively utilizes the hidden shared knowledge from some MMP types to enhance predictions of other, distinct target substrate-cleavage sites. Our computational framework uses support vector machines combined with transfer machine learning and feature selection. To demonstrate the value of the model, we extracted a variety of substrate sequence-derived features and compared the performance of our method using both 5-fold cross-validation and independent tests. The results show that our transfer-learning-based method provides a robust performance, which is at least comparable to traditional feature-selection methods for prediction of MMP-2,-3,-7,-8,-9 and-12 substrate-cleavage sites on independent tests. The results also demonstrate that our proposed computational framework provides a useful alternative for the characterization of sequence-level determinants of MMP-substrate specificity. © 2017 The Author(s).",,,2-s2.0-85024909398
"Wu C., Kim T.W., Choi H.Y., Strukov D.B., Yang J.J.","Flexible three-dimensional artificial synapse networks with correlated learning and trainable memory capability",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030311796&doi=10.1038%2fs41467-017-00803-1&partnerID=40&md5=7057b8a236227edb70d1b5e4a6958ee3","If a three-dimensional physical electronic system emulating synapse networks could be built, that would be a significant step toward neuromorphic computing. However, the fabrication complexity of complementary metal-oxide-semiconductor architectures impedes the achievement of three-dimensional interconnectivity, high-device density, or flexibility. Here we report flexible three-dimensional artificial chemical synapse networks, in which two-terminal memristive devices, namely, electronic synapses (e-synapses), are connected by vertically stacking crossbar electrodes. The e-synapses resemble the key features of biological synapses: unilateral connection, long-term potentiation/depression, a spike-timing-dependent plasticity learning rule, paired-pulse facilitation, and ultralow-power consumption. The three-dimensional artificial synapse networks enable a direct emulation of correlated learning and trainable memory capability with strong tolerances to input faults and variations, which shows the feasibility of using them in futuristic electronic devices and can provide a physical platform for the realization of smart memories and machine learning and for operation of the complex algorithms involving hierarchical neural networks. © 2017 The Author(s).",,,2-s2.0-85030311796
"Oakden-Rayner L., Carneiro G., Bessen T., Nascimento J.C., Bradley A.P., Palmer L.J.","Precision Radiology: Predicting longevity using feature engineering and deep learning methods in a radiomics framework",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019233675&doi=10.1038%2fs41598-017-01931-w&partnerID=40&md5=ac04e491615033f9bd69ebf67aab8f20","Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous 'manual' clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research - mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives. © 2017 The Author(s).",,,2-s2.0-85019233675
"Aubreville M., Knipfer C., Oetter N., Jaremenko C., Rodner E., Denzler J., Bohr C., Neumann H., Stelzle F., Maier A.","Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029640867&doi=10.1038%2fs41598-017-12320-8&partnerID=40&md5=a464a4cef6f097ec5e118abcb3cde321","Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral epithelium. Despite their high impact on mortality, sufficient screening methods for early diagnosis of OSCC often lack accuracy and thus OSCCs are mostly diagnosed at a late stage. Early detection and accurate outline estimation of OSCCs would lead to a better curative outcome and a reduction in recurrence rates after surgical treatment. Confocal Laser Endomicroscopy (CLE) records sub-surface micro-Anatomical images for in vivo cell structure analysis. Recent CLE studies showed great prospects for a reliable, real-Time ultrastructural imaging of OSCC in situ. We present and evaluate a novel automatic approach for OSCC diagnosis using deep learning technologies on CLE images. The method is compared against textural feature-based machine learning approaches that represent the current state of the art. For this work, CLE image sequences (7894 images) from patients diagnosed with OSCC were obtained from 4 specific locations in the oral cavity, including the OSCC lesion. The present approach is found to outperform the state of the art in CLE image recognition with an area under the curve (AUC) of 0.96 and a mean accuracy of 88.3% (sensitivity 86.6%, specificity 90%). © 2017 The Author(s).",,,2-s2.0-85029640867
"Ashfaq R.A.R., He Y.-L., Chen D.-G.","Toward an efficient fuzziness based instance selection methodology for intrusion detection system",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032367344&doi=10.1007%2fs13042-016-0557-4&partnerID=40&md5=c23911bfc4ee24727395099026b56689","Building a high quality classifier is one of the key problems in the field of machine learning (ML) and pattern recognition. Many ML algorithms have suffered from high computational power in the presence of large scale data sets. This paper proposes a fuzziness based instance selection technique for the large data sets to increase the efficiency of supervised learning algorithms by improving the shortcomings of designing an effective intrusion detection system (IDS). The proposed methodology is dependent on a new kind of single layer feed-forward neural network (SLFN), called random weight neural network (RWNN). At the first stage, a membership vector corresponding to every training instance is obtained by using RWNN for computing the fuzziness. Secondly, the training instances (along with their fuzziness values) according to the actual class labels are grouped separately. After this, the instances having low fuzziness values in each group are extracted, which are used to build a reduced data set. The instances outputted by the proposed method are used as an input for ML classifiers, which result in reducing the learning time and also increasing the learning capability. The proposed methodology exhibits that the reduced data set can easily learn the boundaries between class labels. The most obvious finding from this study is a considerable increase in the accuracy rate with unseen examples when compared with other instance selection method, i.e., IB2. The proposed method provides the better generalization and fast learning capability. The reasonability of the proposed methodology is theoretically explained and experiments on well known ID data sets support its usefulness. © 2016, Springer-Verlag Berlin Heidelberg.","Fuzziness; Instance selection; Intrusion detection system; Random weight neural network; Sample reduction","Computer crime; Data reduction; Fuzzy set theory; Fuzzy systems; Learning algorithms; Learning systems; Mercury (metal); Network security; Pattern recognition; Fuzziness; Instance selection; Intrusion Detection Systems; Random weight; Sample reduction; Intrusion detection",2-s2.0-85032367344
"Satorres Martínez S., Ortega Vázquez C., Gámez García J., Gómez Ortega J.","Quality inspection of machined metal parts using an image fusion technique",2017,"Measurement: Journal of the International Measurement Confederation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026750070&doi=10.1016%2fj.measurement.2017.08.002&partnerID=40&md5=36c52dc0693cb449d330d58692b93132","Suppliers of metallic components with high-precision surfaces are facing a trend towards zero-defect tolerance regarding their finishing. Because of the lack of standardization and the difficulty of the task, this inspection is generally done manually. This work aims to detect defects on machined metal parts even if their orientation and shape are very similar to the surface finishing. A machine vision system, performing the detection of flaws on textured surfaces is fully described. One of its main devices, the lighting system, has been carefully designed to ensure the imaging of defects. Hence, multiple images are acquired under different lighting conditions, processed separately, and merged into one. Features extracted from this fused image and the former processing steps establish the feature space for a supervised learning classifier based on artificial neural networks. Results of the automated inspection show that the system works effectively with a low value of false rejections, which makes it suitable for industrial applications. © 2017","Automated surface inspection system; Image fusion; Machine vision; Metal surface inspection","Computer vision; Image processing; Inspection; Lighting; Metals; Neural networks; Surface defects; Automated inspection; Automated surface inspection systems; Image fusion techniques; Lighting conditions; Machine vision systems; Metal surfaces; Metallic component; Quality inspection; Image fusion",2-s2.0-85026750070
"Qiu X., Zhang L., Nagaratnam Suganthan P., Amaratunga G.A.J.","Oblique random forest ensemble via Least Square Estimation for time series forecasting",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028356347&doi=10.1016%2fj.ins.2017.08.060&partnerID=40&md5=5902c839f36250b7a85cd54acb900c18","Recent studies in Machine Learning indicates that the classifiers most likely to be the bests are the random forests. As an ensemble classifier, random forest combines multiple decision trees to significant decrease the overall variances. Conventional random forest employs orthogonal decision tree which selects one “optimal” feature to split the data instances within a non-leaf node according to impurity criteria such as Gini impurity, information gain and so on. However, orthogonal decision tree may fail to capture the geometrical structure of the data samples. Motivated by this, we make the first attempt to study the oblique random forest in the context of time series forecasting. In each node of the decision tree, instead of the single “optimal” feature based orthogonal classification algorithms used by standard random forest, a least square classifier is employed to perform partition. The proposed method is advantageous with respect to both efficiency and accuracy. We empirically evaluate the proposed method on eight generic time series datasets and five electricity load demand time series datasets from the Australian Energy Market Operator and compare with several other benchmark methods. © 2017 Elsevier Inc.","Ensemble learning; Neural networks; Oblique random forest; Support vector regression; Time series forecasting","Data mining; Decision trees; Learning systems; Neural networks; Time series; Classification algorithm; Ensemble classifiers; Ensemble learning; Geometrical structure; Least square estimation; Random forests; Support vector regression (SVR); Time series forecasting; Forecasting",2-s2.0-85028356347
"Dubin R., Dvir A., Pele O., Hadar O.","I Know What You Saw Last Minute-Encrypted HTTP Adaptive Video Streaming Title Classification",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028884030&doi=10.1109%2fTIFS.2017.2730819&partnerID=40&md5=2642984a8c60ccdadbd7713399d40fbd","Desktops can be exploited to violate privacy. There are two main types of attack scenarios: Active and passive. We consider the passive scenario where the adversary does not interact actively with the device, but is able to eavesdrop on the network traffic of the device from the network side. In the near future, most Internet traffic will be encrypted and thus passive attacks are challenging. Previous research has shown that information can be extracted from encrypted multimedia streams. This includes video title classification of non HTTP adaptive streams. This paper presents algorithms for encrypted HTTP adaptive video streaming title classification. We show that an external attacker can identify the video title from video HTTP adaptive streams sites, such as YouTube. To the best of our knowledge, this is the first work that shows this. We provide a large data set of 15000 YouTube video streams of 2100 popular video titles that was collected under real-world network conditions. We present several machine learning algorithms for the task and run a thorough set of experiments, which shows that our classification accuracy is higher than 95%. We also show that our algorithms are able to classify video titles that are not in the training set as unknown and some of the algorithms are also able to eliminate false prediction of video titles and instead report unknown. Finally, we evaluate our algorithm robustness to delays and packet losses at test time and show that our solution is robust to these changes. © 2017 IEEE.","classification; encrypted traffic; HTTP adaptive video streaming; HTTP2; YouTube","Classification (of information); Cryptography; Internet; Learning algorithms; Learning systems; Media streaming; Video streaming; Adaptive video streaming; Algorithm design and analysis; Bit rates; Encrypted traffic; HTTP2; Streaming media; Wireless fidelities; YouTube; HTTP",2-s2.0-85028884030
"Swarup Das A., Mehta S., Subramaniam L.V.","AnnoFin–A hybrid algorithm to annotate financial text",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024107698&doi=10.1016%2fj.eswa.2017.07.016&partnerID=40&md5=fa15ad6d13a3dec7b3324f9fe844f96e","In this work, we study the problem of annotating a large volume of Financial text by learning from a small set of human-annotated training data. The training data is prepared by randomly selecting some text sentences from the large corpus of financial text. Conventionally, bootstrapping algorithm is used to annotate large volume of unlabeled data by learning from a small set of annotated data. However, the small set of annotated data have to be carefully chosen as seed data. Thus, our approach is a digress from the conventional approach of bootstrapping as we let the users randomly select the seed data. We show that our proposed algorithm has an accuracy of 73.56% in classifying the financial texts into the different categories (“Accounting”, “Cost”, “Employee”, “Financing”, “Sales”, “Investments”, “Operations”, “Profit”, “Regulations” and “Irrelevant”) even when the training data is just 30% of the total data set. Additionally, the accuracy improves by an approximate average of 2% for an increase of the training data by 10% and the accuracy of our system is 77.91% when the training data is about 50% of the total data set. As a dictionary of hand chosen keywords prepared by domain experts are often used for financial text extraction, we assumed the existence of almost linearly separable hyperplanes between the different classes and therefore, we have used Linear Support Vector Machine along with a modified version of Label Propagation Algorithm which exploits the notion of neighborhood (in Euclidean space) for classification. We believe that our proposed techniques will be of help to Early Warning Systems used in banks where large volumes of unstructured texts need to be processed for better insights about a company. © 2017 Elsevier Ltd","Clustering; Financial sentences; Label propagation algorithm; SVM; Text classification","Classification (of information); Clustering algorithms; Education; Finance; Geometry; Support vector machines; Text processing; Vector spaces; Annotated training data; Bootstrapping algorithm; Clustering; Conventional approach; Financial sentences; Label propagation; Linear Support Vector Machines; Text classification; Personnel training",2-s2.0-85024107698
"Pedretti G., Milo V., Ambrogio S., Carboni R., Bianchi S., Calderoni A., Ramaswamy N., Spinelli A.S., Ielmini D.","Memristive neural network for on-line learning and tracking with brain-inspired spike timing dependent plasticity",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023760967&doi=10.1038%2fs41598-017-05480-0&partnerID=40&md5=c8c0e2d42b9dfb7b4a43d464a7953990","Brain-inspired computation can revolutionize information technology by introducing machines capable of recognizing patterns (images, speech, video) and interacting with the external world in a cognitive, humanlike way. Achieving this goal requires first to gain a detailed understanding of the brain operation, and second to identify a scalable microelectronic technology capable of reproducing some of the inherent functions of the human brain, such as the high synaptic connectivity (~104) and the peculiar time-dependent synaptic plasticity. Here we demonstrate unsupervised learning and tracking in a spiking neural network with memristive synapses, where synaptic weights are updated via brain-inspired spike timing dependent plasticity (STDP). The synaptic conductance is updated by the local time-dependent superposition of pre-and post-synaptic spikes within a hybrid one-transistor/one-resistor (1T1R) memristive synapse. Only 2 synaptic states, namely the low resistance state (LRS) and the high resistance state (HRS), are sufficient to learn and recognize patterns. Unsupervised learning of a static pattern and tracking of a dynamic pattern of up to 4 × 4 pixels are demonstrated, paving the way for intelligent hardware technology with up-scaled memristive neural networks. © 2017 The Author(s).",,,2-s2.0-85023760967
"Long N.P., Lim D.K., Mo C., Kim G., Kwon S.W.","Development and assessment of a lysophospholipid-based deep learning model to discriminate geographical origins of white rice",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027866872&doi=10.1038%2fs41598-017-08892-0&partnerID=40&md5=964d666289796562da33093b44b6fb8f","Geographical origin determination of white rice has become the major issue of food industry. However, there is still lack of a high-throughput method for rapidly and reproducibly differentiating the geographical origins of commercial white rice. In this study, we developed a method that employed lipidomics and deep learning to discriminate white rice from Korea to China. A total of 126 white rice of 30 cultivars from different regions were utilized for the method development and validation. By using direct infusion-mass spectrometry-based targeted lipidomics, 17 lysoglycerophospholipids were simultaneously characterized within minutes per sample. Unsupervised data exploration showed a noticeable overlap of white rice between two countries. In addition, lysophosphatidylcholines (lysoPCs) were prominent in white rice from Korea while lysophosphatidylethanolamines (lysoPEs) were enriched in white rice from China. A deep learning prediction model was built using 2014 white rice and validated using two different batches of 2015 white rice. The model accurately discriminated white rice from two countries. Among 10 selected predictors, lysoPC(18:2), lysoPC(14:0), and lysoPE(16:0) were the three most important features. Random forest and gradient boosting machine models also worked well in this circumstance. In conclusion, this study provides an architecture for high-throughput classification of white rice from different geographical origins. © 2017 The Author(s).",,,2-s2.0-85027866872
"Pierella C., Abdollahi F., Thorp E., Farshchiansadegh A., Pedersen J., Seáñez-González I., Mussa-Ivaldi F.A., Casadio M.","Learning new movements after paralysis: Results from a home-based study",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022071589&doi=10.1038%2fs41598-017-04930-z&partnerID=40&md5=6ca1c147585028e6c8c6d5f3d13884bd","Body-machine interfaces (BMIs) decode upper-body motion for operating devices, such as computers and wheelchairs. We developed a low-cost portable BMI for survivors of cervical spinal cord injury and investigated it as a means to support personalized assistance and therapy within the home environment. Depending on the specific impairment of each participant, we modified the interface gains to restore a higher level of upper body mobility. The use of the BMI over one month led to increased range of motion and force at the shoulders in chronic survivors. Concurrently, subjects learned to reorganize their body motions as they practiced the control of a computer cursor to perform different tasks and games. The BMI allowed subjects to generate any movement of the cursor with different motions of their body. Through practice subjects demonstrated a tendency to increase the similarity between the body motions used to control the cursor in distinct tasks. Nevertheless, by the end of learning, some significant and persistent differences appeared to persist. This suggests the ability of the central nervous system to concurrently learn operating the BMI while exploiting the possibility to adapt the available mobility to the specific spatio-temporal requirements of each task. © 2017 The Author(s).",,,2-s2.0-85022071589
"Wang J., Zheng C., Chen W., Wu X.","Learning aggregated features and optimizing model for semantic labeling",2017,"Visual Computer",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983396840&doi=10.1007%2fs00371-016-1302-4&partnerID=40&md5=d76fa2141599eca2cea90e4c4d07a828","Semantic labeling for indoor scenes has been extensively developed with the wide availability of affordable RGB-D sensors. However, it is still a challenging task for multi-class recognition, especially for “small” objects. In this paper, a novel semantic labeling model based on aggregated features and contextual information is proposed. Given an RGB-D image, the proposed model first creates a hierarchical segmentation using an adapted gPb/UCM algorithm. Then, a support vector machine is trained to predict initial labels using aggregated features, which fuse small-scale appearance features, mid-scale geometric features, and large-scale scene features. Finally, a joint multi-label Conditional random field model that exploits both spatial and attributive contextual relations is constructed to optimize the initial semantic and attributive predicted results. The experimental results on the public NYU v2 dataset demonstrate the proposed model outperforms the existing state-of-the-art methods on the challenging 40 dominant classes task, and the model also achieves a good performance on a recent SUN RGB-D dataset. Especially, the prediction accuracy of “small” classes has been improved significantly. © 2016, Springer-Verlag Berlin Heidelberg.","Aggregated features; Conditional random field; Joint optimizing model; Object attribute; Semantic scene understanding","Aggregates; Random processes; Semantics; Aggregated features; Conditional random field; Object attributes; Optimizing models; Scene understanding; Image segmentation",2-s2.0-84983396840
"Ma L., Song D., Liao L., Wang J.","PSVM: a preference-enhanced SVM model using preference data for classification",2017,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024098161&doi=10.1007%2fs11432-016-9020-4&partnerID=40&md5=f1ab138ae465fec97d08e921f19ed021","Classification is an essential task in data mining, machine learning and pattern recognition areas. Conventional classification models focus on distinctive samples from different categories. There are fine-grained differences between data instances within a particular category. These differences form the preference information that is essential for human learning, and, in our view, could also be helpful for classification models. In this paper, we propose a preference-enhanced support vector machine (PSVM), that incorporates preference-pair data as a specific type of supplementary information into SVM. Additionally, we propose a two-layer heuristic sampling method to obtain effective preference-pairs, and an extended sequential minimal optimization (SMO) algorithm to fit PSVM. To evaluate our model, we use the task of knowledge base acceleration-cumulative citation recommendation (KBA-CCR) on the TREC-KBA-2012 dataset and seven other datasets from UCI, StatLib and mldata.org. The experimental results show that our proposed PSVM exhibits high performance with official evaluation metrics. © 2017, Science China Press and Springer-Verlag GmbH Germany.","classification; preference; sampling; sequential minimal optimization (SMO); SVM","Education; Heuristic methods; Human form models; Knowledge based systems; Optimization; Pattern recognition; Sampling; Support vector machines; Classification models; Evaluation metrics; Heuristic sampling; preference; Preference information; Sequential minimal optimization; Sequential minimal optimization algorithms; Supplementary information; Classification (of information)",2-s2.0-85024098161
"Aravkin A., Burke J.V., Ljung L., Lozano A., Pillonetto G.","Generalized Kalman smoothing: Modeling and algorithms",2017,"Automatica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028849496&doi=10.1016%2fj.automatica.2017.08.011&partnerID=40&md5=8b190816827a8e746e2c969bd2790fb2","State-space smoothing has found many applications in science and engineering. Under linear and Gaussian assumptions, smoothed estimates can be obtained using efficient recursions, for example Rauch–Tung–Striebel and Mayne–Fraser algorithms. Such schemes are equivalent to linear algebraic techniques that minimize a convex quadratic objective function with structure induced by the dynamic model. These classical formulations fall short in many important circumstances. For instance, smoothers obtained using quadratic penalties can fail when outliers are present in the data, and cannot track impulsive inputs and abrupt state changes. Motivated by these shortcomings, generalized Kalman smoothing formulations have been proposed in the last few years, replacing quadratic models with more suitable, often nonsmooth, convex functions. In contrast to classical models, these general estimators require use of iterated algorithms, and these have received increased attention from control, signal processing, machine learning, and optimization communities. In this survey we show that the optimization viewpoint provides the control and signal processing community great freedom in the development of novel modeling and inference frameworks for dynamical systems. We discuss general statistical models for dynamic systems, making full use of nonsmooth convex penalties and constraints, and providing links to important models in signal processing and machine learning. We also survey optimization techniques for these formulations, paying close attention to dynamic problem structure. Modeling concepts and algorithms are illustrated with numerical examples. © 2017 Elsevier Ltd",,"Artificial intelligence; Dynamical systems; Functions; Learning systems; Optimization; Surveys; Control and signal processing; Gaussian assumption; Kalman smoothing; Model and algorithms; Modeling concepts; Optimization techniques; Quadratic objective functions; Science and engineering; Signal processing",2-s2.0-85028849496
"Shukla M., Dos Santos R., Fong A., Lu C.-T.","DERIV: distributed brand perception tracking framework",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021138081&doi=10.1186%2fs40537-017-0078-3&partnerID=40&md5=d69cce420dd8b3bf19aea05db6ba1048","Determining user’s perception of a brand in short periods of time has become crucial for business. Distilling brand perception directly from people’s comments in social media has promise. Current techniques for determining brand perception, such as surveys of handpicked users by mail, in person, phone or online, are time consuming and increasingly inadequate. The DERIV system distills storylines from open data representing direct consumer voice into a brand perception. The framework summarizes perception of a brand in comparison to peer brands with in-memory distributed algorithms utilizing supervised machine learning techniques. Experiments performed with open data and models built with storylines of known peer brands show the technique as highly scalable and accurate in capturing brand perception from vast amounts of social data compared to sentiment analysis. © 2017, The Author(s).","Big data; Brand perception; Distributed learning; In-memory distribution",,2-s2.0-85021138081
"Prusa J.D., Khoshgoftaar T.M.","Improving deep neural network design with new text data representations",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017014012&doi=10.1186%2fs40537-017-0065-8&partnerID=40&md5=fbe68c7ea07ed92aeb88d8923d636933","Using traditional machine learning approaches, there is no single feature engineering solution for all text mining and learning tasks. Thus, researchers must determine and implement the best feature engineering approach for each text classification task; however, deep learning allows us to skip this step by extracting and learning high-level features automatically from low-level text representations. Convolutional neural networks, a popular type of neural network for deep learning, have been shown to be effective at performing feature extraction and classification for many domains including text. Recently, it was demonstrated that convolutional neural networks can be used to train classifiers from character-level representations of text. This approach achieved superior performance compared to classifiers trained on word-level text representations, likely due to the use of character-level representations preserving more information from the data. Training neural networks from character level data requires a large volume of instances; however, the large volume of training data and model complexity makes training these networks a slow and computationally expensive task. In this paper, we propose a new method of creating character-level representations of text to reduce the computational costs associated with training a deep convolutional neural network. We demonstrate that our method of character embedding greatly reduces training time and memory use, while significantly improving classification performance. Additionally, we show that our proposed embedding can be used with padded convolutional layers to enable the use of current convolutional network architectures, while still facilitating faster training and higher performance than the previous approach for learning from character-level text. © 2017, The Author(s).","Big data; Character embedding; Convolutional neural networks; Deep learning; GPU computing; Sentiment; Text mining",,2-s2.0-85017014012
"Banerjee A., Maji P.","Stomped-t: A novel probability distribution for rough-probabilistic clustering",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028731427&doi=10.1016%2fj.ins.2017.08.083&partnerID=40&md5=34e22296a19b20a456bd5ad2f6f8476d","Rough clustering is one of the principal research areas in data mining, machine learning, pattern recognition, and bioinformatics. Among different variants of rough clustering, rough-probabilistic clustering is a new concept introduced recently. In rough-probabilistic clustering, a class is defined as the union of two disjoint regions, namely, a crisp lower approximation region and a probabilistic boundary region. In this regard, stomped normal (SN) distribution provides a statistical modeling of the data set in rough-probabilistic clustering framework. The SN distribution models the central tendency, dispersion, and width of the lower approximation region of each class using its mean, variance, and width parameter, respectively. However, it does not take into consideration the property of kurtosis of the class distribution, which controls the concentration of values around mean and shape of the tail of data distribution. In this background, a novel probability distribution, named stomped-t (St) distribution, is introduced in the paper for rough-probabilistic clustering. The proposed probability distribution incorporates the property of kurtosis into the SN framework. The proposed St probability distribution is then integrated within the rough-probabilistic clustering framework for precise and robust clustering of the data. The efficacy of the proposed clustering algorithm is demonstrated for unsupervised data clustering and image segmentation problems, along with a comparative performance analysis with related algorithms. © 2017 Elsevier Inc.","Expectation-maximization algorithm; Hidden Markov random field model; Image segmentation; Rough-probabilistic clustering; Stomped normal distribution","Cluster analysis; Data mining; Higher order statistics; Image segmentation; Learning systems; Markov processes; Maximum principle; Normal distribution; Pattern recognition; Probability; Probability distributions; Class distributions; Comparative performance analysis; Distribution models; Expectation-maximization algorithms; Hidden Markov random field model; Lower approximation; Probabilistic clustering; Statistical modeling; Clustering algorithms",2-s2.0-85028731427
"Tian M., Wang W.","Some sets of orthogonal polynomial kernel functions",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029513712&doi=10.1016%2fj.asoc.2017.08.010&partnerID=40&md5=4a84d203ec04c0dfba645b5fa63331bb","Kernel methods provide high performance in a variety of machine learning tasks. However, the success of kernel methods is heavily dependent on the selection of the right kernel function and proper setting of its parameters. Several sets of kernel functions based on orthogonal polynomials have been proposed recently. Besides their good performance in the error rate, these kernel functions have only one parameter chosen from a small set of integers, and it facilitates kernel selection greatly. Two sets of orthogonal polynomial kernel functions, namely the triangularly modified Chebyshev kernels and the triangularly modified Legendre kernels, are proposed in this study. Furthermore, we compare the construction methods of some orthogonal polynomial kernels and highlight the similarities and differences among them. Experiments on 32 data sets are performed for better illustration and comparison of these kernel functions in classification and regression scenarios. In general, there is difference among these orthogonal polynomial kernels in terms of accuracy, and most orthogonal polynomial kernels can match the commonly used kernels, such as the polynomial kernel, the Gaussian kernel and the wavelet kernel. Compared with these universal kernels, the orthogonal polynomial kernels each have a unique easily optimized parameter, and they store statistically significantly less support vectors in support vector classification. New presented kernels can obtain better generalization performance both for classification tasks and regression tasks. © 2017 Elsevier B.V.","Classification; Generalization; Kernel selection; Orthogonal polynomial kernel; Regression","Classification (of information); Learning systems; Polynomials; Regression analysis; Classification tasks; Construction method; Generalization; Generalization performance; Kernel selection; Orthogonal polynomial; Regression; Support vector classification; Orthogonal functions",2-s2.0-85029513712
"Jacob A.","Modelling speech emotion recognition using logistic regression and decision trees",2017,"International Journal of Speech Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029584048&doi=10.1007%2fs10772-017-9457-6&partnerID=40&md5=f86aa627c5abc95ceb415b48aaa288a9","Speech emotion recognition has been one of the interesting issues in speech processing over the last few decades. Modelling of the emotion recognition process serves to understand as well as assess the performance of the system. This paper compares two different models for speech emotion recognition using vocal tract features namely, the first four formants and their respective bandwidths. The first model is based on a decision tree and the second one employs logistic regression. Whereas the decision tree models are based on machine learning, regression models have a strong statistical basis. The logistic regression models and the decision tree models developed in this work for several cases of binary classifications were validated by speech emotion recognition experiments conducted on a Malayalam emotional speech database of 2800 speech files, collected from ten speakers. The models are not only simple, but also meaningful since they indicate the contribution of each predictor. The experimental results indicate that speech emotion recognition using formants and bandwidths was better modelled using decision trees, which gave higher emotion recognition accuracies compared to logistic regression. The highest accuracy obtained using decision tree was 93.63%, for the classification of positive valence emotional speech as surprised or happy, using seven features. When using logistic regression for the same binary classification, the highest accuracy obtained was 73%, with eight features. © 2017, Springer Science+Business Media, LLC.","Decision trees; Logistic regression; Modelling; Speech emotion recognition","Bandwidth; Binary trees; Classification (of information); Decision trees; Fisher information matrix; Learning systems; Models; Regression analysis; Speech; Speech analysis; Speech processing; Trees (mathematics); Binary classification; Decision tree models; Emotion recognition; Emotional speech; Logistic regression models; Logistic regressions; Regression model; Speech emotion recognition; Speech recognition",2-s2.0-85029584048
"Nidheesh N., Abdul Nazeer K.A., Ameer P.M.","An enhanced deterministic K-Means clustering algorithm for cancer subtype prediction from gene expression data",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032453629&doi=10.1016%2fj.compbiomed.2017.10.014&partnerID=40&md5=215ea56f69700bc227707846ad60d948","Background Clustering algorithms with steps involving randomness usually give different results on different executions for the same dataset. This non-deterministic nature of algorithms such as the K-Means clustering algorithm limits their applicability in areas such as cancer subtype prediction using gene expression data. It is hard to sensibly compare the results of such algorithms with those of other algorithms. The non-deterministic nature of K-Means is due to its random selection of data points as initial centroids. Method We propose an improved, density based version of K-Means, which involves a novel and systematic method for selecting initial centroids. The key idea of the algorithm is to select data points which belong to dense regions and which are adequately separated in feature space as the initial centroids. Results We compared the proposed algorithm to a set of eleven widely used single clustering algorithms and a prominent ensemble clustering algorithm which is being used for cancer data classification, based on the performances on a set of datasets comprising ten cancer gene expression datasets. The proposed algorithm has shown better overall performance than the others. Conclusion There is a pressing need in the Biomedical domain for simple, easy-to-use and more accurate Machine Learning tools for cancer subtype prediction. The proposed algorithm is simple, easy-to-use and gives stable results. Moreover, it provides comparatively better predictions of cancer subtypes from gene expression data. © 2017 Elsevier Ltd","Cancer subtype prediction; Centroid initialization; Clustering; Density based; Gene expression data; K-Means","Classification (of information); Diseases; Forecasting; Gene expression; Genes; Learning systems; Cancer subtypes; Centroid initializations; Clustering; Density-based; Gene Expression Data; K-means; Clustering algorithms",2-s2.0-85032453629
"Papa J.P., Rosa G.H., Papa L.P.","A binary-constrained Geometric Semantic Genetic Programming for feature selection purposes",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030842836&doi=10.1016%2fj.patrec.2017.10.002&partnerID=40&md5=16a3cda1eb783696b724842df9b359ba","Feature selection concerns the task of finding the subset of features that are most relevant to some specific problem in the context of machine learning. By selecting proper features, one can reduce the computational complexity of the learned model, and to possibly enhance its effectiveness by reducing the well-known overfitting. During the last years, the problem of feature selection has been modeled as an optimization task, where the idea is to find the subset of features that maximize some fitness function, which can be a given classifier's accuracy or even some measure concerning the samples’ separability in the feature space, for instance. In this paper, we introduced Geometric Semantic Genetic Programming (GSGP) in the context of feature selection, and we experimentally showed it can work properly with both conic and non-conic fitness landscapes. We observed that there is no need to restrict the feature selection modeling into GSGP constraints, which can be quite useful to adopt the semantic operators to a broader range of applications. © 2017 Elsevier B.V.","Feature selection; Geometric Semantic Genetic Programming; Optimum-path forest","Genetic algorithms; Genetic programming; Geometry; Learning systems; Semantics; Fitness functions; Fitness landscape; Geometric semantics; Optimization task; Optimum-path forests; Selection model; Semantic operators; Specific problems; Feature extraction",2-s2.0-85030842836
"Costa C., Santos M.Y.","The data scientist profile and its representativeness in the European e-Competence framework and the skills framework for the information age",2017,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028778460&doi=10.1016%2fj.ijinfomgt.2017.07.010&partnerID=40&md5=72abf74e23deed95f4be8fbb5f265229","The activities in our current world are mainly supported by data-driven web applications, making extensive use of databases and data services. Such phenomenon led to the rise of Data Scientists as professionals of major relevance, which extract value from data and create state-of-the-art data artifacts that generate even more increased value. During the last years, the term Data Scientist attracted significant attention. Consequently, it is relevant to understand its origin, knowledge base and skills set, in order to adequately describe its profile and distinguish it from others like Business Analyst. This work proposes a conceptual model for the professional profile of a Data Scientist and evaluates the representativeness of this profile in two commonly recognized competences/skills frameworks in the field of Information and Communications Technology (ICT), namely in the European e-Competence (e-CF) framework and the Skills Framework for the Information Age (SFIA). The results indicate that a significant part of the knowledge base and skills set of Data Scientists are related with ICT competences/skills, including programming, machine learning and databases. The Data Scientist professional profile has an adequate representativeness in these two frameworks, but it is mainly seen as a multi-disciplinary profile, combining contributes from different areas, such as computer science, statistics and mathematics. © 2017 Elsevier Ltd","Conceptual model; Data science; Data scientist; Knowledge; Skills","Knowledge based systems; Learning systems; Conceptual model; Data science; Data scientist; Knowledge; Skills; Network function virtualization",2-s2.0-85028778460
"Vani K., Gupta D.","Text plagiarism classification using syntax based linguistic features",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026407025&doi=10.1016%2fj.eswa.2017.07.006&partnerID=40&md5=e0a8c3f0086dcdc7d8e7c84c437080e4","The proposed work models document level text plagiarism detection as a binary classification problem, where the task is to distinguish a given suspicious-source document pair as plagiarized or non-plagiarized. The objective is to explore the potency of syntax based linguistic features extracted using shallow natural language processing techniques for plagiarism classification task. Shallow syntactic features, viz., part of speech tags and chunks are utilized after effective pre-processing and filtrations for pruning the irrelevant information. The work further proposes the modelling of this classification phase as an intermediate stage, which will be post candidate source retrieval and before exhaustive passage level detections. A two-phase feature selection approach is proposed, which improves the effectiveness of classification by selecting appropriate set of features as the input to machine learning based classifiers. The proposed approach is evaluated on smaller and larger test conditions using the corpus of plagiarized short answers (PSA) and plagiarism instances collected from PAN corpus respectively. Under both the test conditions, performances are evaluated using general as well as advanced classification metrics. Another main contribution of the current work is the analysis of dependencies and impact of the extracted features, upon the type and complexity of plagiarism imposed in the documents. The proposed results are compared with the two state-of-the-art approaches and they outperform the baseline approaches significantly. This in turn reflects the cogency of syntactic linguistic features in document level plagiarism classification, especially for the instances close to manual or real plagiarism scenarios. © 2017 Elsevier Ltd","Chunks; Linguistic features; Plagiarism classification; POS tags; Syntactic features","Information retrieval systems; Intellectual property; Learning algorithms; Linguistics; Natural language processing systems; Syntactics; Text processing; Binary classification problems; Chunks; Classification tasks; Intermediate stage; Linguistic features; Part-of-speech tags; Plagiarism detection; Syntactic features; Classification (of information)",2-s2.0-85026407025
"Chen Y., Zeng Z., Lu J.","Neighborhood rough set reduction with fish swarm algorithm",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990949436&doi=10.1007%2fs00500-016-2393-6&partnerID=40&md5=02cf812ec7bb974734047172d23848d4","Feature reduction refers to the problem of deleting those input features that are less predictive of a given outcome; a problem encountered in many areas such as pattern recognition, machine learning and data mining. In particular, it has been successfully applied in tasks that involve datasets containing huge numbers of features. Rough set theory has been used as such a data set preprocessor with much success, but current methods are inadequate at solving the problem of numerical feature reduction. As the classical rough set model can just be used to evaluate categorical features, we introduce a neighborhood rough set model to deal with numerical datasets by defining a neighborhood relation. However, this method is still not enough to find the optimal subsets regularly. In this paper, we propose a new feature reduction mechanism based on fish swarm algorithm (FSA) in an attempt to polish up this. The method is then applied to the problem of finding optimal feature subsets in the neighborhood rough set reduction process. We define three foraging behaviors of fish to find the optimal subsets and a fitness function to evaluate the best solutions. We construct the neighborhood feature reduction algorithm based on FSA and design some experiments comparing with a heuristic neighborhood feature reduction method. Experimental results show that the FSA-based neighborhood reduction method is suitable to deal with numerical data and more possibility to find an optimal reduct. © 2016, Springer-Verlag Berlin Heidelberg.","Feature reduction; Fish swarm algorithm; Granular computing; Neighborhood system; Rough set theory","Artificial intelligence; Computation theory; Data mining; Data reduction; Granular computing; Heuristic methods; Learning systems; Numerical methods; Optimization; Pattern recognition; Problem solving; Set theory; Categorical features; Feature reduction; Fish-swarm algorithms; Foraging behaviors; Neighborhood relation; Neighborhood rough sets; Neighborhood systems; Numerical datasets; Rough set theory",2-s2.0-84990949436
"Maity M., Dhane D., Mungle T., Maiti A.K., Chakraborty C.","Web-Enabled Distributed Health-Care Framework for Automated Malaria Parasite Classification: an E-Health Approach",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032464001&doi=10.1007%2fs10916-017-0834-0&partnerID=40&md5=f47b947a9ca2a12984c1d0f0da7f36b4","Web-enabled e-healthcare system or computer assisted disease diagnosis has a potential to improve the quality and service of conventional healthcare delivery approach. The article describes the design and development of a web-based distributed healthcare management system for medical information and quantitative evaluation of microscopic images using machine learning approach for malaria. In the proposed study, all the health-care centres are connected in a distributed computer network. Each peripheral centre manages its’ own health-care service independently and communicates with the central server for remote assistance. The proposed methodology for automated evaluation of parasites includes pre-processing of blood smear microscopic images followed by erythrocytes segmentation. To differentiate between different parasites; a total of 138 quantitative features characterising colour, morphology, and texture are extracted from segmented erythrocytes. An integrated pattern classification framework is designed where four feature selection methods viz. Correlation-based Feature Selection (CFS), Chi-square, Information Gain, and RELIEF are employed with three different classifiers i.e. Naive Bayes’, C4.5, and Instance-Based Learning (IB1) individually. Optimal features subset with the best classifier is selected for achieving maximum diagnostic precision. It is seen that the proposed method achieved with 99.2% sensitivity and 99.6% specificity by combining CFS and C4.5 in comparison with other methods. Moreover, the web-based tool is entirely designed using open standards like Java for a web application, ImageJ for image processing, and WEKA for data mining considering its feasibility in rural places with minimal health care facilities. © 2017, Springer Science+Business Media, LLC.","Computer-aided diagnosis; Electronic healthcare system; Feature extraction; Feature selection; Malaria screening; Supervised classification","accuracy; algorithm; Article; blood smear; classifier; computer security; erythrocyte; health care management; health service; human; image quality; learning algorithm; light exposure; malaria; medical information; microscopy; noise; Plasmodium; quantitative analysis; sensitivity and specificity; signal noise ratio; telehealth",2-s2.0-85032464001
"Lughofer E., Richter R., Neissl U., Heidl W., Eitzinger C., Radauer T.","Explaining classifier decisions linguistically for stimulating and improving operators labeling behavior",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027400157&doi=10.1016%2fj.ins.2017.08.012&partnerID=40&md5=613cc8489efc5a3b73e8beca442746e0","In decision support and classification systems, there is usually the necessity that operators or experts provide class labels for a significant number of process samples in order to be able to establish reliable machine learning classifiers. Such labels are often affected by significant uncertainty and inconsistency due to varying human's experience and constitutions during the labeling process. This typically results in significant, unintended class overlaps. We propose several new concepts for providing enhanced explanations of classifier decisions in linguistic (human readable) form. These are intended to help operators to better understand the decision process and support them during sample annotation to improve their certainty and consistency in successive labeling cycles. This is expected to lead to better, more consistent data sets (streams) for use in training and updating classifiers. The enhanced explanations are composed of (1) grounded reasons for classification decisions, represented as linguistically readable fuzzy rules, (2) a classifier's level of uncertainty in relation to its decisions and possible alternative suggestions, (3) the degree of novelty of current samples and (4) the levels of impact of the input features on the current classification response. The last of these are based on a newly developed approach for eliciting instance-based feature importance levels, and are also used to reduce the lengths of the rules to a maximum of 3 to 4 antecedent parts to ensure readability for operators and users. The proposed techniques were embedded within an annotation GUI and applied to a real-world application scenario from the field of visual inspection. The usefulness of the proposed linguistic explanations was evaluated based on experiments conducted with six operators. The results indicate that there is approximately an 80% chance that operator/user labeling behavior improves significantly when enhanced linguistic explanations are provided, whereas this chance drops to 10% when only the classifier responses are shown. © 2017 Elsevier Inc.","Classification reasons; Classifier certainty; Degree of novelty; Instance-based feature importance levels; Linguistic explanation of classifier decisions; Operators’ Labeling behavior; Transparent fuzzy rules","Decision support systems; Electric grounding; Fuzzy inference; Fuzzy rules; Learning systems; Linguistics; Application scenario; Classification decision; Classification system; Classifier decisions; Decision supports; Degree of novelty; Instance-based feature importance levels; Visual inspection; Classification (of information)",2-s2.0-85027400157
"Rafiei M., Niknam T., Khooban M.H.","Probabilistic electricity price forecasting by improved clonal selection algorithm and wavelet preprocessing",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962258029&doi=10.1007%2fs00521-016-2279-7&partnerID=40&md5=fb2e1522067fd80bcb993bf9a51f5f18","Probabilistic forecasting is an appropriate tool that helps electricity markets participants to improve their decision making. Due to changes in electricity prices, the point forecasting accuracy cannot be guaranteed. Hence, participants are more interested in results of probabilistic forecasting methods such as prediction intervals method. In this paper, a hybrid approach for probabilistic electricity price forecasting is presented. This model is based on using improved clonal selection algorithm and extreme learning machine for neural networks training process and wavelet preprocess. The wavelet is utilized to decompose data into well behaved subsets, which increases accuracy of the model. Also, due to the high required computational time for training the neural networks, autocorrelation function is used to reduce the number of neural networks inputs. Finally, in order to evaluate the proposed probabilistic forecasting method, the Ontario and Australian electricity markets data are used. © 2016, The Natural Computing Applications Forum.","Autocorrelation function; Improved clonal selection algorithm; Prediction intervals; Probabilistic forecasting; Wavelet preprocess","Algorithms; Autocorrelation; Commerce; Costs; Decision making; Forecasting; Learning systems; Autocorrelation functions; Clonal selection algorithms; Prediction interval; Preprocess; Probabilistic forecasting; Power markets",2-s2.0-84962258029
"Melnikov A.A., Makmal A., Dunjko V., Briegel H.J.","Projective simulation with generalization",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032696034&doi=10.1038%2fs41598-017-14740-y&partnerID=40&md5=66c1ee75a6e1d6d63c21bc71722fce8c","The ability to generalize is an important feature of any intelligent agent. Not only because it may allow the agent to cope with large amounts of data, but also because in some environments, an agent with no generalization capabilities cannot learn. In this work we outline several criteria for generalization, and present a dynamic and autonomous machinery that enables projective simulation agents to meaningfully generalize. Projective simulation, a novel, physical approach to artificial intelligence, was recently shown to perform well in standard reinforcement learning problems, with applications in advanced robotics as well as quantum experiments. Both the basic projective simulation model and the presented generalization machinery are based on very simple principles. This allows us to provide a full analytical analysis of the agent's performance and to illustrate the benefit the agent gains by generalizing. Specifically, we show that already in basic (but extreme) environments, learning without generalization may be impossible, and demonstrate how the presented generalization machinery enables the projective simulation agent to learn. © 2017 The Author(s).",,"artificial intelligence; machine; reinforcement; robotics; simulation",2-s2.0-85032696034
"Wang G.","A novel SVM ensemble approach for automatic document classification",2017,"ICIC Express Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032589979&doi=10.24507%2ficicel.11.12.1773&partnerID=40&md5=c02dacda17d78b68369922d8328d5d53","Support Vector Machine (SVM) is a widely used technique in automatic document classification due to its ability to efficiently handle relatively high-dimensional and large-scale datasets without decreasing classification accuracy. However, SVM still suffers from some problems, e.g., the multi-class and kernel selection. In this paper, we propose a new SVM ensemble approach, called Bagging-RS SVM, which is based on two popular ensemble strategies, i.e., bagging and random subspace and aims at building accurate and diverse classifiers. Four benchmark datasets are selected to demonstrate the effectiveness and feasibility of the proposed methods. Experimental results reveal that Bagging-RS SVM gets the best performance among the eight methods, i.e., SVM, Bagging SVM, Random Subspace SVM, Boosting SVM, DT, NB and KNN. All these results illustrate that Bagging-RS SVM can be used as an alternative technique for automatic document classification. © 2017. ICIC Express Letters Office. All rights reserved.","Automatic document classification; Bagging; Ensemble learning; Random subspace; SVM",,2-s2.0-85032589979
"Koprowski R.","Classification",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005974415&doi=10.1007%2f978-3-319-50490-2_5&partnerID=40&md5=77b9e095757830d3dc32942e4f777f26","The acquired image features such as mean brightness, contrast, energy and homogeneity can be used for machine learning and classification. © Springer International Publishing AG 2017.",,,2-s2.0-85005974415
"Bleidorn W., Hopwood C.J., Wright A.G.","Using big data to advance personality theory",2017,"Current Opinion in Behavioral Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028462185&doi=10.1016%2fj.cobeha.2017.08.004&partnerID=40&md5=497d3be78de871e1ec4b723a45c62d68","Big data has led to remarkable advances in society. One of the most exciting applications in psychological science has been the development of computer-based assessment tools to assess human behavior and personality traits. Thus far, machine learning approaches to personality assessment have been focused on maximizing predictive validity, but have been underused to advance our understanding of personality. In this paper, we review recent machine learning studies of personality and discuss recommendations for how big data and machine learning research can be used to advance personality theory. © 2017",,,2-s2.0-85028462185
"Jiao Y., Wang X.-H., Chen R., Tang T.-Y., Zhu X.-Q., Teng G.-J.","Predictive models of minimal hepatic encephalopathy for cirrhotic patients based on large-scale brain intrinsic connectivity networks",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029512713&doi=10.1038%2fs41598-017-11196-y&partnerID=40&md5=f265b848c4ed3ab46e34668fd77aa588","We aimed to find the most representative connectivity patterns for minimal hepatic encephalopathy (MHE) using large-scale intrinsic connectivity networks (ICNs) and machine learning methods. Resting-state fMRI was administered to 33 cirrhotic patients with MHE and 43 cirrhotic patients without MHE (NMHE). The connectivity maps of 20 ICNs for each participant were obtained by dual regression. A Bayesian machine learning technique, called Graphical Model-based Multivariate Analysis, was applied to determine ICN regions that characterized group differences. The most representative ICNs were evaluated by the performance of three machine learning methods (support vector machines (SVMs), multilayer perceptrons (MLP), and C4.5). The clinical significance of these potential biomarkers was further tested. The temporal lobe network (TLN), and subcortical network (SCN), and sensorimotor network (SMN) were selected as representative ICNs. The distinct functional integration patterns of the representative ICNs were significantly correlated with behavior criteria and Child-Pugh scores. Our findings suggest the representative ICNs based on GAMMA can distinguish MHE from NMHE and provide supplementary information to current MHE diagnostic criteria. © 2017 The Author(s).",,,2-s2.0-85029512713
"Ouahilal M., Mohajir M.E., Chahhou M., Mohajir B.E.E.","A novel hybrid model based on Hodrick–Prescott filter and support vector regression algorithm for optimizing stock market price prediction",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030856570&doi=10.1186%2fs40537-017-0092-5&partnerID=40&md5=f50e3852ede484d21979407258317f7f","Predicting stock market price is considered as a challenging task of financial time series analysis, which is of great interest to stock investors, stock traders and applied researchers. Many machine learning techniques have been used in this area to predict the stock market price, including regression algorithms which can be useful tools to provide good performance of financial time series prediction. Support Vector Regression is one of the most powerful algorithms in machine learning. There have been countless successes in utilizing SVR algorithm for stock market prediction. In this paper, we propose a novel hybrid approach based on machine learning and filtering techniques. Our proposed approach combines Support Vector Regression and Hodrick–Prescott filter in order to optimize the prediction of stock price. To assess the performance of this proposed approach, we have conducted several experiments using real world datasets. The principle objective of this paper is to demonstrate the improvement in predictive performance of stock market and verify the works of our proposed model in comparison with other optimized models. The experimental results confirm that the proposed algorithm constitutes a powerful model for predicting stock market prices. © 2017, The Author(s).","Business analytics; Decision support; Financial time series forecasting; Hodrick–Prescott filter; Noise filtering techniques; Stock price prediction; Support vector regression",,2-s2.0-85030856570
"Liu J., Xu C., Yang W., Shu Y., Zheng W., Zhou F.","Multiple similarly-well solutions exist for biomedical feature selection and classification problems",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031016565&doi=10.1038%2fs41598-017-13184-8&partnerID=40&md5=6db59beabed8c5a32f8adbcdf30496c5","Binary classification is a widely employed problem to facilitate the decisions on various biomedical big data questions, such as clinical drug trials between treated participants and controls, and genome-wide association studies (GWASs) between participants with or without a phenotype. A machine learning model is trained for this purpose by optimizing the power of discriminating samples from two groups. However, most of the classification algorithms tend to generate one locally optimal solution according to the input dataset and the mathematical presumptions of the dataset. Here we demonstrated from the aspects of both disease classification and feature selection that multiple different solutions may have similar classification performances. So the existing machine learning algorithms may have ignored a horde of fishes by catching only a good one. Since most of the existing machine learning algorithms generate a solution by optimizing a mathematical goal, it may be essential for understanding the biological mechanisms for the investigated classification question, by considering both the generated solution and the ignored ones. © 2017 The Author(s).",,,2-s2.0-85031016565
"Najafabadi M.M., Khoshgoftaar T.M., Villanustre F., Holt J.","Large-scale distributed L-BFGS",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024831970&doi=10.1186%2fs40537-017-0084-5&partnerID=40&md5=065bc3bf08fa1b6dc0a9e31bdcfcf2fb","With the increasing demand for examining and extracting patterns from massive amounts of data, it is critical to be able to train large models to fulfill the needs that recent advances in the machine learning area create. L-BFGS (Limited-memory Broyden Fletcher Goldfarb Shanno) is a numeric optimization method that has been effectively used for parameter estimation to train various machine learning models. As the number of parameters increase, implementing this algorithm on one single machine can be insufficient, due to the limited number of computational resources available. In this paper, we present a parallelized implementation of the L-BFGS algorithm on a distributed system which includes a cluster of commodity computing machines. We use open source HPCC Systems (High-Performance Computing Cluster) platform as the underlying distributed system to implement the L-BFGS algorithm. We initially provide an overview of the HPCC Systems framework and how it allows for the parallel and distributed computations important for Big Data analytics and, subsequently, we explain our implementation of the L-BFGS algorithm on this platform. Our experimental results show that our large-scale implementation of the L-BFGS algorithm can easily scale from training models with millions of parameters to models with billions of parameters by simply increasing the number of commodity computational nodes. © 2017, The Author(s).","HPCC systems; Large-scale L-BFGS implementation; Parallel and distributed processing",,2-s2.0-85024831970
"Li C., Rubín De Celis Leal D., Rana S., Gupta S., Sutti A., Greenhill S., Slezak T., Height M., Venkatesh S.","Rapid Bayesian optimisation for synthesis of short polymer fiber materials",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025147910&doi=10.1038%2fs41598-017-05723-0&partnerID=40&md5=8a384914dcb8d17ff65bcc22188540ac","The discovery of processes for the synthesis of new materials involves many decisions about process design, operation, and material properties. Experimentation is crucial but as complexity increases, exploration of variables can become impractical using traditional combinatorial approaches. We describe an iterative method which uses machine learning to optimise process development, incorporating multiple qualitative and quantitative objectives. We demonstrate the method with a novel fluid processing platform for synthesis of short polymer fibers, and show how the synthesis process can be efficiently directed to achieve material and process objectives. © 2017 The Author(s).",,,2-s2.0-85025147910
"Holopainen M., Sarlin P.","Toward robust early-warning models: a horse race, ensembles and model uncertainty",2017,"Quantitative Finance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029709696&doi=10.1080%2f14697688.2017.1357972&partnerID=40&md5=6301896dc6e04a362a15cca3c668cf94","This paper presents first steps towards robust models for crisis prediction. We conduct a horse race of conventional statistical methods and more recent machine learning methods as early-warning models. As individual models are in the literature most often built in isolation of other methods, the exercise is of high relevance for assessing the relative performance of a wide variety of methods. Further, we test various ensemble approaches to aggregating the information products of the built models, providing more robust basis for measuring country-level vulnerabilities. Finally, we provide approaches to estimating model uncertainty in early-warning exercises, particularly model performance uncertainty and model output uncertainty. The approaches put forward in this paper are shown with Europe as a playground. Generally, our results show that the conventional statistical approaches are outperformed by more advanced machine learning methods, such as k-nearest neighbours and neural networks, and particularly by model aggregation approaches through ensemble learning. © 2017 Informa UK Limited, trading as Taylor & Francis Group","Early-warning models; Ensembles; Financial stability; Horse race; Model uncertainty",,2-s2.0-85029709696
"Eichinger P., Alberts E., Delbridge C., Trebeschi S., Valentinitsch A., Bette S., Huber T., Gempt J., Meyer B., Schlegel J., Zimmer C., Kirschke J.S., Menze B.H., Wiestler B.","Diffusion tensor image features predict IDH genotype in newly diagnosed WHO grade II/III gliomas",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031778790&doi=10.1038%2fs41598-017-13679-4&partnerID=40&md5=ead1b0a32306398750aa44b86467bb82","We hypothesized that machine learning analysis based on texture information from the preoperative MRI can predict IDH mutational status in newly diagnosed WHO grade II and III gliomas. This retrospective study included in total 79 consecutive patients with a newly diagnosed WHO grade II or III glioma. Local binary pattern texture features were generated from preoperative B0 and fractional anisotropy (FA) diffusion tensor imaging. Using a training set of 59 patients, a single hidden layer neural network was then trained on the texture features to predict IDH status. The model was validated based on the prediction accuracy calculated in a previously unseen set of 20 gliomas. Prediction accuracy of the generated model was 92% (54/59 cases; AUC = 0.921) in the training and 95% (19/20; AUC = 0.952) in the validation cohort. The ten most important features were comprised of tumor size and both B0 and FA texture information, underlining the joint contribution of imaging data to classification. Machine learning analysis of DTI texture information and tumor size reliably predicts IDH status in preoperative MRI of gliomas. Such information may increasingly support individualized surgical strategies, supplement pathological analysis and highlight the potential of radiogenomics. © 2017 The Author(s).",,,2-s2.0-85031778790
"Chen S., Harmon S., Perk T., Li X., Chen M., Li Y., Jeraj R.","Diagnostic classification of solitary pulmonary nodules using dual time 18F-FDG PET/CT image texture features in granuloma-endemic regions",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028328736&doi=10.1038%2fs41598-017-08764-7&partnerID=40&md5=2649c985b8b7f208955a854b2aac7075","Lung cancer, the most commonly diagnosed cancer worldwide, usually presents as solid pulmonary nodules (SPNs) on early diagnostic images. Classification of malignant disease at this early timepoint is critical for improving the success of surgical resection and increasing 5-year survival rates. 18F-fluorodeoxyglucose (18F-FDG) PET/CT has demonstrated value for SPNs diagnosis with high sensitivity to detect malignant SPNs, but lower specificity in diagnosing malignant SPNs in populations with endemic infectious lung disease. This study aimed to determine whether quantitative heterogeneity derived from various texture features on dual time FDG PET/CT images (DTPI) can differentiate between malignant and benign SPNs in patients from granuloma-endemic regions. Machine learning methods were employed to find optimal discrimination between malignant and benign nodules. Machine learning models trained by texture features on DTPI images achieved significant improvements over standard clinical metrics and visual interpretation for discriminating benign from malignant SPNs, especially by texture features on delayed FDG PET/CT images. © 2017 The Author(s).",,,2-s2.0-85028328736
"Schoening T., Jones D.O.B., Greinert J.","Compact-Morphology-based poly-metallic Nodule Delineation",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031769823&doi=10.1038%2fs41598-017-13335-x&partnerID=40&md5=22b55025d30a4043a8f7ff157ba53c36","Poly-metallic nodules are a marine resource considered for deep sea mining. Assessing nodule abundance is of interest for mining companies and to monitor potential environmental impact. Optical seafloor imaging allows quantifying poly-metallic nodule abundance at spatial scales from centimetres to square kilometres. Towed cameras and diving robots acquire high-resolution imagery that allow detecting individual nodules and measure their sizes. Spatial abundance statistics can be computed from these size measurements, providing e.g. seafloor coverage in percent and the nodule size distribution. Detecting nodules requires segmentation of nodule pixels from pixels showing sediment background. Semi-supervised pattern recognition has been proposed to automate this task. Existing nodule segmentation algorithms employ machine learning that trains a classifier to segment the nodules in a high-dimensional feature space. Here, a rapid nodule segmentation algorithm is presented. It omits computation-intense feature-based classification and employs image processing only. It exploits a nodule compactness heuristic to delineate individual nodules. Complex machine learning methods are avoided to keep the algorithm simple and fast. The algorithm has successfully been applied to different image datasets. These data sets were acquired by different cameras, camera platforms and in varying illumination conditions. Their successful analysis shows the broad applicability of the proposed method. © 2017 The Author(s).",,"classification; classifier; diving; human; human experiment; illumination; image processing; imagery; morphology; pattern recognition; robotics; sediment; statistics",2-s2.0-85031769823
"Dagan-Wiener A., Nissim I., Ben Abu N., Borgonovo G., Bassoli A., Niv M.Y.","Bitter or not? BitterPredict, a tool for predicting taste from chemical structure",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029893239&doi=10.1038%2fs41598-017-12359-7&partnerID=40&md5=ec5ddcf476afae898884e8b9ef77b267","Bitter taste is an innately aversive taste modality that is considered to protect animals from consuming toxic compounds. Yet, bitterness is not always noxious and some bitter compounds have beneficial effects on health. Hundreds of bitter compounds were reported (and are accessible via the BitterDB http://bitterdb.agri.huji.ac.il/dbbitter.php), but numerous additional bitter molecules are still unknown. The dramatic chemical diversity of bitterants makes bitterness prediction a difficult task. Here we present a machine learning classifier, BitterPredict, which predicts whether a compound is bitter or not, based on its chemical structure. BitterDB was used as the positive set, and non-bitter molecules were gathered from literature to create the negative set. Adaptive Boosting (AdaBoost), based on decision trees machine-learning algorithm was applied to molecules that were represented using physicochemical and ADME/Tox descriptors. BitterPredict correctly classifies over 80% of the compounds in the hold-out test set, and 70-90% of the compounds in three independent external sets and in sensory test validation, providing a quick and reliable tool for classifying large sets of compounds into bitter and non-bitter groups. BitterPredict suggests that about 40% of random molecules, and a large portion (66%) of clinical and experimental drugs, and of natural products (77%) are bitter. © 2017 The Author(s).",,,2-s2.0-85029893239
"Jeon S., Song H.B., Kim J., Lee B.J., Managuli R., Kim J.H., Kim J.H., Kim C.","In Vivo Photoacoustic Imaging of Anterior Ocular Vasculature: A Random Sample Consensus Approach",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021642875&doi=10.1038%2fs41598-017-04334-z&partnerID=40&md5=2f1da93d9c52f4bd558b39feb7d9f806","Visualizing ocular vasculature is important in clinical ophthalmology because ocular circulation abnormalities are early signs of ocular diseases. Photoacoustic microscopy (PAM) images the ocular vasculature without using exogenous contrast agents, avoiding associated side effects. Moreover, 3D PAM images can be useful in understanding vessel-related eye disease. However, the complex structure of the multi-layered vessels still present challenges in evaluating ocular vasculature. In this study, we demonstrate a new method to evaluate blood circulation in the eye by combining in vivo PAM imaging and an ocular surface estimation method based on a machine learning algorithm: a random sample consensus algorithm. By using the developed estimation method, we were able to visualize the PA ocular vascular image intuitively and demonstrate layer-by-layer analysis of injured ocular vasculature. We believe that our method can provide more accurate evaluations of the eye circulation in ophthalmic applications. © The Author(s) 2017.",,,2-s2.0-85021642875
"Risse M., Ohl L.","Using dynamic model averaging in state space representation with dynamic Occam's window and applications to the stock and gold market",2017,"Journal of Empirical Finance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032035181&doi=10.1016%2fj.jempfin.2017.09.005&partnerID=40&md5=7d5f4c888a2a0776918a0d6ac1cb6fed","We combine the Onorante and Raftery (2016) dynamic Occam's window approach with the Raftery et al. (2010) DMA/DMS estimator in state space representation to create forecasts using a data-rich forecasting environment. Our approach is mainly related to economic and financial time series that are subject to periods of high volatility, which increases the necessity of a time varying parameter framework. In a forecasting exercise for the stock and gold markets, we highlight the economic value-added of our approach by applying a simple trading rule to the return series. By combining both assets, we show that our approach performs better when compared to alternative forecasting models such as machine learning algorithms and standard DMA/DMS. Results for the complexity of the forecasting models highlight the advantages of high dimensional forecasting approaches in times of economic uncertainty, such as the recent financial crisis. The economic performance of the trading rule weakens when we consider transaction costs. © 2017 Elsevier B.V.","Dynamic model averaging; Dynamic occam's window; Forecasting; State space representation; Trading rule",,2-s2.0-85032035181
"Klein C., Hiestand T., Ghadri J.-R., Templin C., Jäncke L., Hänggi J.","Takotsubo Syndrome - Predictable from brain imaging data",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024383821&doi=10.1038%2fs41598-017-05592-7&partnerID=40&md5=208cec21381f9193fb68cf933d4a2972","Takotsubo syndrome (TTS) is characterized by acute left ventricular dysfunction, with a hospital-mortality rate similar to acute coronary syndrome (ACS). However, the aetiology of TTS is still unknown. In the present study, a multivariate pattern analysis using machine learning with multimodal magnetic resonance imaging (MRI) data of the human brain of TTS patients and age- and gender-matched healthy control subjects was performed. We found consistent structural and functional alterations in TTS patients compared to the control group. In particular, anatomical and neurophysiological measures from brain regions constituting the emotional-autonomic control system contributed to a prediction accuracy of more than 82%. Thus, our findings demonstrate homogeneous neuronal alterations in TTS patients and substantiate the importance of the concept of a brain-heart interaction in TTS. © 2017 The Author(s).",,,2-s2.0-85024383821
"Li Y., Li T., Liu H.","Recent advances in feature selection and its applications",2017,"Knowledge and Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018783981&doi=10.1007%2fs10115-017-1059-8&partnerID=40&md5=b93a63a04972d57cd3ee24ebad4241cc","Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval. © 2017, Springer-Verlag London.","Data mining; Feature selection; Survey",,2-s2.0-85018783981
"Yousefi S., Amrollahi F., Amgad M., Dong C., Lewis J.E., Song C., Gutman D.A., Halani S.H., Vega J.E.V., Brat D.J., Cooper L.A.D.","Predicting clinical outcomes from large scale cancer genomic profiles with deep survival models",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029478351&doi=10.1038%2fs41598-017-11817-6&partnerID=40&md5=f8a07bb7c34888787646a243ab86efcd","Translating the vast data generated by genomic platforms into accurate predictions of clinical outcomes is a fundamental challenge in genomic medicine. Many prediction methods face limitations in learning from the high-dimensional profiles generated by these platforms, and rely on experts to hand-select a small number of features for training prediction models. In this paper, we demonstrate how deep learning and Bayesian optimization methods that have been remarkably successful in general high-dimensional prediction tasks can be adapted to the problem of predicting cancer outcomes. We perform an extensive comparison of Bayesian optimized deep survival models and other state of the art machine learning methods for survival analysis, and describe a framework for interpreting deep survival models using a risk backpropagation technique. Finally, we illustrate that deep survival models can successfully transfer information across diseases to improve prognostic accuracy. We provide an open-source software implementation of this framework called SurvivalNet that enables automatic training, evaluation and interpretation of deep survival models. © 2017 The Author(s).",,,2-s2.0-85029478351
"Ye Y., Zhang R., Zheng W., Liu S., Zhou F.","RIFS: A randomly restarted incremental feature selection algorithm",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031292096&doi=10.1038%2fs41598-017-13259-6&partnerID=40&md5=f7ccaabceeb5bb5e8803b85b787e8a71","The advent of big data era has imposed both running time and learning efficiency challenges for the machine learning researchers. Biomedical OMIC research is one of these big data areas and has changed the biomedical research drastically. But the high cost of data production and difficulty in participant recruitment introduce the paradigm of ""large p small n"" into the biomedical research. Feature selection is usually employed to reduce the high number of biomedical features, so that a stable data-independent classification or regression model may be achieved. This study randomly changes the first element of the widely-used incremental feature selection (IFS) strategy and selects the best feature subset that may be ranked low by the statistical association evaluation algorithms, e.g. t-test. The hypothesis is that two low-ranked features may be orchestrated to achieve a good classification performance. The proposed Randomly re-started Incremental Feature Selection (RIFS) algorithm demonstrates both higher classification accuracy and smaller feature number than the existing algorithms. RIFS also outperforms the existing methylomic diagnosis model for the prostate malignancy with a larger accuracy and a lower number of transcriptomic features. © 2017 The Author(s).",,,2-s2.0-85031292096
"Taneja I., Reddy B., Damhorst G., Dave Zhao S., Hassan U., Price Z., Jensen T., Ghonge T., Patel M., Wachspress S., Winter J., Rappleye M., Smith G., Healey R., Ajmal M., Khan M., Patel J., Rawal H., Sarwar R., Soni S., Anwaruddin S., Davis B., Kumar J., White K., Bashir R., Zhu R.","Combining Biomarkers with EMR Data to Identify Patients in Different Phases of Sepsis",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029125816&doi=10.1038%2fs41598-017-09766-1&partnerID=40&md5=50e4de9fb0651d395fc0a3772a505231","Sepsis is a leading cause of death and is the most expensive condition to treat in U.S. hospitals. Despite targeted efforts to automate earlier detection of sepsis, current techniques rely exclusively on using either standard clinical data or novel biomarker measurements. In this study, we apply machine learning techniques to assess the predictive power of combining multiple biomarker measurements from a single blood sample with electronic medical record data (EMR) for the identification of patients in the early to peak phase of sepsis in a large community hospital setting. Combining biomarkers and EMR data achieved an area under the receiver operating characteristic (ROC) curve (AUC) of 0.81, while EMR data alone achieved an AUC of 0.75. Furthermore, a single measurement of six biomarkers (IL-6, nCD64, IL-1ra, PCT, MCP1, and G-CSF) yielded the same predictive power as collecting an additional 16 hours of EMR data(AUC of 0.80), suggesting that the biomarkers may be useful for identifying these patients earlier. Ultimately, supervised learning using a subset of biomarker and EMR data as features may be capable of identifying patients in the early to peak phase of sepsis in a diverse population and may provide a tool for more timely identification and intervention. © 2017 The Author(s).",,,2-s2.0-85029125816
"Zhang Q., Li J., Wang D., Wang Y.","Finding disagreement pathway signatures and constructing an ensemble model for cancer classification",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028564706&doi=10.1038%2fs41598-017-10258-5&partnerID=40&md5=462bb1a068fba484c25b2828a7e21d12","Cancer classification based on molecular level is a relatively routine research procedure with advances in high-throughput molecular profiling techniques. However, the number of genes typically far exceeds the number of the sample size in gene expression studies. The existing gene selection methods are almost based on statistics and machine learning, overlooking relevant biological principles or knowledge while working with biological data. Here, we propose a robust ensemble learning paradigm, which incorporates multiple pathways information, to predict cancer classification. We compare the proposed method with other methods, such as Elastic SCAD and PPDMF, and estimate the classification performance. The results show that the proposed method has the higher performances on most metrics and robust performance. We further investigate the biological mechanism of the ensemble feature genes. The results demonstrate that the ensemble feature genes are associated with drug targets/clinically-relevant cancer. In addition, some core biological pathways and biological process underlying clinically-relevant phenotypes are identified by function annotation. Overall, our research can provide a new perspective for the further study of molecular activities and manifestations of cancer. © 2017 The Author(s).",,,2-s2.0-85028564706
"Smith A., López-Solà M., McMahon K., Pedler A., Sterling M.","Multivariate pattern analysis utilizing structural or functional MRI—In individuals with musculoskeletal pain and healthy controls: A systematic review",2017,"Seminars in Arthritis and Rheumatism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024406356&doi=10.1016%2fj.semarthrit.2017.06.005&partnerID=40&md5=5e1285d522f9ab9e7f16e18862563e93","Objective The purpose of this systematic review is to systematically review the evidence relating to findings generated by multivariate pattern analysis (MVPA) following structural or functional magnetic resonance imaging (fMRI) to determine if this analysis is able to: a) Discriminate between individuals with musculoskeletal pain and healthy controls, b) Predict pain perception in healthy individuals stimulated with a noxious stimulus compared to those stimulated with a non-noxious stimulus. Methods MEDLINE, CINAHL, Embase, PEDro, Google Scholar, Cochrane library and Web of Science were systematically screened for relevant literature using different combinations of keywords regarding structural and functional MRI analysed with MVPA, both in individuals with musculoskeletal pain and healthy controls. Reference lists of included articles were hand-searched for additional literature. Eligible articles were assessed on risk of bias and reviewed by two independent researchers. Results The search query returned 18 articles meeting the inclusion criteria. Methodological quality varied from poor to good. Seven studies investigated the ability of machine-learning algorithms to differentiate patient groups from healthy control participants. Overall, the review demonstrated that MVPA can discriminate between individuals with MSK pain and healthy controls with an overall accuracy ranging from 53% to 94%. Twelve studies utilized healthy control participants (using them as their own controls), during experimental pain paradigms aimed to investigate the ability of machine-learning to differentiate individuals stimulated with noxious stimuli from those stimulated with non-noxious stimuli, with ‘pain’ detection rates ranging from 60% to 94%. However, significant heterogeneity in patient conditions, study methodology and brain imaging techniques resulted in various findings that make study comparisons and formal conclusions challenging. Conclusion There is preliminary and emerging evidence that MVPA analyses of structural or functional MRI are able to discriminate between patients and healthy controls, and also discriminate between noxious and non-noxious stimulation. No prospective studies were found in this review to allow determination of the prognostic or diagnostic capabilities or treatment responsiveness of these analyses. Future studies would also benefit from combining various behavioural, genotype and phenotype data into analyses to assist with development of sensitive and specific signatures that could guide future individualized patient treatment options and evaluate how treatments exert their effects. © 2017 Elsevier Inc.","Functional MRI; Multivariate pattern analysis; Musculoskeletal pain; Systematic review",,2-s2.0-85024406356
"Yang Y., Luo T., Li Z., Zhang X., Yu P.S.","A Robust Method for Inferring Network Structures",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024107139&doi=10.1038%2fs41598-017-04725-2&partnerID=40&md5=52f471e9e4b3d358ef8153c4eef9e34f","Inferring the network structure from limited observable data is significant in molecular biology, communication and many other areas. It is challenging, primarily because the observable data are sparse, finite and noisy. The development of machine learning and network structure study provides a great chance to solve the problem. In this paper, we propose an iterative smoothing algorithm with structure sparsity (ISSS) method. The elastic penalty in the model is introduced for the sparse solution, identifying group features and avoiding over-fitting, and the total variation (TV) penalty in the model can effectively utilize the structure information to identify the neighborhood of the vertices. Due to the non-smoothness of the elastic and structural TV penalties, an efficient algorithm with the Nesterov's smoothing optimization technique is proposed to solve the non-smooth problem. The experimental results on both synthetic and real-world networks show that the proposed model is robust against insufficient data and high noise. In addition, we investigate many factors that play important roles in identifying the performance of ISSS. © 2017 The Author(s).",,"experimental model; neighborhood; noise; punishment",2-s2.0-85024107139
"Shiba K., Tamura R., Imamura G., Yoshikawa G.","Data-driven nanomechanical sensing: Specific information extraction from a complex system",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020863376&doi=10.1038%2fs41598-017-03875-7&partnerID=40&md5=20911257e6fd4be6e575b12f649e4114","Smells are known to be composed of thousands of chemicals with various concentrations, and thus, the extraction of specific information from such a complex system is still challenging. Herein, we report for the first time that the nanomechanical sensing combined with machine learning realizes the specific information extraction, e.g. alcohol content quantification as a proof-of-concept, from the smells of liquors. A newly developed nanomechanical sensor platform, a Membrane-type Surface stress Sensor (MSS), was utilized. Each MSS channel was coated with functional nanoparticles, covering diverse analytes. The smells of 35 liquid samples including water, teas, liquors, and water/EtOH mixtures were measured using the functionalized MSS array. We selected characteristic features from the measured responses and kernel ridge regression was used to predict the alcohol content of the samples, resulting in successful alcohol content quantification. Moreover, the present approach provided a guideline to improve the quantification accuracy; hydrophobic coating materials worked more effectively than hydrophilic ones. On the basis of the guideline, we experimentally demonstrated that additional materials, such as hydrophobic polymers, led to much better prediction accuracy. The applicability of this data-driven nanomechanical sensing is not limited to the alcohol content quantification but to various fields including food, security, environment, and medicine. © 2017 The Author(s).",,,2-s2.0-85020863376
"Jacobs C.B., Maksov A.B., Muckley E.S., Collins L., Mahjouri-Samani M., Ievlev A., Rouleau C.M., Moon J.-W., Graham D.E., Sumpter B.G., Ivanov I.N.","UV-activated ZnO films on a flexible substrate for room temperature O2 and H2O sensing",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025452921&doi=10.1038%2fs41598-017-05265-5&partnerID=40&md5=9f78441d2a002db65ef5c1a7dc5b1646","We demonstrate that UV-light activation of polycrystalline ZnO films on flexible polyimide (Kapton) substrates can be used to detect and differentiate between environmental changes in oxygen and water vapor. The in-plane resistive and impedance properties of ZnO films, fabricated from bacteria-derived ZnS nanoparticles, exhibit unique resistive and capacitive responses to changes in O2 and H2O. We propose that the distinctive responses to O2 and H2O adsorption on ZnO could be utilized to statistically discriminate between the two analytes. Molecular dynamic simulations (MD) of O2 and H2O adsorption energy on ZnO surfaces were performed using the large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) with a reactive force-field (ReaxFF). These simulations suggest that the adsorption mechanisms differ for O2 and H2O adsorption on ZnO, and are governed by the surface termination and the extent of surface hydroxylation. Electrical response measurements, using DC resistance, AC impedance spectroscopy, and Kelvin Probe Force Microscopy (KPFM), demonstrate differences in response to O2 and H2O, confirming that different adsorption mechanisms are involved. Statistical and machine learning approaches were applied to demonstrate that by integrating the electrical and kinetic responses the flexible ZnO sensor can be used for detection and discrimination between O2 and H2O at low temperature. © 2017 The Author(s).",,,2-s2.0-85025452921
"Serrano J.I., Romero J.P., Castillo M.D.D., Rocon E., Louis E.D., Benito-León J.","A data mining approach using cortical thickness for diagnosis and characterization of essential tremor /692/53/2421 /692/617/375/346 /129 /141 /123 article",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019849163&doi=10.1038%2fs41598-017-02122-3&partnerID=40&md5=32a165260e081acb53382bdf6bf9016e","Essential tremor (ET) is one of the most prevalent movement disorders. Being that it is a common disorder, its diagnosis is considered routine. However, misdiagnoses may occur regularly. Over the past decade, several studies have identified brain morphometric changes in ET, but these changes remain poorly understood. Here, we tested the informativeness of measuring cortical thickness for the purposes of ET diagnosis, applying feature selection and machine learning methods to a study sample of 18 patients with ET and 18 age- and sex-matched healthy control subjects. We found that cortical thickness features alone distinguished the two, ET from controls, with 81% diagnostic accuracy. More specifically, roughness (i.e., the standard deviation of cortical thickness) of the right inferior parietal and right fusiform areas was shown to play a key role in ET characterization. Moreover, these features allowed us to identify subgroups of ET patients as well as healthy subjects at risk for ET. Since treatment of tremors is disease specific, accurate and early diagnosis plays an important role in tremor management. Supporting the clinical diagnosis with novel computer approaches based on the objective evaluation of neuroimage data, like the one presented here, may represent a significant step in this direction. © 2017 The Author(s).",,,2-s2.0-85019849163
"Xie L., He S., Wen Y., Bo X., Zhang Z.","Discovery of novel therapeutic properties of drugs from transcriptional responses based on multi-label classification",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026812434&doi=10.1038%2fs41598-017-07705-8&partnerID=40&md5=6edf56dc551f971db0201f1ba56660e3","Drug repositioning strategies have improved substantially in recent years. At present, two advances are poised to facilitate new strategies. First, the LINCS project can provide rich transcriptome data that reflect the responses of cells upon exposure to various drugs. Second, machine learning algorithms have been applied successfully in biomedical research. In this paper, we developed a systematic method to discover novel indications for existing drugs by approaching drug repositioning as a multi-label classification task and used a Softmax regression model to predict previously unrecognized therapeutic properties of drugs based on LINCS transcriptome data. This approach to complete the said task has not been achieved in previous studies. By performing in silico comparison, we demonstrated that the proposed Softmax method showed markedly superior performance over those of other methods. Once fully trained, the method showed a training accuracy exceeding 80% and a validation accuracy of approximately 70%. We generated a highly credible set of 98 drugs with high potential to be repositioned for novel therapeutic purposes. Our case studies included zonisamide and brinzolamide, which were originally developed to treat indications of the nervous system and sensory organs, respectively. Both drugs were repurposed to the cardiovascular category. © 2017 The Author(s).",,,2-s2.0-85026812434
"Estevez J., Chen V.L., Podlaha O., Li B., Le A., Vutien P., Chang E.T., Rosenberg-Hasson Y., Jiang Z., Pflanz S., Ge D., Gaggar A., Nguyen M.H.","Differential Serum Cytokine Profiles in Patients with Chronic Hepatitis B, C, and Hepatocellular Carcinoma",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029876309&doi=10.1038%2fs41598-017-11975-7&partnerID=40&md5=287c77346787ecb8c6a93c6b776dbc7b","Cytokines play an important role in the pathogenesis of cirrhosis and hepatocellular carcinoma (HCC), most cases of which are related to either hepatitis B virus (HBV) or hepatitis C virus (HCV). Prior studies have examined differences in individual cytokine levels in patients with chronic liver disease, but comprehensive cytokine profiling data across different clinical characteristics are lacking. We examined serum cytokine profiles of 411 patients with HCC (n = 102: 32% HBV, 54% HCV, 14% non-viral) and without HCC (n = 309: 39% HBV, 39% HCV, 22% non-viral). Multiplex analysis (Luminex 200 IS) was used to measure serum levels of 51 common cytokines. Random forest machine learning was used to obtain receiver operator characteristic curves and to determine individual cytokine importance using Z scores of mean fluorescence intensity for individual cytokines. Among HCC and non-HCC patients, cytokine profiles differed between HBV and HCV patients (area under curve (AUC) 0.82 for HCC, 0.90 for non-HCC). Cytokine profiles did not distinguish cirrhotic HBV patients with and without HCC (AUC 0.503) or HCV patients with and without HCC (AUC 0.63). In conclusion, patients with HBV or HCV infection, with or without HCC, have distinctly different cytokine profiles, suggesting potential differences in disease pathogenesis and/or disease characteristics. © 2017 The Author(s).",,,2-s2.0-85029876309
"Thibodeau A., Márquez E.J., Shin D.-G., Vera-Licona P., Ucar D.","Chromatin interaction networks revealed unique connectivity patterns of broad H3K4me3 domains and super enhancers in 3D chromatin",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032637695&doi=10.1038%2fs41598-017-14389-7&partnerID=40&md5=8a98abadd544a4bc4124e12e9ac72ad7","Broad domain promoters and super enhancers are regulatory elements that govern cell-specific functions and harbor disease-associated sequence variants. These elements are characterized by distinct epigenomic profiles, such as expanded deposition of histone marks H3K27ac for super enhancers and H3K4me3 for broad domains, however little is known about how they interact with each other and the rest of the genome in three-dimensional chromatin space. Using network theory methods, we studied chromatin interactions between broad domains and super enhancers in three ENCODE cell lines (K562, MCF7, GM12878) obtained via ChIA-PET, Hi-C, and Hi-CHIP assays. In these networks, broad domains and super enhancers interact more frequently with each other compared to their typical counterparts. Network measures and graphlets revealed distinct connectivity patterns associated with these regulatory elements that are robust across cell types and alternative assays. Machine learning models showed that these connectivity patterns could effectively discriminate broad domains from typical promoters and super enhancers from typical enhancers. Finally, targets of broad domains in these networks were enriched in disease-causing SNPs of cognate cell types. Taken together these results suggest a robust and unique organization of the chromatin around broad domains and super enhancers: loci critical for pathologies and cell-specific functions. © 2017 The Author(s).",,,2-s2.0-85032637695
"Heredia B., Khoshgoftaar T.M., Prusa J.D., Crawford M.","Improving detection of untrustworthy online reviews using ensemble learners combined with feature selection",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026891026&doi=10.1007%2fs13278-017-0456-z&partnerID=40&md5=af0a06fa7e3a2a8b831b921accb2b39b","As fake reviews become more prominent on the web, a method to differentiate between untruthful and truthful reviews becomes increasingly necessary. However, detection of false reviews may be difficult, as determining the validity of a review based solely on text can be nearly impossible for a human. In this study, we aim to determine the effectiveness of machine learning techniques, specifically ensemble techniques and the combination of feature selection and ensemble techniques, for the detection of spam reviews. In addition to traditional ensemble techniques, such as Boosting and Bagging, we employ techniques that combine ensemble methods with a form of feature selection: Select-Boost, Select-Bagging and Random Forest. For Select-Boost and Select-Bagging, we combine the Boosting and Bagging approaches with three different feature rankers. Random Forest was performed using 100, 250, and 500 trees. Our results show a combination of Select-Boost, multinomial naïve Bayes and, either Chi-squared or signal-to-noise, significantly outperforms all methods except Random Forest using 500 trees. There is no significant difference between the feature subset sizes tested when using Select-Boost with multinomial naïve Bayes, regardless of the feature selection technique employed. To the best of our knowledge, this is the first study to examine the effect of a combination of ensemble techniques and feature selection in the domain of spam review detection. © 2017, Springer-Verlag GmbH Austria.","Ensemble; Feature selection; Random Forest; Select-Bagging; Select-Boost; Spam review",,2-s2.0-85026891026
"Takagi Y., Sakai Y., Lisi G., Yahata N., Abe Y., Nishida S., Nakamae T., Morimoto J., Kawato M., Narumoto J., Tanaka S.C.","A neural marker of obsessive-compulsive disorder from whole-brain functional connectivity",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027098844&doi=10.1038%2fs41598-017-07792-7&partnerID=40&md5=00c708b74b6d6c4e7fc749d771e482bc","Obsessive-compulsive disorder (OCD) is a common psychiatric disorder with a lifetime prevalence of 2-3%. Recently, brain activity in the resting state is gathering attention for exploring altered functional connectivity in psychiatric disorders. Although previous resting-state functional magnetic resonance imaging studies investigated the neurobiological abnormalities of patients with OCD, there are concerns that should be addressed. One concern is the validity of the hypothesis employed. Most studies used seed-based analysis of the fronto-striatal circuit, despite the potential for abnormalities in other regions. A hypothesis-free study is a promising approach in such a case, while it requires researchers to handle a dataset with large dimensions. Another concern is the reliability of biomarkers derived from a single dataset, which may be influenced by cohort-specific features. Here, our machine learning algorithm identified an OCD biomarker that achieves high accuracy for an internal dataset (AUC = 0.81; N = 108) and demonstrates generalizability to an external dataset (AUC = 0.70; N = 28). Our biomarker was unaffected by medication status, and the functional networks contributing to the biomarker were distributed widely, including the frontoparietal and default mode networks. Our biomarker has the potential to deepen our understanding of OCD and to be applied clinically. © 2017 The Author(s).",,,2-s2.0-85027098844
"Chennu A., Färber P., De'ath G., De Beer D., Fabricius K.E.","A diver-operated hyperspectral imaging and topographic surveying system for automated mapping of benthic habitats",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026845357&doi=10.1038%2fs41598-017-07337-y&partnerID=40&md5=5891f32fa0d43034209e6b3171cf35ba","We developed a novel integrated technology for diver-operated surveying of shallow marine ecosystems. The HyperDiver system captures rich multifaceted data in each transect: hyperspectral and color imagery, topographic profiles, incident irradiance and water chemistry at a rate of 15-30 m2 per minute. From surveys in a coral reef following standard diver protocols, we show how the rich optical detail can be leveraged to generate photopigment abundance and benthic composition maps. We applied machine learning techniques, with a minor annotation effort (&lt;2% of pixels), to automatically generate cm-scale benthic habitat maps of high taxonomic resolution and accuracy (93-97%). The ability to efficiently map benthic composition, photopigment densities and rugosity at reef scales is a compelling contribution to modernize reef monitoring. Seafloor-level hyperspectral images can be used for automated mapping, avoiding operator bias in the analysis and deliver the degree of detail necessary for standardized environmental monitoring. The technique can deliver fast, objective and economic reef survey results, making it a valuable tool for coastal managers and reef ecologists. Underwater hyperspectral surveying shares the vantage point of the high spatial and taxonomic resolution restricted to field surveys, with analytical techniques of remote sensing and provides targeted validation for aerial monitoring. © 2017 The Author(s).",,,2-s2.0-85026845357
"Sriwanna K., Boongoen T., Iam-On N.","Graph clustering-based discretization of splitting and merging methods (GraphS and GraphM)",2017,"Human-centric Computing and Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026905920&doi=10.1186%2fs13673-017-0103-8&partnerID=40&md5=bc35d840bcc12ca24b4ed9fa82637083","Discretization plays a major role as a data preprocessing technique used in machine learning and data mining. Recent studies have focused on multivariate discretization that considers relations among attributes. The general goal of this method is to obtain the discrete data, which preserves most of the semantics exhibited by original continuous data. However, many techniques generate the final discrete data that may be less useful with natural groups of data not being maintained. This paper presents a novel graph clustering-based discretization algorithm that encodes different similarity measures into a graph representation of the examined data. The intuition allows more refined data-wise relations to be obtained and used with the effective graph clustering technique based on normalized association to discover nature graphs accurately. The goodness of this approach is empirically demonstrated over 30 standard datasets and 20 imbalanced datasets, compared with 11 well-known discretization algorithms using 4 classifiers. The results suggest the new approach is able to preserve the natural groups and usually achieve the efficiency in terms of classifier performance, and the desired number of intervals than the comparative methods. © 2017, The Author(s).","Data mining; Graph clustering; Multivariate discretization; Normalized association; Normalized cuts",,2-s2.0-85026905920
"Lourenço J., Watkins E.R., Obolski U., Peacock S.J., Morris C., Maiden M.C.J., Gupta S.","Lineage structure of Streptococcus pneumoniae may be driven by immune selection on the groEL heat-shock protein",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028031491&doi=10.1038%2fs41598-017-08990-z&partnerID=40&md5=08f6d7bbe1caad5c9b81d75662a7ef01","Populations of Streptococcus pneumoniae (SP) are typically structured into groups of closely related organisms or lineages, but it is not clear whether they are maintained by selection or neutral processes. Here, we attempt to address this question by applying a machine learning technique to SP whole genomes. Our results indicate that lineages evolved through immune selection on the groEL chaperone protein. The groEL protein is part of the groESL operon and enables a large range of proteins to fold correctly within the physical environment of the nasopharynx, thereby explaining why lineage structure is so stable within SP despite high levels of genetic transfer. SP is also antigenically diverse, exhibiting a variety of distinct capsular serotypes. Associations exist between lineage and capsular serotype but these can be easily perturbed, such as by vaccination. Overall, our analyses indicate that the evolution of SP can be conceptualized as the rearrangement of modular functional units occurring on several different timescales under different pressures: some patterns have locked in early (such as the epistatic interactions between groESL and a constellation of other genes) and preserve the differentiation of lineages, while others (such as the associations between capsular serotype and lineage) remain in continuous flux. © 2017 The Author(s).",,,2-s2.0-85028031491
"Inubushi M., Yoshimura K.","Reservoir Computing beyond Memory-Nonlinearity Trade-off",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028681399&doi=10.1038%2fs41598-017-10257-6&partnerID=40&md5=0459c51c7785849d9174395ee367bda4","Reservoir computing is a brain-inspired machine learning framework that employs a signal-driven dynamical system, in particular harnessing common-signal-induced synchronization which is a widely observed nonlinear phenomenon. Basic understanding of a working principle in reservoir computing can be expected to shed light on how information is stored and processed in nonlinear dynamical systems, potentially leading to progress in a broad range of nonlinear sciences. As a first step toward this goal, from the viewpoint of nonlinear physics and information theory, we study the memory-nonlinearity trade-off uncovered by Dambre et al. (2012). Focusing on a variational equation, we clarify a dynamical mechanism behind the trade-off, which illustrates why nonlinear dynamics degrades memory stored in dynamical system in general. Moreover, based on the trade-off, we propose a mixture reservoir endowed with both linear and nonlinear dynamics and show that it improves the performance of information processing. Interestingly, for some tasks, significant improvements are observed by adding a few linear dynamics to the nonlinear dynamical system. By employing the echo state network model, the effect of the mixture reservoir is numerically verified for a simple function approximation task and for more complex tasks. © 2017 The Author(s).",,"information processing; information science; memory; nonlinear system; physics",2-s2.0-85028681399
"Ilic S., Akabayov S.R., Froimovici R., Meiry R., Vilenchik D., Hernandez A., Arthanari H., Akabayov B.","Modulation of RNA primer formation by Mn(II)-substituted T7 DNA primase",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025640838&doi=10.1038%2fs41598-017-05534-3&partnerID=40&md5=c0c92e1b7d27464106f56638e661ef9f","Lagging strand DNA synthesis by DNA polymerase requires RNA primers produced by DNA primase. The N-terminal primase domain of the gene 4 protein of phage T7 comprises a zinc-binding domain that recognizes a specific DNA sequence and an RNA polymerase domain that catalyzes RNA polymerization. Based on its crystal structure, the RNA polymerase domain contains two Mg(II) ions. Mn(II) substitution leads to elevated RNA primer synthesis by T7 DNA primase. NMR analysis revealed that upon binding Mn(II), T7 DNA primase undergoes conformational changes near the metal cofactor binding site that are not observed when the enzyme binds Mg(II). A machine-learning algorithm called linear discriminant analysis (LDA) was trained by using the large collection of Mn(II) and Mg(II) binding sites available in the protein data bank (PDB). Application of the model to DNA primase revealed a preference in the enzyme's second metal binding site for Mn(II) over Mg(II), suggesting that T7 DNA primase activity modulation when bound to Mn(II) is based on structural changes in the enzyme. © 2017 The Author(s).",,,2-s2.0-85025640838
"Movaghar A., Mailick M., Sterling A., Greenberg J., Saha K.","Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020222380&doi=10.1038%2fs41598-017-02682-4&partnerID=40&md5=4ab4acf19987921467b406d4c443ed1e","Millions of people globally are at high risk for neurodegenerative disorders, infertility or having children with a disability as a result of the Fragile X (FX) premutation, a genetic abnormality in FMR1 that is underdiagnosed. Despite the high prevalence of the FX premutation and its effect on public health and family planning, most FX premutation carriers are unaware of their condition. Since genetic testing for the premutation is resource intensive, it is not practical to screen individuals for FX premutation status using genetic testing. In a novel approach to phenotyping, we have utilized audio recordings and cognitive profiling assessed via self-Administered questionnaires on 200 females. Machine-learning methods were developed to discriminate FX premutation carriers from mothers of children with autism spectrum disorders, the comparison group. By using a random forest classifier, FX premutation carriers could be identified in an automated fashion with high precision and recall (0.81 F1 score). Linguistic and cognitive phenotypes that were highly associated with FX premutation carriers were high language dysfluency, poor ability to organize material, and low self-monitoring. Our framework sets the foundation for computational phenotyping strategies to pre-screen large populations for this genetic variant with nominal costs. © 2017 The Author(s).",,,2-s2.0-85020222380
"Orfanoudaki G., Markaki M., Chatzi K., Tsamardinos I., Economou A.","MatureP: Prediction of secreted proteins with exclusive information from their mature regions",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020748649&doi=10.1038%2fs41598-017-03557-4&partnerID=40&md5=165cce78552526c31f2a5def33a77ded","More than a third of the cellular proteome is non-cytoplasmic. Most secretory proteins use the Sec system for export and are targeted to membranes using signal peptides and mature domains. To specifically analyze bacterial mature domain features, we developed MatureP, a classifier that predicts secretory sequences through features exclusively computed from their mature domains. MatureP was trained using Just Add Data Bio, an automated machine learning tool. Mature domains are predicted efficiently with ∼92% success, as measured by the Area Under the Receiver Operating Characteristic Curve (AUC). Predictions were validated using experimental datasets of mutated secretory proteins. The features selected by MatureP reveal prominent differences in amino acid content between secreted and cytoplasmic proteins. Amino-terminal mature domain sequences have enhanced disorder, more hydroxyl and polar residues and less hydrophobics. Cytoplasmic proteins have prominent amino-terminal hydrophobic stretches and charged regions downstream. Presumably, secretory mature domains comprise a distinct protein class. They balance properties that promote the necessary flexibility required for the maintenance of non-folded states during targeting and secretion with the ability of post-secretion folding. These findings provide novel insight in protein trafficking, sorting and folding mechanisms and may benefit protein secretion biotechnology.",,,2-s2.0-85020748649
"Radiya-Dixit E., Zhu D., Beck A.H.","Automated Classification of Benign and Malignant Proliferative Breast Lesions",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028458648&doi=10.1038%2fs41598-017-10324-y&partnerID=40&md5=4f240b803dd0b98432246f8f806d9f94","Misclassification of breast lesions can result in either cancer progression or unnecessary chemotherapy. Automated classification tools are seen as promising second opinion providers in reducing such errors. We have developed predictive algorithms that automate the categorization of breast lesions as either benign usual ductal hyperplasia (UDH) or malignant ductal carcinoma in situ (DCIS). From diagnosed breast biopsy images from two hospitals, we obtained 392 biomarkers using Dong et al.'s (2014) computational tools for nuclei identification and feature extraction. We implemented six machine learning models and enhanced them by reducing prediction variance, extracting active features, and combining multiple algorithms. We used the area under the curve (AUC) of the receiver operating characteristic (ROC) curve for performance evaluation. Our top-performing model, a Combined model with Active Feature Extraction (CAFE) consisting of two logistic regression algorithms, obtained an AUC of 0.918 when trained on data from one hospital and tested on samples of the other, a statistically significant improvement over Dong et al.'s AUC of 0.858. Pathologists can substantially improve their diagnoses by using it as an unbiased validator. In the future, our work can also serve as a valuable methodology for differentiating between low-grade and high-grade DCIS. © 2017 The Author(s).",,,2-s2.0-85028458648
"Sander E.L., Wootton J.T., Allesina S.","Ecological Network Inference from Long-Term Presence-Absence Data",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026797164&doi=10.1038%2fs41598-017-07009-x&partnerID=40&md5=d07ee19ec62d8332ded48262d5e810c4","Ecological communities are characterized by complex networks of trophic and nontrophic interactions, which shape the dy-namics of the community. Machine learning and correlational methods are increasingly popular for inferring networks from co-occurrence and time series data, particularly in microbial systems. In this study, we test the suitability of these methods for inferring ecological interactions by constructing networks using Dynamic Bayesian Networks, Lasso regression, and Pear-son's correlation coefficient, then comparing the model networks to empirical trophic and nontrophic webs in two ecological systems. We find that although each model significantly replicates the structure of at least one empirical network, no model significantly predicts network structure in both systems, and no model is clearly superior to the others. We also find that networks inferred for the Tatoosh intertidal match the nontrophic network much more closely than the trophic one, possibly due to the challenges of identifying trophic interactions from presence-Absence data. Our findings suggest that although these methods hold some promise for ecological network inference, presence-Absence data does not provide enough signal for models to consistently identify interactions, and networks inferred from these data should be interpreted with caution. © 2017 The Author(s).",,,2-s2.0-85026797164
"Varol O., Ferrara E., Menczer F., Flammini A.","Early detection of promoted campaigns on social media",2017,"EPJ Data Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021772851&doi=10.1140%2fepjds%2fs13688-017-0111-y&partnerID=40&md5=dff6873bb61ba8f6cbe311caddfb3088","Social media expose millions of users every day to information campaigns - some emerging organically from grassroots activity, others sustained by advertising or other coordinated efforts. These campaigns contribute to the shaping of collective opinions. While most information campaigns are benign, some may be deployed for nefarious purposes, including terrorist propaganda, political astroturf, and financial market manipulation. It is therefore important to be able to detect whether a meme is being artificially promoted at the very moment it becomes wildly popular. This problem has important social implications and poses numerous technical challenges. As a first step, here we focus on discriminating between trending memes that are either organic or promoted by means of advertisement. The classification is not trivial: ads cause bursts of attention that can be easily mistaken for those of organic trends. We designed a machine learning framework to classify memes that have been labeled as trending on Twitter. After trending, we can rely on a large volume of activity data. Early detection, occurring immediately at trending time, is a more challenging problem due to the minimal volume of activity data that is available prior to trending. Our supervised learning framework exploits hundreds of time-varying features to capture changing network and diffusion patterns, content and sentiment information, timing signals, and user meta-data. We explore different methods for encoding feature time series. Using millions of tweets containing trending hashtags, we achieve 75% AUC score for early detection, increasing to above 95% after trending. We evaluate the robustness of the algorithms by introducing random temporal shifts on the trend time series. Feature selection analysis reveals that content cues provide consistently useful signals; user features are more informative for early detection, while network and timing features are more helpful once more data is available. © 2017, The Author(s).","advertising; early detection; information campaigns; social media","Education; Marketing; Time series; Diffusion patterns; information campaigns; Market manipulation; Social implication; Social media; Technical challenges; Temporal shifts; Timing signals; Social networking (online)",2-s2.0-85021772851
"Sarker A.","A customizable pipeline for social media text normalization",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029235304&doi=10.1007%2fs13278-017-0464-z&partnerID=40&md5=92c1566b886ea45f4ae3060bec4c16c0","Social networks are persistently generating text-based data that encapsulate vast amounts of knowledge. However, the presence of non-standard terms and misspellings in texts originating from social networks poses a crucial challenge for natural language processing and machine learning systems that attempt to mine this knowledge. To address this problem, we propose a sequential, modular, and hybrid pipeline for social media text normalization. In the first phase, text preprocessing techniques and social media-specific vocabularies gathered from publicly available sources are used to transform, with high precision, out-of-vocabulary terms into in-vocabulary terms. A sequential language model, generated using the partially normalized texts from the first phase, is then utilized to normalize short, high-frequency, ambiguous terms. A supervised learning module is employed to normalize terms based on a manually annotated training corpus. Finally, a tunable, distributed language model-based backoff module at the end of the pipeline enables further customization of the system to specific domains of text. We performed intrinsic evaluations of the system on a publicly available domain-independent dataset from Twitter, and our system obtained an F-score of 0.836, outperforming other benchmark systems for the task. We further performed brief, task-oriented evaluations of the system to illustrate the customizability of the system to domain-specific tasks and the effects of normalization on downstream applications. The modular design enables the easy customization of the system to distinct types domain-specific social media text, in addition to its off-the-shelf application to generic social media text. © 2017, Springer-Verlag GmbH Austria.","Lexical normalization; Natural language processing; Social media data preparation; Social media text normalization; Social network mining; Text mining",,2-s2.0-85029235304
"Arrey-Mbi T.B., Klusewitz S.M., Villines T.C.","Long-Term Prognostic Value of Coronary Computed Tomography Angiography",2017,"Current Treatment Options in Cardiovascular Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031502078&doi=10.1007%2fs11936-017-0588-5&partnerID=40&md5=37e3be5c4e98674d47b35574c6cc2dbe","Coronary CT angiography (CTA) is a highly accurate test for the diagnosis of coronary artery disease (CAD), with its use guided by numerous contemporary appropriate use criteria and clinical guidelines. Unique among non-invasive tests for CAD, coronary CTA provides direct visualization of coronary atherosclerosis for the assessment of angiographic stenosis, as well as validated measures of plaque vulnerability. Long-term studies now clearly demonstrate that the absence of CAD on coronary CTA identifies a patient that is at very low risk for future cardiovascular events. Conversely, the presence, location, and severity of CAD as measured on coronary CTA provide powerful prognostic information that is superior to traditional risk factors and other clinical variables. Observational studies and data obtained from clinical trials suggest that the anatomic information derived from coronary CTA significantly increases the utilization of statins and aspirin. Furthermore, these changes are associated with reductions in the risk for mortality, revascularizations, and incident myocardial infarctions among subjects with coronary atherosclerosis. As a result, current societal consensus statements have attempted to standardize coronary CTA reporting, to include incorporation of vulnerable plaque features and recommendations on the use of preventive therapies, such as statins, so to more consistently link important prognostic findings on coronary CTA to appropriate preventive and therapeutic interventions. Automated measures of total coronary plaque volume, machine learning, and CT-derived fractional flow reserve may further refine the prognostic accuracy of coronary CTA. Herein, we summarize recently published literature that reports the long-term (≥ 5 years of follow-up) prognostic usefulness of coronary CTA. © 2017, US Government (outside the USA).","Atherosclerosis; Cardiac CT; Coronary artery disease; Coronary CT angiography; Prognosis",,2-s2.0-85031502078
"Falconer E., El-Hay T., Alevras D., Docherty J.P., Yanover C., Kalton A., Goldschmidt Y., Rosen-Zvi M.","Integrated multisystem analysis in a mental health and criminal justice ecosystem",2017,"Health and Justice",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016017924&doi=10.1186%2fs40352-017-0049-y&partnerID=40&md5=422a41f138585ed8059c7a40a9e8f956","Background: Patients with a serious mental illness often receive care that is fragmented due to reduced availability of or access to resources, and inadequate, discontinuous, and uncoordinated care across health, social services, and criminal justice organizations. This article describes the creation of a multisystem analysis that derives insights from an integrated dataset including patient access to case management services, medical services, and interactions with the criminal justice system. Methods: Data were combined from electronic systems within a US mental health ecosystem that included mental health and substance abuse services, as well as data from the criminal justice system. Cox models were applied to test the associations between delivery of services and re-incarceration. Additionally, machine learning was used to train and validate a predictive model to examine effects of non-modifiable risk factors (age, past arrests, mental health diagnosis) and modifiable risk factors (outpatient, medical and case management services, and use of a jail diversion program) on re-arrest outcome. Results: An association was found between past arrests and admission to crisis stabilization services in this population (N = 10,307). Delivery of case management or medical services provided after release from jail was associated with a reduced risk for re-arrest. Predictive models linked non-modifiable and modifiable risk factors and outcomes and predicted the probability of re-arrests with fair accuracy (area under the receiver operating characteristic curve of 0.67). Conclusions: By modeling the complex interactions between risk factors, service delivery, and outcomes, systems of care might be better enabled to meet patient needs and improve outcomes. © 2017, The Author(s).","Arrest; Criminal justice system; Healthcare system; Hospitalization; Risk factors; Serious mental illness",,2-s2.0-85016017924
"Delgado S., Higuera C., Calle-Espinosa J., Morán F., Montero F.","A SOM prototype-based cluster analysis methodology",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021673327&doi=10.1016%2fj.eswa.2017.06.022&partnerID=40&md5=dfe784084556c9621294836a6b2aad09","Data clustering is aimed at finding groups of data that share common hidden properties. These kinds of techniques are especially critical at early stages of data analysis where no information about the dataset is available. One of the mayor shortcomings of the clustering algorithms is the difficulty for non-experts users to configure them and, in some cases, interpret the results. In this work a computational approach with a two-layer structure based on Self-Organizing Map (SOM) is presented for cluster analysis. In the first level, a quantization of the data samples using topology-preserving metrics to automatically determine the number of units in the SOM is proposed. In the second level the obtained SOM prototypes are clustered by means of a connectivity analysis to explore the quality of the partitioning with different number of clusters. The most important benefit of this two-layer procedure is that computational load decreases considerably in comparison with data based clustering methods, making it possible to cluster large data sets and to consider several different clustering alternatives in a limited time. This methodology produces a two-dimensional map representation of the, usually, high dimensional input space, along with quantitative information on viable clustering alternatives, which facilitates the exploration of the possible partitions in a dataset. The efficiency and interpretation of the methodology is illustrated by its application to artificial, benchmark and real complex biological datasets. The experimental results demonstrate the ability of the method to identify possible segmentations in a dataset, compared to algorithms that only yield a single clustering solution. The proposed algorithm tackles the intrinsic limitations of SOM and the parameter settings associated with the clustering methodology, without requiring the number of clusters or the SOM architecture as a prerequisite, among others. This way, it makes possible its application even by researchers with a limited expertise in machine learning. © 2017 Elsevier Ltd","Clustering; Metabolic network; Self-organizing map; Topology preserving; Unsupervised","Benchmarking; Cluster analysis; Complex networks; Conformal mapping; Quality control; Self organizing maps; Topology; Clustering; Computational approach; Connectivity analysis; Metabolic network; Quantitative information; Topology preserving; Two-layer structures; Unsupervised; Clustering algorithms",2-s2.0-85021673327
"Li J., Fong S., Wong R.K., Millham R., Wong K.K.L.","Elitist Binary Wolf Search Algorithm for Heuristic Feature Selection in High-Dimensional Bioinformatics Datasets",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021641980&doi=10.1038%2fs41598-017-04037-5&partnerID=40&md5=47c6d637adaa36f00a0b39ecf4605ef9","Due to the high-dimensional characteristics of dataset, we propose a new method based on the Wolf Search Algorithm (WSA) for optimising the feature selection problem. The proposed approach uses the natural strategy established by Charles Darwin; that is, 'It is not the strongest of the species that survives, but the most adaptable'. This means that in the evolution of a swarm, the elitists are motivated to quickly obtain more and better resources. The memory function helps the proposed method to avoid repeat searches for the worst position in order to enhance the effectiveness of the search, while the binary strategy simplifies the feature selection problem into a similar problem of function optimisation. Furthermore, the wrapper strategy gathers these strengthened wolves with the classifier of extreme learning machine to find a sub-dataset with a reasonable number of features that offers the maximum correctness of global classification models. The experimental results from the six public high-dimensional bioinformatics datasets tested demonstrate that the proposed method can best some of the conventional feature selection methods up to 29% in classification accuracy, and outperform previous WSAs by up to 99.81% in computational time. © The Author(s) 2017.",,"bioinformatics; classifier; experimental model; memory; nonhuman; species; wolf",2-s2.0-85021641980
"de León A.D., Lalla-Ruiz E., Melián-Batista B., Marcos Moreno-Vega J.","A Machine Learning-based system for berth scheduling at bulk terminals",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021118168&doi=10.1016%2fj.eswa.2017.06.010&partnerID=40&md5=a94068c72a54c28e72493cb00087d351","The increasing volume of maritime freight is presented as a challenge to those skilled terminal managers seeking to maintain or increase their market share. In this context, an efficient management of scarce resources as berths arises as a reasonable option for reducing costs while enhancing the productivity of the overall terminal. In this work, we tackle the berth scheduling operations by considering the Bulk Berth Allocation Problem (Bulk-BAP). This problem, for a given yard layout and location of the cargo facilities, aims to coordinate the berthing and yard activities for giving service to those vessels arriving at the terminal. Considering the multitude of scenarios arising in this environment and theNo Free Lunch theorem, the drawback concerning the selection of the best algorithm for solving the Bulk-BAP in each particular case is addressed by a Machine Learning-based system. It provides, based on the scenario at hand, a ranking of algorithms sorted by appropriateness. The computational study shows an increase in the quality of the provided solutions when the algorithm to be used is selected according to the features of the instance instead of selecting the best algorithm on average. © 2017 Elsevier Ltd","Berth allocation problem; Bulk transportation; Decision support; Machine-learning; Machine-learning; Meta-learning","Artificial intelligence; Competition; Computation theory; Learning systems; Reservation systems; Scheduling; Service vessels; Transfer cases (vehicles); Algorithm for solving; Berth allocation problem; Computational studies; Decision supports; Efficient managements; Metalearning; Scheduling operations; Terminal managers; Education",2-s2.0-85021118168
"Dantas Dias M.L., Rocha Neto A.R.","Training soft margin support vector machines by simulated annealing: A dual approach",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020868064&doi=10.1016%2fj.eswa.2017.06.016&partnerID=40&md5=7b0271dae6f8497e882e09eb0038ac33","A theoretical advantage of support vector machines (SVM) is the empirical and structural risk minimization which balances the complexity of the model against its success at fitting the training data. Metaheuristics have mostly been used with support vector machines to either tune hyperparameters or to perform feature selection. In this paper, we present a new approach to obtain sparse support vector machines (SVM) based on simulated annealing (SA), named SATE. In our proposal, SA was used to solve the quadratic optimization problem that emerges from support vector machines rather than tune the hyperparameters. We have compared our proposal with sequential minimal optimization (SMO), kernel adatron (KA), a usual QP solver, as well as with recent Particle Swarm Optimization (PSO) and Genetic Algorithms(GA)-based versions. Generally speaking, one can infer that the SATE is equivalent to SMO in terms of accuracy and mean of support vectors and sparser than KA, QP, LPSO, and GA. SATE also has higher accuracies than the GA and PSO-based versions. Moreover, SATE successfully embedded the SVM constraints and provides a competitive classifier while maintaining its simplicity and high sparseness in the solution. © 2017 Elsevier Ltd","Learning methods; Simulated annealing; Support vector machines","Genetic algorithms; Optimization; Particle swarm optimization (PSO); Quadratic programming; Simulated annealing; Vectors; Hyperparameters; Learning methods; Meta heuristics; New approaches; Quadratic optimization problems; Sequential minimal optimization; Soft-margin support vector machines; Structural risk minimization; Support vector machines",2-s2.0-85020868064
"Lin N.-H., Hsu C.-Y., Luo Y., Nagurka M.L., Sung J.-L., Hong C.-Y., Yen C.-W.","Detecting rapid eye movement sleep using a single EEG signal channel",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020870553&doi=10.1016%2fj.eswa.2017.06.017&partnerID=40&md5=37cc60719769a2a76a5ab5dbc04ce0ec","Sleep stage scoring is generally determined in a polysomnographic (PSG) study where technologists use electroencephalogram (EEG), electromyogram (EMG), and electrooculogram (EOG) signals to determine the sleep stages. Such a process is time consuming and labor intensive. To reduce the workload and to improve the sleep stage scoring performance of sleep experts, this paper introduces an intelligent rapid eye movement (REM) sleep detection method that requires only a single EEG channel. The proposed approach distinguishes itself from previous automatic sleep staging methods by introducing two sets of auxiliary features to help resolve the difficulties caused by interpersonal EEG signal differences. In addition to adopting conventional time and frequency domain features, two empirical rules are introduced to enhance REM detection performance based on sleep being a continuous process. The approach was tested with 779,661 epochs obtained from 947 overnight PSG studies. The REM sleep detection results show a kappa coefficient at 0.752, an accuracy level of 0.930, a sensitivity score of 0.814, and a positive predictive value of 0.775. The results also show that the performance of the approach varies with the ratio of REM sleep and the severity of sleep apnea of the subjects. The experimental results also show that it is possible to improve the performance of an automatic sleep staging method by tailoring it to subgroups of persons that have similar sleep architecture and clinical characteristics. © 2017 Elsevier Ltd","Automatic sleep staging; Electroencephalography; Machine learning; Rapid eye movement sleep","Electroencephalography; Electrophysiology; Eye movements; Learning systems; Clinical characteristics; Detection performance; Electro-encephalogram (EEG); Positive predictive values; Rapid eye movement; Scoring performance; Sleep staging; Time and frequency domains; Sleep research",2-s2.0-85020870553
"Costa F.G.D., Duarte F.S.L.G., Vallim R.M.M., Mello R.F.D.","Multidimensional surrogate stability to detect data stream concept drift",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020442466&doi=10.1016%2fj.eswa.2017.06.005&partnerID=40&md5=6cc034b200f078d625dd8c8b3ba7e547","Concept drift detection plays a very important role in the context of data streams. It allows to point out data behavior modifications along time, which are intrinsically associated to the phenomena responsible for producing such sequences of observations. By detecting such modifications, one can better understand those phenomena and take better decisions in different application domains, e.g. stock market, climate change, population growth, etc. Besides several proposals, most of the studies lack in formal guarantees to ensure the concept drift detection. More recently, Vallim and Mello proposed 1DFT (Unidimensional Fourier Transform), an algorithm that detects drifts on unidimensional streams while holding a stability property based on surrogate series. Motivated by their work we here propose the multidimensional surrogate stability concept, which extends their approach to multidimensional data streams. In addition, our approach, named MDFT (Multidimensional Fourier Transform), employs a different and more robust measurement to analyze drifts, which is based on the Shannon's and Von Neumann's Entropies to quantify variations in data spaces. As final contribution, MDFT allows unidimensional streams to be reconstructed in phase spaces so their data dependencies can also be analyzed to take conclusions on concept drifts along time. Experiments considered seven 120,000-observation synthetic data streams. Synthetic data was taken into account as it allows us to define the exact points of change, using the largest Lyapunov exponent, for which our approach should trigger the concept drift events. Experiments compared MDFT against the main algorithms to detect concept drift in the context of Machine Learning (Page-Hinkley Test – PHT, Adaptive Windowing – ADWIN, and Cumulative Sum Control Chart – CUSUM) and Dynamical Systems (Recurrence Quantification Analysis using different measurements – RQA, and Permutation Entropy – PE). Results confirm MDFT outperforms the other algorithms in terms of an average measurement (using the Euclidean distance) based on: the Missed Detection Rate (MDR), the Mean Time for Detection (MTD) and the Mean Time between False Alarms (MTFA). © 2017 Elsevier Ltd","Concept drift; Data streams; Multidimensional; Surrogate data","Adaptive control systems; Climate change; Data communication systems; Dynamical systems; Electronic trading; Entropy; Fourier series; Learning systems; Lyapunov methods; Phase space methods; Stability; Concept drifts; Cumulative sum control charts; Data stream; Largest Lyapunov exponent; Multidimensional; Multidimensional Fourier transform; Recurrence quantification analysis; Surrogate data; Population statistics",2-s2.0-85020442466
"Peng Y., Kong W., Yang B.","Orthogonal extreme learning machine for image classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020099908&doi=10.1016%2fj.neucom.2017.05.058&partnerID=40&md5=34881421e29d9340eaee1c2ccfb5810c","Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks in which the parameters of hidden units are randomly generated and thus the output weights can be analytically calculated. From the hidden to output layer, ELM essentially learns the output weight matrix based on the least squares regression formula that can be used for both classification/regression and dimensionality reduction. In this paper, we impose the orthogonal constraint on the output weight matrix and then formulate an orthogonal extreme learning machine (OELM) model, which produces orthogonal basis functions and can have more locality preserving power from ELM feature space to output layer than ELM. Since the locality preserving ability is potentially related to the discriminating power, the OELM is expect to have more discriminating power than ELM. Considering the case that the number of hidden units is usually greater than the number of classes, we propose an effective method to optimize the OELM objective by solving an orthogonal procrustes problem. Experiments by pairwisely comparing OELM with ELM on three widely used image data sets show the effectiveness of learning orthogonal mapping especially when given only limited training samples. © 2017 Elsevier B.V.","Extreme learning machine; Image classification; Orthogonal constraint; Orthogonal procrustes problem","Feedforward neural networks; Image classification; Knowledge acquisition; Learning algorithms; Learning systems; Matrix algebra; Network layers; Radial basis function networks; Dimensionality reduction; Extreme learning machine; Least squares regression; Locality preserving power; Orthogonal basis function; Orthogonal constraints; Orthogonal Procrustes problem; Single-hidden layer feedforward neural networks; Orthogonal functions; Article; artificial neural network; classification; image processing; information processing; learning algorithm; machine learning; mathematical computing; mathematical model; orthogonal extreme learning machine; pattern recognition; priority journal; process optimization; regression analysis",2-s2.0-85020099908
"Tang J., Tian Y.","A multi-kernel framework with nonparallel support vector machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020207686&doi=10.1016%2fj.neucom.2017.05.036&partnerID=40&md5=6459b78d74a19baa093475c60c3db6a6","Multiple kernel learning (MKL) serves as an attractive research direction in current kernel machine learning field. It can flexibly process diverse characteristics of patterns such as heterogeneous information or irregular data, non-flat distribution of high-dimensional samples etc. The existing MKL models are usually built on SVM. However, there is still potential to improve the performance of MKL instead of learning based on SVM. Nonparallel support vector machine (NPSVM), as a novel classifier, pursues two nonparallel proximal hyperplanes with several incomparable advantages over the state-of-the-art classifiers. In this paper, we propose a new model termed as MKNPSVM for classification. By integrating NPSVM into the MKL framework, MKNPSVM inherits the advantages of them and opens a new perspective to extend NPSVM to the MKL field. To solve MKNPSVM efficiently, we provide an alternating optimization algorithm (Alter-MKNPSVM for short) as the solution. We theoretically analyze the performance of MKNPSVM from three viewpoints: the generalization capability analysis, the convergence analysis and the comparisons with NPSVM and MKL. Experimental results on eighteen publicly available UCI data sets confirm the effectiveness of our method. © 2017 Elsevier B.V.","Alternating optimization; Multiple kernel learning; Nonparallel support vector machine","Learning systems; Optimization; Alternating optimizations; Convergence analysis; Generalization capability; Heterogeneous information; High-dimensional; Kernel machine learning; Multiple Kernel Learning; State of the art; Support vector machines; algorithm; Article; controlled study; kernel method; measurement accuracy; multiple kernel learning; nonparallel support vector machine; priority journal; process optimization; support vector machine; theoretical study; validation process",2-s2.0-85020207686
"Fan J., Chow T.","Deep learning based matrix completion",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020903809&doi=10.1016%2fj.neucom.2017.05.074&partnerID=40&md5=5878a36bf16ed71e6d7ea469534613ea","Previous matrix completion methods are generally based on linear and shallow models where the given incomplete matrices are of low-rank and the data are assumed to be generated by linear latent variable models. In this paper, we first propose a novel method called AutoEncoder based matrix completion (AEMC). The main idea of AEMC is to utilize the partially observed data to learn and construct a nonlinear latent variable model in the form of AutoEncoder. The hidden layer of the AutoEncoder has much fewer units than the visible layers do. Meanwhile, the unknown entries of the data are recovered to fit the nonlinear latent variable model. Based on AEMC, we further propose a deep learning based matrix completion (DLMC) method. In DLMC, AEMC is used as a pre-training step for both the missing entries and network parameters; the hidden layer of AEMC is then used to learn stacked AutoEncoders (SAEs) with greedy layer-wise training; finally, fine-tuning is carried out on the deep network formed by AEMC and SAEs to obtain the missing entries of the data and the parameters of the network. In addition, we also provide out-of-sample extensions for AEMC and DLMC to recover online incomplete data. AEMC and DLMC are compared with state-of-the-art methods in the tasks of synthetic matrix completion, image inpainting, and collaborative filtering. The experimental results verify the effectiveness and superiority of the proposed methods. © 2017 Elsevier B.V.","AutoEncoder; collaborative filtering; deep learning; image inpainting; Matrix completion; out-of-sample extension","Case based reasoning; Collaborative filtering; Education; Image processing; Learning systems; Matrix algebra; Auto encoders; Image Inpainting; Latent variable modeling; Linear latent variables; Matrix completion; Network parameters; Out-of-sample extension; State-of-the-art methods; Deep learning; Article; artificial neural network; autoencoder based matrix completion; deep learning based matrix completion; digital filtering; image enhancement; information processing; information processing device; machine learning; priority journal",2-s2.0-85020903809
"Liu Z., Tang D., Cai Y., Wang R., Chen F.","A hybrid method based on ensemble WELM for handling multi class imbalance in cancer microarray data",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021142552&doi=10.1016%2fj.neucom.2017.05.066&partnerID=40&md5=6eb7f6d999cd67c7f44ab7c1bbfb7758","DNA microarray technology provides an efficient way to diagnose cancer. However, microarray gene expression data face the challenges of class imbalance and high dimension. The class imbalance problem usually leads to inaccurate results when using traditional feature selection and classification algorithms. Due to fast learning speed and good classification performance, extreme learning machine (ELM) has become one of the best classification algorithms and weighted ELM has been recently presented to deal with the class imbalance. However, they ignored the negative impact of imbalanced feature set. This paper proposes a hybrid method based on WELM to handle the multi class imbalance problem of cancer microarray data at both feature and algorithmic levels. At feature level, a corrected feature subset is searched for each class using class oriented feature selection method, so that the features correlated with the minority class are explicitly selected. At algorithmic level, WELM is further modified to strengthen the input nodes with high discrimination power, and an ensemble model is trained to improve the generalization. That is, multiple modified WELM models are trained on the datasets characterized by different feature subsets; in order to encourage the ensemble diversity, the models with low dissimilarity are removed and the reserved ones are combined as an ensemble model. The experiments are conducted on eight gene expression datasets with multiple cancer types and classification results show that our method significantly outperforms ELM and several recent works. © 2017","Ensemble learning; Extreme learning machine; Feature selection; High dimension; Multi class imbalance","Classification (of information); Diseases; Education; Gene expression; Genes; Knowledge acquisition; Learning systems; Class imbalance; Classification performance; DNA microarray technology; Ensemble learning; Extreme learning machine; Feature selection and classification; High dimensions; Microarray gene expression data; Feature extraction; algorithm; Article; biodiversity; controlled study; ensemble diversity; gene expression; machine learning; measurement accuracy; measurement precision; microarray analysis; priority journal; weighted extreme learning machine",2-s2.0-85021142552
"Fang Y., Lei J., Li J., Xu L., Lin W., Callet P.L.","Learning visual saliency from human fixations for stereoscopic images",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019705975&doi=10.1016%2fj.neucom.2017.05.050&partnerID=40&md5=5d3c50807133bf7597319efc2bc7f811","In the previous years, a lot of saliency detection algorithms have been designed for saliency computation of visual content. Recently, stereoscopic display techniques have developed rapidly, which results in much requirement of stereoscopic saliency detection for emerging stereoscopic applications. Different from 2D saliency prediction, stereoscopic saliency detection methods have to consider depth factor. We design a novel stereoscopic saliency detection algorithm by machine learning technique. First, the features of luminance, color and texture are extracted to calculate the feature contract for predicting feature maps of stereoscopic images. Furthermore, the depth features are extracted for depth feature map computation. Sematic features including the center-bias factor and other top-down cues are also applied as the features in the proposed stereoscopic saliency detection method. Support Vector Regression (SVR) is applied to learn the saliency detection model of stereoscopic images. Experimental results obtained on a public large-scale eye tracking database demonstrate that the proposed method can predict better saliency results for stereoscopic images than other existing ones. © 2017 Elsevier B.V.","3D image; Machine learning; Stereoscopic image; Stereoscopic saliency detection; Support Vector Regression; Visual attention","Artificial intelligence; Behavioral research; Eye movements; Forecasting; Learning systems; Signal detection; 3-D image; Saliency detection; Stereoscopic image; Support vector regression (SVR); Visual Attention; Stereo image processing; algorithm; Article; color; controlled study; data base; experimental study; eye fixation; eye tracking; imaging and display; luminance; machine learning; physical parameters; prediction; priority journal; regression analysis; stereoscopic image; texture; visual saliency",2-s2.0-85019705975
"Hayashi N., Watanabe S.","Upper bound of Bayesian generalization error in non-negative matrix factorization",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019636753&doi=10.1016%2fj.neucom.2017.04.068&partnerID=40&md5=9948cee7e65ad199b32eb08b07d01afa","Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used for text mining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied. © 2017 Elsevier B.V.","Bayesian learning; Non-negative matrix factorization (NMF); Real log canonical threshold (RLCT)","Data mining; Errors; Factorization; Learning systems; Signal processing; Bayesian learning; Generalization Error; Knowledge discovery method; Matrix factorizations; Nonnegative matrix factorization; Optimization method; Real log canonical threshold (RLCT); Statistical modeling; Matrix algebra; analytical error; Article; Bayesian learning; bioinformatics; data mining; factor analysis; machine learning; mathematical analysis; priority journal; signal processing; statistical model",2-s2.0-85019636753
"Chen J., Ma B., Cao H., Chen J., Fan Y., Li R., Wu W.","Updating initial labels from spectral graph by manifold regularization for saliency detection",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019975581&doi=10.1016%2fj.neucom.2017.04.066&partnerID=40&md5=7c55ff445f5dfed16b6a3d92e6b19de9","This paper presents a novel saliency detection method via updating initial labels from spectral graph in a semi-supervised learning (SSL) framework. For updating labels efficiently with graph-based SSL, two principles generally should be considered. The first one is that the updated labels should not change too much from their initial assignment. The second one is that the updated labels should not change too much between similar samples. To follow the first principle, the biggest eigenvector of Laplacian matrix, which contains rich contrast between background regions and salient regions, is employed to obtain the initial label vector. To follow the second principle, a new graph construction scheme, in which only boundary samples with similar features can be connected with each other, is proposed to reduce the geodesic distance in graph. Then a graph-based manifold regularization framework is exploited to update the label vector for separating salient samples from non-salient samples. A refinement function cooperating with an activation function is further presented for saliency optimization. Experimental results show that the proposed method achieves competitive performance against some recent state-of-the-art algorithms for saliency detection. © 2017 Elsevier B.V.","Label updating; Manifold regularization; Saliency detection; Spectral graph","Matrix algebra; Supervised learning; Activation functions; Competitive performance; Geodesic distances; Laplacian matrices; Manifold regularizations; Saliency detection; Semi-supervised learning (SSL); Spectral graph; Graphic methods; algorithm; Article; controlled study; image analysis; image processing; image segmentation; priority journal; probability; quantitative analysis; supervised machine learning",2-s2.0-85019975581
"Qayyum A., Anwar S.M., Awais M., Majid M.","Medical image retrieval using deep convolutional neural network",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019914033&doi=10.1016%2fj.neucom.2017.05.025&partnerID=40&md5=9ab87ad122f60ea26c0cdd012acc40fc","With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep convolutional neural network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty-four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs. © 2017","Content based medical image retrieval (CBMIR); Convolutional neural networks (CNNs); Deep learning; Similarity metric","Convolution; Deep learning; Deep neural networks; Image retrieval; Medical imaging; Neural networks; Semantics; Classification accuracy; Classification results; Content based medical image retrieval; Convolutional neural network; Feature representation; High-level information; Multimodal medical images; Similarity metrics; Search engines; accuracy; Article; artificial neural network; conceptual framework; content based image retrieval; content based medical image retrieval; convolutional neural network; human; image processing; image retrieval; machine learning; prediction; priority journal",2-s2.0-85019914033
"Chamroukhi F.","Skew t mixture of experts",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025666856&doi=10.1016%2fj.neucom.2017.05.044&partnerID=40&md5=7354470c4fe1a571d5b8b43dd7fed996","Mixture of experts (MoE) is a popular framework in the fields of statistics and machine learning for modeling heterogeneity in data for regression, classification and clustering. MoE for continuous data are usually based on the normal distribution. However, it is known that for data with asymmetric behavior, heavy tails and atypical observations, the use of the normal distribution is unsuitable. We introduce a new robust non-normal mixture of experts modeling using the skew t distribution. The proposed skew t mixture of experts, named STMoE, handles these issues of the normal mixtures experts regarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated expectation conditional maximization (ECM) algorithm to estimate the model parameters by monotonically maximizing the observed data log-likelihood. We describe how the presented model can be used in prediction and in model-based clustering of regression data. Numerical experiments carried out on simulated data show the effectiveness and the robustness of the proposed model in fitting non-linear regression functions as well as in model-based clustering. Then, the proposed model is applied to the real-world data of tone perception for musical data analysis, and the one of temperature anomalies for the analysis of climate change data. The obtained results confirm the usefulness of the model for practical data analysis applications. © 2017 Elsevier B.V.","ECM algorithm; EM algorithm; Mixture of experts; Model-based clustering; Non-linear regression; Skew t distribution","Climate change; Climate models; Data handling; Information analysis; Mixtures; Normal distribution; Regression analysis; ECM algorithm; EM algorithms; Mixture of experts; Model-based clustering; Non-linear regression; T distribution; Clustering algorithms; algorithm; Article; classification; climate change; environmental temperature; machine learning; music; prediction; priority journal; simulation; skew t mixture of experts; statistical concepts; statistical distribution; statistical model; statistical parameters",2-s2.0-85025666856
"Badem H., Basturk A., Caliskan A., Yuksel M.E.","A new efficient training strategy for deep neural networks by hybridization of artificial bee colony and limited–memory BFGS optimization algorithms",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020757063&doi=10.1016%2fj.neucom.2017.05.061&partnerID=40&md5=c93b7238718850c36227bad8ea14aa84","Working up with deep learning techniques requires profound understanding of the mechanisms underlying the optimization of the internal parameters of complex structures. The major factor limiting this understanding is that there exist only a few optimization methods such as gradient descent and Limited–memory Broyden–Fletcher–Goldfarb–Shannon (L-BFGS) to find the best local minima of the problem space for these complex structures such as deep neural network (DNN). Therefore, in this paper, we represent a new training approach named hybrid artificial bee colony based training strategy (HABCbTS) to tune the parameters of a DNN structure, which includes one or more autoencoder layers cascaded to a softmax classification layer. In this strategy, a derivative-free optimization algorithm “ABC” is combined with a derivative-based algorithm “L-BFGS” to construct “HABC”, which is used in the HABCbTS. Detailed simulation results supported by statistical analysis show that the proposed training strategy results in better classification performance compared to the DNN classifier trained with the L-BFGS, ABC and modified ABC. The obtained classification results are also compared with the state-of-the-art classifiers, including MLP, SVM, KNN, DT and NB on 15 data sets with different dimensions and sizes. © 2017 Elsevier B.V.","Artificial bee colony optimization algorithm; Deep learning; Deep neural network; Hybridization; L-BFGS; Stacked autoencoder network; Training strategy","Classification (of information); Complex networks; Deep learning; Deep neural networks; Learning systems; Artificial bee colony optimization algorithms; Auto encoders; Hybridization; L-BFGS; Training strategy; Optimization; Article; artificial neural network; Bayesian learning; classification algorithm; classifier; computer simulation; decision tree; deep neural network; human; intermethod comparison; k nearest neighbor; learning algorithm; mathematical computing; perceptron; performance; priority journal; process optimization; statistical analysis; support vector machine",2-s2.0-85020757063
"Waris M.A., Iosifidis A., Gabbouj M.","CNN-based edge filtering for object proposals",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020766935&doi=10.1016%2fj.neucom.2017.05.071&partnerID=40&md5=262aeaf107452560a5d0b1559d195454","Recent advances in image-based object recognition have exploited object proposals to speed up the detection process by reducing the search space. In this paper, we present a novel idea that utilizes true objectness and semantic image filtering (retrieved within the convolutional layers of a Convolutional Neural Network) to propose effective region proposals. Information learned in fully convolutional layers is used to reduce the number of proposals and enhance their localization by producing highly accurate bounding boxes. The greatest benefit of our method is that it can be integrated into any existing approach exploiting edge-based objectness to achieve consistently high recall across various intersection over union thresholds. Experiments on PASCAL VOC 2007 and ImageNet datasets demonstrate that our approach improves two existing state-of-the-art models with significantly high margins and pushes the boundaries of object proposal generation. © 2017 Elsevier B.V.","Deep learning; Neural networks; Object detection; Object proposals; Region of interest","Convolution; Deep learning; Deep neural networks; Image segmentation; Neural networks; Object detection; Optical character recognition; Semantics; Convolutional neural network; Detection process; Highly accurate; Image based object recognition; Object proposals; Region of interest; Semantic images; State of the art; Object recognition; Article; artificial neural network; automated pattern recognition; convolutional neural network; digital filtering; image processing; image quality; measurement accuracy; network learning; priority journal; recall; scale up; semantics; signal detection; support vector machine",2-s2.0-85020766935
"Khozani Z.S., Bonakdari H., Zaji A.H.","Efficient shear stress distribution detection in circular channels using Extreme Learning Machines and the M5 model tree algorithm",2017,"Urban Water Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019615830&doi=10.1080%2f1573062X.2017.1325495&partnerID=40&md5=33655f27a8ec50f4bfdc913f43a4fab6","With the aid of 174 laboratory data sets, Extreme Learning Machine (ELM) and decision tree (M5) models were investigated in predicting the shear stress distribution in circular channels. To evaluate the sensitivity of the input variables, 15 different input combinations were applied to each model. The calculation results show that the Re and y/P parameter values greatly affect ELM method performance, while y/P and h/D are sensitive to shear stress distribution modeling with M5. The best models among ELM and M5 were compared with an equation based on the Shannon entropy. According to the comparison results, the two proposed models outperform the Shannon entropy equation. Moreover, the ELM method’s function is superior in estimating the shear stress distribution and more adapted to experimental data with average Root Mean Square Error (RMSE) of 0.0236 compared to the M5 method with RMSE of 0.0364. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","circular channel; decision tree; extreme learning machine; Shear stress distribution","algorithm; channel; decision support system; machine learning; modeling; parameterization; prediction; shear stress",2-s2.0-85019615830
"Dinler D., Tural M.K.","Robust semi-supervised clustering with polyhedral and circular uncertainty",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020910930&doi=10.1016%2fj.neucom.2017.04.073&partnerID=40&md5=0261f0aa25f08cef8ff9c397f63a4d55","We consider a semi-supervised clustering problem where the locations of the data objects are subject to uncertainty. Each uncertainty set is assumed to be either a closed convex bounded polyhedron or a closed disk. The final clustering is expected to be in accordance with a given number of instance level constraints. The objective function considered minimizes the total of the sum of the violation costs of the unsatisfied instance level constraints and a weighted sum of squared maximum Euclidean distances between the locations of the data objects and the centroids of the clusters they are assigned to. Given a cluster, we first consider the problem of computing its centroid, namely the centroid computation problem under uncertainty (CCPU). We show that the CCPU can be modeled as a second order cone programing problem and hence can be efficiently solved to optimality. As the CCPU is one of the key ingredients of the several algorithms considered in this paper, a subgradient algorithm is also adopted for its faster solution. We then propose a mixed-integer second order cone programing formulation for the considered clustering problem which is only able to solve small-size instances to optimality. For larger instances, approaches from the semi-supervised clustering literature are modified and compared in terms of computational time and quality. © 2017 Elsevier B.V.","Clustering; Heuristics; Second order cone programing; Semi-supervised learning; Uncertainty","Cluster computing; Optimization; Supervised learning; Clustering; Heuristics; Second order cone; Semi- supervised learning; Uncertainty; Clustering algorithms; algorithm; Article; centroid computation problem under uncertainty; cluster analysis; computer heuristics; controlled study; priority journal; supervised machine learning; uncertainty",2-s2.0-85020910930
"Villuendas-Rey Y., Rey-Benguría C.F., Ferreira-Santiago Á., Camacho-Nieto O., Yáñez-Márquez C.","The Naïve Associative Classifier (NAC): A novel, simple, transparent, and accurate classification model evaluated on financial data",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020482428&doi=10.1016%2fj.neucom.2017.03.085&partnerID=40&md5=d4ecd55cc7111214cb9494162319c3a4","In this paper the Naïve Associative Classifier (NAC), a novel supervised learning model, is presented. Its strengths lie in its simplicity, transparency, transportability and accuracy. The creation, design, implementation and application of the NAC are sustained by an original similarity operator of our own design, the Mixed and Incomplete Data Similarity Operator (MIDSO). One of the key features of MIDSO is its ability to handle missing values as well as mixed numerical and categorical data types. The proposed model was tested by performing numerical experiments using finance-related datasets including credit assignment, bank telemarketing, bankruptcy, and banknote authentication. The experimental results show the adequacy of the model for decision support in those environments, outperforming several state-of-the-art pattern classifiers. Additionally, the advantages and limitations of the NAC, as well as possible improvements, are discussed. © 2017 Elsevier B.V.","Bank deposits; Bank Telemarketing; Mixed data; Savings; Similarity; Supervised classification","Clustering algorithms; Decision support systems; Supervised learning; Bank Telemarketing; Mixed data; Savings; Similarity; Supervised classification; Sodium; accuracy; Article; artificial neural network; Bayes theorem; computer; decision support system; financial management; human; k nearest neighbor; learning; operator; pattern recognition; priority journal; support vector machine",2-s2.0-85020482428
"Safari A., Sohrabi H., Powell S., Shataee S.","A comparative assessment of multi-temporal Landsat 8 and machine learning algorithms for estimating aboveground carbon stock in coppice oak forests",2017,"International Journal of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025666307&doi=10.1080%2f01431161.2017.1356488&partnerID=40&md5=3acf20f151ced622cb8859719b9fcc0f","Remote sensing of low biomass forests has challenges related to the contribution of soil and understory reflectance recorded by sensors, hampering accurate forest aboveground carbon (AGC) quantification. To improve Landsat-based AGC estimates in forests with low biomass, this study explored the use of multi-temporal Landsat 8 Operational Land Imager (OLI) derived spectral information in Zagros forests by testing four machine learning algorithms: support vector machine (SVM), boosted regression trees (BRT), random forest (RF) and multivariate adaptive regression splines (MARS). We selected two forest areas with different levels of human activity for AGC reference plots: un-degraded forest (UD) and highly-degraded forest (HD). The results of the study showed that the Landsat image acquired in the peak of the growing season (10 August) provided the best AGC estimates for the UD site, but that for the HD site, AGC estimates were not affected by the timing of the imagery. The comparison of different modelling methods demonstrated lower accuracies from BRT, considerably biased estimates from SVM, and generally robust results from the RF algorithm. Overall, the study demonstrated the utility of applying the free Landsat 8 OLI dataset to AGC estimation, in particular non-commercial forests in developing countries where little budget is allocated for management. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Artificial intelligence; Budget control; Decision trees; Developing countries; Education; Learning systems; Support vector machines; Above-ground carbons; Biased estimates; Boosted regression trees; Comparative assessment; Modelling method; Multivariate adaptive regression splines; Operational land imager; Spectral information; Learning algorithms; algorithm; assessment method; comparative study; coppice; deciduous forest; estimation method; growing season; Landsat; machine learning; remote sensing",2-s2.0-85025666307
"Pons P., Jaen J., Catala A.","Assessing machine learning classifiers for the detection of animals’ behavior using depth-based tracking",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020183627&doi=10.1016%2fj.eswa.2017.05.063&partnerID=40&md5=497b15ef432a128c8e754d35b1689910","There is growing interest in the automatic detection of animals’ behaviors and body postures within the field of Animal Computer Interaction, and the benefits this could bring to animal welfare, enabling remote communication, welfare assessment, detection of behavioral patterns, interactive and adaptive systems, etc. Most of the works on animals’ behavior recognition rely on wearable sensors to gather information about the animals’ postures and movements, which are then processed using machine learning techniques. However, non-wearable mechanisms such as depth-based tracking could also make use of machine learning techniques and classifiers for the automatic detection of animals’ behavior. These systems also offer the advantage of working in set-ups in which wearable devices would be difficult to use. This paper presents a depth-based tracking system for the automatic detection of animals’ postures and body parts, as well as an exhaustive evaluation on the performance of several classification algorithms based on both a supervised and a knowledge-based approach. The evaluation of the depth-based tracking system and the different classifiers shows that the system proposed is promising for advancing the research on animals’ behavior recognition within and outside the field of Animal Computer Interaction. © 2017 Elsevier Ltd","Animal Computer Interaction; Classification algorithms; Depth-based tracking; Intelligent system; Tracking system","Animals; Artificial intelligence; Behavioral research; Intelligent systems; Knowledge based systems; Learning algorithms; Pattern recognition; Pattern recognition systems; Tracking (position); Wearable technology; Animal-computer interactions; Automatic Detection; Behavior recognition; Classification algorithm; Knowledge-based approach; Machine learning techniques; Remote communication; Tracking system; Learning systems",2-s2.0-85020183627
"Sarlin P., Björk K.-M.","Machine learning in finance—Guest editorial",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021399449&doi=10.1016%2fj.neucom.2017.06.021&partnerID=40&md5=414ca3ffdada10bb1f1edbc287ac52b1",[No abstract available],,"algorithm; artificial neural network; economic aspect; Editorial; finance; forecasting; investment; machine learning; natural language processing; prediction; priority journal",2-s2.0-85021399449
"Bequé A., Lessmann S.","Extreme learning machines for credit scoring: An empirical evaluation",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019633628&doi=10.1016%2fj.eswa.2017.05.050&partnerID=40&md5=2ebe192e17e106a4fcc5dc9cf4d1ff60","Classification algorithms are used in many domains to extract information from data, predict the entry probability of events of interest, and, eventually, support decision making. This paper explores the potential of extreme learning machines (ELM), a recently proposed type of artificial neural network, for consumer credit risk management. ELM possess some interesting properties, which might enable them to improve the quality of model-based decision support. To test this, we empirically compare ELM to established scoring techniques according to three performance criteria: ease of use, resource consumption, and predictive accuracy. The mathematical roots of ELM suggest that they are especially suitable as a base model within ensemble classifiers. Therefore, to obtain a holistic picture of their potential, we assess ELM in isolation and in conjunction with different ensemble frameworks. The empirical results confirm the conceptual advantages of ELM and indicate that they are a valuable alternative to other credit risk modelling methods. © 2017","Artificial neural networks; Classifier ensembles; Credit scoring; Extreme learning machines","Classification (of information); Data mining; Decision making; Decision support systems; Knowledge acquisition; Neural networks; Risk assessment; Risk management; Classification algorithm; Classifier ensembles; Consumer credit risks; Credit scoring; Empirical evaluations; Ensemble classifiers; Extreme learning machine; Performance criterion; Learning systems",2-s2.0-85019633628
"Hu K., Yang W., Gao X.","Microcalcification diagnosis in digital mammography using extreme learning machine based on hidden Markov tree model of dual-tree complex wavelet transform",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020023501&doi=10.1016%2fj.eswa.2017.05.062&partnerID=40&md5=44820b63c4f529789cd67cbdc27d756b","Diagnosis of benign and malignant microcalcifications in digital mammography using Computer-aided Diagnosis (CAD) system is critical for the early diagnosis of breast cancer. Wavelet transform based diagnosis methods are effective to accomplish this task, but limited by representing the correlation within each wavelet scale, these methods neglect the correlation between wavelet scales. In this paper, we apply the hidden Markov tree model of dual-tree complex wavelet transform (DTCWT-HMT) for microcalcification diagnosis in digital mammography. DTCWT-HMT can effectively capture the correlation between different wavelet coefficients and model the statistical dependencies and non-Gaussian statistics of real signals, is used to characterize microcalcifications for the diagnosis of benign and malignant cases. The combined features which consist of the DTCWT-HMT features and the DTCWT features are optimized by genetic algorithm (GA). Extreme learning machine (ELM), an efficient learning theory is employed as the classifier to diagnose the benign and malignant microcalcifications. The validity of the proposed method is evaluated on the Nijmegen, MIAS and DDSM datasets using area under curve (AUC) of receiver operating characteristic (ROC). The AUC values of 0.9856, 0.9941 and 0.9168 of the proposed method are achieved on Nijmegen, MIAS and DDSM, respectively. We compare the proposed method with state-of-the-art diagnosis methods, and the experimental results show the effectiveness of the proposed method for the diagnosis of the benign and malignant microcalcifications in mammograms in terms of the accuracy and stability. © 2017 Elsevier Ltd","Digital mammography; Dual-tree complex wavelet transform; Extreme learning machine; Feature extraction; Hidden Markov tree model; Microcalcification diagnosis","Computer aided diagnosis; Diagnosis; E-learning; Feature extraction; Genetic algorithms; Hidden Markov models; Image segmentation; Knowledge acquisition; Learning systems; Partial discharges; Digital mammography; Dual-tree complex wavelet transform; Extreme learning machine; Hidden Markov tree model; Microcalcifications; Wavelet transforms",2-s2.0-85020023501
"Song Q., Liu A., Yang S.Y.","Stock portfolio selection using learning-to-rank algorithms with news sentiment",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021208422&doi=10.1016%2fj.neucom.2017.02.097&partnerID=40&md5=d098e4b31adf87fb065f5d8b715483d2","In this study, we apply learning-to-rank algorithms to design trading strategies using relative performance of a group of stocks based on investors’ sentiment toward these stocks. We show that learning-to-rank algorithms are effective in producing reliable rankings of the best and the worst performing stocks based on investors’ sentiment. More specifically, we use the sentiment shock and trend indicators introduced in the previous studies, and we design stock selection rules of holding long positions of the top 25% stocks and short positions of the bottom 25% stocks according to rankings produced by learning-to-rank algorithms. We then apply two learning-to-rank algorithms, ListNet and RankNet, in stock selection processes and test long-only and long-short portfolio selection strategies using 10 years of market and news sentiment data. Through backtesting of these strategies from 2006 to 2014, we demonstrate that our portfolio strategies produce risk-adjusted returns superior to the S&P 500 index return, the hedge fund industry average performance - HFRIEMN, and some sentiment-based approaches without learning-to-rank algorithm during the same period. © 2017 Elsevier B.V.","Financial news sentiment; Learning-to-rank; Long-short strategy; Stock portfolio selection; Trading strategy","Commerce; Education; Financial markets; Investments; Learning algorithms; Multivariable control systems; Financial news; Learning to rank; Long-short strategy; Stock portfolio selections; Trading strategies; Electronic trading; Article; financial management; investment; learning to rank algorithm; machine learning; market; prediction; priority journal; reliability",2-s2.0-85021208422
"Zhai D., Chaudhuri T., Soh Y.C.","Modeling and optimization of different sparse Augmented Firefly Algorithms for ACMV systems under two case studies",2017,"Building and Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028947394&doi=10.1016%2fj.buildenv.2017.08.032&partnerID=40&md5=f6d775136fd8c883cfbd24a10edbaf32","This paper examines the six different schemes of sparse Augmented Firefly Algorithm (AFA) for studying the balancing of energy efficiency and indoor thermal comfort of smart buildings. Based on the well-trained Extreme Learning Machines (ELM) and Neural Networks (NN) models of energy consumption, ambient air temperature and air velocity which have earlier been established and validated through experimental studies, our current optimization problem is formulated to associate indoor thermal comfort with energy efficiency of buildings, so that we can evaluate the key parameters that will influence the balancing of these two demands. The optimizations of the objective functions are carried out in real-time by using novel techniques of sparse AFA. We examined six different schemes of AFA, which are different in random-wandering size and random-wandering distribution. This is so that small and large regions with different wandering can be comprehensively studied. Moreover, the Energy Saving Rates (ESRs) of different operating frequencies are predicted through a third order polynomial regression to minimize the Mean Squared Errors (MSE) of the cost functions. Evaluations of the six different schemes show that the scheme named Large Region Gaussian Wandering (LRGW) generally outperforms the others. Given the best experimental results of AFA optimizations and demonstrated through an experimental room, the maximum potential ESR are about −26.5% for Case 1 of general offices and −9.83% for Case 2 of lecture theatres/conference rooms. These are achieved while maintaining indoor thermal comfort in the pre-defined comfort zone. © 2017 Elsevier Ltd","Air-Conditioning and Mechanical Ventilation Systems (ACMV); Augmented Firefly Algorithm (AFA); Energy efficiency; Extreme Learning Machines (ELM); Neural Networks (NN); Predicted Mean Vote (PMV); Thermal comfort","Air; Air conditioning; Bioluminescence; Cost functions; Energy conservation; Energy utilization; Function evaluation; Knowledge acquisition; Learning systems; Mean square error; Optimization; Thermal comfort; Ventilation; Extreme learning machine; Firefly algorithms; Mechanical ventilation system; Neural network (nn); Predicted mean vote; Energy efficiency; air conditioning; algorithm; artificial neural network; energy efficiency; indoor air; machine learning; optimization; temperature effect",2-s2.0-85028947394
"Beguin J., Fuglstad G.-A., Mansuy N., Paré D.","Predicting soil properties in the Canadian boreal forest with limited data: Comparison of spatial and non-spatial statistical approaches",2017,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026480591&doi=10.1016%2fj.geoderma.2017.06.016&partnerID=40&md5=f4c1236528f60ffc7d56bbde6fc03f0f","Digital soil mapping (DSM) involves the use of georeferenced information and statistical models to map predictions and uncertainties related to soil properties. Many remote regions of the globe, such as boreal forest ecosystems, are characterized by low sampling efforts and limited availability of field soil data. Although DSM is an expanding topic in soil science, little guidance currently exists to select the appropriate combination of statistical methods and model formulation in the context of limited data availability. Using the Canadian managed forest as a case study, the main objective of this study was to investigate to which extent the choice of statistical method and model specification could improve the spatial prediction of soil properties with limited data. More specifically, we compared the cross-product performance of eight statistical approaches (linear, additive and geostatistical models, and four machine-learning techniques) and three model formulations (“covariates only”: a suite of environmental covariates only; “spatial only”: a function of geographic coordinates only; and “covariates + spatial”: a combination of both covariates and spatial functions) to predict five key forest soil properties in the organic layer (thickness and C:N ratio) and in the top 15 cm of the mineral horizon (carbon concentration, percentage of sand, and bulk density). Our results show that 1) although strong differences in predictive performance occurred across all statistical approaches and model formulations, spatially explicit models consistently had higher R2 and lower RMSE values than non-spatial models for all soil properties, except for the C:N ratio; 2) Bayesian geostatistical models were among the best methods, followed by ordinary kriging and machine-learning methods; and 3) comparative analyses made it possible to identify the more performant models and statistical methods to predict specific soil properties. We make modeling tools and code available (e.g., Bayesian geostastical models) that increase DSM capabilities and support existing efforts toward the production of improved digital soil products with limited data. © 2017 Elsevier B.V.","Bayesian analyses; Boosted regression trees; Boreal forest; Cross-validation; Digital soil mapping; Geostatistic; Kriging; Machine-learning; Random forests; Spatial autocorrelation","Carbon; Decision trees; Ecosystems; Education; Forecasting; Forestry; Interpolation; Learning systems; Mapping; Soil surveys; Soils; Statistics; Bayesian Analysis; Boosted regression trees; Boreal forests; Cross validation; Digital soil mappings; Geostatistic; Kriging; Random forests; Spatial autocorrelations; Statistical methods; autocorrelation; Bayesian analysis; boreal forest; forest soil; geostatistics; kriging; machine learning; model validation; regression analysis; soil property; soil science; Canada",2-s2.0-85026480591
"Rönnqvist S., Sarlin P.","Bank distress in the news: Describing events through deep learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021743187&doi=10.1016%2fj.neucom.2016.12.110&partnerID=40&md5=dc79cd375dc21b49fdac0a32d67142bf","While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics. © 2017 Elsevier B.V.","Bank distress; Distributional semantics; Event detection; Financial risk; Neural networks; Text mining","Data mining; Education; Finance; Natural language processing systems; Neural networks; Semantics; Bank distress; Distributional semantics; Event detection; Financial risks; Text mining; Deep learning; Article; artificial neural network; descriptive research; European; financial management; government; language; machine learning; model; prediction; priority journal; risk assessment; semantics",2-s2.0-85021743187
"Wang H., Gu J., Wang S.","An effective intrusion detection framework based on SVM with feature augmentation",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029208208&doi=10.1016%2fj.knosys.2017.09.014&partnerID=40&md5=c91a527e037dae8b4ee8628844aba927","Network security is becoming increasingly important in our daily lives—not only for organizations but also for individuals. Intrusion detection systems have been widely used to prevent information from being compromised, and various machine-learning techniques have been proposed to enhance the performance of intrusion detection systems. However, higher-quality training data is an essential determinant that could improve detection performance. It is well known that the marginal density ratio is the most powerful univariate classifier. In this paper, we propose an effective intrusion detection framework based on a support vector machine (SVM) with augmented features. More specifically, we implement the logarithm marginal density ratios transformation to form the original features with the goal of obtaining new and better-quality transformed features that can greatly improve the detection capability of an SVM-based detection model. The NSL-KDD dataset is used to evaluate the proposed method, and the empirical results show that it achieves a better and more robust performance than existing methods in terms of accuracy, detection rate, false alarm rate and training speed. © 2017 Elsevier B.V.","Intrusion detection; Marginal density ratios transformation; Network security; Support vector machine","Computer crime; Feature extraction; Learning systems; Mercury (metal); Network security; Support vector machines; Detection capability; Detection models; Detection performance; Intrusion Detection Systems; Machine learning techniques; Marginal densities; Quality training; Robust performance; Intrusion detection",2-s2.0-85029208208
"Kusakci A.O., Ayvaz B., Karakaya E.","Towards an autonomous human chromosome classification system using Competitive Support Vector Machines Teams (CSVMT)",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020161367&doi=10.1016%2fj.eswa.2017.05.070&partnerID=40&md5=a5ad9d2f0b0582de72a42855e8f5ada6","In broad terms, karyotyping is the process of examination and classification of human chromosome images to diagnose genetic diseases and disorders. It requires time consuming manual examination of cell images by a cytogeneticist to distinguish chromosome classes from each other. Thus, a reliable autonomous human chromosome classification system not only saves time and money but also reduces errors due to the inadequate knowledge level of the expert. Human cell contains 23 pairs of chromosome, 22 autosomes and a pair of sex chromosomes. Hence, we face a multi-class classification task which represents a challenging case for any sort of classifier. In this work, to solve this classification problem, we propose a novel methodology consisting two stages: (i) data preparation and training, and (ii) testing. To determine the most informative content of the dataset several preliminary experiments are conducted and a Principal Component Analysis is done. Then, a single Support Vector Machine (SVMij) is trained to separate a pair of classes, (i,j) where a numerical optimization method Pattern Search (PS), is employed to find the optimal parameters for the SVMij. Considering 22 pairs of autosomes, 22 × 22 experts are trained and optimized. The cluster of experts, we obtain is named as Competitive SVM Teams (CSVMTs) where each SVMij competes with the others to label a new classification instance. The final output of the classifier is determined by majority voteing. The results obtained on Copenhagen dataset proves the merit of the algorithm as correct classification rates (CRR) on train and test samples are 99.55% and 97.84% respectively, which are higher than any accuracy rate achieved so far in the related literature. © 2017 Elsevier Ltd","Chromosome classification; Committee machines; Karyotyping; Support Vector Machines","Chromosomes; Classification (of information); Learning systems; Numerical methods; Optimization; Principal component analysis; Statistical tests; Chromosome classification; Classification rates; Committee machines; Karyotyping; Manual examination; Multi-class classification; Novel methodology; Numerical optimizations; Support vector machines",2-s2.0-85020161367
"Shynkevich Y., McGinnity T.M., Coleman S.A., Belatreche A., Li Y.","Forecasting price movements using technical indicators: Investigating the impact of varying input window length",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021121819&doi=10.1016%2fj.neucom.2016.11.095&partnerID=40&md5=fcd7fa1c9cf247c502ee07ab7242615a","The creation of a predictive system that correctly forecasts future changes of a stock price is crucial for investment management and algorithmic trading. The use of technical analysis for financial forecasting has been successfully employed by many researchers. Input window length is a time frame parameter required to be set when calculating many technical indicators. This study explores how the performance of the predictive system depends on a combination of a forecast horizon and an input window length for forecasting variable horizons. Technical indicators are used as input features for machine learning algorithms to forecast future directions of stock price movements. The dataset consists of ten years daily price time series for fifty stocks. The highest prediction performance is observed when the input window length is approximately equal to the forecast horizon. This novel pattern is studied using multiple performance metrics: prediction accuracy, winning rate, return per trade and Sharpe ratio. © 2017","Decision making; Financial forecasting; Stock price prediction; Technical trading","Commerce; Costs; Decision making; Financial markets; Forecasting; Investments; Learning algorithms; Financial forecasting; Investment management; Performance metrics; Prediction accuracy; Prediction performance; Stock price movements; Stock price prediction; Technical trading; Electronic trading; algorithm; Article; calculation; cost; economic aspect; finance; forecasting; investment; machine learning; mathematical parameters; moving average; prediction; priority journal; relative strength index; stock price movement; technical indicator",2-s2.0-85021121819
"Liu C., Wang W., Tu G., Xiang Y., Wang S., Lv F.","A new Centroid-Based Classification model for text categorization",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028600724&doi=10.1016%2fj.knosys.2017.08.020&partnerID=40&md5=78ece8d120d08ef52720d31a0237c109","The automatic text categorization technique has gained significant attention among researchers because of the increasing availability of online text information. Therefore, many different learning approaches have been designed in the text categorization field. Among them, the widely used method is the Centroid-Based Classifier (CBC) due to its theoretical simplicity and computational efficiency. However, the classification accuracy of CBC greatly depends on the data distribution. Thus it leads to a misfit model and also has poor classification performance when the data distribution is highly skewed. In this paper, a new classification model named as Gravitation Model (GM) is proposed to solve the class-imbalanced classification problem. In the training phase, each class is weighted by a mass factor, which can be learned from the training data, to indicate data distribution of the corresponding class. In the testing phase, a new document will be assigned to a particular class with the max gravitational force. The performance comparisons with CBC and its variants based on the results of experiments conducted on twelve real datasets show that the proposed gravitation model consistently outperforms CBC together with the Class-Feature-Centroid Classifier (CFC). Also, it obtains the classification accuracy competitive to the DragPushing (DP) method while it maintains a more stable performance. Thus, the proposed gravitation model is proved to be less over-fitting and has higher learning ability than CBC model. © 2017 The Authors","Centroid-Based Classifier; Gravitation Model; Machine learning; Text categorization","Computational efficiency; Gravitation; Learning systems; Text processing; Automatic text categorization; Centroid-based classifications; Classification accuracy; Classification performance; Gravitation models; Imbalanced classification; Performance comparison; Text categorization; Classification (of information)",2-s2.0-85028600724
"Musto C., Lops P., de Gemmis M., Semeraro G.","Semantics-aware Recommender Systems exploiting Linked Open Data and graph-based features",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028328141&doi=10.1016%2fj.knosys.2017.08.015&partnerID=40&md5=02b7c647c1441f8e06381e39a52a1842","The recent spread of Linked Open Data (LOD) fueled the research in the area of Recommender Systems, since the (semantic) data points available in the LOD cloud can be exploited to improve the performance of recommendation algorithms by enriching item representations with new and relevant features. In this article we investigate the impact of the features gathered from the LOD cloud on a hybrid recommendation framework based on three classification algorithms, Random Forests, Naïve Bayes and Logistic Regression. Specifically, we extend the representation of the items by introducing two new types of features: LOD-based features, structured data extracted from the LOD cloud, as the genre of a movie or the writer of a book, and graph-based features, computed on the ground of the topological characteristics of both the bipartite graph-based representation connecting users and items, and the tripartite representation connecting users, items and properties in the LOD cloud. In the experimental session we assess the effectiveness of these novel features; results show that the use of information coming from the LOD cloud could improve the overall accuracy of our recommendation framework. Finally, our approach outperform several state-of-the-art recommendation techniques, thus confirming the insights behind this research. © 2017","Classifiers; Linked Open Data; Machine learning; Recommender Systems; Semantics","Classifiers; Data mining; Decision trees; Graphic methods; Learning systems; Semantics; Topology; Classification algorithm; Graph-based features; Hybrid recommendation; Linked open data (LOD); Linked open datum; Recommendation algorithms; Recommendation techniques; Topological characteristics; Recommender systems",2-s2.0-85028328141
"Xie T., Zheng Q., Zhang W.","Recognizing physical contexts of mobile video learners via smartphone sensors",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029213715&doi=10.1016%2fj.knosys.2017.09.002&partnerID=40&md5=1a496681eb36bc24a44323e6bf4dfd31","Current studies can effectively recognize several human activities in a single semantic context, but don't recognize the semantics of a single activity in different contexts. The main challenge is the conflicting phone usages as well as the special requirements of the energy consumption. This paper tests a classic learning scenario regarding mobile video viewing and validates the proposed recognition method by comprehensively taking the recognizing accuracy, effectiveness and the energy consumption into consideration. Readings of four carefully-selected sensors are collected and a wide range of machine learning algorithms are investigated. The results show the combination of accelerometer, light and sound sensors is better than that of acceleration, light and gyroscope sensors, the features with respect to energy spectral don't improve the recognition accuracy, and the system reaches robustness in a few minutes. The proposed method is simple, effective and practical in real applications of pervasive learning. © 2017","Context recognition; Mobile video learners; Physical context; Smartphone sensors","Energy utilization; Learning systems; Semantics; Smartphones; 71.35.Lk; Context recognition; Learning scenarios; Mobile video; Pervasive learning; Physical context; Recognition accuracy; Recognition methods; Learning algorithms",2-s2.0-85029213715
"Khatami A., Khosravi A., Nguyen T., Lim C.P., Nahavandi S.","Medical image analysis using wavelet transform and deep belief networks",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019982802&doi=10.1016%2fj.eswa.2017.05.073&partnerID=40&md5=9e4cb40b3fc95710b4f59e5e9467d083","This paper introduces a three-step framework for classifying multiclass radiography images. The first step utilizes a de-noising technique based on wavelet transform (WT) and the statistical Kolmogorov Smirnov (KS) test to remove noise and insignificant features of the images. An unsupervised deep belief network (DBN) is designed for learning the unlabelled features in the second step. Although small-scale DBNs have demonstrated significant potential, the computational cost of training the restricted Boltzmann machine is a major issue when scaling to large networks. Moreover, noise in radiography images can cause a significant corruption of information that hinders the performance of DBNs. The combination of WT and KS test in the first step helps improve performance of DBNs. Discriminative feature subsets obtained in the first two steps serve as inputs into classifiers in the third step for evaluations. Five frequently used classifiers including naive Bayes, radial basis function network, random forest, sequential minimal optimization, and support vector machine and four different case studies are implemented for experiments using the Image Retrieval in Medical Application data set. The experimental results show that the three-step framework has significantly reduced computational cost and yielded a great performance for multiclass radiography image classification. Along with effective applications in image processing in other fields published in the literature, deep learning network in this paper has again demonstrated its robustness in handling a complex set of medical images. This implies that the proposed approach can be implemented in real practice for analysing noisy radiography images, which have many useful medical applications such as diagnosis of diseases related to lung, breast, musculoskeletal or pediatric studies. © 2017 Elsevier Ltd","Classification; Deep belief network; Feature extraction; Kolmogorov Smirnov test; Radiography image; Wavelet transform","Bayesian networks; Decision trees; Deep learning; Diagnosis; Feature extraction; Image analysis; Image classification; Image compression; Image processing; Image retrieval; Medical applications; Medical imaging; Optimization; Radial basis function networks; Radiography; Wavelet transforms; Deep belief network (DBN); Deep belief networks; Discriminative features; Image retrieval in medical applications; Kolmogorov-Smirnov test; Radiography images; Restricted boltzmann machine; Sequential minimal optimization; Classification (of information)",2-s2.0-85019982802
"Morvari F., Ghasemi A.","Priority-based adaptive access barring for M2M communications in LTE networks using learning automata",2017,"International Journal of Communication Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019145578&doi=10.1002%2fdac.3325&partnerID=40&md5=f69f1c281838e936e00ced68adbcd3b5","Supporting a huge number of machine-to-machine devices with different priorities in Long Term Evolution networks is addressed in this paper. We propose a learning automaton (LA)–based scheme for dynamically allocating random access resources to different classes of machine-to-machine devices according to their priorities and their demands in each cycle. We then use another LA-based scheme to adjust the barring factor for each class to control the possible overload. We show that by appropriate updating procedure for these LAs, the system performance asymptotically converges to the optimal performance in which the evolved node B knows the number of access-attempting devices from each class a priori. Simulation results are provided to show the performance of the proposed scheme in random access resource allocation to defined classes and adjusting the barring factor for each of them. Copyright © 2017 John Wiley & Sons, Ltd.","access barring; learning automaton; machine-to-machine communications; random access","Automation; Long Term Evolution (LTE); Wireless telecommunication systems; Access barring; Different class; Learning Automata; Learning automaton; Machine to machines; Optimal performance; Priority-based; Random access; Machine-to-machine communication",2-s2.0-85019145578
"Li Y., Zhao Y., Wu Q.","GbA: A graph-based thread partition approach in speculative multithreading",2017,"Concurrency Computation ",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030644309&doi=10.1002%2fcpe.4294&partnerID=40&md5=9dbe41d69c3bb7e7196786264ec74597","Speculative multithreading is an effective technique to automatically parallelize sequential programs. Conventional thread partition approaches primarily include heuristic rule–based (HR-based) and machine learning–based. Heuristic rule–based approaches are effective to parallelize one type of programs and can seldom obtain the respective optimal partitions for different programs, and existing machine learning–based approaches usually use vector-based characterization to represent a program, but easily ignore control information among basic blocks and partitions along other paths except the critical path. This paper proposes a novel graph-based thread partition approach to overcome these 2 bottlenecks. Our approach characterizes programs with graphs, integrating feature and control informations, extracting good partition scheme successfully, and also applies a machine-learning algorithm to predict partition for unseen programs. Prophet, which consists of an automatic parallelization compiler and a multicore simulator, evaluates the performance of multithreaded programs. Experiment results reveal that our approach delivers a maximum performance improvement of about 55.49% on an 8 core than HR-based approach and a maximum 97.67% performance improvement over HR-based partition for SPEC2000 benchmarks. This result suggests that graph-based thread partition approach is effective for thread partition in speculative multithreading. Copyright © 2017 John Wiley & Sons, Ltd.","graph-based thread partition; machine learning; speculative multithreading","Artificial intelligence; Benchmarking; Graphic methods; Heuristic programming; Learning algorithms; Learning systems; Multicore programming; Multiprocessing programs; Multitasking; Automatic Parallelization; Control information; Graph-based; Heuristic rules; Multi-threaded programs; Optimal partitions; Sequential programs; Speculative multithreading; Program compilers",2-s2.0-85030644309
"Turewicz M., Kohl M., Ahrens M., Mayer G., Uszkoreit J., Naboulsi W., Bracht T., Megger D.A., Sitek B., Marcus K., Eisenacher M.","BioInfra.Prot: A comprehensive proteomics workflow including data standardization, protein inference, expression analysis and data publication",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021167905&doi=10.1016%2fj.jbiotec.2017.06.005&partnerID=40&md5=7fb64e7dde40b745f764ac10e0ac89eb","The analysis of high-throughput mass spectrometry-based proteomics data must address the specific challenges of this technology. To this end, the comprehensive proteomics workflow offered by the de.NBI service center BioInfra.Prot provides indispensable components for the computational and statistical analysis of this kind of data. These components include tools and methods for spectrum identification and protein inference, protein quantification, expression analysis as well as data standardization and data publication. All particular methods of the workflow which address these tasks are state-of-the-art or cutting edge. As has been shown in previous publications, each of these methods is adequate to solve its specific task and gives competitive results. However, the methods included in the workflow are continuously reviewed, updated and improved to adapt to new scientific developments. All of these particular components and methods are available as stand-alone BioInfra.Prot services or as a complete workflow. Since BioInfra.Prot provides manifold fast communication channels to get access to all components of the workflow (e.g., via the BioInfra.Prot ticket system: bioinfraprot@rub.de) users can easily benefit from this service and get support by experts. © 2017 The Authors","Bioinformatics service; Computational proteomics; de.NBI; Health and disease; Mass spectrometry; Workflow","Mass spectrometry; Proteins; Spectrometry; Standardization; Bioinformatics services; Computational proteomics; Data standardization; De.NBI; Expression analysis; Protein quantification; Scientific development; Workflow; Molecular biology; Article; machine learning; priority journal; protein analysis; protein expression; proteomics; standardization; statistical analysis; workflow",2-s2.0-85021167905
"Pfeuffer J., Sachsenberg T., Alka O., Walzer M., Fillbrunn A., Nilse L., Schilling O., Reinert K., Kohlbacher O.","OpenMS – A platform for reproducible analysis of mass spectrometry data",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020715888&doi=10.1016%2fj.jbiotec.2017.05.016&partnerID=40&md5=ebc5f6e097ac34a0257fddcdddb59840","Background In recent years, several mass spectrometry-based omics technologies emerged to investigate qualitative and quantitative changes within thousands of biologically active components such as proteins, lipids and metabolites. The research enabled through these methods potentially contributes to the diagnosis and pathophysiology of human diseases as well as to the clarification of structures and interactions between biomolecules. Simultaneously, technological advances in the field of mass spectrometry leading to an ever increasing amount of data, demand high standards in efficiency, accuracy and reproducibility of potential analysis software. Results This article presents the current state and ongoing developments in OpenMS, a versatile open-source framework aimed at enabling reproducible analyses of high-throughput mass spectrometry data. It provides implementations of frequently occurring processing operations on MS data through a clean application programming interface in C++ and Python. A collection of 185 tools and ready-made workflows for typical MS-based experiments enable convenient analyses for non-developers and facilitate reproducible research without losing flexibility. Conclusions OpenMS will continue to increase its ease of use for developers as well as users with improved continuous integration/deployment strategies, regular trainings with updated training materials and multiple sources of support. The active developer community ensures the incorporation of new features to support state of the art research. © 2017 The Authors","Analysis workflows; Mass spectrometry; Reproducible research; Software libraries; Tool collection","Application programming interfaces (API); C++ (programming language); Computer programming; Diagnosis; Mass spectrometry; Open source software; Continuous integrations; Mass spectrometry data; Open source frameworks; Processing operations; Reproducible research; Software libraries; Technological advances; Work-flows; Spectrometry; Article; bioinformatics; computer analysis; computer interface; data analysis software; data mining; documentation; machine learning; mass spectrometry; priority journal; workflow",2-s2.0-85020715888
"Viegas E.K., Santin A.O., Oliveira L.S.","Toward a reliable anomaly-based intrusion detection in real-world environments",2017,"Computer Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028339927&doi=10.1016%2fj.comnet.2017.08.013&partnerID=40&md5=56b138e0efa4f052e9e4179e957eea09","A popular approach for detecting network intrusion attempts is to monitor the network traffic for anomalies. Extensive research effort has been invested in anomaly-based network intrusion detection using machine learning techniques; however, in general these techniques remain a research topic, rarely being used in real-world environments. In general, the approaches proposed in the literature lack representative datasets and reliable evaluation methods that consider real-world network properties during the system evaluation. In general, the approaches adopt a set of assumptions about the training data, as well as about the validation methods, rendering the created system unreliable for open-world usage. This paper presents a new method for creating intrusion databases. The objective is that the databases should be easy to update and reproduce with real and valid traffic, representative, and publicly available. Using our proposed method, we propose a new evaluation scheme specific to the machine learning intrusion detection field. Sixteen intrusion databases were created, and each of the assumptions frequently adopted in studies in the intrusion detection literature regarding network traffic behavior was validated. To make machine learning detection schemes feasible, we propose a new multi-objective feature selection method that considers real-world network properties. The results show that most of the assumptions frequently applied in studies in the literature do not hold when using a machine learning detection scheme for network-based intrusion detection. However, the proposed multi-objective feature selection method allows the system accuracy to be improved by considering real-world network properties during the model creation process. © 2017 Elsevier B.V.","Anomaly-based classifier; Intrusion databases; Machine learning-based intrusion detection; Multi-objective feature selection","Artificial intelligence; Classification (of information); Database systems; Feature extraction; Learning systems; Mercury (metal); Anomaly-based intrusion detection; Feature selection methods; Machine learning techniques; Multi objective; Network intrusion detection; Network-based intrusion detection; Real world environments; Reliable evaluation method; Intrusion detection",2-s2.0-85028339927
"Gómez S.E., Martínez B.C., Sánchez-Esguevillas A.J., Hernández Callejo L.","Ensemble network traffic classification: Algorithm comparison and novel ensemble scheme proposal",2017,"Computer Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030660345&doi=10.1016%2fj.comnet.2017.07.018&partnerID=40&md5=49611bf7ee2321b3a41cc8046799ee29","Network Traffic Classification (NTC) is a key piece for network monitoring, Quality-of-Service management and network security. Machine Learning algorithms have drawn the attention of many researchers during the last few years as a promising solution for network traffic classification. In Machine Learning, ensemble algorithms are classifiers formed by a set of base estimators that cooperate to build more complex models according to given training and classification strategies. Resulting models normally exhibit significant accuracy improvements compared to single estimators, but also extra time cost, which may obstruct the application of these methods to online NTC. This paper studies and compares the performance of seven popular ensemble algorithms based on Decision Trees, focusing on model accuracy, byte accuracy, and latency to determine whether ensemble learning can be properly applied to this modeling task. We show that some of the studied algorithms overcome single Decision Tree in terms of model accuracy and byte accuracy. However, the notable latency increase hinders the application of these methods in real time contexts. Additionally, we introduce a novel ensemble classifier that exploits the imbalanced populations presented in traffic networks datasets to achieve faster classifications. The experimental results show that our scheme retains the accuracy improvements of ensemble methods but with low latency punishment, enhancing the prospect of ensembles methods for online network traffic classification. © 2017 Elsevier B.V.",,"Artificial intelligence; Classification (of information); Data mining; Decision trees; Learning systems; Network security; Quality of service; Telecommunication traffic; Accuracy Improvement; Algorithm comparison; Ensemble algorithms; Ensemble classifiers; Ensemble learning; Ensemble networks; Network Monitoring; Network traffic classification; Learning algorithms",2-s2.0-85030660345
"Karimpanal T.G., Wilhelm E.","Identification and off-policy learning of multiple objectives using adaptive clustering",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021774400&doi=10.1016%2fj.neucom.2017.04.074&partnerID=40&md5=7cd19cbed9ceec97e6d4f54ff10d36b5","In this work, we present a methodology that enables an agent to make efficient use of its exploratory actions by autonomously identifying possible objectives in its environment and learning them in parallel. The identification of objectives is achieved using an online and unsupervised adaptive clustering algorithm. The identified objectives are learned (at least partially) in parallel using Q−learning. Using a simulated agent and environment, it is shown that the converged or partially converged value function weights resulting from off-policy learning can be used to accumulate knowledge about multiple objectives without any additional exploration. We claim that the proposed approach could be useful in scenarios where the objectives are initially unknown or in real world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed. © 2017 Elsevier B.V.","Adaptive clustering; Multiobjective learning; Off-policy; Q-learning; Reinforcement learning","Autonomous agents; Clustering algorithms; Reinforcement learning; Adaptive clustering; Adaptive clustering algorithms; Multi-objective learning; Multiple-objectives; Q-learning; Real-world scenario; Simulated agents; Value functions; Education; adaptive clustering; algorithm; Article; cluster analysis; controlled study; limit of quantitation; machine learning; normal distribution; off policy learning; priority journal; probability; reinforcement; simulation",2-s2.0-85021774400
"Mannion P., Devlin S., Mason K., Duggan J., Howley E.","Policy invariance under reward transformations for multi-objective reinforcement learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023761455&doi=10.1016%2fj.neucom.2017.05.090&partnerID=40&md5=edcfc572092078ef87028bc4e12a2a0c","Reinforcement Learning (RL) is a powerful and well-studied Machine Learning paradigm, where an agent learns to improve its performance in an environment by maximising a reward signal. In multi-objective Reinforcement Learning (MORL) the reward signal is a vector, where each component represents the performance on a different objective. Reward shaping is a well-established family of techniques that have been successfully used to improve the performance and learning speed of RL agents in single-objective problems. The basic premise of reward shaping is to add an additional shaping reward to the reward naturally received from the environment, to incorporate domain knowledge and guide an agent's exploration. Potential-Based Reward Shaping (PBRS) is a specific form of reward shaping that offers additional guarantees. In this paper, we extend the theoretical guarantees of PBRS to MORL problems. Specifically, we provide theoretical proof that PBRS does not alter the true Pareto front in both single- and multi-agent MORL. We also contribute the first published empirical studies of the effect of PBRS in single- and multi-agent MORL problems. © 2017 Elsevier B.V.","Multi-agent systems; Multi-objective; Potential-based; Reinforcement learning; Reward shaping","Education; Reinforcement learning; Domain knowledge; Empirical studies; Learning speed; Multi objective; Potential-based; Reward shaping; Single objective; Theoretical guarantees; Multi agent systems; accuracy; Article; calculation; learning algorithm; linear system; machine learning; online system; policy; prediction; priority journal; probability; reinforcement; reward; theoretical study",2-s2.0-85023761455
"Ruiz-Montiel M., Mandow L., Pérez-de-la-Cruz J.-L.","A temporal difference method for multi-objective reinforcement learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021340135&doi=10.1016%2fj.neucom.2016.10.100&partnerID=40&md5=96bcf2bab2134268a18aa60ccef26f43","This work describes MPQ-learning, an algorithm that approximates the set of all deterministic non-dominated policies in multi-objective Markov decision problems, where rewards are vectors and each component stands for an objective to maximize. MPQ-learning generalizes directly the ideas of Q-learning to the multi-objective case. It can be applied to non-convex Pareto frontiers and finds both supported and unsupported solutions. We present the results of the application of MPQ-learning to some benchmark problems. The algorithm solves successfully these problems, so showing the feasibility of this approach. We also compare MPQ-learning to a standard linearization procedure that computes only supported solutions and show that in some cases MPQ-learning can be as effective as the scalarization method. © 2017 Elsevier B.V.","MOMDPs; Multi-objective optimization; Q-learning; Reinforcement learning","Benchmarking; Education; Multiobjective optimization; Optimization; Bench-mark problems; Linearization procedures; Markov decision problem; MOMDPs; Pareto frontiers; Q-learning; Scalarization method; Temporal difference methods; Reinforcement learning; accuracy; algorithm; Article; feasibility study; linear system; machine learning; Markov chain; nonlinear system; policy; priority journal; probability; reinforcement",2-s2.0-85021340135
"Drugan M., Wiering M., Vamplew P., Chetty M.","Special issue on multi-objective reinforcement learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021442254&doi=10.1016%2fj.neucom.2017.06.020&partnerID=40&md5=ca233a8d718682e1971c74d016c2661d","Many real-life problems involve dealing with multiple objectives. For example, in network routing the criteria may consist of energy consumption, latency, and channel capacity, which are in essence conflicting objectives. As in many problems there may be multiple (conflicting) objectives, there usually does not exist a single optimal solution. In those cases, it is desirable to obtain a set of trade-off solutions between the objectives. This problem has in the last decade also gained the attention of many researchers in the field of reinforcement learning (RL). RL addresses sequential decision problems in initially (possibly) unknown stochastic environments. The goal is the maximization of the agent's reward in an environment that is not always completely observable. The purpose of this special issue is to obtain a broader picture on the algorithmic techniques at the confluence between multi-objective optimization and reinforcement learning. The growing interest in multi-objective reinforcement learning (MORL) was reflected in the quantity and quality of submissions received for this special issue. After a rigorous review process, seven papers were accepted for publication, and they reflect the diversity of research being carried out within this emerging field of research. The accepted papers consider many different aspects of algorithmic design and the evaluation and this editorial puts them in a unified framework. © 2017 Elsevier B.V.",,"Education; Energy utilization; Multiobjective optimization; Stochastic systems; Algorithmic design; Algorithmic techniques; Conflicting objectives; Multiple-objectives; Optimal solutions; Real-life problems; Sequential decisions; Stochastic environment; Reinforcement learning; decision making; Editorial; information processing; priority journal; problem solving; process optimization; qualitative research; quantitative study; reinforcement; reward; stochastic model; task performance; unsupervised machine learning",2-s2.0-85021442254
"Parisi S., Pirotta M., Peters J.","Manifold-based multi-objective policy search with sample reuse",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021240639&doi=10.1016%2fj.neucom.2016.11.094&partnerID=40&md5=31f45a6b2c49b0b8d138e921505cfd38","Many real-world applications are characterized by multiple conflicting objectives. In such problems optimality is replaced by Pareto optimality and the goal is to find the Pareto frontier, a set of solutions representing different compromises among the objectives. Despite recent advances in multi-objective optimization, achieving an accurate representation of the Pareto frontier is still an important challenge. Building on recent advances in reinforcement learning and multi-objective policy search, we present two novel manifold-based algorithms to solve multi-objective Markov decision processes. These algorithms combine episodic exploration strategies and importance sampling to efficiently learn a manifold in the policy parameter space such that its image in the objective space accurately approximates the Pareto frontier. We show that episode-based approaches and importance sampling can lead to significantly better results in the context of multi-objective reinforcement learning. Evaluated on three multi-objective problems, our algorithms outperform state-of-the-art methods both in terms of quality of the learned Pareto frontier and sample efficiency. © 2017 Elsevier B.V.","Black-box optimization; Importance sampling; Multi-objective; Policy search; Reinforcement learning","Education; Importance sampling; Markov processes; Optimization; Pareto principle; Reinforcement learning; Black-box optimization; Conflicting objectives; Exploration strategies; Markov Decision Processes; Multi objective; Multi-objective problem; Policy search; State-of-the-art methods; Multiobjective optimization; algorithm; Article; controlled study; hidden Markov model; machine learning; policy search; priority journal; reinforcement learning; stratified sample",2-s2.0-85021240639
"Brys T., Harutyunyan A., Vrancx P., Nowé A., Taylor M.E.","Multi-objectivization and ensembles of shapings in reinforcement learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021786936&doi=10.1016%2fj.neucom.2017.02.096&partnerID=40&md5=2da175d165db8b7da361bea4254cd769","Ensemble techniques are a powerful approach to creating better decision makers in machine learning. Multiple decision makers are trained to solve a given task, grouped in an ensemble, and their decisions are aggregated. The ensemble derives its power from the diversity of its components, as the assumption is that they make mistakes on different inputs, and that the majority is more likely to be correct than any individual component. Diversity usually comes from the different algorithms employed by the decision makers, or the different inputs used to train the decision makers. We advocate a third way to achieve this diversity, called diversity of evaluation, using the principle of multi-objectivization. This is the process of taking a single-objective problem and transforming it into a multi-objective problem in order to solve the original problem faster and/or better. This is either done through decomposition of the original objective, or the addition of extra objectives, typically based on some (heuristic) domain knowledge. This process basically creates a diverse set of feedback signals for what is underneath still a single-objective problem. In the context of ensemble techniques, these various ways to evaluate a (solution to a) problem allow different components of the ensemble to look at the problem in different ways, generating the necessary diversity for the ensemble. In this paper, we argue for the combination of multi-objectivization and ensemble techniques as a powerful tool to boost solving performance in reinforcement learning. We inject various pieces of heuristic information through reward shaping, creating several distinct enriched reward signals, which can strategically be combined using ensemble techniques to reduce sample complexity. We provide theoretical guarantees and demonstrate the potential of the approach with a range of experiments. © 2017 Elsevier B.V.","Ensemble techniques; Multi-objectivization; Reinforcement learning; Reward shaping","Decision making; Education; Reinforcement learning; Ensemble techniques; Heuristic information; Individual components; Multi-objective problem; Multi-objectivization; Reward shaping; Solving performance; Theoretical guarantees; Problem solving; Article; decision making; ensemble technique; heuristics; learning algorithm; limit of quantitation; measurement accuracy; multi objectivization; priority journal; process optimization; reinforcement; reward; task performance",2-s2.0-85021786936
"Kessler A., Dankwa S., Bernabeu M., Harawa V., Danziger S.A., Duffy F., Kampondeni S.D., Potchen M.J., Dambrauskas N., Vigdorovich V., Oliver B.G., Hochman S.E., Mowrey W.B., MacCormick I.J.C., Mandala W.L., Rogerson S.J., Sather D.N., Aitchison J.D., Taylor T.E., Seydel K.B., Smith J.D., Kim K.","Linking EPCR-Binding PfEMP1 to Brain Swelling in Pediatric Cerebral Malaria",2017,"Cell Host and Microbe",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032590759&doi=10.1016%2fj.chom.2017.09.009&partnerID=40&md5=470f09b0b1f287b18582513a14d27410","Brain swelling is a major predictor of mortality in pediatric cerebral malaria (CM). However, the mechanisms leading to swelling remain poorly defined. Here, we combined neuroimaging, parasite transcript profiling, and laboratory blood profiles to develop machine-learning models of malarial retinopathy and brain swelling. We found that parasite var transcripts encoding endothelial protein C receptor (EPCR)-binding domains, in combination with high parasite biomass and low platelet levels, are strong indicators of CM cases with malarial retinopathy. Swelling cases presented low platelet levels and increased transcript abundance of parasite PfEMP1 DC8 and group A EPCR-binding domains. Remarkably, the dominant transcript in 50% of swelling cases encoded PfEMP1 group A CIDRα1.7 domains. Furthermore, a recombinant CIDRα1.7 domain from a pediatric CM brain autopsy inhibited the barrier-protective properties of EPCR in human brain endothelial cells in vitro. Together, these findings suggest a detrimental role for EPCR-binding CIDRα1 domains in brain swelling. Brain swelling is associated with cerebral malaria mortality, but the parasite and host factors responsible for development of brain swelling are unknown. Kessler et al. demonstrate an association of low platelet count and EPCR-binding PfEMP1 with brain swelling in children with cerebral malaria. © 2017 Elsevier Inc.","brain swelling; cerebral malaria; EPCR; PfEMP1; Plasmodium falciparum; var",,2-s2.0-85032590759
"Liu J., Osadchy M., Ashton L., Foster M., Solomon C.J., Gibson S.J.","Deep convolutional neural networks for Raman spectrum recognition: A unified solution",2017,"Analyst",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032280695&doi=10.1039%2fc7an01371j&partnerID=40&md5=41e6c7a5b343c13e798281786af04044","Machine learning methods have found many applications in Raman spectroscopy, especially for the identification of chemical species. However, almost all of these methods require non-trivial preprocessing such as baseline correction and/or PCA as an essential step. Here we describe our unified solution for the identification of chemical species in which a convolutional neural network is trained to automatically identify substances according to their Raman spectrum without the need for preprocessing. We evaluated our approach using the RRUFF spectral database, comprising mineral sample data. Superior classification performance is demonstrated compared with other frequently used machine learning algorithms including the popular support vector machine method. © 2017 The Royal Society of Chemistry.",,,2-s2.0-85032280695
"Jin W., Liu Y., Gao Z.","Fast property prediction in an industrial rubber mixing process with local ELM model",2017,"Journal of Applied Polymer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021233258&doi=10.1002%2fapp.45391&partnerID=40&md5=3f2989ba299679094c47707372352217","Online property prediction in industrial rubber mixing processes is not an easy task. An efficient data-driven prediction model is developed in this work. The regularized extreme learning machine (RELM) is utilized as the fundamental soft sensor model. To better capture distinguished characteristics in multiple recipes and operating modes, a just-in-time RELM modeling method is developed. The number of hidden neurons and the value of regularization parameter of the just-in-time RELM model can be efficiently selected using a fast leave-one-out strategy. Consequently, without the time-consuming laboratory analysis process, the Mooney viscosity can be online predicted once a mixing batch has been discharged. The industrial Mooney viscosity prediction results show its better prediction performance in comparison with traditional approaches. © 2017 Wiley Periodicals, Inc. J. Appl. Polym. Sci. 2017, 134, 45391. © 2017 Wiley Periodicals, Inc.","extreme learning machine; just-in-time learning; Mooney viscosity; rubber mixing process; soft sensor","Forecasting; Knowledge acquisition; Learning systems; Mixing; Rubber; Viscosity; Extreme learning machine; Just-in-time learning; Mooney viscosity; Rubber mixing process; Soft sensors; Education",2-s2.0-85021233258
"Gupta M., Asadullah A., Padmanabhuni S., Serebrenik A.","Reducing user input requests to improve IT support ticket resolution process",2017,"Empirical Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032791476&doi=10.1007%2fs10664-017-9532-2&partnerID=40&md5=4768425c924fde4bd01137dc0ecfe14b","Management and maintenance of IT infrastructure resources such as hardware, software and network is an integral part of software development and maintenance projects. Service management ensures that the tickets submitted by users, i.e. software developers, are serviced within the agreed resolution times. Failure to meet those times induces penalty on the service provider. To prevent a spurious penalty on the service provider, non-working hours such as waiting for user inputs are not included in the measured resolution time, that is, a service level clock pauses its timing. Nevertheless, the user interactions slow down the resolution process, that is, add to user experienced resolution time and degrade user experience. Therefore, this work is motivated by the need to analyze and reduce user input requests in tickets’ life cycle. To address this problem, we analyze user input requests and investigate their impact on user experienced resolution time. We distinguish between input requests of two types: real, seeking information from the user to process the ticket and tactical, when no information is asked but the user input request is raised merely to pause the service level clock. Next, we propose a system that preempts a user at the time of ticket submission to provide additional information that the analyst, a person responsible for servicing the ticket, is likely to ask, thus reducing real user input requests. Further, we propose a detection system to identify tactical user input requests. To evaluate the approach, we conducted a case study in a large global IT company. We observed that around 57% of the tickets have user input requests in the life cycle, causing user experienced resolution time to be almost twice as long as the measured service resolution time. The proposed preemptive system preempts the information needs with an average accuracy of 94– 99% across five cross validations while traditional approaches such as logistic regression and naive Bayes have accuracy in the range of 50– 60%. The detection system identifies around 15% of the total user input requests as tactical. Therefore, the proposed solution can efficiently bring down the number of user input requests and, hence, improve the user-experienced resolution time. © 2017 Springer Science+Business Media, LLC","Machine learning; Process mining; Service level agreement; Software process; Ticket resolution time","Clocks; Learning algorithms; Learning systems; Life cycle; Logistic regressions; Process mining; Resolution time; Service Level Agreements; Service resolutions; Software development and maintenances; Software process; Traditional approaches; Software design",2-s2.0-85032791476
"Aswani R., Kar A.K., Vigneswara Ilavarasan P.","Detection of Spammers in Twitter marketing: A Hybrid Approach Using Social Media Analytics and Bio Inspired Computing",2017,"Information Systems Frontiers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032808258&doi=10.1007%2fs10796-017-9805-8&partnerID=40&md5=a13f94d1c4c323259099f03d1afb9d83","Customer engagement is drastically improved through Web 2.0 technologies, especially social media platforms like Twitter. These platforms are often used by organizations for marketing, of which creation of numerous spam profiles for content promotion is common. The present paper proposes a hybrid approach for identifying the spam profiles by combining social media analytics and bio inspired computing. It adopts a modified K-Means integrated Levy flight Firefly Algorithm (LFA) with chaotic maps as an extension to Firefly Algorithm (FA) for spam detection in Twitter marketing. A total of 18,44,701 tweets have been analyzed from 14,235 Twitter profiles on 13 statistically significant factors derived from social media analytics. A Fuzzy C-Means Clustering approach is further used to identify the overlapping users in two clusters of spammers and non-spammers. Six variants of K-Means integrated FA including chaotic maps and levy flights are tested. The findings indicate that FA with chaos for tuning attractiveness coefficient using Gauss Map converges to a working solution the fastest. Further, LFA with chaos for tuning the absorption coefficient using sinusoidal map outperforms the rest of the approaches in terms of accuracy. © 2017 Springer Science+Business Media, LLC","Bio inspired computing; Firefly algorithm; Machine learning; Social media analytics; Spam detection; Twitter analytics","Bioluminescence; Chaotic systems; Commerce; Learning systems; Lyapunov methods; Marketing; Optimization; Bio-inspired computing; Firefly algorithms; Social media analytics; Spam detection; Twitter analytics; Social networking (online)",2-s2.0-85032808258
"Martínez-Arzate S.G., Tenorio-Borroto E., Barbabosa Pliego A., Díaz-Albiter H.M., Vázquez-Chagoyán J.C., González-Díaz H.","PTML Model for Proteome Mining of B-Cell Epitopes and Theoretical-Experimental Study of Bm86 Protein Sequences from Colima, Mexico",2017,"Journal of Proteome Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032788062&doi=10.1021%2facs.jproteome.7b00477&partnerID=40&md5=8790ef5b28832ae5b8a661b2e88fae7c","In this work, we developed a general perturbation theory and machine learning method for data mining of proteomes to discover new B-cell epitopes useful for vaccine design. The method predicts the epitope activity ϵq(cqj) of one query peptide (q-peptide) under a set of experimental query conditions (cqj). The method uses as input the sequence of the q-peptide. The method also uses as input information about the sequence and epitope activity ϵr(crj) of a peptide of reference (r-peptide) assayed under similar experimental conditions (crj). The model proposed here is able to classify 1raquo;048raquo;190 pairs of query and reference peptide sequences from the proteome of many organisms reported on IEDB database. These pairs have variations (perturbations) under sequence or assay conditions. The model has accuracy, sensitivity, and specificity between 71 and 80% for training and external validation series. The retrieved information contains structural changes in 83raquo;683 peptides sequences (Seq) determined in experimental assays with boundary conditions involving 1448 epitope organisms (Org), 323 host organisms (Host), 15 types of in vivo process (Proc), 28 experimental techniques (Tech), and 505 adjuvant additives (Adj). Afterward, we reported the experimental sampling, isolation, and sequencing of 15 complete sequences of Bm86 gene from state of Colima, Mexico. Last, we used the model to predict the epitope immunogenic scores under different experimental conditions for the 26raquo;112 peptides obtained from these sequences. The model may become a useful tool for epitope selection toward vaccine design. The theoretical-experimental results on Bm86 protein may help the future design of a new vaccine based on this protein. © 2017 American Chemical Society.","B-cell epitope; Bm86 protein; epitope prediction; machine learning; PCR; perturbation theory; proteome mining",,2-s2.0-85032788062
"Liu Z., Ma C., Gao C., Yang H., Lan R., Luo X.","Cost-sensitive collaborative representation based classification via probability estimation with addressing the class imbalance",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032817842&doi=10.1007%2fs11042-017-5359-5&partnerID=40&md5=50764e2621cd24dff53c4ff8d569c899","Collaborative representation has been successfully used in pattern recognition and machine learning. However, most existing collaborative representation classification methods are to achieve the highest classification accuracy, assuming the same losses for different misclassifications. This assumption, however, may not hold in many real-word applications as different types of misclassification could lead to different losses. Meanwhile, the class distribution of data is highly imbalanced in real-world applications. To address this problem, a novel Cost-Sensitive Collaborative Representation based Classification (CSCRC) method via Probability Estimation with Addressing the Class Imbalance was proposed. Unlike traditional methods, the class label of test samples is predicted by minimizing the misclassification losses which are obtained via computing the posterior probabilities. In this paper, a Gaussian function was defined as a probability distribution of collaborative representation coefficient vector and the probability distribution was transformed into collaborative representation framework via logarithmic operator. The experiments show that our proposed method performs competitively compared with existing methods. © 2017 Springer Science+Business Media, LLC","Collaborative representation; Cost-sensitive learning; Loss function; Probability estimate","Cost benefit analysis; Cost estimating; Costs; Learning systems; Pattern recognition; Probability; Classification accuracy; Classification methods; Collaborative representations; Cost-sensitive learning; Loss functions; Posterior probability; Probability estimate; Probability estimation; Probability distributions",2-s2.0-85032817842
"Wang S., Hou Y., Li Z., Dong J., Tang C.","Combining ConvNets with hand-crafted features for action recognition based on an HMM-SVM classifier",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032786521&doi=10.1007%2fs11042-017-5335-0&partnerID=40&md5=24c0437eb0fc58f74505e64a9292278f","This paper proposes a new framework for RGB-D-based action recognition that takes advantages of hand-designed features from skeleton data and deeply learned features from depth maps, and exploits effectively both the local and global temporal information. Specifically, depth and skeleton data are firstly augmented for deep learning and making the recognition insensitive to view variance. Secondly, depth sequences are segmented using the handcrafted features based on skeleton joints motion histogram to exploit the local temporal information. All training segments are clustered using an Infinite Gaussian Mixture Model (IGMM) through Bayesian estimation and labelled for training Convolutional Neural Networks (ConvNets) on the depth maps. Thus, a depth sequence can be reliably encoded into a sequence of segment labels. Finally, the sequence of labels is fed into a joint Hidden Markov Model and Support Vector Machine (HMM-SVM) classifier to explore the global temporal information for final recognition. The proposed framework was evaluated on the widely used MSRAction-Pairs, MSRDailyActivity3D and UTD-MHAD datasets and achieved promising results. © 2017 Springer Science+Business Media, LLC","Action recognition; Convolutional neural networks; Hidden Markov model; Support vector machine","Bayesian networks; Classification (of information); Convolution; Gaussian distribution; Hidden Markov models; Markov processes; Musculoskeletal system; Neural networks; Support vector machines; Trellis codes; Action recognition; Bayesian estimations; Convolutional neural network; Infinite Gaussian mixture models; Motion histograms; Skeleton joints; SVM classifiers; Temporal information; Palmprint recognition",2-s2.0-85032786521
"Zhao G., Liu J., Jiang J., Wang W.","A deep cascade of neural networks for image inpainting, deblurring and denoising",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032825197&doi=10.1007%2fs11042-017-5320-7&partnerID=40&md5=1b722affdc56d3c0080e2eed63467614","In recent years, we have witnessed the great success of deep learning on various problems both in low and high-level computer visions. The low-level vision problems, including inpainting, deblurring, denoising, super-resolution, and so on, are highly anticipated to occur in machine vision and image processing. Many deep learning based methods have been proposed to solve low-level vision problems. Most researches treat these problems independently; however, most of the time they appear concurrently. Motivated by the success of generative model in the field of image generation, we develop a deep cascade of neural networks to solve the inpainting, deblurring, denoising problems at the same time. Our model contains two networks: inpainting GAN and deblurring-denoising network. Inpainting GAN generates the coarse patches to fill the lost part in damaged image, and the deblurring-denoising network, stacked by a convolutional auto-encoder, will further refine them. Unlike other methods that handle each problem separately, our method jointly optimizes the two sub-networks. Because GAN training is not only unstable but also difficult, we adopt the Wasserstein distance as the loss function of the inpainting GAN and propose a gradual training strategy. Learning from the idea of residual learning, we utilize skip connections to pass image details from input to reconstruction layer. Experimental results have demonstrated that the proposed model can achieve state-of-the-art performance. Through the experiments, we also demonstrated the effectiveness of the cascade architecture. © 2017 Springer Science+Business Media, LLC","Auto-encoder; Deblurring; Denoising; Inpainting; Wasserstein GAN","Computer vision; Deep learning; Deep neural networks; Image processing; Learning systems; Signal encoding; Auto encoders; De-noising; Deblurring; Inpainting; Wasserstein GAN; Image enhancement",2-s2.0-85032825197
"Rivera R., Wang J., Yu X., Demirkan G., Hopper M., Bian X., Tahsin T., Magee D.M., Qiu J., LaBaer J., Wallstrom G.","Automatic Identification and Quantification of Extra-Well Fluorescence in Microarray Images",2017,"Journal of Proteome Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032818945&doi=10.1021%2facs.jproteome.7b00267&partnerID=40&md5=5af526e5b4c31f97afbf45be33fe720d","In recent studies involving NAPPA microarrays, extra-well fluorescence is used as a key measure for identifying disease biomarkers because there is evidence to support that it is better correlated with strong antibody responses than statistical analysis involving intraspot intensity. Because this feature is not well quantified by traditional image analysis software, identification and quantification of extra-well fluorescence is performed manually, which is both time-consuming and highly susceptible to variation between raters. A system that could automate this task efficiently and effectively would greatly improve the process of data acquisition in microarray studies, thereby accelerating the discovery of disease biomarkers. In this study, we experimented with different machine learning methods, as well as novel heuristics, for identifying spots exhibiting extra-well fluorescence (rings) in microarray images and assigning each ring a grade of 1-5 based on its intensity and morphology. The sensitivity of our final system for identifying rings was found to be 72% at 99% specificity and 98% at 92% specificity. Our system performs this task significantly faster than a human, while maintaining high performance, and therefore represents a valuable tool for microarray image analysis. © 2017 American Chemical Society.","bioinformatics; biomarker; image analysis; nucleic acid programmable protein array (NAPPA); protein array",,2-s2.0-85032818945
"Dippel A.","The Big Data Game: On the Ludic Constitution of the Collaborative Production of Knowledge in High-Energy Physics at CERN [Das Big Data Game: Zur spielerischen Konstitution kollaborativer Wissensproduktion in der Hochenergiephysik am CERN]",2017,"NTM International Journal of History and Ethics of Natural Sciences, Technology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032812860&doi=10.1007%2fs00048-017-0181-8&partnerID=40&md5=9e7d6e828c68ba2e2c5d99e82ea3c832","This article looks at how games and play contribute to the big data-driven production of knowledge in High-Energy Physics, with a particular focus on the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN), where the author has been conducting anthropological fieldwork since 2014. The ludic (playful) aspect of knowledge production is analyzed here in three different dimensions: the Symbolic, the Ontological, and the Epistemic. The first one points towards CERN as place where a cosmological game of probability is played with the help of Monte-Carlo simulations. The second one can be seen in the agonistic infrastructures of competing experimental collaborations. The third dimension unfolds in ludic platforms, such as online Challenges and citizen science games, which contribute to the development of machine learning algorithms, whose function is necessary in order to process the huge amount of data gathered from experimental events. Following Clifford Geertz, CERN itself is characterized as a site of deep play, a concept that contributes to understanding wider social and cultural orders through the analysis of ludic collective phenomena. The article also engages with Peter Galison’s idea of the trading zone, proposing to comprehend it in the age of big data as a Playground. Thus the author hopes to contribute to a wider discussion in the historiographical and social study of science and technology, as well as in cultural anthropology, by recognizing the ludic in science as a central element of understanding collaborative knowledge production. © 2017 Springer International Publishing AG","Big data; Collaboration; Competition; Knowledge production; Play; Science and technology studies",,2-s2.0-85032812860
"Kang F., Li J.-S., Wang Y., Li J.","Extreme learning machine-based surrogate model for analyzing system reliability of soil slopes",2017,"European Journal of Environmental and Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964027586&doi=10.1080%2f19648189.2016.1169225&partnerID=40&md5=13c76b37ed6a2d9846aeeb2aee21e3ac","Geotechnical engineering problems are characterised by many sources of uncertainty, and reliability analysis is needed to take the uncertainties into account. An intelligent surrogate model based on extreme learning machine is proposed for slope system reliability analysis. The weights and bias which play an important role in the performance of ELM are optimised by a nature inspired artificial bee colony algorithm. The system failure probability of soil slopes is estimated by Monte Carlo simulation via the proposed surrogate model. Experimental results show that the proposed method is feasible, effective and simple to implement system reliability analysis of soil slopes. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","artificial bee colony algorithm; extreme learning machine; slope reliability analysis; surrogate model","Evolutionary algorithms; Geotechnical engineering; Intelligent systems; Knowledge acquisition; Learning systems; Monte Carlo methods; Optimization; Reliability; Soils; Systems engineering; Uncertainty analysis; Analysis of soils; Artificial bee colony algorithms; Extreme learning machine; Slope reliability analysis; Sources of uncertainty; Surrogate model; System failure probability; System reliability; Reliability analysis",2-s2.0-84964027586
"Zafeiris D., Vadakekolathu J., Wagner S., Pockley A.G., Ball G.R., Rutella S.","Discovery and application of immune biomarkers for hematological malignancies",2017,"Expert Review of Molecular Diagnostics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031407777&doi=10.1080%2f14737159.2017.1381560&partnerID=40&md5=5245ae84fedcce8e882affded94030d3","Introduction: Hematological malignancies originate and progress in primary and secondary lymphoid organs, where they establish a uniquely immune-suppressive tumour microenvironment. Although high-throughput transcriptomic and proteomic approaches are being employed to interrogate immune surveillance and escape mechanisms in patients with solid tumours, and to identify actionable targets for immunotherapy, our knowledge of the immunological landscape of hematological malignancies, as well as our understanding of the molecular circuits that underpin the establishment of immune tolerance, is not comprehensive. Areas covered: This article will discuss how multiplexed immunohistochemistry, flow cytometry/mass cytometry, proteomic and genomic techniques can be used to dynamically capture the complexity of tumour-immune interactions. Moreover, the analysis of multi-dimensional, clinically annotated data sets obtained from public repositories such as Array Express, TCGA and GEO is crucial to identify immune biomarkers, to inform the rational design of immune therapies and to predict clinical benefit in individual patients. We will also highlight how artificial neural network models and alternative methodologies integrating other algorithms can support the identification of key molecular drivers of immune dysfunction. Expert commentary: High-dimensional technologies have the potential to enhance our understanding of immune-cancer interactions and will support clinical decision making and the prediction of therapeutic benefit from immune-based interventions. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","biomarker; gene expression profiling; Hematological malignancies; immunotherapy; leukemia; lymphoma; multiple myeloma; prognosis","arginase; arginase 2; biological marker; blinatumomab; CD135 antigen; CD200 antigen; cd270 antigen; CD276 antigen; CD48 antigen; chlordane; cyclophosphamide; cytotoxic T lymphocyte antigen 4; Fc receptor IIa; galectin 1; indoleamine 2,3 dioxygenase; indoleamine 2,3 dioxygenase 1; interleukin 1 receptor blocking agent; interleukin 10; interleukin 17; interleukin 21; interleukin 6; kynurenine; lymphocyte antigen; programmed death 1 ligand 1; STAT3 protein; toll like receptor 9; tryptophan; tumor necrosis factor; tumor necrosis factor receptor 2; unclassified drug; vasculotropin C; acute lymphoblastic leukemia; acute myeloid leukemia; algorithm; artificial neural network; Bayesian learning; cancer immunotherapy; cancer prognosis; chronic lymphatic leukemia; chronic myeloid leukemia; clinical decision making; decision tree; diffuse large B cell lymphoma; flow cytometry; follicular lymphoma; genomics; hematologic malignancy; hierarchical clustering; Hodgkin disease; human; immunohistochemistry; immunosuppressive treatment; information processing; k mean clustering; machine learning; mass cytometry; multiple myeloma; nonhodgkin lymphoma; principal component analysis; proteomics; random forest; Review; supervised learning; supervised machine learning; support vector machine; tumor immunity; tumor microenvironment; unsupervised machine learning",2-s2.0-85031407777
"Ghobaei-Arani M., Shamsi M., Rahmanian A.A.","An efficient approach for improving virtual machine placement in cloud computing environment",2017,"Journal of Experimental and Theoretical Artificial Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017174061&doi=10.1080%2f0952813X.2017.1310308&partnerID=40&md5=05e679ba554d16356ce298064f81a883","The ever increasing demand for the cloud services requires more data centres. The power consumption in the data centres is a challenging problem for cloud computing, which has not been considered properly by the data centre developer companies. Especially, large data centres struggle with the power cost and the Greenhouse gases production. Hence, employing the power efficient mechanisms are necessary to optimise the mentioned effects. Moreover, virtual machine (VM) placement can be used as an effective method to reduce the power consumption in data centres. In this paper by grouping both virtual and physical machines, and taking into account the maximum absolute deviation during the VM placement, the power consumption as well as the service level agreement (SLA) deviation in data centres are reduced. To this end, the best-fit decreasing algorithm is utilised in the simulation to reduce the power consumption by about 5% compared to the modified best-fit decreasing algorithm, and at the same time, the SLA violation is improved by 6%. Finally, the learning automata are used to a trade-off between power consumption reduction from one side, and SLA violation percentage from the other side. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Cloud computing; learning automata; power consumption; virtual machine placement; virtualisation","Automata theory; Cloud computing; Data reduction; Economic and social effects; Electric power utilization; Greenhouse gases; Network security; Virtual machine; Best fit decreasing; Cloud computing environments; Learning Automata; Maximum absolute deviations; Power consumption reduction; Service Level Agreements; Virtual machine placements; Virtualisation; Green computing",2-s2.0-85017174061
"Li X., Zhang C., Li W.","Building block level urban land-use information retrieval based on Google Street View images",2017,"GIScience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020716375&doi=10.1080%2f15481603.2017.1338389&partnerID=40&md5=3af199c3d6ea42acf3c7e86c47e25afe","Land-use maps are important references for urban planning and urban studies. Given the heterogeneity of urban land-use types, it is difficult to differentiate different land-use types based on overhead remotely sensed data. Google Street View (GSV) images, which capture the façades of building blocks along streets, could be better used to judge the land-use types of different building blocks based on their façade appearances. Recently developed scene classification algorithms in computer vision community make it possible to categorize different photos semantically based on various image feature descriptors and machine-learning algorithms. Therefore, in this study, we proposed a method to derive detailed land-use information at building block level based on scene classification algorithms and GSV images. Three image feature descriptors (i.e., scale-invariant feature transform-Fisher, histogram of oriented gradients, GIST) were used to represent GSV images of different buildings. Existing land-use maps were used to create training datasets to train support vector machine (SVM) classifiers for categorizing GSV images. The trained SVM classifiers were then applied to case study areas in New York City, Boston, and Houston, to predict the land-use information at building block level. Accuracy assessment results show that the proposed method is suitable for differentiating residential buildings and nonresidential buildings with an accuracy of 85% or so. Since the GSV images are publicly accessible, this proposed method would provide a new way for building block level land-use mapping in future. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","GSV (Google Street View); image features; machine learning; urban land-use mapping","accuracy assessment; algorithm; heterogeneity; image classification; Internet; land use; machine learning; mapping; remote sensing; support vector machine; urban area; urban planning; Boston; Houston; Massachusetts; New York [New York (STT)]; New York [United States]; Texas; United States",2-s2.0-85020716375
"Zhang X., Saitoh S.-I., Hirawake T.","Predicting potential fishing zones of Japanese common squid (Todarodes pacificus) using remotely sensed images in coastal waters of south-western Hokkaido, Japan",2017,"International Journal of Remote Sensing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003881336&doi=10.1080%2f01431161.2016.1266114&partnerID=40&md5=fd95de96ee5488986f51690b02694539","The present study used nighttime visible satellite images to identify the daily presence and absence of the fishing vessel aggregations, targeting the Japanese common squid (Todarodes pacificus) in the coastal waters of south-western Hokkaido, Japan. Here, statistical (generalized additive model (GAM) and generalized linear model (GLM)) and machine learning models (boosted regression tree (BRT)) were developed using a 3-year (2000–2002) presence/absence information from squid fishing aggregations and environmental variables (night-time sea surface temperature (SST), chlorophyll-a (Chl-a) concentration, Kd(490) (diffuse attenuation coefficients of downwelling irradiance at 490 nm), and bathymetry). Our findings showed that BRT outperformed the regression-based models in predicting the potential squid fishing zones during the validation period (2003). Results from BRT indicated that potential fishing zones were closely associated with water depth. Both SST and Chl-a concentration were also found highly influential to squid occurrence, while Kd(490), which is related to the water transparency, showed relatively less impact on the squid distribution. The spatial predictions using daily data from 2000 to 2003 revealed the gradual eastward movement of potential fishing zones between June and December, consistent with the pattern of squid fishing activities. Four experimental fishing surveys were further conducted to validate and improve our model predictions against experience-based fishing surveys. The results showed that the squid catches using our model predictions in 2012 substantially exceeded the average catches of experience-based fishing in 2011. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Artificial intelligence; Atmospheric temperature; Coastal zones; Forecasting; Learning systems; Molluscs; Oceanography; Shellfish; Surface waters; Surveys; Boosted regression trees; Diffuse attenuation coefficients; Environmental variables; Generalized additive model; Generalized linear model; Machine learning models; Remotely sensed images; Sea surface temperature (SST); Fisheries; cephalopod fishery; chlorophyll a; coastal water; environmental factor; prediction; remote sensing; satellite imagery; sea surface temperature; water depth; Hokkaido; Japan; Cephalopoda; Todarodes pacificus",2-s2.0-85003881336
"Nashef A., Rapp H., Nawrot M.P., Prut Y.","Area-specific processing of cerebellar-thalamo-cortical information in primates",2017,"Biological Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032785509&doi=10.1007%2fs00422-017-0738-6&partnerID=40&md5=5e994d70659fd1640484c6f20cea1fc1","The cerebellar-thalamo-cortical (CTC) system plays a major role in controlling timing and coordination of voluntary movements. However, the functional impact of this system on motor cortical sites has not been documented in a systematic manner. We addressed this question by implanting a chronic stimulating electrode in the superior cerebellar peduncle (SCP) and recording evoked multiunit activity (MUA) and the local field potential (LFP) in the primary motor cortex ((Formula presented.)), the premotor cortex ((Formula presented.)) and the somatosensory cortex ((Formula presented.)). The area-dependent response properties were estimated using the MUA response shape (quantified by decomposing into principal components) and the time-dependent frequency content of the evoked LFP. Each of these signals alone enabled good classification between the somatosensory and motor sites. Good classification between the primary motor and premotor areas could only be achieved when combining features from both signal types. Topographical single-site representation of the predicted class showed good recovery of functional organization. Finally, the probability for misclassification had a broad topographical organization. Despite the area-specific response features to SCP stimulation, there was considerable site-to-site variation in responses, specifically within the motor cortical areas. This indicates a substantial SCP impact on both the primary motor and premotor cortex. Given the documented involvement of these cortical areas in preparation and execution of movement, this result may suggest a CTC contribution to both motor execution and motor preparation. The stimulation responses in the somatosensory cortex were sparser and weaker. However, a functional role of the CTC system in somatosensory computation must be taken into consideration. © 2017 Springer-Verlag GmbH Germany","Cerebellum; Local field potential; Machine learning; Motor control; Multiunit activity; Thalamocortical","Brain; Electrophysiology; Learning systems; Mammals; Cerebellum; Local field potentials; Motor control; Multi-unit activity; Thalamocortical; Temperature control",2-s2.0-85032785509
"Rastogi (nee Khemchandani) R., Anand P., Chandra S.","-norm Twin Support Vector Machine-based Regression",2017,"Optimization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028570637&doi=10.1080%2f02331934.2017.1364739&partnerID=40&md5=4897078024e51a5e558a39eb46fe62eb","This paper presents two variants of L1-norm Twin Support Vector Machine-based Regression (L1-norm TWSVR) model. The proposed methods are robust, efficient and own better generalization ability. The first method, termed as L1-norm TWSVR via QPP, results into the solution of a pair of QPPs. L1-norm TWSVR via QPP does not require the inversion of the kernel matrices during the learning process which makes it suitable for the large-scale problems. The second method, termed as L1-norm TWSVR via LPP, results into the solution of a pair of linear programs. The solution vectors of the L1-norm TWSVR via LPP is sparse which increases its prediction speed significantly. The experimental results on several artificial and UCI benchmark data-sets show that the use of L1-norm distances enables the proposed methods to perform better than the existing methods. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","-norm; Regression; Support Vector Machine; Twin Support Vector Machine; Twin Support Vector Regression",,2-s2.0-85028570637
"Jiang Y., Li Y., Yang C., Liu K., Armstrong E.M., Huang T., Moroni D.F., Finch C.J.","A comprehensive methodology for discovering semantic relationships among geospatial vocabularies using oceanographic data discovery as an example",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026542390&doi=10.1080%2f13658816.2017.1357819&partnerID=40&md5=13f3f9ac575fd028fcdab383c3381f82","It is challenging to find relevant data for research and development purposes in the geospatial big data era. One long-standing problem in data discovery is locating, assimilating and utilizing the semantic context for a given query. Most research in the geospatial domain has approached this problem in one of two ways: building a domain-specific ontology manually or discovering automatically, semantic relationships using metadata and machine learning techniques. The former relies on rich expert knowledge but is static, costly and labor intensive, whereas the second is automatic and prone to noise. An emerging trend in information science takes advantage of large-scale user search histories, which are dynamic but subject to user- and crawler-generated noise. Leveraging the benefits of these three approaches and avoiding their weaknesses, a novel methodology is proposed to (1) discover vocabulary-based semantic relationships from user search histories and clickstreams, (2) refine the similarity calculation methods from existing ontologies and (3) integrate the results of ontology, metadata, user search history and clickstream analysis to better determine their semantic relationships. An accuracy assessment by domain experts for the similarity values indicates an 83% overall accuracy for the top 10 related terms over randomly selected sample queries. This research functions as an example for building vocabulary-based semantic relationships for different geographical domains to improve various aspects of data discovery, including the accuracy of the vocabulary relationships of commonly used search terms. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","big data; click behavior; Query augmentation; search history; semantic search; web mining","data mining; Internet; machine learning; metadata; methodology; oceanography; spatial data; World Wide Web",2-s2.0-85026542390
"Sonobe R., Yamaya Y., Tani H., Wang X., Kobayashi N., Mochizuki K.-I.","Assessing the suitability of data from Sentinel-1A and 2A for crop classification",2017,"GIScience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022216039&doi=10.1080%2f15481603.2017.1351149&partnerID=40&md5=d4042949e8f17aba4271a039168303e2","Sentinel-1A C-SAR and Sentinel-2A MultiSpectral Instrument (MSI) provide data applicable to the remote identification of crop type. In this study, six crop types (beans, beetroot, grass, maize, potato, and winter wheat) were identified using five C-SAR images and one MSI image acquired during the 2016 growing season. To assess the potential for accurate crop classification with existing supervised learning models, the four different approaches namely kernel-based extreme learning machine (KELM), multilayer feedforward neural networks, random forests, and support vector machine were compared. Algorithm hyperparameters were tuned using Bayesian optimization. Overall, KELM yielded the highest performance, achieving an overall classification accuracy of 96.8%. Evaluation of the sensitivity of classification models and relative importance of data types using data-based sensitivity analysis showed that the set of VV polarization data acquired on 24 July (Sentinel-1A) and band 4 data (Sentinel-2A) had the greatest potential for use in crop classification. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","agricultural fields; classification; Hokkaido; machine learning; Sentinel-1A; Sentinel-2A","algorithm; Bayesian analysis; crop yield; data acquisition; image classification; machine learning; optimization; satellite data; Sentinel; Hokkaido; Japan; Solanum tuberosum; Triticum aestivum; Zea mays",2-s2.0-85022216039
"Guha P., Bhatnagar T., Pal I., Kamboj U., Mishra S.","Prediction of properties of wheat dough using intelligent deep belief networks",2017,"Journal of Experimental and Theoretical Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021147144&doi=10.1080%2f0952813X.2017.1340976&partnerID=40&md5=ae740aa434b653d783d8433ec4fe9ff9","In this paper, the rheological and chemical properties of wheat dough are predicted using deep belief networks. Wheat grains are stored at controlled environmental conditions. The internal parameters of grains viz., protein, fat, carbohydrates, moisture, ash are determined using standard chemical analysis and viscosity of the dough is measured using Rheometer. Here, fat, carbohydrates, moisture, ash and temperature are considered as inputs whereas protein and viscosity are chosen as outputs. The prediction algorithm is developed using deep neural network where each layer is trained greedily using restricted Boltzmann machine (RBM) networks. The overall network is finally fine-tuned using standard neural network technique. In most literature, it has been found that fine-tuning is done using back-propagation technique. In this paper, a new algorithm is proposed in which each layer is tuned using RBM and the final network is fine-tuned using deep neural network (DNN). It has been observed that with the proposed algorithm, errors between the actual and predicted outputs are less compared to the conventional algorithm. Hence, the given network can be considered as beneficial as it predicts the outputs more accurately. Numerical results along with discussions are presented. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","deep belief networks; deep learning network; proximate analysis; RBM; viscosity; Wheat dough","Carbohydrates; Chemical analysis; Deep learning; Grain (agricultural product); Moisture; Network layers; Proteins; Viscosity; Conventional algorithms; Deep belief networks; Environmental conditions; Learning network; Prediction algorithms; Proximate analysis; Restricted boltzmann machine; Wheat dough; Deep neural networks",2-s2.0-85021147144
"Kramer R., Dell’Amico M., Iori M.","A batching-move iterated local search algorithm for the bin packing problem with generalized precedence constraints",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021888124&doi=10.1080%2f00207543.2017.1341065&partnerID=40&md5=28ec30d8be522bfc6f62c942335b860f","In this paper, we propose a generalisation of the bin packing problem, obtained by adding precedences between items that can assume heterogeneous non-negative integer values. Such generalisation also models the well-known Simple Assembly Line Balancing Problem of type I. To solve the problem, we propose a simple and effective iterated local search algorithm that integrates in an innovative way of constructive procedures and neighbourhood structures to guide the search to local optimal solutions. Moreover, we apply some preprocessing procedures and adapt classical lower bounds from the literature. Extensive computational experiments on benchmark instances suggest that the developed algorithm is able to generate good quality solutions in a reasonable computational time. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","assembly line balancing; batching moves; bin packing; generalised precedence constraints; iterated local search","Assembly; Assembly machines; Benchmarking; Bins; Learning algorithms; Local search (optimization); Assembly line balancing; batching moves; Bin packing; Iterated local search; Precedence constraints; Problem solving",2-s2.0-85021888124
"Liao W., Zhang X., Jiang M.","Multi-objective group scheduling optimization integrated with preventive maintenance",2017,"Engineering Optimization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011919178&doi=10.1080%2f0305215X.2017.1280258&partnerID=40&md5=a3b6b9b6fc5ff968b7abbf02b8d32b11","This article proposes a single-machine-based integration model to meet the requirements of production scheduling and preventive maintenance in group production. To describe the production for identical/similar and different jobs, this integrated model considers the learning and forgetting effects. Based on machine degradation, the deterioration effect is also considered. Moreover, perfect maintenance and minimal repair are adopted in this integrated model. The multi-objective of minimizing total completion time and maintenance cost is taken to meet the dual requirements of delivery date and cost. Finally, a genetic algorithm is developed to solve this optimization model, and the computation results demonstrate that this integrated model is effective and reliable. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","deterioration effect; genetic algorithm; group production; Multi-objective; preventive maintenance","Deterioration; Genetic algorithms; Group technology; Maintenance; Optimization; Production control; Repair; Scheduling; Scheduling algorithms; Integrated modeling; Integration models; Learning and forgetting effects; Multi objective; Optimization modeling; Perfect maintenance; Production Scheduling; Total completion time; Preventive maintenance",2-s2.0-85011919178
"Knaap E.","The Cartography of Opportunity: Spatial Data Science for Equitable Urban Policy",2017,"Housing Policy Debate",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028569851&doi=10.1080%2f10511482.2017.1331930&partnerID=40&md5=4d24dbf4675f4a52107e507027a310ca","As evidence of the contextual effects of place upon individual outcomes has become increasingly solid over time, so too have urban policies and programs designed to connect underserved people with access to spatial opportunity. To this end, many attempts have been made to quantify the geography of opportunity and quite literally plot it on a map by combining evidence from studies on neighborhood effects with spatial data resources and geographic information systems (GIS) technology. Recently, these opportunity maps have not only become increasingly common but their preparation has been encouraged and facilitated by the U.S. Department of Housing and Urban Development. A closer look at the foundations and methods that underlie these exercises offers important lessons I examine the practice of opportunity mapping from both theoretical and methodological perspectives, highlighting several weaknesses of the common methods. Following this, I outline a theoretical framework based on Galster’s categorization of the mechanisms of neighborhood effects. Using data from the Baltimore metropolitan region, I use confirmatory factor analysis to specify a measurement model that verifies the validity of the proposed theoretical framework. The model provides estimates of four latent variables conceived as the essential dimensions of spatial opportunity: social-interactive, environmental, geographic, and institutional. Finally, I develop a neighborhood typology using unsupervised machine learning applied to the four dimensions of opportunity. Results suggest that opportunity mapping can be improved substantially through a better connection to the empirical literature on neighborhood effects, a multivariate statistical framework, and more direct relevance to public policy interventions. © 2017 Virginia Polytechnic Institute and State University.","community development; ecometrics; fair housing; neighborhood effects; Opportunity mapping",,2-s2.0-85028569851
"Martínez-Martínez F., Rupérez-Moreno M.J., Martínez-Sober M., Solves-Llorens J.A., Lorente D., Serrano-López A.J., Martínez-Sanchis S., Monserrat C., Martín-Guerrero J.D.","A finite element-based machine learning approach for modeling the mechanical behavior of the breast tissues under compression in real-time",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030312260&doi=10.1016%2fj.compbiomed.2017.09.019&partnerID=40&md5=30d4a5a2b6c59472d5eb0bc6b924e238","This work presents a data-driven method to simulate, in real-time, the biomechanical behavior of the breast tissues in some image-guided interventions such as biopsies or radiotherapy dose delivery as well as to speed up multimodal registration algorithms. Ten real breasts were used for this work. Their deformation due to the displacement of two compression plates was simulated off-line using the finite element (FE) method. Three machine learning models were trained with the data from those simulations. Then, they were used to predict in real-time the deformation of the breast tissues during the compression. The models were a decision tree and two tree-based ensemble methods (extremely randomized trees and random forest). Two different experimental setups were designed to validate and study the performance of these models under different conditions. The mean 3D Euclidean distance between nodes predicted by the models and those extracted from the FE simulations was calculated to assess the performance of the models in the validation set. The experiments proved that extremely randomized trees performed better than the other two models. The mean error committed by the three models in the prediction of the nodal displacements was under 2 mm, a threshold usually set for clinical applications. The time needed for breast compression prediction is sufficiently short to allow its use in real-time (<0.2 s). © 2017 Elsevier Ltd","Breast biomechanics; Breast compression; Finite element methods; Machine learning; Modeling","Artificial intelligence; Biomechanics; Decision trees; Deformation; Forecasting; Histology; Learning systems; Medical imaging; Models; Tissue; Tissue engineering; Biomechanical behavior; Clinical application; Image-guided Intervention; Machine learning approaches; Machine learning models; Mechanical behavior; Multimodal registration; Tree-based ensembles; Finite element method; adult; aged; Article; biomechanics; breast tissue; clinical practice; compression; decision tree; female; finite element analysis; geometry; gland tissue; human; human experiment; image segmentation; learning algorithm; machine learning; mammography; normal human; priority journal; real time polymerase chain reaction",2-s2.0-85030312260
"Cramer S., Kampouridis M., Freitas A.A., Alexandridis A.K.","An extensive evaluation of seven machine learning methods for rainfall prediction in weather derivatives",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019384458&doi=10.1016%2fj.eswa.2017.05.029&partnerID=40&md5=7674a1d421a7d6f3682e83bba1c9cfc8","Regression problems provide some of the most challenging research opportunities in the area of machine learning, and more broadly intelligent systems, where the predictions of some target variables are critical to a specific application. Rainfall is a prime example, as it exhibits unique characteristics of high volatility and chaotic patterns that do not exist in other time series data. This work's main impact is to show the benefit machine learning algorithms, and more broadly intelligent systems have over the current state-of-the-art techniques for rainfall prediction within rainfall derivatives. We apply and compare the predictive performance of the current state-of-the-art (Markov chain extended with rainfall prediction) and six other popular machine learning algorithms, namely: Genetic Programming, Support Vector Regression, Radial Basis Neural Networks, M5 Rules, M5 Model trees, and k-Nearest Neighbours. To assist in the extensive evaluation, we run tests using the rainfall time series across data sets for 42 cities, with very diverse climatic features. This thorough examination shows that the machine learning methods are able to outperform the current state-of-the-art. Another contribution of this work is to detect correlations between different climates and predictive accuracy. Thus, these results show the positive effect that machine learning-based intelligent systems have for predicting rainfall based on predictive accuracy and with minimal correlations existing across climates. © 2017 Elsevier Ltd","Machine learning; Rainfall; Weather derivatives","Artificial intelligence; Forecasting; Genetic algorithms; Genetic programming; Intelligent systems; Learning systems; Markov processes; Nearest neighbor search; Rain; Time series; K-nearest neighbours; Machine learning methods; Predictive performance; Radial basis neural networks; Research opportunities; State-of-the-art techniques; Support vector regression (SVR); Weather derivatives; Learning algorithms",2-s2.0-85019384458
"Cornejo-Bueno L., Casanova-Mateo C., Sanz-Justo J., Cerro-Prada E., Salcedo-Sanz S.","Efficient Prediction of Low-Visibility Events at Airports Using Machine-Learning Regression",2017,"Boundary-Layer Meteorology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021280127&doi=10.1007%2fs10546-017-0276-8&partnerID=40&md5=1e9c938cf0830dc876fa8f6cb5e2cae5","We address the prediction of low-visibility events at airports using machine-learning regression. The proposed model successfully forecasts low-visibility events in terms of the runway visual range at the airport, with the use of support-vector regression, neural networks (multi-layer perceptrons and extreme-learning machines) and Gaussian-process algorithms. We assess the performance of these algorithms based on real data collected at the Valladolid airport, Spain. We also propose a study of the atmospheric variables measured at a nearby tower related to low-visibility atmospheric conditions, since they are considered as the inputs of the different regressors. A pre-processing procedure of these input variables with wavelet transforms is also described. The results show that the proposed machine-learning algorithms are able to predict low-visibility events well. The Gaussian process is the best algorithm among those analyzed, obtaining over 98% of the correct classification rate in low-visibility events when the runway visual range is > 1000 m, and about 80% under this threshold. The performance of all the machine-learning algorithms tested is clearly affected in extreme low-visibility conditions (< 500 m). However, we show improved results of all the methods when data from a neighbouring meteorological tower are included, and also with a pre-processing scheme using a wavelet transform. Also presented are results of the algorithm performance in daytime and nighttime conditions, and for different prediction time horizons. © 2017, Springer Science+Business Media B.V.","Airports; Algorithms; Fog prediction; Low-visibility events; Machine learning","Airports; Algorithms; Education; Forecasting; Learning systems; Meteorological instruments; Regression analysis; Visibility; Wavelet transforms; Atmospheric conditions; Efficient predictions; Extreme learning machine; Low visibility; Low visibility conditions; Multi-layer perceptrons; Prediction time horizon; Support vector regression (SVR); Learning algorithms; airport; algorithm; fog; machine learning; prediction; regression analysis; visibility; Spain",2-s2.0-85021280127
"ElRafey A., Wojtusiak J.","Recent advances in scaling-down sampling methods in machine learning",2017,"Wiley Interdisciplinary Reviews: Computational Statistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029230224&doi=10.1002%2fwics.1414&partnerID=40&md5=5ebbe8d0cfcb3fc6665c22a987651f80","Data sampling methods have been investigated for decades in the context of machine learning and statistical algorithms, with significant progress made in the past few years driven by strong interest in big data and distributed computing. Most recently, progress has been made in methods that can be broadly categorized into random sampling including density-biased and nonuniform sampling methods; active learning methods, which are a type of semi-supervised learning and an area of intense research; and progressive sampling methods which can be viewed as a combination of the above two approaches. A unified view of scaling-down sampling methods is presented in this article and complemented with descriptions of relevant published literature. WIREs Comput Stat 2017, 9:e1414. doi: 10.1002/wics.1414. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.","data mining; evolutionary computation; health infomratics; machine learning","Artificial intelligence; Big data; Data mining; Distributed computer systems; Evolutionary algorithms; Learning algorithms; Sampling; Signal sampling; Supervised learning; Active learning methods; Data sampling; Nonuniform sampling; Progressive sampling; Random sampling; Scaling down; Semi- supervised learning; Statistical algorithm; Learning systems",2-s2.0-85029230224
"Chen W., Pourghasemi H.R., Kornejady A., Zhang N.","Landslide spatial modeling: Introducing new ensembles of ANN, MaxEnt, and SVM machine learning techniques",2017,"Geoderma",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021392934&doi=10.1016%2fj.geoderma.2017.06.020&partnerID=40&md5=ec09464ddcb0b0c9d5bdcd03a494030e","“Spatial contraindication” is what exactly landslide susceptibility models have been seeking. They are designed for depicting perilous land activities, be it natural or anthropological. To find this pattern, three well-known machine learning models namely maximum entropy (MaxEnt), support vector machine (SVM), and Artificial Neural Network (ANN) were used accompanied by their ensembles (i.e. ANN-SVM, ANN-MaxEnt, ANN-MaxEnt-SVM, and SVM-MaxEnt) in Wanyuan area, China. The models were designed by eleven conditioning factors such as elevation, slope degree, slope aspect, profile and plan curvatures, topographic wetness index, distance to roads, distance to rivers, normalized difference vegetation index (NDVI), land use/land cover (LU/LC), and lithology along with two sets of training (213#) and testing (91#) landslide data. A statistical index (SI) model was implemented to examine the mutual relationship between classes of each factor and the landslide occurrences. Concerning the areal differentiation, the chi-square test was used where SVM and MaxEnt gained the highest and the lowest values, respectively. Afterward, the practicality — as an indicator of producing a focused susceptibility map and addressing highly susceptible classes (IV and V) in a compendious manner with a reduced spatial area — was calculated for models. Accordingly, SVM and MaxEnt were found to be the most and the least practical models having the highest and the lowest spatial area in highly susceptible classes, respectively. The receiver operating characteristic (ROC) curve was used to examine generalization and prediction accuracy of the models. As a result, in the case of validating models separately, ANN gained the highest area under the curve (AUC) with a value of 0.824, followed by SVM (0.819), and MaxEnt (0.75). In the case of validating ensemble models, the ANN-SVM had the highest AUC of all (0.826), followed by ANN-MaxEnt (0.803), SVM-MaxEnt (0.792), and ANN-MaxEnt-SVM (0.811). With regard to the premier model results, three factors namely distance from roads, elevation, and distance from rivers had the highest effect on landslide occurrence. The results of the SI values showed that the spatial combination of the main drivers namely farmlands, − 0.06–0.2 range in NDVI, rocks with inter-bedded limestone and other susceptible classes therein can make at least a prone area of about 30% to landsliding. Such spatial combination of environmental condition and human-made activities can be considered as a contraindication for the residents of the study area, especially at highly susceptible locations. This also addresses areas that further mitigation plans should be taken into account with urgency. © 2017 Elsevier B.V.","ANN; Ensemble models; Maximum entropy; Spatial modeling; SVM","Artificial intelligence; Education; Landslides; Learning algorithms; Learning systems; Lithology; Maximum entropy methods; Neural networks; Statistical tests; Ensemble models; Environmental conditions; Landslide susceptibility; Machine learning techniques; Normalized difference vegetation index; Receiver operating characteristic curves; Spatial modeling; Topographic wetness index; Support vector machines; artificial neural network; land cover; landslide; limestone; maximum entropy analysis; NDVI; slope angle; spatial analysis; support vector machine; China; Sichuan; Wanyuan",2-s2.0-85021392934
"Gilbertson J.K., van Niekerk A.","Value of dimensionality reduction for crop differentiation with multi-temporal imagery and machine learning",2017,"Computers and Electronics in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461748&doi=10.1016%2fj.compag.2017.08.024&partnerID=40&md5=55d9dd67affa6677b9e8c672ad9f8ece","This study evaluates the use of automated and manual feature selection – prior to machine learning – for the differentiation of crops in a Mediterranean climate (Western Cape, South Africa). Five Landsat-8 images covering the different crop class phenological stages were acquired and used to generate a range of spectral and textural features within an object-based image analysis (OBIA) paradigm. The features were used as input to decision trees (DTs), k-nearest neighbour (k-NN), support vector machine (SVM), and random forest (RF) supervised classifiers. Testing was done by performing classifications (using all spatial variables) and then incrementally reducing the feature counts (based on importance allocated to features by filters), feature extraction, and manual (semantic) feature selection. Classification and regression trees (CART) and RF were used as methods to filter feature selection. Feature-extraction methods employed include principal components analysis (PCA) and Tasselled cap transformation (TCT). The classification results were analysed by comparing the overall accuracies and kappa coefficients of each scenario, while McNemar's test was used to assess the statistical significance of differences in accuracies among classifiers. Feature selection was found to improve the overall accuracies of the DT, k-NN, and RF classifications, but reduced the accuracy of SVM. The results showed that SVM with feature extraction (PCA) on individual image dates produced the most accurate classification (96.2%). Semantic groupings of features for classification also revealed that using the image bands and indices is not sufficient for crop classification, and that additional features are needed. The accuracy differences of the classifiers were, however, not statistically significant, which suggests that, although dimensionality reduction can improve crop differentiation when multi-temporal Landsat-8 imagery is used, it had a marginal effect on the results. For operational crop-type classification in the study area (and similar regions), we conclude that the SVM algorithm can be applied to the full set of features generated. © 2017 Elsevier B.V.","Classification and regression trees; Crop type mapping; Feature selection; Machine learning; Pan-sharpening; Principal components analysis; Random forest","Artificial intelligence; Crops; Decision trees; Extraction; Feature extraction; Forestry; Learning systems; Machine components; Mapping; Nearest neighbor search; Principal component analysis; Semantics; Support vector machines; Classification and regression tree; Crop type mappings; Pan-sharpening; Principal components analysis; Random forests; Classification (of information)",2-s2.0-85028461748
"Billings J.M., Eder M., Flood W.C., Dhami D.S., Natarajan S., Whitlow C.T.","Machine Learning Applications to Resting-State Functional MR Imaging Analysis",2017,"Neuroimaging Clinics of North America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031088485&doi=10.1016%2fj.nic.2017.06.010&partnerID=40&md5=c33e42d90c54c7edd7baa939574a0251","Machine learning is one of the most exciting and rapidly expanding fields within computer science. Academic and commercial research entities are investing in machine learning methods, especially in personalized medicine via patient-level classification. There is great promise that machine learning methods combined with resting state functional MR imaging will aid in diagnosis of disease and guide potential treatment for conditions thought to be impossible to identify based on imaging alone, such as psychiatric disorders. We discuss machine learning methods and explore recent advances. © 2017 Elsevier Inc.","Computer science; Function MR imaging; Machine learning; MR imaging; Resting state function MR imaging","aging; alcoholism; Alzheimer disease; artificial neural network; attention deficit disorder; autism; binge drinking; bipolar disorder; decision tree; default mode network; diagnostic accuracy; differential diagnosis; epilepsy; functional connectivity; functional magnetic resonance imaging; human; image analysis; machine learning; major depression; mild cognitive impairment; patient coding; personalized medicine; priority journal; random forest; Review; schizophrenia; social phobia; support vector machine; traumatic brain injury",2-s2.0-85031088485
"Salawu E.O., Hesse E., Stopford C., Davey N., Sun Y.","Applying machine learning methods for characterization of hexagonal prisms from their 2D scattering patterns – an investigation using modelled scattering data",2017,"Journal of Quantitative Spectroscopy and Radiative Transfer",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030631802&doi=10.1016%2fj.jqsrt.2017.07.001&partnerID=40&md5=103acdcd6c807e68e9e6277f64551503","Better understanding and characterization of cloud particles, whose properties and distributions affect climate and weather, are essential for the understanding of present climate and climate change. Since imaging cloud probes have limitations of optical resolution, especially for small particles (with diameter < 25 µm), instruments like the Small Ice Detector (SID) probes, which capture high-resolution spatial light scattering patterns from individual particles down to 1 µm in size, have been developed. In this work, we have proposed a method using Machine Learning techniques to estimate simulated particles’ orientation-averaged projected sizes (PAD) and aspect ratio from their 2D scattering patterns. The two-dimensional light scattering patterns (2DLSP) of hexagonal prisms are computed using the Ray Tracing with Diffraction on Facets (RTDF) model. The 2DLSP cover the same angular range as the SID probes. We generated 2DLSP for 162 hexagonal prisms at 133 orientations for each. In a first step, the 2DLSP were transformed into rotation-invariant Zernike moments (ZMs), which are particularly suitable for analyses of pattern symmetry. Then we used ZMs, summed intensities, and root mean square contrast as inputs to the advanced Machine Learning methods. We created one random forests classifier for predicting prism orientation, 133 orientation-specific (OS) support vector classification models for predicting the prism aspect-ratios, 133 OS support vector regression models for estimating prism sizes, and another 133 OS Support Vector Regression (SVR) models for estimating the size PADs. We have achieved a high accuracy of 0.99 in predicting prism aspect ratios, and a low value of normalized mean square error of 0.004 for estimating the particle's size and size PADs. © 2017 Elsevier Ltd","Aspect ratio; Hexagonal prisms; Ice crystals; Machine learning; Ray tracing with diffraction on facets; Scattering pattern; Size; Zernike moments","Artificial intelligence; Aspect ratio; Climate change; Decision trees; Diffraction; Feature extraction; Forecasting; Light scattering; Mean square error; Prisms; Probes; Ray tracing; Regression analysis; Hexagonal prism; Ice crystals; Scattering pattern; Size; Zernike moments; Learning systems; ice crystal; light scattering; machine learning; modeling; ray tracing; support vector machine; wave scattering",2-s2.0-85030631802
"Wang M.F.Z., Fernandez-Gonzalez R.","(Machine-)Learning to analyze in vivo microscopy: Support vector machines",2017,"Biochimica et Biophysica Acta - Proteins and Proteomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030635266&doi=10.1016%2fj.bbapap.2017.09.013&partnerID=40&md5=d6c52e09f0a0d9b7a62caad8dc38de3e","The development of new microscopy techniques for super-resolved, long-term monitoring of cellular and subcellular dynamics in living organisms is revealing new fundamental aspects of tissue development and repair. However, new microscopy approaches present several challenges. In addition to unprecedented requirements for data storage, the analysis of high resolution, time-lapse images is too complex to be done manually. Machine learning techniques are ideally suited for the (semi-)automated analysis of multidimensional image data. In particular, support vector machines (SVMs), have emerged as an efficient method to analyze microscopy images obtained from animals. Here, we discuss the use of SVMs to analyze in vivo microscopy data. We introduce the mathematical framework behind SVMs, and we describe the metrics used by SVMs and other machine learning approaches to classify image data. We discuss the influence of different SVM parameters in the context of an algorithm for cell segmentation and tracking. Finally, we describe how the application of SVMs has been critical to study protein localization in yeast screens, for lineage tracing in C. elegans, or to determine the developmental stage of Drosophila embryos to investigate gene expression dynamics. We propose that SVMs will become central tools in the analysis of the complex image data that novel microscopy modalities have made possible. This article is part of a Special Issue entitled: Biophysics in Canada, edited by Lewis Kay, John Baenziger, Albert Berghuis and Peter Tieleman. © 2017 Elsevier B.V.","Image analysis; In vivo microscopy; Machine learning; Model organisms","adult; Caenorhabditis elegans; controlled study; developmental stage; Drosophila; embryo; embryo development; gene expression; image analysis; intravital microscopy; nonhuman; priority journal; protein localization; Review; support vector machine",2-s2.0-85030635266
"Granada Torres J.J., Varughese S., Thomas V.A., Chiuchiarelli A., Ralph S.E., Cárdenas Soto A.M., Guerrero González N.","Mitigation of time-varying distortions in Nyquist-WDM systems using machine learning",2017,"Optical Fiber Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029478386&doi=10.1016%2fj.yofte.2017.09.008&partnerID=40&md5=e5dc330da3a75b12e8e1adcdcbe3fa22","We propose a machine learning-based nonsymmetrical demodulation technique relying on clustering to mitigate time-varying distortions derived from several impairments such as IQ imbalance, bias drift, phase noise and interchannel interference. Experimental results show that those impairments cause centroid movements in the received constellations seen in time-windows of 10k symbols in controlled scenarios. In our demodulation technique, the k-means algorithm iteratively identifies the cluster centroids in the constellation of the received symbols in short time windows by means of the optimization of decision thresholds for a minimum BER. We experimentally verified the effectiveness of this computationally efficient technique in multicarrier 16QAM Nyquist-WDM systems over 270 km links. Our nonsymmetrical demodulation technique outperforms the conventional QAM demodulation technique, reducing the OSNR requirement up to ∼0.8 dB at a BER of 1 × 10−2 for signals affected by interchannel interference. © 2017 Elsevier Inc.","Clustering; Coherent optical communications; Digital demodulation; Interchannel interference; Machine learning","Artificial intelligence; Demodulation; Iterative methods; Optical communication; Optical variables measurement; Clustering; Coherent optical communications; Computationally efficient; Decision threshold; Demodulation techniques; Digital demodulation; Interchannel interference; Short time windows; Learning systems",2-s2.0-85029478386
"Curate F., Umbelino C., Perinha A., Nogueira C., Silva A.M., Cunha E.","Sex determination from the femur in Portuguese populations with classical and machine-learning classifiers",2017,"Journal of Forensic and Legal Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028707928&doi=10.1016%2fj.jflm.2017.08.011&partnerID=40&md5=2759899d1bbaec1759ccfce2d52b47fe","The assessment of sex is of paramount importance in the establishment of the biological profile of a skeletal individual. Femoral relevance for sex estimation is indisputable, particularly when other exceedingly dimorphic skeletal regions are missing. As such, this study intended to generate population-specific osteometric models for the estimation of sex with the femur and to compare the accuracy of the models obtained through classical and machine-learning classifiers. A set of 15 standard femoral measurements was acquired in a training sample (100 females; 100 males) from the Coimbra Identified Skeletal Collection (University of Coimbra, Portugal) and models for sex classification were produced with logistic regression (LR), linear discriminant analysis (LDA), support vector machines (SVM), and reduce error pruning trees (REPTree). Under cross-validation, univariable sectioning points generated with REPTree correctly estimated sex in 60.0–87.5% of cases (systematic error ranging from 0.0 to 37.0%), while multivariable models correctly classified sex in 84.0–92.5% of cases (bias from 0.0 to 7.0%). All models were assessed in a holdout sample (24 females; 34 males) from the 21st Century Identified Skeletal Collection (University of Coimbra, Portugal), with an allocation accuracy ranging from 56.9 to 86.2% (bias from 4.4 to 67.0%) in the univariable models, and from 84.5 to 89.7% (bias from 3.7 to 23.3%) in the multivariable models. This study makes available a detailed description of sexual dimorphism in femoral linear dimensions in two Portuguese identified skeletal samples, emphasizing the relevance of the femur for the estimation of sex in skeletal remains in diverse conditions of completeness and preservation. © 2017 Elsevier Ltd and Faculty of Forensic and Legal Medicine","Biological profile; Forensic anthropology; Human identification; Sex diagnosis","adult; aged; Article; discriminant analysis; female; femur; human; logistic regression analysis; machine learning; male; Portuguese (citizen); reduce error pruning tree; sensitivity and sensibility; sex determination; sex difference; support vector machine",2-s2.0-85028707928
"Nariya M.K., Kim J.H., Xiong J., Kleindl P.A., Hewarathna A., Fisher A.C., Joshi S.B., Schöneich C., Forrest M.L., Middaugh C.R., Volkin D.B., Deeds E.J.","Comparative Characterization of Crofelemer Samples Using Data Mining and Machine Learning Approaches With Analytical Stability Data Sets",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593356&doi=10.1016%2fj.xphs.2017.07.013&partnerID=40&md5=b4e108e740352c6219254a3f3aade778","There is growing interest in generating physicochemical and biological analytical data sets to compare complex mixture drugs, for example, products from different manufacturers. In this work, we compare various crofelemer samples prepared from a single lot by filtration with varying molecular weight cutoffs combined with incubation for different times at different temperatures. The 2 preceding articles describe experimental data sets generated from analytical characterization of fractionated and degraded crofelemer samples. In this work, we use data mining techniques such as principal component analysis and mutual information scores to help visualize the data and determine discriminatory regions within these large data sets. The mutual information score identifies chemical signatures that differentiate crofelemer samples. These signatures, in many cases, would likely be missed by traditional data analysis tools. We also found that supervised learning classifiers robustly discriminate samples with around 99% classification accuracy, indicating that mathematical models of these physicochemical data sets are capable of identifying even subtle differences in crofelemer samples. Data mining and machine learning techniques can thus identify fingerprint-type attributes of complex mixture drugs that may be used for comparative characterization of products. © 2017 American Pharmacists Association®","comparative characterization; crofelemer; data mining; supervised learning","crofelemer; accuracy; analytic method; Article; data mining; discriminant analysis; filtration; fractionation; incubation time; machine learning; mathematical model; molecular weight; physical chemistry; principal component analysis; temperature",2-s2.0-85028593356
"Tewary S., Arun I., Ahmed R., Chatterjee S., Chakraborty C.","AutoIHC-scoring: a machine learning framework for automated Allred scoring of molecular expression in ER- and PR-stained breast cancer tissue",2017,"Journal of Microscopy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020738642&doi=10.1111%2fjmi.12596&partnerID=40&md5=f7a455aea5fa04190a4afaa65f243fdf","In prognostic evaluation of breast cancer Immunohistochemical (IHC) markers namely, oestrogen receptor (ER) and progesterone receptor (PR) are widely used. The expert pathologist investigates qualitatively the stained tissue slide under microscope to provide the Allred score; which is clinically used for therapeutic decision making. Such qualitative judgment is time-consuming, tedious and more often suffers from interobserver variability. As a result, it leads to imprecise IHC score for ER and PR. To overcome this, there is an urgent need of developing a reliable and efficient IHC quantifier for high throughput decision making. In view of this, our study aims at developing an automated IHC profiler for quantitative assessment of ER and PR molecular expression from stained tissue images. We propose here to use CMYK colour space for positively and negatively stained cell extraction for proportion score. Also colour features are used for quantitative assessment of intensity scoring among the positively stained cells. Five different machine learning models namely artificial neural network, Naïve Bayes, K-nearest neighbours, decision tree and random forest are considered for learning the colour features using average red, green and blue pixel values of positively stained cell patches. Fifty cases of ER- and PR-stained tissues have been evaluated for validation with the expert pathologist's score. All five models perform adequately where random forest shows the best correlation with the expert's score (Pearson's correlation coefficient = 0.9192). In the proposed approach the average variation of diaminobenzidine (DAB) to nuclear area from the expert's score is found to be 7.58%, as compared to 27.83% for state-of-the-art ImmunoRatio software. © 2017 The Authors Journal of Microscopy © 2017 Royal Microscopical Society","Allred score; breast cancer; ER and PR expression; image analysis; immunohistochemistry; machine learning","diaminobenzidine; estrogen receptor; progesterone receptor; Allred score; Article; artificial neural network; Bayesian learning; breast cancer; cancer tissue; color; correlation coefficient; decision tree; human; immunohistochemistry; k nearest neighbor; machine learning; pathologist; priority journal; protein expression; quantitative analysis; random forest; scoring system",2-s2.0-85020738642
"Orsolic I., Pevec D., Suznjevic M., Skorin-Kapov L.","A machine learning approach to classifying YouTube QoE based on encrypted network traffic",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018750008&doi=10.1007%2fs11042-017-4728-4&partnerID=40&md5=f57be2bb9a298e2adbb24b0301792901","Due to the widespread use of encryption in Over-The-Top video streaming traffic, network operators generally lack insight into application-level quality indicators (e.g., video quality levels, buffer underruns, stalling duration). They are thus faced with the challenge of finding solutions for monitoring service performance and estimating customer Quality of Experience (QoE) degradations based solely on passive monitoring solutions deployed within their network. We address this challenge by considering the concrete case of YouTube, whereby we present a methodology for the classification of end users’ QoE when watching YouTube videos, based only on statistical properties of encrypted network traffic. We have developed a system called YouQ which includes tools for monitoring and analysis of application-level quality indicators and corresponding traffic traces. Collected data is then used for the development of machine learning models for QoE classification based on computed traffic features per video session. To test the YouQ system and methodology, we collected a dataset corresponding to 1060 different YouTube videos streamed across 39 different bandwidth scenarios, and tested various classification models. Classification accuracy was found to be up to 84% when using three QoE classes (“low”, “medium” or “high”) and up to 91% when using binary classification (classes “low” and “high”). To improve the models in the future, we discuss why and when prediction errors occur. Moreover, we have analysed YouTube’s adaptation algorithm, thus providing valuable insight into the logic behind the quality level selection strategy, which may also be of interest in improving future QoE estimation algorithms. © 2017, Springer Science+Business Media New York.","HTTP adaptive streaming; Machine learning; Network measurements; Passive monitoring; Quality of Experience; Video streaming; YouTube","Artificial intelligence; Classification (of information); Cryptography; Learning systems; Statistical tests; Video streaming; Adaptive streaming; Network measurement; Passive monitoring; Quality of experience (QoE); YouTube; Quality of service",2-s2.0-85018750008
"Mathotaarachchi S., Pascoal T.A., Shin M., Benedet A.L., Kang M.S., Beaudry T., Fonov V.S., Gauthier S., Rosa-Neto P.","Identifying incipient dementia individuals using machine learning and amyloid imaging",2017,"Neurobiology of Aging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026316928&doi=10.1016%2fj.neurobiolaging.2017.06.027&partnerID=40&md5=063b549c8ed69a40261efa12ecb26db7","Identifying individuals destined to develop Alzheimer's dementia within time frames acceptable for clinical trials constitutes an important challenge to design studies to test emerging disease-modifying therapies. Although amyloid-β protein is the core pathologic feature of Alzheimer's disease, biomarkers of neuronal degeneration are the only ones believed to provide satisfactory predictions of clinical progression within short time frames. Here, we propose a machine learning–based probabilistic method designed to assess the progression to dementia within 24 months, based on the regional information from a single amyloid positron emission tomography scan. Importantly, the proposed method was designed to overcome the inherent adverse imbalance proportions between stable and progressive mild cognitive impairment individuals within a short observation period. The novel algorithm obtained an accuracy of 84% and an under-receiver operating characteristic curve of 0.91, outperforming the existing algorithms using the same biomarker measures and previous studies using multiple biomarker modalities. With its high accuracy, this algorithm has immediate applications for population enrichment in clinical trials designed to test disease-modifying therapies aiming to mitigate the progression to Alzheimer's disease dementia. © 2017 The Author(s)","Alzheimer's disease; Amyloid; Mild cognitive impairment; Prediction; Random forest; Random under sampling","aged; algorithm; Alzheimer disease; Article; controlled study; dementia; dementia assessment; diagnostic test accuracy study; disease course; female; human; machine learning; major clinical study; male; mild cognitive impairment; positron emission tomography; priority journal; random forest; sensitivity and specificity; support vector machine",2-s2.0-85026316928
"Jiang N., Luk K.D.-K., Hu Y.","A Machine Learning-based Surface Electromyography Topography Evaluation for Prognostic Prediction of Functional Restoration Rehabilitation in Chronic Low Back Pain",2017,"Spine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016032248&doi=10.1097%2fBRS.0000000000002159&partnerID=40&md5=441bbecadc53bc8bc0e8ba62236d2588","Study Design. A retrospective study. Objective. The aim of this study was to investigate the feasibility and applicability of support vector machine (SVM) algorithm in classifying patients with LBP who would obtain satisfactory or unsatisfactory progress after the functional restoration rehabilitation program. Summary of Background Data. Dynamic surface electromyography (SEMG) topography has demonstrated the potential use in predicting the prognosis of functional restoration rehabilitation for patients with low back pain (LBP). However, processing from raw SEMG topography to make prediction is not easy to clinicians. Methods. A total of 30 patients with nonspecific LBP were recruited and divided into ""responding"" and ""non-responding"" group according to the change of Visual analog pain rating scale and Oswestry Disability Index. Each patient received a 12-week functional restoration rehabilitation program. A normal database was calculated from a control group from 48 healthy participants. Root-mean-square difference (RMSD) was extracted from the recorded dynamic SEMG topography during symmetrical and asymmetrical trunk-movement. SVM and cross-validation were applied to the prediction based on the optimized features selected by the sequential floating forward selection (SFFS) algorithm. Results. RMSD feature parameters following rehabilitation in the ""responding"" group showed a significant difference (P<0.05) with the one in the ""nonresponding"" group. The SVM classifier with Quadratic kernel based on SFFS-selected features showed the best prediction performance (accuracy: 96.67%, sensitivity: 100%, specificity: 93.75%, average area under curve [AUC]: 0.8925) comparing with linear kernel (accuracy: 80.00%, sensitivity: 85.71%, specificity: 75.00%, average AUC: 0.7825), polynomial kernel (accuracy: 93.33%, sensitivity: 92.86%, specificity: 93.75%, average AUC: 0.9675), and radial basis function (RBF) kernel (accuracy: 86.67%, sensitivity: 85.71%, specificity: 87.50%, average AUC: 0.7900). Conclusion. The use of SVM-based classifier of SEMG topography can be applied to identify the patient responding to functional restoration rehabilitation, which will help the healthcare worker to improve the efficiency of LBP rehabilitation. © 2017 Wolters Kluwer Health, Inc. All rights reserved.","dynamic surface electromyography topography; functional restoration rehabilitation; low back pain; machine learning","adult; algorithm; Article; clinical article; clinical assessment; controlled study; electromyography; female; functional restoration rehabilitation; health care personnel; human; kernel method; linear kernel; low back pain; machine learning; male; Oswestry Disability Index; pain intensity; physical sensitivity and tolerance; polynomial kernel; priority journal; prognosis; quadratic kernel; radial basis function kernel; rehabilitation care; retrospective study; support vector machine; surface electromyography topography; topography; treatment response; visual analog scale",2-s2.0-85016032248
"Raissi M., Perdikaris P., Karniadakis G.E.","Machine learning of linear differential equations using Gaussian processes",2017,"Journal of Computational Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027729699&doi=10.1016%2fj.jcp.2017.07.050&partnerID=40&md5=ae6f7c740b73b83fb2f1fe03fb5f0d76","This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or “black-box” computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics. © 2017 Elsevier Inc.","Fractional differential equations; Functional genomics; Inverse problems; Probabilistic machine learning; Uncertainty quantification",,2-s2.0-85027729699
"Bourel M., Crisci C., Martínez A.","Consensus methods based on machine learning techniques for marine phytoplankton presence–absence prediction",2017,"Ecological Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029546509&doi=10.1016%2fj.ecoinf.2017.09.004&partnerID=40&md5=51960db569b01a22941794835e5cf562","We performed different consensus methods by combining binary classifiers, mostly machine learning classifiers, with the aim to test their capability as predictive tools for the presence–absence of marine phytoplankton species. The consensus methods were constructed by considering a combination of four methods (i.e., generalized linear models, random forests, boosting and support vector machines). Six different consensus methods were analyzed by taking into account six different ways of combining single-model predictions. Some of these methods are presented here for the first time. To evaluate the performance of the models, we considered eight phytoplankton species presence–absence data sets and data related to environmental variables. Some of the analyzed species are toxic, whereas others provoke water discoloration, which can cause alarm in the population. Besides the phytoplankton data sets, we tested the models on 10 well-known open access data sets. We evaluated the models' performances over a test sample. For most (72%) of the data sets, a consensus method was the method with the lowest classification error. In particular, a consensus method that weighted single-model predictions in accordance with single-model performances (weighted average prediction error — WA-PE model) was the one that presented the lowest classification error most of the time. For the phytoplankton species, the errors of the WA-PE model were between 10% for the species Akashiwo sanguinea and 38% for Dinophysis acuminata. This study provides novel approaches to improve the prediction accuracy in species distribution studies and, in particular, in those concerning marine phytoplankton species. © 2017 Elsevier B.V.","Machine learning; Marine phytoplankton; Non-homogeneous consensus methods; Prediction; Presence–absence data","data processing; data set; environmental factor; error analysis; homogeneity; machine learning; numerical method; performance assessment; phytoplankton; prediction; Akashiwo sanguinea; Dinophysis acuminata",2-s2.0-85029546509
"Imbus J.R., Randle R.W., Pitt S.C., Sippel R.S., Schneider D.F.","Machine learning to identify multigland disease in primary hyperparathyroidism",2017,"Journal of Surgical Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021456934&doi=10.1016%2fj.jss.2017.05.117&partnerID=40&md5=6ad9031ac6278ea000e1db63c50bae51","Background 20%-25% of patients with primary hyperparathyroidism will have multigland disease (MGD). Preoperatative imaging can be inaccurate or unnecessary in MGD. Identification of MGD could direct the need for imaging and inform operative approach. The purpose of this study is to use machine learning (ML) methods to predict MGD. Methods Retrospective review of a prospective database. The ML platform, Waikato Environment for Knowledge Analysis, was used, and we selected models for (1) overall accuracy and (2) preferential identification of MGD. A review of imaging studies was performed on a cohort predicted to have MGD. Results 2010 patients met inclusion criteria: 1532 patients had single adenoma (SA) (76%) and 478 had MGD (24%). After testing many algorithms, we selected two different models for potential integration as clinical decision-support tools. The best overall accuracy was achieved using a boosted tree classifier, RandomTree: 94.1% accuracy; 94.1% sensitivity, 83.8% specificity, 94.1% positive predictive value, and 0.984 area under the receiver operating characteristics curve. To maximize positive predictive value of MGD prediction, a rule-based classifier, JRip, with cost-sensitive learning was used and achieved 100% positive predictive value for MGD. Imaging reviewed from the cohort of 34 patients predicted to have MGD by the cost-sensitive model revealed 39 total studies performed: 28 sestamibi scans and 11 ultrasounds. Only 8 (29%) sestamibi scans and 4 (36%) ultrasounds were correct. Conclusions ML methods can help distinguish MGD early in the clinical evaluation of primary hyperparathyroidism, guiding further workup and surgical planning. © 2017 Elsevier Inc.","Clinical decision support; Diagnosis; Machine learning; Multigland disease; Parathyroid glands; Primary hyperparathyroidism","accuracy; adenoma; adult; algorithm; Article; classifier; clinical decision support system; decision tree; human; machine learning; major clinical study; medical record review; multigland disease; physical disease by body function; predictive value; primary hyperparathyroidism; priority journal; receiver operating characteristic; sensitivity and specificity",2-s2.0-85021456934
"Shouval R., Hadanny A., Shlomo N., Iakobishvili Z., Unger R., Zahger D., Alcalai R., Atar S., Gottlieb S., Matetzky S., Goldenberg I., Beigel R.","Machine learning for prediction of 30-day mortality after ST elevation myocardial infraction: An Acute Coronary Syndrome Israeli Survey data mining study",2017,"International Journal of Cardiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028571788&doi=10.1016%2fj.ijcard.2017.05.067&partnerID=40&md5=70263613ff04d109eeb8bb8e8d4fe002","Background Risk scores for prediction of mortality 30-days following a ST-segment elevation myocardial infarction (STEMI) have been developed using a conventional statistical approach. Objective To evaluate an array of machine learning (ML) algorithms for prediction of mortality at 30-days in STEMI patients and to compare these to the conventional validated risk scores. Methods This was a retrospective, supervised learning, data mining study. Out of a cohort of 13,422 patients from the Acute Coronary Syndrome Israeli Survey (ACSIS) registry, 2782 patients fulfilled inclusion criteria and 54 variables were considered. Prediction models for overall mortality 30 days after STEMI were developed using 6 ML algorithms. Models were compared to each other and to the Global Registry of Acute Coronary Events (GRACE) and Thrombolysis In Myocardial Infarction (TIMI) scores. Results Depending on the algorithm, using all available variables, prediction models' performance measured in an area under the receiver operating characteristic curve (AUC) ranged from 0.64 to 0.91. The best models performed similarly to the Global Registry of Acute Coronary Events (GRACE) score (0.87 SD 0.06) and outperformed the Thrombolysis In Myocardial Infarction (TIMI) score (0.82 SD 0.06, p < 0.05). Performance of most algorithms plateaued when introduced with 15 variables. Among the top predictors were creatinine, Killip class on admission, blood pressure, glucose level, and age. Conclusions We present a data mining approach for prediction of mortality post-ST-segment elevation myocardial infarction. The algorithms selected showed competence in prediction across an increasing number of variables. ML may be used for outcome prediction in complex cardiology settings. © 2017 Elsevier Ireland Ltd","Data mining; Machine learning; Mortality; Outcome; STEMI",,2-s2.0-85028571788
"Liu N.T., Salinas J.","Machine Learning for Predicting Outcomes in Trauma",2017,"Shock",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030784312&doi=10.1097%2fSHK.0000000000000898&partnerID=40&md5=d2b3e9af5a424154dcb7c9e4c8ea6377","To date, there are no reviews on machine learning (ML) for predicting outcomes in trauma. Consequently, it remains unclear as to how ML-based prediction models compare in the triage and assessment of trauma patients. The objective of this review was to survey and identify studies involving ML for predicting outcomes in trauma, with the hypothesis that models predicting similar outcomes may share common features but the performance of ML in these studies will differ greatly. MEDLINE and other databases were searched for studies involving trauma and ML. Sixty-five observational studies involving ML for the prediction of trauma outcomes met inclusion criteria. In total 2,433,180 patients were included in the studies. The studies focused on prediction of the following outcome measures: survival/mortality (n = 34), morbidity/shock/hemorrhage (n = 12), hospital length of stay (n = 7), hospital admission/triage (n = 6), traumatic brain injury (n = 4), life-saving interventions (n = 5), post-traumatic stress disorder (n = 4), and transfusion (n = 1). Six studies were prospective observational studies. Of the 65 studies, 33 used artificial neural networks for prediction. Importantly, most studies demonstrated the benefits of ML models. However, algorithm performance was assessed differently by different authors. Sensitivity-specificity gap values varied greatly from 0.035 to 0.927. Notably, studies shared many features for model development. A common ML feature base may be determined for predicting outcomes in trauma. However, the impact of ML will require further validation in prospective observational studies and randomized clinical trials, establishment of common performance criteria, and high-quality evidence about clinical and economic impacts before ML can be widely accepted in practice. Copyright © 2017 by the Shock Society.","Machine learning; neural networks; prediction models; trauma","artificial neural network; hospital admission; hospitalization; human; injury; length of stay; machine learning; morbidity; observational study; posttraumatic stress disorder; prediction; Review; shock; systematic review; transfusion; traumatic brain injury",2-s2.0-85030784312
"Mansoor H., Elgendy I.Y., Segal R., Bavry A.A., Bian J.","Risk prediction model for in-hospital mortality in women with ST-elevation myocardial infarction: A machine learning approach",2017,"Heart and Lung: Journal of Acute and Critical Care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030783666&doi=10.1016%2fj.hrtlng.2017.09.003&partnerID=40&md5=af4f6cdba6b9c4118965c68a65d6d364","Background Studies had shown that mortality due to ST-elevation myocardial infarction (STEMI) is higher in women compared with men. The purpose of this study is to develop and validate prediction models for all-cause in-hospital mortality in women admitted with STEMI using logistic regression and random forest, and to compare the performance and validity of the different models. Methods Data from the National Inpatient Sample (NIS) data years 2011–2013 were used to identify women admitted with STEMI. The main outcome was all-cause in-hospital mortality. Patients were divided into development and validation cohorts, and trained models were internally validated using 20% of the 2012 data, and externally validated using 2011 and 2013 NIS data. Results Three main models were developed and compared; multivariate logistic regression, full and reduced random forest models. In the multivariate logistic regression, 11 variables were included in the final model based on backward elimination. The full random forest model contained 32 variables, and the reduced model contained 17 variables selected based on individual variable importance. In the internal validation cohort, the C-index was 0.84, 0.81, and 0.80 for the multivariate logistic regression, full, and reduced random forest models, respectively. The models showed good stability in the external validation cohorts with a C-index for the logistic regression, full, and reduced random forest models of 0.84, 0.85, and 0.81 for year 2011, and 0.82, 0.81, and 0.81 for year 2013, respectively. Conclusions Random forest was comparable to logistic regression in predicting in-hospital mortality in women with STEMI, and can be a useful and accurate tool in clinical practice. © 2017 Elsevier Inc.","Machine learning; Mortality; Myocardial infarction; Risk model; Women",,2-s2.0-85030783666
"Ertuğrul Ö.F., Altun Ş.","Developing correlations by extreme learning machine for calculating higher heating values of waste frying oils from their physical properties",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959330639&doi=10.1007%2fs00521-016-2233-8&partnerID=40&md5=a57a241fba2c39a1113e0a1c2dd909a5","In this study, a novel approach was proposed based on extreme learning machine (ELM) for developing correlations in order to calculate higher heating values (HHVs, kj/kg) of waste frying oils from their physical properties such as density (ρ, kg/m3) and kinematic viscosity (v, mm2/s) values. These values can easily be determined by using laboratory equipment. For developing the correlations, an experimental dataset from the literature covering 35 samples was collected to be employed in the training and validation steps. The obtained optimum parameters of artificial neural network in the training stage by ELM were employed to develop new correlations. The HHVs calculated by using density-based correlation (HHV = 50823.183 − 12.34095ρ) showed the mean absolute and relative errors of 145.8048 kJ/kg and 0.3695 %, respectively. In the case of the viscosity-based correlation (HHV = 40172.85 − 17.93615v), they were found as 129.04 kJ/kg and 0.327 %, respectively. Additionally, new correlations were performed better than those available in the literature and those obtained by other machine learning methods; therefore, it is highly suggested that the proposed approach can be used for developing new correlations. © 2016, The Natural Computing Applications Forum.","Extreme learning machine; Higher heating value; Mathematical modeling; Waste frying oils","Artificial intelligence; Calorific value; Knowledge acquisition; Laboratories; Mathematical models; Neural networks; Physical properties; Viscosity; Density-based correlations; Extreme learning machine; Higher heating value; Laboratory equipments; Machine learning methods; New correlations; Optimum parameters; Waste frying oil; Learning systems",2-s2.0-84959330639
"Baldi P., Sadowski P., Lu Z.","Learning in the machine: The symmetries of the deep learning channel",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029542312&doi=10.1016%2fj.neunet.2017.08.008&partnerID=40&md5=b0806e99e9fbddbc9ba7ad728afa7682","In a physical neural system, learning rules must be local both in space and time. In order for learning to occur, non-local information must be communicated to the deep synapses through a communication channel, the deep learning channel. We identify several possible architectures for this learning channel (Bidirectional, Conjoined, Twin, Distinct) and six symmetry challenges: (1) symmetry of architectures; (2) symmetry of weights; (3) symmetry of neurons; (4) symmetry of derivatives; (5) symmetry of processing; and (6) symmetry of learning rules. Random backpropagation (RBP) addresses the second and third symmetry, and some of its variations, such as skipped RBP (SRBP) address the first and the fourth symmetry. Here we address the last two desirable symmetries showing through simulations that they can be achieved and that the learning channel is particularly robust to symmetry variations. Specifically, random backpropagation and its variations can be performed with the same non-linear neurons used in the main input–output forward channel, and the connections in the learning channel can be adapted using the same algorithm used in the forward channel, removing the need for any specialized hardware in the learning channel. Finally, we provide mathematical results in simple cases showing that the learning equations in the forward and backward channels converge to fixed points, for almost any initial conditions. In symmetric architectures, if the weights in both channels are small at initialization, adaptation in both channels leads to weights that are essentially symmetric during and after learning. Biological connections are discussed. © 2017 Elsevier Ltd","Backpropagation; Deep learning; Learning channel; Learning dynamics; Local learning; Neural networks","Backpropagation algorithms; Deep learning; Network architecture; Neural networks; Neurons; Forward channels; Forward-and-backward; Initial conditions; Learning channel; Local learning; Neural systems; Non-linear neurons; Specialized hardware; Backpropagation; accuracy; Article; artificial neural network; back propagation; classification algorithm; linear system; machine learning; mathematical analysis; mathematical model; nonlinear system; priority journal; probability; simulation",2-s2.0-85029542312
"Cui D.-M., Yan W., Wang X.-Q., Lu L.-M.","Towards intelligent interpretation of low strain pile integrity testing results using machine learning techniques",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032584430&doi=10.3390%2fs17112443&partnerID=40&md5=1755b975ad1b508aed4e0f07c3fea5b1","Low strain pile integrity testing (LSPIT), due to its simplicity and low cost, is one of the most popular NDE methods used in pile foundation construction. While performing LSPIT in the field is generally quite simple and quick, determining the integrity of the test piles by analyzing and interpreting the test signals (reflectograms) is still a manual process performed by experienced experts only. For foundation construction sites where the number of piles to be tested is large, it may take days before the expert can complete interpreting all of the piles and delivering the integrity assessment report. Techniques that can automate test signal interpretation, thus shortening the LSPIT’s turnaround time, are of great business value and are in great need. Motivated by this need, in this paper, we develop a computer-aided reflectogram interpretation (CARI) methodology that can interpret a large number of LSPIT signals quickly and consistently. The methodology, built on advanced signal processing and machine learning technologies, can be used to assist the experts in performing both qualitative and quantitative interpretation of LSPIT signals. Specifically, the methodology can ease experts’ interpretation burden by screening all test piles quickly and identifying a small number of suspected piles for experts to perform manual, in-depth interpretation. We demonstrate the methodology’s effectiveness using the LSPIT signals collected from a number of real-world pile construction sites. The proposed methodology can potentially enhance LSPIT and make it even more efficient and effective in quality control of deep foundation construction. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Deep foundation; Defect detection; Extreme learning machine; Neural network; Non-destructive evaluation; Pile integrity testing; Wavelet decomposition","Artificial intelligence; Computer aided analysis; Foundations; Learning systems; Neural networks; Nondestructive examination; Quality control; Signal processing; Wavelet decomposition; Deep foundations; Defect detection; Extreme learning machine; Non destructive evaluation; Pile integrity; Piles",2-s2.0-85032584430
"Liu W., Zhang L., Tao D., Cheng J.","Support vector machine active learning by Hessian regularization",2017,"Journal of Visual Communication and Image Representation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026915862&doi=10.1016%2fj.jvcir.2017.08.001&partnerID=40&md5=7d51c5644ae2037cfefab4b79c61cbd0","It is time-consuming and expensive to gather and label the growing multimedia data that is easily accessible with the prodigious development of Internet technology and digital sensors. Hence, it is essential to develop a technique that can efficiently be utilized for the large-scale multimedia data especially when labeled data is rare. Active learning is showing to be one useful approach that greedily chooses queries from unlabeled data to be labeled for further learning and then minimizes the estimated expected learning error. However, most active learning methods only take into account the labeled data in the training of the classifier. In this paper, we introduce a semi-supervised algorithm to learn the classifier and then perform active learning scheme on top of the semi-supervised scheme. Particularly, we employ Hessian regularization into support vector machine to boost the classifier. Hessian regularization exploits the potential geometry structure of data space (including labeled and unlabeled data) and then significantly leverages the performance in each round. To evaluate the proposed algorithm, we carefully conduct extensive experiments including image segmentation and human activity recognition on popular datasets respectively. The experimental results demonstrate that our method can achieve a better performance than the traditional active learning methods. © 2017 Elsevier Inc.","Active learning; Activity recognition; Hessian; Image segmentation; Manifold regularization; Semi-supervised","Image segmentation; Learning systems; Pattern recognition; Support vector machines; Active Learning; Activity recognition; Hessian; Manifold regularizations; Semi-supervised; Artificial intelligence",2-s2.0-85026915862
"Iranitalab A., Khattak A.","Comparison of four statistical and machine learning methods for crash severity prediction",2017,"Accident Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027874583&doi=10.1016%2fj.aap.2017.08.008&partnerID=40&md5=a93b0659f62f3f945c86ea574cdfd77e","Crash severity prediction models enable different agencies to predict the severity of a reported crash with unknown severity or the severity of crashes that may be expected to occur sometime in the future. This paper had three main objectives: comparison of the performance of four statistical and machine learning methods including Multinomial Logit (MNL), Nearest Neighbor Classification (NNC), Support Vector Machines (SVM) and Random Forests (RF), in predicting traffic crash severity; developing a crash costs-based approach for comparison of crash severity prediction methods; and investigating the effects of data clustering methods comprising K-means Clustering (KC) and Latent Class Clustering (LCC), on the performance of crash severity prediction models. The 2012–2015 reported crash data from Nebraska, United States was obtained and two-vehicle crashes were extracted as the analysis data. The dataset was split into training/estimation (2012–2014) and validation (2015) subsets. The four prediction methods were trained/estimated using the training/estimation dataset and the correct prediction rates for each crash severity level, overall correct prediction rate and a proposed crash costs-based accuracy measure were obtained for the validation dataset. The correct prediction rates and the proposed approach showed NNC had the best prediction performance in overall and in more severe crashes. RF and SVM had the next two sufficient performances and MNL was the weakest method. Data clustering did not affect the prediction results of SVM, but KC improved the prediction performance of MNL, NNC and RF, while LCC caused improvement in MNL and RF but weakened the performance of NNC. Overall correct prediction rate had almost the exact opposite results compared to the proposed approach, showing that neglecting the crash costs can lead to misjudgment in choosing the right prediction method. © 2017 Elsevier Ltd","Crash costs; Multinomial logit; Nearest neighbor classification; Random forests; Support vector machines; Traffic crash severity prediction","Accidents; Artificial intelligence; Cluster analysis; Clustering algorithms; Costs; Decision trees; Learning systems; Support vector machines; Crash costs; Multinomial Logit; Nearest neighbor classification; Random forests; Traffic crashes; Forecasting",2-s2.0-85027874583
"Rughetti D., Di Sanzo P., Ciciani B., Quaglia F.","Machine learning-based thread-parallelism regulation in software transactional memory",2017,"Journal of Parallel and Distributed Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022022359&doi=10.1016%2fj.jpdc.2017.06.001&partnerID=40&md5=a5bc76946a5645df123c1cb5546cf133","Transactional Memory (TM) stands as a powerful paradigm for manipulating shared data in concurrent applications. It avoids the drawbacks of coarse grain locking schemes, namely the potentially excessive limitation of concurrency, while jointly providing support for synchronization transparency to the programmers, which is achieved by embedding code-blocks accessing shared data within transactions. On the downside, excessive transaction aborts may arise in scenarios with non-negligible volumes of conflicting data accesses, which might significantly impair performance. TM needs therefore to resort to methods enabling applications to run with the maximum degree of transaction concurrency that still avoids thrashing. In this article, we focus on Software TM (STM) implementations and present a machine learning-based approach that enables the dynamic selection of the best suited number of threads to be kept alive along specific phases of the execution of STM applications, depending on (variations of) the shared data access pattern. Two key contributions are provided with our approach: (i) the identification of the well suited set of features allowing the instantiation of a reliable neural network-based performance model and (ii) the introduction of mechanisms enabling the reduction of the run-time overhead for sampling these features. We integrated a real implementation of our machine learning-based thread-parallelism regulation approach within the TinySTM open source package and present experimental data, based on the STAMP benchmark suite, which show the effectiveness of the presented thread-parallelism regulation policy in optimizing transaction throughput. © 2017 Elsevier Inc.","Concurrency; Performance optimization; Performance prediction; Transactional memory","Application programs; Artificial intelligence; Java programming language; Learning algorithms; Learning systems; Locks (fasteners); Open source software; Storage allocation (computer); Concurrency; Open source package; Performance optimizations; Performance prediction; Shared data access patterns; Software transactional memory; Transaction throughput; Transactional memory; Education",2-s2.0-85022022359
"Costa-jussà M.R., Allauzen A., Barrault L., Cho K., Schwenk H.","Introduction to the special issue on deep learning approaches for machine translation",2017,"Computer Speech and Language",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021932117&doi=10.1016%2fj.csl.2017.03.001&partnerID=40&md5=d76caafb01a80a65103afe950b1335d4","Deep learning is revolutionizing speech and natural language technologies since it is offering an effective way to train systems and obtaining significant improvements. The main advantage of deep learning is that, by developing the right architecture, the system automatically learns features from data without the need of explicitly designing them. This machine learning perspective is conceptually changing how speech and natural language technologies are addressed. In the case of Machine Translation (MT), deep learning was first introduced in standard statistical systems. By now, end-to-end neural MT systems have reached competitive results. This special issue introductory paper addresses how deep learning has been gradually introduced in MT. This introduction covers all topics contained in the papers included in this special issue, which basically are: integration of deep learning in statistical MT; development of the end-to-end neural MT system; and introduction of deep learning in interactive MT and MT evaluation. Finally, this introduction sketches some research directions that MT is taking guided by deep learning. © 2017 Elsevier Ltd","Deep learning; Machine translation","Computational linguistics; Computer aided language translation; Education; Translation (languages); End to end; Learning approach; Machine translations; MT evaluations; Natural languages; Statistical systems; Train systems; Deep learning",2-s2.0-85021932117
"Gašić M., Hakkani-Tür D., Celikyilmaz A.","Spoken language understanding and interaction: machine learning for human-like conversational systems",2017,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020937897&doi=10.1016%2fj.csl.2017.05.006&partnerID=40&md5=be415ebfb842d53d4bc3146d9fad7cd3","In recent years, the interest in research in speech understanding and spoken interaction has soared due to the emergence of virtual personal assistants. However, while the ability of these agents to recognise conversational speech is maturing rapidly, their ability to understand and interact is still limited. At the same time we have witnessed the development of the number of models based on machine learning that made a huge impact on spoken language understanding accuracies and the interaction quality overall. This special issue brings together a number of articles that tackle different aspects of spoken language understanding and interaction: clarifications in dialogues, adaptation to different domains, semantic tagging and error handling. These studies all have a common purpose of building human-like conversational systems. © 2017",,"Artificial intelligence; Education; Learning systems; Semantics; Conversational speech; Conversational systems; Different domains; Interaction quality; Personal assistants; Speech understanding; Spoken interaction; Spoken language understanding; Speech recognition",2-s2.0-85020937897
"Liu H.-T., Wang J., He Y.-L., Ashfaq R.A.R.","Extreme learning machine with fuzzy input and fuzzy output for fuzzy regression",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960410600&doi=10.1007%2fs00521-016-2232-9&partnerID=40&md5=dac16580c3c3d4ce8e166e3fc2e03ecc","It is practically and theoretically significant to approximate and simulate a system with fuzzy inputs and fuzzy outputs. This paper proposes a extreme learning machine (ELM)-based fuzzy regression model (FR ELM) in which both inputs and outputs are triangular fuzzy numbers. Algorithm for training FR ELM is designed, and its computational complexity is analyzed. Furthermore, the convergence and error estimation for FR ELM are discussed. Numerical simulations show that the proposed FR ELM can effectively approximate a fuzzy input and fuzzy output system. © 2016, The Natural Computing Applications Forum.","Extreme learning machine; Fuzzy input and fuzzy output; Fuzzy linear regression; Triangular fuzzy number","Fuzzy rules; Fuzzy sets; Knowledge acquisition; Regression analysis; Extreme learning machine; Fuzzy input; Fuzzy linear regression; Fuzzy output; Fuzzy regression models; Fuzzy regressions; Triangular fuzzy numbers; Learning systems",2-s2.0-84960410600
"Murugeswari S., Sukanesh R.","Investigations of severity level measurements for diabetic macular oedema using machine learning algorithms",2017,"Irish Journal of Medical Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019181334&doi=10.1007%2fs11845-017-1598-8&partnerID=40&md5=2cae5c73ff7af2643026900c45097171","Background: The macula is an important part of the human visual system and is responsible for clear and colour vision. Macular oedema happens when fluid and protein deposit on or below the macula of the eye and cause the macula to thicken and swell. Normally, it occurs due to diabetes called diabetic macular oedema. Diabetic macular oedema (DME) is one of the main causes of visual impairment in patients. Aim: The aims of the present study are to detect and localize abnormalities in blood vessels with respect to macula in order to prevent vision loss for the diabetic patients. Methods: In this work, a novel fully computerized algorithm is used for the recognition of various diseases in macula using both fundus images and optical coherence tomography (OCT) images. Abnormal blood vessels are segmented using thresholding algorithm. The classification is performed by three different classifiers, namely, the support vector machine (SVM), cascade neural network (CNN) and partial least square (PLS) classifiers, which are employed to identify whether the image is normal or abnormal. Conclusion: The results of all of the classifiers are compared based on their accuracy. The classifier accuracies of the SVM, cascade neural network and partial least square are 98.33, 97.16 and 94.34%, respectively. While analysing DME using both images, OCT produced efficient output than fundus images. Information about the severity of the disease and the localization of the pathologies is very useful to the ophthalmologist for diagnosing disease and choosing the proper treatment for a patient to prevent vision loss. © 2017, Royal Academy of Medicine in Ireland.","Cascade neural network; Diabetic macular oedema; Partial least square classifier; Support vector machine; Thresholding algorithm","Article; cascade neural network; classifier; diabetic macular edema; diabetic patient; diagnostic test accuracy study; disease severity; human; optical coherence tomography; partial least square; sensitivity and specificity; support vector machine; visual impairment",2-s2.0-85019181334
"Zhou W., Yu L., Zhou Y., Qiu W., Wu M.-W., Luo T.","Blind quality estimator for 3D images based on binocular combination and extreme learning machine",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022334694&doi=10.1016%2fj.patcog.2017.06.008&partnerID=40&md5=06a8de1b508e160d58523a595c1b0b8c","Over recent years, blind quality estimators for three-dimensional (3D) images have received increasing attention. In this paper, we describe a blind quality estimator for 3D images based on binocular combination and an extreme learning machine (ELM) for more accurate alignment with subjective human experience. First, two binocular combinations of stimuli are generated using different combination strategies. Various binocular quality-aware features of these combinations are then extracted by local binary pattern operators, which give an effective description of the degradation pattern. Finally, these features are mapped to the subjective quality score of the distorted 3D image using an ELM. Experimental results using three benchmark databases confirm that the proposed metric is effective and achieves competitive prediction performance when compared with most current full reference and blind metrics. © 2017 Elsevier Ltd","3D image quality assessment; Binocular combination; Extreme learning machine; Local pattern","Binoculars; Bins; Education; Knowledge acquisition; Learning systems; 3-D image; Binocular combination; Combination strategies; Extreme learning machine; Local binary pattern operators; Local patterns; Prediction performance; Three-dimensional (3D) image; Benchmarking",2-s2.0-85022334694
"Zhu C.","Double-fold localized multiple matrix learning machine with Universum",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007727&doi=10.1007%2fs10044-016-0548-9&partnerID=40&md5=d03cdfdb2886f933b32d9cef637a5b88","Matrix learning, multiple-view learning, Universum learning, and local learning are four hot spots of present research. Matrix learning aims to design feasible machines to process matrix patterns directly. Multiple-view learning takes pattern information from multiple aspects, i.e., multiple-view information into account. Universum learning can reflect priori knowledge about application domain and improve classification performances. A good local learning approach is important to the finding of local structures and pattern information. Our previous proposed learning machine, double-fold localized multiple matrix learning machine is a one with multiple-view information, local structures, and matrix learning. But this machine does not take Universum learning into account. Thus, this paper proposes a double-fold localized multiple matrix learning machine with Universum (Uni-DLMMLM) so as to improve the performance of a learning machine. Experimental results have validated that Uni-DLMMLM (1) makes full use of the domain knowledge of whole data distribution as well as inherits the advantages of matrix learning; (2) combines Universum learning with matrix learning so as to capture more global knowledge; (3) has a good ability to process different kinds of data sets; (4) has a superior classification performance and leads to a low empirical generation risk bound. © 2016, Springer-Verlag London.","Local learning; Matrix learning; Multiple-view learning; Universum learning",,2-s2.0-85032007727
"Duriez T., Brunton S.L., Noack B.R.","Machine learning control (MLC)",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994627528&doi=10.1007%2f978-3-319-40624-4_2&partnerID=40&md5=bd9f7065b0bba6559936d883acd61e3e","This chapter discusses the central topic of this book: the use of powerful techniques from machine learning to discover effective control laws for complex, nonlinear dynamics. The machine learning control (MLC) framework is then developed using genetic programming as a search algorithm to find control laws that are not accessible through linear control theory. Implementation details and example codes are also provided. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994627528
"Pankratz D.G., Choi Y., Imtiaz U., Fedorowicz G.M., Anderson J.D., Colby T.V., Myers J.L., Lynch D.A., Brown K.K., Flaherty K.R., Steele M.P., Groshong S.D., Raghu G., Barth N.M., Walsh P.S., Huang J., Kennedy G.C., Martinez F.J.","Usual interstitial pneumonia can be detected in transbronchial biopsies using machine learning",2017,"Annals of the American Thoracic Society",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031936279&doi=10.1513%2fAnnalsATS.201612-947OC&partnerID=40&md5=dded8b3b150c04b5960319aa10be7dc0","Rationale: Usual interstitial pneumonia (UIP) is the histopathologic hallmark of idiopathic pulmonary fibrosis. Although UIP can be detected by high-resolution computed tomography of the chest, the results are frequently inconclusive, and pathology from transbronchial biopsy (TBB) has poor sensitivity. Surgical lung biopsy may be necessary for a definitive diagnosis. Objectives: To develop a genomic classifier in tissue obtained by TBB that distinguishes UIP from non-UIP, trained against central pathology as the reference standard. Methods: Exome enriched RNA sequencing was performed on 283 TBBs from 84 subjects. Machine learning was used to train an algorithm with high rule-in (specificity) performance using specimens from 53 subjects. Performance was evaluated by crossvalidation and on an independent test set of specimens from 31 subjects. We explored the feasibility of a single molecular test per subject by combining multiple TBBs from upper and lower lobes. To address whether classifier accuracy depends upon adequate alveolar sampling, we tested for correlation between classifier accuracy and expression of alveolar-specific genes. Results: The top-performing algorithm distinguishes UIP from non-UIP conditions in single TBB samples with an area under the receiver operator characteristic curve (AUC) of 0.86, with specificity of 86% (confidence interval = 71-95%) and sensitivity of 63% (confidence interval = 51-74%) (31 test subjects). Performance improves to an AUC of 0.92 when three to five TBB samples per subject are combined at the RNA level for testing. Although we observed a wide range of type I and II alveolar-specific gene expression in TBBs, expression of these transcripts did not correlate with classifier accuracy. Conclusions: We demonstrate proof of principle that genomic analysis and machine learning improves the utility of TBB for the diagnosis of UIP, with greater sensitivity and specificity than pathology in TBB alone. Combining multiple individual subject samples results in increased test accuracy over single sample testing. This approach requires validation in an independent cohort of subjects before application in the clinic. Copyright © 2017 by the American Thoracic Society.","Idiopathic pulmonary fibrosis; Interstitial lung diseases; Molecular diagnostics",,2-s2.0-85031936279
"Harteveld D.O.C., Grant M.R., Pscheidt J.W., Peever T.L.","Predicting ascospore release of Monilinia vaccinii-corymbosi of blueberry with machine learning",2017,"Phytopathology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032505703&doi=10.1094%2fPHYTO-04-17-0162-R&partnerID=40&md5=35e4dc26cefc7cb1b0b9c7e12bf1a45c","Mummy berry, caused by Monilinia vaccinii-corymbosi, causes economic losses of highbush blueberry in the U.S. Pacific Northwest (PNW). Apothecia develop from mummified berries overwintering on soil surfaces and produce ascospores that infect tissue emerging from floral and vegetative buds. Disease control currently relies on fungicides applied on a calendar basis rather than inoculum availability. To establish a prediction model for ascospore release, apothecial development was tracked in three fields, one in western Oregon and two in northwestern Washington in 2015 and 2016. Air and soil temperature, precipitation, soil moisture, leaf wetness, relative humidity and solar radiation were monitored using in-field weather stations and Washington State University’s AgWeatherNet stations. Four modeling approaches were compared: logistic regression, multivariate adaptive regression splines, artificial neural networks, and random forest. A supervised learning approach was used to train the models on two data sets: training (70%) and testing (30%). The importance of environmental factors was calculated for each model separately. Soil temperature, soil moisture, and solar radiation were identified as the most important factors influencing ascospore release. Random forest models, with 78% accuracy, showed the best performance compared with the other models. Results of this research helps PNW blueberry growers to optimize fungicide use and reduce production costs. © 2017 The American Phytopathological Society.",,,2-s2.0-85032505703
"Zhang C., Kwon Y.P., Kramer J., Kim E., Agogino A.M.","Concept Clustering in Design Teams: A Comparison of Human and Machine Clustering",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030481749&doi=10.1115%2f1.4037478&partnerID=40&md5=08f91fdb05164e2cccace612b6d6ae03","Concept clustering is an important element of the product development process. The process of reviewing multiple concepts provides a means of communicating concepts developed by individual team members and by the team as a whole. Clustering, however, can also require arduous iterations and the resulting clusters may not always be useful to the team. In this paper, we present a machine learning approach on natural language descriptions of concepts that enables an automatic means of clustering. Using data from over 1000 concepts generated by student teams in a graduate new product development class, we provide a comparison between the concept clustering performed manually by the student teams and the work automated by a machine learning algorithm. The goal of our machine learning tool is to support design teams in identifying possible areas of ""over-clustering"" and/or ""under-clustering"" in order to enhance divergent concept generation processes.","Concept clustering; Design process; Machine learning; Word embedding","Artificial intelligence; Education; Learning systems; Product development; Students; Concept clustering; Concept generation process; Design process; Machine learning approaches; Natural languages; New product development; Product development process; Word embedding; Learning algorithms",2-s2.0-85030481749
"Chang P., Zhang J., Hu J., Song Z.","A Deep Neural Network Based on ELM for Semi-supervised Learning of Image Classification",2017,"Neural Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032805347&doi=10.1007%2fs11063-017-9709-0&partnerID=40&md5=93ab9c1d4f8fa68e7248e808ec1b0118","Deep learning has become one of very important machine learning methods in image classification, but most of them require a long training time to solve a non-convex optimization problem. In comparison, the training of extreme learning machine (ELM) is very simple, fast and effective. In order to combine the advantages of both methods, many researchers have tried to introduce ELM to deep architectures (Kasun et al. in IEEE Intell Syst 28:31–34, 2013; Yu et al. in Neurocomputing 149:308–315, 2015; Tissera and McDonnell in Neurocomputing 174:42–49, 2016 and in: Proceedings of ELM-2014, vol 1, Proceedings in adaptation, learning and optimization, vol 3, 2016; Junying et al. in Neurocomputing 171:63–72, 2016; Uzair et al. in Neural Comput Appl, 2015) to solve unsupervised learning and supervised learning problems. In this paper, we propose a new deep neural network based on ELM called discriminative deep ELM (DDELM) to address the semi-supervised learning problems in image classification. The proposed deep architecture consists of several stacked unsupervised ELMs and an additional label layer on the top layer of the stacked model. Experiments on three standard image data show that DDELM outperforms both representative semi-supervised learning algorithms and existing deep architectures such as DCNN in terms of accuracy and training time. © 2017 Springer Science+Business Media, LLC","Deep learning; ELM; Image classification; Semi-supervised learning","Aluminum; Convex optimization; Deep learning; Deep neural networks; Image classification; Learning systems; Network architecture; Optimization; Supervised learning; Deep architectures; Extreme learning machine; Machine learning methods; Neurocomputing; Nonconvex optimization; Semi- supervised learning; Standard images; Supervised learning problems; Learning algorithms",2-s2.0-85032805347
"Jootoo A., Lattanzi D.","Bridge Type Classification: Supervised Learning on a Modified NBI Data Set",2017,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028523009&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000712&partnerID=40&md5=fbe27c3ac8f9b3d17e7860c450ef2f68","A key phase in the bridge design process is the selection of the structural system. Due to budget and time constraints, engineers typically rely on engineering judgment and prior experience when selecting a structural system, often considering a limited range of design alternatives. The objective of this study was to explore the suitability of supervised machine learning as a preliminary design aid that provides guidance to engineers with regards to the statistically optimal bridge type to choose, ultimately improving the likelihood of optimized design, design standardization, and reduced maintenance costs. In order to devise this supervised learning system, data for more than 600,000 bridges from the National Bridge Inventory (NBI) database were analyzed. Key attributes for determining the bridge structure type were identified through three feature selection techniques. Potentially useful attributes like seismic intensity and historic data on the cost of materials (steel and concrete) were then added. Decision tree, Bayes network, and support vector machines were used for predicting the bridge design type. Due to state-to-state variations in material availability, material costs, and design codes, supervised learning models based on the complete data set did not yield favorable results. Supervised learning models were then trained and tested using 10-fold cross validation on data for each state. Inclusion of seismic data improved the model performance noticeably. The data were then resampled to reduce the bias of the models toward more common design types, and the supervised learning models thus constructed showed further improvements in performance. The average recall and precision for the state models were 88.6 and 88.0% using decision trees, 84.0 and 83.7% using Bayesian networks, and 80.8 and 75.6% using SVM. © 2017 American Society of Civil Engineers.","Bayesian networks; Bridge classification; Decision trees; Machine learning; National Bridge Inventory; Preliminary design; Supervised learning; Support vector machines","Artificial intelligence; Bayesian networks; Budget control; Classification (of information); Costs; Decision trees; Learning systems; Seismology; Supervised learning; Support vector machines; 10-fold cross-validation; Engineering judgments; Material availability; National Bridge Inventory; Preliminary design; Reduced maintenance costs; Selection techniques; Supervised machine learning; Availability",2-s2.0-85028523009
"Jurek A., Hong J., Chi Y., Liu W.","A novel ensemble learning approach to unsupervised record linkage",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022219737&doi=10.1016%2fj.is.2017.06.006&partnerID=40&md5=e214416938c0e9fd6c82255024095f90","Record linkage is a process of identifying records that refer to the same real-world entity. Many existing approaches to record linkage apply supervised machine learning techniques to generate a classification model that classifies a pair of records as either match or non-match. The main requirement of such an approach is a labelled training dataset. In many real-world applications no labelled dataset is available hence manual labelling is required to create a sufficiently sized training dataset for a supervised machine learning algorithm. Semi-supervised machine learning techniques, such as self-learning or active learning, which require only a small manually labelled training dataset have been applied to record linkage. These techniques reduce the requirement on the manual labelling of the training dataset. However, they have yet to achieve a level of accuracy similar to that of supervised learning techniques. In this paper we propose a new approach to unsupervised record linkage based on a combination of ensemble learning and enhanced automatic self-learning. In the proposed approach an ensemble of automatic self-learning models is generated with different similarity measure schemes. In order to further improve the automatic self-learning process we incorporate field weighting into the automatic seed selection for each of the self-learning models. We propose an unsupervised diversity measure to ensure that there is high diversity among the selected self-learning models. Finally, we propose to use the contribution ratios of self-learning models to remove those with poor accuracy from the ensemble. We have evaluated our approach on 4 publicly available datasets which are commonly used in the record linkage community. Our experimental results show that our proposed approach has advantages over the state-of-the-art semi-supervised and unsupervised record linkage techniques. In 3 out of 4 datasets it also achieves comparable results to those of the supervised approaches. © 2017","Classification; Data matching; Ensemble learning; Unsupervised record linkage","Artificial intelligence; Classification (of information); Data handling; Learning algorithms; Learning systems; Supervised learning; Classification models; Data matching; Ensemble learning; Ensemble learning approach; Real-world entities; Record linkage; Self-learning models; Supervised machine learning; Education",2-s2.0-85022219737
"Senft E., Baxter P., Kennedy J., Lemaignan S., Belpaeme T.","Supervised autonomy for online learning in human-robot interaction",2017,"Pattern Recognition Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016436619&doi=10.1016%2fj.patrec.2017.03.015&partnerID=40&md5=a704a74d95de3c54b941128db84d7f66","When a robot is learning it needs to explore its environment and how its environment responds on its actions. When the environment is large and there are a large number of possible actions the robot can take, this exploration phase can take prohibitively long. However, exploration can often be optimised by letting a human expert guide the robot during its learning. Interactive machine learning, in which a human user interactively guides the robot as it learns, has been shown to be an effective way to teach a robot. It requires an intuitive control mechanism to allow the human expert to provide feedback on the robot's progress. This paper presents a novel method which combines Reinforcement Learning and Supervised Progressively Autonomous Robot Competencies (SPARC). By allowing the user to fully control the robot and by treating rewards as implicit, SPARC aims to learn an action policy while maintaining human supervisory oversight of the robot's behaviour. This method is evaluated and compared to Interactive Reinforcement Learning in a robot teaching task. Qualitative and quantitative results indicate that SPARC allows for safer and faster learning by the robot, whilst not placing a high workload on the human teacher. © 2017","Human-Robot interaction; Interactive machine learning; Progressive Autonomy; Reinforcement learning; Robotics; Supervised autonomy","Artificial intelligence; Control theory; E-learning; Learning systems; Man machine systems; Reinforcement learning; Robotics; Robots; Teaching; Action policies; Exploration phase; Interactive machine learning; Interactive Reinforcement Learning; Intuitive controls; Progressive Autonomy; Quantitative result; Supervised autonomy; Human robot interaction",2-s2.0-85016436619
"Torres Vega M., Mocanu D.C., Liotta A.","Unsupervised deep learning for real-time assessment of video streaming services",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020116594&doi=10.1007%2fs11042-017-4831-6&partnerID=40&md5=4c64558e34501c0cbeb9ef674bb476d5","Evaluating quality of experience in video streaming services requires a quality metric that works in real time and for a broad range of video types and network conditions. This means that, subjective video quality assessment studies, or complex objective video quality assessment metrics, which would be best suited from the accuracy perspective, cannot be used for this tasks (due to their high requirements in terms of time and complexity, in addition to their lack of scalability). In this paper we propose a light-weight No Reference (NR) method that, by means of unsupervised machine learning techniques and measurements on the client side is able to assess quality in real-time, accurately and in an adaptable and scalable manner. Our method makes use of the excellent density estimation capabilities of the unsupervised deep learning techniques, the restricted Boltzmann machines, and light-weight video features computed just on the impaired video to provide a delta of quality degradation. We have tested our approach in two network impaired video sets, the LIMP and the ReTRiEVED video quality databases, benchmarking the results of our method against the well-known full reference metric VQM. We have obtained levels of accuracy of at least 85% in both datasets using all possible cases. © 2017, The Author(s).","Deep learning; No-reference video quality assessment; Quality of experience; Unsupervised machine learning","Artificial intelligence; Complex networks; Deep learning; Image quality; Learning algorithms; Learning systems; Quality of service; Video signal processing; Video streaming; No-reference video quality assessments; Objective video quality; Quality of experience (QoE); Restricted boltzmann machine; Subjective video quality; Unsupervised machine learning; Video quality database; Video streaming services; Benchmarking",2-s2.0-85020116594
"Pertusi D.A., Moura M.E., Jeffryes J.G., Prabhu S., Walters Biggs B., Tyo K.E.J.","Predicting novel substrates for enzymes with minimal experimental effort with active learning",2017,"Metabolic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031768728&doi=10.1016%2fj.ymben.2017.09.016&partnerID=40&md5=a6b839410b49a17d226e82cd8a05c470","Enzymatic substrate promiscuity is more ubiquitous than previously thought, with significant consequences for understanding metabolism and its application to biocatalysis. This realization has given rise to the need for efficient characterization of enzyme promiscuity. Enzyme promiscuity is currently characterized with a limited number of human-selected compounds that may not be representative of the enzyme's versatility. While testing large numbers of compounds may be impractical, computational approaches can exploit existing data to determine the most informative substrates to test next, thereby more thoroughly exploring an enzyme's versatility. To demonstrate this, we used existing studies and tested compounds for four different enzymes, developed support vector machine (SVM) models using these datasets, and selected additional compounds for experiments using an active learning approach. SVMs trained on a chemically diverse set of compounds were discovered to achieve maximum accuracies of ~80% using ~33% fewer compounds than datasets based on all compounds tested in existing studies. Active learning-selected compounds for testing resolved apparent conflicts in the existing training data, while adding diversity to the dataset. The application of these algorithms to wide arrays of metabolic enzymes would result in a library of SVMs that can predict high-probability promiscuous enzymatic reactions and could prove a valuable resource for the design of novel metabolic pathways. © 2017 International Metabolic Engineering Society","Active learning; Enzyme promiscuity; Machine learning","Artificial intelligence; Learning systems; Metabolism; Statistical tests; Substrates; Support vector machines; Active Learning; Computational approach; Enzymatic reaction; High probability; ITS applications; Maximum accuracies; Metabolic enzymes; Metabolic pathways; Enzymes; enzyme; algorithm; Article; data base; enzyme metabolism; enzyme substrate; enzyme substrate complex; machine learning; predictive value; priority journal; probability; support vector machine",2-s2.0-85031768728
"Affonso C., Rossi A.L.D., Vieira F.H.A., de Carvalho A.C.P.D.L.F.","Deep learning for biological image classification",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019578783&doi=10.1016%2fj.eswa.2017.05.039&partnerID=40&md5=e8424aa7f2b615a3ac2cf26cec4fcca9","A number of industries use human inspection to visually classify the quality of their products and the raw materials used in the production process, this process could be done automatically through digital image processing. The industries are not always interested in the most accurate technique for a given problem, but most appropriate for the expected results, there must be a balance between accuracy and computational cost. This paper investigates the classification of the quality of wood boards based on their images. For such, it compares the use of deep learning, particularly Convolutional Neural Networks, with the combination of texture-based feature extraction techniques and traditional techniques: Decision tree induction algorithms, Neural Networks, Nearest neighbors and Support vector machines. Reported studies show that Deep Learning techniques applied to image processing tasks have achieved predictive performance superior to traditional classification techniques, mainly in high complex scenarios. One of the reasons pointed out is their embedded feature extraction mechanism. Deep Learning techniques directly identify and extract features, considered by them to be relevant, in a given image dataset. However, empirical results for the image data set have shown that the texture descriptor method proposed, regardless of the strategy employed is very competitive when compared with Convolutional Neural Network for all the performed experiments. The best performance of the texture descriptor method could be caused by the nature of the image dataset. Finally are pointed out some perspectives of futures developments with the application of Active learning and Semi supervised methods. © 2017","Deep learning; Image classification; Machine learning; Wood classification","Artificial intelligence; Convolution; Data mining; Decision trees; Deep learning; Extraction; Feature extraction; Image processing; Image retrieval; Image texture; Learning algorithms; Learning systems; Neural networks; Supervised learning; Classification technique; Convolutional neural network; Decision tree induction; Extraction mechanisms; Feature extraction techniques; Predictive performance; Semi-supervised method; Traditional techniques; Image classification",2-s2.0-85019578783
"Kholghi M., De Vine L., Sitbon L., Zuccon G., Nguyen A.","Clinical information extraction using small data: An active learning approach based on sequence representations and word embeddings",2017,"Journal of the Association for Information Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030108381&doi=10.1002%2fasi.23936&partnerID=40&md5=4c408144d65a831ca31238cc5217dba2","This article demonstrates the benefits of using sequence representations based on word embeddings to inform the seed selection and sample selection processes in an active learning pipeline for clinical information extraction. Seed selection refers to choosing an initial sample set to label to form an initial learning model. Sample selection refers to selecting informative samples to update the model at each iteration of the active learning process. Compared to supervised machine learning approaches, active learning offers the opportunity to build statistical classifiers with a reduced amount of training samples that require manual annotation. Reducing the manual annotation effort can support automating the clinical information extraction process. This is particularly beneficial in the clinical domain, where manual annotation is a time-consuming and costly task, as it requires extensive labor from clinical experts. Our empirical findings demonstrate that (a) using sequence representations along with the length of sequence for seed selection shows potential towards more effective initial models, and (b) using sequence representations for sample selection leads to significantly lower manual annotation efforts, with up to 3% and 6% fewer tokens and concepts requiring annotation, respectively, compared to state-of-the-art query strategies. © 2017 ASIS&T",,"Classification (of information); Data mining; Information analysis; Information retrieval; Learning systems; Supervised learning; Active-learning process; Empirical findings; Manual annotation; Query strategies; Sample selection; State of the art; Statistical classifier; Supervised machine learning; Artificial intelligence; classifier; embedding; extraction; human; pipeline; plant seed; sampling; supervised machine learning",2-s2.0-85030108381
"Bouillon M., Anquetil E.","Online active supervision of an evolving classifier for customized-gesture-command learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020816320&doi=10.1016%2fj.neucom.2016.12.094&partnerID=40&md5=51298b6bf03fb62cb1793031c63bff67","Touch sensitive interfaces enable new interaction methods, like gesture commands. For users to easily memorize more than a dozen of gesture commands, it is important to enable gesture set customization. The classifier used to recognize drawn symbols must hence be customizable – able to learn from very few data – and evolving – able to learn new classes on-the-fly and improve during its use. The objective of this work is to obtain a gesture command system that cooperates as best as possible with the user: that learns from its mistakes without soliciting the user too often. This paper presents a novel approach for the online active learning of gesture commands, with three contributions. The IntuiSup supervisor monitors the learning process and user interactions. The Evolving Sampling by Uncertainty (ESU) algorithm enables to maintain the error/interaction compromise over time. The Boosted-ESU (B-ESU) method optimizes interaction impact to fasten system learning speed. The efficiency of our approach is evaluated on the publicly available ILG Data Base of gesture commands. Experimentation shows the effectiveness of the supervision strategy and improvements both in term of accuracy and learning speed. © 2017 Elsevier B.V.","Handwritten gesture command; Online active supervision; User interaction","Computer applications; Neural networks; Active Learning; Gesture commands; Interaction methods; Learning process; Online active supervision; Set customization; System learning; User interaction; E-learning; accuracy; algorithm; Article; automated pattern recognition; boosted evolving sampling by uncertainty; error; evolving sampling by uncertainty; experimentation; fuzzy system; gesture; human; human computer interaction; mathematical model; online system; priority journal; reference database; supervised machine learning; time",2-s2.0-85020816320
"Picek S., Heuser A., Guilley S.","Template attack versus Bayes classifier",2017,"Journal of Cryptographic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030781526&doi=10.1007%2fs13389-017-0172-7&partnerID=40&md5=5c044414eaa402b36ceefe47d19cc1c4","Side-channel attacks represent one of the most powerful categories of attacks on cryptographic devices with profiled attacks in a prominent place as the most powerful among them. Indeed, for instance, template attack is a well-known real-world attack that is also the most powerful attack from the information theoretical perspective. On the other hand, machine learning techniques have proved their quality in a numerous applications where one is definitely side-channel analysis. As one could expect, most of the research concerning supervised machine learning and side-channel analyses concentrated on more powerful machine learning techniques. Although valid from the practical perspective, such attacks often remain lacking from the more theoretical side. In this paper, we investigate several Bayes classifiers, which present simple supervised techniques that have significant similarities with the template attack. More specifically, our analysis aims to investigate what is the influence of the feature (in)dependence in datasets with different amount of noise and to offer further insight into the efficiency of machine learning for side-channel analysis. © 2017, Springer-Verlag GmbH Germany.","Bayes classifier; Feature dependence; Supervised machine learning; Template attack","Artificial intelligence; Cryptography; Information theory; Learning algorithms; Learning systems; Quality control; Supervised learning; Bayes Classifier; Cryptographic devices; Feature dependence; Machine learning techniques; Real-world attack; Side-channel analysis; Supervised machine learning; Template Attacks; Side channel attack",2-s2.0-85030781526
"Pan Y., Sun T., Liu Y., Yu H.","Composite learning from adaptive backstepping neural network control",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029671555&doi=10.1016%2fj.neunet.2017.08.005&partnerID=40&md5=409ca9595071e9a3439953e4221920c1","In existing neural network (NN) learning control methods, the trajectory of NN inputs must be recurrent to satisfy a stringent condition termed persistent excitation (PE) so that NN parameter convergence is obtainable. This paper focuses on command-filtered backstepping adaptive control for a class of strict-feedback nonlinear systems with functional uncertainties, where an NN composite learning technique is proposed to guarantee convergence of NN weights to their ideal values without the PE condition. In the NN composite learning, spatially localized NN approximation is employed to handle functional uncertainties, online historical data together with instantaneous data are exploited to generate prediction errors, and both tracking errors and prediction errors are employed to update NN weights. The influence of NN approximation errors on the control performance is also clearly shown. The distinctive feature of the proposed NN composite learning is that NN parameter convergence is guaranteed without the requirement of the trajectory of NN inputs being recurrent. Illustrative results have verified effectiveness and superiority of the proposed method compared with existing NN learning control methods. © 2017 Elsevier Ltd","Adaptive control; Backstepping; Learning control; Mismatched uncertainty; Neural network; Parameter convergence","Backstepping; Errors; Learning algorithms; Neural networks; Adaptive back-stepping; Adaptive Control; Functional uncertainty; Learning control; Mismatched uncertainty; Neural network control; Parameter convergence; Strict-feedback nonlinear systems; Adaptive control systems; Article; artificial neural network; control system; machine learning; neural network learning control; nonlinear system; prediction; priority journal; simulation; uncertainty",2-s2.0-85029671555
"Xu D., Wu J., Li D., Tian Y., Zhu X., Wu X.","SALE: Self-adaptive LSH encoding for multi-instance learning",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019444056&doi=10.1016%2fj.patcog.2017.04.029&partnerID=40&md5=e9776af6cb1541de1751a37703cdf2b7","Multi-instance learning (MIL) is commonly used to classify a set of instances, also known as a bag, where labels for the training set are only available for each bag. Many MIL methods exist, but they often suffer from high computation complexity and the key information from MIL being ignored, which deteriorates the classification performance. Recently, locality-sensitive hashing (LSH), with its high scalability, has shown the ability in enhancing MIL performance. However, for these LSH-based methods, the fixed number of bits is used to represent each projected dimension, resulting in subtle information loss and the algorithm performance reduction. In this paper, we propose a self-adaptive LSH encoding method for MIL, termed as SALE. SALE uses LSH to generate the primary batches, followed by a self-adaptive process for reconstruction. Reconstructed bags are transformed into random super histograms (RSH) using an incomplete coding method, and then weighted through a scheme that takes advantage of key instances. These weighted RSHs are used to train the learning model. SALE efficiently deals with large MIL problems, due to its low complexity and RSH's ability to exploit key information of MIL. Experiments demonstrate SALE's good performance compared to state-of-the-art MIL methods. © 2017 Elsevier Ltd","Locality-sensitive hashing; Machine learning; Multi-instance learning; Self-adaptive learning","Classification (of information); Encoding (symbols); Signal encoding; Algorithm performance; Classification performance; Computation complexity; High scalabilities; Information loss; Locality sensitive hashing; Multi-instance learning; Self-adaptive learning; Learning systems",2-s2.0-85019444056
"Bartocci E., Bortolussi L., Brázdil T., Milios D., Sanguinetti G.","Policy learning in continuous-time Markov decision processes using Gaussian Processes",2017,"Performance Evaluation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029590224&doi=10.1016%2fj.peva.2017.08.007&partnerID=40&md5=ec67075dc71ced996972d06a7b37d544","Continuous-time Markov decision processes provide a very powerful mathematical framework to solve policy-making problems in a wide range of applications, ranging from the control of populations to cyber–physical systems. The key problem to solve for these models is to efficiently compute an optimal policy to control the system in order to maximise the probability of satisfying a set of temporal logic specifications. Here we introduce a novel method based on statistical model checking and an unbiased estimation of a functional gradient in the space of possible policies. Our approach presents several advantages over the classical methods based on discretisation techniques, as it does not assume the a-priori knowledge of a model that can be replaced by a black-box, and does not suffer from state-space explosion. The use of a stochastic moment-based gradient ascent algorithm to guide our search considerably improves the efficiency of learning policies and accelerates the convergence using the momentum term. We demonstrate the strong performance of our approach on two examples of non-linear population models: an epidemiology model with no permanent recovery and a queuing system with non-deterministic choice. © 2017 Elsevier B.V.","Continuous-Time Markov decision processes; Gaussian processes; Machine learning; Policy learning","Embedded systems; Gaussian distribution; Gaussian noise (electronic); Learning algorithms; Learning systems; Markov processes; Model checking; Stochastic systems; Continuous time markov decision process; Gaussian Processes; Mathematical frameworks; Policy learning; State-space explosion; Statistical model checking; Temporal logic specifications; Unbiased estimation; Continuous time systems",2-s2.0-85029590224
"Li H., Wang Y., Wang H., Zhou B.","Multi-window based ensemble learning for classification of imbalanced streaming data",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014518180&doi=10.1007%2fs11280-017-0449-x&partnerID=40&md5=5a8514ca7a43f966454dea3f26fcf898","Imbalanced streaming data is commonly encountered in real-world data mining and machine learning applications, and has attracted much attention in recent years. Both imbalanced data and streaming data in practice are normally encountered together; however, little research work has been studied on the two types of data together. In this paper, we propose a multi-window based ensemble learning method for the classification of imbalanced streaming data. Three types of windows are defined to store the current batch of instances, the latest minority instances, and the ensemble classifier. The ensemble classifier consists of a set of latest sub-classifiers, and the instances employed to train each sub-classifier. All sub-classifiers are weighted prior to predicting the class labels of newly arriving instances, and new sub-classifiers are trained only when the precision is below a predefined threshold. Extensive experiments on synthetic datasets and real-world datasets demonstrate that the new approach can efficiently and effectively classify imbalanced streaming data, and generally outperforms existing approaches. © 2017, Springer Science+Business Media New York.","Class imbalance; Ensemble learning; Multi-window; Streaming data","Data mining; Learning systems; Class imbalance; Ensemble classifiers; Ensemble learning; Machine learning applications; Multi-Windows; Real-world datasets; Streaming data; Synthetic datasets; Classification (of information)",2-s2.0-85014518180
"Malatras A., Geneiatakis D., Vakalis I.","On the efficiency of user identification: a system-based approach",2017,"International Journal of Information Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977104414&doi=10.1007%2fs10207-016-0340-2&partnerID=40&md5=a346e1aad8571d097076c09ba590a731","In the Internet era, users’ fundamental privacy and anonymity rights have received significant research and regulatory attention. This is not only a result of the exponential growth of data that users generate when accomplishing their daily task by means of computing devices with advanced capabilities, but also because of inherent data properties that allow them to be linked with a real or soft identity. Service providers exploit these facts for user monitoring and identification, albeit impacting users’ anonymity, based mainly on personal identifiable information or on sensors that generate unique data to provide personalized services. In this paper, we report on the feasibility of user identification using general system features like memory, CPU and network data, as provided by the underlying operating system. We provide a general framework based on supervised machine learning algorithms both for distinguishing users and informing them about their anonymity exposure. We conduct a series of experiments to collect trial datasets for users’ engagement on a shared computing platform. We evaluate various well-known classifiers in terms of their effectiveness in distinguishing users, and we perform a sensitivity analysis of their configuration setup to discover optimal settings under diverse conditions. Furthermore, we examine the bounds of sampling data to eliminate the chances of user identification and thus promote anonymity. Overall results show that under certain configurations users’ anonymity can be preserved, while in other cases users’ identification can be inferred with high accuracy, without relying on personal identifiable information. © 2016, The Author(s).","Anonymity; Machine learning; User identification","Artificial intelligence; Learning systems; Natural language processing systems; Sensitivity analysis; Supervised learning; Anonymity; Computing devices; Computing platform; Exponential growth; Personalized service; Service provider; Supervised machine learning; User identification; Learning algorithms",2-s2.0-84977104414
"Kuster C., Rezgui Y., Mourshed M.","Electrical load forecasting models: A critical systematic review",2017,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027888921&doi=10.1016%2fj.scs.2017.08.009&partnerID=40&md5=9d13434eb139bf713b5de51354e2b744","Electricity forecasting is an essential component of smart grid, which has attracted increasing academic interest. Forecasting enables informed and efficient responses for electricity demand. However, various forecasting models exist making it difficult for inexperienced researchers to make an informed model selection. This paper presents a systematic review of forecasting models with the main purpose of identifying which model is best suited for a particular case or scenario. Over 113 different case studies reported across 41 academic papers have been used for the comparison. The timeframe, inputs, outputs, scale, data sample size, error type and value have been taken into account as criteria for the comparison. The review reveals that despite the relative simplicity of all reviewed models, the regression and/or multiple regression are still widely used and efficient for long and very long-term prediction. For short and very short-term prediction, machine-learning algorithms such as artificial neural networks, support vector machines, and time series analysis (including Autoregressive Integrated Moving Average (ARIMA) and the Autoregressive Moving Average (ARMA)) are favoured. The most widely employed independent variables are the building and occupancy characteristics and environmental data, especially in the machine learning models. In many cases, time series analysis and regressions rely on electricity historical data only, without the introduction of exogenous variables. Overall, if the singularity of the different cases made the comparison difficult, some trends are clearly identifiable. Considering the large amount of use cases studied, the meta-analysis of the references led to the identification of best practices within the expert community in relation to forecasting use for electricity consumption and power load prediction. Therefore, from the findings of the meta-analysis, a taxonomy has been defined in order to help researchers make an informed decision and choose the right model for their problem (long or short term, low or high resolution, building to country level). © 2017 Elsevier Ltd","Electric consumption and load prediction; Forecasting models; Long-term/short-term forecasting; Machine learning; Regression; Time series analysis","Artificial intelligence; Education; Electric power transmission networks; Electric power utilization; Forecasting; Harmonic analysis; Learning algorithms; Learning systems; Neural networks; Regression analysis; Smart power grids; Time series analysis; Auto-regressive integrated moving average; Autoregressive moving average; Electrical load forecasting; Electricity-consumption; Forecasting models; Load predictions; Machine learning models; Regression; Electric load forecasting",2-s2.0-85027888921
"Liu L., Chen R.-C.","A novel passenger flow prediction model using deep learning methods",2017,"Transportation Research Part C: Emerging Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028607037&doi=10.1016%2fj.trc.2017.08.001&partnerID=40&md5=21b8c97d7dbeb5b0a63e0a101a5a5f74","Currently, deep learning has been successfully applied in many fields and achieved amazing results. Meanwhile, big data has revolutionized the transportation industry over the past several years. These two hot topics have inspired us to reconsider the traditional issue of passenger flow prediction. As a special structure of deep neural network (DNN), an autoencoder can deeply and abstractly extract the nonlinear features embedded in the input without any labels. By exploiting its remarkable capabilities, a novel hourly passenger flow prediction model using deep learning methods is proposed in this paper. Temporal features including the day of a week, the hour of a day, and holidays, the scenario features including inbound and outbound, and tickets and cards, and the passenger flow features including the previous average passenger flow and real-time passenger flow, are defined as the input features. These features are combined and trained as different stacked autoencoders (SAE) in the first stage. Then, the pre-trained SAE are further used to initialize the supervised DNN with the real-time passenger flow as the label data in the second stage. The hybrid model (SAE-DNN) is applied and evaluated with a case study of passenger flow prediction for four bus rapid transit (BRT) stations of Xiamen in the third stage. The experimental results show that the proposed method has the capability to provide a more accurate and universal passenger flow prediction model for different BRT stations with different passenger flow profiles. © 2017 Elsevier Ltd","Autoencoder; BRT; Deep learning; Passenger flow; Prediction","Big data; Bus transportation; Deep neural networks; Forecasting; Learning systems; Rapid transit; Transportation; Auto encoders; Bus Rapid Transit; Nonlinear features; Passenger flow predictions; Passenger flows; Special structure; Temporal features; Transportation industry; Deep learning; artificial neural network; bus transport; machine learning; modeling; prediction; China; Fujian; Xiamen",2-s2.0-85028607037
"Umehara Y., Moronuki N.","Nondestructive inspection and inline estimation of profiles of copper-filled through-silicon vias with voids by a nano-focus X-ray microscope",2017,"Journal of Japan Institute of Electronics Packaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032633610&partnerID=40&md5=a1cbd33a2c3f91078bbb56c104aa62fe","We propose a nondestructive inline inspection and subsequent simple quantitative estimation of profles of copper-filled through-silicon vias (Cu-TSVs) by viewing one-shot-images of Cu-TSVs with a nanofocus X-ray microscope. A nano-focus X-ray microscope with a resolution of 250 nm makes it possible to observe detailed views of φ5 μm × 50 μm copper pillars and small defects inside them. We use image processing and a supervised machine learning method to classify the void profles. The voids are usually considered to have an indefnite shape. If rotationally symmetric features of the voids are recognized, the voids are modeled with rotationally symmetric structures and their positions in the Cu-TSVs' pillar and their volumes are estimated with quantifed dimensions. A cross-sectional view after focus ion beam (FIB) processing coincides well with the estimates of geometrical parameters. The classifed results can be linked to analysis of mismatches in the conditions during the electro-deposition process.","Cu-TSV; In-line inspection; Machine learning; Nano-focus X-ray microscope; Void","Artificial intelligence; Copper; Electronics packaging; Geometry; Image processing; Inspection; Integrated circuit interconnects; Ion beams; Learning systems; Nondestructive examination; Supervised learning; X ray microscopes; In-line inspections; Nano-focus; Non destructive inspection; Quantitative estimation; Supervised machine learning; Symmetric structures; Through silicon vias; Void; Three dimensional integrated circuits",2-s2.0-85032633610
"Rodan A., Sheta A.F., Faris H.","Bidirectional reservoir networks trained using SVM + privileged information for manufacturing process modeling",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976330844&doi=10.1007%2fs00500-016-2232-9&partnerID=40&md5=83ce66a89271a782c691d5d0874d1e51","In the last decade, a wide range of machine learning approaches were proposed and experimented to model highly nonlinear manufacturing processes. However, improving the performance of such models is challenging due to the complexity and high dimensionality of the manufacturing processes in general. In this paper, we propose bidirectional echo state reservoir networks (Bi-ESNs) trained using support vector machine privileged information method (SVM+) to model a winding machine process. The proposed model will be applied, tested and compared to reported models in the literature such as classical ESN with linear regression, ESN with a linear SVM readout, genetic programming, feedfoward neural network with backpropagation, radial basis function network, adaptive neural fuzzy inference system and local linear wavelet neural network. The developed results show that Bi-ESNs trained with SVM+ are promising. It was able to provide better generalization performance compared to other models. © 2016, Springer-Verlag Berlin Heidelberg.","Engineering process; Privileged information; Recurrent neural network; Reservoir computing; Support vector machines; Winding machines","Artificial intelligence; Complex networks; Fuzzy inference; Genetic algorithms; Genetic programming; Information use; Learning algorithms; Learning systems; Machine windings; Manufacture; Radial basis function networks; Recurrent neural networks; Winding; Winding machines; Adaptive neural fuzzy inference system (ANFIS); Engineering process; Generalization performance; Machine learning approaches; Manufacturing process modeling; Privileged information; Reservoir Computing; Wavelet neural networks; Support vector machines",2-s2.0-84976330844
"Dora L., Agrawal S., Panda R., Abraham A.","Optimal breast cancer classification using Gauss–Newton representation based algorithm",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019364170&doi=10.1016%2fj.eswa.2017.05.035&partnerID=40&md5=5a7ed15dad727f92954a3ae0e4a909cd","Breast cancer is a decisive disease worldwide. It is one of the most widely spread cancer among women. As per the survey, one out of eight women in the world are at risk of breast cancer at some point of time in her life. One of the methods to reduce breast cancer mortality rate is timely detection and effective treatment. That is why, more accurate classification of a breast cancer tumor has become a challenging problem in the medical field. Many classification techniques are proposed in the literature. Today, expert systems and machine learning techniques are being extensively used in the breast cancer classification problem. They provide high classification accuracy and effective diagnostic capabilities. In this paper, we have proposed a novel Gauss-Newton representation based algorithm (GNRBA) for breast cancer classification. It uses the sparse representation with training sample selection. Until now, sparse representation has been successfully applied in pattern recognition only. The proposed method introduces a novel Gauss-Newton based approach to find the optimal weights for the training samples for classification. In addition, it evaluates the sparsity in a computationally efficient way as compared to the conventional l1-norm method. The effectiveness of the GNRBA is examined on the Wisconsin Breast Cancer Database (WBCD) and the Wisconsin Diagnosis Breast Cancer (WDBC) database from the UCI Machine Learning repository. Various performance measures like classification accuracy, sensitivity, specificity, confusion matrices, a statistical test and the area under the receiver operating characteristic (AUC) are reported to show the superiority of the proposed method as compared to classical models. The experimental results show that the proposed GNRBA could be a good alternative for breast cancer classification for clinical experts. © 2017 Elsevier Ltd","Breast cancer classification; Euclidean distance measure; Gauss-Newton representation based algorithm; Sparse representation","Artificial intelligence; Diagnosis; Expert systems; Gaussian distribution; Learning algorithms; Learning systems; Pattern recognition; Sampling; Breast cancer classifications; Euclidean distance measure; Gauss Newton; Gauss-Newton based approach; Machine learning techniques; Receiver operating characteristics; Sparse representation; UCI machine learning repository; Diseases",2-s2.0-85019364170
"Siegel J.E., Bhattacharyya R., Kumar S., Sarma S.E.","Air filter particulate loading detection using smartphone audio and optimized ensemble classification",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030470709&doi=10.1016%2fj.engappai.2017.09.015&partnerID=40&md5=c29594968780a301786e0036b02e7697","Automotive engine intake filters ensure clean air delivery to the engine, though over time these filters load with contaminants hindering free airflow. Today's open-loop approach to air filter maintenance has drivers replace elements at predetermined service intervals, causing costly and potentially harmful over- and under-replacement. The result is that many vehicles consistently operate with reduced power, increased fuel consumption, or excessive particulate-related wear which may harm the catalyst or damage machined engine surfaces. We present a method of detecting filter contaminant loading from audio data collected by a smartphone and a stand microphone. Our machine learning approach to filter supervision uses Mel-Cepstrum, Fourier and Wavelet features as input into a classification model and applies feature ranking to select the best-differentiating features. We demonstrate the robustness of our technique by showing its efficacy for two vehicle types and different microphones, finding a best result of 79.7% accuracy when classifying a filter into three loading states. Refinements to this technique will help drivers supervise their filters and aid in optimally timing their replacement. This will result in an improvement in vehicle performance, efficiency, and reliability, while reducing the cost of maintenance to vehicle owners. © 2017 Elsevier Ltd","Ambient intelligence; Data mining and knowledge discovery; Emerging applications and technology; Intelligent vehicles; Machine learning","Air filters; Air intakes; Ambient intelligence; Artificial intelligence; Bandpass filters; Classification (of information); Data mining; Engines; Filtration; Impurities; Intelligent vehicle highway systems; Learning algorithms; Learning systems; Microphones; Smartphones; Vehicle performance; Vehicles; Classification models; Contaminant loadings; Cost of maintenance; Data mining and knowledge discovery; Emerging applications; Ensemble classification; Machine learning approaches; Particulate loading; Loading",2-s2.0-85030470709
"Goh A.T.C., Zhang Y., Zhang R., Zhang W., Xiao Y.","Evaluating stability of underground entry-type excavations using multivariate adaptive regression splines and logistic regression",2017,"Tunnelling and Underground Space Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026675165&doi=10.1016%2fj.tust.2017.07.013&partnerID=40&md5=4ffe542d22de2bdfaa04468fee899988","The mining industry relies heavily on the use of empirical methods and charts for the design and assessment of entry-type excavations. The commonly adopted empirical design method, commonly referred to as the critical span graph, which was specifically developed for the assessment of rock stability in entry-type excavations, was based on an extensive database of cut and fill mining operations and case histories in Canada. It plots the critical span versus the rock mass rating for the observed case histories and has been widely accepted for an initial span design of cut and fill stopes. Different approaches, either based on classical regression and classification statistical techniques or even the supervised machine learning methods, have been proposed to classify the observed cases into stable, potentially unstable and unstable groups. This paper presents a new assessment approach which combines the use of a multivariate adaptive regression splines (MARS) approach and the logistic regression (LR) method. The proposed MARS_LR model can capture and describe the intrinsic, complex relationship between input descriptors and the dependent response without having to make any assumptions about the underlying relationship. Considering its simplicity in interpretation, predictive accuracy, its data-driven and adaptive nature plus the ability to map the interaction between variables, the use of MARS_LR model in evaluating stability of underground entry-type excavations is promising. © 2017 Elsevier Ltd","Basis function; Entry-type excavations; Limit state function; Logistic regression; Multivariate adaptive regression splines; Stability","Convergence of numerical methods; Excavation; Function evaluation; Learning systems; Splines; Stability; Supervised learning; Assessment approaches; Basis functions; Complex relationships; Limit state functions; Logistic regressions; Multivariate adaptive regression splines; Statistical techniques; Supervised machine learning; Regression analysis; accuracy assessment; design method; excavation; machine learning; mining industry; multivariate analysis; regression analysis; stability analysis; underground construction; Canada",2-s2.0-85026675165
"Peng X., Shen J.","A twin-hyperspheres support vector machine with automatic variable weights for data classification",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024485686&doi=10.1016%2fj.ins.2017.07.007&partnerID=40&md5=639dc0b1ff34d49b825bf136f44a1290","This paper proposes a novel twin-hyperspheres support vector machine (THSVM) classifier for binary classification, called the automatic variable-weighted THSVM (VTHSVM) classifier. By solving a single optimization problem, this classifier not only finds a pair of hyperspheres for classification, but also automatically constructs a weight vector for each class in order to describe the dissimilarity of different classes. This VTHSVM is extended to the kernel case by the fact that a kernel can be written as a sum of one's evaluated on each variable separately. The main advantage of this method is that it allows the use of adaptive distance, which is suitable to find an as compact as possible hypersphere for each class. Experiments with synthetic and benchmark datasets indicate VTHSVM obtains better performance than some other classifiers. © 2017 Elsevier Inc.","Alternate learning strategy; Hyperspheres; Pattern recognition; Twin support vector machine; Variable weights","Benchmarking; Optimization; Pattern recognition; Support vector machines; Vectors; Benchmark datasets; Binary classification; Data classification; Hyper-spheres; Learning strategy; Optimization problems; Twin support vector machines; Variable weight; Classification (of information)",2-s2.0-85024485686
"Suárez-Cetrulo A.L., Cervantes A.","An online classification algorithm for large scale data streams: iGNGSVM",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020486793&doi=10.1016%2fj.neucom.2016.12.093&partnerID=40&md5=464f2be0f9dbe24d178e6743b455398c","Stream Processing has recently become one of the current commercial trends to face huge amounts of data. However, normally these techniques need specific infrastructures and high resources in terms of memory and computing nodes. This paper shows how mini-batch techniques and topology extraction methods can help making gigabytes of data to be manageable for just one server using computationally costly Machine Learning techniques as Support Vector Machines. The algorithm iGNGSVM is proposed to improve the performance of Support Vector Machines in datasets where the data is continuously arriving. It is benchmarked against a mini-batch version of LibSVM, achieving good accuracy rates and performing faster than this. © 2017 Elsevier B.V.","Data classification; Growing Neural Gas; Large datasets; Online learning; Support Vector Machines; Topology extraction","Classification (of information); Extraction; Learning systems; Support vector machines; Topology; Data classification; Extraction method; Growing neural gas; Large datasets; Machine learning techniques; On-line classification; Online learning; Stream processing; Data mining; accuracy; algorithm; Article; calculation; classification algorithm; cost benefit analysis; data analysis; extraction; linear system; mathematical computing; nonlinear system; online system; priority journal; support vector machine",2-s2.0-85020486793
"Minaei S., Shafiee S., Polder G., Moghadam-Charkari N., van Ruth S., Barzegar M., Zahiri J., Alewijn M., Kuś P.M.","VIS/NIR imaging application for honey floral origin determination",2017,"Infrared Physics and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541305&doi=10.1016%2fj.infrared.2017.09.001&partnerID=40&md5=b969f648c8fb784f3f2a407bd25fbfd1","Nondestructive methods are of utmost importance for honey characterization. This study investigates the potential application of VIS-NIR hyperspectral imaging for detection of honey flower origin using machine learning techniques. Hyperspectral images of 52 honey samples were taken in transmittance mode in the visible/near infrared (VIS-NIR) range (400–1000 nm). Three different machine learning algorithms were implemented to predict honey floral origin using honey spectral images. These methods, included radial basis function (RBF) network, support vector machine (SVM), and random forest (RF). Principal component analysis (PCA) was also exploited for dimensionality reduction. According to the obtained results, the best classifier (RBF) achieved a precision of 94% in a fivefold cross validation experiment using only the first two PCs. Mapping of the classifier results to the test set images showed 90% accuracy for honey images. Three types of honey including buckwheat, rapeseed and heather were classified with 100% accuracy. The proposed approach has great potential for honey floral origin detection. As some other honey properties can also be predicted using image features, in addition to floral origin detection, this method may be applied to predict other honey characteristics. © 2017 Elsevier B.V.","Honey floral origin; NIR hyperspectral imaging; Radial basis function network; Random forest; Support vector machine","Artificial intelligence; Classification (of information); Decision trees; Food products; Functions; Infrared devices; Learning algorithms; Learning systems; Principal component analysis; Radial basis function networks; Spectroscopy; Support vector machines; Dimensionality reduction; Honey floral origin; Imaging applications; Machine learning techniques; NIR hyperspectral imaging; Nondestructive methods; Random forests; Visible/near infrared; Hyperspectral imaging",2-s2.0-85030541305
"Feuz K.D., Cook D.J.","Collegial activity learning between heterogeneous sensors",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016103330&doi=10.1007%2fs10115-017-1043-3&partnerID=40&md5=4e1b1cf615afb1b0755459f7e2d3c1ec","Activity recognition algorithms have matured and become more ubiquitous in recent years. However, these algorithms are typically customized for a particular sensor platform. In this paper, we introduce PECO, a Personalized activity ECOsystem, that transfers learned activity information seamlessly between sensor platforms in real time so that any available sensor can continue to track activities without requiring its own extensive labeled training data. We introduce a multi-view transfer learning algorithm that facilitates this information handoff between sensor platforms and provide theoretical performance bounds for the algorithm. In addition, we empirically evaluate PECO using datasets that utilize heterogeneous sensor platforms to perform activity recognition. These results indicate that not only can activity recognition algorithms transfer important information to new sensor platforms, but any number of platforms can work together as colleagues to boost performance. © 2017, Springer-Verlag London.","Activity recognition; Machine learning; Pervasive computing; Transfer learning",,2-s2.0-85016103330
"Welikala R.A., Foster P.J., Whincup P.H., Rudnicka A.R., Owen C.G., Strachan D.P., Barman S.A.","Automated arteriole and venule classification using deep learning for retinal images from the UK Biobank cohort",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029315560&doi=10.1016%2fj.compbiomed.2017.09.005&partnerID=40&md5=8b999c8840dd14be96d0a594fa59b4f5","The morphometric characteristics of the retinal vasculature are associated with future risk of many systemic and vascular diseases. However, analysis of data from large population based studies is needed to help resolve uncertainties in some of these associations. This requires automated systems that extract quantitative measures of vessel morphology from large numbers of retinal images. Associations between retinal vessel morphology and disease precursors/outcomes may be similar or opposing for arterioles and venules. Therefore, the accurate detection of the vessel type is an important element in such automated systems. This paper presents a deep learning approach for the automatic classification of arterioles and venules across the entire retinal image, including vessels located at the optic disc. This comprises of a convolutional neural network whose architecture contains six learned layers: three convolutional and three fully-connected. Complex patterns are automatically learnt from the data, which avoids the use of hand crafted features. The method is developed and evaluated using 835,914 centreline pixels derived from 100 retinal images selected from the 135,867 retinal images obtained at the UK Biobank (large population-based cohort study of middle aged and older adults) baseline examination. This is a challenging dataset in respect to image quality and hence arteriole/venule classification is required to be highly robust. The method achieves a significant increase in accuracy of 8.1% when compared to the baseline method, resulting in an arteriole/venule classification accuracy of 86.97% (per pixel basis) over the entire retinal image. © 2017 Elsevier Ltd","Arteriole/venule classification; Convolutional neural networks; Deep learning; Epidemiological studies; Retinal images; UK Biobank","Automation; Blood vessels; Classification (of information); Convolution; Deep learning; Image classification; Neural networks; Pixels; Population statistics; Uncertainty analysis; Automatic classification; Classification accuracy; Convolutional neural network; Epidemiological studies; Morphometric characteristics; Quantitative measures; Retinal image; UK Biobank; Ophthalmology; adult; Article; automation; biobank; cohort analysis; convolutional neural network; diagnostic error; eye bank; false positive result; fundus camera; human; image enhancement; image processing; image quality; image segmentation; machine learning; measurement accuracy; middle aged; optic disk; predictive value; priority journal; retina blood vessel; retina image; retina vein; retina venule; retinal arteriole; sensitivity and specificity; true positive result; United Kingdom",2-s2.0-85029315560
"Shtiliyanova A., Bellocchi G., Borras D., Eza U., Martin R., Carrère P.","Kriging-based approach to predict missing air temperature data",2017,"Computers and Electronics in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030854670&doi=10.1016%2fj.compag.2017.09.033&partnerID=40&md5=6e3b3d5a230dddbe3d3e19b65f4f9a4b","The geo-statistical Kriging method is conventionally used in the spatial dimension to predict missing values in a series by utilizing information from neighbouring data, supported by the hypothesis that mathematical expectation is a function of distance between observations. By using a data-driven machine learning-based inferencing and exploration framework, this research applies a Kriging-based interpolation in the temporal dimension to fill in data gaps in time-series of air temperatures. It assesses its performance for artificial gap scenarios (ranging in length from single one to six consecutive data points) generated using data with both daily and hourly resolutions from five sites in Europe (Laqueuille, France; Grillenburg, Germany; Monte Bondone, Italy; Oensingen, Switzerland; Rothamsted, United Kingdom) and one in France overseas (Sedael, Réunion Island). Results show that the method is capable of predicting missing temperatures with acceptable accuracy, especially with the hourly resolution and for non-high elevation sites: modeling efficiency (EF ≤ 1, optimum) >0.8, with the exception of Monte Bondone, placed at >2000 m a.s.l. (EF < 0). With daily data, maximum temperature was correctly predicted at all sites (0.6 ≤ EF < 0.9), while some less accuracy (down to EF < 0.4) was noted when predicting missing daily minimum temperatures. In conclusion, the method appears suitable to be applied to fill in hourly temperature gaps, requiring more stringent hypotheses concerning daily data and mountain sites (but further studies are required to draw concluding recommendations). © 2017 Elsevier B.V.","Air temperature; Daily and hourly resolutions; Data-driven machine learning; Infilling; Kriging-based temporal interpolation","Artificial intelligence; Atmospheric temperature; Forecasting; Functions; Learning systems; Air temperature; Daily and hourly resolutions; Data driven; Infilling; Temporal interpolation; Interpolation; accuracy assessment; air temperature; exploration; geostatistics; interpolation; kriging; machine learning; mountain region; numerical model; temporal analysis; Europe; France",2-s2.0-85030854670
"Hathout R.M., Gad H.A., Metwally A.A.","Gelatinized-core liposomes: Toward a more robust carrier for hydrophilic molecules",2017,"Journal of Biomedical Materials Research - Part A",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028379201&doi=10.1002%2fjbm.a.36175&partnerID=40&md5=85aaeccdc48aa3a5070712d08669e7b0","The use of liposomes as a delivery system for hydrophobic and hydrophilic drugs is well recognized. However, they possess several limitations that remained unresolved, including stability problems, low entrapment of the hydrophilic drugs, and the subsequent rapid release. This study introduces a novel approach to incorporate gelatin in the liposomal core to overcome these limitations. A rheological study was conducted to select suitable masses of the gelatin used in the liposomal formulations. Moreover, a full-factorial experimental design was utilized to compare the newly produced gel–core liposomes to the conventional liposomes with respect to the amount of a model hydrophilic molecule loading. An advanced machine learning method, namely, artificial neural networks was utilized to capture the effects of gelatin and cholesterol incorporation in the liposomes on the entrapment efficiency. The results revealed the successful preparation of the novel vesicles and their superiority over the conventional liposomes in drug loading, sustaining the drug release and stability which pose the newly introduced liposomal system as a successful delivery carrier for hydrophilic molecules and drugs. © 2017 Wiley Periodicals, Inc. J Biomed Mater Res Part A: 105A: 3086–3092, 2017. © 2017 Wiley Periodicals, Inc.","artificial neural networks; full-factorial; gelatin; hydrophilic; liposomes","Hydrophilicity; Learning systems; Molecules; Neural networks; Entrapment efficiency; Full factorial; Gelatin; Hydrophilic; Hydrophilic molecules; Hydrophobic and hydrophilic; Liposomal formulation; Machine learning methods; Liposomes; doxycycline; gel core liposome; gelatin; liposome; salicylate sodium; unclassified drug; animal tissue; Article; artificial neural network; drug release; drug stability; experimental design; factorial design; flow kinetics; gelatinization; hydrophilicity; liposomal delivery; machine learning; nonhuman; particle size",2-s2.0-85028379201
"Park J.-W., Kim E.","Runtime prediction of parallel applications with workload-aware clustering",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017169457&doi=10.1007%2fs11227-017-2038-2&partnerID=40&md5=08d569c6aac92a92d1ec20f6c24af3da","Traditionally, many science fields require great support for a massive workflow, which utilizes multiple cores simultaneously. In order to support such large-scale scientific workflows, high-capacity parallel systems such as supercomputers are widely used. To increase the utilization of these systems, most schedulers use backfilling policy based on user’s estimated runtime. However, it is found to be extremely inaccurate because users overestimate their jobs. Therefore, in this paper, an efficient machine learning approach is present to predict the runtime of parallel application. The proposed method is divided into three phases. First is to analyze important feature of the history log data by factor analysis. Second is to carry out clustering for the parallel program based on the important features. Third is to build a prediction models by pattern similarity of parallel program log data and estimate runtime. In the experiments, we use workload logs on parallel systems (i.e., NASA-iPSC, LANL-CM5, SDSC-Par95, SDSC-Par96, and CTC-SP2) to evaluate the effectiveness of our approach. Comparing root-mean-square error with other techniques, experimental results show that the proposed method improves the accuracy up to 69.56%. © 2017, Springer Science+Business Media New York.","Machine learning approach; Runtime prediction; Support vector regression; Workload-aware clustering","Artificial intelligence; Data flow analysis; Learning systems; Mean square error; NASA; Supercomputers; Important features; Machine learning approaches; Parallel application; Root mean square errors; Runtimes; Scientific workflows; Support vector regression (SVR); Workload-aware clustering; Forecasting",2-s2.0-85017169457
"Chong E., Park F.C.","Movement prediction for a lower limb exoskeleton using a conditional restricted Boltzmann machine",2017,"Robotica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997241442&doi=10.1017%2fS0263574716000795&partnerID=40&md5=79df31c8e2a83f1209c4d4e014d68581","We propose a novel class of unsupervised learning-based algorithms that extend the conditional restricted Boltzmann machine to predict, in real-time, a lower limb exoskeleton wearer's intended movement type and future trajectory. During training, our algorithm automatically clusters unlabeled exoskeletal measurement data into movement types. Our predictor then takes as input a short time series of measurements, and outputs in real-time both the movement type and the forward trajectory time series. Physical experiments with a prototype exoskeleton demonstrate that our method more accurately and stably predicts both movement type and the forward trajectory compared to existing methods. © Copyright Cambridge University Press 2016.","Conditional restricted Boltzmann machine; Lower limb exoskeleton; Movement prediction; Trajectory generation; Unsupervised learning","Forecasting; Time series; Trajectories; Unsupervised learning; Conditional restricted boltzmann machines; Learning-based algorithms; Lower limb; Measurement data; Movement prediction; Physical experiments; Short time series; Trajectory generation; Exoskeleton (Robotics)",2-s2.0-84997241442
"Sun W., Trevor B.","Combining k-nearest-neighbor models for annual peak breakup flow forecasting",2017,"Cold Regions Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030852214&doi=10.1016%2fj.coldregions.2017.08.009&partnerID=40&md5=0630945506056929ee4d83fcd4876d4b","In practice, water resources managers and river forecasters often face the problem in identifying from the historical record years that show a similar pattern of weather and river conditions as the current year is following. The k-nearest neighbor (kNN) algorithm is such a type of machine learning method, where no assumptions on the dataset distributions are made, but a minimal (no explicit) training stage is required. This is especially useful in river ice forecasting where weather and river condition indicators often do not obey the typical theoretical assumptions; the complex river ice mechanisms are difficult to directly quantify. However, few applications of kNN to river ice studies have been reported. Since the predictive performance of a single kNN is limited, we combine multiple KNN models (first-level) based on different numbers of inputs which are linearly or nonlinearly correlated to annual peak breakup flow (APBF); we use both simple average methods and kNN models as combining models (second-level), and we apply the kNN-based combination models to APBF forecasting of the Smoky River at Watino, Canada. The Mahalanobis distance is chosen in kNN since neither the standardization of input variables nor the assignments of weights to variables are required. The historical breakup data at this site for the past 33 years (1980 to 2012) were collected to facilitate the performance comparison of models. The results show that both training and validation performances of the combining models are substantially better than those of member models. The proposed framework can be potentially applied to various forecasting problems in water resources management practice. © 2017 Elsevier B.V.","Breakup; Combination; Flooding; Forecasting; Machine learning; River ice","Artificial intelligence; Drop breakup; Forecasting; Ice; Learning systems; Motion compensation; Nearest neighbor search; Oil well flooding; Personnel training; Rivers; Water resources; Combination; K nearest neighbor algorithm; Machine learning methods; Mahalanobis distances; Performance comparison; Predictive performance; River ice; Water resources management; Weather forecasting; flooding; forecasting method; machine learning; management practice; nearest neighbor analysis; peak flow; river ice; water management; water resource; Canada",2-s2.0-85030852214
"Schleif F.-M., Tino P.","Indefinite Core Vector Machine",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023183700&doi=10.1016%2fj.patcog.2017.06.003&partnerID=40&md5=82fb1508e14a9dda57aac9a0d52a40d4","The recently proposed Krĕin space Support Vector Machine (KSVM) is an efficient classifier for indefinite learning problems, but with quadratic to cubic complexity and a non-sparse decision function. In this paper a Krĕin space Core Vector Machine (iCVM) solver is derived. A sparse model with linear runtime complexity can be obtained under a low rank assumption. The obtained iCVM models can be applied to indefinite kernels without additional preprocessing. Using iCVM one can solve CVM with usually troublesome kernels having large negative eigenvalues or large numbers of negative eigenvalues. Experiments show that our algorithm is similar efficient as the Krĕin space Support Vector Machine but with substantially lower costs, such that also large scale problems can be processed. © 2017 Elsevier Ltd","Classification; Core Vector Machine; Indefinite learning; Krĕin space; Linear complexity; Nyström; Sparse","Classification (of information); Education; Eigenvalues and eigenfunctions; Support vector machines; Vectors; Core vector machines; Decision functions; Indefinite kernel; Indefinite learning; Large-scale problem; Linear complexity; Run time complexity; Sparse; Vector spaces",2-s2.0-85023183700
"Xie C., Lv J., Li X.","Finding a good initial configuration of parameters for restricted Boltzmann machine pre-training",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031744770&doi=10.1007%2fs00500-016-2205-z&partnerID=40&md5=a3008ab4482bd6ea80c02f3c7d05dc6b","Restricted Boltzmann machines (RBMs) have been successfully applied in unsupervised learning and image density-based modeling. The aim of the pre-training step for RBMs is to discover an unknown stationary distribution based on the sample data that has the lowest energy. However, conventional RBM pre-training is sensitive to the initial weights and bias. The selection of initial values in RBM pre-training will directly affect the capabilities and efficiency of the learning process. This paper uses principal component analysis to capture the principal component directions of the training data. A set of initial parameter values for the RBM can be obtained by computing the same reconstruction of the data. Experiments on the Yale and MNIST datasets show that the proposed method not only retains a strong learning ability, but also significantly accelerates the learning speed. © 2016, Springer-Verlag Berlin Heidelberg.","PCA; Pre-training; RBM; Unsupervised learning","Unsupervised learning; Initial configuration; Initial parameter; Learning abilities; Learning process; Pre-training; Principal Components; Restricted boltzmann machine; Stationary distribution; Principal component analysis",2-s2.0-85031744770
"Pratama M., Lughofer E., Er M.J., Anavatti S., Lim C.-P.","Data driven modelling based on Recurrent Interval-Valued Metacognitive Scaffolding Fuzzy Neural Network",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021191119&doi=10.1016%2fj.neucom.2016.10.093&partnerID=40&md5=a6520eff3d35107503006b8499382ac6","The Metacognitive Scaffolding Learning Machine (McSLM), combining the concept of metacognition—what-to-learn, how-to-learn, and when-to-learn, and the Scaffolding theory—a tutoring theory for a learner to learn a complex task, has been successfully developed to enhance the capability of Evolving Intelligent Systems (EIS) in processing non-stationary data streams. Three issues, namely uncertainty, temporal behaviour, unknown system order, are however uncharted by any existing McSLMs and all McSLMs in the literature are designed for classification problems. This paper proposes a novel McSLM, called Recurrent Interval-Valued Metacognitive Scaffolding Fuzzy Neural Network (RIVMcSFNN) and used to solve regression and time-series modelling problems from data streams. RIVMcSFNN presents a novel recurrent network architecture as a cognitive constituent, which features double local recurrent connections at both the hidden layer and the consequent layer. The new recurrent network architecture is driven by the interval-valued multivariate Gaussian function in the hidden node and the nonlinear Wavelet function in the consequent node. As with its predecessors, the RIVMcSFNN characterises an open structure, where it can automatically grow, prune, adjust, merge, recall its hidden node and can select relevant data samples on the fly using an online active learning methodology. The RIVMcSFNN is also equipped with the online dimensionality reduction technique to cope with the curse of dimensionality. All learning mechanisms are carried out in the single-pass and local learning mode and actualise the plug-and-play learning principle, which aims to minimise the use of pre-and/or post-training steps. The efficacy of our algorithm was tested using numerous data-driven modelling problems and comprehensive comparisons with its counterparts. The RIVMcSFNN demonstrated substantial improvements in both accuracy and complexity against existing variants of the McSLMs and EISs. © 2017","Evolving fuzzy system; Fuzzy neural network; Metacognitive learning; Online learning; Type-2 fuzzy system","Complex networks; Data communication systems; E-learning; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Fuzzy systems; Intelligent systems; Learning systems; Network architecture; Scaffolds; Dimensionality reduction techniques; Evolving Fuzzy Systems; Evolving intelligent systems; Meta-cognitive learning; Metacognitive scaffoldings; Multivariate Gaussian functions; Online learning; Type-2 fuzzy systems; Education; accuracy; algorithm; Article; calculation; data analysis; fuzzy system; geometry; kernel method; linear system; machine learning; nonlinear system; prediction; priority journal; probability; recurrent interval valued metacognitive scaffolding fuzzy neural network; uncertainty; wavelet analysis",2-s2.0-85021191119
"Lee S., Kim W.","Sentiment labeling for extending initial labeled data to improve semi-supervised sentiment classification",2017,"Electronic Commerce Research and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030475861&doi=10.1016%2fj.elerap.2017.09.006&partnerID=40&md5=cb6a59476b1a93f5df8223bcdd438ad5","In recent decades, analyzing the sentiments in online customer reviews has become important to many businesses and researchers. However, insufficient amount of labeled training corpus is a bottleneck for machine learning approaches. Self-training is one of the promising semi-supervised techniques which does not require large amounts of labeled data. However, self-training also suffers from an incorrect labeling problem along with insufficient amount of labeled data. This study proposed a semi-supervised learning framework that adds only confidently predicted data to the training corpus in order to enrich the initial classifier in self-training. The experimental results indicate that the proposed method performed better than self-training. © 2017 Elsevier B.V.","Concatenated vector; Paragraph vector; Self-training; Sentiment classification; Sentiment labeling; Topic model","Learning algorithms; Learning systems; Machine learning approaches; Online customer reviews; Self training; Semi- supervised learning; Semi-supervised; Sentiment classification; Topic Modeling; Training corpus; Supervised learning",2-s2.0-85030475861
"Memar S., Delrobaei M., Gilmore G., McIsaac K., Jog M.","Segmentation and detection of physical activities during a sitting task in Parkinson's disease participants using multiple inertial sensors",2017,"Journal of Applied Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020786634&doi=10.1016%2fj.jab.2017.05.002&partnerID=40&md5=c92cdd0ff5795eaf7e94092cdbd42947","Introduction The development of inertial sensors in motion capture systems enables precise measurement of motor symptoms in Parkinson's disease (PD). The type of physical activities performed by the PD participants is an important factor to compute objective scores for specific motor symptoms of the disease. The goal of this study is to propose an approach to automatically detect the physical activities over a period time and segment the time stamps for such detected activities. Methods A wearable motion capture sensor system using inertial measurement units (IMUs) was used for data collection. Data from the sensors attached to the shoulders, elbows, and wrists were utilized for detecting and segmenting the activities. An unsupervised machine learning algorithm was employed to extract suitable features from the appropriate sensors and classify the data points to the corresponding activity group. Results The performance of the proposed technique was evaluated with respect to the manually labeled and segmented activities. The experimental results reveal that the proposed auto detection technique – by obtaining high average scores of accuracy (96%), precision (96%), and recall (98%) – is able to effectively detect the activities during the sitting task and segment them to the proper time stamps. © 2017 Faculty of Health and Social Sciences, University of South Bohemia in Ceske Budejovice","Activity detection; Auto segmentation; Machine learning; Parkinson's disease","accelerometry; Article; automation; case control study; clinical article; controlled study; data collection method; diagnostic accuracy; diagnostic test accuracy study; double blind procedure; elbow; female; human; image segmentation; inertial measurement unit; male; measurement; motor activity; Parkinson disease; pattern recognition; physical activity; recall; shoulder; signal processing; sitting; task performance; unsupervised machine learning; wrist",2-s2.0-85020786634
"Trambaiolli L.R., Biazoli C.E., Jr., Balardin J.B., Hoexter M.Q., Sato J.R.","The relevance of feature selection methods to the classification of obsessive-compulsive disorder based on volumetric measures",2017,"Journal of Affective Disorders",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021401561&doi=10.1016%2fj.jad.2017.06.061&partnerID=40&md5=b199c1558fc8be71dde83c4c5b9a0bc2","Background Magnetic resonance images (MRI) show detectable anatomical and functional differences between individuals with obsessive-compulsive disorder (OCD) and healthy subjects. Moreover, machine learning techniques have been proposed as tools to identify potential biomarkers and, ultimately, to support clinical diagnosis. However, few studies to date have investigated feature selection (FS) influences in OCD MRI-based classification. Methods Volumes of cortical and subcortical structures, from MRI data of 38 OCD patients (split into two groups according symptoms severity) and 36 controls, were submitted to seven feature selection algorithms. FS aims to select the most relevant and less redundant features which discriminate between two classes. Then, a classification step was applied, from which the classification performances before and after different FS were compared. For the performance evaluation, leave-one-subject-out accuracies of Support Vector Machine classifiers were considered. Results Using different FS algorithms, performance improvement was achieved for Controls vs. All OCD discrimination (19.08% of improvement reducing by 80% the amount of features), Controls vs. Low OCD (20.10%, 75%), Controls vs. High OCD (17.32%, 85%) and Low OCD vs. High OCD (10.53%, 75%). Furthermore, all algorithms pointed out classical cortico-striato-thalamo-cortical circuitry structures as relevant features for OCD classification. Limitations Limitations include the sample size and using only filter approaches for FS. Conclusions Our results suggest that FS positively impacts OCD classification using machine-learning techniques. Complementarily, FS algorithms were able to select biologically plausible features automatically. © 2017","Feature selection; Machine learning; Magnetic resonance imaging; Obsessive-compulsive disorder","adult; Article; brain cortex; brain size; clinical article; controlled study; corpus striatum; female; human; learning algorithm; male; neuroimaging; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; obsessive compulsive disorder; priority journal; subcortex; support vector machine; thalamus",2-s2.0-85021401561
"Mo H.M., Nwet K.T., Soe K.M.","CRF-based named entity recognition for Myanmar language",2017,"Advances in Intelligent Systems and Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992316868&doi=10.1007%2f978-3-319-48490-7_24&partnerID=40&md5=213b64b8d74e6935f21c871ad6770b61","Named Entity recognition (NER) is a subtask of information extraction and information retrieval that automatically identify proper nouns in texts and classify into predefined categories of name types. This paper introduces the effort on identification and classification of Named Entities in written Myanmar scripts in a statistical way. A statistical approach for NER of Myanmar Language using one of the supervised machine learning approaches called Conditional Random Fields (CRF) has been proposed for this task. © Springer International Publishing AG 2017.","CRF; Myanmar language; Named entity recognition","Artificial intelligence; Classification (of information); Information retrieval; Learning systems; Natural language processing systems; Supervised learning; Conditional random field; Myanmar language; Myanmars; Named entities; Named entity recognition; Proper nouns; Statistical approach; Supervised machine learning; Random processes",2-s2.0-84992316868
"Semwal V.B., Singha J., Sharma P.K., Chauhan A., Behera B.","An optimized feature selection technique based on incremental feature analysis for bio-metric gait data classification",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000977780&doi=10.1007%2fs11042-016-4110-y&partnerID=40&md5=c808d95e28d1bc45ff4c7807b873d57a","The classification of humanoid locomotion is a troublesome exercise because of non-linearity associate with gait signals. The classification using the different machine learning technique leads for over fitting and under fitting. To select the optimized feature is a difficult task. The high dimension feature vector requires a high computational cost. The hand craft feature selection machine learning techniques performed poor. We have used the incremental feature selection strategy for feature selection. In this paper we first selected the feature and identify the principle feature then we classify gait data using different machine learning technique (KNN, ANN, SVM, DNN and classifier fusion) and shown the performance comparison. Experimental result on real time datasets propose method is better than previous method as far as humanoid locomotion classification is concerned and the generalization accuracy provided by new feature selection method i.e. incremental feature selection (IFS) with analysis of variance (ANOVA) (Zhang et al., 20). During the feature extraction, 17 features were selected from the existing literatures (Wang et al. in IEEE Trans Circ Syst Video Technol 14(2):149–158, 15). Using all the features could lead to over fitting, information redundancy and dimension disaster. Thus, a system with optimal features was selected using ANOVA combined with IFA. These selected features were then fed as an input to the ANN, SVM, KNN and DNN model. These individual classifiers were combined to produce classifier fusion model. The 5-fold cross-validation was used to evaluate the performance of the proposed model. Based on the empirical results it may be concluded that classifier fusion provides satisfactory results (92.23 %) compared to other individual classifiers. One-way analysis of variance test, Friedman’s test and Kruskal-Wallis test has also been conducted to validate the statistical significance of the results. The proposed system can be used as recommender system based on behavioral gait pattern about the performance of player of Indian cricket team, Biometric and help to diagnosis Parkinson disease. © 2016, Springer Science+Business Media New York.","Biometric identification; Bipedal locomotion; Feature selection; Gait; Incremental feature selection (IFA)","Analysis of variance (ANOVA); Artificial intelligence; Biometrics; Diagnosis; Face recognition; Feature extraction; Learning algorithms; Learning systems; Biometric identifications; Bipedal locomotion; Feature selection methods; Gait; Incremental feature selection (IFA); Information redundancies; Machine learning techniques; Statistical significance; Classification (of information)",2-s2.0-85000977780
"Ali M., Sarwar A., Sharma V., Suri J.","Artificial neural network based screening of cervical cancer using a hierarchical modular neural network architecture (HMNNA) and novel benchmark uterine cervix cancer database",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032804156&doi=10.1007%2fs00521-017-3246-7&partnerID=40&md5=19b285387ba1c18e57778635149e63a8","The work reported in this paper presents a novel hierarchical modular neural network architecture (HMNNA) for automated screening of cervical cancer. HMNNA consists of three neural networks trained specifically on different areas of problem space under consideration, and the trained networks are then arranged in a tree structure forming hierarchical modular neural network architecture. The three specialized neural networks are trained by Levenberg–Maarquardt neural network algorithm. As compared to the standard back propagation algorithm, Levenberg–Maarquardt is fast and stable for convergence with only one drawback, i.e., storage requirement for estimated Hessian Matrix. For training and testing of HMNNA, a huge primary database is created which contains 8091 cervical cell images pertaining to 200 clinical cases collected from two health care institutions of northern India. The raw cases of cervical cancer in the form of Pap smear slides were photographed under a multi-headed digital microscope. Individual cells were manually cropped off from these slide images which were then passed through a feature extraction module for morphological profiling. Each cell was calibrated on the basis of 40 features from both cytoplasm and nucleus. After profiling, these cells were vigilantly assigned cell classes as per the latest 2001-Bethesda system of cervical cancer cell classification, by trained cytotechnicians and histopathologists. HMNNA is also trained and tested on the Herlev Benchmark dataset created by the Denmark University, which consists of 1417 cervical cancer cells. Both the datasets have seven classes of diagnosis, i.e., superficial squamous, intermediate squamous, columnar, mild dysplasia, moderate dysplasia, severe dysplasia, and carcinoma in situ, corresponding to the level of abnormality in cervical cells. These datasets are available in public domain at http://digitalpapsmeardb.in/and http://mde-lab.aegean.gr/index.php/downloads.The screening potential of the HMNNA is compared with 25 well-known machine learning algorithms available in MatlabR2016 (Machine learning and statistics toolbox 10.2) and monolithic neural network algorithms available in Matlab neural network pattern recognition toolbox. The HMNNA outperformed in all the 25 algorithms for both the datasets. For the Novel Benchmark database, it produced a classification accuracy of 95.32% with an F-value of 0.949310 and classification accuracy of 88.41% with an F-value of 0.89145 for the Herlev dataset. The screening potential of HMNNA was also evaluated and compared with the other diagnostic systems available in the recently published literature and was found to be performing much better than the counterparts on multiple parameters of performance evaluation. © 2017 The Natural Computing Applications Forum","Cervical cancer; Machine learning; Modular neural networks; Neural networks; Pap smear test","Artificial intelligence; Automation; Backpropagation; Backpropagation algorithms; Cells; Classification (of information); Computerized tomography; Cytology; Database systems; Digital devices; Diseases; Learning algorithms; Learning systems; Neural networks; Pattern recognition; Trees (mathematics); Cervical cancer cells; Cervical cancers; Classification accuracy; Modular neural networks; Neural network algorithm; Pap smear; Standard back propagation algorithms; Storage requirements; Network architecture",2-s2.0-85032804156
"Ding S., Zhao X., Zhang J., Zhang X., Xue Y.","A review on multi-class TWSVM",2017,"Artificial Intelligence Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032820178&doi=10.1007%2fs10462-017-9586-y&partnerID=40&md5=61ccd969542b8bf6bee3c09cf94f6822","Twin support vector machines (TWSVM), a novel machine learning algorithm developing from traditional support vector machines (SVM), is one of the typical nonparallel support vector machines. Since the TWSVM has superiorities of the simple model, the high training speed and the good performance, it has drawn extensive attention. The initial TWSVM can only handle binary classification, however, the multi-class classification problems are also common in practice. How to extend TWSVM from binary classification to multi-class classification is an interesting issue. Many researchers have devoted to the study of multi-class TWSVM. Although the study of multi-class TWSVM has made great progress, there is little literature on the comparisons and summaries of different multi-class classifiers based on TWSVM, which not only makes it difficult for novices to understand the essential differences, but also leads to the problem that how to choose the suitable multi-class TWSVM for a practical multi-class classification problem. This paper aims to review the development of multi-class TWSVM in recent years. We group multi-classTWSVM reasonably and analyze them with the respect to the basic theories and geometric meaning. According to the structures of the multi-class TWSVM, we divide them to the following groups: “one-versus-rest” strategy based multi-classTWSVM, “one-versus-one” strategy based multi-class TWSVM, binary tree structure based multi-class TWSVM, “one-versus-one-versus-rest” strategy based multi-class TWSVM and “all-versus-one” strategy based multi-class TWSVM. Although the training processes of direct acyclic graph based multi-class TWSVM are much similar to that of “one-versus-one” multi-class TWSVM, the decision processes of direct acyclic graph based multi-class TWSVM have their own characteristics and disadvantages, so we divide them to a separate group. This paper analyzes and summarizes the basic thoughts, theories, applicability and complexities of different multi-class TWSVM of different groups and presents experimental results to compare the performances. © 2017 Springer Science+Business Media B.V.","Multi-class TWSVM; Multiple birth support vector machine (MBSVM); Support vector machines (SVM); Twin support vector machines (TWSVM)","Binary trees; Bins; Classifiers; Graph theory; Graphic methods; Group theory; Learning algorithms; Learning systems; Support vector machines; Trees (mathematics); Vectors; Binary tree structure; Direct acyclic graphs; Multi-class classification; Multi-class classifier; Multi-class TWSVM; Multiclass classification problems; Multiple birth support vector machine (MBSVM); Twin support vector machines; Classification (of information)",2-s2.0-85032820178
"Lowe D.J., Pearce N.J.G., Jorgensen M.A., Kuehn S.C., Tryon C.A., Hayward C.L.","Correlating tephras and cryptotephras using glass compositional analyses and numerical and statistical methods: Review and evaluation",2017,"Quaternary Science Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029532816&doi=10.1016%2fj.quascirev.2017.08.003&partnerID=40&md5=bbfc333c9a69897053a63ca0ca1b7abe","We define tephras and cryptotephras and their components (mainly ash-sized particles of glass ± crystals in distal deposits) and summarize the basis of tephrochronology as a chronostratigraphic correlational and dating tool for palaeoenvironmental, geological, and archaeological research. We then document and appraise recent advances in analytical methods used to determine the major, minor, and trace elements of individual glass shards from tephra or cryptotephra deposits to aid their correlation and application. Protocols developed recently for the electron probe microanalysis of major elements in individual glass shards help to improve data quality and standardize reporting procedures. A narrow electron beam (diameter ∼3–5 μm) can now be used to analyze smaller glass shards than previously attainable. Reliable analyses of ‘microshards’ (defined here as glass shards &lt;32 μm in diameter) using narrow beams are useful for fine-grained samples from distal or ultra-distal geographic locations, and for vesicular or microlite-rich glass shards or small melt inclusions. Caveats apply, however, in the microprobe analysis of very small microshards (≤∼5 μm in diameter), where particle geometry becomes important, and of microlite-rich glass shards where the potential problem of secondary fluorescence across phase boundaries needs to be recognised. Trace element analyses of individual glass shards using laser ablation inductively coupled plasma-mass spectrometry (LA-ICP-MS), with crater diameters of 20 μm and 10 μm, are now effectively routine, giving detection limits well below 1 ppm. Smaller ablation craters (&lt;10 μm) can be subject to significant element fractionation during analysis, but the systematic relationship of such fractionation with glass composition suggests that analyses for some elements at these resolutions may be quantifiable. In undertaking analyses, either by microprobe or LA-ICP-MS, reference material data acquired using the same procedure, and preferably from the same analytical session, should be presented alongside new analytical data. In part 2 of the review, we describe, critically assess, and recommend ways in which tephras or cryptotephras can be correlated (in conjunction with other information) using numerical or statistical analyses of compositional data. Statistical methods provide a less subjective means of dealing with analytical data pertaining to tephra components (usually glass or crystals/phenocrysts) than heuristic alternatives. They enable a better understanding of relationships among the data from multiple viewpoints to be developed and help quantify the degree of uncertainty in establishing correlations. In common with other scientific hypothesis testing, it is easier to infer using such analysis that two or more tephras are different rather than the same. Adding stratigraphic, chronological, spatial, or palaeoenvironmental data (i.e. multiple criteria) is usually necessary and allows for more robust correlations to be made. A two-stage approach is useful, the first focussed on differences in the mean composition of samples, or their range, which can be visualised graphically via scatterplot matrices or bivariate plots coupled with the use of statistical tools such as distance measures, similarity coefficients, hierarchical cluster analysis (informed by distance measures or similarity or cophenetic coefficients), and principal components analysis (PCA). Some statistical methods (cluster analysis, discriminant analysis) are referred to as ‘machine learning’ in the computing literature. The second stage examines sample variance and the degree of compositional similarity so that sample equivalence or otherwise can be established on a statistical basis. This stage may involve discriminant function analysis (DFA), support vector machines (SVMs), canonical variates analysis (CVA), and ANOVA or MANOVA (or its two-sample special case, the Hotelling two-sample T2 test). Randomization tests can be used where distributional assumptions such as multivariate normality underlying parametric tests are doubtful. Compositional data may be transformed and scaled before being subjected to multivariate statistical procedures including calculation of distance matrices, hierarchical cluster analysis, and PCA. Such transformations may make the assumption of multivariate normality more appropriate. A sequential procedure using Mahalanobis distance and the Hotelling two-sample T2 test is illustrated using glass major element data from trachytic to phonolitic Kenyan tephras. All these methods require a broad range of high-quality compositional data which can be used to compare ‘unknowns’ with reference (training) sets that are sufficiently complete to account for all possible correlatives, including tephras with heterogeneous glasses that contain multiple compositional groups. Currently, incomplete databases are tending to limit correlation efficacy. The development of an open, online global database to facilitate progress towards integrated, high-quality tephrostratigraphic frameworks for different regions is encouraged. © 2017 Elsevier Ltd","Bivariate plot; Cluster analysis; Cryptotephra; Crystal; Electron probe; EPMA; Glass-shard analysis; LA-ICP-MS; Laser ablation; Machine learning; Melt inclusion; Microlite; Microshard; Multivariate statistics; Similarity coefficients; Statistical distance; Tephra; Tephrochronology; Tephrostratigraphy; Volcanic glass","Ablation; Artificial intelligence; Cluster analysis; Cluster computing; Crystals; Data handling; Deposits; Discriminant analysis; Electron probe microanalysis; Glass; Heuristic methods; Hierarchical systems; Inductively coupled plasma mass spectrometry; Laser ablation; Learning systems; Mass spectrometers; Mass spectrometry; Matrix algebra; Multivariant analysis; Numerical methods; Probes; Statistical mechanics; Statistical methods; Statistical tests; Statistics; Stratigraphy; Support vector machines; Trace analysis; Trace elements; Bivariate plots; Cryptotephra; Electron probe; Glass shards; La-ICP-MS; Melt inclusions; Microlites; Microshard; Multivariate statistics; Similarity coefficients; Statistical distance; Tephra; Tephrochronology; Tephrostratigraphy; Volcanic glass; Principal component analysis; ablation; activity coefficient; chemical composition; chronostratigraphy; cluster analysis; correlation; crystal structure; dating method; electron probe analysis; glass; machine learning; mass spectrometry; melt inclusion; mineral; numerical method; paleoenvironment; statistical analysis; tephra; tephrochronology; volcanic glass",2-s2.0-85029532816
"Salucci M., Vrba J., Merunka I., Massa A.","Real-time brain stroke detection through a learning-by-examples technique—An experimental assessment",2017,"Microwave and Optical Technology Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028037417&doi=10.1002%2fmop.30821&partnerID=40&md5=93b26c342b4de1e30ac614963a3c8043","The real-time detection of brain strokes is addressed within the Learning-by-Examples (LBE) framework. Starting from scattering measurements at microwave regime, a support vector machine (SVM) is exploited to build a robust decision function able to infer in real-time whether a stroke is present or not in the patient head. The proposed approach is validated in a laboratory-controlled environment by considering experimental measurements for both training and testing SVM phases. The obtained results prove that a very high detection accuracy can be yielded even though using a limited amount of training data. © 2017 Wiley Periodicals, Inc.","brain stroke detection; experimental validation; learning-by-examples; support vector machines","Electrical engineering; Instruments; Brain strokes; Controlled environment; Experimental assessment; Experimental validations; Learning by examples; Real-time detection; Scattering measurements; Training and testing; Support vector machines",2-s2.0-85028037417
"Salazar F., Toledo M.Á., González J.M., Oñate E.","Early detection of anomalies in dam performance: A methodology based on boosted regression trees",2017,"Structural Control and Health Monitoring",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017544298&doi=10.1002%2fstc.2012&partnerID=40&md5=fc0cdd5d530eb69a74b59085397658fd","The advances in information and communication technologies led to a general trend towards the availability of more detailed information on dam behaviour. This allows applying advanced data-based algorithms in its analysis, which has been reflected in an increasing interest in the field. However, most of the related literature is limited to the evaluation of model prediction accuracy, whereas the ulterior objective of data analysis is dam safety assessment. In this work, a machine-learning algorithm (boosted regression trees) is the core of a methodology for early detection of anomalies. It also includes a criterion to determine whether certain discrepancy between predictions and observations is normal, a procedure to compute a realistic estimate of the model accuracy, and an original approach to identify extraordinary load combinations. The performance of causal and noncausal models is assessed in terms of their ability to detect different types of anomalies, which were artificially introduced on reference time series generated with a numerical model of a 100-m-high arch dam. The final approach was implemented in an online application to visualise the results in an intuitive way to support decision making. Copyright © 2017 John Wiley & Sons, Ltd.","anomaly detection; boosted regression trees; dam monitoring; dam safety; machine learning","Arch dams; Artificial intelligence; Decision making; Forestry; Learning algorithms; Learning systems; Regression analysis; Safety engineering; Trees (mathematics); Anomaly detection; Boosted regression trees; Dam safety; General trends; Information and Communication Technologies; Load combination; Model prediction; On-line applications; Dams",2-s2.0-85017544298
"Yim W.-W., Kwan S.W., Yetisgen M.","Classifying tumor event attributes in radiology reports",2017,"Journal of the Association for Information Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029382177&doi=10.1002%2fasi.23937&partnerID=40&md5=8ce579ff92fb2157e1f9553d07f3cac2","Radiology reports contain vital diagnostic information that characterizes patient disease progression. However, information from reports is represented in free text, which is difficult to query against for secondary use. Automatic extraction of important information, such as tumor events using natural language processing, offers possibilities in improved clinical decision support, cohort identification, and retrospective evidence-based research for cancer patients. The goal of this work was to classify tumor event attributes: negation, temporality, and malignancy, using biomedical ontology and linguistically enriched features. We report our results on an annotated corpus of 101 hepatocellular carcinoma patient radiology reports, and show that the improved classification improves overall template structuring. Classification performances for negation identification, past temporality classification, and malignancy classification were at 0.94, 0.62, and 0.77 F1, respectively. Incorporating the attributes into full templates led to an improvement of 0.72 F1 for tumor-related events over a baseline of 0.65 F1. Improvement of negation, malignancy, and temporality classifications led to significant improvements in template extraction for the majority of categories. We present our machine-learning approach to identifying these several tumor event attributes from radiology reports, as well as highlight challenges and areas for improvement. © 2017 ASIS&T",,"Decision support systems; Extraction; Learning algorithms; Learning systems; Natural language processing systems; Radiation; Radiology; Tumors; Automatic extraction; Biomedical ontologies; Classification performance; Clinical decision support; Disease progression; Evidence based researches; Hepatocellular carcinoma; Machine learning approaches; Diagnosis",2-s2.0-85029382177
"Kalaie S., Gooya A.","Vascular tree tracking and bifurcation points detection in retinal images using a hierarchical probabilistic model",2017,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028327168&doi=10.1016%2fj.cmpb.2017.08.018&partnerID=40&md5=f4eef29f08ff811fd79fcb7b93289a2b","Background and Objective Retinal vascular tree extraction plays an important role in computer-aided diagnosis and surgical operations. Junction point detection and classification provide useful information about the structure of the vascular network, facilitating objective analysis of retinal diseases. Methods In this study, we present a new machine learning algorithm for joint classification and tracking of retinal blood vessels. Our method is based on a hierarchical probabilistic framework, where the local intensity cross sections are classified as either junction or vessel points. Gaussian basis functions are used for intensity interpolation, and the corresponding linear coefficients are assumed to be samples from class-specific Gamma distributions. Hence, a directed Probabilistic Graphical Model (PGM) is proposed and the hyperparameters are estimated using a Maximum Likelihood (ML) solution based on Laplace approximation. Results The performance of proposed method is evaluated using precision and recall rates on the REVIEW database. Our experiments show the proposed approach reaches promising results in bifurcation point detection and classification, achieving 88.67% precision and 88.67% recall rates. Conclusions This technique results in a classifier with high precision and recall when comparing it with Xu's method. © 2017 Elsevier B.V.","Bifurcation; Classification; Machine learning; Probabilistic graphical model; Retinal blood vessel tracking","Artificial intelligence; Bifurcation (mathematics); Classification (of information); Computer aided diagnosis; Diagnosis; Forestry; Graphic methods; Learning algorithms; Learning systems; Maximum likelihood; Ophthalmology; Surgery; Gaussian basis functions; Laplace approximation; Precision and recall; Probabilistic framework; Probabilistic graphical models; Probabilistic graphical models (PGM); Probabilistic modeling; Retinal blood vessels; Blood vessels",2-s2.0-85028327168
"A.G. R., Abdulla M.S., Asharaf A.S.","Lightly trained support vector data description for novelty detection",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019880831&doi=10.1016%2fj.eswa.2017.05.007&partnerID=40&md5=401bafc0bf8dc36cb7ae381730ee89a2","Anomaly (or outlier) detection is well researched objective in data mining due to its importance and inherent challenges. An outlier could be the key discovery to be made from large datasets and the insights gathered from them could be of significance in a wide variety of domains like information security, business intelligence, clinical decision support, financial monitoring etc. Recently, Support Vector Data Description (SVDD) driven approaches are shown as having good predictive accuracy. This paper proposes a novel low-complexity anomaly detection algorithm based on Support Vector Data Description (SVDD). The proposed algorithm reduces the complexity by avoiding the calculation of Lagrange multipliers of an objective function, instead locates an approximate pre-image of the SVDD sphere's center, within the input space itself. The crux of the training algorithm is a gradient descent of the primal objective function using Simultaneous Perturbation Stochastic Approximation (SPSA). Experiments using datasets obtained from UCI machine learning repository have demonstrated that the accuracies of the proposed approach are comparable while the training time is much lesser than Classical SVDD. © 2017 Elsevier Ltd","One-class classification; Outlier detection; Scaling; SVDD","Approximation algorithms; Computational complexity; Data mining; Decision support systems; Lagrange multipliers; Learning algorithms; Learning systems; Optimization; Security of data; Statistics; Stochastic systems; Anomaly-detection algorithms; One-class Classification; Outlier Detection; Scaling; Simultaneous perturbation stochastic approximation; Support vector data description; SVDD; UCI machine learning repository; Data description",2-s2.0-85019880831
"Tong D., Qu Y.R., Prasanna V.K.","Accelerating Decision Tree Based Traffic Classification on FPGA and Multicore Platforms",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021808413&doi=10.1109%2fTPDS.2017.2714661&partnerID=40&md5=e572cae8016f9fe093af81685af60e3e","Machine learning (ML) algorithms have been shown to be effective in classifying a broad range of applications in the Internet traffic. In this paper, we propose algorithms and architectures to realize online traffic classification using flow level features. First, we develop a traffic classifier based on C4.5 decision tree algorithm and Entropy-MDL (Minimum Description Length) discretization algorithm. It achieves an overall accuracy of 97.92 percent for classifying eight major applications. Next we propose approaches to accelerate the classifier on FPGA (Field Programmable Gate Array) and multicore platforms. We optimize the original classifier by merging it with discretization. Our implementation of this optimized decision tree achieves 7500+ Million Classifications Per Second (MCPS) on a state-of-the-art FPGA platform and 75-150 MCPS on two state-of-the-art multicore platforms. We also propose a divide and conquer approach to handle imbalanced decision trees. Our implementation of the divide-and-conquer approach achieves 10,000+ MCPS on a state-of-the-art FPGA platform and 130-340 MCPS on two state-of-the-art multicore platforms. We conduct extensive experiments on both platforms for various application scenarios to compare the two approaches. © 1990-2012 IEEE.","decision tree; FPGA; high throughput; machine learning; multicore; Traffic classification","Acceleration; Artificial intelligence; Decision trees; Education; Field programmable gate arrays (FPGA); Learning algorithms; Learning systems; Logic gates; Optimization; Signal receivers; Telecommunication traffic; Throughput; Trees (mathematics); Classification algorithm; High throughput; Multi core; Multi-core processing; Traffic classification; Data mining",2-s2.0-85021808413
"Tayal D.K., Yadav S.K.","Sentiment analysis on social campaign “Swachh Bharat Abhiyan” using unigram method",2017,"AI and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982182197&doi=10.1007%2fs00146-016-0672-5&partnerID=40&md5=a04b83d136777dc8ae2d9505b8abe1ee","Sentiment analysis is the field of natural language processing to analyze opinionated data, for the purpose of decision making. An opinion is a statement about a subject which expresses the sentiments as well as the emotions of the opinion makers on the topic. In this paper, we develop a sentiment analysis tool namely SENTI-METER. This tool estimates the success rate of social campaigns based on the algorithms we developed that analyze the sentiment of word as well as blog. Social campaigns have a huge impact on the mindset of people. One such campaign was launched in India on October 2, 2014, named Swachh Bharat Abhiyan (SBA). Our tool computes an elaborated analysis of Swachh Bharat Abhiyan, which examines the success rate of this social campaign. Here, we performed the location-wise analysis of the campaign and predict the degree of polarity of tweets along with the monthly and weekly analysis of the tweets. The experiments were conducted in five phases namely extraction and preprocessing of tweets, tokenization, sentiment evaluation of a line, sentiment evaluation of a blog (document) and analysis. Our tool is also capable of handling transliterated words. Unbiased tweets were extracted from Twitter related to this specific campaign, and on comparing with manual tagging we were able to achieve 84.47 % accuracy using unigram machine learning approach. This approach helps the government to implement the social campaigns effectively for the betterment of the society. © 2016, Springer-Verlag London.","Lexical analysis; SENTI-METER; Sentiment analysis; Social campaign; Swachh Bharat Abhiyan","Artificial intelligence; Behavioral research; Computational linguistics; Data mining; Decision making; Internet; Learning algorithms; Learning systems; Social networking (online); Lexical analysis; Machine learning approaches; NAtural language processing; Sentiment analysis; Social campaign; Swachh Bharat Abhiyan; Tokenization; Natural language processing systems",2-s2.0-84982182197
"Štajduhar I., Tomić M., Lerga J.","Mirroring quasi-symmetric organ observations for reducing problem complexity",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019650109&doi=10.1016%2fj.eswa.2017.05.041&partnerID=40&md5=31356fd2be369ce5f411fa5cab2a357e","Following an obvious growth of available collections of medical images in recent years, both in number and in size, machine learning has nowadays become an important tool for solving various image-analysis-related problems, such as organ segmentation or injury/pathology detection. The potential of learning algorithms to produce models having good generalisation properties is highly dependent on model complexity and the amount of available data. Bearing in mind that complex concepts require the use of complex models, it is of paramount importance to mitigate representation complexity, where possible, therefore enabling the utilisation of simpler models for performing the same task. When dealing with image collections of quasi-symmetric organs, or imaging observations of organs taken from different quasi-symmetric perspectives, one way of reducing representation complexity would be aligning all the images in a collection for left-right or front-rear orientation. That way, a learning algorithm would not be dealing with learning redundant symmetric representations. In this paper, we study in detail the influence of such within-class variation on model complexity, and present a possible solution, that can be applied to medical-imaging computer-aided diagnosis systems. The proposed method involves compacting the data, extracting features and then learning to separate the mirror-image representation classes from one another. Two efficient approaches are considered for performing such orientation separation: a fully automated unsupervised approach and a semi-automated supervised approach. Both solutions are directly applicable to imaging data. Method performance is illustrated on two 2D and one 3D real-world publicly-available medical datasets, concerning different parts of human anatomy, and observed using different imaging techniques: colour fundus photography, mammography CT scans and volumetric knee-joint MR scans. Experimental results suggest that efficient organ-mirroring orientation-classifier models, having expected classification accuracy greater than 99%, can be estimated using either the unsupervised or the supervised approach. In the presence of noise, however, an equally good performance can be achieved only by using the supervised approach, learning from a small subset of labelled data. © 2017 Elsevier Ltd","Machine learning; Medical image analysis; Model complexity; Organ orientation; Within-class variation","Artificial intelligence; Automation; Computer aided diagnosis; Diagnosis; Image analysis; Image segmentation; Imaging techniques; Joints (anatomy); Learning algorithms; Learning systems; Medical imaging; Medical problems; Classification accuracy; Computer aided diagnosis systems; Extracting features; Fundus photography; Model complexity; Organ segmentation; Unsupervised approaches; Within class; Computerized tomography",2-s2.0-85019650109
"Ahmed H.I., Elfeshawy N.A., Elzoghdy S.F., El-sayed H.S., Faragallah O.S.","A Neural Network-Based Learning Algorithm for Intrusion Detection Systems",2017,"Wireless Personal Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029596100&doi=10.1007%2fs11277-017-4663-8&partnerID=40&md5=870914e4489cf84bf97a5a7ef7d5f1a9","Recently, intrusion detection systems (IDS) have been introduced to effectively secure networks. Using neural networks and machine learning in detecting and classifying intrusions are powerful alternative solutions. In this research paper, both of Gradient descent with momentum (GDM)-based back-propagation (BP) and Gradient descent with momentum and adaptive gain (GDM/AG)-based BP algorithms are utilized for training neural networks to operate like IDS. To investigate the efficiency of the two proposed learning schemes, a neural network based IDS is built using the proposed learning algorithms. The efficiency of both algorithms is inspected in terms of convergence speed to achieve system learning, and elapsed learning time using various settings of neural network parameters. The result demonstrated that the GDM/AG-based BP learning algorithm outperforms the GDM-based BP learning algorithm. © 2017, Springer Science+Business Media, LLC.","Back-propagation (BP); Intrusion detection systems (IDSs); Neural networks (NNs)","Backpropagation; Backpropagation algorithms; Computer crime; Efficiency; Intrusion detection; Learning systems; Mercury (metal); Network security; Neural networks; Alternative solutions; BP learning algorithms; Convergence speed; Intrusion Detection Systems; Learning schemes; Network-based learning; Neural network parameters; Neural networks (NNS); Learning algorithms",2-s2.0-85029596100
"Antunes F., Ribeiro B., Pereira F.","Probabilistic modeling and visualization for bankruptcy prediction",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024122365&doi=10.1016%2fj.asoc.2017.06.043&partnerID=40&md5=dd12a133c6f82e5a039d55d9d0a3fb62","In accounting and finance domains, bankruptcy prediction is of great utility for all of the economic stakeholders. The challenge of accurate assessment of business failure prediction, specially under scenarios of financial crisis, is known to be complicated. Although there have been many successful studies on bankruptcy detection, seldom probabilistic approaches were carried out. In this paper we assume a probabilistic point-of-view by applying Gaussian processes (GP) in the context of bankruptcy prediction, comparing it against the support vector machines (SVM) and the logistic regression (LR). Using real-world bankruptcy data, an in-depth analysis is conducted showing that, in addition to a probabilistic interpretation, the GP can effectively improve the bankruptcy prediction performance with high accuracy when compared to the other approaches. We additionally generate a complete graphical visualization to improve our understanding of the different attained performances, effectively compiling all the conducted experiments in a meaningful way. We complete our study with an entropy-based analysis that highlights the uncertainty handling properties provided by the GP, crucial for prediction tasks under extremely competitive and volatile business environments. © 2017 Elsevier B.V.","Bankruptcy prediction; Gaussian processes; Graphical visualization; Machine learning","Gaussian distribution; Gaussian noise (electronic); Learning systems; Plant shutdowns; Support vector machines; Uncertainty analysis; Visualization; Bankruptcy prediction; Business failure prediction; Entropy-based analysis; Gaussian Processes; Graphical visualization; Probabilistic approaches; Probabilistic interpretation; Probabilistic modeling; Forecasting",2-s2.0-85024122365
"Gadermayr M., Eschweiler D., Jeevanesan A., Klinkhammer B.M., Boor P., Merhof D.","Segmenting renal whole slide images virtually without training data",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030156085&doi=10.1016%2fj.compbiomed.2017.09.014&partnerID=40&md5=fc540454db3c24062bdb1bb96f6c8ad8","Digital pathology is a field of increasing interest and requires automated systems for processing huge amounts of digital data. The development of supervised-learning based automated systems is aggravated by the fact that image properties can change from slide to slide. In this work, the focus is on the segmentation of the glomeruli constituting the most important regions-of-interest in renal histopathology. We propose and investigate a two-stage pipeline consisting of a weakly supervised patch-based detection and a precise segmentation. The proposed methods do not need any previously obtained training data. For adapting and optimizing this model, a kernel two-sample test is applied. For the segmentation stage, unsupervised segmentation methods including level-set and polygon-fitting approaches are adapted, combined and evaluated. Overall, with the best performing polygon-fitting segmentation method, 51% of glomeruli were segmented with sufficient accuracy (DSC > 0.8). 42% of the detections were false positives. Due to the difficult application scenario in combination with the small required training corpus, the obtained performance is assessed as good. Strategies for increasing the segmentation performance even further are discussed in detail. © 2017 Elsevier Ltd","Glomeruli; Kidney; Level-set; Polygon-fitting; Weakly supervised","Automation; E-learning; Geometry; Image segmentation; Glomeruli; Kidney; Level Set; Polygon-fitting; Weakly supervised; Numerical methods; accuracy; Article; automation; false positive result; glomerulus; image segmentation; kernel method; nonhuman; priority journal; supervised machine learning; support vector machine",2-s2.0-85030156085
"Hamaya M., Matsubara T., Noda T., Teramae T., Morimoto J.","Learning assistive strategies for exoskeleton robots from user-robot physical interaction",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017537132&doi=10.1016%2fj.patrec.2017.04.007&partnerID=40&md5=bbaef39defdd409f5468732733a96f78","Social demand for exoskeleton robots that physically assist humans has been increasing in various situations due to the demographic trends of aging populations. With exoskeleton robots, an assistive strategy is a key ingredient. Since interactions between users and exoskeleton robots are bidirectional, the assistive strategy design problem is complex and challenging. In this paper, we explore a data-driven learning approach for designing assistive strategies for exoskeletons from user-robot physical interaction. We formulate the learning problem of assistive strategies as a policy search problem and exploit a data-efficient model-based reinforcement learning framework. Instead of explicitly providing the desired trajectories in the cost function, our cost function only considers the user's muscular effort measured by electromyography signals (EMGs) to learn the assistive strategies. The key underlying assumption is that the user is instructed to perform the task by his/her own intended movements. Since the EMGs are observed when the intended movements are achieved by the user's own muscle efforts rather than the robot's assistance, EMGs can be interpreted as the “cost” of the current assistance. We applied our method to a 1-DoF exoskeleton robot and conducted a series of experiments with human subjects. Our experimental results demonstrated that our method learned proper assistive strategies that explicitly considered the bidirectional interactions between a user and a robot with only 60 seconds of interaction. We also showed that our proposed method can cope with changes in both the robot dynamics and movement trajectories. © 2017 The Authors","Exoskeleton robot; Human-in-the- loop; Human-robot physical interaction; Reinforcement learning","Cost functions; Costs; Electromyography; Exoskeleton (Robotics); Machine design; Reinforcement learning; Robots; Bi-directional interaction; Desired trajectories; Electromyography signals; Exoskeleton robots; Human-in-the-loop; Model-based reinforcement learning; Movement trajectories; Physical interactions; Human robot interaction",2-s2.0-85017537132
"Dora S., Suresh S., Sundararajan N.","Online Meta-neuron based Learning Algorithm for a spiking neural classifier",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020213116&doi=10.1016%2fj.ins.2017.05.050&partnerID=40&md5=1b9ebf10524b4be766308effdd680f84","This paper presents a new spiking neural network architecture with a meta-neuron which envelopes all the pre- and postsynaptic neurons in the network. The concept of the meta-neuron is inspired by the role of astrocytes in modulating synaptic plasticity in biological neural networks. The meta-neuron utilizes the global information stored in the network (synaptic weights) and the local information present in the input spike pattern to determine a weight sensitivity modulation factor for a given synapse. Based on the weight sensitivity modulation factor and the postsynaptic potential of a neuron, the meta-neuron based learning rule updates the synaptic weights in the network to produce precise shifts in the spike times of the postsynaptic neurons. Using this learning rule, an Online Meta-neuron based Learning Algorithm (OMLA) is presented for an evolving spiking neural classifier. The learning algorithm employs heuristic learning strategies for learning each input spike pattern. It can choose to add a neuron, update the network parameters or delete a spike pattern depending on the spike times of the output neurons. OMLA employs a meta-neuron with memory that stores only those spike patterns which are used to add a neuron to the network. These spike patterns (spike patterns in meta-neuron memory) are used as representative of past information stored in the network during subsequent neuron additions. The performance of OMLA has been compared with both the existing online learning and batch learning algorithms for spiking neural networks using the UCI machine learning benchmark data sets. The statistical comparison clearly indicates that the OMLA performs better than other existing online learning algorithms for spiking neural networks. Since, OMLA uses both, the global as well as the local information in the network, it is also able to perform better than other batch learning algorithms. © 2017 Elsevier Inc.","Evolving architecture; Meta-neuron; Online learning; Pattern classification; Spiking neural networks","Benchmarking; E-learning; Intelligent agents; Learning systems; Memory architecture; Modulation; Network architecture; Neural networks; Neurons; Pattern recognition; Biological neural networks; Online learning; Online learning algorithms; Post-synaptic neurons; Post-synaptic potentials; Spiking neural networks; Statistical comparisons; Synaptic plasticity; Learning algorithms",2-s2.0-85020213116
"Xu C., Ge L., Zhang Y., Dehmer M., Gutman I.","Prediction of therapeutic peptides by incorporating q-Wiener index into Chou's general PseAAC",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030483326&doi=10.1016%2fj.jbi.2017.09.011&partnerID=40&md5=e4c982eb9d701758affd5435584b6409","As therapeutic peptides have been taken into consideration in disease therapy in recent years, many biologists spent time and labor to verify various functional peptides from a large number of peptide sequences. In order to reduce the workload and increase the efficiency of identification of functional proteins, we propose a sequence-based model, q-FP (functional peptide prediction based on the q-Wiener Index), capable of recognizing potentially functional proteins. We extract three types of features by mixing graphic representation and statistical indices based on the q-Wiener index and physicochemical properties of amino acids. Our support-vector-machine-based model achieves an accuracy of 96.71%, 93.34%, 98.40%, and 91.40% for anticancer, virulent, and allergenic proteins datasets, respectively, by using 5-fold cross validation. © 2017 Elsevier Inc.","Graphic representation; Physicochemical property; Protein prediction; q-Wiener index; Support vector machine","Disease control; Forecasting; Graph theory; Molecular structure; Peptides; Support vector machines; Functional peptides; Functional proteins; Graphic representation; Physico-chemical properties of amino acids; Physicochemical property; Protein prediction; Therapeutic peptides; Wiener index; Proteins; amino acid composition; amino acid sequence; Article; chemical parameters; composition of the moment vector; controlled study; DNA sequence; machine learning; measurement accuracy; physical chemistry; priority journal; protein analysis; RNA sequence; sensitivity and specificity; support vector machine; validation process; Wiener index",2-s2.0-85030483326
"Kang F., Xu B., Li J., Zhao S.","Slope stability evaluation using Gaussian processes with various covariance functions",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024394520&doi=10.1016%2fj.asoc.2017.07.011&partnerID=40&md5=cd86a34408f96449587c33d3f44fd977","This paper presents a stability evaluation method for slopes based on Gaussian processes (GPs), which is a popular machine learning technique for nonlinear system modeling. Covariance function is one of the most critical parts in GPs modeling, because it determines the properties of sample functions drawn from the Gaussian process prior. Sixteen covariance functions are tested on several datasets for slope stability evaluation problems. Experimental: results show that GPs models can reflect the complex relationship between input and output variables. The obtained results are better or similar to the results obtained by several other existing methods, such as artificial neural networks, support vector machines, etc. The other important attractions of GPs include a simple training process and a predictive distribution of the system output. © 2017 Elsevier B.V.","Bayesian modeling; Covariance functions; Factor of safety; Gaussian process regression; Slope stability","Bayesian networks; Complex networks; Function evaluation; Gaussian distribution; Gaussian noise (electronic); Learning systems; Neural networks; Safety factor; Stability; Bayesian model; Covariance function; Factor of safety; Gaussian process regression; Machine learning techniques; Nonlinear system modeling; Predictive distributions; Slope stability evaluation; Slope stability",2-s2.0-85024394520
"Mithal V., Nayak G., Khandelwal A., Kumar V., Oza N.C., Nemani R.","RAPT: Rare Class Prediction in Absence of True Labels",2017,"IEEE Transactions on Knowledge and Data Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028472592&doi=10.1109%2fTKDE.2017.2739739&partnerID=40&md5=df47a692fbda61a4ada296b40c2605ba","Many real-world problems involve learning models for rare classes in situations where there are no gold standard labels for training samples but imperfect labels are available for all instances. In this paper, we present RAPT, a three step predictive modeling framework for classifying rare class in such problem settings. The first step of the proposed framework learns a classifier that jointly optimizes precision and recall by only using imperfectly labeled training samples. We also show that, under certain assumptions on the imperfect labels, the quality of this classifier is almost as good as the one constructed using perfect labels. The second and third steps of the framework make use of the fact that imperfect labels are available for all instances to further improve the precision and recall of the rare class. We evaluate the RAPT framework on two real-world applications of mapping forest fires and urban extent from earth observing satellite data. The experimental results indicate that RAPT can be used to identify forest fires and urban areas with high precision and recall by using imperfect labels, even though obtaining expert annotated samples on a global scale is infeasible in these applications. © 2017 IEEE.","Classification algorithms; machine learning algorithms","Buildings; Classification (of information); Deforestation; Earth (planet); Fire hazards; Fires; Forestry; Learning systems; Network function virtualization; Personnel training; Sampling; Satellites; Classification algorithm; Earth observing satellite; Precision and recall; Predictive modeling; Predictive models; Real-world problem; Training data; Urban areas; Learning algorithms",2-s2.0-85028472592
"Zheng Y., Jiang Z., Xie F., Zhang H., Ma Y., Shi H., Zhao Y.","Feature extraction from histopathological images based on nucleus-guided convolutional neural network for breast lesion classification",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023191265&doi=10.1016%2fj.patcog.2017.05.010&partnerID=40&md5=923d164f148194fa32a68ba3ea6ae589","Feature extraction is a crucial and challenging aspect in the computer-aided diagnosis of breast cancer with histopathological images. In recent years, many machine learning methods have been introduced to extract features from histopathological images. In this study, a novel nucleus-guided feature extraction framework based on convolutional neural network is proposed for histopathological images. The nuclei are first detected from images, and then used to train a designed convolutional neural network with three hierarchy structures. Through the trained network, image-level features including the pattern and spatial distribution of the nuclei are extracted. The proposed features are evaluated through the classification experiment on a histopathological image database of breast lesions. The experimental results show that the extracted features effectively represent histopathological images, and the proposed framework achieves a better classification performance for breast lesions than the compared state-of-the-art methods. © 2017 Elsevier Ltd","Breast cancer; Computer-aided diagnosis; Convolutional neural network; Feature extraction; Histopathological image","Classification (of information); Computer aided diagnosis; Convolution; Diseases; Extraction; Feature extraction; Image classification; Image retrieval; Learning systems; Medical imaging; Neural networks; Breast Cancer; Breast lesion; Classification performance; Convolutional neural network; Hierarchy structure; Histopathological images; Machine learning methods; State-of-the-art methods; Image processing",2-s2.0-85023191265
"Liu W., Ye M., Wei J., Hu X.","Compressed constrained spectral clustering framework for large-scale data sets",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028331614&doi=10.1016%2fj.knosys.2017.08.003&partnerID=40&md5=e4941de7d18e4a71c26a91651e939197","The method of incorporating constraint information into spectral clustering, i.e., \constrained spectral clustering (CSC), can greatly improve clustering accuracy, and thus has been widely employed in the machine learning literature. In this paper, we propose a compressed CSC framework by combining specific graph constructions with a recently introduced CSC model. Particularly, our framework has ability to avoid losing the main partition information in the compression process. By presenting a theoretical analysis and empirical results, we demonstrate that our new framework can achieve the same clustering solution as that of the original model with the specific graph structure. In addition, because our framework utilizes landmark-based graph construction and the approximate matrix decomposition simultaneously, it can be applied to both feature and graph data in a more general way. Moreover, the parameter setting in our framework is rather simple, and therefore it is very practical. Experimental results indicate that our framework has advantages in terms of efficiency and effectiveness. © 2017","Constrained spectral clustering; Effectiveness; Efficiency; Landmark; Matrix decomposition","Clustering algorithms; Efficiency; Learning systems; Matrix algebra; Clustering solutions; Constrained spectral clustering; Constraint information; Effectiveness; Landmark; Large scale data sets; Machine learning literature; Matrix decomposition; Network function virtualization",2-s2.0-85028331614
"Ahmad S., Lavin A., Purdy S., Agha Z.","Unsupervised real-time anomaly detection for streaming data",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020719859&doi=10.1016%2fj.neucom.2017.04.070&partnerID=40&md5=9e99f987e8e50f3937ebf8b39a79cb6b","We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics. © 2017 The Author(s)","Anomaly detection; Benchmark dataset; Concept drift; Hierarchical Temporal Memory; Streaming data; Unsupervised learning","Signal detection; Unsupervised learning; Anomaly detection; Benchmark datasets; Concept drifts; Streaming data; Temporal memory; Real time systems; Article; benchmarking; classification algorithm; controlled study; data analysis; learning algorithm; mathematical computing; online system; priority journal; time series analysis; unsupervised machine learning",2-s2.0-85020719859
"Rodríguez J., Cañete L., Monroy R., Medina-Pérez M.A.","Experimenting with masquerade detection via user task usage",2017,"International Journal on Interactive Design and Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995495089&doi=10.1007%2fs12008-016-0360-1&partnerID=40&md5=eb94467f316e3c2cbad52f176c0483c2","Detecting whether a given activity in a device, corresponds to a legitimate device user or not, is usually carried out by looking for deviations in behavior against a normal usage baseline. One approach to this problem, called masquerade detection, uses the file system navigation information, comprised of the files, folders, and how the user navigates between them, to construct the normal usage baseline. Atop the file system navigation approach for masquerade detection, there is an alternate representation of file system usage which abstracts away a collection of interrelated files into a single symbol, denoting a task; thus, touching any of these files amounts to simply as executing a task. In this paper, we propose a refined notion of the task abstraction, which allows for a better characterization of the user. The improved abstraction makes possible to obtain a better Masquerade Detection System with increased efficiency, resulting in a faster detection of masqueraders. © 2016, Springer-Verlag France.","Computer security; Machine learning; Masquerade detection; Supervised classification","Abstracting; Artificial intelligence; File organization; Learning systems; Security of data; Supervised learning; File systems; Masquerade detection; Navigation in formation; Supervised classification; Air navigation",2-s2.0-84995495089
"Primativo S., Clark C., Yong K.X.X., Firth N.C., Nicholas J., Alexander D., Warren J.D., Rohrer J.D., Crutch S.J.","Eyetracking metrics reveal impaired spatial anticipation in behavioural variant frontotemporal dementia",2017,"Neuropsychologia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031797102&doi=10.1016%2fj.neuropsychologia.2017.10.014&partnerID=40&md5=65be312fbc53582ba393ccbeb5f3486f","Eyetracking technology has had limited application in the dementia field to date, with most studies attempting to discriminate syndrome subgroups on the basis of basic oculomotor functions rather than higher-order cognitive abilities. Eyetracking-based tasks may also offer opportunities to reduce or ameliorate problems associated with standard paper-and-pencil cognitive tests such as the complexity and linguistic demands of verbal test instructions, and the problems of tiredness and attention associated with lengthy tasks that generate few data points at a slow rate. In the present paper we adapted the Brixton spatial anticipation test to a computerized instruction-less version where oculomotor metrics, rather than overt verbal responses, were taken into account as indicators of high level cognitive functions. Twelve bvFTD (in whom spatial anticipation deficits were expected), six SD patients (in whom deficits were predicted to be less frequent) and 38 healthy controls were presented with a 10 × 7 matrix of white circles. During each trial (N = 24) a black dot moved across seven positions on the screen, following 12 different patterns. Participants’ eye movements were recorded. Frequentist statistical analysis of standard eye movement metrics were complemented by a Bayesian machine learning (ML) approach in which raw eyetracking time series datasets were examined to explore the ability to discriminate diagnostic group performance not only on the overall performance but also on individual trials. The original pen and paper Brixton test identified a spatial anticipation deficit in 7/12 (58%) of bvFTD and in 2/6 (33%) of SD patients. The eyetracking frequentist approach reported the deficit in 11/12 (92%) of bvFTD and in none (0%) of the SD patients. The machine learning approach had the main advantage of identifying significant differences from controls in 24/24 individual trials for bvFTD patients and in only 12/24 for SD patients. Results indicate that the fine grained rich datasets obtained from eyetracking metrics can inform us about high level cognitive functions in dementia, such as spatial anticipation. The ML approach can help identify conditions where subtle deficits are present and, potentially, contribute to test optimisation and the reduction of testing times. The absence of instructions also favoured a better distinction between different clinical groups of patients and can help provide valuable disease-specific markers. © 2017 The Authors","Anticipatory saccades; Behavioural variant frontotemporal dementia; Eye movements; Machine learning; Spatial anticipation","adult; aged; Article; Bayesian learning; Brixton spatial anticipation test; clinical article; clinical assessment; cognition; cognitive defect; controlled study; disease marker; executive function test; eye fixation; eye tracking; female; frontal variant frontotemporal dementia; human; male; neuroimaging; saccadic velocity; semantic dementia; spatial anticipation deficit",2-s2.0-85031797102
"Ahmedt-Aristizabal D., Fookes C., Dionisio S., Nguyen K., Cunha J.P.S., Sridharan S.","Automated analysis of seizure semiology and brain electrical activity in presurgery evaluation of epilepsy: A focused survey",2017,"Epilepsia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030624113&doi=10.1111%2fepi.13907&partnerID=40&md5=18b493694cb4535ff648e9b3d10bc11b","Epilepsy being one of the most prevalent neurological disorders, affecting approximately 50 million people worldwide, and with almost 30–40% of patients experiencing partial epilepsy being nonresponsive to medication, epilepsy surgery is widely accepted as an effective therapeutic option. Presurgical evaluation has advanced significantly using noninvasive techniques based on video monitoring, neuroimaging, and electrophysiological and neuropsychological tests; however, certain clinical settings call for invasive intracranial recordings such as stereoelectroencephalography (SEEG), aiming to accurately map the eloquent brain networks involved during a seizure. Most of the current presurgical evaluation procedures focus on semiautomatic techniques, where surgery diagnosis relies immensely on neurologists’ experience and their time-consuming subjective interpretation of semiology or the manifestations of epilepsy and their correlation with the brain's electrical activity. Because surgery misdiagnosis reaches a rate of 30%, and more than one-third of all epilepsies are poorly understood, there is an evident keen interest in improving diagnostic precision using computer-based methodologies that in the past few years have shown near-human performance. Among them, deep learning has excelled in many biological and medical applications, but has advanced insufficiently in epilepsy evaluation and automated understanding of neural bases of semiology. In this paper, we systematically review the automatic applications in epilepsy for human motion analysis, brain electrical activity, and the anatomoelectroclinical correlation to attribute anatomical localization of the epileptogenic network to distinctive epilepsy patterns. Notably, recent advances in deep learning techniques will be investigated in the contexts of epilepsy to address the challenges exhibited by traditional machine learning techniques. Finally, we discuss and propose future research on epilepsy surgery assessment that can jointly learn across visually observed semiologic patterns and recorded brain electrical activity. Wiley Periodicals, Inc. © 2017 International League Against Epilepsy.","Deep learning; Epileptogenic network; Facial expression; Human motion; Machine learning",,2-s2.0-85030624113
"Lughofer E.","On-line active learning: A new paradigm to improve practical useability of data stream modeling methods",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021725695&doi=10.1016%2fj.ins.2017.06.038&partnerID=40&md5=5b9a29b56986674fba83487e2154dc5e","The central purpose of this survey is to provide readers an insight into the recent advances and challenges in on-line active learning. Active learning has attracted the data mining and machine learning community since around 20 years. This is because it served for important purposes to increase practical applicability of machine learning techniques, such as (i) to reduce annotation and measurement costs for operators and measurement equipments, (ii) to reduce manual labeling effort for experts and (iii) to reduce computation time for model training. Almost all of the current techniques focus on the classical pool-based approach, which is off-line by nature as iterating over a pool of (unlabeled) reference samples a multiple times to choose the most promising ones for improving the performance of the classifiers. This is achieved by (time-intensive) re-training cycles on all labeled samples available so far. For the on-line, stream mining case, the challenge is that the sample selection strategy has to operate in a fast, ideally single-pass manner. Some first approaches have been proposed during the last decade (starting from around 2005) with the usage of machine learning (ML) oriented incremental classifiers, which are able to update their parameters based on selected samples, but not their structures. Since 2012, on-line active learning concepts have been proposed in connection with the paradigm of evolving models, which are able to expand their knowledge into feature space regions so far unexplored. This opened the possibility to address a particular type of uncertainty, namely that one which stems from a significant novelty content in streams, as, e.g., caused by drifts, new operation modes, changing system behaviors or non-stationary environments. We will provide an overview about the concepts and techniques for sample selection and active learning within these two principal major research lines (incremental ML models versus evolving systems), a comparison of their essential characteristics and properties (raising some advantages and disadvantages), and a study on possible evaluation techniques for them. We conclude with an overview of real-world application examples where various on-line AL approaches have been already successfully applied in order to significantly reduce user's interaction efforts and costs for model updates. © 2017 Elsevier Inc.","Data stream mining; Evolving models; Incremental ML and DM methods; Interaction effort and cost reduction; On-line active learning; Single-pass sample selection; Uncertainty and novelty in streams","Artificial intelligence; Cost reduction; Costs; Data communication systems; Data mining; E-learning; Learning systems; Personnel training; Active Learning; Data stream mining; Evolving models; Incremental ML and DM methods; Sample selection; Uncertainty and novelty in streams; Education",2-s2.0-85021725695
"Li X., Ng M.K., Ye Y., Wang E.K., Xu X.","Block linear discriminant analysis for visual tensor objects with frequency or time information",2017,"Journal of Visual Communication and Image Representation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026915794&doi=10.1016%2fj.jvcir.2017.08.004&partnerID=40&md5=134524f6a2bd879c5babea478ac02a45","Recently, due to the advancement of acquisition techniques, visual tensor data have been accumulated in a great variety of engineering fields, e.g., biometrics, neuroscience, surveillance and remote sensing. How to analyze and learn with such tensor objects thus becomes an important and growing interest in machine learning community. In this paper, we propose a block linear discriminant analysis (BLDA) algorithm to extract features for visual tensor objects such as multichannel/hyperspectral face images or human gait videos. Taking the inherent characteristic of such tensor data into account, we unfold tensor objects according to their spatial information and frequency/time information, and represent them in a block matrix form. As a result, the block form between-class and within-class scatter matrices are constructed, and a related block eigen-decomposition is solved to extract features for classification. Comprehensive experiments have been carried out to test the effectiveness of the proposed method, and the results show that BLDA outperforms existing algorithms like DATER, 2DLDA, GTDA, UMLDA, STDA and MPCA for visual tensor object analysis. © 2017 Elsevier Inc.","Between-class scatter; Discriminant analysis; Gait recognition; Hyperspectral face recognition; Visual tensors; Within-class scatter","Discriminant analysis; Face recognition; Image retrieval; Learning systems; Matrix algebra; Pattern recognition; Remote sensing; Between class scatter; Block linear discriminant analysis; Gait recognition; HyperSpectral; Inherent characteristics; Machine learning communities; Spatial informations; Within class scatter; Tensors",2-s2.0-85026915794
"Jadooki S., Mohamad D., Saba T., Almazyad A.S., Rehman A.","Fused features mining for depth-based hand gesture recognition to classify blind human communication",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960113763&doi=10.1007%2fs00521-016-2244-5&partnerID=40&md5=6fc82f4f1895e6aa4ef25122271764fe","Gesture recognition and hand pose tracking are applicable techniques in human–computer interaction fields. Depth data obtained by depth cameras present a very informative explanation of the body or in particular hand pose that it can be used for more accurate gesture recognition systems. The hand detection and feature extraction process are very challenging task in the RGB images that they can be effectively dissolved with simple ways with depth data. However, depth data could be combined with the color information for more reliable recognition. A common hand gesture recognition system requires identifying the hand and its position or direction, extracting some useful features and applying a suitable machine-learning method to detect the performed gesture. This paper presents the novel fusion of the enhanced features for the classification of static signs of the sign language. It begins by explaining how the hand can be separated from the scene by depth data. Then, a combination feature extraction method is introduced for extracting some appropriate features of the images. Finally, an artificial neural network classifier is trained with these fused features and applied to critically analyze various descriptors performance. © 2016, The Natural Computing Applications Forum.","DCT; Depth data; Fused features mining; Hand gesture recognition; Moment invariant","Artificial intelligence; Classification (of information); Extraction; Feature extraction; Human computer interaction; Image processing; Learning systems; Neural networks; Palmprint recognition; Artificial neural network classifiers; Computer interaction; Depth data; Feature extraction methods; Gesture recognition system; Hand-gesture recognition; Machine learning methods; Moment invariant; Gesture recognition",2-s2.0-84960113763
"Dornaika F., Kejani M.T., Bosaghzadeh A.","Graph construction using adaptive Local Hybrid Coding scheme",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029518429&doi=10.1016%2fj.neunet.2017.08.002&partnerID=40&md5=e25d3e1b0b8b16e990b65f3c5ac2b69e","It is well known that dense coding with local bases (via Least Square coding schemes) can lead to large quantization errors or poor performances of machine learning tasks. On the other hand, sparse coding focuses on accurate representation without taking into account data locality due to its tendency to ignore the intrinsic structure hidden among the data. Local Hybrid Coding (LHC) (Xiang et al., 2014) was recently proposed as an alternative to the sparse coding scheme that is used in Sparse Representation Classifier (SRC). The LHC blends sparsity and bases-locality criteria in a unified optimization problem. It can retain the strengths of both sparsity and locality. Thus, the hybrid codes would have some advantages over both dense and sparse codes. This paper introduces a data-driven graph construction method that exploits and extends the LHC scheme. In particular, we propose a new coding scheme coined Adaptive Local Hybrid Coding (ALHC). The main contributions are as follows. First, the proposed coding scheme adaptively selects the local and non-local bases of LHC using data similarities provided by Locality-constrained Linear code. Second, the proposed ALHC exploits local similarities in its solution. Third, we use the proposed coding scheme for graph construction. For the task of graph-based label propagation, we demonstrate high classification performance of the proposed graph method on four benchmark face datasets: Extended Yale, PF01, PIE, and FERET. © 2017 Elsevier Ltd","Classification; Graph construction; Label propagation; Local Hybrid Code; Sparse coding","Benchmarking; Classification (of information); Coding errors; Graphic methods; Learning systems; Optimization; Classification performance; Graph construction; Graph-construction method; Label propagation; Local Hybrid Code; Sparse coding; Sparse representation; Unified optimizations; Codes (symbols); Article; classifier; least square analysis; linear system; local hybrid coding; machine learning; mathematical analysis; priority journal; process optimization; quality control",2-s2.0-85029518429
"Ranasinghe R.A.T.M., Jaksa M.B., Nejad F.P., Kuo Y.L.","Predicting the effectiveness of rolling dynamic compaction using genetic programming",2017,"Proceedings of the Institution of Civil Engineers: Ground Improvement",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032024669&doi=10.1680%2fjgrim.15.00009&partnerID=40&md5=920133eb9b120e1c3973abe83ec00a86","Rolling dynamic compaction (RDC) is a soil compaction method that involves impacting the ground with a non-circular roller. This technique is currently in widespread use internationally and has proven to be suitable for many compaction applications, with improved capabilities over traditional compaction equipment. However, there is still a lack of knowledge about a priori estimation of the effectiveness of RDC on different soil profiles. To this end, the aim of this paper is to develop a reliable predictive tool based on a machine-learning approach: linear genetic programming (LGP). The models are developed from a database of cone penetration test (CPT)-based case histories. It is shown that the developed LGP-based correlations yield accurate predictions for unseen data and, in addition, that the results of a parametric study demonstrate its generalisation capabilities. Furthermore, the selected optimal LGP-based model is found to yield superior performance when compared with an artificial neural network model recently developed by the authors. It is concluded that the LGP-based model developed in this study is capable of providing reliable predictions of the effectiveness of RDC under various ground conditions. © 2017, Thomas Telford Services Ltd. All rights reserved.","Field testing & monitoring; Geotechnical engineering; Strength & testing of materials","Compaction; Forecasting; Genetic algorithms; Geotechnical engineering; Learning systems; Materials testing; Neural networks; Soil mechanics; Soils; Accurate prediction; Artificial neural network modeling; Cone penetration tests; Dynamic compaction; Field testing; Ground conditions; Linear genetic programming; Machine learning approaches; Genetic programming; compaction; dynamic property; field method; geotechnical engineering; ground conditions; monitoring; soil improvement; soil profile; strength",2-s2.0-85032024669
"Sherren K., Parkins J.R., Smit M., Holmlund M., Chen Y.","Digital archives, big data and image-based culturomics for social impact assessment: Opportunities and challenges",2017,"Environmental Impact Assessment Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027580428&doi=10.1016%2fj.eiar.2017.08.002&partnerID=40&md5=cf796e798db9b642e8344d72fad773fc","Social impact assessment (SIA) is well-established but uses conventional approaches that have become less effective in recent decades, particularly in relation to declining survey response rates and a lack of youth engagement. Images from digital archives and social media sources are poised to advance the research and practice of SIA by transcending text-based methods with insights into changing landscapes, and human engagement with them. This viewpoint describes progress, challenges and cautions toward the development of such tools (defined as culturomics), using hydroelectricity cases to illustrate potential approaches. These tools build on foundational work in a range of disciplines, including the humanities and computer science. We describe necessary advances in machine learning, image digitization, and data aggregation and visualization techniques, as well as ways to ensure that such tools are carefully tested, applied and interpreted. Challenges include the automation, acquisition and management of datasets, and using these tools appropriately and equitably. Critically, culturomics of any kind must not be used as a replacement for engagement with people, but as complementary to inclusive stakeholder engagement. © 2017 Elsevier Inc.","Archives; Computational social sciences; Cultural ecosystem services; Digital humanities; Machine learning; Social media; Volunteered geographic information","Artificial intelligence; Big data; Data visualization; Ecosystems; Learning systems; Social networking (online); Surveys; Archives; Computational social science; Digital humanities; Ecosystem services; Social media; Volunteered geographic information; Economic and social effects",2-s2.0-85027580428
"Slanzi G., Pizarro G., Velásquez J.D.","Biometric information fusion for web user navigation and preferences analysis: An overview",2017,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019897318&doi=10.1016%2fj.inffus.2017.02.006&partnerID=40&md5=1654666619dc450c1155407d740d9c53","Throughout the years having knowledge of Web users’ interests, navigational actions and preferences has gained importance due to the objectives of organizations and companies. Traditionally this field has been studied from the Web Mining perspective, particularly through the Web Usage Mining (WUM) concept, which consists of the application of machine learning techniques over data originated in the Web (Web data) for automatic extraction of behavioral patterns from Web users. WUM makes use of data sources that approximate users’ behavior, such as weblogs or clickstreams among others; however these sources imply a considerable degree of subjectivity to interpret. For that reason, the application of biometric tools with the possibility of measuring actual responses to the stimuli presented via websites has become of interest in this field. Instead of doing separate analyses, information fusion (IF) tries to improve results by developing efficient methods for transforming information from different sources into a single representation, which then could be used to guide biometric data fusion to complement the traditional WUM studies and obtain better results. This paper presents a survey of Biometric IF applied to the WUM field, by first defining WUM and its main applications, later explaining how the Biometric IF could be applied and finally reviewing several studies that apply this concept to WUM. © 2017 Elsevier B.V.","Biometric data; Information fusion; Web usage mining","Biometrics; Data fusion; Information fusion; Learning systems; Metadata; Websites; Automatic extraction; Behavioral patterns; Biometric data; Biometric informations; Clickstreams; Machine learning techniques; Separate analysis; Web usage mining; Data mining",2-s2.0-85019897318
"Aftab M., Chen C., Chau C.-K., Rahwan T.","Automatic HVAC control with real-time occupancy recognition and simulation-guided model predictive control in low-cost embedded system",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028732660&doi=10.1016%2fj.enbuild.2017.07.077&partnerID=40&md5=3625117968279459aa1503ab18f69544","Intelligent building automation systems can reduce the energy consumption of heating, ventilation and air-conditioning (HVAC) units by sensing the comfort requirements automatically and scheduling the HVAC operations dynamically. Traditional building automation systems rely on fairly inaccurate occupancy sensors and basic predictive control using oversimplified building thermal response models, all of which prevent such systems from reaching their full potential. Such limitations can now be avoided due to the recent developments in embedded system technologies, which provide viable low-cost computing platforms with powerful processors and sizeable memory storage in a small footprint. As a result, building automation systems can now efficiently execute highly sophisticated computational tasks, such as real-time video processing and accurate thermal-response simulations. With this in mind, we designed and implemented an occupancy-predictive HVAC control system in a low-cost yet powerful embedded system (using Raspberry Pi 3) to demonstrate the following key features for building automation: (1) real-time occupancy recognition using video-processing and machine-learning techniques, (2) dynamic analysis and prediction of occupancy patterns, and (3) model predictive control for HVAC operations guided by real-time building thermal response simulations (using an on-board EnergyPlus simulator). We deployed and evaluated our system for providing automatic HVAC control in the large public indoor space of a mosque, thereby achieving significant energy savings. © 2017 Elsevier B.V.","Automatic HVAC control; Embedded system; Model predictive control; Occupancy recognition","Air conditioning; Automation; Buildings; Climate control; Cost benefit analysis; Costs; Embedded systems; Energy conservation; Energy utilization; Intelligent buildings; Learning systems; Model predictive control; Real time systems; Video signal processing; Building automation systems; Embedded system technology; HVAC control; Machine learning techniques; Occupancy recognition; Real-time video processing; Traditional buildings; Ventilation and air conditioning; Pattern recognition systems",2-s2.0-85028732660
"Gu H., Ren S., Si F., Xu Z.","Evolved FCM framework for working condition classification in furnace system",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969794860&doi=10.1007%2fs00500-016-2184-0&partnerID=40&md5=63d97b7db5f2073a0c6e908cac14fcc0","In this paper, an evolved FCM-based clustering method combined with entropy theory is proposed to develop a working condition classification model for the furnace system in coal-fired power plants. To overcome the disadvantage in beforehand determination of clustering number in basic FCM method, Silhouette index is selected as a parameter to evaluate clustering number adaptively in the process. Each time the FCM runs, the selected Silhouette index evaluates the clustering results considering both close and separation degree. Six datasets from UCI machine learning repository are used to certify the effectiveness of the evolved FCM method. Furthermore, pressure sequences from a 300-MW boiler are then discussed as the industrial case study. Three kinds of entropy values, featured from pressure sequence in time–frequency domain, are obtained for further clustering analysis. The clustering results show the strong relationship between boiler’s load and pressure sequences in furnace system. This method can be considered a reference method for data mining in other fluctuating and time-varying sequences. © 2016, Springer-Verlag Berlin Heidelberg.","Entropy; Evolved FCM; Pressure sequence; Silhouette index","Artificial intelligence; Boilers; Cluster analysis; Clustering algorithms; Coal; Data mining; Entropy; Fossil fuel power plants; Furnaces; Learning systems; Classification models; Clustering analysis; Clustering results; Coal-fired power plant; Evolved FCM; Industrial case study; Silhouette indices; UCI machine learning repository; Frequency domain analysis",2-s2.0-84969794860
"Miranda P.B.C., Prudêncio R.B.C., Pappa G.L.","H3AD: A hybrid hyper-heuristic for algorithm design",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020848353&doi=10.1016%2fj.ins.2017.05.029&partnerID=40&md5=b5a86257b9a28b4c5d1247c618a88c95","Designing an algorithm to solve a given problem is a challenging task due to the variety of possible design choices and the lack of clear guidelines on how to choose and/or combine them. Optimization and machine learning techniques have been used to make the algorithm design process more independent on human intervention. Hyper-heuristic approaches, in particular, have been proposed to search the space of algorithms/heuristics and/or their components, and iteratively combine and adapt them for specific problems. Although flexible to produce customized algorithms, hyper-heuristics can be extremely costly procedures. This paper proposes a novel hybrid hyper-heuristic (H3AD), which combines an automated algorithm selection approach with a generative hyper-heuristic. This combination intends to reduce the cost of providing an algorithm for a new input problem by reusing algorithms previously built by hyper-heuristics to solve similar problems. H3AD was evaluated in a case study to optimize the design of Particle Swarm Optimization algorithms in unconstrained continuous optimization problems. The results showed that H3AD provided appropriate recommendations of algorithms, reusing the algorithms generated by the hyper-heuristic to new input problems. Besides, H3AD drastically reduced the time of providing a customized algorithm when compared to generative hyper-heuristics, without a significant loss of optimization performance. © 2017 Elsevier Inc.","Algorithm design; Algorithm selection; Hyper-heuristics","Design; Evolutionary algorithms; Heuristic algorithms; Heuristic methods; Iterative methods; Learning systems; Particle swarm optimization (PSO); Problem solving; Algorithm design; Algorithm design process; Algorithm selection; Automated algorithms; Continuous optimization problems; Hyper-heuristics; Machine learning techniques; Particle swarm optimization algorithm; Optimization",2-s2.0-85020848353
"Ceron J.D., Lopez D.M., Ramirez G.A.","A mobile system for sedentary behaviors classification based on accelerometer and location data",2017,"Computers in Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021065666&doi=10.1016%2fj.compind.2017.06.005&partnerID=40&md5=4610a8c0c0ac57a46e47d24b419483e3","Background Sedentary behaviors are associated to the development of noncommunicable diseases (NCD) such as cardiovascular diseases (CVD), type 2 diabetes, and cancer. Accelerometers and inclinometers have been used to estimate sedentary behaviors, however a major limitation is that these devices do not provide enough contextual information in order to recognize the specific sedentary behavior performed, e.g., sitting or lying watching TV, using the PC, sitting at work, driving, etc. Objective Propose and evaluate the precision of a mobile system for objectively measuring six sedentary behaviors using accelerometer and location data. Results The system is implemented as an Android Mobile App, which identifies individual's sedentary behaviors based on accelerometer data taken from the smartphone or a smartwatch, and symbolic location data obtained from Bluetooth Low Energy (BLE) beacons. The system infers sedentary behaviors by means of a supervised Machine Learning Classifier. The precision of the classification of five of the six studied sedentary behaviors exceeded 95% using accelerometer data from a smartwatch attached to the wrist and 98% using accelerometer data from a smartphone put into the pocket. Statistically significant improvement in the average precision of the classification due to the use of BLE beacons was found by comparing the precision of the classification using accelerometer data only, and BLE beacons localization technology. Conclusions The proposed system provides contextual information of specific sedentary behaviors by inferring with very high precision the physical location where the sedentary event occurs. Moreover, it was found that, when accelerometers are put in the user's pocket, instead of the wrist and, when symbolic location is inferred using BLE beacons; the precision in the classification is improved. In practice, the proposed system has the potential to contribute to the understanding of the context and determinants of sedentary behaviors, necessary for the implementation and monitoring of personalized noncommunicable diseases prevention programs, for instance, sending sedentary behavior alerts, or providing personalized recommendations on physical activity. The system could be used at work to promote active breaks and healthy habits. © 2017","Accelerometer; BLE beacons; Data mining; Sedentary behaviors classification; Sensor mining","Accelerometers; Data mining; Diseases; Learning systems; Location; Signal encoding; Smartphones; Wearable computers; BLE beacons; Bluetooth low energies (BLE); Cardiovascular disease; Contextual information; Localization technologies; Non-communicable disease; Personalized recommendation; Supervised machine learning; Classification (of information)",2-s2.0-85021065666
"Mosleh Z., Salehi M.H., Jafari A., Esfandiarpoor Borujeni I., Mehnatkesh A.","Identifying sources of soil classes variations with digital soil mapping approaches in the Shahrekord plain, Iran",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032726323&doi=10.1007%2fs12665-017-7100-0&partnerID=40&md5=f95cebc709c7799696ee953cde15229e","Mapping the spatial distribution of soil classes is useful for proper soil and land-use management. This study investigates the ability of different digital soil mapping (DSM) approaches to predict taxonomic classes up to the family level in the Shahrekord plain of Chaharmahal-Va-Bakhtiari province, Iran. A total of 120 pedons were dug at various map units of a semi-detailed soil map with 750-m intervals. After pedons description, soil samples were taken from different genetic horizons. Based on the pedon descriptions and soil analytical data, pedons were classified up to the family level. Different machine learning techniques such as artificial neural networks, boosted regression tree, random forest and multinomial logistic regression were used to test the predictive power for mapping the soil classes. Overall accuracy (OA), adjusted kappa index and brier scores (BS) were used to determine the accuracy of the prediction. The model with the highest OA (i.e., the highest adjusted kappa) and the lowest BS values was considered as the most accurate model for each soil taxonomic level. Results showed that the different models had the same ability for the prediction of the soil classes across all taxonomic levels while a considerable decreasing trend was observed for their accuracy at subgroup and family levels. The terrain attributes were the most important environmental covariates to predict the soil classes in all taxonomic levels, but they could not display the soil variation entirely. This shows that the unexplained variations are controlled by unobserved variations in environment, which can be due to management over the time. Results suggest that the DSM approaches have not enough prediction accuracy for the soil classes at lower taxonomic levels that focus on the soil properties affecting land use and management. Further studies may still be required to distinguish new environmental covariates and introduce new tools to capture the complex nature of soils. © 2017, Springer-Verlag GmbH Germany.","Digital soil mapping approaches; Models’ accuracy; Soil taxonomic levels","Decision trees; Forecasting; Forestry; Land use; Landforms; Learning systems; Mapping; Neural networks; Soil testing; Soils; Boosted regression trees; Digital soil mappings; Land use and managements; Land-use management; Machine learning techniques; Multinomial logistic regression; Overall accuracies; Prediction accuracy; Soil surveys",2-s2.0-85032726323
"Zhao J., Xie X., Xu X., Sun S.","Multi-view learning overview: Recent progress and new challenges",2017,"Information Fusion",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015035411&doi=10.1016%2fj.inffus.2017.02.007&partnerID=40&md5=5cb558b0ac8b45730fa7c987d9060670","Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets. Since the last survey of multi-view machine learning in early 2013, multi-view learning has made great progress and developments in recent years, and is facing new challenges. This overview first reviews theoretical underpinnings to understand the properties and behaviors of multi-view learning. Then multi-view learning methods are described in terms of three classes to offer a neat categorization and organization. For each category, representative algorithms and newly proposed algorithms are presented. The main feature of this survey is that we provide comprehensive introduction for the recent developments of multi-view learning methods on the basis of coherence with early methods. We also attempt to identify promising venues and point out some specific challenges which can hopefully promote further research in this rapidly developing field. © 2017 Elsevier B.V.","Co-regularization; Co-training; Margin consistency; Multi-view learning; Statistical learning theory","Artificial intelligence; Data fusion; Data integration; Surveys; Co-training; Generalization performance; Margin consistency; Multi-view learning; Multiple features; Multiple views; Recent progress; Statistical learning theory; Learning systems",2-s2.0-85015035411
"Tao Y., Zhou J.","Automatic apple recognition based on the fusion of color and 3D feature for robotic fruit picking",2017,"Computers and Electronics in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029796233&doi=10.1016%2fj.compag.2017.09.019&partnerID=40&md5=dc9bfa083bd8072e99cf324c78cdf489","Accurate apple recognition is a vital step in the operation of robotic fruit picking. To improve robot recognition ability and perception in three-dimensional (3D) space, an automatic recognition method was proposed to achieve apple recognition from point cloud data. First, an improved 3D descriptor (Color-FPFH) with the fusion of color features and 3D geometry features was extracted from the preprocessed point clouds. Then, a classification category was subdivided into apple, branch, and leaf to provide the system with a more comprehensive perception capability. A classifier based on the support vector machine, optimized using a genetic algorithm, was trained by the three data classes. Finally, the results of recognition and lateral comparison were obtained by comparison with the different 3D descriptors and other classic classifiers. The results showed that the proposed method exhibited better performance. In addition, the feasibility of estimating the occurrence of blocking using proposed method was discussed. © 2017 Elsevier B.V.","Agricultural robot; Apple detection; Machine learning; Point cloud data","Agricultural machinery; Color; Genetic algorithms; Learning systems; Robotics; Three dimensional computer graphics; Agricultural robot; Automatic recognition method; Color features; Perception capability; Point cloud data; Robot recognition; Robotic fruit pickings; Three-dimensional (3D) space; Fruits; cellular automaton; color; comparative study; data assimilation; detection method; fruit; geometry; machine learning; numerical method; operations technology; optimization; robotics; Malus x domestica",2-s2.0-85029796233
"Rubio F., Martínez-Gómez J., Flores M.J., Puerta J.M.","Integration of contextual information into the scene classification problem",2017,"Robotics and Autonomous Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030165442&doi=10.1016%2fj.robot.2017.08.010&partnerID=40&md5=921934366eb7317357658be4a7a89310","The task of identifying the semantic localization of a robot has commonly been treated as a classification problem, where images are taken as input and a set of predefined labels is the output. While traditional approaches have focused on the performance of the image features extracted from computer vision techniques, the contextual information that can come with the images has not been taken into account. In this work, we present an approach for integrating this information in a scene classification pipeline where we opt for Bayesian network classifiers in addition to standard support vector machine ones. The approach is evaluated in two scenarios, one in which the contextual information is directly provided with the images, and the other where it must be inferred in an additional stage. The evaluation was performed using two families of classifiers over two datasets, and the results obtained show how the scene classification problem can benefit from the integration of contextual information. © 2017 Elsevier B.V.","Descriptor generation; Machine learning; Robotics; Scene classification","Bayesian networks; Learning systems; Robotics; Semantics; Bayesian network classifiers; Computer vision techniques; Contextual information; Descriptors; Image features; Scene classification; Traditional approaches; Classification (of information)",2-s2.0-85030165442
"Troisi J., Sarno L., Martinelli P., Di Carlo C., Landolfi A., Scala G., Rinaldi M., D’Alessandro P., Ciccone C., Guida M.","A metabolomics-based approach for non-invasive diagnosis of chromosomal anomalies",2017,"Metabolomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031419472&doi=10.1007%2fs11306-017-1274-z&partnerID=40&md5=4e786082fa2681f65efff95f6e01f596","Introduction: Chromosomal anomalies (CA) are the most frequent fetal anomalies. Objective: To evaluate the diagnostic performance of a machine learning ensemble model based on the maternal serum metabolomic fingerprint of fetal aneuploidies during the second trimester. Methods: This is a case-control pilot study. Metabolomic profiles have been obtained on serum of 328 mothers (220 controls and 108 cases), using gas chromatography coupled to mass spectrometry. Eight machines learning and classification models were built and optimized. An ensemble model was built using a voting scheme. All samples were randomly divided into two sets. One was used as training set, the other one for diagnostic performance assessment. Results: Ensemble machine learning model correctly classified all cases and controls. The accuracy was the same for trisomy 21 and 18; also, the other CA were correctly detected. Elaidic, stearic, linolenic, myristic, benzoic, citric and glyceric acid, mannose, 2-hydroxy butyrate, phenylalanine, proline, alanine and 3-methyl histidine were selected as the most relevant metabolites in class separation. Conclusion: The proposed model, based on the maternal serum metabolomic fingerprint of fetal aneuploidies during the second trimester, correctly identifies all the cases of chromosomal abnormalities. Overall, this preliminary analysis appeared suggestive of a metabolic environment conductive to increased oxidative stress and a disturbance in the fetal central nervous system development. Maternal serum metabolomics can be a promising tool in the screening of chromosomal defects. Moreover, metabolomics allows to extend our knowledge about biochemical alterations caused by aneuploidies and responsible for the observed phenotypes. © 2017, Springer Science+Business Media, LLC.","Chromosomal abnormalities; Gas chromatography mass spectrometry; Machine learning; Metabolomics; Screening test",,2-s2.0-85031419472
"Salman M., Qaisar S., Qamar A.M.","Classification and legality analysis of bowling action in the game of cricket",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019682191&doi=10.1007%2fs10618-017-0511-4&partnerID=40&md5=9017c8495eb1dd013238019c67127781","One of the hot topics in modern era of cricket is to decide whether the bowling action of a bowler is legal or not. Because of the complex bio-mechanical movement of the bowling arm, it is not possible for the on-field umpire to declare a bowling action as legal or illegal. Inertial sensors are currently being used for activity recognition in cricket for the coaching of bowlers and detecting the legality of their moves, since a well trained and legal bowling action is highly significant for the career of a cricket player. After extensive analysis and research, we present a system to detect the legality of the bowling action based on real time multidimensional physiological data obtained from the inertial sensors mounted on the bowlers arm. We propose a method to examine the movement of the bowling arm in the correct rotation order with a precise angle. The system evaluates the bowling action using various action profiles. The action profiles are used so as to simplify the complex bio-mechanical movement of the bowling arm along with minimizing the size of the data provided to the classifier. The events of interest are identified and tagged. Algorithms such as support vector machines, k-nearest neighbor, Naïve Bayes, random forest, and artificial neural network are trained over statistical features extracted from the tagged data. To accomplish the reliability of outcome measures, the technical error of measurement was adopted. The proposed method achieves very high accuracy in the correct classification of bowling action. © 2017, The Author(s).","Activity recognition; Chucking; Classification; Cricket; Feature extraction; Inertial sensors; Machine learning","Biomechanics; Classification (of information); Complex networks; Decision trees; Feature extraction; Inertial navigation systems; Kinematics; Learning systems; Mechanics; Nearest neighbor search; Neural networks; Pattern recognition; Activity recognition; Chucking; Cricket; Inertial sensor; K-nearest neighbors; Mechanical movements; Physiological data; Statistical features; Data mining",2-s2.0-85019682191
"Jha S., Seshia S.A.","A theory of formal synthesis via inductive learning",2017,"Acta Informatica",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012886819&doi=10.1007%2fs00236-017-0294-5&partnerID=40&md5=b4c826078a3d5c2b4a1e45a57fa84968","Formal synthesis is the process of generating a program satisfying a high-level formal specification. In recent times, effective formal synthesis methods have been proposed based on the use of inductive learning. We refer to this class of methods that learn programs from examples as formal inductive synthesis. In this paper, we present a theoretical framework for formal inductive synthesis. We discuss how formal inductive synthesis differs from traditional machine learning. We then describe oracle-guided inductive synthesis (OGIS), a framework that captures a family of synthesizers that operate by iteratively querying an oracle. An instance of OGIS that has had much practical impact is counterexample-guided inductive synthesis (CEGIS). We present a theoretical characterization of CEGIS for learning any program that computes a recursive language. In particular, we analyze the relative power of CEGIS variants where the types of counterexamples generated by the oracle varies. We also consider the impact of bounded versus unbounded memory available to the learning algorithm. In the special case where the universe of candidate programs is finite, we relate the speed of convergence to the notion of teaching dimension studied in machine learning theory. Altogether, the results of the paper take a first step towards a theoretical foundation for the emerging field of formal inductive synthesis. © 2017, Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Iterative methods; Learning systems; Class of methods; Formal synthesis; Inductive learning; Recursive languages; Speed of convergence; Theoretical foundations; Theoretical framework; Unbounded memory; Learning algorithms",2-s2.0-85012886819
"Song Q., Zhao M.-R., Zhou X.-H., Xue Y., Zheng Y.-J.","Predicting gastrointestinal infection morbidity based on environmental pollutants: Deep learning versus traditional models",2017,"Ecological Indicators",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021269352&doi=10.1016%2fj.ecolind.2017.06.037&partnerID=40&md5=31bbea6835aa2f63ddb2267e821d8ca0","Accurate morbidity prediction can contribute greatly to the efficiency of medical services. Gastrointestinal infectious diseases are largely influenced by environmental pollutants, but predicting their morbidity based on pollution indicators is quite difficult because of the complex relationship between the pollutants and the infections. This study presents a deep neural network (DNN) model for estimating the morbidity of gastrointestinal infections based on 129 types of pollutants contained in soil and water. The DNN uses a deep Boltzmann machine (DBM) to model the unknown probabilistic relationship between the pollutants, and employs a Gaussian mixture model (GMM) to output the estimated morbidity. We also propose an evolutionary algorithm for efficiently training the DNN. Experiment on a data set from four counties in central China shows that the proposed model can estimate the morbidity much more accurately than traditional neural network and linear regression models. © 2017 Elsevier Ltd","Deep neural network (DNN); Environmental pollutants; Gastrointestinal infections; Prediction","Complex networks; Deep learning; Diseases; Forecasting; Gaussian distribution; Pollution; Regression analysis; Water pollution; Complex relationships; Deep boltzmann machines; Environmental pollutants; Gastrointestinal infections; Gaussian Mixture Model; Infectious disease; Linear regression models; Pollution indicators; Deep neural networks; algorithm; artificial neural network; infectious disease; morbidity; numerical model; pollutant; prediction; China",2-s2.0-85021269352
"Ma J., Wu F., Jiang T., Zhao Q., Kong D.","Ultrasound image-based thyroid nodule automatic segmentation using convolutional neural networks",2017,"International Journal of Computer Assisted Radiology and Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026518541&doi=10.1007%2fs11548-017-1649-7&partnerID=40&md5=38aaeaf11ccdfef5c9a98b989ca53389","Purpose: Delineation of thyroid nodule boundaries from ultrasound images plays an important role in calculation of clinical indices and diagnosis of thyroid diseases. However, it is challenging for accurate and automatic segmentation of thyroid nodules because of their heterogeneous appearance and components similar to the background. In this study, we employ a deep convolutional neural network (CNN) to automatically segment thyroid nodules from ultrasound images. Methods: Our CNN-based method formulates a thyroid nodule segmentation problem as a patch classification task, where the relationship among patches is ignored. Specifically, the CNN used image patches from images of normal thyroids and thyroid nodules as inputs and then generated the segmentation probability maps as outputs. A multi-view strategy is used to improve the performance of the CNN-based model. Additionally, we compared the performance of our approach with that of the commonly used segmentation methods on the same dataset. Results: The experimental results suggest that our proposed method outperforms prior methods on thyroid nodule segmentation. Moreover, the results show that the CNN-based model is able to delineate multiple nodules in thyroid ultrasound images accurately and effectively. In detail, our CNN-based model can achieve an average of the overlap metric, dice ratio, true positive rate, false positive rate, and modified Hausdorff distance as 0.8683 ± 0.0056 , 0.9224 ± 0.0027 , 0.915 ± 0.0077 , 0.0669 ± 0.0032 , 0.6228 ± 0.1414 on overall folds, respectively. Conclusion: Our proposed method is fully automatic without any user interaction. Quantitative results also indicate that our method is so efficient and accurate that it can be good enough to replace the time-consuming and tedious manual segmentation approach, demonstrating the potential clinical applications. © 2017, CARS.","Convolutional neural network; Segmentation; Thyroid nodule; Ultrasound image","adult; aged; Article; automation; comparative effectiveness; computer assisted diagnosis; controlled study; convolutional neural network; diagnostic accuracy; diagnostic error; diagnostic test accuracy study; echography; experimental study; false positive result; female; human; image analysis; image segmentation; intermethod comparison; machine learning; major clinical study; male; modified hausdorff distance; parameters; predictive value; priority journal; support vector machine; syndrome delineation; thyroid nodule; true positive result",2-s2.0-85026518541
"Cui S., Mao L., Xiong S.","Brain tumor automatic segmentation using fully convolutional networks",2017,"Journal of Medical Imaging and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030085889&doi=10.1166%2fjmihi.2017.2179&partnerID=40&md5=0830a3e64a14ed6f2265876d24f5bfc3","Brain tumor segmentation has the most important significance in tumor diagnose, analysis, and treatment. Magnetic resonance imaging (MRI) provides rich and valuable information for physicians to analyze tumors. However, intracranial tumors can grow anywhere in the brain and have complexed structure and amorphous shape, which makes manual segmentation inconceivable to mark millions of imaging data from MRI. This paper explores a new method for brain tumor automatic segmentation of MRI images using the Fully Convolutional Networks. We start the intensity normalization in pre-processing phase to improve comparability of different scans. Then, we take the whole slices across four modalities as four-channel input images to train network, which can overcome the individual difference of brain tumor and difficulty to set patch size in the methods based on the center pixel classification of image patch. The experiment results of 220 patients from Brain Tumor Segmentation Challenge 2015 database show that the proposed method is effective. The average Dice Similarity Coefficient metric are 84.21%, 77.36%, and 85.07% in the complete, core, and enhanced regions, respectively. The average time of segmenting one slice is around 0.15 s. The proposed method can adapt to the differences of brain tumors, and segment brain tumors quickly accurately. Copyright © 2017 American Scientific Publishers All rights reserved.","Brain tumor segmentation; Fully convolutional network; Magnetic resonance imaging (MRI)","Article; brain slice; brain tumor; convolutional neural network; fine tuning network; fully convolutional network; human; image processing; image segmentation; machine learning; major clinical study; nuclear magnetic resonance imaging; process optimization; support vector machine",2-s2.0-85030085889
"O'Neill T.J., Davenport E.M., Murugesan G., Montillo A., Maldjian J.A.","Applications of Resting State Functional MR Imaging to Traumatic Brain Injury",2017,"Neuroimaging Clinics of North America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027439785&doi=10.1016%2fj.nic.2017.06.006&partnerID=40&md5=da4ecb6cd5a851aee3bbd90e4ada5d53","Traumatic brain injury (TBI) is an important public health issue. TBI includes a broad spectrum of injury severities and abnormalities. Functional MR imaging (fMR imaging), both resting state (rs) and task, has been used often in research to study the effects of TBI. Although rs-fMR imaging is not currently applicable in clinical diagnosis of TBI, computer-aided tools are making this a possibility for the future. Specifically, graph theory is being used to study the change in networks after TBI. Machine learning methods allow researchers to build models capable of predicting injury severity and recovery trajectories. © 2017 Elsevier Inc.","BOLD; fMR imaging; Graph theory; Machine learning; Magnetoencephalography; Resting state; TBI","automation; brain dysfunction; caudate nucleus; clinical protocol; cognitive defect; computer assisted tomography; convalescence; default mode network; diagnostic imaging; frontoparietal cortex; functional connectivity; functional magnetic resonance imaging; hippocampus; human; image processing; image reconstruction; independent component analysis; injury severity; machine learning; magnetoencephalography; medial prefrontal cortex; neuroimaging; orbital cortex; outcome assessment; parietal cortex; patient monitoring; posterior cingulate; precuneus; prediction; priority journal; quantitative analysis; Review; sensorimotor function; thalamus; traumatic brain injury",2-s2.0-85027439785
"Doostan M., Chowdhury B.H.","Power distribution system fault cause analysis by using association rule mining",2017,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024865776&doi=10.1016%2fj.epsr.2017.07.005&partnerID=40&md5=ffed7ea0d5c27daf753fcbe7e538c534","In recent years, with the increasing requirements on power distribution utilities to ensure system reliability and to improve customers and regulators satisfaction, utilities seek to find practical solutions that enable them to restrict specific faults or to better manage their responses to unavoidable power outages. For achieving either, it is crucial to acquire a profound understanding of different faults by exploring their underlying causes and identifying key variables related to those causes. Currently, statistical models as well as advanced data analytics techniques are common tools to gain such understanding. Although basic statistical analysis provides a general knowledge of the primary causes of faults; nevertheless, it falls short of describing nuanced conditions that lead to a fault. On the other hand, applying sophisticated algorithms can produce deeper insight into the main causes; however, it would be computationally burdensome and might require a tremendous amount of running time. In order to overcome these problems, this paper proposes a novel approach for fault cause analysis by using association rule mining. The primary goals are to characterize faults according to their underlying causes and to identify important variables that strongly impact fault frequency. This paper proposes a step-by-step procedure, which deals with data preparation, practical issues associated with fault data sets, and implementation of association rule mining. The procedure is followed by a comprehensive case study to demonstrate how the proposed approach can be used to mine for causal structures and identify frequent patterns for vegetation, animal, equipment failure, public accident, and lightning-related faults. © 2017 Elsevier B.V.","Apriori algorithm; Association rule mining; Fault cause analysis; Machine learning; Pattern recognition; RandomForestSRC; ROSE; SMOTE; Synthetic data","Association rules; Customer satisfaction; Learning systems; Pattern recognition; Apriori algorithms; RandomForestSRC; ROSE; SMOTE; Synthetic data; Outages",2-s2.0-85024865776
"Vanderhaegen S., Canters F.","Mapping urban form and function at city block level using spatial metrics",2017,"Landscape and Urban Planning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026523390&doi=10.1016%2fj.landurbplan.2017.05.023&partnerID=40&md5=1cbb5dd72ae90c5e23fb4841044e53b1","This paper focuses on the potential of urban metrics describing the presence and the configuration of built-up and open space areas for mapping distinct types of urban form and function at city block level. Next to traditional, patch-based metrics used in landscape ecology, alternative metrics are proposed, measuring the presence and the spatial arrangement of built-up and open space areas along a set of radial transects, along contours parallel to the urban block boundary and along the block's perimeter, as well as metrics describing the internal composition of the built-up area. Use of the proposed metrics for identifying different types of urban form and function was tested on the Brussels Capital Region. Large-scale vector data was used to define built-up structures and to analyse the morphological properties of the built-up area at block level. Decision tree classification was applied in conjunction with bootstrap aggregation to gain insight in the distinctive character of the defined metrics, and the robustness of land use and urban form classification based on these metrics. Our study points out the shortcomings of traditional landscape ecological metrics for mapping urban form and emphasizes the need for alternative approaches for analysing urban landscapes, more explicitly describing the morphological characteristics of the urban fabric. © 2017 Elsevier B.V.","Brussels Capital Region; Land use; Machine learning; Spatial metrics; Urban form; Urban function","Decision trees; Ecology; Land use; Learning systems; Mapping; Brussels Capital Region; Decision tree classification; Internal composition; Morphological characteristic; Morphological properties; Spatial arrangements; Spatial metrics; Urban form; Structures (built objects); analytical method; building; land use; landscape ecology; machine learning; mapping; open space; spatial analysis; transect; urban area; urban design; urban morphology; Belgium; Brussels [Belgium]",2-s2.0-85026523390
"Panapakidis I.P., Christoforidis G.C.","Implementation of modified versions of the K-means algorithm in power load curves profiling",2017,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031313915&doi=10.1016%2fj.scs.2017.08.002&partnerID=40&md5=1882fb92870eb9934265b054364e811e","Recent approaches in load profiling involve the utilization of clustering algorithms to classify a load data set with little or no external information on its structure and relationships between the data is available. In the load profiling literature many algorithms have been presented. K-means is the most commonly used algorithms. While its robustness is already displayed, the main limitation lies on its dependence on the initialization phase. The present paper proposes two novel modified versions of the algorithm in order to deal with the aforementioned problem. Clustering is part of a multi-stage load profiling framework. Apart from the clustering itself, other stages include the selection of pattern representation technique and the extraction of “representative” consumers. Apart from the expressing the patterns as time sequences of load values, two other are utilized that refer to a dimensionality reduction method and statistical indexes. The comparison of the algorithms is held through four clustering validity indicators. Simulation results indicate that the proposed modified versions of the K-means lead to higher clustering accuracy in all cases examined. Moreover, the multi-stage clustering approach followed in this study leads to lower clustering time requirements, a fact that is significant in cases with vast amount of data. © 2017 Elsevier Ltd","Electricity consumer categorization; Load modeling; Load profiles; Machine learning; Time-series clustering","Classification (of information); Electric load management; Learning systems; Clustering validity indicators; Dimensionality reduction method; Electricity consumers; Load modeling; Load profiles; Multi-stage clustering; Pattern representation; Time series clustering; Clustering algorithms",2-s2.0-85031313915
"A. Aziz A.S., Hanafi S.E.-O., Hassanien A.E.","Comparison of classification techniques applied for network intrusion detection and classification",2017,"Journal of Applied Logic",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007012627&doi=10.1016%2fj.jal.2016.11.018&partnerID=40&md5=ffcb4fda25c4464e2cc06363bf983912","In a previous research, a multi-agent artificial immune system for network intrusion detection and classification was proposed and tested, where a multi-layer detection and classification process was executed on each agent, for each host in the network. In this paper, we show the experiments that were held to chose the appropriate classifiers by testing different classifiers and comparing them to increase the detection accuracy and obtain more information on the detected anomalies. It will be shown that no single classifier should be used for all types of attacks, due to different classification rates obtained. This is due to attacks representations in the train set and dependency between features used to detect them. It will also be shown that a basic and simple classifier such as Naive Bayes has better classification results in the case of low-represented attacks, and the basic decision trees such as Naive-Bayes Tree and Best-First Tree give very good results compared to well-known J48 (Weka implementation of C4.5) and Random Forest decision trees. Based on these experiments and their results, Naive Bayes and Best-First tree classifiers were selected to classify the anomaly-detected traffic. It was shown that in the detection phase, 90% of anomalies were detected, and in the classification phase, 88% of false positives were successfully labeled as normal traffic connections, and 79% of DoS and Probe attacks were labeled correctly, mostly by NB, NBTree, and BFTree classifiers. © 2016 Elsevier B.V.","Artificial Immune Systems; Intrusion Classification; Intrusion Detection; Machine Learning","Classifiers; Decision trees; Forestry; Immune system; Intrusion detection; Learning systems; Mercury (metal); Multi agent systems; Artificial Immune System; Classification process; Classification rates; Classification results; Classification technique; Detection accuracy; Network intrusion detection; Traffic connections; Classification (of information)",2-s2.0-85007012627
"Hong P., Sun H., Sha L., Pu Y., Khatri K., Yu X., Tang Y., Lin C.","GlycoDeNovo – an Efficient Algorithm for Accurate de novo Glycan Topology Reconstruction from Tandem Mass Spectra",2017,"Journal of the American Society for Mass Spectrometry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031900715&doi=10.1007%2fs13361-017-1760-6&partnerID=40&md5=a8077eb0d8830ec8c79d7423928daed3","A major challenge in glycomics is the characterization of complex glycan structures that are essential for understanding their diverse roles in many biological processes. We present a novel efficient computational approach, named GlycoDeNovo, for accurate elucidation of the glycan topologies from their tandem mass spectra. Given a spectrum, GlycoDeNovo first builds an interpretation-graph specifying how to interpret each peak using preceding interpreted peaks. It then reconstructs the topologies of peaks that contribute to interpreting the precursor ion. We theoretically prove that GlycoDeNovo is highly efficient. A major innovative feature added to GlycoDeNovo is a data-driven IonClassifier which can be used to effectively rank candidate topologies. IonClassifier is automatically learned from experimental spectra of known glycans to distinguish B- and C-type ions from all other ion types. Our results showed that GlycoDeNovo is robust and accurate for topology reconstruction of glycans from their tandem mass spectra. [Figure not available: see fulltext.]. © 2017, American Society for Mass Spectrometry.","De novo glycan sequencing; Electronic excitation dissociation; Fourier-transform ion cyclotron resonance mass spectrometry; Machine learning","Ions; Learning systems; Mass spectrometers; Mass spectrometry; Topology; Candidate topologies; Computational approach; De novo glycan sequencing; Electronic excitation; Experimental spectra; Fourier transform ion cyclotron resonance mass spectrometry; Tandem mass spectra; Topology reconstruction; Polysaccharides; glycan; monosaccharide; Article; catalyst; computer analysis; derivatization; fractionation; glycobiology; ion cyclotron resonance mass spectrometry; machine learning; measurement accuracy; methylation; tandem mass spectrometry",2-s2.0-85031900715
"Starc J., Mladenić D.","Constructing a Natural Language Inference dataset using generative neural networks",2017,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019756259&doi=10.1016%2fj.csl.2017.04.009&partnerID=40&md5=4e0febdb5b913e1f0c5bb0b995b64d60","Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric—the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one. © 2017 Elsevier Ltd","Dataset construction; Generative neural network; Machine learning; Natural language generation; Natural Language Inference; Recurrent neural network","Learning systems; Natural language processing systems; Recurrent neural networks; Classification accuracy; Distinguishability; Generative model; Logical relations; Natural language generation; Natural language understanding; Natural languages; Training example; Classification (of information)",2-s2.0-85019756259
"Kavakiotis I., Samaras P., Triantafyllidis A., Vlahavas I.","FIFS: A data mining method for informative marker selection in high dimensional population genomic data",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030696585&doi=10.1016%2fj.compbiomed.2017.09.020&partnerID=40&md5=c32cf88d4315867fc05c00cb67cacc11","Background and objective Single Nucleotide Polymorphism (SNPs) are, nowadays, becoming the marker of choice for biological analyses involving a wide range of applications with great medical, biological, economic and environmental interest. Classification tasks i.e. the assignment of individuals to groups of origin based on their (multi-locus) genotypes, are performed in many fields such as forensic investigations, discrimination between wild and/or farmed populations and others. Τhese tasks, should be performed with a small number of loci, for computational as well as biological reasons. Thus, feature selection should precede classification tasks, especially for Single Nucleotide Polymorphism (SNP) datasets, where the number of features can amount to hundreds of thousands or millions. Methods In this paper, we present a novel data mining approach, called FIFS – Frequent Item Feature Selection, based on the use of frequent items for selection of the most informative markers from population genomic data. It is a modular method, consisting of two main components. The first one identifies the most frequent and unique genotypes for each sampled population. The second one selects the most appropriate among them, in order to create the informative SNP subsets to be returned. Results The proposed method (FIFS) was tested on a real dataset, which comprised of a comprehensive coverage of pig breed types present in Britain. This dataset consisted of 446 individuals divided in 14 sub-populations, genotyped at 59,436 SNPs. Our method outperforms the state-of-the-art and baseline methods in every case. More specifically, our method surpassed the assignment accuracy threshold of 95% needing only half the number of SNPs selected by other methods (FIFS: 28 SNPs, Delta: 70 SNPs Pairwise FST: 70 SNPs, In: 100 SNPs.) Conclusion Our approach successfully deals with the problem of informative marker selection in high dimensional genomic datasets. It offers better results compared to existing approaches and can aid biologists in selecting the most informative markers with maximum discrimination power for optimization of cost-effective panels with applications related to e.g. species identification, wildlife management, and forensics. © 2017 Elsevier Ltd","Ancestry informative marker; Big data; Bioinformatics; Data mining; Feature selection; Frequent pattern mining; Machine learning; Population genomics; Single nucleotide polymorphism","Big data; Bioinformatics; Classification (of information); Cost effectiveness; Feature extraction; Genes; Learning systems; Nucleotides; Population statistics; Ancestry informative marker; Classification tasks; Data mining methods; Forensic investigation; Frequent pattern mining; Genomics; Single nucleotide polymorphisms; Species identification; Data mining; Article; bioinformatics; breed difference; data mining; frequent item feature selection; gene frequency; genotype; Great Britain; heterozygosity; nonhuman; pig breed; population; population genetics; priority journal; single nucleotide polymorphism",2-s2.0-85030696585
"Roudier P., Malone B.P., Hedley C.B., Minasny B., McBratney A.B.","Comparison of regression methods for spatial downscaling of soil organic carbon stocks maps",2017,"Computers and Electronics in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029601035&doi=10.1016%2fj.compag.2017.08.021&partnerID=40&md5=f6ed756a28c4b14bf57e6a13c019166c","This paper presents a refinement of the dissever algorithm, a framework for downscaling spatial information based on available environmental covariates proposed by Malone et al. (2012). While the original algorithm models the relationships between the target variable and the covariates using a general additive model (GAM), the modified procedure presented in this paper allows the user to choose between a wide range of regression methods. These developments have been implemented in an open-source package for the R statistical environment, and tested by downscaling soil organic carbon stocks (SOCS) maps available on two study sites in Australia and New Zealand using 4 different regression methods: linear model (LM), GAM, random forest (RF), and Cubist (CU). In this study, the spatial resolution of a set of reference maps were degraded to a coarser resolution, so to assess the performance of the different downscaling methods. On the Australian site, the 1-km SOCS coarse resolution map has been downscaled to a 90-m resolution. The best results were achieved using either CU or RF (R2=0.91 and 0.94 respectively). On the New Zealand site, the 250-m SOCS coarse resolution map has been downscaled to a 10-m resolution. The best results were achieved using GAM (R2=0.90). The results illustrate that the optimal regression methods for downscaling spatial information using dissever vary on a case-by-case basis. In particular, simpler approaches such as LM or GAM outperformed more complex approaches in cases where only a limited number of pixels are available to train the downscaling algorithm. This demonstrate the value of an implementation that facilitates testing of different regression strategies. © 2017 Elsevier B.V.","Digital soil mapping; Machine learning; R; Spatial downscaling","Decision trees; Learning systems; Organic carbon; Soil surveys; Soils; Digital soil mappings; Down-scaling; Downscaling methods; General additive models; Open source package; Original algorithms; Soil organic Carbon stocks; Spatial informations; Regression analysis; algorithm; carbon sequestration; comparative study; digital mapping; downscaling; machine learning; map; organic carbon; pixel; regression analysis; sampler; software; soil carbon; spatial resolution; Australia; New Zealand",2-s2.0-85029601035
"Stančić I., Musić J., Grujić T.","Gesture recognition system for real-time mobile robot control based on inertial sensors and motion strings",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029641125&doi=10.1016%2fj.engappai.2017.08.013&partnerID=40&md5=b62466a05630e0ff2d25bc0a377632ca","Navigating and controlling a mobile robot in an indoor or outdoor environment by using a range of body-worn sensors is becoming an increasingly interesting research area in the robotics community. In such scenarios, hand gestures offer some unique capabilities for human–robot interaction inherent to nonverbal communication with features and application scenarios not possible with the currently predominant vision-based systems. Therefore, in this paper, we propose and develop an effective inertial-sensor-based system, worn by the user, along with a microprocessor and wireless module for communication with the robot at distances of up to 250 m. Possible features describing hand-gesture dynamics are introduced and their feasibility is demonstrated in an off-line scenario by using several classification methods (e.g., random forests and artificial neural networks). Refined motion features are then used in K-means unsupervised clustering for motion primitive extraction, which forms the motion strings used for real-time classification. The system demonstrated an F1 score of 90.05% with the possibility of gesture spotting and null class classification (e.g., undefined gestures were discarded from the analysis). Finally, to demonstrate the feasibility of the proposed algorithm, it was implemented in an Arduino-based 8-bit ATmega2560 microcontroller for control of a mobile, tracked robot platform. © 2017 Elsevier Ltd","Hand gestures; Human–robot interaction; Inertial sensors; Machine learning; Mobile robot control; Real-time classification","Decision trees; Gesture recognition; Human robot interaction; Inertial navigation systems; Learning systems; Mobile robots; Neural networks; Robots; Wearable sensors; Wireless sensor networks; Hand gesture; Inertial sensor; Mobile robot control; Real time; Robot interactions; Visual servoing",2-s2.0-85029641125
"Chang C.-C., Liao B.-H.","Active learning based on minimization of the expected path-length of random walks on the learned manifold structure",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022324164&doi=10.1016%2fj.patcog.2017.06.001&partnerID=40&md5=88c0aa6c2b4ef6384336b1981568422c","Active learning algorithms aim at selecting important samples to label for subsequent machine learning tasks. Many active learning algorithms make use of the reproducing kernel Hilbert space (RKHS) induced by a Gaussian radial basis function (RBF) kernel and leverage the geometrical structure of the data for query-sample selection. Parameters for the kernel function and the k-nearest-neighborhood graph must be properly set beforehand. As a tool exploring the structure of data, active learning algorithms with automatic tuning of those parameters are desirable. In this paper, local linear embedding (LLE) with convex constraints on neighbor weights is used to learn the geometrical structure of the data in the RKHS induced by a Gaussian RBF kernel. Automatic tuning of the kernel parameter is based on the assumption that the geometrical structure of the data in the RKHS is sparse and local. With the Markov matrix established based on the learned LLE weight matrix, the total expected path-length of the random walks from all samples to selected samples is proposed to be a criterion for query-sample selection. A greedy algorithm having a guaranteed solution bound is developed to select query samples and a two-phase scheme is also proposed for scaling the proposed active learning algorithm. Experimental results on data sets including hundreds to tens of thousands of samples have shown the feasibility of the proposed approach. © 2017 Elsevier Ltd","Active learning; Locally linear embedding; Random walks; Submodular set functions","Artificial intelligence; Education; Geometry; Matrix algebra; Radial basis function networks; Random processes; Active Learning; Active-learning algorithm; Gaussian radial basis functions; K-nearest neighborhoods; Locally linear embedding; Random Walk; Reproducing Kernel Hilbert spaces; Submodular set functions; Learning algorithms",2-s2.0-85022324164
"van Hecke K., de Croon G.C.H.E., Hennes D., Setterfield T.P., Saenz-Otero A., Izzo D.","Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning",2017,"Acta Astronautica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026883328&doi=10.1016%2fj.actaastro.2017.07.038&partnerID=40&md5=5a0174f4aa750df9555d4ba61ba6bf86","Although machine learning holds an enormous promise for autonomous space robots, it is currently not employed because of the inherent uncertain outcome of learning processes. In this article we investigate a learning mechanism, Self-Supervised Learning (SSL), which is very reliable and hence an important candidate for real-world deployment even on safety-critical systems such as space robots. To demonstrate this reliability, we introduce a novel SSL setup that allows a stereo vision equipped robot to cope with the failure of one of its cameras. The setup learns to estimate average depth using a monocular image, by using the stereo vision depths from the past as trusted ground truth. We present preliminary results from an experiment on the International Space Station (ISS) performed with the MIT/NASA SPHERES VERTIGO satellite. The presented experiments were performed on October 8th, 2015 on board the ISS. The main goals were (1) data gathering, and (2) navigation based on stereo vision. First the astronaut Kimiya Yui moved the satellite around the Japanese Experiment Module to gather stereo vision data for learning. Subsequently, the satellite freely explored the space in the module based on its (trusted) stereo vision system and a pre-programmed exploration behavior, while simultaneously performing the self-supervised learning of monocular depth estimation on board. The two main goals were successfully achieved, representing the first online learning robotic experiments in space. These results lay the groundwork for a follow-up experiment in which the satellite will use the learned single-camera depth estimation for autonomous exploration in the ISS, and are an advancement towards future space robots that continuously improve their navigation capabilities over time, even in harsh and completely unknown space environments. © 2017 IAA","Monocular depth estimation; Persistent self-supervised learning; Space robotics; Stereo vision","Cameras; Computer vision; Distance education; Learning systems; Robotics; Robots; Safety engineering; Satellites; Space research; Space stations; Stereo image processing; Supervised learning; Autonomous exploration; Depth Estimation; Enabling technologies; International Space stations; Japanese experiment modules; Real world deployment; Safety critical systems; Space robotics; Stereo vision",2-s2.0-85026883328
"Ruan Y., Xue X., Liu H., Tan J., Li X.","Quantum Algorithm for K-Nearest Neighbors Classification Based on the Metric of Hamming Distance",2017,"International Journal of Theoretical Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027975843&doi=10.1007%2fs10773-017-3514-4&partnerID=40&md5=e9775beb871b481165167bc912fa3b72","K-nearest neighbors (KNN) algorithm is a common algorithm used for classification, and also a sub-routine in various complicated machine learning tasks. In this paper, we presented a quantum algorithm (QKNN) for implementing this algorithm based on the metric of Hamming distance. We put forward a quantum circuit for computing Hamming distance between testing sample and each feature vector in the training set. Taking advantage of this method, we realized a good analog for classical KNN algorithm by setting a distance threshold value t to select k − nearest neighbors. As a result, QKNN achieves O(n3) performance which is only relevant to the dimension of feature vectors and high classification accuracy, outperforms Llyod’s algorithm (Lloyd et al. 2013) and Wiebe’s algorithm (Wiebe et al. 2014). © 2017, Springer Science+Business Media, LLC.","Classification; Hamming distance; K-nearest neighbors algorithm; Machine learning; Quantum algorithm",,2-s2.0-85027975843
"Koziarski M., Krawczyk B., Woźniak M.","The deterministic subspace method for constructing classifier ensembles",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030315529&doi=10.1007%2fs10044-017-0655-2&partnerID=40&md5=be2b398583a8bc7b988b4f64595eb160","Ensemble classification remains one of the most popular techniques in contemporary machine learning, being characterized by both high efficiency and stability. An ideal ensemble comprises mutually complementary individual classifiers which are characterized by the high diversity and accuracy. This may be achieved, e.g., by training individual classification models on feature subspaces. Random Subspace is the most well-known method based on this principle. Its main limitation lies in stochastic nature, as it cannot be considered as a stable and a suitable classifier for real-life applications. In this paper, we propose an alternative approach, Deterministic Subspace method, capable of creating subspaces in guided and repetitive manner. Thus, our method will always converge to the same final ensemble for a given dataset. We describe general algorithm and three dedicated measures used in the feature selection process. Finally, we present the results of the experimental study, which prove the usefulness of the proposed method. © 2017, The Author(s).","Classifier diversity; Classifier ensemble; Feature subspaces; Machine learning",,2-s2.0-85030315529
"Llanos F., Xie Z., Chandrasekaran B.","Hidden Markov modeling of frequency-following responses to Mandarin lexical tones",2017,"Journal of Neuroscience Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028017854&doi=10.1016%2fj.jneumeth.2017.08.010&partnerID=40&md5=01aa9dfa4b574daf465b158e68f72a26","Background The frequency-following response (FFR) is a scalp-recorded electrophysiological potential reflecting phase-locked activity from neural ensembles in the auditory system. The FFR is often used to assess the robustness of subcortical pitch processing. Due to low signal-to-noise ratio at the single-trial level, FFRs are typically averaged across thousands of stimulus repetitions. Prior work using this approach has shown that subcortical encoding of linguistically-relevant pitch patterns is modulated by long-term language experience. New method We examine the extent to which a machine learning approach using hidden Markov modeling (HMM) can be utilized to decode Mandarin tone-categories from scalp-record electrophysiolgical activity. We then assess the extent to which the HMM can capture biologically-relevant effects (language experience-driven plasticity). To this end, we recorded FFRs to four Mandarin tones from 14 adult native speakers of Chinese and 14 of native English. We trained a HMM to decode tone categories from the FFRs with varying size of averages. Results and comparisons with existing methods Tone categories were decoded with above-chance accuracies using HMM. The HMM derived metric (decoding accuracy) revealed a robust effect of language experience, such that FFRs from native Chinese speakers yielded greater accuracies than native English speakers. Critically, the language experience-driven plasticity was captured with average sizes significantly smaller than those used in the extant literature. Conclusions Our results demonstrate the feasibility of HMM in assessing the robustness of neural pitch. Machine-learning approaches can complement extant analytical methods that capture auditory function and could reduce the number of trials needed to capture biological phenomena. © 2017 Elsevier B.V.","Frequency-following response; Hidden markov model; Machine learning; Pitch encoding; Plasticity",,2-s2.0-85028017854
"Meng D., Zhao Q., Jiang L.","A theoretical understanding of self-paced learning",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020810893&doi=10.1016%2fj.ins.2017.05.043&partnerID=40&md5=6003a005bf0a1ffd98dcba45c74b9a83","Self-paced learning (SPL) is a recently proposed methodology designed by mimicking through the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically demonstrated to be effective in these applications. However, the literature is in lack of the theoretical understanding of SPL. Regarding this research gap, this study attempts to provide some new theoretical understanding of the SPL scheme. Specifically, we prove that the solution strategy on SPL accords with a majorization minimization algorithm implemented on an implicit objective function. Furthermore, we found that the loss function contained in this implicit objective has a similar configuration with the non-convex regularized penalty (NCRP) known in statistics and machine learning. Such connection inspires us to discover more intrinsic relationships between the SPL regimes and the NCRP forms, like smoothly clipped absolute deviation (SCAD), logarithmic penalty (LOG) and non-convex exponential penalty (EXP). The insight of the robustness under SPL can then be finely explained. We also analyze the capability of SPL regarding its easy loss-prior-embedding property, and provide an insightful interpretation of the effectiveness mechanism under current SPL variations. Moreover, we design a group-partial-order loss prior, which is especially useful for weakly labeled large-scale data processing tasks. By applying SPL with this loss prior to the FCVID dataset, which is currently one of the largest manually annotated video dataset, our method achieves state-of-the-art performance above existing methods, which further supports the proposed theoretical arguments. © 2017","Curriculum learning; Multimedia event detection; Non-convex regularized penalty; Self-paced learning","Data handling; Learning systems; Large-scale data processing; Minimization algorithms; Multimedia event detections; Non-convex regularized penalty; Self-paced learning; Smoothly clipped absolute deviation; State-of-the-art performance; Theoretical arguments; Pattern recognition",2-s2.0-85020810893
"Fuller D., Buote R., Stanley K.","A glossary for big data in population and public health: Discussion and commentary on terminology and research methods",2017,"Journal of Epidemiology and Community Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031786632&doi=10.1136%2fjech-2017-209608&partnerID=40&md5=da036bc9e142236745e04d435f32b806","The volume and velocity of data are growing rapidly and big data analytics are being applied to these data in many fields. Population and public health researchers may be unfamiliar with the terminology and statistical methods used in big data. This creates a barrier to the application of big data analytics. The purpose of this glossary is to define terms used in big data and big data analytics and to contextualise these terms. We define the five Vs of big data and provide definitions and distinctions for data mining, machine learning and deep learning, among other terms. We provide key distinctions between big data and statistical analysis methods applied to big data. We contextualise the glossary by providing examples where big data analysis methods have been applied to population and public health research problems and provide brief guidance on how to learn big data analysis methods. © Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017.",,"data interpretation; data mining; machine learning; public health; research method; statistical analysis; terminology; data analysis; data mining; human; machine learning; nomenclature; public health; statistical analysis",2-s2.0-85031786632
"Turvey C., Fortney J.","The Use of Telemedicine and Mobile Technology to Promote Population Health and Population Management for Psychiatric Disorders",2017,"Current Psychiatry Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031998230&doi=10.1007%2fs11920-017-0844-0&partnerID=40&md5=fd2848bf2cf3a2b7e65b0fd6d827636c","Purpose of Review: This article discusses recent applications in telemedicine to promote the goals of population health and population management for people suffering psychiatric disorders. Recent Findings: The use of telemedicine to promote collaborative care, self-monitoring and chronic disease management, and population screening has demonstrated broad applicability and effectiveness. Collaborative care using videoconferencing to facilitate mental health specialty consults has demonstrated effectiveness in the treatment of depression, PTSD, and also ADHD in pediatric populations. Mobile health is currently being harnessed to monitor patient symptom trajectories with the goal of using machine learning algorithms to predict illness relapse. Patient portals serve as a bridge between patients and providers. They provide an electronically secure shared space for providers and patients to collaborate and optimize care. Summary: To date, research has supported the effectiveness of telemedicine in promoting population health. Future endeavors should focus on developing the most effective clinical protocols for using these technologies to ensure long-term use and maximum effectiveness in reducing population burden of mental health. © 2017, Springer Science+Business Media, LLC.","Behavioral health; Collaborative care; Mobile applications; Patient portals; Telemedicine; Videoconferencing","adult; algorithm; attention deficit disorder; child; chronic disease; clinical effectiveness; consultation; depression; health promotion; human; machine learning; mass communication; mass screening; mental disease; mental health; mental health care; mobile technology; patient care; patient monitoring; posttraumatic stress disorder; prediction; relapse; Review; self monitoring; telemedicine; videoconferencing",2-s2.0-85031998230
"Chow J.C.L.","Internet-based computer technology on radiotherapy",2017,"Reports of Practical Oncology and Radiotherapy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028922084&doi=10.1016%2fj.rpor.2017.08.005&partnerID=40&md5=ea914fbf70ea19102db7643a78906aff","Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well. © 2017 Greater Poland Cancer Centre","Big data; Cloud computing; Computer technology; Machine learning; Radiotherapy","cloud computing; human; Internet; machine learning; radiation dose; radiotherapy; treatment planning",2-s2.0-85028922084
"Kleindl P.A., Xiong J., Hewarathna A., Mozziconacci O., Nariya M.K., Fisher A.C., Deeds E.J., Joshi S.B., Middaugh C.R., Schöneich C., Volkin D.B., Forrest M.L.","The Botanical Drug Substance Crofelemer as a Model System for Comparative Characterization of Complex Mixture Drugs",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027546669&doi=10.1016%2fj.xphs.2017.07.012&partnerID=40&md5=10b25ba8da2f5c495bd8d28d02f6f775","Crofelemer is a botanical polymeric proanthocyanidin that inhibits chloride channel activity and is used clinically for treating HIV-associated secretory diarrhea. Crofelemer lots may exhibit significant physicochemical variation due to the natural source of the raw material. A variety of physical, chemical, and biological assays were used to identify potential critical quality attributes (CQAs) of crofelemer, which may be useful in characterizing differently sourced and processed drug products. Crofelemer drug substance was extracted from tablets of one commercial drug product lot, fractionated, and subjected to accelerated thermal degradation studies to produce derivative lots with variations in chemical and physical composition potentially representative of manufacturing and raw material variation. Liquid chromatography, UV absorbance spectroscopy, mass spectrometry, and nuclear magnetic resonance analysis revealed substantial changes in the composition of derivative lots. A chloride channel inhibition cell-based bioassay suggested that substantial changes in crofelemer composition did not necessarily result in major changes to bioactivity. In 2 companion papers, machine learning and data mining approaches were applied to the analytical and biological data sets presented herein, along with chemical stability data sets derived from forced degradation studies, to develop an integrated mathematical model that can identify CQAs which are most relevant in distinguishing between different populations of crofelemer. © 2017 American Pharmacists Association®","analytical characterization; biopharmaceuticals; circular dichroism; complex mixture; mass spectrometry; NMR spectroscopy; polymer chemical degradation; polymeric drugs; stability; UV-Visible spectroscopy","chloride channel; crofelemer; mytesi; Article; bioassay; biological activity; chemical composition; comparative study; controlled study; data mining; degradation; derivatization; drug manufacture; drug mixture; drug structure; fractionation; liquid chromatography; machine learning; mass spectrometry; mathematical model; nuclear magnetic resonance spectroscopy; physical chemistry; tablet formulation; ultraviolet spectroscopy",2-s2.0-85027546669
"Hewarathna A., Mozziconacci O., Nariya M.K., Kleindl P.A., Xiong J., Fisher A.C., Joshi S.B., Middaugh C.R., Forrest M.L., Volkin D.B., Deeds E.J., Schöneich C.","Chemical Stability of the Botanical Drug Substance Crofelemer: A Model System for Comparative Characterization of Complex Mixture Drugs",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028352488&doi=10.1016%2fj.xphs.2017.06.022&partnerID=40&md5=af7ccbf3d4544934d07a2f9237991ecf","As the second of a 3-part series of articles in this issue concerning the development of a mathematical model for comparative characterization of complex mixture drugs using crofelemer (CF) as a model compound, this work focuses on the evaluation of the chemical stability profile of CF. CF is a biopolymer containing a mixture of proanthocyanidin oligomers which are primarily composed of gallocatechin with a small contribution from catechin. CF extracted from drug product was subjected to molecular weight–based fractionation and thiolysis. Temperature stress and metal-catalyzed oxidation were selected for accelerated and forced degradation studies. Stressed CF samples were size fractionated, thiolyzed, and analyzed with a combination of negative-ion electrospray ionization mass spectrometry (ESI-MS) and reversed-phase-HPLC with UV absorption and fluorescence detection. We further analyzed the chemical stability data sets for various CF samples generated from reversed-phase-HPLC-UV and ESI-MS using data-mining and machine learning approaches. In particular, calculations based on mutual information of over 800,000 data points in the ESI-MS analytical data set revealed specific CF cleavage and degradation products that were differentially generated under specific storage/degradation conditions, which were not initially identified using traditional analysis of the ESI-MS results. © 2017 American Pharmacists Association®","chemical stability; complex mixture; crofelemer; HPLC; machine learning; mass spectrometry; mutual information scores; oxidation","crofelemer; Article; calculation; data mining; degradation; drug analysis; drug mixture; electrospray mass spectrometry; fluorescence imaging; fractionation; light absorption; machine learning; mathematical model; negative ion electrospray; reversed phase high performance liquid chromatography; temperature stress; thermostability",2-s2.0-85028352488
"Zahnd G., Hoogendoorn A., Combaret N., Karanasos A., Péry E., Sarry L., Motreff P., Niessen W., Regar E., van Soest G., Gijsen F., van Walsum T.","Contour segmentation of the intima, media, and adventitia layers in intracoronary OCT images: application to fully automatic detection of healthy wall regions",2017,"International Journal of Computer Assisted Radiology and Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027331627&doi=10.1007%2fs11548-017-1657-7&partnerID=40&md5=4228910661e7bb2920bb1b4e12b6091b","Purpose: Quantitative and automatic analysis of intracoronary optical coherence tomography images is useful and time-saving to assess cardiovascular risk in the clinical arena. Methods: First, the interfaces of the intima, media, and adventitia layers are segmented, by means of an original front propagation scheme, running in a 4D multi-parametric space, to simultaneously extract three non-crossing contours in the initial cross-sectional image. Second, information resulting from the tentative contours is exploited by a machine learning approach to identify healthy and diseased regions of the arterial wall. The framework is fully automatic. Results: The method was applied to 40 patients from two different medical centers. The framework was trained on 140 images and validated on 260 other images. For the contour segmentation method, the average segmentation errors were 29±46μm for the intima–media interface, 30±50μm for the media–adventitia interface, and 50±64μm for the adventitia–periadventitia interface. The classification method demonstrated a good accuracy, with a median Dice coefficient equal to 0.93 and an interquartile range of (0.78–0.98). Conclusion: The proposed framework demonstrated promising offline performances and could potentially be translated into a reliable tool for various clinical applications, such as quantification of tissue layer thickness and global summarization of healthy regions in entire pullbacks. © 2017, The Author(s).","Contour segmentation; Coronary artery; Machine learning; Optical coherence tomography","adventitia; adventitia periadventitia interface; arterial wall thickness; artery wall; Article; cardiovascular parameters; cardiovascular risk; clinical article; cross-sectional study; human; image analysis; image segmentation; intima; machine learning; media adventitia interface; media layer; optical coherence tomography; priority journal; two-dimensional imaging",2-s2.0-85027331627
"Renault T.","Intraday online investor sentiment and return patterns in the U.S. stock market",2017,"Journal of Banking and Finance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025807197&doi=10.1016%2fj.jbankfin.2017.07.002&partnerID=40&md5=5b00ed6143966b08bdc0335dada43299","We implement a novel approach to derive investor sentiment from messages posted on social media before we explore the relation between online investor sentiment and intraday stock returns. Using an extensive dataset of messages posted on the microblogging platform StockTwits, we construct a lexicon of words used by online investors when they share opinions and ideas about the bullishness or the bearishness of the stock market. We demonstrate that a transparent and replicable approach significantly outperforms standard dictionary-based methods used in the literature while remaining competitive with more complex machine learning algorithms. Aggregating individual message sentiment at half-hour intervals, we provide empirical evidence that online investor sentiment helps forecast intraday stock index returns. After controlling for past market returns, we find that the first half-hour change in investor sentiment predicts the last half-hour S&P 500 index ETF return. Examining users’ self-reported investment approach, holding period and experience level, we find that the intraday sentiment effect is driven by the shift in the sentiment of novice traders. Overall, our results provide direct empirical evidence of sentiment-driven noise trading at the intraday level. © 2017 Elsevier B.V.","Asset pricing; Intraday return predictability; Investor sentiment; Machine learning; Social media; Textual analysis",,2-s2.0-85025807197
"Luo Y., Thompson W.K., Herr T.M., Zeng Z., Berendsen M.A., Jonnalagadda S.R., Carson M.B., Starren J.","Natural Language Processing for EHR-Based Pharmacovigilance: A Structured Review",2017,"Drug Safety",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021117594&doi=10.1007%2fs40264-017-0558-6&partnerID=40&md5=1c9e112cd49dc5054ee9c25a3ecb5230","The goal of pharmacovigilance is to detect, monitor, characterize and prevent adverse drug events (ADEs) with pharmaceutical products. This article is a comprehensive structured review of recent advances in applying natural language processing (NLP) to electronic health record (EHR) narratives for pharmacovigilance. We review methods of varying complexity and problem focus, summarize the current state-of-the-art in methodology advancement, discuss limitations and point out several promising future directions. The ability to accurately capture both semantic and syntactic structures in clinical narratives becomes increasingly critical to enable efficient and accurate ADE detection. Significant progress has been made in algorithm development and resource construction since 2000. Since 2012, statistical analysis and machine learning methods have gained traction in automation of ADE mining from EHR narratives. Current state-of-the-art methods for NLP-based ADE detection from EHRs show promise regarding their integration into production pharmacovigilance systems. In addition, integrating multifaceted, heterogeneous data sources has shown promise in improving ADE detection and has become increasingly adopted. On the other hand, challenges and opportunities remain across the frontier of NLP application to EHR-based pharmacovigilance, including proper characterization of ADE context, differentiation between off- and on-label drug-use ADEs, recognition of the importance of polypharmacy-induced ADEs, better integration of heterogeneous data sources, creation of shared corpora, and organization of shared-task challenges to advance the state-of-the-art. © 2017, Springer International Publishing AG.",,"drug; adverse drug reaction; drug surveillance program; electronic health record; human; information; machine learning; methodology; natural language processing; off label drug use; polypharmacy; priority journal; Review; statistical analysis; symbolism",2-s2.0-85021117594
"Abualigah L.M., Khader A.T., Hanandeh E.S., Gandomi A.H.","A novel hybridization strategy for krill herd algorithm applied to clustering techniques",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024898893&doi=10.1016%2fj.asoc.2017.06.059&partnerID=40&md5=993b0f36e6ccf6bb5ae814a24983a057","Krill herd (KH) is a stochastic nature-inspired optimization algorithm that has been successfully used to solve numerous complex optimization problems. This paper proposed a novel hybrid of KH algorithm with harmony search (HS) algorithm, namely, H-KHA, to improve the global (diversification) search ability. The enhancement includes adding global search operator (improvise a new solution) of the HS algorithm to the KH algorithm for improving the exploration search ability by a new probability factor, namely, Distance factor, thereby moving krill individuals toward the best global solution. The effectiveness of the proposed H-KHA is tested on seven standard datasets from the UCI Machine Learning Repository that are commonly used in the domain of data clustering, also six common text datasets that are used in the domain of text document clustering. The experiments reveal that the proposed hybrid KHA with HS algorithm (H-KHA) enhanced the results in terms of accurate clusters and high convergence rate. Mostly, the performance of H-KHA is superior or at least highly competitive with the original KH algorithm, well-known clustering techniques and other comparative optimization algorithms. © 2017 Elsevier B.V.","Data clustering; Global exploration; Hybridization; Krill herd algorithm; Text clustering","Cluster analysis; Optimization; Stochastic systems; Text processing; Complex optimization problems; Data clustering; Global exploration; Hybridization; Optimization algorithms; Text Clustering; Text Document Clustering; UCI machine learning repository; Clustering algorithms",2-s2.0-85024898893
"Becker R.A., Dreier D.A., Manibusan M.K., Cox L.A.T., Simon T.W., Bus J.S.","How well can carcinogenicity be predicted by high throughput “characteristics of carcinogens” mechanistic data?",2017,"Regulatory Toxicology and Pharmacology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014955801&doi=10.1016%2fj.yrtph.2017.08.021&partnerID=40&md5=c28388b655bba39184f2381fc24af1d2","IARC has begun using ToxCast/Tox21 data in efforts to represent key characteristics of carcinogens to organize and weigh mechanistic evidence in cancer hazard determinations and this implicit inference approach also is being considered by USEPA. To determine how well ToxCast/Tox21 data can explicitly predict cancer hazard, this approach was evaluated with statistical analyses and machine learning prediction algorithms. Substances USEPA previously classified as having cancer hazard potential were designated as positives and substances not posing a carcinogenic hazard were designated as negatives. Then ToxCast/Tox21 data were analyzed both with and without adjusting for the cytotoxicity burst effect commonly observed in such assays. Using the same assignments as IARC of ToxCast/Tox21 assays to the seven key characteristics of carcinogens, the ability to predict cancer hazard for each key characteristic, alone or in combination, was found to be no better than chance. Hence, we have little scientific confidence in IARC's inference models derived from current ToxCast/Tox21 assays for key characteristics to predict cancer. This finding supports the need for a more rigorous mode-of-action pathway-based framework to organize, evaluate, and integrate mechanistic evidence with animal toxicity, epidemiological investigations, and knowledge of exposure and dosimetry to evaluate potential carcinogenic hazards and risks to humans. © 2017 The Authors","Cancer hazard classification; Cancer prediction modeling; High throughput screening (HTS); IARC","carcinogen; Article; carcinogenicity; cytotoxicity; data analysis; dosimetry; environmental exposure; epidemiological data; hazard assessment; human; machine learning; prediction; priority journal; risk assessment; statistical analysis; toxicity testing",2-s2.0-85014955801
"Afridi M.J., Ross A., Liu X., Bennewitz M.F., Shuboni D.D., Shapiro E.M.","Intelligent and automatic in vivo detection and quantification of transplanted cells in MRI",2017,"Magnetic Resonance in Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029451479&doi=10.1002%2fmrm.26571&partnerID=40&md5=98a02645374a58ada4c2990be55104fb","Purpose: Magnetic resonance imaging (MRI)-based cell tracking has emerged as a useful tool for identifying the location of transplanted cells, and even their migration. Magnetically labeled cells appear as dark contrast in T2*-weighted MRI, with sensitivities of individual cells. One key hurdle to the widespread use of MRI-based cell tracking is the inability to determine the number of transplanted cells based on this contrast feature. In the case of single cell detection, manual enumeration of spots in three-dimensional (3D) MRI in principle is possible; however, it is a tedious and time-consuming task that is prone to subjectivity and inaccuracy on a large scale. This research presents the first comprehensive study on how a computer-based intelligent, automatic, and accurate cell quantification approach can be designed for spot detection in MRI scans. Methods: Magnetically labeled mesenchymal stem cells (MSCs) were transplanted into rats using an intracardiac injection, accomplishing single cell seeding in the brain. T2*-weighted MRI of these rat brains were performed where labeled MSCs appeared as spots. Using machine learning and computer vision paradigms, approaches were designed to systematically explore the possibility of automatic detection of these spots in MRI. Experiments were validated against known in vitro scenarios. Results: Using the proposed deep convolutional neural network (CNN) architecture, an in vivo accuracy up to 97.3% and in vitro accuracy of up to 99.8% was achieved for automated spot detection in MRI data. Conclusion: The proposed approach for automatic quantification of MRI-based cell tracking will facilitate the use of MRI in large-scale cell therapy studies. Magn Reson Med 78:1991–2002, 2017. © 2016 International Society for Magnetic Resonance in Medicine. © 2016 International Society for Magnetic Resonance in Medicine","cell therapy; iron oxide; Machine learning; MRI",,2-s2.0-85029451479
"Onorati F., Regalia G., Caborni C., Migliorini M., Bender D., Poh M.-Z., Frazier C., Kovitch Thropp E., Mynatt E.D., Bidwell J., Mai R., LaFrance W.C., Jr., Blum A.S., Friedman D., Loddenkemper T., Mohammadpour-Touserkani F., Reinsberger C., Tognetti S., Picard R.W.","Multicenter clinical assessment of improved wearable multimodal convulsive seizure detectors",2017,"Epilepsia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731602&doi=10.1111%2fepi.13899&partnerID=40&md5=9d19713ee9335ae5e5e87553caa23424","Objective: New devices are needed for monitoring seizures, especially those associated with sudden unexpected death in epilepsy (SUDEP). They must be unobtrusive and automated, and provide false alarm rates (FARs) bearable in everyday life. This study quantifies the performance of new multimodal wrist-worn convulsive seizure detectors. Methods: Hand-annotated video-electroencephalographic seizure events were collected from 69 patients at six clinical sites. Three different wristbands were used to record electrodermal activity (EDA) and accelerometer (ACM) signals, obtaining 5,928 h of data, including 55 convulsive epileptic seizures (six focal tonic–clonic seizures and 49 focal to bilateral tonic–clonic seizures) from 22 patients. Recordings were analyzed offline to train and test two new machine learning classifiers and a published classifier based on EDA and ACM. Moreover, wristband data were analyzed to estimate seizure-motion duration and autonomic responses. Results: The two novel classifiers consistently outperformed the previous detector. The most efficient (Classifier III) yielded sensitivity of 94.55%, and an FAR of 0.2 events/day. No nocturnal seizures were missed. Most patients had <1 false alarm every 4 days, with an FAR below their seizure frequency. When increasing the sensitivity to 100% (no missed seizures), the FAR is up to 13 times lower than with the previous detector. Furthermore, all detections occurred before the seizure ended, providing reasonable latency (median = 29.3 s, range = 14.8–151 s). Automatically estimated seizure durations were correlated with true durations, enabling reliable annotations. Finally, EDA measurements confirmed the presence of postictal autonomic dysfunction, exhibiting a significant rise in 73% of the convulsive seizures. Significance: The proposed multimodal wrist-worn convulsive seizure detectors provide seizure counts that are more accurate than previous automated detectors and typical patient self-reports, while maintaining a tolerable FAR for ambulatory monitoring. Furthermore, the multimodal system provides an objective description of motor behavior and autonomic dysfunction, aimed at enriching seizure characterization, with potential utility for SUDEP warning. Wiley Periodicals, Inc. © 2017 International League Against Epilepsy.","Convulsive seizures; Electrodermal activity; Epilepsy; Machine learning",,2-s2.0-85032731602
"Dipnall J.F., Pasco J.A., Berk M., Williams L.J., Dodd S., Jacka F.N., Meyer D.","Getting RID of the blues: Formulating a Risk Index for Depression (RID) using structural equation modeling",2017,"Australian and New Zealand Journal of Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032749500&doi=10.1177%2f0004867417726860&partnerID=40&md5=865d0611d6cad3e1d220d0bfaeceb4ed","Objective: While risk factors for depression are increasingly known, there is no widely utilised depression risk index. Our objective was to develop a method for a flexible, modular, Risk Index for Depression using structural equation models of key determinants identified from previous published research that blended machine-learning with traditional statistical techniques. Methods: Demographic, clinical and laboratory variables from the National Health and Nutrition Examination Study (2009–2010, N = 5546) were utilised. Data were split 50:50 into training:validation datasets. Generalised structural equation models, using logistic regression, were developed with a binary outcome depression measure (Patient Health Questionnaire-9 score ⩾ 10) and previously identified determinants of depression: demographics, lifestyle-environs, diet, biomarkers and somatic symptoms. Indicative goodness-of-fit statistics and Areas Under the Receiver Operator Characteristic Curves were calculated and probit regression checked model consistency. Results: The generalised structural equation model was built from a systematic process. Relative importance of the depression determinants were diet (odds ratio: 4.09; 95% confidence interval: [2.01, 8.35]), lifestyle-environs (odds ratio: 2.15; 95% CI: [1.57, 2.94]), somatic symptoms (odds ratio: 2.10; 95% CI: [1.58, 2.80]), demographics (odds ratio:1.46; 95% CI: [0.72, 2.95]) and biomarkers (odds ratio:1.39; 95% CI: [1.00, 1.93]). The relationships between demographics and lifestyle-environs and depression indicated a potential indirect path via somatic symptoms and biomarkers. The path from diet was direct to depression. The Areas under the Receiver Operator Characteristic Curves were good (logistic:training = 0.850, validation = 0.813; probit:training = 0.849, validation = 0.809). Conclusion: The novel Risk Index for Depression modular methodology developed has the flexibility to add/remove direct/indirect risk determinants paths to depression using a structural equation model on datasets that take account of a wide range of known risks. Risk Index for Depression shows promise for future clinical use by providing indications of main determinant(s) associated with a patient’s predisposition to depression and has the ability to be translated for the development of risk indices for other affective disorders. © 2017, © The Royal Australian and New Zealand College of Psychiatrists 2017.","Depression; index; machine-learning; psychiatry; structural equation modelling",,2-s2.0-85032749500
"Swager A.-F., van der Sommen F., Klomp S.R., Zinger S., Meijer S.L., Schoon E.J., Bergman J.J.G.H.M., de With P.H., Curvers W.L.","Computer-aided detection of early Barrett's neoplasia using volumetric laser endomicroscopy",2017,"Gastrointestinal Endoscopy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018983661&doi=10.1016%2fj.gie.2017.03.011&partnerID=40&md5=1782b15a580da931c3f28921b4670299","Background and Aims Volumetric laser endomicroscopy (VLE) is an advanced imaging system that provides a near-microscopic resolution scan of the esophageal wall layers up to 3-mm deep. VLE has the potential to improve detection of early neoplasia in Barrett's esophagus (BE). However, interpretation of VLE images is complex because of the large amount of data that need to be interpreted in real time. The aim of this study was to investigate the feasibility of a computer algorithm to identify early BE neoplasia on ex vivo VLE images. Methods We used 60 VLE images from a database of high-quality ex vivo VLE-histology correlations, obtained from BE patients ± neoplasia (30 nondysplastic BE [NDBE] and 30 high-grade dysplasia/early adenocarcinoma images). VLE features from a recently developed clinical VLE prediction score for BE neoplasia served as input for the algorithm: (1) higher VLE surface than subsurface signal and (2) lack of layering. With this input, novel clinically inspired algorithm features were developed, based on signal intensity statistics and grayscale correlations. For comparison, generic image analysis methods were examined for their performance to detect neoplasia. For classification of the images in the NDBE or neoplastic group, several machine learning methods were evaluated. Leave-1-out cross-validation was used for algorithm validation. Results Three novel clinically inspired algorithm features were developed. The feature “layering and signal decay statistics” showed the optimal performance compared with the other clinically features (“layering” and “signal intensity distribution”) and generic image analyses methods, with an area under the receiver operating characteristic curve (AUC) of.95. Corresponding sensitivity and specificity were 90% and 93%, respectively. In addition, the algorithm showed a better performance than the clinical VLE prediction score (AUC.81). Conclusions This is the first study in which a computer algorithm for BE neoplasia was developed based on VLE images with direct histologic correlates. The algorithm showed good performance to detect BE neoplasia in ex vivo VLE images compared with the performance of a recently developed clinical VLE prediction score. This study suggests that an automatic detection algorithm has the potential to assist endoscopists in detecting early neoplasia on VLE. Future studies on in vivo VLE scans are needed to further validate the algorithm. © 2017 American Society for Gastrointestinal Endoscopy",,"aged; algorithm; Article; Barrett esophagus; computer assisted diagnosis; diagnostic test accuracy study; ex vivo study; feasibility study; female; gastrointestinal endoscopy; histology; human; human tissue; image analysis; intermethod comparison; machine learning; major clinical study; male; priority journal; sensitivity and specificity; volumetric laser endomicroscopy",2-s2.0-85018983661
"Naue J., Hoefsloot H.C.J., Mook O.R.F., Rijlaarsdam-Hoekstra L., van der Zwalm M.C.H., Henneman P., Kloosterman A.D., Verschure P.J.","Chronological age prediction based on DNA methylation: Massive parallel sequencing and random forest regression",2017,"Forensic Science International: Genetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027892453&doi=10.1016%2fj.fsigen.2017.07.015&partnerID=40&md5=1ad16a20feb2bde8e99decfa29f3ec5c","The use of DNA methylation (DNAm) to obtain additional information in forensic investigations showed to be a promising and increasing field of interest. Prediction of the chronological age based on age-dependent changes in the DNAm of specific CpG sites within the genome is one such potential application. Here we present an age-prediction tool for whole blood based on massive parallel sequencing (MPS) and a random forest machine learning algorithm. MPS allows accurate DNAm determination of pre-selected markers and neighboring CpG-sites to identify the best age-predictive markers for the age-prediction tool. 15 age-dependent markers of different loci were initially chosen based on publicly available 450K microarray data, and 13 finally selected for the age tool based on MPS (DDO, ELOVL2, F5, GRM2, HOXC4, KLF14, LDB2, MEIS1-AS3, NKIRAS2, RPA2, SAMD10, TRIM59, ZYG11A). Whole blood samples of 208 individuals were used for training of the algorithm and a further 104 individuals were used for model evaluation (age 18–69). In the case of KLF14, LDB2, SAMD10, and GRM2, neighboring CpG sites and not the initial 450K sites were chosen for the final model. Cross-validation of the training set leads to a mean absolute deviation (MAD) of 3.21 years and a root-mean square error (RMSE) of 3.97 years. Evaluation of model performance using the test set showed a comparable result (MAD 3.16 years, RMSE 3.93 years). A reduced model based on only the top 4 markers (ELOVL2, F5, KLF14, and TRIM59) resulted in a RMSE of 4.19 years and MAD of 3.24 years for the test set (cross validation training set: RMSE 4.63 years, MAD 3.64 years). The amplified region was additionally investigated for occurrence of SNPs in case of an aberrant DNAm result, which in some cases can be an indication for a deviation in DNAm. Our approach uncovered well-known DNAm age-dependent markers, as well as additional new age-dependent sites for improvement of the model, and allowed the creation of a reliable and accurate epigenetic tool for age-prediction without restriction to a linear change in DNAm with age. © 2017","Age prediction; DNA methylation; Machine learning; Massive parallel sequencing",,2-s2.0-85027892453
"Latourelle J.C., Beste M.T., Hadzi T.C., Miller R.E., Oppenheim J.N., Valko M.P., Wuest D.M., Church B.W., Khalil I.G., Hayete B., Venuto C.S.","Large-scale identification of clinical and genetic predictors of motor progression in patients with newly diagnosed Parkinson's disease: a longitudinal cohort study and validation",2017,"The Lancet Neurology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029904077&doi=10.1016%2fS1474-4422%2817%2930328-9&partnerID=40&md5=1c8f2c85f24f440223346ec42887a62d","Background Better understanding and prediction of progression of Parkinson's disease could improve disease management and clinical trial design. We aimed to use longitudinal clinical, molecular, and genetic data to develop predictive models, compare potential biomarkers, and identify novel predictors for motor progression in Parkinson's disease. We also sought to assess the use of these models in the design of treatment trials in Parkinson's disease. Methods A Bayesian multivariate predictive inference platform was applied to data from the Parkinson's Progression Markers Initiative (PPMI) study (NCT01141023). We used genetic data and baseline molecular and clinical variables from patients with Parkinson's disease and healthy controls to construct an ensemble of models to predict the annual rate of change in combined scores from the Movement Disorder Society—Unified Parkinson's Disease Rating Scale (MDS-UPDRS) parts II and III. We tested our overall explanatory power, as assessed by the coefficient of determination (R2), and replicated novel findings in an independent clinical cohort from the Longitudinal and Biomarker Study in Parkinson's disease (LABS-PD; NCT00605163). The potential utility of these models for clinical trial design was quantified by comparing simulated randomised placebo-controlled trials within the out-of-sample LABS-PD cohort. Findings 117 healthy controls and 312 patients with Parkinson's disease from the PPMI study were available for analysis, and 317 patients with Parkinson's disease from LABS-PD were available for validation. Our model ensemble showed strong performance within the PPMI cohort (five-fold cross-validated R2 41%, 95% CI 35–47) and significant—albeit reduced—performance in the LABS-PD cohort (R2 9%, 95% CI 4–16). Individual predictive features identified from PPMI data were confirmed in the LABS-PD cohort. These included significant replication of higher baseline MDS-UPDRS motor score, male sex, and increased age, as well as a novel Parkinson's disease-specific epistatic interaction, all indicative of faster motor progression. Genetic variation was the most useful predictive marker of motor progression (2·9%, 95% CI 1·5–4·3). CSF biomarkers at baseline showed a more modest (0·3%, 95% CI 0·1–0·5) but still significant effect on prediction of motor progression. The simulations (n=5000) showed that incorporating the predicted rates of motor progression (as assessed by the annual change in MDS-UPDRS score) into the final models of treatment effect reduced the variability in the study outcome, allowing significant differences to be detected at sample sizes up to 20% smaller than in naive trials. Interpretation Our model ensemble confirmed established and identified novel predictors of Parkinson's disease motor progression. Improvement of existing prognostic models through machine-learning approaches should benefit trial design and evaluation, as well as clinical disease monitoring and treatment. Funding Michael J Fox Foundation for Parkinson's Research and National Institute of Neurological Disorders and Stroke. © 2017 Elsevier Ltd",,"alpha synuclein; biological marker; placebo; Article; clinical feature; cohort analysis; controlled study; disease course; disease simulation; female; follow up; gene linkage disequilibrium; genetic association; genetic identification; genetic variability; genetic variation; genotype; human; longitudinal study; machine learning; major clinical study; male; motor dysfunction; Parkinson disease; prediction; predictor variable; priority journal; protein cerebrospinal fluid level; randomized controlled trial; sex difference; Unified Parkinson Disease Rating Scale; validation study; genetics; Parkinson disease; pathophysiology; Cohort Studies; Female; Humans; Male; Parkinson Disease",2-s2.0-85029904077
"Usha M., Kavitha P.","Anomaly based intrusion detection for 802.11 networks with optimal features using SVM classifier",2017,"Wireless Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969850606&doi=10.1007%2fs11276-016-1300-5&partnerID=40&md5=e228ff8e3f9f1ce14f1a8f2844b132b4","The Wireless Fidelity (WiFi) is a widely used wireless technology due to its flexibility and mobility in the presence of vulnerable security features. Several attempts to secure 802.11 standard ends up with the inadequate security mechanisms that are vulnerable to various attacks and intrusions. Thus, integration of external defense mechanism like intrusion detection system (IDS) is inevitable. An anomaly-based IDS employs machine learning algorithms to detect attacks. Selecting the best set of features is central to ensure the performance of the classifier in terms of speed of learning, accuracy, and reliability. This paper proposes a normalized gain based IDS for MAC Intrusions (NMI) to improve the IDS performance significantly. The proposed NMI includes two primary components OFSNP and DCMI. The first component is optimal feature selection using NG and PSO (OFSNP) and the second component is Detecting and Categorizing MAC 802.11 Intrusions (DCMI) using SVM classifier. The OFSNP ranks the features using an independent measure as normalized gain (NG) and selects the optimal set of features using semi-supervised clustering (SSC). The SSC is based on particle swarm optimization (PSO) that uses labeled and unlabeled features simultaneously to find a group of optimal features. Using the optimal set of features, the proposed DCMI utilizes a rapid and straightforward support vector machine (SVM) learning that classifies the attacks under the appropriate classes. Thus, the proposed NMI achieves a better trade-off between detection accuracy and learning time. The experimental results show that the NMI accurately detects and classifies the 802.11 specific intrusions and also, it reduces the false positives and computation complexity by decreasing the number of features. © 2016, Springer Science+Business Media New York.","Intrusion detection system; MAC 802.11; Normalized gain (NG); Particle swarm optimization (PSO); Support vector machine (SVM) classifier; WLAN","Artificial intelligence; Classification (of information); Clustering algorithms; Complex networks; Computer crime; Economic and social effects; Intrusion detection; Learning algorithms; Learning systems; Mercury (metal); Particle swarm optimization (PSO); Support vector machines; Wi-Fi; Wireless telecommunication systems; Anomaly-based intrusion detection; Computation complexity; Intrusion Detection Systems; MAC 802.11; Normalized gains; Optimal feature selections; Semi-supervised Clustering; WLAN; Feature extraction",2-s2.0-84969850606
"Goh Y.M., Ubeynarayana C.U.","Construction accident narrative classification: An evaluation of text mining techniques",2017,"Accident Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028564740&doi=10.1016%2fj.aap.2017.08.026&partnerID=40&md5=b29a7695083edc5aca0e7bc5e4c9d753","Learning from past accidents is fundamental to accident prevention. Thus, accident and near miss reporting are encouraged by organizations and regulators. However, for organizations managing large safety databases, the time taken to accurately classify accident and near miss narratives will be very significant. This study aims to evaluate the utility of various text mining classification techniques in classifying 1000 publicly available construction accident narratives obtained from the US OSHA website. The study evaluated six machine learning algorithms, including support vector machine (SVM), linear regression (LR), random forest (RF), k-nearest neighbor (KNN), decision tree (DT) and Naive Bayes (NB), and found that SVM produced the best performance in classifying the test set of 251 cases. Further experimentation with tokenization of the processed text and non-linear SVM were also conducted. In addition, a grid search was conducted on the hyperparameters of the SVM models. It was found that the best performing classifiers were linear SVM with unigram tokenization and radial basis function (RBF) SVM with uni-gram tokenization. In view of its relative simplicity, the linear SVM is recommended. Across the 11 labels of accident causes or types, the precision of the linear SVM ranged from 0.5 to 1, recall ranged from 0.36 to 0.9 and F1 score was between 0.45 and 0.92. The reasons for misclassification were discussed and suggestions on ways to improve the performance were provided. © 2017 Elsevier Ltd","Accident classification; Construction safety; Data mining; Support vector machine; Text mining","Accidents; Classification (of information); Decision trees; Learning algorithms; Learning systems; Nearest neighbor search; Radial basis function networks; Support vector machines; Text processing; Construction accidents; Construction safety; Hyperparameters; K nearest neighbor (KNN); Misclassifications; Radial Basis Function(RBF); Text mining; Text mining techniques; Data mining",2-s2.0-85028564740
"Nasirahmadi A., Edwards S.A., Matheson S.M., Sturm B.","Using automated image analysis in pig behavioural research: Assessment of the influence of enrichment substrate provision on lying behaviour",2017,"Applied Animal Behaviour Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021640756&doi=10.1016%2fj.applanim.2017.06.015&partnerID=40&md5=1fa1b6e4821a7ce41c95d56522c33bc2","Visual monitoring of pig behaviours over long periods is very time consuming and has possibility for observer bias. Automated image processing techniques now give the potential to carry out behavioural research in a more effective way. To illustrate this, an image processing technique was applied to identify whether any changes in pig lying behaviour which might be detrimental to welfare resulted from an enrichment provision treatment. The lying patterns of pigs in 6 enriched pens were compared with those of 6 control pens, which had only a suspended enrichment toy, to determine whether daily provision of a rooting material (maize silage) onto a solid plate in the lying area of a fully slatted pen resulted in changed lying time and location. Pigs were monitored by top view CCTV cameras and animals were extracted from their background using image processing algorithms. An ellipse fitting technique was applied to localize each pig and the centre of each fitted ellipse was used in x–y coordinates to find the lying positions after use of an algorithm to remove images in motion preceding the scan. Each pen was virtually subdivided into four zones and the position of each lying pig obtained at 10 min intervals over a series of 24 h periods. Results of a validation study showed that the image processing technique had an accuracy of 93–95% when compared to visual scoring. Results from image processing indicated that once daily provision of rooting material significantly changed the diurnal activity pattern (p < 0.001) and resulted in a modified diurnal pattern of resting location. The study demonstrates that machine vision can be used as a precise and rapid method for quantifying pig lying behaviour for research or practical applications. © 2017 Elsevier B.V.","Enrichment material; Image processing; Lying behaviour; Pig","activity pattern; algorithm; automation; behavioral response; diurnal activity; image analysis; image processing; machine learning; pig; Animalia; Suidae; Zea mays",2-s2.0-85021640756
"Vivanti R., Szeskin A., Lev-Cohain N., Sosna J., Joskowicz L.","Automatic detection of new tumors and tumor burden evaluation in longitudinal liver CT scan studies",2017,"International Journal of Computer Assisted Radiology and Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028616371&doi=10.1007%2fs11548-017-1660-z&partnerID=40&md5=6532bbd203032cd29a707cbe5f630377","Purpose: Radiological longitudinal follow-up of liver tumors in CT scans is the standard of care for disease progression assessment and for liver tumor therapy. Finding new tumors in the follow-up scan is essential to determine malignancy, to evaluate the total tumor burden, and to determine treatment efficacy. Since new tumors are typically small, they may be missed by examining radiologists. Methods: We describe a new method for the automatic detection and segmentation of new tumors in longitudinal liver CT studies and for liver tumors burden quantification. Its inputs are the baseline and follow-up CT scans, the baseline tumors delineation, and a tumor appearance prior model. Its outputs are the new tumors segmentations in the follow-up scan, the tumor burden quantification in both scans, and the tumor burden change. Our method is the first comprehensive method that is explicitly designed to find new liver tumors. It integrates information from the scans, the baseline known tumors delineations, and a tumor appearance prior model in the form of a global convolutional neural network classifier. Unlike other deep learning-based methods, it does not require large tagged training sets. Results: Our experimental results on 246 tumors, of which 97 were new tumors, from 37 longitudinal liver CT studies with radiologist approved ground-truth segmentations, yields a true positive new tumors detection rate of 86 versus 72% with stand-alone detection, and a tumor burden volume overlap error of 16%. Conclusions: New tumors detection and tumor burden volumetry are important for diagnosis and treatment. Our new method enables a simplified radiologist-friendly workflow that is potentially more accurate and reliable than the existing one by automatically and accurately following known tumors and detecting new tumors in the follow-up scan. © 2017, CARS.","Liver tumors detection; Liver tumors segmentation; Longitudinal CT study; Radiological assessment","contrast medium; Article; cancer diagnosis; follow up; human; image processing; image segmentation; liver parenchyma; liver tumor; machine learning; priority journal; tumor volume; volumetry; x-ray computed tomography",2-s2.0-85028616371
"Yoo J., Johansson K.H., Jin Kim H.","Indoor Localization Without a Prior Map by Trajectory Learning from Crowdsourced Measurements",2017,"IEEE Transactions on Instrumentation and Measurement",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028996599&doi=10.1109%2fTIM.2017.2729438&partnerID=40&md5=3ece6d7b2c1c1bbc0fa49fa59c7c3098","Accommodation of a situation when a prior map is not available in an indoor localization system is valuable to cost-effective operations by removing a need for map drawing and map updating. This paper suggests a trajectory learning method using crowdsourced measurements in order to support the absence of map. A localization framework based on a particle filter is formalized by machine-learning-based feature extraction and Gaussian process (GP) regression. The feature extraction algorithm reduces dimensionality of sparse measurement vector, and it is applied to detect floor level and designated landmarks. Also, the combination of the feature extraction and the GP regression is used for modeling nonlinear relationship between location and measurement. By this combination, locations of Wi-Fi access points are not required to be known. From the field experimental results, we confirm that the detections of floor level and landmarks are accurate, the learned trajectories are close to the true map, and positioning accuracy is improved thanks to the learning-aided localization. © 1963-2012 IEEE.","Crowdsourced measurement; feature extraction; Gaussian process (GP) regression; indoor localization; trajectory learning","Cost effectiveness; Extraction; Feature extraction; Floors; Gaussian distribution; Gaussian noise (electronic); Heuristic algorithms; Learning systems; Principal component analysis; Regression analysis; Trajectories; Atmospheric measurement; Gaussian process; Indoor localization; Training data; Wireless fidelities; Indoor positioning systems",2-s2.0-85028996599
"Kerr W.T., Janio E.A., Braesch C.T., Le J.M., Hori J.M., Patel A.B., Gallardo N.L., Bauirjan J., D'Ambrosio S.R., Chau A.M., Hwang E.S., Davis E.C., Buchard A., Torres-Barba D., Al Banna M., Barritt S.E., Cho A.Y., Engel J., Jr., Cohen M.S., Stern J.M.","Identifying psychogenic seizures through comorbidities and medication history",2017,"Epilepsia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029375356&doi=10.1111%2fepi.13888&partnerID=40&md5=08e30b30ae32644de9842e99cfaa3240","Objective: Low-cost evidence-based tools are needed to facilitate the early identification of patients with possible psychogenic nonepileptic seizures (PNES). Prior to accurate diagnosis, patients with PNES do not receive interventions that address the cause of their seizures and therefore incur high medical costs and disability due to an uncontrolled seizure disorder. Both seizures and comorbidities may contribute to this high cost. Methods: Based on data from 1,365 adult patients with video-electroencephalography–confirmed diagnoses from a single center, we used logistic and Poisson regression to compare the total number of comorbidities, number of medications, and presence of specific comorbidities in five mutually exclusive groups of diagnoses: epileptic seizures (ES) only, PNES only, mixed PNES and ES, physiologic nonepileptic seizurelike events, and inconclusive monitoring. To determine the diagnostic utility of comorbid diagnoses and medication history to differentiate PNES only from ES only, we used multivariate logistic regression, controlling for sex and age, trained using a retrospective database and validated using a prospective database. Results: Our model differentiated PNES only from ES only with a prospective accuracy of 78% (95% confidence interval =72–84%) and area under the curve of 79%. With a few exceptions, the number of comorbidities and medications was more predictive than a specific comorbidity. Comorbidities associated with PNES were asthma, chronic pain, and migraines (p < 0.01). Comorbidities associated with ES were diabetes mellitus and nonmetastatic neoplasm (p < 0.01). The population-level analysis suggested that patients with mixed PNES and ES may be a population distinct from patients with either condition alone. Significance: An accurate patient-reported medical history and medication history can be useful when screening for possible PNES. Our prospectively validated and objective score may assist in the interpretation of the medication and medical history in the context of the seizure description and history. Wiley Periodicals, Inc. © 2017 International League Against Epilepsy.","Asthma; Machine learning; Migraines; Psychogenic nonepileptic attack disorder; Screening",,2-s2.0-85029375356
"Ma L., Ma F., Li J., Gu Q., Yang S., Wu D., Feng J., Ding J.","Characterizing and modeling regional-scale variations in soil salinity in the arid oasis of Tarim Basin, China",2017,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019658720&doi=10.1016%2fj.geoderma.2017.05.016&partnerID=40&md5=cfae101fd391d8da1511ccf09b8deced","Soil spatial variations are scale dependent and can be controlled by many environmental factors. Numerous factors have been related to variations of soil salinity, some of them combined empirical mode decomposition (EMD) method and correlation analysis. However, environmental factors that essentially affect soil-water balance are not given enough attention. In addition, further analysis is needed in exploring how well the environmental factors can interpret the variations in soil salinity at different scales, especially in arid oasis areas and at large scales. This paper explores the potential of modeling variations in soil salinity via the EMD and Random Forest modeling of remote sensing based environmental factors. A case study is presented for Tarim basin, Xinjiang, China, using land surface temperature (LST), evapotranspiration (ET), TRMM precipitation (TRM) and digital elevation model (DEM) products. Soil salinity and its decompositions were first correlated with environmental factors for feature selection. Then, those selected environmental factors and their decompositions were correlated and coupled with their counterparts of soil salinity to evaluate their synchronization. Finally, those IMF components of environmental factors that had high correlation coefficients and were coupled well with corresponding IMF components of soil salinity were identified and divided into different feature sets for modeling. Mean absolute error and mean bias error were adopted for accuracy assessment of the models. Our results indicate that soil salinity series can be separated into eight scales ranging from 170 km to 480 km. IMF components 5–7 account for most of the variation and can be modeled using the corresponding IMF components of different combinations of DEM, ET, LST and TRM. IMF components 6–7 are well coupled with LST and ET at approximately 475 km scale. Overall, regional-scale modeling of variations in soil salinity based on remote sensing products is possible. Reasonably accurate results can be obtained in arid oasis areas where researchers and policy makers must focus on preventing the loss of agricultural productivity and ecosystem health. © 2017 Elsevier B.V.","Correlation; Coupling; Empirical mode decomposition; Random Forest; Remote sensing","Atmospheric temperature; Correlation methods; Couplings; Decision trees; Decomposition; Ecology; Ecosystems; Factor analysis; Productivity; Remote sensing; Signal processing; Soil moisture; Surveying; Agricultural productivity; Correlation coefficient; Digital elevation model; Empirical Mode Decomposition; Environmental factors; Land surface temperature; Random forest modeling; Random forests; Soils; arid environment; correlation; decomposition analysis; empirical analysis; environmental factor; machine learning; numerical model; oasis; remote sensing; salinization; soil water; spatial variation; China; Tarim Basin; Xinjiang Uygur",2-s2.0-85019658720
"Pang G., Perdikaris P., Cai W., Karniadakis G.E.","Discovering variable fractional orders of advection–dispersion equations from field data using multi-fidelity Bayesian optimization",2017,"Journal of Computational Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027849899&doi=10.1016%2fj.jcp.2017.07.052&partnerID=40&md5=897b8047d37170401d22fbc577af9599","The fractional advection–dispersion equation (FADE) can describe accurately the solute transport in groundwater but its fractional order has to be determined a priori. Here, we employ multi-fidelity Bayesian optimization to obtain the fractional order under various conditions, and we obtain more accurate results compared to previously published data. Moreover, the present method is very efficient as we use different levels of resolution to construct a stochastic surrogate model and quantify its uncertainty. We consider two different problem set ups. In the first set up, we obtain variable fractional orders of one-dimensional FADE, considering both synthetic and field data. In the second set up, we identify constant fractional orders of two-dimensional FADE using synthetic data. We employ multi-resolution simulations using two-level and three-level Gaussian process regression models to construct the surrogates. © 2017 Elsevier Inc.","Fractional modeling; Gaussian process regression; Inverse problem; Machine learning; Model uncertainty; Porous media; Uncertainty quantification",,2-s2.0-85027849899
"Echigoya Y., Lim K.R.Q., Trieu N., Bao B., Miskew Nichols B., Vila M.C., Novak J.S., Hara Y., Lee J., Touznik A., Mamchaoui K., Aoki Y., Takeda S., Nagaraju K., Mouly V., Maruyama R., Duddy W., Yokota T.","Quantitative Antisense Screening and Optimization for Exon 51 Skipping in Duchenne Muscular Dystrophy",2017,"Molecular Therapy",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028502281&doi=10.1016%2fj.ymthe.2017.07.014&partnerID=40&md5=3417a48af18cbca1b145b0f79e06e750","Duchenne muscular dystrophy (DMD), the most common lethal genetic disorder, is caused by mutations in the dystrophin (DMD) gene. Exon skipping is a therapeutic approach that uses antisense oligonucleotides (AOs) to modulate splicing and restore the reading frame, leading to truncated, yet functional protein expression. In 2016, the US Food and Drug Administration (FDA) conditionally approved the first phosphorodiamidate morpholino oligomer (morpholino)-based AO drug, eteplirsen, developed for DMD exon 51 skipping. Eteplirsen remains controversial with insufficient evidence of its therapeutic effect in patients. We recently developed an in silico tool to design antisense morpholino sequences for exon skipping. Here, we designed morpholino AOs targeting DMD exon 51 using the in silico tool and quantitatively evaluated the effects in immortalized DMD muscle cells in vitro. To our surprise, most of the newly designed morpholinos induced exon 51 skipping more efficiently compared with the eteplirsen sequence. The efficacy of exon 51 skipping and rescue of dystrophin protein expression were increased by up to more than 12-fold and 7-fold, respectively, compared with the eteplirsen sequence. Significant in vivo efficacy of the most effective morpholino, determined in vitro, was confirmed in mice carrying the human DMD gene. These findings underscore the importance of AO sequence optimization for exon skipping. Exon skipping is a promising approach to treating Duchenne muscular dystrophy; however, the clinical benefit of treatment has not been demonstrated, and the recent FDA approval remains controversial. Echigoya et al. develop a screening method to identify efficacious oligonucleotides, showing that exon skipping efficacy dramatically improves with sequence optimization. © 2017 The American Society of Gene and Cell Therapy","antisense morpholino; Becker muscular dystrophy; BMD; clinical trial candidate screening; drisapersen; Duchenne muscular dystrophy; eteplirsen; exon skipping; Exondys 51; hDMD/Dmd-null mice; machine learning; mdx52 mice",,2-s2.0-85028502281
"Pillai J.J.","Functional Connectivity",2017,"Neuroimaging Clinics of North America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028864664&doi=10.1016%2fj.nic.2017.08.001&partnerID=40&md5=1fb500d14347f1985974f8545ae392a5",[No abstract available],,"BOLD signal; brain function; brain mapping; brain tumor; degenerative disease; Editorial; epilepsy; frequency domain analysis; functional connectivity; functional magnetic resonance imaging; graph theoretic analysis; human; independent component analysis; language; machine learning; medical specialist; mental disease; neuroimaging; personalized medicine; priority journal; sensorimotor function; statistical analysis; traumatic brain injury",2-s2.0-85028864664
"Geertz-Hansen H.M., Kiemer L., Nielsen M., Stanchev K., Blom N., Brunak S., Petersen T.N.","Protein features as determinants of wild-type glycoside hydrolase thermostability",2017,"Proteins: Structure, Function and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031103339&doi=10.1002%2fprot.25357&partnerID=40&md5=dca4f04f5ef4dfd368f13f4136e0bd9b","Thermostable enzymes for conversion of lignocellulosic biomass into biofuels have significant advantages over enzymes with more moderate themostability due to the challenging application conditions. Experimental discovery of thermostable enzymes is highly cost intensive, and the development of in-silico methods guiding the discovery process would be of high value. To develop such an in-silico method and provide the data foundation of it, we determined the melting temperatures of 602 fungal glycoside hydrolases from the families GH5, 6, 7, 10, 11, 43, and AA9 (formerly GH61). We, then used sequence and homology modeled structure information of these enzymes to develop the ThermoP melting temperature prediction method. Futhermore, in the context of thermostability, we determined the relative importance of 160 molecular features, such as amino acid frequencies and spatial interactions, and exemplified their biological significance. The presented prediction method is made publicly available at http://www.cbs.dtu.dk/services/ThermoP. © 2017 Wiley Periodicals, Inc.","biofuels; bioinformatics; biomass; cellulases; lignocellulose; machine learning; melting temperature; prediction; protein thermal stability","glycosidase; fungal protein; glycosidase; Article; Aspergillus niger; Aspergillus oryzae; computer model; controlled study; enzyme active site; Escherichia coli; melting temperature; molecular cloning; nonhuman; phylogeny; prediction; priority journal; protein expression; sequence homology; thermostability; wild type; amino acid sequence; biology; biomass; chemistry; classification; enzyme stability; heat; machine learning; molecular model; Amino Acid Sequence; Biomass; Computational Biology; Enzyme Stability; Fungal Proteins; Glycoside Hydrolases; Hot Temperature; Machine Learning; Models, Molecular",2-s2.0-85031103339
"Oliveira S., Zêzere J.L., Queirós M., Pereira J.M.","Assessing the social context of wildfire-affected areas. The case of mainland Portugal",2017,"Applied Geography",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029701079&doi=10.1016%2fj.apgeog.2017.09.004&partnerID=40&md5=0eef3f3480455a47fe5762e720358084","Wildfires cause different impacts, depending on the conditions and resilience level of the exposed communities. Wildfire occurrence in mainland Portugal was assessed with regard to socioeconomic and demographic parameters, to identify the most distinctive conditions of fire-affected areas, without implying the existence of causal relationships. The latest population and agriculture census data were used to retrieve conditions at the civil parish level, regarding demographic patterns, social and labor conditions, physical structures and agricultural activities. To identify differences between parishes, two groups were created with the communities that showed the highest and lowest 20% of wildfire incidence between 2007 and 2014, separately for density of fire events and for burned area. A stepwise approach based on classification trees and random Forest methods was applied to identify the best discriminant variables between the groups. First, irrelevant variables were removed by an interactive process based on misclassification rates. The second step used random Forest analysis to the remaining variables to evaluate their importance in distinguishing the groups. In the final step, cluster analysis was applied to test the correspondence between the clusters created with the selected variables and the initial groups. Results showed that parishes with higher fire density have higher population density, higher proportion of young and educated people, larger families and more overcrowded buildings. On the contrary, parishes with larger burned area are less populated, less attractive to foreigners, have a higher proportion of elderly people, more degraded housing conditions and agricultural activities, visible in the density of sheep and goat and pastures, are still relevant. The cluster analysis demonstrated a better performance of the model for wildfire density, revealing a strong association with socioeconomic dynamics with an agreement above 0.85, much higher than for burned areas which is 0.29. Overall, the spatial distribution of wildfire impacts is framed by societal settings and particular conditions must be further understood to improve the coping capacity of affected communities. © 2017 Elsevier Ltd","Cluster analysis; Coping capacity; Portugal; Random forest; Social context; Wildfire impacts","assessment method; cluster analysis; coping strategy; demography; machine learning; socioeconomic status; spatial distribution; wildfire; Portugal; Capra hircus; Ovis aries",2-s2.0-85029701079
"Milo M.W., Roan M.J.","Detecting anomalous patterns in time-series data using sparse hierarchically parameterized transition matrices",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964319751&doi=10.1007%2fs10044-016-0544-0&partnerID=40&md5=ea23c2b88933f1a08ccb8aeb93e9b31a","Anomaly detection in time-series data is a relevant problem in many fields such as stochastic data analysis, quality assurance, and predictive modeling. Markov models are an effective tool for time-series data analysis. Previous approaches utilizing Markov models incorporate transition matrices (TMs) at varying dimensionalities and resolutions. Other analysis methods treat TMs as vectors for comparison using search algorithms such as the nearest neighbors comparison algorithm, or use TMs to calculate the probability of discrete subsets of time-series data. We propose an analysis method that treats the elements of a TM as random variables, parameterizing them hierarchically. This approach creates a metric for determining the “normalcy” of a TM generated from a subset of time-series data. The advantages of this novel approach are discussed in terms of computational efficiency, accuracy of anomaly detection, and robustness when analyzing sparse data. Unlike previous approaches, this algorithm is developed with the expectation of sparse TMs. Accounting for this sparseness significantly improves the detection accuracy of the proposed method. Detection rates in a variety of time-series data types range from (97 % TPR, 2.1 % FPR) to (100 % TPR, <0.1 % FPR) with very small sample sizes (20–40 samples) in data with sparse transition probability matrices. © 2016, Springer-Verlag London.","Machine learning; Markov processes; Monte Carlo; Pattern recognition; Real-time systems; Signal processing; Statistical models; Stochastic processes",,2-s2.0-84964319751
"Waddington G.S.","Self report balance status is not reliable post concussion",2017,"Journal of Science and Medicine in Sport",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029794960&doi=10.1016%2fj.jsams.2017.09.004&partnerID=40&md5=81e0c68f7d0db70b7029aa06ddc9d59c",[No abstract available],,"ankle instability; attitude; body equilibrium; concussion; Editorial; exercise; hip; human; machine learning; motor performance; physical activity; self report; sport injury; strength",2-s2.0-85029794960
"Cunha-Filho M., Araújo M.R., Gelfuso G.M., Gratieri T.","FDM 3D printing of modified drug-delivery systems using hot melt extrusion: A new approach for individualized therapy",2017,"Therapeutic Delivery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032217407&doi=10.4155%2ftde-2017-0067&partnerID=40&md5=51a98c5bb3f21ab81f5579c390874a76","The production process of 3D-printed drugs offers unique advantages such as the possibility of individualizing the drug therapy and easily associating different drugs and release technologies in the same pharmaceutical unit. Fused deposition modeling, a 3D printing technique, seems especially interesting for pharmaceutical applications, due to its low cost, precise and reproducible control of the printed structures, and versatility for industrial and laboratory scale. This technique combined with another technology already adapted for the pharmaceutical industry, the hot melt extrusion, is able to incorporate various mechanisms of modified drug release. This special report aims to bring together data of the experimental progress achieved using the fused deposition modeling 3D printing combined with hot melt extrusion technique and its potential in drug delivery. </inline-graphic. © 2017 Future Science Ltd.","3D printing; FDM; fused deposition modeling; HME; hot melt extrusion","nanocarrier; polymer; solid lipid nanoparticle; Article; drug delivery system; drug industry; drug release; fusion deposition modeling; hot melt extrusion; machine learning; personalized medicine; priority journal; reproducibility; three dimensional printing",2-s2.0-85032217407
"Fang B., Huang Z., Li Y., Wang Y.","υ-Support vector machine based on discriminant sparse neighborhood preserving embedding",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964222909&doi=10.1007%2fs10044-016-0547-x&partnerID=40&md5=38b8c72e43e651faf5f0d7eb51a40432","In this paper, we mainly focus on two issues (1) SVM is very sensitive to noise. (2) The solution of SVM does not take into consideration of the intrinsic structure and the discriminant information of the data. To address these two problems, we first propose an integration model to integrate both the local manifold structure and the local discriminant information into ℓ1 graph embedding. Then we add the integration model into the objection function of υ-support vector machine. Therefore, a discriminant sparse neighborhood preserving embedding υ-support vector machine (υ-DSNPESVM) method is proposed. The theoretical analysis demonstrates that υ-DSNPESVM is a reasonable maximum margin classifier and can obtain a very lower generalization error upper bound by minimizing the integration model and the upper bound of margin error. Moreover, in the nonlinear case, we construct the kernel sparse representation-based ℓ1 graph for υ-DSNPESVM, which is more conducive to improve the classification accuracy than ℓ1 graph constructed in the original space. Experimental results on real datasets show the effectiveness of the proposed υ-DSNPESVM method. © 2016, Springer-Verlag London.","Local manifold structure; Manifold learning; Margin error; Objective function",,2-s2.0-84964222909
"Lee D., Park Y.-G., Park J.-B., Roh J.H.","Very short-term wind power ensemble forecasting without numerical weather prediction through the predictor design",2017,"Journal of Electrical Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032866790&doi=10.5370%2fJEET.2017.12.6.2177&partnerID=40&md5=f18c5d499770abfda9041fc1228305b6","The goal of this paper is to provide the specific forecasting steps and to explain how to design the forecasting architecture and training data sets to forecast very short-term wind power when the numerical weather prediction (NWP) is unavailable, and when the sampling periods of the wind power and training data are different. We forecast the very short-term wind power every 15 minutes starting two hours after receiving the most recent measurements up to 40 hours for a total of 38 hours, without using the NWP data but using the historical weather data. Generally, the NWP works as a predictor and can be converted to wind power forecasts through machine learning-based forecasting algorithms. Without the NWP, we can still build the predictor by shifting the historical weather data and apply the machine learning-based algorithms to the shifted weather data. In this process, the sampling intervals of the weather and wind power data are unified. To verify our approaches, we participated in the 2017 wind power forecasting competition held by the European Energy Market conference and ranked sixth. We have shown that the wind power can be accurately forecasted through the data shifting although the NWP is unavailable. © The Korean Institute of Electrical Engineers.","Ensemble forecasting; Gradient boosting machine; Wind power forecasting","Artificial intelligence; Forecasting; Learning algorithms; Learning systems; Meteorology; Wind power; Ensemble forecasting; Forecasting algorithm; Gradient boosting; Historical weather datum; Numerical weather prediction; Training data sets; Wind power forecast; Wind power forecasting; Weather forecasting",2-s2.0-85032866790
"Pandiyan V., Tjahjowidodo T.","In-process endpoint detection of weld seam removal in robotic abrasive belt grinding process",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021117678&doi=10.1007%2fs00170-017-0646-x&partnerID=40&md5=0dfd41c9c504765167fd80f9bd7131a8","This paper proposes a novel approach for in-process endpoint detection of weld seam removal during robotic abrasive belt grinding process using discrete wavelet transform (DWT) and support vector machine (SVM). A virtual sensing system is developed consisting of a force sensor, accelerometer sensor and machine learning algorithm. This work also presents the trend of the sensor signature at each stage of weld seam evolution during its removal process. The wavelet decomposition coefficient is used to represent all possible types of transients in vibration and force signals generated during grinding over weld seam. “Daubechies-4” wavelet function was used to extract features from the sensors. An experimental investigation using three different weld profile conditions resulting from the weld seam removal process using abrasive belt grinding was identified. The SVM-based classifier was employed to predict the weld state. The results demonstrate that the developed diagnostic methodology can reliably predict endpoint at which weld seam is removed in real time during compliant abrasive belt grinding. © 2017, Springer-Verlag London Ltd.","Abrasive belt grinding; DWT; Surface finish/integrity; SVM; Weld seam removal","Abrasive belts; Abrasives; Discrete wavelet transforms; Grinding (machining); Image retrieval; Learning algorithms; Robotics; Support vector machines; Virtual machine; Wavelet decomposition; Wavelet transforms; Abrasive belt grinding; Accelerometer sensor; Decomposition coefficient; Diagnostic methodology; Experimental investigations; Surface finishes; SVM-based classifiers; Weld seam; Welds",2-s2.0-85021117678
"Zhang L., Wu X., Zhu H., Abourizk S.M.","Performing Global Uncertainty and Sensitivity Analysis from Given Data in Tunnel Construction",2017,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029410032&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000714&partnerID=40&md5=41a35455f42274d15fdc8783f24aafcc","This paper develops a novel hybrid approach that integrates metamodeling, machine learning algorithms, and a variance decomposition technique to support global uncertainty and sensitivity (US) analysis under uncertainty. It consists of three main steps: (1) metamodel construction; (2) metamodel validation; and (3) global US analysis. A multi-input and multioutput metamodel, with least-squares support vector machine (LSSVM) and particle swarm optimization (PSO) algorithms incorporated, is built in order to simulate system behaviors of tunnel-induced building damage. Three indicators - mean absolute percentage error (MAPE), variance of absolute percentage error (VAPE), and mean square percentage error (MSPE) - are proposed to test the prediction performance of the metamodel. The extended Fourier amplitude sensitivity test (EFAST) is used to perform global US analysis on the basis of the well-trained metamodel. The novelty of the developed approach lies in its capability of learning from given data to identify relationships between model inputs and outputs to provide an access for conducting global US analysis. The collected data from the construction of the Wuhan Metro system (WMS) in China are used in a case study to demonstrate the effectiveness and applicability of the developed approach. Results indicate that the developed approach is capable of (1) predicting and assessing the magnitude of tunnel-induced building damage in terms of the cumulative distribution function (CDF) of model outputs, and (2) identifying the most significant and insignificant factors for possible dimension reduction to improve the understanding of the model behavior. This research contributes to (1) the body of knowledge by proposing a more appropriate research methodology that can cope with aleatory and epistemic uncertainty and support global US analysis based on given data, and (2) the state of practice by providing a data-driven metamodel technique to simulate system behaviors of tunnel-induced building damage with high reliability and reduce dependency on domain experts. © 2017 American Society of Civil Engineers.","Extended Fourier amplitude sensitivity test (EFAST); Metamodel; Monte Carlo simulation; Tunnel-induced building damage; Uncertainty and sensitivity analysis","Behavioral research; Distribution functions; Errors; Intelligent systems; Learning algorithms; Learning systems; Monte Carlo methods; Optimization; Particle swarm optimization (PSO); Reliability analysis; Sensitivity analysis; Subways; Aleatory and epistemic uncertainties; Building damage; Extended fourier amplitude sensitivity test (eFAST); Global uncertainty and sensitivity analysis; Least squares support vector machines; Meta model; Particle swarm optimization algorithm; Uncertainty and sensitivity analysis; Uncertainty analysis",2-s2.0-85029410032
"Sherly Alphonse A., Dharma D.","A novel Monogenic Directional Pattern (MDP) and pseudo-Voigt kernel for facilitating the identification of facial emotions",2017,"Journal of Visual Communication and Image Representation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032471386&doi=10.1016%2fj.jvcir.2017.10.008&partnerID=40&md5=be801ca321333102b5a13ab4943a3dea","Facial expressions are the best way of communicating human emotions. This paper proposes a novel Monogenic Directional Pattern (MDP) for extracting features from the face. To reduce the time spent on choosing the best kernel, a novel pseudo-Voigt kernel is chosen as the common kernel for dimension reduction proposed as pseudo-Voigt kernel-based Generalized Discriminant Analysis (PVK-GDA). The pseudo-Voigt kernel-based Extreme Learning Machine (PVK-ELM) is used for better recognition of facial emotions. The efficiency of the approach is proved by experimenting with the Japanese Female Facial Expression (JAFFE), Cohn Kanade (CK+), Multimedia Understanding Group (MUG), Static Facial Expressions in the Wild (SFEW) and Oulu-Chinese Academy of Science, Institute of Automation (Oulu-CASIA) datasets. This approach achieves better classification accuracy of 96.7% for JAFFE, 99.4% for CK+, 98.6% for MUG, 35.6% for SFEW and 88% for Oulu-CASIA, which is certainly higher when compared to other techniques in the literature. © 2017 Elsevier Inc.","Directional pattern; ELM; Monogenic; Pseudo-Voigt","Directional patterns (antenna); Discriminant analysis; Learning systems; Chinese Academy of Sciences; Classification accuracy; Dimension reduction; Extracting features; Extreme learning machine; Generalized discriminant analysis; Monogenic; Pseudo-Voigt; Face recognition",2-s2.0-85032471386
"Yang T.-H., Wu C.-H., Huang K.-Y., Su M.-H.","Coupled HMM-based multimodal fusion for mood disorder detection through elicited audio–visual signals",2017,"Journal of Ambient Intelligence and Humanized Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031827065&doi=10.1007%2fs12652-016-0395-y&partnerID=40&md5=076088597602ef3651bf21c5dcf2e241","Mood disorders encompass a wide array of mood issues, including unipolar depression (UD) and bipolar disorder (BD). In diagnostic evaluation on the outpatients with mood disorder, a high percentage of BD patients are initially misdiagnosed as having UD. It is crucial to establish an accurate distinction between BD and UD to make a correct and early diagnosis, leading to improvements in treatment and course of illness. In this study, eliciting emotional videos are firstly used to elicit the patients’ emotions. After watching each video clips, their facial expressions and speech responses are collected when they are interviewing with a clinician. In mood disorder detection, the facial action unit (AU) profiles and speech emotion profiles (EPs) are obtained, respectively, by using the support vector machines (SVMs) which are built via facial features and speech features adapted from two selected databases using a denoising autoencoder-based method. Finally, a Coupled Hidden Markov Model (CHMM)-based fusion method is proposed to characterize the temporal information. The CHMM is modified to fuse the AUs and the EPs with respect to six emotional videos. Experimental results show the promising advantage and efficacy of the CHMM-based fusion approach for mood disorder detection. © 2016, Springer-Verlag Berlin Heidelberg.","Autoencoder adaptation; Coupled Hidden Markov Model; Mood disorder detection; Multimodal fusion","Diagnosis; Learning systems; Markov processes; Support vector machines; Auto encoders; Bipolar disorder; Coupled hidden Markov models; Facial Expressions; Mood disorders; Multi-modal fusion; Support vector machine (SVMs); Temporal information; Hidden Markov models",2-s2.0-85031827065
"Ziaeefard S., Miller M.H., Rastgaar M., Mahmoudian N.","Co-robotics hands-on activities: A gateway to engineering design and STEM learning",2017,"Robotics and Autonomous Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030161086&doi=10.1016%2fj.robot.2017.07.013&partnerID=40&md5=ddc5ad4f18b8e9fc5d2841926cbad881","This paper presents the effect of meaningful learning contexts and hands-on activities, facilitated using two robots that work with people (co-robots), in broadening and sustaining pre-college student engagement in Science, Technology, Engineering, and Mathematics (STEM). The two co-robots are: (1) a Glider for Underwater Problem-solving and Promotion of Interest in Engineering or GUPPIE and (2) a Neurally controlled manipulator called Neu-pulator. The co-robots are easy and inexpensive to manufacture, with readily available lightweight and durable components. They are also modular to accommodate a variety of learning activities that help young students to learn crosscutting concepts and engineering practice. The early assessment results show that students’ interests in activities related to robotics depend on their perception of the difficulty and their confidence level. The key is to start early when the students are young. The challenge is to break the barriers and define tasks as fun activities with a learn and play approach that can be rewarding. In this work, using a meaningful context – as in co-robots that help humans – in a hands-on project-based program that integrates different aspect of design, science, and technology is found effective in increasing students’ enthusiasm and participation. The co-robots and the hands-on activities can be easily adopted in classrooms by teachers with no engineering background who seek innovative ways to connect interdisciplinary core ideas and standards to the concepts they need to teach. © 2017 Elsevier B.V.","Assistive robots; Co-robots; Engineering design; Hands-on activity; K-12 stem education; Marine robots; Robotics education","Education; Engineering education; Machine design; Manipulators; Marine education; Problem solving; Robotics; Robots; Standards; STEM (science, technology, engineering and mathematics); Teaching; Assistive robots; Engineering design; Hands-on activities; K-12 STEM education; Marine robots; Robotics education; Students",2-s2.0-85030161086
"Beigi P., Rohling R., Salcudean S.E., Ng G.C.","CASPER: computer-aided segmentation of imperceptible motion—a learning-based tracking of an invisible needle in ultrasound",2017,"International Journal of Computer Assisted Radiology and Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021198700&doi=10.1007%2fs11548-017-1631-4&partnerID=40&md5=f0fb252ee649d7d9906eacbb87e9104c","Purpose: This paper presents a new micro-motion-based approach to track a needle in ultrasound images captured by a handheld transducer. Methods: We propose a novel learning-based framework to track a handheld needle by detecting microscale variations of motion dynamics over time. The current state of the art on using motion analysis for needle detection uses absolute motion and hence work well only when the transducer is static. We have introduced and evaluated novel spatiotemporal and spectral features, obtained from the phase image, in a self-supervised tracking framework to improve the detection accuracy in the subsequent frames using incremental training. Our proposed tracking method involves volumetric feature selection and differential flow analysis to incorporate the neighboring pixels and mitigate the effects of the subtle tremor motion of a handheld transducer. To evaluate the detection accuracy, the method is tested on porcine tissue in-vivo, during the needle insertion in the biceps femoris muscle. Results: Experimental results show the mean, standard deviation and root-mean-square errors of 1. 28 ∘, 1. 09 ∘ and 1. 68 ∘ in the insertion angle, and 0.82, 1.21, 1.47 mm, in the needle tip, respectively. Conclusions: Compared to the appearance-based detection approaches, the proposed method is especially suitable for needles with ultrasonic characteristics that are imperceptible in the static image and to the naked eye. © 2017, CARS.","Interventions; Motion detection; Needle tracking; Optical flow; Ultrasound","animal tissue; Article; biceps femoris muscle; echography; evaluation study; flow measurement; gold standard; hand tremor; human; image analysis; image segmentation; in vivo study; intermethod comparison; measurement accuracy; measurement error; motion; nonhuman; online analysis; optic flow; priority journal; spatiotemporal analysis; support vector machine; volumetry",2-s2.0-85021198700
"Yi Q., Wang H., Guo R., Li S., Jiang Y.","Laser ultrasonic quantitative recognition based on wavelet packet fusion algorithm and SVM",2017,"Optik",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029579971&doi=10.1016%2fj.ijleo.2017.08.105&partnerID=40&md5=12a983763adcbe655075995a1b3e38ce","With the development of industrialization and modern technology, laser ultrasonic technique is more and more used in aerospace, machinery and electronics, measurements, metallurgy chemical engineering, materials science, railway transportation, bridge engineering, etc. In order to maintain the excellent characteristics of new materials (such as thermal properties, mechanical properties, chemical properties and optical properties, material structure must be early diagnosed and monitored before properties change. Nondestructive Testing technology plays a great role in monitoring reliability of industry products. This paper proposes a new feature selection method based on wavelet packet algorithm, and applies SVM (support vector machine) for quantitative classification on the ultrasonic echo data generated by cracks in Laser ultrasonic experiment. By combining the nondestructive device and dimension reduction method in machine learning, this paper analyses the scatter plot of two cracks in 2d and the fitting surface in 3d and give the quantitative index for determining the performance of used methods. © 2017 Elsevier GmbH","Dimension reduction; Laser ultrasonic; Quantitative recognition; SVM classification; Wavelet packet fusion","Cracks; Learning systems; Machinery; Materials handling; Nondestructive examination; Optical properties; Support vector machines; Testing; Ultrasonic applications; Dimension reduction; Laser ultrasonics; Quantitative recognition; SVM classification; Wavelet Packet; Ultrasonic testing",2-s2.0-85029579971
"Khreich W., Khosravifar B., Hamou-Lhadj A., Talhi C.","An anomaly detection system based on variable N-gram features and one-class SVM",2017,"Information and Software Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025661400&doi=10.1016%2fj.infsof.2017.07.009&partnerID=40&md5=3433201785ea445f8124bf362cb380fe","Context: Run-time detection of system anomalies at the host level remains a challenging task. Existing techniques suffer from high rates of false alarms, hindering large-scale deployment of anomaly detection techniques in commercial settings.Objective: To reduce the false alarm rate, we present a new anomaly detection system based on a novel feature extraction technique, which combines the frequency with the temporal information from system call traces, and on one-class support vector machine (OC-SVM) detector.Method: The proposed feature extraction approach starts by segmenting the system call traces into multiple n-grams of variable length and mapping them to fixed-size sparse feature vectors, which are then used to train OC-SVM detectors.Results: The results achieved on a real-world system call dataset show that our feature vectors with up to 6-grams outperform the term vector models (using the most common weighting schemes) proposed in related work. More importantly, our anomaly detection system using OC-SVM with a Gaussian kernel, trained on our feature vectors, achieves a higher-level of detection accuracy (with a lower false alarm rate) than that achieved by Markovian and n-gram based models as well as by the state-of-the-art anomaly detection techniques.Conclusion: The proposed feature extraction approach from traces of events provides new and general data representations that are suitable for training standard one-class machine learning algorithms, while preserving the temporal dependencies among these events. © 2017 Elsevier B.V.","Anomaly detection systems; Feature extraction; Intrusion detection and prevention; Software security; System calls; Tracing","Alarm systems; Distributed computer systems; Errors; Extraction; Image segmentation; Intrusion detection; Learning algorithms; Markov processes; Scales (weighing instruments); Signal detection; Support vector machines; Vectors; Anomaly detection systems; Intrusion detection and prevention; Software security; System calls; Tracing; Feature extraction",2-s2.0-85025661400
"Yu K.M.","The role of time in phonetic spaces: Temporal resolution in Cantonese tone perception",2017,"Journal of Phonetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026826968&doi=10.1016%2fj.wocn.2017.06.004&partnerID=40&md5=579d682b15bdef36e14d695000b3c9f8","The role of temporal resolution in speech perception (e.g. whether tones are parameterized with fundamental frequency sampled every 10 ms, or just twice in the syllable) is sometimes overlooked, and the temporal resolution relevant for tonal perception is still an open question. The choice of temporal resolution matters because how we understand the recognition, dispersion, and learning of phonetic categories is entirely predicated on what parameters we use to define the phonetic space that they lie in. Here, we present a tonal perception experiment in Cantonese where we used interrupted speech in trisyllabic stimuli to study the effect of temporal resolution on human tonal identification. We also performed acoustic classification of the stimuli with support vector machines. Our results show that just a few samples per syllable are enough for humans and machines to classify Cantonese tones with reasonable accuracy, without much difference in performance from having the full speech signal available. The confusion patterns and machine classification results suggest that loss of detailed information about the temporal alignment and shape of fundamental frequency contours was a major cause of decreasing accuracy as resolution decreased. Moreover, machine classification experiments show that for accurate identification of rising tones in Cantonese, it is crucial to extend the temporal window for sampling to the following syllable, due to peak delay. © 2017 The Author(s)","Cantonese; Interrupted speech; Temporal integration; Temporal resolution; Tone perception; Tone recognition","experimental model; human; learning; perception; sampling; speech; stimulus; support vector machine",2-s2.0-85026826968
"Masood M.K., Soh Y.C., Jiang C.","Occupancy estimation from environmental parameters using wrapper and hybrid feature selection",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025604283&doi=10.1016%2fj.asoc.2017.07.003&partnerID=40&md5=d61e1a8477e91e62eb226478da109813","Occupancy information is essential to facilitate demand-driven operations of air-conditioning and mechanical ventilation (ACMV) systems. Environmental sensors are increasingly being explored as cost effective and non-intrusive means to obtain the occupancy information. This requires the extraction and selection of useful features from the sensor data. In past works, feature selection has generally been implemented using filter-based approaches. In this work, we introduce the use of wrapper and hybrid feature selection for better occupancy estimation. To achieve a fast computation time, we introduce a ranking-based incremental search in our algorithms, which is more efficient than the exhaustive search used in past works. For wrapper feature selection, we propose the WRANK-ELM, which searches an ordered list of features using the extreme learning machine (ELM) classifier. For hybrid feature selection, we propose the RIG-ELM, which is a filter–wrapper hybrid that uses the relative information gain (RIG) criterion for feature ranking and the ELM for the incremental search. We present experimental results in an office space with a multi-sensory network to validate the proposed algorithms. © 2017 Elsevier B.V.","ELM; Hybrid feature selection; Occupancy estimation; Wrapper","Computational efficiency; Office buildings; Ventilation; Environmental parameter; Environmental sensor; Extreme learning machine; Hybrid feature selections; Incremental search; Mechanical ventilation; Relative information; Wrapper; Feature extraction",2-s2.0-85025604283
"Zhang X., Jia J., Gao K., Zhang Y., Zhang D., Li J., Tian Q.","Trip outfits advisor: Location-oriented clothing recommendation",2017,"IEEE Transactions on Multimedia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032195848&doi=10.1109%2fTMM.2017.2696825&partnerID=40&md5=85f115fd29afcfc55f96115569fcccde","When packing for a journey, have you ever asked 'what clothes should I take with me?' Wearing appropriate and aesthetically pleasing clothing when traveling is a concern for many of us. Our data observation of photos from several popular travel websites reveals that people's choice of clothing items and their color combinations have strong correlations with the weather, the season, and the main type of attraction at the destination. This leads to an interesting and novel problem: can the correlation between clothing and locations be automatically learned from social photos and leveraged for location-oriented clothing recommendations? In this paper, we systematically study this problem and propose a hybrid multilabel convolutional neural network combined with the support vector machine (mCNN-SVM) approach to capture the intrinsic and complex correlations between clothing attributes and location attributes. Specifically, we adapt the CNN architecture to multilabel learning and fine-tune it using each fine-grained clothing item. Then, the recognized items are fed to the SVM to learn the correlations. Experiments on three fashion datasets and a benchmark journey outfit dataset show that our proposed approach outperforms several baselines by over 10.52-16.38% in terms of the mAP for clothing item recognition and outperforms several alternative methods by over 9.59-29.41% in terms of the mAP when ranking clothing by appropriateness for travel destinations. Finally, an interesting case study demonstrates the effectiveness of our method by answering what items to wear, how to match them, and how to dress in an aesthetically pleasing manner for a journey. © 1999-2012 IEEE.","Clothing recommendation; fashion analysis; location-based multimedia system","Multimedia systems; Neural networks; Support vector machines; Clothing recommendation; Color combination; Complex correlation; Convolutional neural network; fashion analysis; Location based; Multi-label learning; Strong correlation; Location",2-s2.0-85032195848
"Hartley P., Flamary R., Jackson N., Tagore A.S., Metcalf R.B.","Support vector machine classification of strong gravitational lenses",2017,"Monthly Notices of the Royal Astronomical Society",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028698299&doi=10.1093%2fmnras%2fstx1733&partnerID=40&md5=33985bd234ac861ccd00b970b032cd96","The imminent advent of very large-scale optical sky surveys, such as Euclid and the Large Synoptic Survey Telescope (LSST), makes it important to find efficient ways of discovering rare objects such as strong gravitational lens systems, where a background object is multiply gravitationally imaged by a foreground mass. As well as finding the lens systems, it is important to reject false positives due to intrinsic structure in galaxies, and much work is in progress with machine learning algorithms such as neural networks in order to achieve both these aims. We present and discuss a support vector machine (SVM) algorithm which makes use of a Gabor filter bank in order to provide learning criteria for separation of lenses and non-lenses, and demonstrate using blind challenges that under certain circumstances, it is a particularly efficient algorithm for rejecting false positives.We compare the SVM engine with a large-scale human examination of 100 000 simulated lenses in a challenge data set, and also apply the SVM method to survey images from the Kilo Degree Survey. © 2017 The Authors Published by Oxford University Press on behalf of the Royal Astronomical Society.","Galaxies: general; Gravitational lensing: strong; Methods: data analysis; Methods: statistical; Surveys",,2-s2.0-85028698299
"Becattini F., Seidenari L., Del Bimbo A.","Indexing quantized ensembles of exemplar-SVMs with rejecting taxonomies",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019232024&doi=10.1007%2fs11042-017-4794-7&partnerID=40&md5=acacc28b683b83ea99f628ebc8e3557d","Ensembles of Exemplar-SVMs have been introduced as a framework for Object Detection but have rapidly found a large interest in a wide variety of computer vision applications such as mid-level feature learning, tracking and segmentation. What makes this technique so attractive is the possibility of associating to instance specific classifiers one or more semantic labels that can be transferred at test time. To guarantee its effectiveness though, a large collection of classifiers has to be used. This directly translates in a high computational footprint, which could make the evaluation step prohibitive. To overcome this issue we organize Exemplar-SVMs into a taxonomy, exploiting the joint distribution of Exemplar scores. This permits to index the classifiers at a logarithmic cost, while maintaining the label transfer capabilities of the method almost unaffected. We propose different formulations of the taxonomy in order to maximize the speed gain. In particular we propose a highly efficient Vector Quantized Rejecting Taxonomy to discard unpromising image regions during evaluation, performing computations in a quantized domain. This allow us to obtain ramarkable speed gains, with an improvement up to more than two orders of magnitude. To verify the robustness of our indexing data structure with reference to a standard Exemplar-SVM ensemble, we experiment with the Pascal VOC 2007 benchmark on the Object Detection competition and on a simple segmentation task. © 2017, Springer Science+Business Media New York.","Efficient data structures; Exemplar SVM; Label transfer; Media indexing; Object detection; Taxonomy learning; Vector quantization","Data structures; Indexing (of information); Object recognition; Semantics; Support vector machines; Taxonomies; Vector quantization; Computer vision applications; Efficient data structures; Exemplar SVM; Joint distributions; Media indexing; Orders of magnitude; Taxonomy learning; Transfer capability; Object detection",2-s2.0-85019232024
"Cho S., Fong S., Park Y.W., Cho K.","Simulation framework of ubiquitous network environments for designing diverse network robots",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962840626&doi=10.1016%2fj.future.2016.03.016&partnerID=40&md5=f168a2ae61ec01c3c1c33a9e222d2138","Smart homes provide residents with services that offer convenience using sensor networks and a variety of ubiquitous instruments. Network robots based on such networks can perform direct services for these residents. Information from various ubiquitous instruments and sensors located in smart homes is shared with network robots. These robots effectively help residents in their daily routine by accessing this information. However, the development of network robots in an actual environment requires significant time, space, labor, and money. A network robot that has not been fully developed may cause physical damage in unexpected situations. In this paper, we propose a framework that allows the design and simulation of network robot avatars and a variety of smart homes in a virtual environment to address the above problems. This framework activates a network robot avatar based on information obtained from various sensors mounted in the smart home; these sensors identify the daily routine of the human avatar residing in the smart home. Algorithms that include reinforcement learning and action planning are integrated to enable the network robot avatar to serve the human avatar. Further, this paper develops a network robot simulator to verify whether the network robot functions effectively using the framework. © 2016 Elsevier B.V.","Human–robot interaction; Network robot; Planning; Reinforcement learning; Simulation; Smart home","Automation; Intelligent buildings; Machine design; Planning; Reinforcement learning; Robot programming; Robots; Sensor networks; Virtual addresses; Virtual reality; Actual environments; Design and simulation; Physical damages; Robot simulators; Simulation; Simulation framework; Smart homes; Ubiquitous networks; Human robot interaction",2-s2.0-84962840626
"Song B., Luo J.","Mining Patent Precedents for Data-Driven Design: The Case of Spherical Rolling Robots",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030474081&doi=10.1115%2f1.4037613&partnerID=40&md5=96e09f0a2fdfca90b2cf6010e70f757a","Data-driven engineering designers often search for design precedents in patent databases to learn about relevant prior arts, seek design inspiration, or assess the novelty of their own new inventions. However, patent retrieval relevant to the design of a specific product or technology is often unstructured and unguided, and the resultant patents do not sufficiently or accurately capture the prior design knowledge base. This paper proposes an iterative and heuristic methodology to comprehensively search for patents as precedents of the design of a specific technology or product for data-driven design. The patent retrieval methodology integrates the mining of patent texts, citation relationships, and inventor information to identify relevant patents; particularly, the search keyword set, citation network, and inventor set are expanded through the designer's heuristic learning from the patents identified in prior iterations. The method relaxes the requirement for initial search keywords while improving patent retrieval completeness and accuracy. We apply the method to identify self-propelled spherical rolling robot (SPSRRs) patents. Furthermore, we present two approaches to further integrate, systemize, visualize, and make sense of the design information in the retrieved patent data for exploring new design opportunities. Our research contributes to patent data-driven design.",,"Heuristic methods; Iterative methods; Knowledge based systems; Machine design; Patents and inventions; Product design; Citation networks; Data-driven design; Design information; Design knowledge; Design precedents; Engineering designer; Heuristic learning; Patent retrieval; Search engines",2-s2.0-85030474081
"Eriksson A., Stanton N.A.","The chatty co-driver: A linguistics approach applying lessons learnt from aviation incidents",2017,"Safety Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019637279&doi=10.1016%2fj.ssci.2017.05.005&partnerID=40&md5=8900972085f6311ca58125a9d8bf06e1","Drivers of contemporary vehicles are now able to relinquish control of the driving task to the vehicle, essentially allowing the driver to be completely hands and feet free. However, changes to legislation taking effect in 2016 will require the driver to be able to override the automated driving systems or switch them off completely. Initially this functionality is likely to be limited to certain areas, such as motorways. This creates a situation where the driver is expected to take control of the vehicle after being removed from the driving control-loop for extended periods of time, which places high demand on coordination between driver and automation. Resuming control after being removed from the control-loop have proven difficult in domains where automation is prevalent, such as aviation. Therefore the authors propose the Gricean Maxims of Successful Conversation as a means to identify, and mitigate flaws in Human-Automation-Interaction. As automated driving systems have yet to penetrate the market to a sufficient level to apply the Maxims, the authors applied the Maxims to two accidents in aviation. By applying the Maxims to the case studies from a Human-Automation-Interaction perspective, the authors were able to identify lacking feedback in different components of the pilot interface. By applying this knowledge to the driving domain, the authors argue that the Maxims could be used as a means to bridge the gulf of evaluation, by allowing the automation to act like a chatty co-driver, thereby increasing system transparency and reducing the effects of being out-of-the-loop. © 2017 Elsevier Ltd","Automated driving; Common ground; Communication; Human automation collaboration; Learning from incidents; Transfer of control","Communication; Man machine systems; Vehicles; Automated driving; Common ground; Human automation collaboration; Learning from incidents; Transfer of controls; Automation; aircraft accident; airplane pilot; Article; automation; driving automation; feedback system; human; interpersonal communication; knowledge; linguistics; priority journal",2-s2.0-85019637279
"Kumar H.S., Pai S.P., Sriram N.S., Vijay G.S.","Rolling element bearing fault diagnostics: Development of health index",2017,"Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031685573&doi=10.1177%2f0954406216656214&partnerID=40&md5=ec6fe67faa4bc07a8073485317b7d2c0","This article develops and compares health indices using different approaches namely singular value decomposition, average value of the cumulative feature and Mahalanobis distance for assessing the rolling element bearing condition. The vibration signals for four conditions of rolling element bearing are acquired from a customized bearing test rig under variable load conditions. Seventeen statistical features are extracted from wavelet coefficients of the denoised signals. Feature selection is performed using singular value decomposition and kernel Fisher discriminant analysis. These selected features are used in these three approaches to develop health indices. Finally, a comparison of the three proposed approaches is made to select the best approach which can be effectively used for fault diagnosis of rolling element bearings. © Institution of Mechanical Engineers.","cumulative feature; Feature extraction; health index; Mahalanobis distance; singular value decomposition","Bearings (machine parts); Discriminant analysis; Fault detection; Feature extraction; Fisher information matrix; Health; Learning algorithms; Occupational risks; Singular value decomposition; cumulative feature; De-noised signals; Health indices; Kernel fisher discriminant analysis; Mahalanobis distances; Rolling Element Bearing; Statistical features; Wavelet coefficients; Roller bearings",2-s2.0-85031685573
"Schulte O., Khademi M., Gholami S., Zhao Z., Javan M., Desaulniers P.","A Markov Game model for valuing actions, locations, and team performance in ice hockey",2017,"Data Mining and Knowledge Discovery",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016064852&doi=10.1007%2fs10618-017-0496-z&partnerID=40&md5=d326b123e2d1df0a830f44ce947aecf7","We apply the Markov Game formalism to develop a context-aware approach to valuing player actions, locations, and team performance in ice hockey. The Markov Game formalism uses machine learning and AI techniques to incorporate context and look-ahead. Dynamic programming is applied to learn value functions that quantify the impact of actions on goal scoring. Learning is based on a massive new dataset, from SportLogiq, that contains over 1.3M events in the National Hockey League. The SportLogiq data include the location of an action, which has previously been unavailable in hockey analytics. We give examples showing how the model assigns context and location aware values to a large set of 13 action types. Team performance can be assessed as the aggregate value of actions performed by the team’s players, or the aggregate value of states reached by the team. Model validation shows that the total team action and state value both provide a strong indicator predictor of team success, as measured by the team’s average goal ratio. © 2017, The Author(s).","Markov Game; National Hockey League; Q-learning; Sports analytics","Aggregates; Dynamic programming; Learning algorithms; Learning systems; Location; Context-aware approaches; Location-aware; Markov games; Model validation; National Hockey League; Q-learning; Team performance; Value functions; Sports",2-s2.0-85016064852
"Marteau P.-F., Gibet S., Reverdy C.","Adaptive down-sampling and dimension reduction in time elastic kernel machines for efficient recognition of isolated gestures",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994718299&doi=10.1007%2f978-3-319-45763-5_3&partnerID=40&md5=00750bc44d5cd2b4306bb676de0a0a97","In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT, whose element Mi, j represents the value of feature i at timestamps j. Many studies have addressed dimensionality reduction considering only the size of the feature vector lying in ℝD to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. In return, very few of these methods have explicitly addressed the dimensionality reduction along the time axis. Yet this is a major issue when considering the use of elastic distances which are characterized by a quadratic complexity along the time axis.We present in this paper an evaluation of straightforward approaches aiming at reducing the dimensionality of the matrix M for each movement, leading to consider both the dimensionality reduction of the feature vector as well as its reduction along the time axis. The dimensionality reduction of the feature vector is achieved by selecting remarkable joints in the skeleton performing the movement, basically the extremities of the articulatory chains composing the skeleton. The temporal dimensionality reduction is achieved using either a regular or adaptive down-sampling that seeks to minimize the reconstruction error of the movements. Elastic and Euclidean kernels are then compared through support vector machine learning. Two data sets that are widely referenced in the domain of human gesture recognition, and quite distinctive in terms of quality of motion capture, are used for the experimental assessment of the proposed approaches. On these data sets we experimentally show that it is feasible, and possibly desirable, to significantly reduce simultaneously the size of the feature vector and the number of skeleton frames to represent body movements while maintaining a very good recognition rate. The method proves to give satisfactory results at a level currently reached by state-of-the-art methods on these data sets.We experimentally show that the computational complexity reduction that is obtained makes this approach eligible for real-time applications. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994718299
"Ke X., Zhou M., Niu Y., Guo W.","Data equilibrium based automatic image annotation by fusing deep model and semantic propagation",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023184684&doi=10.1016%2fj.patcog.2017.05.020&partnerID=40&md5=d750c2a573a206348054b526a59b169f","Automatic image annotation is a challenging research problem that includes a large number of tags and various features. Traditional shallow machine learning algorithms lack generalization performance when dealing with complex classification problems. Automatic image annotation based on a stacked auto-encoder (SAE) is proposed to enhance the annotation generalization performance. In this paper, two kinds of strategies, the annotation model and the annotation process, are proposed to solve the main problem of unbalanced data in image annotation. 1) For the annotation model itself, to improve the annotation effect of low frequency tags, we propose a balanced and stacked auto-encoder (BSAE) that can enhance training for low frequency tags. On the basis of this model, a robust BSAE (RBSAE) algorithm which enhances training for sub BSAE model by group is proposed to enhance the annotation stability. This strategy ensures that the model itself has a strong ability to deal with the problem of unbalanced data. 2) For the annotation process, we propose a framework of attribute discrimination annotation (ADA). We first take an unknown image. Then we construct a local equilibrium dataset based on the unknown image and discriminate the high- and low-frequency attribute of the image to determine the corresponding annotation process. One process called the local semantic propagation (LDE-SP) algorithm annotates the low frequency image and the RBSAE algorithm annotates the high frequency image. This strategy improves the overall image annotation effect and ensures that the annotation process has a strong ability to deal with the problem of unbalanced data. For each SAE (including BSAE and RBSAE) annotation model, we propose two kinds of optimization methods, namely, one that is based on non-linear optimization and one on linear optimization. Experimental results on three benchmark datasets show that the proposed model outperforms the previous models in many performance indices. © 2017 Elsevier Ltd","Data equilibrium; Deep learning; Image annotation; SAE; Semantic propagation","Benchmarking; Deep learning; Education; Image analysis; Image retrieval; Learning algorithms; Learning systems; Nonlinear programming; Optimization; Problem solving; Semantics; Automatic image annotation; Generalization performance; Image annotation; Low frequency images; Non-linear optimization; Optimization method; Performance indices; Semantic propagation; Image processing",2-s2.0-85023184684
"Yu C., Wang N., Yang L.T., Yao D., Hsu C.-H., Jin H.","A semi-supervised social relationships inferred model based on mobile phone data",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007415956&doi=10.1016%2fj.future.2016.11.027&partnerID=40&md5=8801b0fa97c514ef0702b98478e2e90e","Exploring the relationships of humans is an important study in the mobile communication network. But the relationship prediction accuracy is not good enough when the number of known relationship labels (e.g., “friend” and “colleague”) is small, especially when the number of different relation classes are imbalanced in the mobile communication network. To deal with issues, we present a semi-supervised social relationships inferred model. This model can infer the relationships based on a large amount of unlabeled data or a small amount of labeled data. The model is a co-training style semi-supervised model which is combined with the support vector machine and naive Bayes. The final relationship labels are decided by the two classifiers. The proposed model is evaluated by a real mobile communication network dataset and the experiment results show that the model is effective in relationship mining, especially when the relationship network is in a stable state. © 2016 Elsevier B.V.","Graphic structure; Mobile phone data; Semi-supervised learning; Social relationship mining","Cellular telephones; Mobile phones; Social aspects; Supervised learning; Telephone sets; Graphic structures; Mobile communication networks; Mobile phone datum; Prediction accuracy; Relationship networks; Semi- supervised learning; Semi-supervised; Social relationships; Mobile telecommunication systems",2-s2.0-85007415956
"Xu L., Yu B., Zhang Y.","An alternating direction and projection algorithm for structure-enforced matrix factorization",2017,"Computational Optimization and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018851239&doi=10.1007%2fs10589-017-9913-x&partnerID=40&md5=7999e7156c7fc269a6c00c9d4f4b33cd","Structure-enforced matrix factorization (SeMF) represents a large class of mathematical models appearing in various forms of principal component analysis, sparse coding, dictionary learning and other machine learning techniques useful in many applications including neuroscience and signal processing. In this paper, we present a unified algorithm framework, based on the classic alternating direction method of multipliers (ADMM), for solving a wide range of SeMF problems whose constraint sets permit low-complexity projections. We propose a strategy to adaptively adjust the penalty parameters which is the key to achieving good performance for ADMM. We conduct extensive numerical experiments to compare the proposed algorithm with a number of state-of-the-art special-purpose algorithms on test problems including dictionary learning for sparse representation and sparse nonnegative matrix factorization. Results show that our unified SeMF algorithm can solve different types of factorization problems as reliably and as efficiently as special-purpose algorithms. In particular, our SeMF algorithm provides the ability to explicitly enforce various combinatorial sparsity patterns that, to our knowledge, has not been considered in existing approaches. © 2017, Springer Science+Business Media New York.","Adaptive penalty parameter; Alternating direction method; Dictionary learning; Matrix factorization; Projection; Sparse optimization","Factorization; Learning systems; Optimization; Principal component analysis; Signal processing; Adaptive penalty; Alternating direction methods; Dictionary learning; Matrix factorizations; Projection; Sparse optimizations; Matrix algebra",2-s2.0-85018851239
"Fan Y.-R., Wang Y., Huang T.-Z.","Enhanced joint sparsity via iterative support detection",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021663940&doi=10.1016%2fj.ins.2017.06.034&partnerID=40&md5=c910edc0de6fb274855fc2e5d682f773","Joint sparsity has attracted considerable attention in recent years in many fields including sparse signal recovery in compressive sensing, statistics, and machine learning. Traditional convex models with joint sparsity suffer from the suboptimal performance though enjoying tractable computation. In this paper, we propose a new non-convex joint sparsity model, and develop a corresponding multi-stage adaptive convex relaxation algorithm. This method extends the idea of iterative support detection (ISD) from the single vector estimation to the multi-vector estimation by considering the joint sparsity prior. We provide some preliminary theoretical analysis including convergence analysis and a sufficient recovery condition. Numerical experiments from both compressive sensing and multi-task feature learning show the better performance of the proposed method in comparison with several state-of-the-art alternatives. Moreover, we demonstrate that the extension of ISD from the single vector to multi-vector estimation is not trivial. While ISD does not well reconstruct the single channel sparse Bernoulli signal, it does achieve significantly improved performance when recovering the multi-channel sparse Bernoulli signal thanks to its ability of natural incorporation of the joint sparsity structure. © 2017","Compressive sensing; Iterative support detection; Joint sparsity; Multi-task feature learning; Non-convex optimization; ℓ2,1-norm minimization","Compressed sensing; Convex optimization; Education; Numerical methods; Recovery; Relaxation processes; Signal reconstruction; Vectors; Compressive sensing; Feature learning; Joint sparsity; Joint sparsity models; Nonconvex optimization; Numerical experiments; Sparse signal recoveries; Sub-optimal performance; Iterative methods",2-s2.0-85021663940
"Li H., Xu Z., Li T., Sun G., Raymond Choo K.-K.","An optimized approach for massive web page classification using entity similarity based on semantic network",2017,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016200388&doi=10.1016%2fj.future.2017.03.003&partnerID=40&md5=2acbd45669b559f2dcebfd6cd75dff11","With the development of mobile technology, the users browsing habits are gradually shifted from only information retrieval to active recommendation. The classification mapping algorithm between users interests and web contents has been become more and more difficult with the volume and variety of web pages. Some big news portal sites and social media companies hire more editors to label these new concepts and words, and use the computing servers with larger memory to deal with the massive document classification, based on traditional supervised or semi-supervised machine learning methods. This paper provides an optimized classification algorithm for massive web page classification using semantic networks, such as Wikipedia, WordNet. In this paper, we used Wikipedia data set and initialized a few category entity words as class words. A weight estimation algorithm based on the depth and breadth of Wikipedia network is used to calculate the class weight of all Wikipedia Entity Words. A kinship-relation association based on content similarity of entity was therefore suggested optimizing the unbalance problem when a category node inherited the probability from multiple fathers. The keywords in the web page are extracted from the title and the main text using N-gram with Wikipedia Entity Words, and Bayesian classifier is used to estimate the page class probability. Experimental results showed that the proposed method obtained good scalability, robustness and reliability for massive web pages. © 2017 Elsevier B.V.","Entity class probability; Hereditary weight; Kinship-relation association; Semantic network; Web page classification","Conformal mapping; Information retrieval systems; Learning systems; Probability; Semantics; Supervised learning; Web crawler; Websites; Bayesian classifier; Class probabilities; Classification algorithm; Document Classification; Entity similarities; Hereditary weight; Semantic network; Web page classification; Semantic Web",2-s2.0-85016200388
"Jin X., Baker K., Christensen D., Isley S.","Foresee: A user-centric home energy management system for energy efficiency and demand response",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028320403&doi=10.1016%2fj.apenergy.2017.08.166&partnerID=40&md5=987641db0e58ad6fffd114aece56c4f4","This paper presents foresee™, a user-centric home energy management system that can help optimize how a home operates to concurrently meet users’ needs, achieve energy efficiency and commensurate utility cost savings, and reliably deliver grid services based on utility signals. Foresee is built on a multiobjective model predictive control framework, wherein the objectives consist of energy cost, thermal comfort, user convenience, and carbon emission. Foresee learns user preferences on different objectives and acts on their behalf to operate building equipment, such as home appliances, photovoltaic systems, and battery storage. In this work, machine-learning algorithms were used to derive data-driven appliance models and usage patterns to predict the home's future energy consumption. This approach enables highly accurate predictions of comfort needs, energy costs, environmental impacts, and grid service availability. Simulation studies were performed on field data from a residential building stock data set collected in the Pacific Northwest. Results indicated that foresee generated up to 7.6% whole-home energy savings without requiring substantial behavioral changes. When responding to demand response events, foresee was able to provide load forecasts upon receipt of event notifications and delivered the committed demand response services with 10% or fewer errors. Foresee fully utilized the potential of the battery storage and controllable building loads and delivered up to 7.0-kW load reduction and 13.5-kW load increase. These benefits are provided while maintaining the occupants’ thermal comfort or convenience in using their appliances. © 2017 Elsevier Ltd","Demand response; Energy efficiency; Home energy management system; Model predictive control; Smart grid; User preference","Carbon; Costs; Digital storage; Domestic appliances; Electric batteries; Energy conservation; Energy efficiency; Energy management; Energy utilization; Environmental impact; Forecasting; Grid computing; Learning algorithms; Learning systems; Model predictive control; Photovoltaic cells; Smart power grids; Thermal comfort; Building equipments; Demand response; Home energy management systems; Multi-objective modeling; Photovoltaic systems; Residential building stocks; Smart grid; User preference; Energy management systems; building; control system; cost analysis; demand analysis; energy budget; energy efficiency; energy planning; environmental impact; equipment; smart grid; Pacific Northwest",2-s2.0-85028320403
"Qu T., Zhang J.H., Chan F.T.S., Srivastava R.S., Tiwari M.K., Park W.-Y.","Demand prediction and price optimization for semi-luxury supermarket segment",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029546623&doi=10.1016%2fj.cie.2017.09.004&partnerID=40&md5=07d65bd0bca478f3f7e38ae57d853ecb","Offline retail stores face day-to-day challenges in clearing out expensive and high-end luxury products. In case of high-end priced products, the demand is seasonal and sensitive. The investment involves high risk and revenues vary beyond fathomable bounds. The primary objective of this research is to present a decision-support system for retail pricing and revenue optimization of these retail products. The sales data of past 2.5 years from prominent retail stores across 45 different regions has been used to develop this study. A regression tree/random forest-based machine learning algorithm is used to predict weekly demand. It incorporates price, holidays, discounts, inventory and other regional factors in decision making. Following this, the demand-price interdependencies are quantified and integrated into an integer linear programming model for optimal price allocation. This methodology has been implemented on offline retailing of expensive products which generally follow high variation in demand. The expected revenue has been optimized by branch & bound and branch & cut method, followed by root node analysis. The solution is further optimized by heuristic methods. © 2017 Elsevier Ltd","Analytics; Assignment; Branch and bound; Data mining; Integer programming; Retailing","Artificial intelligence; Costs; Data mining; Decision making; Decision support systems; Economics; Heuristic methods; Integer programming; Learning algorithms; Learning systems; Optimization; Retail stores; Sales; Analytics; Assignment; Demand prediction; Expensive products; Integer linear programming models; Primary objective; Retailing; Revenue optimization; Branch and bound method",2-s2.0-85029546623
"Zhou B., Li W., Hu J.","A new segmented oversampling method for imbalanced data classification using quasi-linear SVM",2017,"IEEJ Transactions on Electrical and Electronic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022322535&doi=10.1002%2ftee.22480&partnerID=40&md5=c6fb1a4185f64f6205eea3534e5f053d","Data imbalance occurs on most real-world classification problems and decreases the performance of classifiers. An oversampling method addresses the imbalance problem by generating synthetic samples to balance the data distribution. However, many of the existing oversampling methods have potential problems in generating wrong and unnecessary synthetic samples, which makes the learning tasks difficult. This paper proposes a new segmented oversampling method for imbalanced data classification. The input space is first partitioned into several linearly separable local partitions along the potential separation boundary by introducing a bottom-up, minimal-spanning-tree-based clustering method; an oversampling method is then applied within each local linear partition to prevent the generation of wrong and unnecessary synthetic samples; a quasi-linear support vector machine is finally used to realize the classification by taking advantages of the local linear partitions. Simulation results on different real-world datasets show that the proposed segmented oversampling method is effective for imbalanced data classifications. © 2017 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc. © 2017 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc.","imbalanced classification; kernel composition; local linear partition; oversampling method; support vector machine","Cluster analysis; Support vector machines; Vector spaces; Imbalanced classification; Kernel compositions; Local linear; Minimal spanning tree; Over sampling; Performance of classifier; Real-world datasets; Separation boundaries; Classification (of information)",2-s2.0-85022322535
"Le T., David Jeong H.","NLP-Based Approach to Semantic Classification of Heterogeneous Transportation Asset Data Terminology",2017,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026525681&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000701&partnerID=40&md5=87bcfdbe68e2a48f890ad1f87b566f24","The inconsistency of data terminology has imposed big challenges on integrating transportation project data from distinct sources. Differences in meaning of data elements may lead to miscommunication between data senders and receivers. Semantic relations between terms in digital dictionaries, such as ontologies, can enable the semantics of a data element to be transparent and unambiguous to computer systems. However, because of the lack of effective automated methods, identifying these relations is labor intensive and time consuming. This paper presents a novel integrated methodology that leverages multiple computational techniques to extract heterogeneous American-English data terms used in different highway agencies and their semantic relations from design manuals and other technical specifications. The proposed method implements natural language processing (NLP) to detect data elements from text documents and uses machine learning to determine the semantic relatedness among terms using their occurrence statistics in a corpus. The study also consists of developing an algorithm that classifies semantically related terms into three different lexical groups including synonymy, hyponymy, and meronymy. The key merit in this technique is that the detection of semantic relations uses only linguistic information in texts and does not depend on other existing hand-coded semantic resources. A case study was undertaken that implemented the proposed method on a 16-million-word corpus of roadway design manuals to extract and classify roadway data items. The developed classifier was evaluated using a human-encoded test set, and the results show an overall performance of 92.76% in precision and 81.02% recall. © 2017 American Society of Civil Engineers.","Data sharing; Heterogeneous data terminology; Natural language processing; Semantic interoperability; Semantic relation; Transportation data; Vector space model","Highway planning; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Semantics; Terminology; Vector spaces; Data Sharing; Heterogeneous data; Semantic interoperability; Semantic relations; Vector space models; Classification (of information)",2-s2.0-85026525681
"Tsai H.-H., Wu M.-E., Chung W.-H., Lu C.-Y.","On the study of trading strategies within limited arbitrage based on SVM",2017,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992313207&doi=10.1007%2f978-3-319-48490-7_15&partnerID=40&md5=418da2f882672c96e100e4c01d13070d","Limited arbitrage will impede the operation of market, then confuses the well-known “Efficient Market Hypothesis” in theory and investment decision of market participants in practice. We develop a contrarian trading strategy, tailored to this kind of situation, with the trading signals derived the technical indicator: BIAS, to indirectly verify the existence of limited arbitrage by testing all listed stocks in Taiwan. Further, we use the well-known machine learning, SVM, to confirm the classification method in this study being free of subjective discretion. Thus we have a robust evidence to support the usefulness of this trading strategy. © Springer International Publishing AG 2017.","BIAS; Contrarian; Limited arbitrage; SVM","Artificial intelligence; Decision theory; Financial markets; Investments; Learning systems; BIAS; Classification methods; Contrarian; Efficient market hypothesis; Investment decisions; Limited arbitrage; Market participants; Technical indicator; Commerce",2-s2.0-84992313207
"Hua J.-C., Noorian F., Moss D., Leong P.H.W., Gunaratne G.H.","High-dimensional time series prediction using kernel-based Koopman mode regression",2017,"Nonlinear Dynamics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029473070&doi=10.1007%2fs11071-017-3764-y&partnerID=40&md5=19524836db348e46959dcd5759f18980","We propose a novel methodology for high-dimensional time series prediction based on the kernel method extension of data-driven Koopman spectral analysis, via the following methodological advances: (a) a new numerical regularization method, (b) a natural ordering of Koopman modes which provides a fast alternative to the sparsity-promoting procedure, (c) a predictable Koopman modes selection technique which is equivalent to cross-validation in machine learning, (d) an optimization method for selected Koopman modes to improve prediction accuracy, (e) prediction model generation and selection based on historical error measures. The prediction accuracy of this methodology is excellent: for example, when it is used to predict clients’ order flow time series of foreign exchange, which is almost random, it can achieve more than 10% improvement on root-mean-square error over auto-regressive moving average. This methodology also opens up new possibilities for data-driven modeling and forecasting complex systems that generate the high-dimensional time series. We believe that this methodology will be of interest to the community of scientists and engineers working on quantitative finance, econometrics, system biology, neurosciences, meteorology, oceanography, system identification and control, data mining, machine learning, and many other fields involving high-dimensional time series and spatio-temporal data. © 2017, Springer Science+Business Media B.V.","Complex systems; Data-driven Koopman operator; Dynamic mode decomposition; High-dimensional time series; Kernel methods; Spatio-temporal dynamics","Artificial intelligence; Data mining; Economics; Forecasting; Large scale systems; Learning systems; Mean square error; Numerical methods; Spectrum analysis; Statistics; Time series; Data driven; Dynamic mode decompositions; High-dimensional; Kernel methods; Spatio-temporal dynamics; Time series analysis",2-s2.0-85029473070
"Keyvanrad M.A., Homayounpour M.M.","Effective sparsity control in deep belief networks using normal regularization term",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017227189&doi=10.1007%2fs10115-017-1049-x&partnerID=40&md5=083b9404da21d46cdda1dcae5e413aee","Nowadays the use of deep network architectures has become widespread in machine learning. Deep belief networks (DBNs) have deep network architectures to create a powerful generative model using training data. Deep belief networks can be used in classification and feature learning. A DBN can be learned unsupervised, and then the learned features are suitable for a simple classifier (like a linear classifier) with a few labeled data. In addition, according to researches, by using sparsity in DBNs we can learn useful low-level feature representations for unlabeled data. In sparse representation, we have the property that learned features can be interpreted, i.e., correspond to meaningful aspects of the input, and capture factors of variation in the data. Different methods are proposed to build sparse DBNs. In this paper, we proposed a new method that has different behavior according to deviation of the activation of the hidden units from a (low) fixed value. In addition, our proposed regularization term has a variance parameter that can control the force degree of sparseness. According to the results, our new method achieves the best recognition accuracy on the test sets in different datasets with different applications (image, speech and text) and we can achieve incredible results when using a different number of training samples, especially when we have a few samples for training. © 2017, Springer-Verlag London.","Deep belief network; Normal sparse RBM; Quadratic sparse RBM; Rate distortion sparse RBM; Restricted Boltzmann machine",,2-s2.0-85017227189
"Attallah O., Karthikesalingam A., Holt P.J.E., Thompson M.M., Sayers R., Bown M.J., Choke E.C., Ma X.","Using multiple classifiers for predicting the risk of endovascular aortic aneurysm repair re-intervention through hybrid feature selection",2017,"Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030761580&doi=10.1177%2f0954411917731592&partnerID=40&md5=05b87d2e8a5ea468631fc761a92f15da","Feature selection is essential in medical area; however, its process becomes complicated with the presence of censoring which is the unique character of survival analysis. Most survival feature selection methods are based on Cox's proportional hazard model, though machine learning classifiers are preferred. They are less employed in survival analysis due to censoring which prevents them from directly being used to survival data. Among the few work that employed machine learning classifiers, partial logistic artificial neural network with auto-relevance determination is a well-known method that deals with censoring and perform feature selection for survival data. However, it depends on data replication to handle censoring which leads to unbalanced and biased prediction results especially in highly censored data. Other methods cannot deal with high censoring. Therefore, in this article, a new hybrid feature selection method is proposed which presents a solution to high level censoring. It combines support vector machine, neural network, and K-nearest neighbor classifiers using simple majority voting and a new weighted majority voting method based on survival metric to construct a multiple classifier system. The new hybrid feature selection process uses multiple classifier system as a wrapper method and merges it with iterated feature ranking filter method to further reduce features. Two endovascular aortic repair datasets containing 91% censored patients collected from two centers were used to construct a multicenter study to evaluate the performance of the proposed approach. The results showed the proposed technique outperformed individual classifiers and variable selection methods based on Cox's model such as Akaike and Bayesian information criterions and least absolute shrinkage and selector operator in p values of the log-rank test, sensitivity, and concordance index. This indicates that the proposed classifier is more powerful in correctly predicting the risk of re-intervention enabling doctor in selecting patients' future follow-up plan. © Institution of Mechanical Engineers.","censoring; Cox's proportional hazard model; endovascular aortic repair; hybrid feature selection; Multiple classifier system; survival analysis","Artificial intelligence; Bioinformatics; Blood vessels; Feature extraction; Forecasting; Hazards; Learning systems; Nearest neighbor search; Neural networks; Patient treatment; Repair; Statistical tests; censoring; Endovascular; Hybrid feature selections; Multiple classifier systems; Proportional hazard models; Survival analysis; Classification (of information)",2-s2.0-85030761580
"Zhou W., Ma Y., Zhang J., Hu J., Zhang M., Wang Y., Li Y., Wu L., Pan Y., Zhang Y., Zhang X., Zhang X., Zhang Z., Zhang J., Li H., Lu L., Jin L., Wang J., Yuan Z., Liu J.","Predictive model for inflammation grades of chronic hepatitis B: Large-scale analysis of clinical parameters and gene expressions",2017,"Liver International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018582240&doi=10.1111%2fliv.13427&partnerID=40&md5=c2fd4e69f6767445310600e3221a3ad7","Background: Liver biopsy is the gold standard to assess pathological features (eg inflammation grades) for hepatitis B virus-infected patients although it is invasive and traumatic; meanwhile, several gene profiles of chronic hepatitis B (CHB) have been separately described in relatively small hepatitis B virus (HBV)-infected samples. We aimed to analyse correlations among inflammation grades, gene expressions and clinical parameters (serum alanine amino transaminase, aspartate amino transaminase and HBV-DNA) in large-scale CHB samples and to predict inflammation grades by using clinical parameters and/or gene expressions. Methods: We analysed gene expressions with three clinical parameters in 122 CHB samples by an improved regression model. Principal component analysis and machine-learning methods including Random Forest, K-nearest neighbour and support vector machine were used for analysis and further diagnosis models. Six normal samples were conducted to validate the predictive model. Results: Significant genes related to clinical parameters were found enriching in the immune system, interferon-stimulated, regulation of cytokine production, anti-apoptosis, and etc. A panel of these genes with clinical parameters can effectively predict binary classifications of inflammation grade (area under the ROC curve [AUC]: 0.88, 95% confidence interval [CI]: 0.77-0.93), validated by normal samples. A panel with only clinical parameters was also valuable (AUC: 0.78, 95% CI: 0.65-0.86), indicating that liquid biopsy method for detecting the pathology of CHB is possible. Conclusions: This is the first study to systematically elucidate the relationships among gene expressions, clinical parameters and pathological inflammation grades in CHB, and to build models predicting inflammation grades by gene expressions and/or clinical parameters as well. © 2017 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd","clinical predictive model; gene expressions; HBV infection; inflammation grades","alanine aminotransferase; aspartate aminotransferase; virus DNA; alanine aminotransferase blood level; apoptosis; Article; aspartate aminotransferase blood level; bioinformatics; chronic hepatitis B; cytokine production; gene expression; human; human tissue; k nearest neighbor; liquid biopsy; liver biopsy; microarray analysis; nonhuman; prediction; random forest; receiver operating characteristic; support vector machine",2-s2.0-85018582240
"Li Z., Bors A.G.","Steganalysis of 3D objects using statistics of local feature sets",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021098690&doi=10.1016%2fj.ins.2017.06.011&partnerID=40&md5=bdd06ce50aa7fe4e300cf11f5b06831c","3D steganalysis aims to identify subtle invisible changes produced in graphical objects through digital watermarking or steganography. Sets of statistical representations of 3D features, extracted from both cover and stego 3D mesh objects, are used as inputs into machine learning classifiers in order to decide whether any information was hidden in the given graphical object. The features proposed in this paper include those representing the local object curvature, vertex normals, the local geometry representation in the spherical coordinate system. The effectiveness of these features is tested in various combinations with other features used for 3D steganalysis. The relevance of each feature for 3D steganalysis is assessed using the Pearson correlation coefficient. Six different 3D watermarking and steganographic methods are used for creating the stego-objects used in the evaluation study. © 2017 Elsevier Inc.","3D Objects; Information hiding; Local feature; Steganalysis; Steganography; Watermarking","Classification (of information); Correlation methods; Digital watermarking; Learning systems; Watermarking; 3D object; Information hiding; Local feature; Local feature sets; Pearson correlation coefficients; Spherical coordinate systems; Statistical representations; Steganalysis; Steganography",2-s2.0-85021098690
"Rocha A.D., Lima-Monteiro P., Parreira-Rocha M., Barata J.","Artificial immune systems based multi-agent architecture to perform distributed diagnosis",2017,"Journal of Intelligent Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032838777&doi=10.1007%2fs10845-017-1370-y&partnerID=40&md5=111bb65cc414f72d4540fd104e25ed80","Cyber-physical systems (CPS) emerge as a new idea to implement new manufacturing paradigms. There paradigms aim at answering the socio-economic factors that characterise modern enterprises, such as mass customisation and new markets. The authors propose an architecture that performs distributed diagnosis. The proposed solution uses artificial immune systems (AIS) to perform evolutionary diagnose. Industrial approaches to machine diagnosis are centralised. The authors pretend to make a CPS capable of distributed diagnosis with learning capabilities. An architecture capable of machine diagnosis and learning is also presented. This is done by bio-inspired algorithms. These were rated by a fuzzy inference system. The algorithms were tested for situations a system may endure and for their learning capability. The results of the obtained research, study and development are hereby presented. These results constitute proof of the sustainability of the AIS paradigm as a solution to distributed diagnosis. © 2017 Springer Science+Business Media, LLC","Artificial immune systems; Bio-inspired algorithms; Cyber-physical systems; Multi-agent systems","Cyber Physical System; Embedded systems; Fuzzy inference; Immune system; Artificial Immune System; Bio-inspired algorithms; Cyber-physical systems (CPS); Fuzzy inference systems; Learning capabilities; Manufacturing paradigm; Multiagent architecture; Socio-economic factor; Multi agent systems",2-s2.0-85032838777
"Wang X., Wei X., Gao S., Liu Y., Li Z.","A Novel Auction-Based Query Pricing Schema",2017,"International Journal of Parallel Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032791845&doi=10.1007%2fs10766-017-0534-x&partnerID=40&md5=d5cbb33b1fc01ab7318fb9640cadad90","As a common processing method, query is widely used in many areas, such as graph processing, machine learning, statistics. However, queries are usually priced according to vendor-specified fixed views (API) or number of transactions, which ignores query heterogeneity(computing resource consumption for query and information that the answer brings) and violates the microeconomic principles. In this work we study the relational query pricing problem and design efficient auctions by taking into account both information (i.e., data) value and query resource consumption. Different from the existing query pricing schemes, query auction determines data prices that reflect the demand–supply of shared computing resources and information value (i.e., price discovery). We target query auction that runs in polynomial time and achieves near-optimal social welfare with a good approximation ratio, while elicits truthful bids from consumers. Towards these goals, we adapt the posted pricing framework in game-theoretic perspective by casting the query auction design into an Integer Linear Programming problem, and design a primal-dual algorithm to approximate the NP-hard optimization problem. Theoretical analysis and empirical studies driven by a real-world data market benchmark verify the efficiency of our query auction schema. © 2017 Springer Science+Business Media, LLC","Auction design; Query pricing; Query processing","Costs; Game theory; Integer programming; Learning systems; Optimization; Polynomial approximation; Processing; Approximation ratios; Auction design; Game-theoretic perspectives; Integer Linear Programming; Optimization problems; Primal dual algorithms; Relational queries; Resource consumption; Query processing",2-s2.0-85032791845
"Chatzikonstantinou I., Sariyildiz I.S.","Addressing design preferences via auto-associative connectionist models: Application in sustainable architectural Façade design",2017,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028565910&doi=10.1016%2fj.autcon.2017.08.007&partnerID=40&md5=9ae44d13568cc386bb8277c948d5a685","Truly successful designs are characterized by both satisfaction of design goals and the presence of desirable physical features. Experienced design professionals are able to exercise their cognition to satisfy both aspects to a high degree. However, complex design tasks represent challenges for human cognition, and as such computational decision support systems emerge as a relevant topic. We present a computational decision support framework for treating preferences related to physical design features. The proposed framework is based on auto-associative machine learning models that inductively learn relationships between design features characterizing highly performing designs. The knowledge matter to be learned is derived through multi-objective stochastic optimization. The resulting auto-associative models are excited with a preference vector containing a favorable composition of design features. The models are able to alleviate those relationships that result in shortcomings of performance. The model thus outputs well performing design solution, where preferences pertaining to physical features are also satisfied, to the extent possible. The paper focuses on the applicability of the proposed approach in architectural design, as an exceptional example of complex design, discusses methods to evaluate model performance, and validates the proposed method through an application focusing on the design of a sustainable façade. © 2017","Architecture; Auto-associative model; Cognition; Daylight; Decision support; Energy; Façade design; Preferences","Architecture; Artificial intelligence; Daylighting; Decision support systems; Learning systems; Optimization; Associative models; Cognition; Decision supports; Energy; Preferences; Architectural design",2-s2.0-85028565910
"Ryu J., Kim D.-H.","Real-time gait subphase detection using an EMG signal graph matching (ESGM) algorithm based on EMG signals",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019592914&doi=10.1016%2fj.eswa.2017.05.006&partnerID=40&md5=dffa7555de144cb83f41542dc9206ecb","This study presents a gait subphase recognition method using an electromyogram (EMG) with a signal graph matching (ESGM) algorithm. Existing pattern recognition and machine learning using EMG signals has several innate problems in gait subphase detection. With respect to time domain features, their feature values may be analogous because two different gait steps may have similar muscle activation. In addition, the current gait subphase might not be recognized until the next gait subphase passes because the window size needed for feature extraction is larger than the period of the gait subphase. The ESGM algorithm is a new approach that compares reference EMG signals and input EMG signals according to time variance to solve these problems and considers variations of physiological muscle activity. We also determined all the elements of the ESGM algorithm using kinematic gait analysis and optimized the algorithm using experiments. Therefore, the ESGM algorithm reflects better timing characteristics of EMG signals than the time domain feature extraction algorithm. In addition, it can provide real-time and user-adaptive recognition of the gait subphase by using only EMG signals. Experimental results show that the average accuracy of the proposed method is 13% better than existing methods and the average detection latency of the proposed method was 5.5 times lower than existing methods. © 2017 Elsevier Ltd","EMG signal; EMG signal graph matching; Gait assistant robot; Gait subphase detection; Rehabilitation","Electromyography; Extraction; Feature extraction; Learning systems; Muscle; Patient rehabilitation; Pattern matching; Pattern recognition; Time domain analysis; Assistant robot; EMG signal; Kinematic gait analysis; Recognition methods; Sub-phase; Time domain feature extraction; Time domain features; Timing characteristics; Biomedical signal processing",2-s2.0-85019592914
"Frey R.M., Xu R., Ammendola C., Moling O., Giglio G., Ilic A.","Mobile recommendations based on interest prediction from consumer's installed apps–insights from a large-scale field study",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027970995&doi=10.1016%2fj.is.2017.08.006&partnerID=40&md5=ce6cf2e552cd1fc1e8ca530f59b589a5","Recommender systems are essential in mobile commerce to benefit both companies and individuals by offering highly personalized products and services. One key pre-requirement of applying such systems is to gain decent knowledge about each individual consumer through user profiling. However, most existing profiling approaches on mobile suffer problems such as non-real-time, intrusive, cold-start, and non-scalable, which prevents them from being adopted in reality. To tackle the problems, this work developed real-time machine-learning models to predict user profiles of smartphone users from openly accessible data, i.e. app installation logs. Results from a study with 904 participants showed that the models are able to predict interests on average 48.81% better than a random guess in terms of precision and 13.80% better in terms of recall. Since the effectiveness of such predictive models is unknown in practice, the predictive models were evaluated in a large-scale field experiment with 73,244 participants. Results showed that by leveraging our models, personalized mobile recommendations can be enabled and the corresponding click-through-rate can be improved by up to 228.30%. Supplementary information, study data, and software can be found at https://www.autoidlabs.ch/mobile-analytics. © 2017 Elsevier Ltd","Click-through rate; M-commerce; Mobile recommender system; Personalization; User profiling","Commerce; Forecasting; Learning systems; Recommender systems; Click-through rate; M-commerce; Mobile recommender systems; Personalizations; User profiling; Mobile commerce",2-s2.0-85027970995
"Deng C., Li C., Zhu Z., Lin W., Xi L.","Evaluating the impacts of atmospheric correction, seasonality, environmental settings, and multi-temporal images on subpixel urban impervious surface area mapping with Landsat data",2017,"ISPRS Journal of Photogrammetry and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031095970&doi=10.1016%2fj.isprsjprs.2017.09.015&partnerID=40&md5=14153f6a1df9648e81944fa0316cb385","Due to the heterogeneity of urban environments, subpixel urban impervious surface mapping is a challenging task in urban environmental studies. Factors, such as atmospheric correction, climate conditions, seasonal effect, urban settings, substantially affect fractional impervious surface estimation. Their impacts, however, have not been well studied and documented. In this research, we performed direct and comprehensive examinations to explore the impacts of these factors on subpixel estimation when using an effective machine learning technique (Random Forest) and provided solutions to alleviate these influences. Four conclusions can be drawn based on the repeatable experiments in three study areas under different climate conditions (humid continental, tropical monsoon, and Mediterranean climates). First, the performance of subpixel urban impervious surface mapping using top-of-atmosphere (TOA) reflectance imagery is comparable to, and even slightly better than, the surface reflectance imagery provided by U.S. Geological Services in all seasons and in all testing regions. Second, the effect of images with leaf-on/off season varies, and is contingent upon different climate regions. Specifically, humid continental areas may prefer the leaf-on imagery (e.g., summer), while the tropical monsoon and Mediterranean regions seem to favor the fall and winter imagery. Third, the overall estimation performance in the humid continental area is somewhat better than the other regions. Finally, improvements can be achieved by using multi-season imagery, but the increments become less obvious when including more than two seasons. The strategy and results of this research could improve and accommodate regional/national subpixel land cover mapping using Landsat images for large-scale environmental studies. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Atmospheric correction; Impervious surface; Landsat; Multi-temporal images; Random forest; Seasonality","Atmospheric thermodynamics; Decision trees; Image enhancement; Learning systems; Pixels; Reflection; Atmospheric corrections; Impervious surface; LANDSAT; Multi-temporal image; Random forests; Seasonality; Mapping",2-s2.0-85031095970
"Liu K., Liu B.","Optimization of smooth blasting parameters for mountain tunnel construction with specified control indices based on a GA and ISVR coupling algorithm",2017,"Tunnelling and Underground Space Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032741950&doi=10.1016%2fj.tust.2017.09.007&partnerID=40&md5=839961131eefa711d509ca26f365c43a","The smooth blasting method has been widely used in the construction of mountain tunnels to decrease the volume of overbreak or underbreak and maintain the tunnel outline in the design shape. However, due to the shortcomings of existing optimization theories and the complexity of rock masses, optimizing the smooth blasting parameters in arbitrary geological conditions with specified control indices is challenging. Eighteen on-site smooth blasting experiments were conducted during the construction of the long Foling highway tunnel in China. These experimental data were used as the training samples for machine learning. By training these samples, an improved support vector regression (ISVR) model was proposed to map the relation between the inputs, comprising the geological conditions (the basic quality [BQ] grade of the rock mass, saturated uniaxial compression strength of rock, and overburden depth) and control indices and the outputs of the smooth blasting parameters, including the spacing of perimeter holes and relief holes, minimum burden and linear charge concentration of perimeter holes. A genetic algorithm (GA) was coupled with an ISVR algorithm to automatically search the optimal parameters of the ISVR model during the training process. Using the ISVR model, the optimization of smooth blasting parameters can be obtained based on certain geological conditions of surrounding rock and specified control indices, including the crown settlement, thickness of the blasting damage zone (BDZ) in which the travelling velocity of ultrasonic waves is reduced significantly due to explosive vibration, volume of overbreak or underbreak, and radial decoupling ratio. According to the application results of the Foling tunnel, the ISVR model was shown to be superior since it can outperform certain existing models. As geological conditions and control indices are comprehensively considered, the proposed ISVR model of smooth blasting parameters is expected to be more feasible and reliable and is thus recommended for use in similar tunnel projects. © 2017 Elsevier Ltd","Field test; Mountain tunnel; Optimization with specified control indices; Smooth blasting; Support vector regression","Blasting; Electric charge; Explosives; Genetic algorithms; Geology; Landforms; Learning systems; Optimization; Parameter estimation; Quality control; Railroad tunnels; Rock mechanics; Rocks; Site selection; Control index; Field test; Mountain tunnels; Smooth blasting; Support vector regression (SVR); Tunnels",2-s2.0-85032741950
"Akhatou I., Sayago A., González-Domínguez R., Fernández-Recamales Á.","Application of Targeted Metabolomics to Investigate Optimum Growing Conditions to Enhance Bioactive Content of Strawberry",2017,"Journal of Agricultural and Food Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032634921&doi=10.1021%2facs.jafc.7b03701&partnerID=40&md5=5a4de6f66ca78d545413c8d2b096c6cb","A simple, sensitive, and rapid assay based on liquid chromatography coupled to tandem mass spectrometry was designed for simultaneous quantitation of secondary metabolites in order to investigate the influence of variety and agronomic conditions on the biosynthesis of bioactive compounds in strawberry. For this purpose, strawberries belonging to three varieties with different sensitivity to environmental conditions ('Camarosa', 'Festival', 'Palomar') were grown in a soilless system under multiple agronomic conditions (electrical conductivity, substrate type, and coverage). Targeted metabolomic analysis of polyphenolic compounds, combined with advanced chemometric methods based on learning machines, revealed significant differences in multiple bioactives, such as chlorogenic acid, ellagic acid rhamnoside, sanguiin H10, quercetin 3-O-glucuronide, catechin, procyanidin B2, pelargonidin 3-O-glucoside, cyanidin 3-O-glucoside, and pelargonidin 3-O-rutinoside, which play a pivotal role in organoleptic properties and beneficial healthy effects of these polyphenol-rich foods. © 2017 American Chemical Society.","agronomic practices; phenolic compounds; strawberry; targeted metabolomics; UHPLC-ESI-MS/MS; variety","Agronomy; Biochemistry; Chromatography; Flavonoids; Learning systems; Liquid chromatography; Mass spectrometry; Metabolites; Phenols; Plants (botany); Polyphenolic compounds; Agronomic practices; ESI-MS/MS; Metabolomics; Phenolic compounds; strawberry; variety; Fruits",2-s2.0-85032634921
"Chen S., Hu C.","Estimating sea surface salinity in the northern Gulf of Mexico from satellite ocean color measurements",2017,"Remote Sensing of Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029385816&doi=10.1016%2fj.rse.2017.09.004&partnerID=40&md5=283febba5316816de1bbfeee1cfdefe7","Sea surface salinity (SSS) is an important parameter to characterize physical and biogeochemical processes, yet its remote estimation in coastal waters has been difficult because satellite sensors designed to “measure” SSS lack sufficient resolution and coverage, and higher-resolution ocean color measurements suffer from optical and biogeochemical complexity when used to estimate SSS. In the northern Gulf of Mexico (GOM), this challenge is addressed through modeling, validation, and extensive tests in contrasting environments. Specifically, using extensive SSS datasets collected by many groups spanning &gt; 10 years and MODIS (Moderate Resolution Imaging Spectroradiometer) and SeaWiFS (Sea-Viewing Wide Field-of-View Sensor) estimated remote sensing reflectance (Rrs) at 412, 443, 488 (490), 555, and 667 (670) nm and sea surface temperature (SST), a multilayer perceptron neural network-based (MPNN) SSS model has been developed and validated with a spatial resolution of ~ 1 km. The MPNN was selected over many other empirical approaches such as principle component analysis (PCA), multi-nonlinear regression (MNR), decision tree, random forest, and supporting vector machines (SVMs) after extensive evaluations. The MPNN was trained by a back-propagation learning technique with Levenberg-Marquardt optimization and Bayesian regularization. The model showed an overall performance of root mean square error (RMSE) = 1.2, with coefficient of determination (R2) = 0.86, mean bias (MB) = 0.0, and mean ratio (MR) = 1.0 for SSS ranging between ~ 1 and ~ 37 (N = 3640). Validation using an independent dataset showed a RMSE of 1.1, MB of 0.0, and MR of 1.0 for SSS ranging between ~ 27 and ~ 37 (N = 412). The model with its original parameterization has been tested in the Mississippi-Atchafalaya coastal region, Florida's Big Bend region, and in the offshore Mississippi River plume, with satisfactory performance obtained in each case. Comparison with concurrent Aquarius-derived SSS maps (110-km resolution) showed similar agreement in offshore waters as indicated above, but the new 1-km resolution SSS maps revealed more finer-scale features as well as salinity gradients in coastal waters. The sensitivity of the model to realistic model input errors in satellite-derived SST and Rrs was also thoroughly examined, with uncertainties in the model-derived SSS being always &lt; 1 for SSS &gt; 30. The extensive validation, evaluation, and sensitivity test all indicated the robustness of the MPNN model in estimating SSS in most, if not all, coastal waters and offshore plumes in the northern GOM. Thus, the model provided a basis for generating near real-time 1-km resolution SSS maps from satellite measurements. However, the model showed limitations when applied to regions with known algal blooms or upwelling as they both led to low Rrs in the blue bands that may be falsely recognized as caused by low SSS. © 2017","Gulf of Mexico; MODIS; Neural network; Remote sensing reflectance; Sea surface salinity; SeaWiFS","Backpropagation; Band structure; Biogeochemistry; Colorimetry; Decision trees; Image reconstruction; Imaging techniques; Mean square error; Neural networks; Principal component analysis; Radiometers; Reflection; Remote sensing; Satellite imagery; Satellites; Surface waters; Gulf of Mexico; MODIS; Remote-sensing reflectance; Sea surface salinity; SeaWiFs; Oceanography; artificial neural network; biogeochemical cycle; data set; model validation; MODIS; ocean color; optimization; reflectance; remote sensing; satellite sensor; sea surface salinity; SeaWiFS; support vector machine; Atlantic Ocean; Gulf of Mexico; algae",2-s2.0-85029385816
"Zhang J., Du J., Zhang S., Liu D., Hu Y., Hu J., Wei S., Dai L.","Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023205741&doi=10.1016%2fj.patcog.2017.06.017&partnerID=40&md5=d9fa09db32c7144d5375ee62856be910","Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize HMEs in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes HME images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the CROHME international competition. Using the official training dataset, WAP significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55% on CROHME 2014 and 44.55% on CROHME 2016. © 2017 Elsevier Ltd","Attention; Handwritten mathematical expression recognition; Neural network","Character recognition; Competition; Deep learning; Education; Latexes; Neural networks; Pattern recognition; Recurrent neural networks; Watches; Attention; Convolutional neural network; Expression recognition; International competitions; Mathematical expressions; Network-based approach; State-of-the-art methods; Two-dimensional structures; Deep neural networks",2-s2.0-85023205741
"Bilen H., Fernando B., Gavves E., Vedaldi A.","Action Recognition with Dynamic Image Networks",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032735055&doi=10.1109%2fTPAMI.2017.2769085&partnerID=40&md5=7c7c21ed9c7e40d4f19d4e57947f7de7","We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of &#x0027;rank pooling&#x0027;. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. We call the resulting representation dynamic image because it summarizes the video dynamics in addition to appearance. This powerful idea allows to convert any video to an image so that existing CNN models pre-trained with still images can be immediately extended to videos. We also present an efficient approximate rank pooling operator that runs two orders of magnitude faster than the standard ones with any loss in ranking performance and can be formulated as a CNN layer. To demonstrate the power of the representation, we introduce a novel four stream CNN architecture which can learn from RGB and optical flow frames as well as from their dynamic image representations. We show that the proposed network achieves state-of-the-art performance, 95.5% and 72.5% accuracy, in the UCF101 and HMDB51 respectively. IEEE","convolutional neural networks; deep learning; human action classification; motion representation; video classification","Convolution; Deep learning; Neural networks; Compact representation; Convolutional neural network; Human action classifications; Motion representation; Orders of magnitude; Ranking performance; State-of-the-art performance; Video classification; Optical flows",2-s2.0-85032735055
"Duriez T., Brunton S.L., Noack B.R.","Future developments",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994559671&doi=10.1007%2f978-3-319-40624-4_8&partnerID=40&md5=b1de2e0566ac240625325626ce5f04d1","This chapter anticipates future developments in machine learning control (MLC), a rapidly evolving discipline with applications of epic proportions. First, methodological advantages are considered. Big data and machine learning offer a huge opportunities to learn the optimal control faster and to address more complex problems, e.g. time-varying plants and high-dimensional actuation and sensing. Second, we outline high-impact engineering applications of MLC in turbulence and nonlinear control. These applications may range from everyday life to transport and industrial production. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994559671
"Duriez T., Brunton S.L., Noack B.R.","Taming nonlinear dynamics with MLC",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994558878&doi=10.1007%2f978-3-319-40624-4_5&partnerID=40&md5=e7674fa72110425a2a0edba91cbf129c","We investigate the application of machine learning control (MLC) to the stabilization of a nonlinear dynamical system. This plant features a frequently observed frequency crosstalk between actuation and unstable dynamics. MLC explores and exploits this frequency crosstalk as an enabling actuation mechanism. MLC-based feedback is benchmarked against open- and closed-loop forcing derived from Kryloff-Bogoliubov approximation. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994558878
"Duriez T., Brunton S.L., Noack B.R.","Taming real world flow control experiments with MLC",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994614048&doi=10.1007%2f978-3-319-40624-4_6&partnerID=40&md5=d743d04eb53b365416fa0bd6743913c5","In this chapter, we present applications of machine learning control (MLC) to flow control experiments. Examples range from mixing enhancement of laminar flow to separation mitigation of a turbulent boundary layer. The discussion highlights the physical actuation mechanisms, challenges of alternative model-based control and enabling implementations of MLC in the data aquisition system. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994614048
"Duriez T., Brunton S.L., Noack B.R.","Introduction",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994571293&doi=10.1007%2f978-3-319-40624-4_1&partnerID=40&md5=73fb32d62009e47ffc08c99c5240fa1c","This chapter provides an introduction to machine learning control (MLC), a surprisingly simple model-free technology to tame complex nonlinear systems. We identify the need for MLC in the context of the benefits and challenges of existing feedback control strategies, with motivation from the grand challenge problem of feedback turbulence control. MLC provides a powerful new framework to control complex dynamical systems that are currently beyond the capability of existing methods in control. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994571293
"Duriez T., Brunton S.L., Noack B.R.","Benchmarking MLC against linear control",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994609099&doi=10.1007%2f978-3-319-40624-4_4&partnerID=40&md5=3833f8c01982f27ec08697a8aa756c81","In this chapter, we demonstrate the use of genetic programming for machine learning control (MLC) on linear systems where optimal control laws are known. In particular, we benchmark MLC against the linear quadratic regulator (LQR) for full-state feedback and the Kalman filter for full-state estimation, providing code for each example. MLC is able to identify the optimal linear control solutions and outperforms linear control even for small nonlinearity. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994609099
"Feder S., Sundermann B., Wersching H., Teuber A., Kugel H., Teismann H., Heindel W., Berger K., Pfleiderer B.","Sample heterogeneity in unipolar depression as assessed by functional connectivity analyses is dominated by general disease effects",2017,"Journal of Affective Disorders",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021644155&doi=10.1016%2fj.jad.2017.06.055&partnerID=40&md5=b508fe4db7c244c79b137c3108b9e474","Objectives Combinations of resting-state fMRI and machine-learning techniques are increasingly employed to develop diagnostic models for mental disorders. However, little is known about the neurobiological heterogeneity of depression and diagnostic machine learning has mainly been tested in homogeneous samples. Our main objective was to explore the inherent structure of a diverse unipolar depression sample. The secondary objective was to assess, if such information can improve diagnostic classification. Materials and methods We analyzed data from 360 patients with unipolar depression and 360 non-depressed population controls, who were subdivided into two independent subsets. Cluster analyses (unsupervised learning) of functional connectivity were used to generate hypotheses about potential patient subgroups from the first subset. The relationship of clusters with demographical and clinical measures was assessed. Subsequently, diagnostic classifiers (supervised learning), which incorporated information about these putative depression subgroups, were trained. Results Exploratory cluster analyses revealed two weakly separable subgroups of depressed patients. These subgroups differed in the average duration of depression and in the proportion of patients with concurrently severe depression and anxiety symptoms. The diagnostic classification models performed at chance level. Limitations It remains unresolved, if subgroups represent distinct biological subtypes, variability of continuous clinical variables or in part an overfitting of sparsely structured data. Conclusions Functional connectivity in unipolar depression is associated with general disease effects. Cluster analyses provide hypotheses about potential depression subtypes. Diagnostic models did not benefit from this additional information regarding heterogeneity. © 2017 Elsevier B.V.","Cluster-analysis; Depression; Diagnostic classification; Functional connectivity; Heterogeneity; Subtypes","antidepressant agent; immunomodulating agent; monoamine oxidase inhibitor; serotonin uptake inhibitor; adult; Article; classification; clinical assessment; clinical outcome; cluster analysis; controlled study; demography; female; functional connectivity; human; major clinical study; major depression; male; mental patient; priority journal; randomized controlled trial",2-s2.0-85021644155
"Duriez T., Brunton S.L., Noack B.R.","MLC tactics and strategy",2017,"Fluid Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994589826&doi=10.1007%2f978-3-319-40624-4_7&partnerID=40&md5=07507aa7b742bd078ab8f8ad2c07fb59","In this chapter, we provide good practices for applying machine learning control (MLC) to a real-world flow control experiment. The recipes include common experimental challenges, like defining a cost function, implementing MLC on the computer, and dealing with imperfect plants, actuation and sensing. In addition, we show how MLC can learn faster by preconditioning the control problem and by planning, monitoring and post-processing the experimental campaign. Most of the advice is formulated for the non-ideal flow control experiment, but is easily applicable for any other real-world application. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994589826
"Akyol K., Gültepe Y.","A Study on Liver disease diagnosis based on assessing the importance of attributes",2017,"International Journal of Intelligent Systems and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032267476&doi=10.5815%2fijisa.2017.11.01&partnerID=40&md5=d374ed0ede105637933116dd8066c5a3","Liver is a needful body organ that forms an important barrier between the gastrointestinal blood, which contains large amounts of toxins, and antigens. Liver diseases contain hepatitis B and hepatitis C virus infections, alcoholic liver disease, nonalcoholic fatty liver disease and associated cirrhosis, liver failure and hepatocellular carcinoma are primary causes of death. The main purpose of this study is to investigate which attributes are important for effective diagnosis of liver disorders by performing the machine learning approach based on the combination of Stability Selection and Random Forest methods. In order to generate more accuracy, dataset was balanced by utilizing the Random Under-Sampling method. Important ones in all attributes were detected by utilizing the Stability Selection method which was performed on sub-datasets, which were obtained with 5 fold cross-validation technique. By sending these datasets to the Random Forest algorithm, the performance of the proposed approach was evaluated within the frame of accuracy and sensitive metrics. The experimental results clearly show that the Random Under-Sampling method can potentially improve the performance of the combination of Stability Selection and Random Forest methods in machine learning. And, the combination of these methods provides new perspectives for the diagnosis of this disease and other medical diseases. © 2017 MECS.","Classification; Liver disease; Random forest; Stability selection; Under-sampling",,2-s2.0-85032267476
"Bi S., Wang Q., Sun M., Wang J.","Automatic monolayer identification based on genetic neural network",2017,"Journal of Medical Imaging and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030118342&doi=10.1166%2fjmihi.2017.2174&partnerID=40&md5=2f9c227be13d5ed294c210c1207aa7be","Peripheral blood cell morphology examination plays important role in disease diagnosis. For recent years automatic microscopic examination becomes a trend in medical’s development. Meanwhile, the automatic recognition of monolayer in blood smear is the basis for automated blood cell morphology test. In this paper, an automatic recognition method is proposed, which is based on image processing and machine learning. The method consists of three steps: Firstly, six characters to describe monolayer image are extracted as the input for machine learning. Secondly, genetic algorithm is utilized to optimize neural network weight and threshold parameters. Finally, a 3-layer back propagation neural net is built with the given optimal parameters in order to identify monolayer image. 667 typical labeled images have been collected for training of back propagation neural net, and another 849 images for test. The experimental results show that the recognition rate of the training set is 90.1%, and that of the test set is up to 94.1%. Copyright © 2017 American Scientific Publishers All rights reserved.","Back propagation neural network; Blood cell morphology; Feature extraction; Genetic algorithm; Image processing; Monolayer",,2-s2.0-85030118342
"Romanowski A., Skuza M.","Towards predicting stock price moves with aid of sentiment analysis of Twitter social network data and big data processing environment",2017,"Studies in Computational Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994613999&doi=10.1007%2f978-3-319-47208-9_7&partnerID=40&md5=cf4a107fb1d5abb4e92c70a18e828edf","This chapter illustrates design and evaluation of a sentiment analysis based system that may be used to predict future stock prices. Social media information is processed in order to extract opinions that are associated with Apple Inc. company. The authors took advantage of large datasets available from Twitter micro blogging platform and widely available stock market records. Data was collected during 3 months and processed for further analysis. Machine learning was employed to conduct sentiment classification of data in order to estimate future stock prices. Calculations were performed in distributed environment according to Map Reduce programming model. Evaluation and discussion of predictions results for different time intervals and input datasets is discussed in terms of efficiency and feasibility of the chosen approach. © Springer International Publishing AG 2017.","Big data processing; Sentiment analysis; Social networks analysis; Stock market prediction",,2-s2.0-84994613999
"Kim H.-I., Kim J.-B., Park R.-H.","Efficient and Fast Iris Localization Using Binary Radial Gradient Features for Human-Computer Interaction",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022325878&doi=10.1142%2fS0218001417560158&partnerID=40&md5=c53c2432e1c3302fe659fc5403dfe2a9","This paper proposes an efficient and fast iris localization method. It uses support vector machine learning of iris features that represent closed outer and inner iris boundaries encompassing a low-intensity region. In addition, depending on the location of the iris in an eye image, an iris detection method is proposed based on three sub-datasets of eye images (middle, right, and left sub-datasets) with different iris features. The proposed method is implemented using fast sliding window and fast computation of the iris detection score with binary features. Compared with state-of-the-art methods, experimental results show that the proposed method is twice as fast and has comparable accuracy, even when factoring in head rotation, glasses, and highlights. © 2017 World Scientific Publishing Company.","binary feature; fast sliding window; gaze estimation; human-computer interaction; Iris localization; radial gradient","Bins; Feature extraction; Binary features; Fast sliding; Gaze estimation; Iris localization; Radial gradient; Human computer interaction",2-s2.0-85022325878
"Slimene A., Zagrouba E.","Overlapping area hyperspheres for kernel-based similarity method",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012115758&doi=10.1007%2fs10044-017-0604-0&partnerID=40&md5=4fbd2f7506adcf78d9855c5bbcfdd221","Measuring similarity between sets of objects is a key step in a wide areas of machine learning. Popular examples include general classification framework and numerous applications in computer vision. In this paper, we propose a kernel-based similarity method which is inspired from an interesting biological behavior of trees and induced mathematically by formulating it as a quadratic optimization problem in a reproducing kernel Hilbert space (RKHS). The proposed method is compared to the maximum mean discrepancy, a recent and challenging kernel similarity method. We conduct and present several numerical experiments on synthetic data as well as real-word image data. The proposed method yields favorable performances in terms of classification performances in the context of supervised classification tasks on the challenging Caltech101 dataset and other datasets such as USPS and ETH80. Furthermore, the efficiency of the proposed method in the context of image segmentation through unsupervised clustering of superpixels has been also asserted. © 2017, Springer-Verlag London.","Image classification; Kernel methods; Overlapping hyperspheres; Similarity methods",,2-s2.0-85012115758
"Sudha M.","Evolutionary and Neural Computing Based Decision Support System for Disease Diagnosis from Clinical Data Sets in Medical Practice",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029931757&doi=10.1007%2fs10916-017-0823-3&partnerID=40&md5=0c406e5de2ef221753c68ef9cc56cf1b","As a recent trend, various computational intelligence and machine learning approaches have been used for mining inferences hidden in the large clinical databases to assist the clinician in strategic decision making. In any target data the irrelevant information may be detrimental, causing confusion for the mining algorithm and degrades the prediction outcome. To address this issue, this study attempts to identify an intelligent approach to assist disease diagnostic procedure using an optimal set of attributes instead of all attributes present in the clinical data set. In this proposed Application Specific Intelligent Computing (ASIC) decision support system, a rough set based genetic algorithm is employed in pre-processing phase and a back propagation neural network is applied in training and testing phase. ASIC has two phases, the first phase handles outliers, noisy data, and missing values to obtain a qualitative target data to generate appropriate attribute reduct sets from the input data using rough computing based genetic algorithm centred on a relative fitness function measure. The succeeding phase of this system involves both training and testing of back propagation neural network classifier on the selected reducts. The model performance is evaluated with widely adopted existing classifiers. The proposed ASIC system for clinical decision support has been tested with breast cancer, fertility diagnosis and heart disease data set from the University of California at Irvine (UCI) machine learning repository. The proposed system outperformed the existing approaches attaining the accuracy rate of 95.33%, 97.61%, and 93.04% for breast cancer, fertility issue and heart disease diagnosis. © 2017, Springer Science+Business Media, LLC.","Clinical decision support; Disease prediction; Feature reduction and hybrid computing; Genetic algorithm; Neural network; Rough set",,2-s2.0-85029931757
"Mirzamomen Z., Kangavari M.R.","A framework to induce more stable decision trees for pattern classification",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961218092&doi=10.1007%2fs10044-016-0542-2&partnerID=40&md5=8c81592a3b7d6d14801474177d6dbe64","Decision tree learning algorithms are known to be unstable, such that small changes in the training data can result in highly different output models. Instability is an important issue in the context of machine learning which is usually overlooked. In this paper, we illustrate and discuss the problem of instability of decision tree induction algorithms and propose a framework to induce more stable decision trees. In the proposed framework, the split test encompasses two advantageous properties: First, it is able to contribute multiple attributes. Second, it has a polylithic structure. The first property alleviates the race between the competing attributes to be installed at an internal node, which is the major cause of instability. The second property has the potential of improving the stability by providing the locality of the effect of the instances on the split test. We illustrate the effectiveness of the proposed framework by providing a complying decision tree learning algorithm and conducting several experiments. We have evaluated the structural stability of the algorithms by employing three measures. The experimental results reveal that the decision trees induced by the proposed framework exhibit great stability and competitive accuracy in comparison with several well-known decision tree learning algorithms. © 2016, Springer-Verlag London.","Decision tree; Split measure; Split test; Stability",,2-s2.0-84961218092
"Jaquess K.J., Gentili R.J., Lo L.-C., Oh H., Zhang J., Rietschel J.C., Miller M.W., Tan Y.Y., Hatfield B.D.","Empirical evidence for the relationship between cognitive workload and attentional reserve",2017,"International Journal of Psychophysiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029838329&doi=10.1016%2fj.ijpsycho.2017.09.007&partnerID=40&md5=6d9060e8d89086c2a8323807e99a1279","While the concepts of cognitive workload and attentional reserve have been thought to have an inverse relationship for some time, such a relationship has never been empirically tested. This was the purpose of the present study. Aspects of the electroencephalogram were used to assess both cognitive workload and attentional reserve. Specifically, spectral measures of cortical activation were used to assess cognitive workload, while amplitudes of the event-related potential from the presentation of unattended “novel” sounds were used to assess attentional reserve. The relationship between these two families of measures was assessed using canonical correlation. Twenty-seven participants performed a flight simulator task under three levels of challenge. Verification of manipulation was performed using self-report measures of task demand, objective task performance, and heart rate variability using electrocardiography. Results revealed a strong, negative relationship between the spectral measures of cortical activation, believed to be representative of cognitive workload, and ERP amplitudes, believed to be representative of attentional reserve. This finding provides support for the theoretical and intuitive notion that cognitive workload and attentional reserve are inversely related. The practical implications of this result include improved state classification using advanced machine learning techniques, enhanced personnel selection/recruitment/placement, and augmented learning/training. © 2017 Elsevier B.V.","Attention; EEG; ERP; Workload","adult; Article; attention; cognitive reserve; electrocardiography; electroencephalography; event related potential; female; heart rate variability; human; male; normal human; simulator; task performance; young adult",2-s2.0-85029838329
"Laurenziello M., Montaruli G., Gallo C., Tepedino M., Guida L., Perillo L., Troiano G., Muzio L.L., Ciavarella D.","Determinants of maxillary canine impaction: Retrospective clinical and radiographic study",2017,"Journal of Clinical and Experimental Dentistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032825127&doi=10.4317%2fjced.54095&partnerID=40&md5=41f2ef1f439f4958d8d9518cb43e4811","Background: The aim of this study is to evaluate determinants of maxillary canine impaction taking into account both canine position related variables and the pattern of facial growth. Material and Methods: A retrospective clinical and radiographic analysis was carried out on 109 patients aged between 9 and 10 years at the time of first evaluation. At baseline, SN-GoMe angle, the interincisal angle, the canine angle a and the canine distance d were used to characterize canine location and vertical facial growth. At the end of a two years follow up period the eruption state of each canine of each patient was recorded and accordingly classified as erupted or impacted on a clinical and radiographic basis. Univariate and multivariate statistical analyses were performed, including correlation among the studied variables and principal components analysis; several machine learning methods were also used in order to built a predictive model. Results: At the end of the two years follow up period after the first examination, 54 (24.77%) canines were classified as impacted. Except for Angle a values, there were no statistically significant differences between impacted and erupted canines. The studied variables were not significantly correlated, except for the SN-GoMe Angle and the distance d in the impacted canine group and the angle a and the distance d in erupted canines group. All variables, except for SN-GoMe Angle in erupted canines, have a partial communality with the first two principal components greater than 50%. Among the learning machine methods tested to classify data, the best performance was obtained by the random forest method, with an overall accuracy in predicting canine eruption of 88.3%. Conclusions: The studied determinants are easy to perform measurements on 2D routinely executed radiographic images; they seems independently related to canine impaction and have reliable accuracy in predicting maxillary canine eruption. © Medicina Oral S.L. C.I.F. B.","Canine impaction; Determinants; Facial growth",,2-s2.0-85032825127
"Bauer A.J., Just M.A.","A brain-based account of “basic-level” concepts",2017,"NeuroImage",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027836916&doi=10.1016%2fj.neuroimage.2017.08.049&partnerID=40&md5=b00d6d6ddc71f4b787eddaf34d69769e","This study provides a brain-based account of how object concepts at an intermediate (basic) level of specificity are represented, offering an enriched view of what it means for a concept to be a basic-level concept, a research topic pioneered by Rosch and others (Rosch et al., 1976). Applying machine learning techniques to fMRI data, it was possible to determine the semantic content encoded in the neural representations of object concepts at basic and subordinate levels of abstraction. The representation of basic-level concepts (e.g. bird) was spatially broad, encompassing sensorimotor brain areas that encode concrete object properties, and also language and heteromodal integrative areas that encode abstract semantic content. The representation of subordinate-level concepts (robin) was less widely distributed, concentrated in perceptual areas that underlie concrete content. Furthermore, basic-level concepts were representative of their subordinates in that they were neurally similar to their typical but not atypical subordinates (bird was neurally similar to robin but not woodpecker). The findings provide a brain-based account of the advantages that basic-level concepts enjoy in everyday life over subordinate-level concepts: the basic level is a broad topographical representation that encompasses both concrete and abstract semantic content, reflecting the multifaceted yet intuitive meaning of basic-level concepts. © 2017 Elsevier Inc.","Basic level; fMRI; Level of abstraction; MVPA; Neural representation; Object concept",,2-s2.0-85027836916
"Vázquez Doce O.","Measurements of the dielectron continuum in pp, p-Pb and Pb-Pb collisions with ALICE at the LHC",2017,"Nuclear Physics A",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032014131&doi=10.1016%2fj.nuclphysa.2017.04.037&partnerID=40&md5=dc47353969cdef5762b9d3d849b2f42a","Dielectrons produced in ultra-relativistic heavy-ion collisions provide a unique probe of the whole system evolution as they are unperturbed by final-state interactions. The dielectron continuum is extremely rich in physics sources: thermal radiation is of particular interest as it carries information about the temperature of the hot and dense system created in such collisions. The dielectron invariant mass distribution is sensitive to medium modifications of the spectral function of vector mesons that are linked to the potential restoration of chiral symmetry. Correlated electron pairs from semi-leptonic charm and beauty decays provide information about the heavy-quark energy loss. A summary of the LHC Run-1 preliminary results in all three collisions systems (pp, p-Pb and Pb-Pb) is presented. Furthermore, the status of the ongoing Run-2 analyses is discussed with a focus on pp collisions collected with a high charged-particle multiplicity trigger, on new analysis methods to separate prompt from non-prompt sources, and on the usage of machine learning methods for background rejection. © 2017 The Author(s)","dielectrons; electromagnetic radiation; heavy flavour; heavy-ion collisions; QGP",,2-s2.0-85032014131
"Ray P.P., Dash D., De D.","A Systematic Review of Wearable Systems for Cancer Detection: Current State and Challenges",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030482290&doi=10.1007%2fs10916-017-0828-y&partnerID=40&md5=f13698dec619b785f5d3f389142d170a","Rapid growth of sensor and computing platforms have introduced the wearable systems. In recent years, wearable systems have led to new applications across all medical fields. The aim of this review is to present current state-of-the-art approach in the field of wearable system based cancer detection and identify key challenges that resist it from clinical adoption. A total of 472 records were screened and 11 were finally included in this study. Two types of records were studied in this context that includes 45% research articles and 55% manufactured products. The review was performed per PRISMA guidelines where considerations was given to records that were published or reported between 2009 and 2017. The identified records included 4 cancer detecting wearable systems such as breast cancer (36.3%), skin cancer (36.3%), prostate cancer (18.1%), and multi-type cancer (9%). Most works involved sensor based smart systems comprising of microcontroller, Bluetooth module, and smart phone. Few demonstrated Ultra-Wide Band (i.e. UWB) antenna based wearable systems. Skin cancer detecting wearable systems were most comprehensible ones. The current works are gradually progressing with seamless integration of sensory units along with smart networking. However, they lack in cloud computing and long-range communication paradigms. Artificial intelligence and machine learning are key ports that need to be attached with current wearable systems. Further, clinical inertia, lack of awareness, and high cost are altogether pulling back the actual growth of such system. It is well comprehended that upon sincere orientation of all identified challenges, wearable systems would emerge as vital alternative to futuristic cancer detection. © 2017, Springer Science+Business Media, LLC.","Cancer detection; Internet of things; Wearable devices; Wearable monitoring systems",,2-s2.0-85030482290
"Montoye A.H.K., Pivarnik J.M., Mudd L.M., Biswas S., Pfeiffer K.A.","Evaluation of the activPAL accelerometer for physical activity and energy expenditure estimation in a semi-structured setting",2017,"Journal of Science and Medicine in Sport",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018741652&doi=10.1016%2fj.jsams.2017.04.011&partnerID=40&md5=45f7202940f50f458cf6eee1165f05de","Objectives Evaluate accuracy of the activPAL and its proprietary software for prediction of time spent in physical activity (PA) intensities (sedentary, light, and moderate-to-vigorous) and energy expenditure (EE) and compare its accuracy to that of a machine learning model (ANN) developed from raw activPAL data. Design Semi-structured accelerometer validation in a laboratory setting. Methods Participants (n = 41 [20 male]; age = 22.0 ± 4.2) completed a 90-min protocol performing 13 activities for 3–10 min each and choosing activity order, duration, and intensity. Participants wore an activPAL accelerometer (right thigh) and a portable metabolic analyzer. Criterion measures of time spent in sedentary, light, and moderate-to-vigorous PA were determined using measured MET values of ≤1.5, 1.6–2.9, and ≥3.0, respectively. Estimated times in each PA intensity from the activPAL software and ANN were compared with the criterion using repeated measures ANOVA. Window-by-window EE prediction was assessed using correlations and root mean square error. Results activPAL software-estimated sedentary time was not different from the criterion, but light PA was overestimated (6.2 min) and moderate- to vigorous PA was underestimated (4.3 min). ANN-estimated sedentary time and light PA were not different from the criterion, but moderate- to vigorous PA was overestimated (1.8 min). For EE estimation, the activPAL software had lower correlations (r = 0.76 vs. r = 0.89) and higher error (1.74 vs. 1.07 METs) than the ANN. Conclusions The ANN had higher accuracy for estimation of EE and PA than the activPAL software in this semi-structured laboratory setting, indicating potential for the ANN to be used in PA assessment. © 2017 Sports Medicine Australia","Accelerometry; Activity monitor; Ambulatory; Health behavior; Indirect calorimetry; Sedentary behavior","accelerometer; adult; Article; artificial neural network; controlled study; energy expenditure; female; human; human experiment; male; measurement accuracy; normal human; physical activity; prediction; software; time; validation study",2-s2.0-85018741652
"Ahmadi Azqhandi M.H., Ghaedi M., Yousefi F., Jamshidi M.","Application of random forest, radial basis function neural networks and central composite design for modeling and/or optimization of the ultrasonic assisted adsorption of brilliant green on ZnS-NP-AC",2017,"Journal of Colloid and Interface Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024867995&doi=10.1016%2fj.jcis.2017.05.098&partnerID=40&md5=67a8a09baea9373b11d81913a377320e","Two machine learning approach (i.e. Radial Basis Function Neural Network (RBF-NN) and Random Forest (RF) was developed and evaluated against a quadratic response surface model to predict the maximum removal efficiency of brilliant green (BG) from aqueous media in relation to BG concentration (4–20 mg L−1), sonication time (2–6 min) and ZnS-NP-AC mass (0.010–0.030 g) by ultrasound-assisted. All three (i.e. RBF network, RF and polynomial) model were compared against the experimental data using four statistical indices namely, coefficient of determination (R2), root mean square error (RMSE), mean absolute error (MAE) and absolute average deviation (AAD). Graphical plots were also used for model comparison. The obtained results using RBF network and RF exhibit a better performance in comparison to classical statistical model for both dyes. The significant factors were optimized using desirability function approach (DFA) combined central composite design (CCD) and genetic algorithm (GA) approach. The obtained optimal point was located in the valid region and the experimental confirmation tests were conducted showing a good accordance between the predicted optimal points and the experimental data. The properties of ZnS-NPs-AC were identified by X-ray diffraction; field emission scanning electron microscopy, energy dispersive X-ray spectroscopy (EDS) and Fourier transformation infrared spectroscopy. Various isotherm models for fitting the experimental equilibrium data were studied and Langmuir model was chosen as an efficient model. Various kinetic models for analysis of experimental adsorption data were studied and pseudo second order model was chosen as an efficient model. Moreover, ZnS nanoparticles loaded on activated carbon efficiently were regenerated using methanol and after five cycles the removal percentage do not change significantly. © 2017 Elsevier Inc.","Brilliant green; Desirability function; Radial basis function neural networks; Random forest; Response surface methodology; Zinc sulfide nanoparticle","Activated carbon; Carbon; Decision trees; Field emission microscopes; Fourier transforms; Functions; Genetic algorithms; Infrared spectroscopy; Mean square error; Nanoparticles; Reforestation; Scanning electron microscopy; Surface properties; X ray diffraction; X ray spectroscopy; Zinc sulfide; Brilliant green; Desirability function; Radial basis function neural networks; Random forests; Response surface methodology; Zinc sulfide nanoparticles; Radial basis function networks; activated carbon; brilliant green; metal nanoparticle; zinc sulfide; absolute average deviation; adsorption; aqueous solution; Article; artificial neural network; central composite design; coefficient of determination; concentration (parameters); controlled study; energy dispersive X ray spectroscopy; field emission scanning electron microscopy; genetic algorithm; infrared spectroscopy; isotherm; mean absolute error; priority journal; radial basis function neural network; random forest; response surface method; root mean square error; statistical model; statistical parameters; ultrasound; X ray diffraction",2-s2.0-85024867995
"Israel A., Grossman E.","Elevated High-Density Lipoprotein Cholesterol Is Associated with Hyponatremia in Hypertensive Patients",2017,"American Journal of Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026221314&doi=10.1016%2fj.amjmed.2017.05.030&partnerID=40&md5=7c2732cff34b663f9a07a3a84f34e134","Background Recently, the Systolic Blood Pressure Intervention Trial (SPRINT) showed that intensive lowering of systolic blood pressure is beneficial, but is associated with more adverse events. Hyponatremia was notably more frequent in the intensive treatment group. Investigating its risk factors is crucial for preventing this complication. Our objective in this study was to identify risk factors for hyponatremia in the adult population. Methods We investigated the baseline demographic, clinical, and laboratory data from the 9361 participants of SPRINT to identify the best predictors of hyponatremia (serum sodium ≤130 mEq/L), and adverse events, which could be attributed to hyponatremia, using machine learning and multivariable Cox proportional hazards models. We confirmed our results in the independent National Health and Nutrition Examination Survey (NHANES) cohort between the years 2005 and 2010 (16,501 participants). Results Elevated baseline high-density lipoprotein cholesterol (HDL-C) was a strong predictor of future hyponatremia. Multivariable Cox regression showed hyponatremia events to be significantly increased for SPRINT participants with baseline HDL-C levels in the highest quintile (hazard ratio [HR] 2.8; 95% confidence interval [CI], 2.2-3.7; P <.001), and were also associated with treatment-related serious adverse events (HR 1.6; 95% CI, 1.3-2.1; P <.001). We confirmed the association between HDL-C and hyponatremia in the NHANES cohort (HR 2.5; 95% CI, 1.7-3.7; P <.001). Conclusions Elevated HDL-C (≥62 mg/dL) is a risk factor for hyponatremia. Thus, hypertensive patients with elevated HDL-C should be closely monitored for hyponatremia when treated for hypertension. © 2017 Elsevier Inc.","Antihypertensive treatment; Hypertension; Hyponatremia; Thiazide diuretics","antihypertensive agent; high density lipoprotein cholesterol; aged; blood; blood pressure; complication; drug effects; drug monitoring; female; human; hypertension; hyponatremia; male; medication therapy management; middle aged; predictive value; procedures; prognosis; proportional hazards model; risk factor; statistics and numerical data; Aged; Antihypertensive Agents; Blood Pressure; Cholesterol, HDL; Drug Monitoring; Female; Humans; Hypertension; Hyponatremia; Male; Medication Therapy Management; Middle Aged; Predictive Value of Tests; Prognosis; Proportional Hazards Models; Risk Factors",2-s2.0-85026221314
"Tong E., Patrie J., Tong S., Evans A., Michel P., Eskandari A., Wintermark M.","Time-resolved CT assessment of collaterals as imaging biomarkers to predict clinical outcomes in acute ischemic stroke",2017,"Neuroradiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028744948&doi=10.1007%2fs00234-017-1914-z&partnerID=40&md5=6e678ce7068ef21280cf7bfc477fc69e","Purpose: Collateral circulation plays a pivotal role in the pathophysiology of acute ischemic stroke and is increasingly recognized as a promising biomarker for predicting the clinical outcome. However, there is no single established grading system. We designed a novel machine-learning software that allows non-invasive, objective, and quantitative assessment of collaterals according to their vascular territories. Our goal is to investigate the prognostic and predictive value of this collateral score for the prediction of acute stroke outcome. Methods: This is a retrospective study of 135 patients with anterior circulation stroke treated with IV TPA. An equation using this collateral score (adjusting for age, baseline NIHSS, and recanalization) was derived to predict the clinical outcome (90-day mRS). The primary analyses focused on determining the prognostic value of our newly developed collateral scores. Secondary analyses examined the interrelationships between the collateral score and other variables. Results: The collateral score emerged as a statistically significant prognostic biomarker for good clinical outcome (p < 0.033) among recanalized patients, but not among non-recanalized patients (p < 0.497). Our results also showed that collateral score was a predictive biomarker (p < 0.044). These results suggest that (1) patients with good collateral score derive more benefit from successful recanalization than patients with poor collateral score and (2) collateral status is inconsequential if recanalization is not achieved. Conclusion: Our data results reinforce the importance of careful patient selection for recanalization therapy to avoid futile recanalization. The paucity of collaterals predicts poor clinical outcome despite recanalization. On the other hand, robust collaterals warrant consideration for recanalization therapy given the better odds of good clinical outcome. © 2017, Springer-Verlag GmbH Germany.","Biomarker; Collateral grading system; CT perfusion; Quantitative; Stroke","alteplase; iohexol; adult; Article; brain circulus arteriosus; brain ischemia; clinical outcome; collateral circulation; computed tomographic angiography; computed tomography scanner; female; follow up; human; inferior sagittal sinus; internal carotid artery; major clinical study; male; middle cerebral artery; National Institutes of Health Stroke Scale; outcome assessment; patient selection; priority journal; prognosis; Rankin scale; recanalization; retrospective study; treatment outcome; x-ray computed tomography",2-s2.0-85028744948
"Abdesselam R., Aazi F.-Z.","Comparison of proximity measures for a topological discrimination",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994762023&doi=10.1007%2f978-3-319-45763-5_5&partnerID=40&md5=ade98b33c184db2f886009c198978f2d","The results of any operation of clustering or classification of objects strongly depend on the proximity measure chosen. The user has to select one measure among many existing ones. Yet, according to the notion of topological equivalence chosen, some measures are more or less equivalent. In this paper, we propose a new approach to compare and classify proximity measures in a topological structure and in a context of discrimination. The concept of topological equivalence uses the basic notion of local neighborhood. We define the topological equivalence between two proximity measures, in the context of discrimination, through the topological structure induced by each measure. We propose a criterion for choosing the ""best"" measure, adapted to the data considered, among some of the most used proximity measures for quantitative or qualitative data. The principle of the proposed approach is illustrated using two real datasets with conventional proximity measures of literature for quantitative and qualitative variables. Afterward, we conduct experiments to evaluate the performance of this discriminant topological approach and to test if the proximity measure selected as the ""best"" discriminant changes in terms of the size or the dimensions of the used data. The ""best"" discriminating proximity measure will be verified a posteriori using a supervised learning method of type Support Vector Machine, discriminant analysis or Logistic regression applied in a topological context. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994762023
"Longo F., Nicoletti L., Padovano A.","Smart operators in industry 4.0: A human-centered approach to enhance operators’ capabilities and competencies within the new smart factory context",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029476237&doi=10.1016%2fj.cie.2017.09.016&partnerID=40&md5=3e55fa511df1f783336603e1ffe259fc","As the Industry 4.0 takes shape, human operators experience an increased complexity of their daily tasks: they are required to be highly flexible and to demonstrate adaptive capabilities in a very dynamic working environment. It calls for tools and approaches that could be easily embedded into everyday practices and able to combine complex methodologies with high usability requirements. In this perspective, the proposed research work is focused on the design and development of a practical solution, called Sophos-MS, able to integrate augmented reality contents and intelligent tutoring systems with cutting-edge fruition technologies for operators’ support in complex man-machine interactions. After establishing a reference methodological framework for the smart operator concept within the Industry 4.0 paradigm, the proposed solution is presented, along with its functional and non-function requirements. Such requirements are fulfilled through a structured design strategy whose main outcomes include a multi-layered modular solution, Sophos-MS, that relies on Augmented Reality contents and on an intelligent personal digital assistant with vocal interaction capabilities. The proposed approach has been deployed and its training potentials have been investigated with field experiments. The experimental campaign results have been firstly checked to ensure their statistical relevance and then analytically assessed in order to show that the proposed solution has a real impact on operators’ learning curves and can make the difference between who uses it and who does not. © 2017","Augmented reality; Industry 4.0; Intelligent vocal assistance; Smart factory; Smart operators","Computer aided instruction; Personal digital assistants; Augmented reality content; Design and Development; Experimental campaign; Function requirements; Intelligent tutoring system; Intelligent vocal assistance; Methodological frameworks; Usability requirements; Augmented reality",2-s2.0-85029476237
"Changfeng F., Zhengguang S., Lianfu H., Chao L.","Faults diagnosis of self-validating air data sensing system based on ensemble EMD and multiclass RVM",2017,"International Journal of Control and Automation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031017211&doi=10.14257%2fijca.2017.10.9.11&partnerID=40&md5=3400f69bfb6fdec406004463222b76f4","Aiming at the non-linear fault features extraction, multiple fault patterns classification, and uncertain reasoning problem in faults diagnosis, a novel strategy by using multiclass relevance vector machine (RVM) coupled with ensemble empirical mode decomposition (EMD) for faults diagnosis of the self-validating air data sensing (SVADS) system is proposed. The working principle of the EEMD, especially the improved anti-mode mixing and the clear physical meaning of the decomposed intrinsic mode function (IMF), is emphasized for distinct faults features extraction, in which the energy and cutting ratio feature is picked up and different faults types can be validly distinguished from each other. The multiclass RVM is then employed for the faults diagnosis of multiple faults types in SVADS system, in which its advantages of multi-patterns simultaneous outputs, small sample learning and uncertain description of classification results by a form of probability are fully used. Based on the prototype design and failure mode analysis of the SVADS system, a real experimental system is designed, and then the fault-free and faulty data sample are collected to verify the performance of the proposed strategy. From the anti-mode mixing capacity of ensemble EMD and fault diagnosis accuracy of multiclass RVM under small sample, the performance comparison among different methods or different sample number is done. Results demonstrate that the proposed scheme provides a better solution to the faults diagnosis of SVADS system. © 2017 SERSC Australia.","Air data sensing system; Ensemble EMD; Faults diagnosis; Multiclass RVM",,2-s2.0-85031017211
"Gupta S., Mittal P., Madhu M.K., Sharma V.K.","IL17eScan: A tool for the identification of peptides inducing IL-17 response",2017,"Frontiers in Immunology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032582840&doi=10.3389%2ffimmu.2017.01430&partnerID=40&md5=b6f0054797f08d46d35acc2e01bc257e","IL-17 cytokines are pro-inflammatory cytokines and are crucial in host defense against various microbes. Induction of these cytokines by microbial antigens has been investigated in the case of ischemic brain injury, gingivitis, candidiasis, autoimmune myocarditis, etc. In this study, we have investigated the ability of amino acid sequence of antigens to induce IL-17 response using machine-learning approaches. A total of 338 IL-17-inducing and 984 IL-17 non-inducing peptides were retrieved from Immune Epitope Database. 80% of the data were randomly selected as training dataset and rest 20% as validation dataset. To predict the IL-17-inducing ability of peptides/protein antigens, different sequence-based machine-learning models were developed. The performance of support vector machine (SVM) and random forest (RF) was compared with different parameters to predict IL-17-inducing epitopes (IIEs). The dipeptide composition-based SVM-model displayed an accuracy of 82.4% with Matthews correlation coefficient = 0.62 at polynomial (t = 1) kernel on 10-fold cross-validation and outperformed RF. Amino acid residues Leu, Ser, Arg, Asn, and Phe and dipeptides LL, SL, LK, IL, LI, NL, LR, FK, SF, and LE are abundant in IIEs. The present tool helps in the identification of IIEs using machine-learning approaches. The induction of IL-17 plays an important role in several inflammatory diseases, and identification of such epitopes would be of great help to the immunologists. © 2017 Gupta, Mittal, Madhu and Sharma.","Interleukin-17; Machine learning; Pro-inflammatory cytokines; Random forest; Support vector machine","epitope; HLA antigen; interleukin 17; interleukin 4; amino acid composition; amino acid sequence; Article; biofilm; controlled study; cytokine response; echography; gene frequency; human; nonhuman; protein analysis; random forest; rheumatoid arthritis; support vector machine",2-s2.0-85032582840
"Zhang X., Yu T., Pan Z., Yang B., Bao T.","Lifelong Learning for Complementary Generation Control of Interconnected Power Grids with High-Penetration Renewables and EVs",2017,"IEEE Transactions on Power Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032737338&doi=10.1109%2fTPWRS.2017.2767318&partnerID=40&md5=40da613128601737525888e0413dca9a","This paper proposes a lifelong learning (LL) based complementary generation control (CGC) of interconnected power grids with high-penetration renewable energy sources and electric vehicles (EVs). The wind farms (WFs), photovoltaic stations (PVs), EVs are aggregated as a wide-area virtual power plant (WVPP) for automatic generation control (AGC), which can significantly accelerate the system response and reduce the regulation costs of balancing unexpected power mismatches between generation side and demand side. Under such framework, CGC is decomposed into a multi-layer generation command dispatch (GCD) to rapidly compute an optimal solution. LL is firstly employed for the primary layer CGC between conventional power plants and a WVPP; Then the secondary layer is implemented according to the ascending order of regulation costs of all reserve sources; Lastly, the tertiary layer is accomplished by a coordinated control in each WFs or PVs, while an online optimization of EVs is adopted by considering the charging demands. The imitation learning is introduced to improve the learning efficiency of agents with transfer learning, thus an online optimization of CGC can be satisfied. Case studies are carried out to evaluate the performance of LL for multi-layer CGC of AGC on a practical Hainan power grid of southern China. IEEE","Automatic generation control; automatic generation control; complementary generation control; Generators; high-penetration renewables; Lifelong learning; Optimization; Power grids; Water heating; wide-area virtual power plant; Wind turbines","Electric machine control; Electric power system control; Electric power system interconnection; Gas generators; Optimization; Renewable energy resources; Wind power; Wind turbines; Automatic generation control; Generation controls; Life long learning; Power grids; Renewables; Virtual power plants; Water heating; Electric power transmission networks",2-s2.0-85032737338
"Yao H., Wang Q., Wang L., Zhang P., Li M., Liu Y.","An Intrusion Detection Framework Based on Hybrid Multi-Level Data Mining",2017,"International Journal of Parallel Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032691418&doi=10.1007%2fs10766-017-0537-7&partnerID=40&md5=672f4810772a2707eaa1471150fc338d","With the dramatic opening-up of network, network security becomes a severe social problem with the rapid development of network technology. Intrusion Detection System (IDS) is an innovative and proactive network security technology, which becomes a hot topic in both industry and academia in recent years. There are four main characteristics of intrusion data that affect the performance of IDS including multicomponent, data imbalance, time-varying and unknown attacks. We propose a novel IDS framework called HMLD to address these issues, which is an exquisite designed framework based on Hybrid Multi-Level Data Mining. In this paper, we use KDDCUP99 dataset to evaluate the performance of HMLD. The experimental results show that HMLD can reach 96.70% accuracy which is nearly 1% higher than the recent proposed optimal algorithm SVM+ELM+Modified K-Means. In details, HMLD greatly increased the detection accuracy of DoS attacks and R2L attacks. © 2017 Springer Science+Business Media, LLC","Data engineering; Intrusion detection system; KDDCUP99; Machine learning; Multi-level","Computer crime; Data mining; Denial-of-service attack; Intrusion detection; Learning systems; Mercury (metal); Data engineering; Detection accuracy; Intrusion Detection Systems; KDDCUP99; Multilevels; Network technologies; Optimal algorithm; Proactive networks; Network security",2-s2.0-85032691418
"Chodrow P.S.","Structure and information in spatial segregation",2017,"Proceedings of the National Academy of Sciences of the United States of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032646633&doi=10.1073%2fpnas.1708201114&partnerID=40&md5=52d3dc5ea2dfd4ddb39b4e23c78b1bee","Ethnoracial residential segregation is a complex, multiscalar phenomenon with immense moral and economic costs. Modeling the structure and dynamics of segregation is a pressing problem for sociology and urban planning, but existing methods have limitations. In this paper, we develop a suite of methods, grounded in information theory, for studying the spatial structure of segregation. We first advance existing profile and decomposition methods by posing two related regionalization methods, which allow for profile curves with nonconstant spatial scale and decomposition analysis with nonarbitrary areal units. We then formulate a measure of local spatial scale, which may be used for both detailed, within-city analysis and intercity comparisons. These methods highlight detailed insights in the structure and dynamics of urban segregation that would be otherwise easy to miss or difficult to quantify. They are computationally efficient, applicable to a broad range of study questions, and freely available in open source software. © 2017, National Academy of Sciences. All rights reserved.","Diversity; Information theory; Machine learning; Multiscale analysis; Segregation","Article; Asian; city planning; community structure; decomposition; density; Hispanic; information science; machine learning; mathematics; priority journal; racial segregation; social segregation",2-s2.0-85032646633
"Malki Z.","Shape and geometric features-based semantic image retrieval using multi-class support vector machine",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032793185&partnerID=40&md5=85cc5041615c61f02916af83dabe97ae","In this paper, a new approach to retrieve semantic images based on shape and geometric features of image in conjunction with multi-class support vector machine is proposed. Zernike moment as shape feature is to verify the invariance of objects for silhouette image. In addition, a set of geometrical features is to explore the objects shape using two features of rectangularity and circularity. Then the extracted features are normalized and employed for multi-class support vector machine either for learning or retrieving processes. The retrieving process relies on three main tasks which namely Query Engine, Matching Module and Ontology Manger, respectively. Query Engine is to build the input text or image query using SPARQL language. The matching module extracts the shape and geometric features of image’s objects and employ them to Ontology Manger which in turn inserts them in ontology knowledge base. Benchmark mammals have been conducted to empirically conclude the outcome of proposed approach. Our experiment on text and image retrieval yields efficient results to problematic phenomena than previously reported. © 2005 – ongoing JATIT & LLS.","Multi-class support vector machine; Query engine; SPARQL; Zernike moment",,2-s2.0-85032793185
"Ma J., Ren Z., Zhao G., Zhang Y., Koh C.","A New Reliability Analysis Method Combining Adaptive Kriging With Weight Index Monte Carlo Simulation",2017,"IEEE Transactions on Magnetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032725185&doi=10.1109%2fTMAG.2017.2763198&partnerID=40&md5=b2636567ba43197669d11cf67ef4192b","In order to extend the application of reliability-based design optimization to engineering electromagnetic problems, the main contribution of this paper is to propose an efficient reliability analysis method: adaptive Kriging-assisted weight index Monte Carlo Simulation (MCS). The Kriging model is constructed to substitute for expensive finite-element analysis of engineering problem during optimization and reliability analysis. To improve the interpolation accuracy of Kriging, a learning function is adopted in adaptive sampling process. A weight index form of MCS method is proposed to enhance the efficiency of conventional MCS method. Finally, taking the result of conventional MCS as a reference, proposed reliability analysis method is compared with reliability index approach and sensitivity-assisted MCS methods. IEEE","Adaptation models; Adaptive Kriging (AK); Indexes; Monte Carlo methods; Monte Carlo simulation (MCS); Optimization; reliability analysis; Reliability engineering; reliability-based design optimization (RBDO); Uncertainty","Finite element method; Intelligent systems; Interpolation; Machine design; Monte Carlo methods; Optimization; Reliability; Uncertainty analysis; Adaptation models; Indexes; Kriging; Reliability engineering; Reliability-based design optimization; Uncertainty; Reliability analysis",2-s2.0-85032725185
"Gawade S., Jaykumar N.","Illustration of semi-supervised feature selection using effective frameworks",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032836198&partnerID=40&md5=5a85dda8b1b20363b0f00c9075a76369","Semi- supervised feature selection is a supervised feature jobs as well as method that uses the unlabeled data for guiding that the small quantity of labeled dataset with a huge quantity of unlabeled dataset. Semi-supervised feature selection is drop between the unsupervised selection and supervised selection. Feature selections have been playing an essential task in the different studies as well as application area of machine learning. In this paper we have explored a three-different level framework for semi-supervised feature selection. Which are mainly feature selection methods center on discovering relevant features for optimizing high-dimensional data. In this paper, we have shown that the relevance need three essential frameworks which provide an efficient feature selection in the semi-supervised context. In the constraint selection framework they select pair wise constraints which can be extracted from the labeled part of data. The Relevance analysis framework shows original utilized realization which competently merges the direct of the restricted geometrical construction of unlabeled data with a selected constraint from the first framework. It allows us to verify the set of relevant features. For the CSFSR and efficiency framework, is to find out and supply the redundant features from the relevant ones which can be chosen from the second framework. It also shows the comparison between third framework and prims algorithm for better feature selection. Result of this proposed system is efficiency with statistical and graphical view. © 2005 – ongoing JATIT & LLS.","Constraint; Dimensionality reduction; Feature selection; Redundancy; Relevance; Semi supervised",,2-s2.0-85032836198
"Tomiazzi J.S., Judai M.A., Nai G.A., Pereira D.R., Antunes P.A., Favareto A.P.A.","Evaluation of genotoxic effects in Brazilian agricultural workers exposed to pesticides and cigarette smoke using machine-learning algorithms",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032677170&doi=10.1007%2fs11356-017-0496-y&partnerID=40&md5=c8daa2ce58c38887cf311d4773157e42","Monitoring exposure to xenobiotics by biomarker analyses, such as a micronucleus assay, is extremely important for the precocious detection and prevention of diseases, such as oral cancer. The aim of this study was to evaluate genotoxic effects in rural workers who were exposed to cigarette smoke and/or pesticides and to identify possible classification patterns in the exposure groups. The sample included 120 participants of both sexes aged between 18 and 39, who were divided into the following four groups: control group (CG), smoking group (SG), pesticide group (PG), and smoking + pesticide group (SPG). Their oral mucosa cells were stained with Giemsa for cytogenetic analysis. The total numbers of nuclear abnormalities (CG = 27.16 ± 14.32, SG = 118.23 ± 74.78, PG = 184.23 ± 52.31, and SPG = 191.53 ± 66.94) and micronuclei (CG = 1.46 ± 1.40, SG = 12.20 ± 10.79, PG = 21.60 ± 8.24, and SPG = 20.26 ± 12.76) were higher (p < 0.05) in the three exposed groups compared to the GC. In this study, we considered several different classification algorithms (the artificial neural network, K-nearest neighbors, support vector machine, and optimum path forest). All of the algorithms displayed good classification (accuracy > 80%) when using dataset2 (without the redundant exposure type SPG). It is clear that the data form a robust pattern and that classifiers could be successfully trained on small datasets from the exposure groups. In conclusion, exposing agricultural workers to pesticides and/or tobacco had genotoxic potential, but concomitant exposure to xenobiotics did not lead to additive or potentiating effects. © 2017 Springer-Verlag GmbH Germany","Agriculture; Genotoxicity; Machine learning; Micronucleus; Pesticide; Smoking",,2-s2.0-85032677170
"Piersanti S., Orlandi A.","Genetic Algorithm Optimization for the Total Radiated Power of a Meandered Line by Using an Artificial Neural Network",2017,"IEEE Transactions on Electromagnetic Compatibility",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032733823&doi=10.1109%2fTEMC.2017.2764623&partnerID=40&md5=20e75a64ba7819212a8a610788def21d","One of the state-of-the-art optimization strategies is the introduction of an artificial neural network in place of a more time-consuming numerical tool to compute the cost function. This work describes the development of a genetic algorithm optimization strategy for a meandered microstrip line by using an artificial neural network whose training set has been designed by a uniform sampling of the global design space. The results in terms of the total radiated electromagnetic power are discussed and compared with those obtained by the initial and not optimized configuration. IEEE","Artificial neural network (ANN); Biological cells; electromagnetic (EM) radiation; Genetic algorithms; genetic algorithms (GAs); machine learning; meandered line; Microstrip; nature-inspired algorithms; Neural networks; Optimization; Scattering parameters; signal integrity; total radiated power (TRP); Training","Bioinformatics; Cost functions; Genetic algorithms; Learning systems; Neural networks; Personnel training; Scattering parameters; Biological cells; Genetic algorithm (GAs); meandered line; Microstripes; Nature inspired algorithms; Signal Integrity; Total radiated power; Optimization",2-s2.0-85032733823
"Cong W., Yang J., Ai D., Song H., Chen G., Liang X., Liang P., Wang Y.","Global Patch Matching (GPM) for freehand 3D ultrasound reconstruction",2017,"BioMedical Engineering Online",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032567319&doi=10.1186%2fs12938-017-0411-2&partnerID=40&md5=121187406e2ac6b455a39d08a1cfaa40","Background: 3D ultrasound volume reconstruction from B-model ultrasound slices can provide more clearly and intuitive structure of tissue and lesion for the clinician. Methods: This paper proposes a novel Global Path Matching method for the 3D reconstruction of freehand ultrasound images. The proposed method composes of two main steps: bin-filling scheme and hole-filling strategy. For the bin-filling scheme, this study introduces two operators, including the median absolute deviation and the inter-quartile range absolute deviation, to calculate the invariant features of each voxel in the 3D ultrasound volume. And the best contribution range for each voxel is obtained by calculating the Euclidian distance between current voxel and the voxel with the minimum invariant features. Hence, the intensity of the filling vacant voxel can be obtained by weighted combination of the intensity distribution of pixels in the best contribution range. For the hole-filling strategy, three conditions, including the confidence term, the data term and the gradient term, are designed to calculate the weighting coefficient of the matching patch of the vacant voxel. While the matching patch is obtained by finding patches with the best similarity measure that defined by the three conditions in the whole 3D volume data. Results: Compared with VNN, PNN, DW, FMM, BI and KR methods, the proposed Global Path Matching method can restore the 3D ultrasound volume with minimum difference. Conclusions: Experimental results on phantom and clinical data sets demonstrate the effectiveness and robustness of the proposed method for the reconstruction of ultrasound volume. © 2017 The Author(s).","3D ultrasound reconstruction; Matching patch; Optimal contribution range","Bins; Filling; Three dimensional computer graphics; Ultrasonics; 3-D ultrasound; Freehand 3D ultrasound; Intensity distribution; Inter quartile ranges; Matching patch; Median absolute deviation; Optimal contribution range; Weighting coefficient; Image reconstruction; Article; controlled study; distance weighted method; echography; fast marching method; global path matching method; image reconstruction; intermethod comparison; kernel method; machine learning; measurement accuracy; pixel nearest neighbor; priority journal; three dimensional imaging; voxel nearest neighbor",2-s2.0-85032567319
"Yao Y., Zhai J., Cao Y., Ding X., Liu J., Luo Y.","Data analytics enhanced component volatility model",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018937059&doi=10.1016%2fj.eswa.2017.05.025&partnerID=40&md5=dd76034ef9c96f281527c2c6fb1c83ad","Volatility modelling and forecasting have attracted many attentions in both finance and computation areas. Recent advances in machine learning allow us to construct complex models on volatility forecasting. However, the machine learning algorithms have been used merely as additional tools to the existing econometrics models. The hybrid models that specifically capture the characteristics of the volatility data have not been developed yet. We propose a new hybrid model, which is constructed by a low-pass filter, the autoregressive neural network and an autoregressive model. The volatility data is decomposed by the low-pass filter into long and short term components, which are then modelled by the autoregressive neural network and an autoregressive model respectively. The total forecasting result is aggregated by the outputs of two models. The experimental evaluations using one-hour and one-day realized volatility across four major foreign exchanges showed that the proposed model significantly outperforms the component GARCH, EGARCH and neural network only models in all forecasting horizons. © 2017 Elsevier Ltd","Autoregressive neural network; Hybrid model; Two-component; Volatility model","Artificial intelligence; Economic analysis; Economics; Electronic trading; Forecasting; Learning algorithms; Learning systems; Statistics; Autoregressive neural networks; Component volatility models; Experimental evaluation; Hybrid model; Modelling and forecasting; Two-component; Volatility forecasting; Volatility modeling; Low pass filters",2-s2.0-85018937059
"Lyu C., Chen B., Ren Y., Ji D.","Long short-term memory RNN for biomedical named entity recognition",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032585583&doi=10.1186%2fs12859-017-1868-5&partnerID=40&md5=8d5ed82b5552477509337bcc40a85208","Background: Biomedical named entity recognition(BNER) is a crucial initial step of information extraction in biomedical domain. The task is typically modeled as a sequence labeling problem. Various machine learning algorithms, such as Conditional Random Fields (CRFs), have been successfully used for this task. However, these state-of-the-art BNER systems largely depend on hand-crafted features. Results: We present a recurrent neural network (RNN) framework based on word embeddings and character representation. On top of the neural network architecture, we use a CRF layer to jointly decode labels for the whole sentence. In our approach, contextual information from both directions and long-range dependencies in the sequence, which is useful for this task, can be well modeled by bidirectional variation and long short-term memory (LSTM) unit, respectively. Although our models use word embeddings and character embeddings as the only features, the bidirectional LSTM-RNN (BLSTM-RNN) model achieves state-of-the-art performance - 86.55% F1 on BioCreative II gene mention (GM) corpus and 73.79% F1 on JNLPBA 2004 corpus. Conclusions: Our neural network architecture can be successfully used for BNER without any manual feature engineering. Experimental results show that domain-specific pre-trained word embeddings and character-level representation can improve the performance of the LSTM-RNN models. On the GM corpus, we achieve comparable performance compared with other systems using complex hand-crafted features. Considering the JNLPBA corpus, our model achieves the best results, outperforming the previously top performing systems. The source code of our method is freely available under GPL at https://github.com/lvchen1989/BNER. © 2017 The Author(s).","Biomedical named entity recognition; Character representation; LSTM; Recurrent neural network; Word embeddings","Artificial intelligence; Brain; Learning algorithms; Learning systems; Memory architecture; Natural language processing systems; Network architecture; Random processes; Recurrent neural networks; Biomedical named entity recognition; Character representations; Conditional Random Fields(CRFs); Embeddings; Long-range dependencies; LSTM; Recurrent neural network (RNN); State-of-the-art performance; Long short-term memory; embedding; nervous system; short term memory",2-s2.0-85032585583
"Harrigan A.G., Schmirler K., Arppe A., Antonsen L., Trosterud T., Wolvengrey A.","Learning from the computational modelling of Plains Cree verbs",2017,"Morphology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032704250&doi=10.1007%2fs11525-017-9315-x&partnerID=40&md5=62302bec006ce43224b3285ffb3f7446","This paper describes the ongoing process of creating a computational morphological model of Plains Cree, a language native to North America, making use of finite-state machines, and with a focus on verbs. We cover prior linguistic theoretical and descriptive models of Plains Cree, moving on to the computational implementation of (chiefly) inflectional phenomena, followed by relevant morphophonological processes. We evaluate the performance of our computational implementation with a hand-verified corpus of Plains Cree, and present a discussion of the morphological complexity found in the corpus, as compared to that of our model and its theoretical underpinnings. The results of this evaluation and research into natural language use inform us about the practical extent of morphological complexity for a polysynthetic language, and allow us to identify avenues for improvement of the model. Finally, this computational model for Plains Cree offers the opportunity to create various digital tools and applications for language users for the maintenance and revitalization of this language in the 21st century. © 2017 Springer Science+Business Media B.V.","Computational modelling; Finite state transducer; Morphological modelling; Plains Cree",,2-s2.0-85032704250
"Jungnickel L., Kruse C., Vaeth M., Kirkevang L.-L.","Quality aspects of ex vivo root canal treatments done by undergraduate dental students using four different endodontic treatment systems",2017,"Acta Odontologica Scandinavica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032704018&doi=10.1080%2f00016357.2017.1396494&partnerID=40&md5=8457d29473f22111730a3f066b9d28b0","Objective: To evaluate factors associated with treatment quality of ex vivo root canal treatments performed by undergraduate dental students using different endodontic treatment systems. Material and methods: Four students performed root canal treatment on 80 extracted human teeth using four endodontic treatment systems in designated treatment order following a Latin square design. Lateral seal and length of root canal fillings was radiographically assessed; for lateral seal, a graded visual scale was used. Treatment time was measured separately for access preparation, biomechanical root canal preparation, obturation and for the total procedure. Mishaps were registered. An ANOVA mirroring the Latin square design was performed. Results: Use of machine-driven nickel-titanium systems resulted in overall better quality scores for lateral seal than use of the manual stainless-steel system. Among systems with machine-driven files, scores did not significantly differ. Use of machine-driven instruments resulted in shorter treatment time than manual instrumentation. Machine-driven systems with few files achieved shorter treatment times. With increasing number of treatments, root canal–filling quality increased, treatment time decreased; a learning curve was plotted. No root canal shaping file separated. Conclusions: The use of endodontic treatment systems with machine-driven files led to higher quality lateral seal compared to the manual system. The three contemporary machine-driven systems delivered comparable results regarding quality of root canal fillings; they were safe to use and provided a more efficient workflow than the manual technique. Increasing experience had a positive impact on the quality of root canal fillings while treatment time decreased. © 2017 Acta Odontologica Scandinavica Society","Endodontic training; learning curve; Ni–Ti instruments; reciprocation; rotation",,2-s2.0-85032704018
"Santos R., Murrieta-Flores P., Calado P., Martins B.","Toponym matching through deep neural networks",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032681752&doi=10.1080%2f13658816.2017.1390119&partnerID=40&md5=12e7a2fdc1fafc6324dca43244f72117","Toponym matching, i.e. pairing strings that represent the same real-world location, is a fundamental problemfor several practical applications. The current state-of-the-art relies on string similarity metrics, either specifically developed for matching place names or integrated within methods that combine multiple metrics. However, these methods all rely on common sub-strings in order to establish similarity, and they do not effectively capture the character replacements involved in toponym changes due to transliterations or to changes in language and culture over time. In this article, we present a novel matching approach, leveraging a deep neural network to classify pairs of toponyms as either matching or nonmatching. The proposed network architecture uses recurrent nodes to build representations from the sequences of bytes that correspond to the strings that are to be matched. These representations are then combined and passed to feed-forward nodes, finally leading to a classification decision. We present the results of a wide-ranging evaluation on the performance of the proposed method, using a large dataset collected from the GeoNames gazetteer. These results show that the proposed method can significantly outperform individual similarity metrics from previous studies, as well as previous methods based on supervised machine learning for combining multiple metrics. © 2017 Informa UK Limited, trading as Taylor & Francis Group","approximate string matching; deep neural networks; duplicate detection; geographic information retrieval; recurrent neural networks; Toponym matching",,2-s2.0-85032681752
"Wu J., Shen L., Yang W.","Internal force corrections with machine learning for quantum mechanics/molecular mechanics simulations",2017,"Journal of Chemical Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031694655&doi=10.1063%2f1.5006882&partnerID=40&md5=13b16d98d03a5ca4841eafe1d0031a31","Ab initio quantum mechanics/molecular mechanics (QM/MM) molecular dynamics simulation is a useful tool to calculate thermodynamic properties such as potential of mean force for chemical reactions but intensely time consuming. In this paper, we developed a new method using the internal force correction for low-level semiempirical QM/MM molecular dynamics samplings with a predefined reaction coordinate. As a correction term, the internal force was predicted with a machine learning scheme, which provides a sophisticated force field, and added to the atomic forces on the reaction coordinate related atoms at each integration step. We applied this method to two reactions in aqueous solution and reproduced potentials of mean force at the ab initio QM/MM level. The saving in computational cost is about 2 orders of magnitude. The present work reveals great potentials for machine learning in QM/MM simulations to study complex chemical processes. © 2017 Author(s).",,"Artificial intelligence; Mechanics; Molecular dynamics; Molecular modeling; Quantum theory; Reaction kinetics; Solutions; Thermodynamic properties; Computational costs; Molecular dynamics simulations; Orders of magnitude; Potential of mean force; Potentials of mean forces; Quantum mechanics/molecular mechanics; Reaction coordinates; Semi-empirical QM/MM molecular dynamics; Learning systems",2-s2.0-85031694655
"Huang X.-L., Ma X., Hu F.","Editorial: Machine Learning and Intelligent Communications",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032501890&doi=10.1007%2fs11036-017-0962-2&partnerID=40&md5=323eb69b021cd4483bd162cbd2e74bb8",[No abstract available],,,2-s2.0-85032501890
"Lu X., He Z., Yi S., Chen W.-S.","Joint of locality- and globality-preserving projections",2017,"Signal, Image and Video Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032352530&doi=10.1007%2fs11760-017-1194-4&partnerID=40&md5=19a2646eede2badde5c1f5f8af30dfd4","Dimensionality reduction is an important topic in machine learning community, which is widely used in the areas of face recognition, visual detection and tracking. Preserving local and global structures simultaneously is crucial for dimensionality reduction. In this paper, local and global approaches are generalized, respectively, and then a unified framework that joins the effective local and global terms is presented for unsupervised dimensionality reduction. Furthermore, to search for the optimal integration parameter, the proposed method uses two different search schemes named JLGP and IJLGP, respectively, where JLGP corresponds to the manual search scheme and IJLGP corresponds to the automatic search schemes. The promising experimental results on four benchmark datasets validate the effectiveness of the proposed method. © 2017 Springer-Verlag London Ltd.","Face recognition; Joint preserving of globality and locality; Unsupervised dimensionality reduction","Learning systems; Automatic searches; Benchmark datasets; Dimensionality reduction; Global approaches; Globality; Machine learning communities; Optimal integration; Unified framework; Face recognition",2-s2.0-85032352530
"Becker H., Fleureau J., Guillotel P., Wendling F., Merlet I., Albera L.","Emotion recognition based on high-resolution EEG recordings and reconstructed brain sources",2017,"IEEE Transactions on Affective Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032741837&doi=10.1109%2fTAFFC.2017.2768030&partnerID=40&md5=9c7d464e1ae69c2b084570cd09907159","Electroencephalography (EEG)-based emotion recognition is currently a hot issue in the affective computing community. Numerous studies have been published on this topic, following generally the same schema: 1) presentation of emotional stimuli to a number of subjects during the recording of their EEG, 2) application of machine learning techniques to classify the subjects&#x0027; emotions. The proposed approaches vary mainly in the type of features extracted from the EEG and in the employed classifiers, but it is difficult to compare the reported results due to the use of different datasets. In this paper, we present a new database for the analysis of valence (positive or negative emotions), which is made publicly available. The database comprises physiological recordings and 257-channel EEG data, contrary to all previously published datasets, which include at most 62 EEG channels. Furthermore, we reconstruct the brain activity on the cortical surface by applying source localization techniques. We then compare the performances of valence classification that can be achieved with various features extracted from all source regions (source space features) and from all EEG channels (sensor space features), showing that the source reconstruction improves the classification results. Finally, we discuss the influence of several parameters on the classification scores. IEEE","EEG; emotion recognition; functional connectivity; source localization","Brain; Brain; Brain models; Classification (of information); Classification (of information); Database systems; Electrophysiology; Electrophysiology; Feature extraction; Learning systems; Learning systems; Physiology; Speech recognition; Speech recognition; Affective Computing; Classification results; Classification results; Emotion recognition; Emotion recognition; Functional connectivity; Functional connectivity; Machine learning techniques; Machine learning techniques; Physiological recordings; Physiological recordings; Source localization; Source localization; Source reconstruction; Source reconstruction; Videos; Electroencephalography; Electroencephalography",2-s2.0-85032741837
"Ordozgoiti B., Canaval S.G., Mozo A.","Iterative column subset selection",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032379057&doi=10.1007%2fs10115-017-1115-4&partnerID=40&md5=1ce90d11d5afff9ab47fe066967078cb","Dimensionality reduction is often a crucial step for the successful application of machine learning and data mining methods. One way to achieve said reduction is feature selection. Due to the impossibility of labelling many data sets, unsupervised approaches are frequently the only option. The column subset selection problem translates naturally to this purpose and has received considerable attention over the last few years, as it provides simple linear models for low-rank data reconstruction. Recently, it was empirically shown that an iterative algorithm, which can be implemented efficiently, provides better subsets than other state-of-the-art methods. In this paper, we describe this algorithm and provide a more in-depth analysis. We carry out numerous experiments to gain insights on its behaviour and derive a simple bound for the norm recovered by the resulting matrix. To the best of our knowledge, this is the first theoretical result of this kind for this algorithm. © 2017 Springer-Verlag London Ltd.","Column subset selection; Data mining; Dimensionality reduction; Machine learning; Unsupervised feature selection",,2-s2.0-85032379057
"Aguilera-Pesantes D., Robayo L.E., Méndez P.E., Mollocana D., Marrero-Ponce Y., Torres F.J., Méndez M.A.","Discovering key residues of dengue virus NS2b-NS3-protease: New binding sites for antiviral inhibitors design",2017,"Biochemical and Biophysical Research Communications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016463497&doi=10.1016%2fj.bbrc.2017.03.107&partnerID=40&md5=f82d967741fc20a526088907322b4ac7","The NS2B-NS3 protease is essential for the Dengue Virus (DENV) replication process. This complex constitutes a target for efficient antiviral discovery because a drug could inhibit the viral polyprotein processing. Furthermore, since the protease is highly conserved between the four Dengue virus serotypes, it is probable that a drug would be equally effective against all of them. In this article, a strategy is reported that allowed us to identify influential residues on the function of the Dengue NS2b-NS3 Protease. Moreover, this is a strategy that could be applied to virtually any protein for the search of alternative influential residues, and for non-competitive inhibitor development. First, we incorporated several features derived from computational alanine scanning mutagenesis, sequence, structure conservation, and other structure-based characteristics. Second, these features were used as variables to obtain a multilayer perceptron model to identify defined groups (clusters) of key residues as possible candidate pockets for binding sites of new leads on the DENV protease. The identified residues included: i) amino acids close to the beta sheet-loop-beta sheet known to be important in its closed conformation for NS2b ii) residues close to the active site, iii) several residues evenly spread on the NS2b-NS3 contact surface, and iv) some inner residues most likely related to the overall stability of the protease. In addition, we found concordance on our list of residues with previously identified amino acids part of a highly conserved peptide studied for vaccine development. © 2017 Elsevier Inc.","Bindability sites; Computational alanine scanning mutagenesis; Dengue virus; DENV NS2b-NS3 protease; Machine learning; Multilayer perceptron; Protease inhibitor","alanine; antivirus agent; nonstructural protein 2; nonstructural protein 3; enzyme inhibitor; NS2B protein, flavivirus; protein binding; viral protein; Article; controlled study; Dengue virus; drug binding site; mutagenesis; nonhuman; perceptron; priority journal; protein function; protein stability; protein structure; sequence analysis; vaccine production; binding site; chemical model; chemistry; Dengue virus; drug design; enzymology; molecular docking; procedures; protein conformation; protein domain; ultrastructure; Binding Sites; Dengue Virus; Drug Design; Enzyme Inhibitors; Models, Chemical; Molecular Docking Simulation; Protein Binding; Protein Conformation; Protein Domains; Sequence Analysis, Protein; Viral Nonstructural Proteins",2-s2.0-85016463497
"Chen P.-H., Zafar H., Galperin-Aizenberg M., Cook T.","Integrating Natural Language Processing and Machine Learning Algorithms to Categorize Oncologic Response in Radiology Reports",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032491917&doi=10.1007%2fs10278-017-0027-x&partnerID=40&md5=6c53cf1c7f30e241d7efc0a4f1a46d57","A significant volume of medical data remains unstructured. Natural language processing (NLP) and machine learning (ML) techniques have shown to successfully extract insights from radiology reports. However, the codependent effects of NLP and ML in this context have not been well-studied. Between April 1, 2015 and November 1, 2016, 9418 cross-sectional abdomen/pelvis CT and MR examinations containing our internal structured reporting element for cancer were separated into four categories: Progression, Stable Disease, Improvement, or No Cancer. We combined each of three NLP techniques with five ML algorithms to predict the assigned label using the unstructured report text and compared the performance of each combination. The three NLP algorithms included term frequency-inverse document frequency (TF-IDF), term frequency weighting (TF), and 16-bit feature hashing. The ML algorithms included logistic regression (LR), random decision forest (RDF), one-vs-all support vector machine (SVM), one-vs-all Bayes point machine (BPM), and fully connected neural network (NN). The best-performing NLP model consisted of tokenized unigrams and bigrams with TF-IDF. Increasing N-gram length yielded little to no added benefit for most ML algorithms. With all parameters optimized, SVM had the best performance on the test dataset, with 90.6 average accuracy and F score of 0.813. The interplay between ML and NLP algorithms and their effect on interpretation accuracy is complex. The best accuracy is achieved when both algorithms are optimized concurrently. © 2017 Society for Imaging Informatics in Medicine","Informatics; Machine learning; Natural language processing; Structured reporting","Artificial intelligence; Diseases; Learning algorithms; Learning systems; Nobelium; Radiation; Radiology; Statistical tests; Support vector machines; Text processing; Decision forest; Feature hashing; Fully connected neural network; Informatics; Logistic regressions; Radiology reports; Structured reporting; Term frequencyinverse document frequency (TF-IDF); Natural language processing systems",2-s2.0-85032491917
"Tang F., Mao B., Fadlullah Z.M., Kato N., Akashi O., Inoue T., Mizutani K.","On Removing Routing Protocol from Future Wireless Networks: A Real-time Deep Learning Approach for Intelligent Traffic Control",2017,"IEEE Wireless Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032734210&doi=10.1109%2fMWC.2017.1700244&partnerID=40&md5=290664ee984f6a662363d44d58f483cb","Recently, deep learning has appeared as a breakthrough machine learning technique for various areas in computer science as well as other disciplines. However, the application of deep learning for network traffic control in wireless/heterogeneous networks is a relatively new area. With the evolution of wireless networks, efficient network traffic control such as routing methodology in the wireless backbone network appears as a key challenge. This is because the conventional routing protocols do not learn from their previous experiences regarding network abnormalities such as congestion and so forth. Therefore, an intelligent network traffic control method is essential to avoid this problem. In this article, we address this issue and propose a new, real-time deep learning based intelligent network traffic control method, exploiting deep Convolutional Neural Networks (deep CNNs) with uniquely characterized inputs and outputs to represent the considered Wireless Mesh Network (WMN) backbone. Simulation results demonstrate that our proposal achieves significantly lower average delay and packet loss rate compared to those observed with the existing routing methods. We particularly focus on our proposed method's independence from existing routing protocols, which makes it a potential candidate to remove routing protocol(s) from future wired/ wireless networks. IEEE","Feature extraction; Machine learning; Proposals; Routing; Routing protocols; Telecommunication traffic; Traffic control","Artificial intelligence; Deep learning; Deep neural networks; Feature extraction; Intelligent networks; Internet protocols; Learning systems; MESH networking; Neural networks; Routing protocols; Telecommunication traffic; Traffic control; Wireless networks; Convolutional neural network; Future wireless networks; Intelligent traffic controls; Machine learning techniques; Network traffic control; Proposals; Routing; Wired-wireless networks; Network routing",2-s2.0-85032734210
"Pan J., Zi Y., Chen J., Zhou Z., Wang B.","LiftingNet: A Novel Deep Learning Network with Layerwise Feature Learning from Noisy Mechanical Data for Fault Classification",2017,"IEEE Transactions on Industrial Electronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032735906&doi=10.1109%2fTIE.2017.2767540&partnerID=40&md5=9270a7e5d9baf5f1133945b33ab8859a","The key challenge of intelligent fault diagnosis is to develop features that can distinguish different categories. Because of the unique properties of mechanical data, pre-determined features based on prior knowledge are usually used as inputs for fault classification. However, proper selection of features often requires expertise knowledge and becomes more difficult and time-consuming when volume of data increases. In this paper, a novel deep learning network (LiftingNet) is proposed to learn features adaptively from raw mechanical data without prior knowledge. Inspired by Convolutional Neural Network (CNN) and Second Generation Wavelet Transform (SGWT), the LiftingNet is constructed to classify mechanical data even though inputs contain considerable noise and randomness. The LiftingNet consists of split layer, predict layer, update layer, pooling layer and full-connection layer. Different kernel sizes are allowed in convolutional layers to improve learning ability. As a multi-layer neural network, deep features are learned from shallow ones to represent complex structures in raw data. Feasibility and effectiveness of the LiftingNet is validated by two motor bearing datasets. Results show that the proposed method could achieve layerwise feature learning and successfully classify mechanical data even with different rotating speed and under the influence of random noise. IEEE","Convolution; convolutional neural network; deep learning; Fault diagnosis; Feature extraction; Intelligent fault diagnosis; Machine learning; Neural networks; second generation wavelet transform; Support vector machines; Transforms","Convolution; Deep learning; Failure analysis; Fault detection; Feature extraction; Learning systems; Mathematical transformations; Network layers; Neural networks; Support vector machines; Wavelet transforms; Complex structure; Convolutional neural network; Fault classification; Feature learning; Intelligent fault diagnosis; Learning abilities; Learning network; Second generation wavelet transform; Classification (of information)",2-s2.0-85032735906
"Fu W., Li S., Fang L., Benediktsson J.A.","Contextual Online Dictionary Learning for Hyperspectral Image Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032732979&doi=10.1109%2fTGRS.2017.2761893&partnerID=40&md5=b7c81bd366fbd3e991c5cb2b25f8f747","Sparse representation (SR) has been successfully used in the classification of hyperspectral images (HSIs) by representing HSI pixels over a dictionary and yielding discriminative sparse coefficients. Most of SR-based classification methods construct the dictionary by directly using some labeled pixels as atoms. Such dictionary can lead to inefficient SR for large-sized HSIs, and may be incomplete when the number of labeled pixels is less than the number of spectral bands. This paper proposes a contextual online dictionary learning (DL) method for HSIs classification, which learns a dictionary over the whole image rather than few labeled pixels. The proposed method can effectively and efficiently improve the adaptive representation capability of different pixels with an online learning mechanism. Specifically, the contextual characteristics of the HSI are integrated with discriminative spectral information for online DL, i.e., pushing similar pixels in neighborhood to share similar sparse coefficients with respect to the well-learned dictionary. By this way, the obtained sparse coefficients are structured and discriminative. Finally, a traditional classifier, i.e., the linear support vector machine, is applied to the sparse coefficients, and the final classification results are obtained. Experimental results on real HSIs show the effectiveness of the proposed method. IEEE","Classification; contextual characteristics; Cost function; Dictionaries; Encoding; hyperspectral images (HSIs); Hyperspectral imaging; Machine learning; online dictionary learning (DL); sparse representation (SR).; Training","Classification (of information); Cost functions; E-learning; Encoding (symbols); Glossaries; Hyperspectral imaging; Image coding; Independent component analysis; Learning systems; Personnel training; Pixels; Spectroscopy; Adaptive representations; Classification methods; Classification results; contextual characteristics; Learned dictionaries; Linear Support Vector Machines; Online dictionary learning; Sparse representation; Image classification",2-s2.0-85032732979
"Gruber A., Ben-Gal I.","A targeted Bayesian network learning for classification",2017,"Quality Technology and Quantitative Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032369681&doi=10.1080%2f16843703.2017.1395109&partnerID=40&md5=441fb3963db9f3069674be78dab598e9","A targeted Bayesian network learning (TBNL) method is proposed to account for a classification objective during the learning stage of the network model. The TBNL approximates the expected conditional probability distribution of the class variable. It effectively manages the trade-off between the classification accuracy and the model complexity by using a discriminative approach, constrained by information theory measurements. The proposed approach also provides a mechanism for maximizing the accuracy via a Pareto frontier over a complexity–accuracy plane, in cases of missing data in the data-sets. A comparative study over a set of classification problems shows the competitiveness of the TBNL mainly with respect to other graphical classifiers. © 2017 International Chinese Association of Quantitative Management","AI; Bayesian classifiers; complexity–accuracy trade-off; information theory; machine learning; target-oriented learning","Artificial intelligence; Bayesian networks; Complex networks; Economic and social effects; Information theory; Learning systems; Probability distributions; Bayesian classifier; Bayesian network learning; Classification accuracy; Comparative studies; Conditional probability distributions; Discriminative approach; Target oriented; Trade off; Classification (of information)",2-s2.0-85032369681
"Edstrom J., Gong Y., Chen D., Wang J., Gong N.","Data-Driven Intelligent Efficient Synaptic Storage for Deep Learning",2017,"IEEE Transactions on Circuits and Systems II: Express Briefs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032725801&doi=10.1109%2fTCSII.2017.2767900&partnerID=40&md5=22fa7e321196e79a4649f4d0beb8f46d","With the availability of big data and advanced hardware technologies, deep learning has been applied in different applications, such as self-driving cars and face recognition. While considering various sources of uncertainty and balancing multiple objectives of a system, the hardware implementation of deep learning needs continuous model updating and intensive synaptic weight storage, and SRAM is critical for the overall performance and energy efficiency. In this brief, we introduce offline data mining to the hardware design process, and the discovered data knowledge combined with a data-driven hardware design technique enables a more intelligent memory with better trade-off between energy efficiency, cost, and classification accuracy, thereby helping relieve the huge burden of data storage in deep learning systems. A 45nm 64 kbits (256 words W 256 bits) synaptic SRAM is presented that enables 45.6% active power saving and 83.2% leakage power saving, with low implementation cost (3.17%) and less than 1% degradation in classification accuracy. IEEE","Artificial neural networks; Biological neural networks; data mining; data-driven; Deep learning; Machine learning; Memory management; Neurons; power efficient; Random access memory; self-correction.; SRAM; Switches; synaptic","Data mining; Deep learning; Digital storage; Economic and social effects; Energy efficiency; Face recognition; Hardware; Integrated circuit design; Learning systems; Neural networks; Neurons; Random access storage; Static random access storage; Switches; Biological neural networks; Data driven; Memory management; Power efficient; Random access memory; Self-correction; synaptic; Big data",2-s2.0-85032725801
"Guo X., Shao S., Ansari N., Khreishah A.","Indoor Localization Using Visible Light via Fusion of Multiple Classifiers",2017,"IEEE Photonics Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032736184&doi=10.1109%2fJPHOT.2017.2767576&partnerID=40&md5=ca65fccd9d92af3aa70ae6de859ce413","We propose a localization technique by fusing multiple classifiers based on received signal strengths (RSSs) of visible light in which different intensity modulated sinusoidal signals emitted by LEDs are captured by Photo Diodes placed at various grid points. First, we obtain some approximate received signal strengths (RSSs) fingerprints by capturing the peaks of power spectral density (PSD) of the received signals at each given grid point. Unlike the existing RSSs based algorithms, several representative machine learning algorithms are adopted to train multiple classifiers based on these RSSs fingerprints. Then, two robust fusion localization algorithms, namely, grid independent least square (GI-LS) and grid dependent least square (GD-LS), are proposed to combine the outputs of these classifiers. A singular value decomposition (SVD) based LS (LS-SVD) method is proposed to mitigate the numerical stability problem when the prediction matrix is singular. Experiments conducted on an intensity modulated direct detection (IM/DD) system show that the probability of having mean square positioning error (MSPE) of less than 5cm achieved by GD-LS is improved by 93.03% and 93.15%, respectively, as compared to those by the RSS ratio (RSSR) and RSS matching methods with the FFT length of 2000. OAPA","Correlation; fusion localization; Indoor positioning; intensity modulated direct detection (IM/DD); Light emitting diodes; machine learning; Machine learning algorithms; Optical transmitters; Prediction algorithms; received signal strengths (RSSs) fingerprints; Receivers; Robustness; visible light communications (VLC)","Artificial intelligence; Correlation detectors; Correlation methods; Indoor positioning systems; Learning systems; Light; Light emitting diodes; Numerical methods; Optical correlation; Optical transmitters; Receivers (containers); Robustness (control systems); Signal receivers; Singular value decomposition; Spectral density; Indoor positioning; Intensity modulated direct detections; Prediction algorithms; Received signal strength; Visible light communications (VLC); Learning algorithms",2-s2.0-85032736184
"Barra A., Genovese G., Sollich P., Tantari D.","Phase transitions in restricted Boltzmann machines with generic priors",2017,"Physical Review E",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032568424&doi=10.1103%2fPhysRevE.96.042156&partnerID=40&md5=4f4d3b41b2e9e78272f52ecbe12eca46","We study generalized restricted Boltzmann machines with generic priors for units and weights, interpolating between Boolean and Gaussian variables. We present a complete analysis of the replica symmetric phase diagram of these systems, which can be regarded as generalized Hopfield models. We underline the role of the retrieval phase for both inference and learning processes and we show that retrieval is robust for a large class of weight and unit priors, beyond the standard Hopfield scenario. Furthermore, we show how the paramagnetic phase boundary is directly related to the optimal size of the training set necessary for good generalization in a teacher-student scenario of unsupervised learning. © 2017 American Physical Society.",,"Personnel training; Phase diagrams; Teaching; Gaussian variables; Hopfield models; Learning process; Optimal size; Paramagnetic phase; Restricted boltzmann machine; Symmetric phase; Training sets; Education",2-s2.0-85032568424
"Paris C., Bruzzone L.","A Sensor-Driven Hierarchical Method for Domain Adaptation in Classification of Remote Sensing Images",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032727492&doi=10.1109%2fTGRS.2017.2761839&partnerID=40&md5=f4c5a0d79237491ac7da8dd0e98d67cb","This paper presents a sensor-driven hierarchical domain adaptation method that aims at transferring the knowledge from a source domain (RS image where reference data are available) to a different but related target domain (RS image where no labeled reference data are available) for solving a classification problem. Due to the different acquisition conditions, a difference in the source and target distributions of the features representing the same class is generally expected. To solve this problem, the proposed method takes advantage from the availability of multisensor data to hierarchically detect features subspaces where for some classes data manifolds are partially (or completely) aligned. These feature subspaces are associated with invariant physical properties of classes measured by the sensors in the scene, i.e., measures having almost the same behavior in both domains. The detection of these invariant feature subspaces allows us to infer labels of the target samples that result more aligned to the source data for the considered subset of classes. Then, the labeled target samples are analyzed in the full feature space to classify the remaining target samples of the same classes. Finally, for those classes for which none of the sensors can measure invariant features, we perform the adaptation via a standard active learning technique. Experimental results obtained on two real multisensor data sets confirm the effectiveness of the proposed method. IEEE","Adaptation models; Classification; data fusion; Data models; domain adaptation (DA); Feature extraction; invariant features; Manifolds; multisensor data acquisition; remote sensing (RS); Standards; Support vector machines; Training; transfer learning.","Automobile engine manifolds; Data acquisition; Data fusion; Data structures; Feature extraction; Image classification; Personnel training; Remote sensing; Space optics; Standards; Support vector machines; Vectors; Adaptation models; Domain adaptation; Invariant features; Multi-sensor data; Transfer learning; Classification (of information)",2-s2.0-85032727492
"Littell P., Tian T., Xu R., Sheikh Z., Mortensen D., Levin L., Tyers F., Hayashi H., Horwood G., Sloto S., Tagtow E., Black A., Yang Y., Mitamura T., Hovy E.","The ARIEL-CMU situation frame detection pipeline for LoReHLT16: a model translation approach",2017,"Machine Translation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032473892&doi=10.1007%2fs10590-017-9205-3&partnerID=40&md5=fbb38b89352dd7cc5dc39e5bd82c6de3","The LoReHLT16 evaluation challenged participants to extract Situation Frames (SFs)—structured descriptions of humanitarian need situations—from monolingual Uyghur text. The ARIEL-CMU SF detector combines two classification paradigms, a manually curated keyword-spotting system and a machine learning classifier. These were applied by translating the models on a per-feature basis, rather than translating the input text. The resulting combined model provides the accuracy of human insight with the generality of machine learning, and is relatively tractable to human analysis and error correction. Other factors contributing to success were automatic dictionary creation, the use of phonetic transcription, detailed, hand-written morphological analysis, and naturalistic glossing for error analysis by humans. The ARIEL-CMU SF pipeline produced the top-scoring LoReHLT16 situation frame detection systems for the metrics SFType, SFType+Place+Need, SFType+Place+Relief, and SFType+Place+Urgency, at each of the three checkpoints. © 2017 Springer Science+Business Media B.V.","Information extraction; LoReHLT; LORELEI; Situation frames","Artificial intelligence; Error correction; Information retrieval; Learning algorithms; Learning systems; Linguistics; Pipelines; Frame detection; Keyword spotting systems; LoReHLT; LORELEI; Model translation; Morphological analysis; Phonetic transcriptions; Situation frames; Rhenium compounds",2-s2.0-85032473892
"Kato S., Shinkuma R.","Priority Control in Communication Networks for Accuracy-Freshness Tradeoff in Realtime Road-Traffic Information Delivery",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731882&doi=10.1109%2fACCESS.2017.2767058&partnerID=40&md5=2042e77de2de6f4c9223e4288ef7fcd4","Delivering real-time road-traffic information to the driver is a straightforward solution to the problem of road traffic congestion. The information is more effective as it is fresh and more accurate. However, real-time road-traffic information delivery has a fundamental problem: an accuracyfreshness tradeoff. Unfortunately, real-time road-traffic information delivery has difficulty satisfying both requirements. To guarantee the freshness, the information needs to be delivered on the basis of the data received by a cloud or edge server before a predetermined deadline. However, only a limited amount of data is received due to bandwidth limitation and processing overhead in communication networks, which results in the poor accuracy of the delivered information. The only way to improve the accuracy is to make the deadline less strict, which results in deteriorating the freshness of information. The proposed system solves this trade-off. The key idea is that data more &#x2018;important&#x2019; for the accuracy of information is more prioritized when the data is transferred in communication networks. In the proposed system, &#x2018;importance&#x2019; is determined by how helpful the data is when the system needs to estimate missing spatial information from a limited amount of received data by using the machine learning technique. In the paper, simulation results verify the proposed system ensures the accuracy of road-traffic information while satisfying the freshness requirement. OAPA","active learning; Communication networks; Data mining; edge computing; Image edge detection; Internet of Things; priority control; Probes; realtime information delivery; road-traffic information; Roads; Servers; Vehicles","Artificial intelligence; Bandwidth; Data mining; Economic and social effects; Edge detection; Internet of things; Learning systems; Probes; Roads and streets; Servers; Street traffic control; Telecommunication networks; Traffic congestion; Transportation; Vehicles; Active Learning; Edge computing; Image edge detection; Priority control; Real-time information; Road traffic informations; Roads; Data communication systems",2-s2.0-85032731882
"Dai J., Hu H., Wu W., Qian Y., Huang D.","Maximal Discernibility Pairs based Approach to Attribute Reduction in Fuzzy Rough Sets",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032745701&doi=10.1109%2fTFUZZ.2017.2768044&partnerID=40&md5=1ea7596027be18bc2191216d30721d6b","Attribute reduction is one of the biggest challenges encountered in computational intelligence, data mining, pattern recognition and machine learning. Effective in feature selection as the rough set theory is, it can only handle symbolic attributes. In order to overcome this drawback, proposed is the fuzzy rough sets, an extended model of rough sets, which is able to deal with imprecision and uncertainty in both symbolic and numerical attributes. Existing attribute selection algorithms based on fuzzy rough set model mainly take the angle of &#x201C;attribute set&#x201D;, which means they define the object function representing the predictive ability for attributes subset with regard to the domain of discourse, rather than follow the view of &#x201C;object pair&#x201D;. Algorithms based on the latter can ignore the object pairs that are already discerned by the selected attributes subsets and thus need only to deal with part of object pairs instead of the whole object pairs from the discourse, which makes such algorithms more efficient in attribute selection. In this paper, we propose the concept of Reduced Maximal Discernibility Pair, which directly adopts the perspective of object pair in the framework of fuzzy rough set model. Then we develop two attribute selection algorithms, named as Reduced Maximal Discernibility Pair Selection (RMDPS) and Weighted Reduced Maximal Discernibility Pair Selection (WRMDPS), based on the reduced maximal discernibility pair. Experiment results show that the proposed algorithms are effective and efficient in attribute selection. IEEE","attribute reduction; Computational modeling; Computer science; Feature extraction; fuzzy discernibility matrix; Fuzzy rough sets; maximal discernibility pairs; Numerical models; Prediction algorithms; Rough sets; Symmetric matrices","Artificial intelligence; Computation theory; Computer science; Data mining; Feature extraction; Learning systems; Numerical models; Pattern recognition; Attribute reduction; Computational model; Discernibility; Discernibility matrix; Fuzzy-rough sets; Prediction algorithms; Symmetric matrices; Rough set theory",2-s2.0-85032745701
"Deng J., Kwok Y.-K.","Large vocabulary automatic chord estimation using bidirectional long short-term memory recurrent neural network with even chance training",2017,"Journal of New Music Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032688233&doi=10.1080%2f09298215.2017.1367820&partnerID=40&md5=4172f978ad587e82f435412b7c96264b","This paper presents an argument for the necessity of a large vocabulary in automatic chord recognition systems, on the grounds of the requirements of machine musicianship. It proposes a system framework with a skewed class-sensitive training scheme that leads to a preliminary solution to large vocabulary automatic chord estimation. This framework applies a bidirectional long short-term memory recurrent neural network architecture, which employs an ‘even chance’ training scheme to make up for the lack of uncommon chords’ exposure. The main drawback of this approach is the low segmentation quality, which inevitably lowers the upper bound of chord estimation accuracy. Under a large vocabulary evaluation, the proposed system can significantly outperform the baseline system in terms of the overall weighted chord symbol recall, and there is no significant difference between them in terms of average chord quality accuracy. The results demonstrate preliminary success in our approach, and also prove the even chance training scheme to be effective in boosting uncommon chord symbol recalls as well as the average chord quality accuracy. © 2017 Informa UK Limited, trading as Taylor & Francis Group","automatic chord estimation; deep learning; large vocabulary; Music information retrieval; recurrent neural network",,2-s2.0-85032688233
"Elforjani M., Shanbr S.","Prognosis of Bearing Acoustic Emission Signals Using Supervised Machine Learning",2017,"IEEE Transactions on Industrial Electronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032746545&doi=10.1109%2fTIE.2017.2767551&partnerID=40&md5=d250d860cb58e2eb653f4f7dc502c63d","Acoustic Emission (AE) technique can be successfully utilized for condition monitoring of various machining and industrial processes. To keep machines function at optimal levels, fault prognosis model to predict the Remaining Useful Life (RUL) of machine components is required. This model is used to analyze the output signals of a machine whilst in operation and accordingly helps to set an early alarm tool that reduces the untimely replacement of components and the wasteful machine downtime. Recent improvements indicate the drive on the way towards incorporation of prognosis and diagnosis machine learning techniques in future machine health management systems. With this in mind, this work employs three supervised machine learning techniques; Support Vector Machine Regression (SVMR), Multilayer Artificial Neural Network (ANN) model and Gaussian Process Regression (GPR), to correlate AE features with corresponding natural wear of slow speed bearings throughout series of laboratory experiments. Analysis of signal parameters such as Signal Intensity Estimator (SIE) and Root Mean Square (RMS) was undertaken to discriminate individual types of early damage. It was concluded that neural networks model with back propagation learning algorithm has an advantage over the other models in estimating the RUL for slow speed bearings if the proper network structure is chosen and sufficient data is provided. IEEE","Acoustic emission; Acoustic Emission; Artificial Neural Network; Artificial neural networks; Condition Monitoring; Data models; Feature extraction; Gaussian Process Regression; Predictive models; Remaining Useful Life; Slow Speed Bearings; Support Vector Machine Regression; Support vector machines; Vibrations","Acoustic emissions; Artificial intelligence; Backpropagation; Backpropagation algorithms; Condition monitoring; Data structures; Digital storage; Estimation; Feature extraction; Gaussian distribution; Gaussian noise (electronic); Industrial emissions; Learning algorithms; Learning systems; Machine components; Neural networks; Regression analysis; Shafts (machine components); Signal analysis; Supervised learning; Support vector machines; Gaussian process regression; Predictive models; Remaining useful lives; Slow speed; Support vector machine regressions; Vibrations; Acoustic emission testing",2-s2.0-85032746545
"Shickel B., Tighe P.J., Bihorac A., Rashidi P.","Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis",2017,"IEEE Journal of Biomedical and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032736236&doi=10.1109%2fJBHI.2017.2767063&partnerID=40&md5=e44a2d8fba171b1a7a4dcc2ba0f161ce","The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and de-identification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research. IEEE","clinical informatics; deep learning; electronic health records; Electronic medical records; Hospitals; Informatics; Machine learning; machine learning; Medical diagnostic imaging; survey","Artificial intelligence; Data mining; Deep learning; Diagnosis; Health; Hospitals; Learning algorithms; Learning systems; Medical computing; Medical imaging; Records management; Surveying; Surveys; Clinical informatics; Electronic health record; Electronic medical record; Informatics; Medical diagnostic imaging; E-learning; diagnostic imaging; electronic health record; electronic medical record; extraction; human; information science; machine learning; phenotype; prediction",2-s2.0-85032736236
"Hafez D., Karabacak A., Krueger S., Hwang Y.-C., Wang L.-S., Zinzen R.P., Ohler U.","McEnhancer: Predicting gene expression via semi-supervised assignment of enhancers to target genes",2017,"Genome Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032202978&doi=10.1186%2fs13059-017-1316-x&partnerID=40&md5=f9832e2a268e2d75929f7c550b08fce6","Transcriptional enhancers regulate spatio-temporal gene expression. While genomic assays can identify putative enhancers en masse, assigning target genes is a complex challenge. We devised a machine learning approach, McEnhancer, which links target genes to putative enhancers via a semi-supervised learning algorithm that predicts gene expression patterns based on enriched sequence features. Predicted expression patterns were 73-98% accurate, predicted assignments showed strong Hi-C interaction enrichment, enhancer-associated histone modifications were evident, and known functional motifs were recovered. Our model provides a general framework to link globally identified enhancers to targets and contributes to deciphering the regulatory genome. © 2017 The Author(s).","Drosophila melanogaster; Enhancer to target gene assignment; Gene expression; Gene regulation; Interpolated Markov model; Machine learning; Semi-supervised model","deoxyribonuclease; accuracy; Article; biological model; data analysis software; Drosophila melanogaster; embryo development; epigenetics; gene assignment; gene control; gene expression regulation; gene interaction; gene sequence; gene targeting; genetic association; histone modification; human; in vivo study; learning algorithm; nonhuman; prediction; reporter gene",2-s2.0-85032202978
"Liu Y., Huang C.","Scene Classification via Triplet Networks",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032748357&doi=10.1109%2fJSTARS.2017.2761800&partnerID=40&md5=eaaf12fb7e7876dfc3b6fbe96739495b","Scene classification is a fundamental task for automatic remote sensing image understanding. In recent years, convolutional neural networks have become a hot research topic in the remote sensing community, and have made great achievements in scene classification. Deep convolutional networks are primarily trained in a supervised way, requiring huge volumes of labeled training samples. However, clearly labeled remote sensing data are usually limited. To address this issue, in this paper, we propose a novel scene classification method via triplet networks, which use weakly labeled images as network inputs. Besides, we initiate a theoretical study on the three existing loss functions for triplet networks, analyzing their different underlying mechanisms for dealing with &#x201C;hard&#x201D; and/or &#x201C;easy&#x201D; triplets during training. Furthermore, four new loss functions are constructed, aiming at laying more stress on &#x201C;hard&#x201D; triplets to improve classification accuracy. Extensive experiments have been conducted, and the experimental results show that triplet networks coupled with our proposed losses achieve a state-of-the-art performance in scene classification tasks. IEEE","Deep learning; difference loss; Feature extraction; Machine learning; Neural networks; ratio loss; Remote sensing; scene classification; Semantics; Training; triplet networks","Convolution; Deep learning; Feature extraction; Learning systems; Neural networks; Personnel training; Semantics; Classification accuracy; Convolutional networks; Convolutional neural network; Hot research topics; Remote sensing data; Remote sensing images; Scene classification; State-of-the-art performance; Remote sensing",2-s2.0-85032748357
"Nizamani Z.A., Liu H., Chen D.M., Niu Z.","Automatic approval prediction for software enhancement requests",2017,"Automated Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032383103&doi=10.1007%2fs10515-017-0229-y&partnerID=40&md5=780110a038fafe0674e4b59cd87996e5","Software applications often receive a large number of enhancement requests that suggest developers to fulfill additional functions. Such requests are usually checked manually by the developers, which is time consuming and tedious. Consequently, an approach that can automatically predict whether a new enhancement report will be approved is beneficial for both the developers and enhancement suggesters. With the approach, according to their available time, the developers can rank the reports and thus limit the number of reports to evaluate from large collection of low quality enhancement requests that are unlikely to be approved. The approach can help developers respond to the useful requests more quickly. To this end, we propose a multinomial naive Bayes based approach to automatically predict whether a new enhancement report is likely to be approved or rejected. We acquire the enhancement reports of open-source software applications from Bugzilla for evaluation. Each report is preprocessed and modeled as a vector. Using these vectors with their corresponding approval status, we train a Bayes based classifier. The trained classifier predicts approval or rejection of the new enhancement reports. We apply different machine learning and neural network algorithms, and it turns out that the multinomial naive Bayes classifier yields the highest accuracy with the given dataset. The proposed approach is evaluated with 40,000 enhancement reports from 35 open source applications. The results of tenfold cross validation suggest that the average accuracy is up to 89.25%. © 2017 Springer Science+Business Media, LLC","Document classification; Machine learning; Multinomial naive Bayes; Software enhancements","Application programs; Artificial intelligence; Classification (of information); Classifiers; Forecasting; Information dissemination; Information retrieval systems; Learning systems; Open systems; Quality control; Software engineering; Cross validation; Document Classification; Enhancement requests; Multinomial naive bayes; Neural network algorithm; Open source application; Software applications; Software enhancements; Open source software",2-s2.0-85032383103
"Ong E., Wong M.U., He Y.","Identification of new features from known bacterial protective vaccine antigens enhances rational vaccine design",2017,"Frontiers in Immunology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032205035&doi=10.3389%2ffimmu.2017.01382&partnerID=40&md5=55a789af8bae22a57eed8c5d1f4a2819","With many protective vaccine antigens reported in the literature and verified experimentally, how to use the knowledge mined from these antigens to support rational vaccine design and study underlying design mechanism remains unclear. In order to address the problem, a systematic bioinformatics analysis was performed on 291 Gram-positive and Gram-negative bacterial protective antigens with experimental evidence manually curated in the Protegen database. The bioinformatics analyses evaluated included subcellular localization, adhesin probability, peptide signaling, transmembrane a-helix and β-barrel, conserved domain, Clusters of Orthologous Groups, and Gene Ontology functional annotations. Here we showed the critical role of adhesins, along with subcellular localization, peptide signaling, in predicting secreted extracellular or surface-exposed protective antigens, with mechanistic explanations supported by functional analysis. We also found a significant negative correlation of transmembrane a-helix to antigen protectiveness in Gram-positive and Gram-negative pathogens, while a positive correlation of transmembrane β-barrel was observed in Gram-negative pathogens. The commonly less-focused cytoplasmic and cytoplasmic membrane proteins could be potentially predicted with the help of other selection criteria such as adhesin probability and functional analysis. The significant findings in this study can support rational vaccine design and enhance our understanding of vaccine design mechanisms. © 2017 Ong, Wong and He.","Adhesin probability; Conserved domains; Functional analysis; Protective antigen; Reverse vaccinology; Subcellular localization; Transmembrane proteins; Vaccine design","adhesin; bacterial vaccine; basic helix loop helix transcription factor; protective agent; amino acid sequence; Article; bioinformatics; cell viability; cellular distribution; immune response; machine learning; nonhuman; proteomics; sensitivity analysis; signal detection; signal transduction",2-s2.0-85032205035
"Meekings S., Schnabel M.A.","Big data on individuals in the architectural design process: combining individual’s data with the architects toolset",2017,"International Journal of Parallel, Emergent and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032352592&doi=10.1080%2f17445760.2017.1390091&partnerID=40&md5=239d91afc89a42c935f2e27972b32af9","It is increasingly common to live in continual flux between reality and virtuality – for architecture this means a dwindling focus on the built environment. For the architectural discipline to be resilient in the face of these rapidly changing user-demands, a proactive relationship with our digital environment is required. It is proposed that the quantity of data that is collected about individuals is becoming large enough to qualify as big data, despite only pertaining to a single person. It has all the hallmarks of the big data phenomon, namely large and diverse fields which with the help of machine learning and cross-referencing can uncover unforeseen patterns. This paper explores how personal big data could be used, with the potential to impact future architectural workflows. We present ways that personal data can be used to develop special connections for architectural design processes. By comparing multiple single-person data sets two key issues are discussed; sourcing relevant data and three-dimensionalizing this data with a particular focus on connections. The paper concludes with a discussion about the future of data as an instrument to aid architectural design processes. © 2017 Informa UK Limited, trading as Taylor & Francis Group","Big data; built environment; digital identity; social media","Architectural design; Architecture; Electronic document identification systems; Learning systems; Built environment; Digital environment; Digital identity; Diverse fields; Key Issues; Social media; User demands; Work-flows; Big data",2-s2.0-85032352592
"Chau T.K., Yu S.S., Fernando T., Ho-Ching Iu H., Small M.","A Load-Forecasting-Based Adaptive Parameter Optimization Strategy of STATCOM Using ANNs for Enhancement of LFOD in Power Systems",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032736096&doi=10.1109%2fTII.2017.2767069&partnerID=40&md5=83c9f8c6470737e6d88ea686d258850d","This paper proposes a load-oriented control parameter optimization strategy for STATCOM to enhance low-frequency oscillation damping (LFOD) and improve stability of overall complex power systems. Frequency deviations of generators of interest are employed as the input signals of the designed supplementary damping controller of STATCOM. In order to obtain the optimal load-oriented control parameters, a day-ahead load-forecasting scheme is devised, using artificial neural network (ANN) learning techniques. The ANN is trained by a set of data over a 4-year period, and then the control parameters are optimized using Particle Swarm Optimization (PSO) technique by minimizing the critical damping index (CDI). The proposed control strategy is implemented in an IEEE standard complex power system, and the numerical results demonstrate that the low-frequency oscillations (LFOs) of the power system can be effectively mitigated using the proposed controller. Compared to conventional robust controller with universal parameters, this novel load-oriented optimal control strategy shows its superiority in alleviating LFOs and enhancing the overall stability of the power system. Since the proposed control scheme aims to adaptively adjust the controller parameters in correspondence to load variations, this study is envisaged to have practical utilizations in industrial applications. IEEE","Artificial neural networks; Automatic voltage control; Damping; Generators; Load forecasting; Load modeling; Low-frequency oscillation damping control; Mathematical model; Power system stability; PSO; STATCOM","Complex networks; Control system stability; Controllers; Damping; Electric current regulators; Electric load management; Electric machine control; Electric power plant loads; Forecasting; Gas generators; Mathematical models; Neural networks; Optimal control systems; Particle swarm optimization (PSO); Semiconducting gallium arsenide; Static synchronous compensators; System stability; Automatic voltage control; Load forecasting; Load modeling; Low frequency oscillations; Power system stability; Electric power system control",2-s2.0-85032736096
"George D., Lehrach W., Kansky K., Lázaro-Gredilla M., Laan C., Marthi B., Lou X., Meng Z., Liu Y., Wang H., Lavin A., Phoenix D.S.","A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs",2017,"Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032354846&doi=10.1126%2fscience.aag2612&partnerID=40&md5=68dac4cde666d46f7047ef3b64dcacb5","Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence. © 2017 American Association for the Advancement of Science.",,,2-s2.0-85032354846
"Oguz C., Sen S.K., Davis A.R., Fu Y.-P., O'Donnell C.J., Gibbons G.H.","Genotype-driven identification of a molecular network predictive of advanced coronary calcium in ClinSeq® and Framingham Heart Study cohorts",2017,"BMC Systems Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032201235&doi=10.1186%2fs12918-017-0474-5&partnerID=40&md5=e95db3d1bdaf8d55edc3a6d9acaa347a","Background: One goal of personalized medicine is leveraging the emerging tools of data science to guide medical decision-making. Achieving this using disparate data sources is most daunting for polygenic traits. To this end, we employed random forests (RFs) and neural networks (NNs) for predictive modeling of coronary artery calcium (CAC), which is an intermediate endo-phenotype of coronary artery disease (CAD). Methods: Model inputs were derived from advanced cases in the ClinSeq® discovery cohort (n=16) and the FHS replication cohort (n=36) from 89th-99th CAC score percentile range, and age-matched controls (ClinSeq® n=16, FHS n=36) with no detectable CAC (all subjects were Caucasian males). These inputs included clinical variables and genotypes of 56 single nucleotide polymorphisms (SNPs) ranked highest in terms of their nominal correlation with the advanced CAC state in the discovery cohort. Predictive performance was assessed by computing the areas under receiver operating characteristic curves (ROC-AUC). Results: RF models trained and tested with clinical variables generated ROC-AUC values of 0.69 and 0.61 in the discovery and replication cohorts, respectively. In contrast, in both cohorts, the set of SNPs derived from the discovery cohort were highly predictive (ROC-AUC ≥0.85) with no significant change in predictive performance upon integration of clinical and genotype variables. Using the 21 SNPs that produced optimal predictive performance in both cohorts, we developed NN models trained with ClinSeq® data and tested with FHS data and obtained high predictive accuracy (ROC-AUC=0.80-0.85) with several topologies. Several CAD and ""vascular aging"" related biological processes were enriched in the network of genes constructed from the predictive SNPs. Conclusions: We identified a molecular network predictive of advanced coronary calcium using genotype data from ClinSeq® and FHS cohorts. Our results illustrate that machine learning tools, which utilize complex interactions between disease predictors intrinsic to the pathogenesis of polygenic disorders, hold promise for deriving predictive disease models and networks. © 2017 The Author(s).","Case-control study; Coronary artery calcium; Coronary heart disease; Genotype data; Neural networks; Random forest; Systems biology",,2-s2.0-85032201235
"Xiao-jian D., Yuan L., Zhi-feng Z., xin X.","Optimization extreme learning machine with ν regularization",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013481733&doi=10.1016%2fj.neucom.2016.05.114&partnerID=40&md5=6e11e9b89754dd4bc93329af31fbd210","The problem of choosing error penalty parameter C for optimization extreme learning machine (OELM) is that it can take any positive value for different applications and it is therefore hard to choose correctly. In this paper, we reformulated OELM to take a new regularization parameter ν (ν-OELM) which is inspired by Schölkopf et al. The regularization in terms of ν is bounded between 0 and 1, and is easier to interpret as compared to C. This paper shows that: (1) ν-OELM and ν-SVM have similar dual optimization formulation, but ν-OELM has less optimization constraints due to its special capability of class separation and (2) experiment results on both artificial and real binary classification problems show that ν-OELM tends to achieve better generalization performance than ν-SVM, OELM and other popular machine learning approaches, and it is computationally efficient on high dimension data sets. Additionally, the optimal parameter ν in ν-OELM can be easily selected from few candidates. © 2017 Elsevier B.V.","Classification; Parameter selection; ν-optimization extreme learning machine","Classification (of information); Knowledge acquisition; Binary classification problems; Computationally efficient; Extreme learning machine; Generalization performance; High-dimension data; Machine learning approaches; Parameter selection; Regularization parameters; Learning systems; Article; classification; machine learning; mathematical computing; mathematical model; mathematical parameters; nu optimization extreme learning machine; nu support vector machine; priority journal; process design",2-s2.0-85013481733
"Sun Y., Chen Y., Yuan Y., Wang G.","Dynamic adjustment of hidden layer structure for convex incremental extreme learning machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012892946&doi=10.1016%2fj.neucom.2016.07.072&partnerID=40&md5=a47203ae045126d072d7898de349343f","Extreme Learning Machine (ELM) is a learning algorithm based on generalized single-hidden-layer feed-forward neural network. Since ELM has an excellent performance on regression and classification problems, it has been paid more and more attention recently. The determination of structure of ELM plays a vital role in ELM applications. Essentially, determination of the structure of ELM is equivalent to the determination of the hidden layer structure. Utilizing a smaller scale of the hidden layer structure can promote faster running speed. In this paper, we propose algorithm PCI-ELM (Pruned-Convex Incremental Extreme Learning Machine) based on CI-ELM (Convex Incremental Extreme Learning Machine). Furthermore, we also present an improved PCI-ELM algorithm, EPCI-ELM (Enhanced Pruned-Convex Incremental Extreme Learning Machine), which introduces a filtering strategy for PCI-ELM during the neurons adding process. In order to adjust the single-hidden-layer feed-forward neural network more flexibly and achieve the most compact form of the hidden layer structure, in this paper, we propose an algorithm which can dynamically determine hidden layer structure, DCI-ELM (Dynamic Convex Incremental Extreme Learning Machine). At the end of this paper, we verify the performance of PCI-ELM, EPCI-ELM and DCI-ELM. The results show that PCI-ELM, EPCI-ELM and DCI-ELM control hidden layer structure very well and construct the more compact single-hidden-layer feed-forward neural network. © 2017 Elsevier B.V.","Convex optimal increment; Dynamic adjustment; Extreme learning machine; Feed-forward neural network","Convex optimization; Knowledge acquisition; Learning algorithms; Convex optimal increment; Dynamic adjustment; ELM algorithms; Extreme learning machine; Filtering strategies; Hidden layers; Incremental extreme learning machine; Single-hidden layer feed-forward neural network; Learning systems; algorithm; Article; artificial neural network; controlled study; enhanced pruned convex incremental extreme learning machine; information processing; machine learning; measurement accuracy; priority journal; process optimization; pruned convex incremental extreme learning machine; running; single hidden layer feed forward neural network",2-s2.0-85012892946
"Xiao W., Zhang J., Li Y., Zhang S., Yang W.","Class-specific cost regulation extreme learning machine for imbalanced classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012879403&doi=10.1016%2fj.neucom.2016.09.120&partnerID=40&md5=397d78b31f8c8e4bc11d5d7355f8faa5","Due to its much faster speed and better generalization performance, extreme learning machine (ELM) has attracted much attention as an effective learning approach. However, ELM rarely involves strategies for imbalanced data distributions which may exist in many fields. Existing approaches for imbalance learning only consider the effect of the number of the class samples ignoring the dispersion degree of the data, and may lead to the suboptimal learning results. In this paper, we will propose a novel ELM, class-specific cost regulation extreme learning machine (CCR-ELM), together with its kernel based extension, for binary and multiclass classification problems with imbalanced data distributions. CCR-ELM introduces class-specific regulation cost for misclassification of each class in the performance index as the tradeoff of structural risk and empirical risk. The performance of CCR-ELM is verified using a number of benchmark datasets and the real blast furnace status diagnosis problem. Experimental results show that CCR-ELM can achieve better performance for classification problems with imbalanced data distributions than the original ELM and existing ELM imbalance learning approach, and the kernel based CCR-ELM can improve the performance further. © 2017","Blast furnace status diagnosis; Class-specific cost regulation extreme learning machine; Extreme learning machine; Imbalanced data distribution","Benchmarking; Blast furnaces; Classification (of information); Costs; Knowledge acquisition; Benchmark datasets; Extreme learning machine; Generalization performance; Imbalanced classification; Imbalanced data; Misclassifications; Multiclass classification problems; Performance indices; Learning systems; Article; artificial neural network; benchmarking; class specific cost regulation extreme learning machine; classification algorithm; empirical risk; furnace; imbalanced classification; information processing; kernel method; machine learning; mathematical model; performance index; performance measurement system; priority journal; problem based learning; risk; structural risk",2-s2.0-85012879403
"Luo M., Zhang L., Liu J., Guo J., Zheng Q.","Distributed extreme learning machine with alternating direction method of multiplier",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013403099&doi=10.1016%2fj.neucom.2016.03.112&partnerID=40&md5=da209fb5e5375ea1722ca4cc63e75d49","Extreme learning machine, as a generalized single-hidden-layer feedforward network, has achieved much attention for its extremely fast learning speed and good generalization performance. However, big data often makes a challenge in large scale learning of extreme learning machine due to the memory limitation of single machine as well as the distributed manner of large scale data in many applications. For the purpose of relieving the limitation of memory with big data, in this paper, we exploit a novel distributed model to implement the extreme learning machine algorithm in parallel for large-scale data set, namely distributed extreme learning machine (DELM). A corresponding algorithm is developed on the basis of alternating direction method of multipliers which has shown its effectiveness in distributed convex optimization. Finally, extensive experiments on some benchmark data sets are carried out to illustrate the effectiveness and superiority of the proposed DELM method with an analysis on the performance of speedup, scaleup and sizeup. © 2017 Elsevier B.V.","Alternating direction method of multiplier; Extreme learning machine; Neuron work","Benchmarking; Convex optimization; Knowledge acquisition; Learning systems; Network layers; Optimization; Alternating direction method of multipliers; Distributed modeling; Extreme learning machine; Feed-forward network; Generalization performance; Large scale data sets; Large-scale learning; Single- machines; Big data; algorithm; analytic method; Article; data base; distributed extreme learning machine; machine learning; mathematical analysis; mathematical computing; mathematical parameters; priority journal; process optimization; statistical analysis; statistical model; statistical parameters",2-s2.0-85013403099
"Mao W., Wang J., He L., Tian Y.","Online sequential prediction of imbalance data with two-stage hybrid strategy by extreme learning machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012898797&doi=10.1016%2fj.neucom.2016.05.111&partnerID=40&md5=ff14181c6e79d2e359ece2d1af03b31b","In many practical engineering applications, data tend to be collected in online sequential way with imbalanced class. Many traditional machine learning methods such as support vector machine and so on generally get biased classifier which leads to lower classification precision for minor class than major class. To get fast and efficient classification, a new online sequential extreme learning machine method with two-stage hybrid strategy is proposed. In offline stage, data-based strategy is employed, and the principal curve is introduced to model the distribution of minority class data. In online stage, algorithm-based strategy is employed, and a new leave-one-out cross-validation method using Sherman–Morrison matrix inversion lemma is proposed to tackle online imbalance data, meanwhile, with add-delete mechanism for updating network weights. And the rationality of this strategy is proved theoretically. The proposed method is evaluated on four UCI datasets and the real-world Macau air pollutant forecasting dataset. The experimental results show that, the proposed method outperforms the classical ELM, OS-ELM and meta-cognitive OS-ELM in terms of generalization performance and numerical stability. © 2017 Elsevier B.V.","Extreme learning machine; Imbalance problem; Leave-one-out cross-validation; Online sequential learning; Principal curve","Knowledge acquisition; Learning systems; Numerical methods; Statistical methods; Extreme learning machine; Imbalance problem; Leave-one-out cross validations; Principal curve; Sequential learning; E-learning; air pollutant; algorithm; Article; artificial neural network; cognition; computer model; computer prediction; conceptual framework; data base; forecasting; Macao; machine learning; online system; priority journal; support vector machine",2-s2.0-85012898797
"Yu Y., Sun Z.","Sparse coding extreme learning machine for classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012931523&doi=10.1016%2fj.neucom.2016.06.078&partnerID=40&md5=f7bfc4cc617a0ce97ca6b55facd3c87e","As one of supervised learning algorithms, extreme learning machine (ELM) has been proposed for training single-hidden-layer feedforward neural networks and shown great generalization performance. ELM randomly assigns the weights and biases between input and hidden layers and only learns the weights between hidden and output layers. Physiological research has shown that neurons at the same layer are laterally inhibited to each other such that outputs of each layer are sparse. However, it is difficult for ELM to accommodate the lateral inhibition by directly using random feature mapping. Therefore, this paper proposes a sparse coding ELM (ScELM) algorithm, which can map the input feature vector into a sparse representation. In this proposed ScELM algorithm, an unsupervised way is used for sparse coding and dictionary is randomly assigned rather than learned. Gradient projection based method is used for the sparse coding. The output weights are trained through the same supervised way as ELM. Experimental results on the benchmark datasets have shown that this proposed ScELM algorithm can outperform other state-of-the-art methods in terms of classification accuracy. © 2017 Elsevier B.V.","Extreme learning machine; Gradient projection; Sparse coding","Classification (of information); Codes (symbols); Feedforward neural networks; Knowledge acquisition; Learning algorithms; Network layers; Radial basis function networks; Classification accuracy; Extreme learning machine; Generalization performance; Gradient projections; Single-hidden layer feedforward neural networks; Sparse coding; Sparse representation; State-of-the-art methods; Learning systems; Article; benchmarking; classification algorithm; controlled study; extreme learning machine; machine learning; measurement accuracy; priority journal; sparse coding extreme learning machine; support vector machine",2-s2.0-85012931523
"Gautam C., Tiwari A., Leng Q.","On the construction of extreme learning machine for online and offline one-class classification—An expanded toolbox",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013371957&doi=10.1016%2fj.neucom.2016.04.070&partnerID=40&md5=585c6df00cc6e45b93b1107bcd6f0979","One-class classification (OCC) has been prime concern for researchers and effectively employed in various disciplines. But, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. In this paper, we present six OCC methods and their thirteen variants based on extreme learning machine (ELM) and online sequential ELM (OSELM). Our proposed classifiers mainly lie in two categories: reconstruction based and boundary based, where three proposed classifiers belong to reconstruction based and three belong to boundary based. We are presenting both types of learning viz., online and offline learning for OCC. Out of six methods, four are offline and remaining two are online methods. Out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. We present a comprehensive discussion on these methods and their comparison to each other. Kernel feature mapping based approaches have been tested with RBF kernel and online version of one-class classifiers is tested with both types of nodes viz., additive and RBF. It is well known fact that threshold decision is a crucial factor in case of OCC, so, three different threshold deciding criteria have been employed so far and analyze the effectiveness of one threshold deciding criteria over another. Further, these methods are tested on two artificial datasets to check their boundary construction capability and on eight benchmark datasets from different discipline to evaluate the performance of the classifiers. Our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and ELM based two one-class classifiers. Through proposed one-class classifiers, we intend to expand the functionality of the most used toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with all the present features of the toolbox. © 2017 Elsevier B.V.","Autoassociative ELM (AAELM); Extreme learning machine (ELM); One-class classification (OCC); One-class ELM (OCELM); Online sequential ELM (OSELM)","Benchmarking; Classification (of information); Iterative methods; Knowledge acquisition; Learning systems; Mapping; Radial basis function networks; Autoassociative ELM (AAELM); Extreme learning machine; One-class Classification; One-class ELM (OCELM); Online sequential ELM (OSELM); E-learning; accuracy; Article; classification algorithm; classifier; data analysis; kernel method; linear system; machine learning; mathematical analysis; mathematical model; online system; prediction; priority journal; support vector machine",2-s2.0-85013371957
"Yin J.-C.","A variable-structure online sequential extreme learning machine for time-varying system prediction",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012249733&doi=10.1016%2fj.neucom.2016.03.114&partnerID=40&md5=02cbd69838de50be1e1bbe44bc84f0ba","A variable-structure online sequential extreme learning machine (OS-ELM) is proposed by incorporating a hidden units pruning strategy. As conventional OS-ELM increases network dimensionality by adding newly-received samples as hidden units, the hidden layer dimension would expand and result in “dimensionality curse” finally. Furthermore, the vast number of hidden units cannot represent time-varying dynamics adaptively and would deteriorate the network generalization capability. Therefore, there is a practical need to adjust the dimension of OS-ELM not only by adding hidden units but also by simultaneously pruning superfluous units which contribute less to the output. To evaluate the individual contribution of existing hidden units, an index is proposed referred to as normalized error reduction ratio. The variable structure OS-ELM adds newly received samples in hidden units, and prunes those units contribute less to current dynamics from network simultaneously, thus the resulted network possesses parsimonious structure which can represent current system dynamics more efficiently. The online network structure adjustment approach can handle samples which are presented one-by-one or chuck-by-chuck. The variable-structure OS-ELM (VS-OSELM) can be implemented for online identification and prediction of time-varying systems. In this study, to evaluate the efficiency of VS-OSELM, it was implemented for real-time prediction of tidal level change which is a complex time-varying process. Online tidal prediction simulations is conducted based on the real measured tidal and meteorological data of Old Port Tampa in Florida, United States. Simulation results demonstrate that the proposed variable-structure OS-ELM is suitable for identification and prediction of complex time-varying systems with high prediction accuracy and fast computation speed. © 2017 Elsevier B.V.","Online sequential extreme learning machine; Pruning strategy; Real-time prediction; Tidal prediction; Time-varying system","Chucks; Complex networks; E-learning; Forecasting; Knowledge acquisition; Learning systems; Meteorology; Online systems; Fast computation speed; Generalization capability; On-line identification; Online sequential extreme learning machine; Pruning strategy; Real-time prediction; Time-varying dynamics; Time-varying process; Time varying systems; Article; controlled study; error; machine learning; measurement accuracy; online system; prediction; priority journal; statistical parameters; time varying system; United States; variable structure online sequential extreme learning machine",2-s2.0-85012249733
"Li S., Niu X., Dou Y., Lv Q., Wang Y.","Heterogeneous blocked CPU-GPU accelerate scheme for large scale extreme learning machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013478449&doi=10.1016%2fj.neucom.2016.05.112&partnerID=40&md5=f28dae57dc2a34944e25ad022485fd09","Extreme learning machine (ELM) has been intensively studied during the last decade due to its high efficiency, effectiveness and easy to implement. Recently, a variant of ELM named local receptive fields based ELM (ELM-LRF) has been proposed to reduce the global connections and introduce local receptive fields to the input layer. However, an ELM-LRF model with large number of hidden neurons spend plenty of time on solving large scale Moore-Penrose Matrix Inversion (MPMI) problem which has heavy computational cost and needs much more runtime memory. Moreover, this procedure can not be directly accelerated by GPU platforms due to the limited memory of GPU devices. In this paper, we propose three efficient approaches to perform ELM-LRF on GPU platform. First we propose a novel blocked LU decomposition algorithm, which overcomes the limitation of global memory size so that any size of ELM-LRF models can be trained. Furthermore, an efficient blocked Cholesky decomposition algorithm is presented to accelerate blocked LU decomposition algorithm according to matrix characteristics in the ELM-LRF model. Finally we present a heterogeneous blocked CPU-GPU parallel algorithm to fully exploit resources on a GPU node such as to accelerate blocked Cholesky decomposition algorithm furthermore in the ELM-LRF model. © 2017 Elsevier B.V.","Blocked CPU-GPU accelerate algorithm; ELM-LRF; GPU","Graphics processing unit; Knowledge acquisition; Cholesky decomposition; Computational costs; ELM-LRF; Extreme learning machine; Lu decomposition; Matrix inversions; Number of hidden neurons; Receptive fields; Learning systems; algorithm; Article; blocked Cholesky decomposition algorithm; blocked LU decomposition algorithm; controlled study; extreme learning machine; heterogeneous blocked CPU GPU accelerate algorithm; intermethod comparison; local receptive fields based extreme learning machine; machine learning; Moore Penrose Matrix Inversion; priority journal",2-s2.0-85013478449
"Jiang M., Pan Z., Li N.","Multi-label text categorization using L21-norm minimization extreme learning machine",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020164725&doi=10.1016%2fj.neucom.2016.04.069&partnerID=40&md5=bfacc70ea149c84905ab8073d2df4b99","Extreme learning machine (ELM) is extended from the generalized single hidden layer feedforward networks where the input weights of the hidden layer nodes can be assigned randomly. It has been widely used for its much faster learning speed and less manual works. Considering the field of multi-label text classification, in this paper, we propose an ELM based algorithm combined with L21-norm minimization of the output weights matrix called L21-norm Minimization ELM, which not only fully inherits the merits of ELM but also facilitates group sparsity and reduces complexity of the learning model. Extensive experiments on several benchmark data sets show that our proposed algorithm can obtain superior performances compared with other common multi-label classification algorithms. © 2017 Elsevier B.V.","Extreme learning machine; L21-norm minimization; Multi-label learning; Text categorization","Classification (of information); Knowledge acquisition; Learning algorithms; Learning systems; Network layers; Extreme learning machine; Feed-forward network; Group sparsities; Hidden layer nodes; Multi label classification; Multi-label learning; Multi-label text classification; Text categorization; Text processing; algorithm; Article; classification; experimental study; extreme learning machine; intermethod comparison; machine learning; mathematical computing; mathematical model; mathematical parameters; priority journal; randomization; weight",2-s2.0-85020164725
"Pang J., Gu Y., Xu J., Kong X., Yu G.","Parallel multi-graph classification using extreme learning machine and MapReduce",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012904239&doi=10.1016%2fj.neucom.2016.03.111&partnerID=40&md5=42f25778a7d15f10e34be45d1b539c74","A multi-graph is represented by a bag of graphs and modeled as a generalization of a multi-instance. Multi-graph classification is a supervised learning problem, which has a wide range of applications, such as scientific publication categorization, bio-pharmaceutical activity tests and online product recommendation. However, existing algorithms are limited to process small datasets due to high computation complexity of multi-graph classification. Specially, the precision is not high enough for a large dataset. In this paper, we propose a scalable and high-precision parallel algorithm to handle the multi-graph classification problem on massive datasets using MapReduce and extreme learning machine. Extensive experiments on real-world and synthetic graph datasets show that the proposed algorithm is effective and efficient. © 2017 Elsevier B.V.","Classification; Extreme learning machine; MapReduce; Multi-graph","Knowledge acquisition; Learning systems; Computation complexity; Extreme learning machine; Graph classification; Map-reduce; Multi-graph; Online product recommendations; Scientific publications; Supervised learning problems; Classification (of information); accuracy; algorithm; Article; classification; conceptual framework; data base; machine learning; mathematical computing; multi graph classification; online analysis; parallel algorithm; pharmacological procedures; practice guideline; priority journal; publication; statistical model",2-s2.0-85012904239
"Peng Y., Lu B.-L.","Discriminative extreme learning machine with supervised sparsity preserving for image classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012920127&doi=10.1016%2fj.neucom.2016.05.113&partnerID=40&md5=fb2c86788d22fcb80598d1dd1f67ce77","In order to seek non-propagation method to train generalized single-hidden layer feed forward neural networks, extreme learning machine was proposed, which has been proven to be an effective and efficient model for both multi-class classification and regression. Different from most of existing studies which consider extreme learning machine as a classifier, we make improvements on it to let it become a feature extraction model in this paper. Specifically, a discriminative extreme learning machine with supervised sparsity preserving (SPELM) model is proposed. From the hidden layer to output layer, SPELM performs as a subspace learning method by considering the discriminative as well as sparsity information of data. The sparsity information of data is identified by solving a supervised sparse representation objective. Experiments are conducted on four widely used image benchmark data sets and the classification results demonstrate the effectiveness of the proposed SPELM model. © 2017 Elsevier B.V.","Extreme learning machine; Group sparsity; Image classification; Sparse representation; Sparsity preserving","Feature extraction; Image classification; Knowledge acquisition; Learning systems; Classification results; Extreme learning machine; Group sparsities; Multi-class classification; Propagation method; Single-hidden layer feed-forward neural network; Sparse representation; Sparsity preserving; Classification (of information); Article; artificial neural network; classification; data analysis; data base; experimental study; image analysis; information processing; machine learning; mathematical computing; mathematical model; mathematical parameters; priority journal; regression analysis; statistical parameters; supervised sparsity preserving model",2-s2.0-85012920127
"Oh B.-S., Sun L., Ahn C.S., Yeo Y.K., Yang Y., Liu N., Lin Z.","Extreme learning machine based mutual information estimation with application to time-series change-points detection",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013408042&doi=10.1016%2fj.neucom.2015.11.138&partnerID=40&md5=2587df5f421945bb03a2503d35fabe24","In this paper, we propose an efficient parameter tuning-free squared-loss mutual information (SMI) estimator in a form of a radial basis function (RBF) network. The input layer of the proposed network propagates a sample pair of two random variables to the hidden layer. The propagated samples are then transformed by a set of Gaussian RBF kernels with randomly determined kernel centers and widths similar to that in an extreme learning machine. The output layer adopts a linear weighting scheme which can be analytically estimated. Our empirical results show that the proposed estimator outperforms the competing state-of-the-art SMI estimators in terms of computational efficiency while showing the comparable estimation accuracy performance. Moreover, the proposed model achieves promising results in an application study of time-series change-points detection and driving stress. © 2017 Elsevier B.V.","Change-points detection; Density ratio approximation; Driving stress; Electrocardiogram; Extreme learning machine; Squared-loss mutual information estimation","Computational efficiency; Electrocardiography; Knowledge acquisition; Radial basis function networks; Time series; Change-points; Density ratio; Driving stress; Extreme learning machine; Mutual information estimations; Learning systems; accuracy; algorithm; Article; car driving; electrocardiogram; information processing; kernel method; least square analysis; machine learning; priority journal; radial basis function; squared loss mutual information; statistical analysis; time series analysis",2-s2.0-85013408042
"Hussain T., Siniscalchi S.M., Lee C., Wang S., Tsao Y., Liao W.","Experimental Study on Extreme Learning Machine Applications for Speech Enhancement",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032456314&doi=10.1109%2fACCESS.2017.2766675&partnerID=40&md5=6966c8ec765ef1e5b601f6d3e45eb672","In wireless telephony and audio data mining applications, it is desirable that noise suppression can be made robust against changing noise conditions and operate in real time (or faster). The learning effectiveness and speed of artificial neural networks are therefore critical factors in applications for speech enhancement tasks. To address these issues, we present an extreme learning machine (ELM) framework, aimed at the effective and fast removal of background noise from a single-channel speech signal, based on a set of randomly chosen hidden units and analytically determined output weights. Because feature learning with shallow ELM may not be effective for natural signals, such as speech, even with a large number of hidden nodes, hierarchical ELM (H-ELM) architectures are deployed by leveraging sparse auto-encoders. In this manner, we not only keep all the advantages of deep models in approximating complicated functions and maintaining strong regression capabilities, but we also overcome the cumbersome and time-consuming features of both greedy layer-wise pre-training and back-propagation (BP) based fine tuning schemes, which are typically adopted for training deep neural architectures. The proposed ELM framework was evaluated on the Aurora&#x2013;4 speech databases. The Aurora4 task provides relatively limited training data, and test speech data corrupted with both additive noise and convolutive distortions for matched and mismatched channels and signal-to-noise ratio (SNR) conditions. In addition, the task includes a subset of testing data involving noise types and SNR levels that are not seen in the training data. The experimental results indicate that when the amount of training data is limited, both ELM and H-ELM based speech enhancement techniques consistently outperform the conventional BP-based shallow and deep learning algorithms, in terms of standardized objective evaluations, under various testing conditions. OAPA","Artificial Neural Networks; Extreme Learning Machine; Hierarchical Extreme Learning Machines; Speech Enhancement","Additive noise; Audio acoustics; Backpropagation; Data mining; Knowledge acquisition; Learning algorithms; Learning systems; Network architecture; Neural networks; Speech; Speech communication; Speech enhancement; Complicated functions; Extreme learning machine; Learning effectiveness; Limited training data; Neural architectures; Objective evaluation; Testing conditions; Wireless telephony; Signal to noise ratio",2-s2.0-85032456314
"Wang Z., Zhao Y., Yuan Y., Wang G., Chen L.","Extreme Learning Machine for large-scale graph classification based on MapReduce",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012882041&doi=10.1016%2fj.neucom.2016.04.071&partnerID=40&md5=cf7e88c620ddf4ad0e491550266f98f4","Discriminative subgraph mining from a large collection of graph objects is a crucial problem for graph classification. Several main memory-based approaches have been proposed to mine discriminative subgraphs, but they always lack scalability and are not suitable for large-scale graph databases. Extreme Learning Machine (ELM) is a simple and efficient Single-hidden Layer Feedforward neural Networks (SLFNs) algorithm with extremely fast learning capacity. In this paper, we propose a discriminative subgraph mining approach based on ELM-Filter strategy within the scalable MapReduce computing model. We randomly partition the collection of graphs among worker nodes, and each worker applies a fast pattern evolutionary method to mine a set of discriminative subgraphs with the help of ELM-Filter strategy in its partition. And, the set of discriminative subgraphs must produce higher ELM training accuracy. The union of all such discriminative subgraphs is the mining result for the input large-scale graphs. Also, based on the proposed Support Graph Vector Model (SGVM) and ELM algorithm, we construct the graph classification model using the mined discriminative subgraphs. Extensive experimental results on both real and synthetic datasets show that our method obviously outperforms the other approaches in terms of both classification accuracy and runtime efficiency. © 2017 Elsevier B.V.","Discriminative subgraph pattern; Extreme Learning Machine; Graph classification; MapReduce","Classification (of information); Distributed computer systems; Feedforward neural networks; Filtration; Knowledge acquisition; Network layers; Classification accuracy; Evolutionary method; Extreme learning machine; Graph classification; Map-reduce; Run-time efficiency; Single-hidden layer feedforward neural networks; Subgraphs; Learning systems; Article; artificial neural network; classification algorithm; computer model; conceptual framework; data base; data mining; Extreme Learning Machine; learning algorithm; mathematical computing; measurement accuracy; priority journal; support vector machine",2-s2.0-85012882041
"Arık O.A., Toksarı M.D.","Multi-objective fuzzy parallel machine scheduling problems under fuzzy job deterioration and learning effects",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032224742&doi=10.1080%2f00207543.2017.1388932&partnerID=40&md5=d918af2f2d666565f629314595fe904f","This paper investigates a multi-objective parallel machine scheduling problem under fully fuzzy environment with fuzzy job deterioration effect, fuzzy learning effect and fuzzy processing times. Due dates are decision variables for the problem and objective functions are to minimise total tardiness penalty cost, to minimise earliness penalty cost and to minimise cost of setting due dates. Due date assignment problems are significant for Just-in-Time (JIT) thought. A JIT company may want to have optimum schedule by minimising cost combination of earliness, tardiness and setting due dates. In this paper, we compare different approaches for modelling fuzzy mathematical programming models with a local search algorithm based on expected values of fuzzy parameters such as job deterioration effect, learning effect and processing times. © 2017 Informa UK Limited, trading as Taylor & Francis Group","due date assignment; fuzzy deterioration; fuzzy learning; local search; parallel machines","Combinatorial optimization; Costs; Deterioration; Just in time production; Local search (optimization); Machinery; Mathematical programming; Scheduling; Due-date assignment; Fuzzy learning; Fuzzy mathematical programming; Fuzzy processing time; Local search; Local search algorithm; Parallel machine; Parallel machine-scheduling problems; Job shop scheduling",2-s2.0-85032224742
"Wang W., Liu X.","The selection of input weights of extreme learning machine: A sample structure preserving point of view",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012866928&doi=10.1016%2fj.neucom.2016.06.079&partnerID=40&md5=c569cd0e774f3fd2d8ef489bd251b7a5","The random assignment strategy for input weights has brought extreme learning machine (ELM) many advantages such as fast learning speed, minimal manual intervention and so on. However, the Monte Carlo (MC) based random sampling method that is typically used to generate input weights of ELM has poor capability of sample structure preserving (SSP), which will degenerate the learning and generalization performance. For this reason, the Quasi-Monte Carlo (QMC) method is revisited and it is shown that the distortion error of QMC projection can obtain faster convergence rate than that of MC for relatively low-dimensional problems. Further, a unified random orthogonal (RO) projection method is proposed, and it is shown that such RO method can always provide the optimal transformation in terms of minimizing the loss of all the distances between samples. Experimental results on real-world benchmark data sets verify the rationality of theoretical analysis and indicate that by enhancing the SSP capability of input weights, QMC and RO projection methods tend to bring ELM algorithms better generalization performance. © 2017","Distance preserving; Extreme learning machine; Generalization performance; Monte Carlo sampling; Orthogonal projection; Quasi-Monte Carlo sequence","Benchmarking; Knowledge acquisition; Learning systems; Distance preserving; Extreme learning machine; Generalization performance; Monte Carlo sampling; Orthogonal projection; Quasi-Monte Carlo; Monte Carlo methods; Article; artificial neural network; controlled study; data analysis; extreme learning machine; learning algorithm; priority journal; quasi Monte Carlo method; sample (statistics); statistical analysis",2-s2.0-85012866928
"Deng C., Wang B., Lin W., Huang G.-B., Zhao B.","Effective visual tracking by pairwise metric learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013436177&doi=10.1016%2fj.neucom.2016.05.115&partnerID=40&md5=ffaaeca9e41e640cfd9e054aba3459f0","For robust visual tracking, appearance modeling should be able to well separate the object from its backgrounds, while accurately adapt to its appearance variations. However, most of the existing tracking methods mainly focus on one of the two aspects; or design two different modules to combine them with the price of double computational cost. In this paper, by using pairwise metric learning, we present a novel appearance model for robust visual tracking. Specifically, visual tracking is viewed as a pairwise regression problem, and extreme learning machine (ELM) is utilized to construct the pairwise regression framework. In ELM-based pairwise training, two constraints are enforced: the target observations must have different regression outputs from those background ones; while the various target observations during tracking should have approximate regression outputs. Thus, the discriminative and generative capabilities are fully considered in a single object tracking model. Moreover, online sequential ELM (OS-ELM) is used to update the resulting appearance model, thereby leading to a more robust tracking process. Extensive experimental evaluations on challenging video sequences demonstrate the effectiveness and efficiency of the proposed tracker. © 2017 Elsevier B.V.","Appearance modeling; Extreme learning machine; Online sequential updating; Pairwise metric learning; Robust visual tracking","Knowledge acquisition; Learning systems; Regression analysis; Tracking (position); Appearance modeling; Extreme learning machine; Metric learning; Online sequential updating; Visual Tracking; Target tracking; Article; conceptual framework; experimental study; eye tracking; machine learning; mathematical analysis; mathematical computing; mathematical parameters; online sequential extreme learning machine; online system; pairwise metric learning; priority journal; regression analysis; statistical model; videorecording",2-s2.0-85013436177
"Oh B.-S., Oh K., Teoh A.B.J., Lin Z., Toh K.-A.","A Gabor-based network for heterogeneous face recognition",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014092835&doi=10.1016%2fj.neucom.2015.11.137&partnerID=40&md5=5ce035142e6566aa13a4279319842f2f","In this paper, we propose a single hidden-layer Gabor-based network for heterogeneous face recognition. The proposed input layer contains novel computational units which propagate geometrically localized input image sub-blocks to hidden nodes. The propagated pixels are then convolved with a set of Gabor kernels followed by a randomly weighted summation and a non-linear activation function operation. The output layer adopts a linear weighting scheme which can be deterministically estimated similar to that in extreme learning machine. Our experiments on three experimental scenarios using BERC visual-thermal infrared database and CASIA visual-near infrared database show promising results for the proposed network. © 2017 Elsevier B.V.","Extreme learning machine; Gabor features; Heterogeneous face recognition; Random weighting","Infrared devices; Knowledge acquisition; Learning systems; Network layers; Computational units; Extreme learning machine; Gabor feature; Localized inputs; Nonlinear activation functions; Random weighting; Weighted summations; Weighting scheme; Face recognition; Article; BERC visual thermal infrared database; CASIA visual near infrared database; controlled study; data base; facial recognition; Gabor based extreme learning machine; Gabor kernel; kernel method; machine learning; priority journal",2-s2.0-85014092835
"Patil A., Shen S., Yao E., Basu A.","Hardware architecture for large parallel array of Random Feature Extractors applied to image recognition",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014162736&doi=10.1016%2fj.neucom.2016.09.118&partnerID=40&md5=ee765ea74fb618a12f935955228a0e05","We demonstrate the use of a low-power and compact hardware implementation of Random Feature Extractor (RFE) core in image recognition applications. We show that weight distributions with zero mean are necessary for good performance in this application. We also show the performance in this application of a recently proposed weight reuse technique that virtually expands the number of random features available from RFE core. Further, we propose a method of reducing computation cost by pruning “incognizant” or redundant random feature neurons. For proof of concept, we validated our approach by using our RFE core as the first stage of Extreme Learning Machine (ELM) – a two layer neural network – and using 12, 800 hidden neurons were able to achieve upto 97.8% accuracy on MNIST database of handwritten digits. The RFE ASIC occupies 5 mm × 5 mm area in 0.35 µm CMOS process. By using the proposed pruning method, an accuracy of 97% is obtained by using only 4749 effective hidden neurons and consuming 6.1 µJ/classify. Input pre-processing and ELM second stage requiring just addition operations can be implemented using digital adders with estimated power consumption of 24.9 nJ/classify. With a total energy consumption of only 6.12 µJ/classify, this low-power mixed signal ASIC can act as a co-processor in portable electronic gadgets with cameras. © 2017 Elsevier B.V.","Energy-efficient neural network; Extreme Learning Machine (ELM); Neural network hardware; Random feature extraction; Sub-threshold VLSI","Energy efficiency; Energy utilization; Feature extraction; Hardware; Image recognition; Knowledge acquisition; Learning systems; Network layers; Neural networks; Neurons; Parallel architectures; Energy efficient; Extreme learning machine; Hardware architecture; Hardware implementations; Neural network hardware; Portable electronics; Subthreshold; Total energy consumption; Image processing; algorithm; Article; artificial neural network; camera; conceptual framework; data base; electricity; electronics; extreme learning machine; image analysis; image processing; machine learning; mathematical analysis; mathematical computing; measurement accuracy; portable equipment; priority journal; randomization; virtual reality",2-s2.0-85014162736
"Cao J., Zhao T., Wang J., Wang R., Chen Y.","Excavation equipment classification based on improved MFCC features and ELM",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013483802&doi=10.1016%2fj.neucom.2016.03.113&partnerID=40&md5=68d955d08247487c6e5e87fe4ee5de7f","An efficient algorithm for earthmoving device recognition is essential for underground high voltage cable protection in the mainland of China. Utilizing acoustic signals generated either by engine or the clash during operations, an intelligent classification system for four representative excavation equipments (namely, electric hammers, hydraulic hammers, cutting machines, and excavators) is developed in this paper. A benchmark acoustic wave database collecting from a real construction site is first established. Then, an improved feature extraction approach based on the Mel-Frequency Cepstrual Coefficients (MFCC) which can efficiently describe the dynamics of acoustics wave is developed. The recent fast and effective extreme learning machine is employed as the classifier in the proposed classification system. Experiments on real collected signals and field testings using our developed software platform are provided to demonstrate the efficiency of the proposed classification system. © 2017 Elsevier B.V.","Acoustic signal processing; Extreme learning machine; Intelligent surveillance system; Δ2MFCC","Acoustic signal processing; Acoustic waves; Excavation; Feature extraction; Hammers; Knowledge acquisition; Learning systems; Machinery; Signal processing; Classification system; Construction sites; Excavation equipment; Extreme learning machine; High voltage cable; Intelligent classification system; Intelligent surveillance systems; Software platforms; Hydraulic machinery; accuracy; Article; back propagation; China; classification algorithm; classifier; general device; linear system; machine learning; mathematical analysis; mathematical model; nonlinear system; priority journal; signal processing; support vector machine",2-s2.0-85013483802
"Qin F., Zhang D., Xing D., Xu D., Li J.","Laser Beam Pointing Control With Piezoelectric Actuator Model Learning",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032437728&doi=10.1109%2fTSMC.2017.2754863&partnerID=40&md5=fdc02f77dc9067bf3373c389a25c424b","The inherent hysteresis property of piezoelectric actuator (PEA) brings challenges to its modeling and control. This paper proposes a model learning method that is suitable for both forward and inverse PEA models. The hysteresis property is learned based on least squares support vector machines (LS-SVMs). A larger dataset is used for training LS-SVM to guarantee a good generalization performance. Support vectors pruning is utilized to reduce the model complexity. The rate-dependent property of PEA is identified as a linear dynamic submodel. Moreover, a pointing control system with two dual-PEA-axis steering mirrors is developed, which can regulate the 4-degree-of-freedom pose of a laser beam. The coordinated control of four PEAs is realized based on the Jacobian matrix. The learned inverse PEA models are used for the feedforward compensation of each PEA's nonlinearity. A series of experiments were conducted to evaluate the proposed method's effectiveness. IEEE","Laser beam pointing control; model learning; piezoelectric actuator (PEA); support vector machines (SVMs).","Actuators; Hysteresis; Inverse problems; Jacobian matrices; Laser beams; Laser mirrors; Piezoelectricity; Support vector machines; Feed-forward compensation; Generalization performance; Hysteresis properties; Laser beam pointing; Least squares support vector machines; Model learning; Pointing control system; Support vector machine (SVMs); Piezoelectric actuators",2-s2.0-85032437728
"Zhang H., Zhang S., Yin Y.","Online sequential ELM algorithm with forgetting factor for real applications",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014256048&doi=10.1016%2fj.neucom.2016.09.121&partnerID=40&md5=68d4ed86d37aae950b75a51df854b7ff","Sequential learning algorithms are a good choice for learning data one-by-one or chunk-by-chunk. Liang et al. has proposed OS-ELM algorithm based on the ordinary ELM algorithm, which produces better generalization performance than other famous sequential learning algorithms. One of the deficiencies of OS-ELM is that all the observations are weighted equally regardless of the acquisition time. However, the training data often have timeliness in many real industrial applications. In this paper, we propose a modified online sequential learning algorithm with the forgetting factor (named WOS-ELM algorithm) that weights the new observations more. Then a convergence analysis is presented to make sure the estimation of output weights tend to converge at the exponential speed with the arriving of new observations. For the determination of the value of forgetting factor, it would change with the forecast error automatically and get rid of excessive human interference. We employ several applications in the simulation part including time-series predication, time-variant system identification and the weather forecast problem. The simulation results show that WOS-ELM is more accurate and robust than other sequential learning algorithms. © 2017 Elsevier B.V.","Extreme learning machine; Forgetting factor; Online learning; Sequential learning","E-learning; Learning systems; Radial basis function networks; Weather forecasting; Convergence analysis; Extreme learning machine; Forgetting factors; Generalization performance; Online learning; Sequential learning; Sequential learning algorithm; Time variant systems; Learning algorithms; analytical error; Article; controlled study; forecasting; learning algorithm; online sequential extreme learning machine algorithm; priority journal; process optimization; simulation; time series analysis; weather",2-s2.0-85014256048
"Yin Y., Zhao Y., Zhang B., Li C., Guo S.","Enhancing ELM by Markov Boundary based feature selection",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014403178&doi=10.1016%2fj.neucom.2016.09.119&partnerID=40&md5=c6379b35595726b5726b70e69fcb4f97","ELM, as an efficient classification technology, has been used in many popular application domains. However, ELM has weak generalization performance when the data set is small with respect to its feature space. In this paper, an enhanced ELM algorithm based on representative features is proposed to address the problem. At first, the method automatically generates some discrete intervals for every continuous feature. Then, it removes the irrelevant features by a method considering the feature interaction and reduces the weakly relevant features by a mutual information based method. Further, the reduction of redundancy features is conducted. Instead of constructing a large Bayesian network using all features, we just select the features of high relevance with the object node by an improved Markov Boundary identifying algorithm. Finally, we obtain the enhanced ELM classifier by training ELM using the extracted representative features and a genetic algorithm based weight assignment mechanism. The experiments conducted on real and synthetic small sample data sets demonstrate that the enhanced ELM classifier based on representative features outperforms the other methods used in our comparison study in terms of both efficiency and effectiveness. © 2017 Elsevier B.V.","Extreme learning machine; Markov Boundary; Representative features; Small sample data","Bayesian networks; Classification (of information); Genetic algorithms; Learning systems; Classification technology; Extreme learning machine; Feature interactions; Generalization performance; Markov boundary; Reduction of redundancy; Representative features; Small sample datum; Feature extraction; accuracy; Article; Bayes theorem; breast cancer; child parent relation; chromosome; controlled study; extreme learning machine; genetic algorithm; human; learning algorithm; leukemia; machine learning; Markov chain; nerve cell network; priority journal",2-s2.0-85014403178
"Kozachok A.V., Kozachok V.I.","Construction and evaluation of the new heuristic malware detection mechanism based on executable files static analysis",2017,"Journal of Computer Virology and Hacking Techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032192992&doi=10.1007%2fs11416-017-0309-3&partnerID=40&md5=fe2744b8d292f88139c01c21d5deb760","The paper presents the application justification of a new set of features collected at the stage of the static analysis of the executable files to address the problem of malicious code detection. In the course of study the following problems were solved: the development of the executable files classifier in the absence of a priori data concerning their functionality; designing the class models of uninfected files and malware during the learning process; the development of malicious code detection procedure using the neural networks mathematical apparatus and decision tree composition relating to the set of features specified on the basis of the executable files static analysis. The paper contains the results of experimental evaluation of the developed detection mechanism efficiency on the basis of neural networks (accuracy was 0.99125) and decision tree composition (accuracy was 0.99240). The obtained data confirmed the hypothesis about the possibility of constructing the heuristic malware analyzer on the basis of features selected during the static analysis of the executable files. © 2017 Springer-Verlag France SAS","Anti-virus protection; Decision trees; Heuristic analysis; Machine learning; Malware; Neural networks","Computer crime; Computer viruses; Decision trees; Feature extraction; Learning systems; Malware; Neural networks; Trees (mathematics); Viruses; Anti virus; Detection mechanism; Experimental evaluation; Following problem; Heuristic analysis; Malicious code detection; Malware detection; Mathematical apparatus; Static analysis",2-s2.0-85032192992
"Adler P., Falk C., Friedler S.A., Nix T., Rybeck G., Scheidegger C., Smith B., Venkatasubramanian S.","Auditing black-box models for indirect influence",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032187305&doi=10.1007%2fs10115-017-1116-3&partnerID=40&md5=5dedc1b9bfb8d45facd684650071d00c","Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models or asserting that certain problematic attributes (such as race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the data set, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if, for example, the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence such as feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available data sets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. To further demonstrate the effectiveness of this technique, we use it to audit a black-box recidivism prediction algorithm. © 2017 Springer-Verlag London Ltd.","Algorithmic accountability; ANOVA; Black-box auditing; Deep learning; Discrimination-aware data mining; Feature influence; Interpretable machine learning",,2-s2.0-85032187305
"Oquendo Y.A., Riddle E.W., Hiller D., Blinman T.A., Kuchenbecker K.J.","Automatically rating trainee skill at a pediatric laparoscopic suturing task",2017,"Surgical Endoscopy and Other Interventional Techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032190746&doi=10.1007%2fs00464-017-5873-6&partnerID=40&md5=82acddc2cbcf4d638a7ac31a6e854367","Background: Minimally invasive surgeons must acquire complex technical skills while minimizing patient risk, a challenge that is magnified in pediatric surgery. Trainees need realistic practice with frequent detailed feedback, but human grading is tedious and subjective. We aim to validate a novel motion-tracking system and algorithms that automatically evaluate trainee performance of a pediatric laparoscopic suturing task. Methods: Subjects (n = 32) ranging from medical students to fellows performed two trials of intracorporeal suturing in a custom pediatric laparoscopic box trainer after watching a video of ideal performance. The motions of the tools and endoscope were recorded over time using a magnetic sensing system, and both tool grip angles were recorded using handle-mounted flex sensors. An expert rated the 63 trial videos on five domains from the Objective Structured Assessment of Technical Skill (OSATS), yielding summed scores from 5 to 20. Motion data from each trial were processed to calculate 280 features. We used regularized least squares regression to identify the most predictive features from different subsets of the motion data and then built six regression tree models that predict summed OSATS score. Model accuracy was evaluated via leave-one-subject-out cross-validation. Results: The model that used all sensor data streams performed best, achieving 71% accuracy at predicting summed scores within 2 points, 89% accuracy within 4, and a correlation of 0.85 with human ratings. 59% of the rounded average OSATS score predictions were perfect, and 100% were within 1 point. This model employed 87 features, including none based on completion time, 77 from tool tip motion, 3 from tool tip visibility, and 7 from grip angle. Conclusions: Our novel hardware and software automatically rated previously unseen trials with summed OSATS scores that closely match human expert ratings. Such a system facilitates more feedback-intensive surgical training and may yield insights into the fundamental components of surgical skill. © 2017 The Author(s)","Box trainer; Intracorporeal suturing; Machine learning; Motion analysis; Objective skill assessment; Pediatric laparoscopic surgery",,2-s2.0-85032190746
"Ding L., Liu Y., Han B., Zhang S., Song B.","HB-File: An efficient and effective high-dimensional big data storage structure based on US-ELM",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014060846&doi=10.1016%2fj.neucom.2016.06.080&partnerID=40&md5=0d2649711561589017a58ba65c475692","With the rapid development of computer and the Internet techniques, the amount of data in all walks of life increases sharply, especially accumulating numerous high-dimensional big data such as the network transactions data, the user reviews data and the multimedia data. High-dimensional big data mixes the typical features of both high-dimensional data and big data, which has also brought new problems and great challenges for processing and optimizing the high-dimensional big data. In this case, the storage structure of high-dimensional big data is a critical factor that can affect the processing performance in a fundamental way. However, due to the huge dimensionality feature of high-dimensional data, the existing data storage techniques, such as row-store and column-store, are not very suitable for high-dimensional and large scale data. Therefore, in this paper, we present an efficient high-dimensional big data storage structure based on US-ELM, High-dimensional Big Data File, named HB-File. Then, we propose a fuzzy cluster algorithm to differentiate the key dimension and non-key dimension of high-dimensional big data based on US-ELM, which can also gain the clusters of key dimension. After that, we propose the execution and API of HB-File based on the open source implementation of MapReduce, Hadoop system. With the intensive experiments, we show the effectiveness of HB-File in satisfying the storage of high-dimensional big data. © 2017","Big data; HDFS; High-dimensional data; US-ELM","Clustering algorithms; Data storage equipment; Digital storage; Open systems; Cluster algorithms; HDFS; High dimensional data; Large scale data; Network transactions; Open source implementation; Processing performance; Storage structures; Big data; Article; artificial neural network; fuzzy system; high dimensional big data storage; information processing; intermethod comparison; machine learning; priority journal",2-s2.0-85014060846
"Zhang S., Huang K., Zhang R., Hussain A.","Learning from Few Samples with Memory Network",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032029937&doi=10.1007%2fs12559-017-9507-z&partnerID=40&md5=724a3c2afaa9c512277514a3bf5e33ca","Neural networks (NN) have achieved great successes in pattern recognition and machine learning. However, the success of a NN usually relies on the provision of a sufficiently large number of data samples as training data. When fed with a limited data set, a NN’s performance may be degraded significantly. In this paper, a novel NN structure is proposed called a memory network. It is inspired by the cognitive mechanism of human beings, which can learn effectively, even from limited data. Taking advantage of the memory from previous samples, the new model achieves a remarkable improvement in performance when trained using limited data. The memory network is demonstrated here using the multi-layer perceptron (MLP) as a base model. However, it would be straightforward to extend the idea to other neural networks, e.g., convolutional neural networks (CNN). In this paper, the memory network structure is detailed, the training algorithm is presented, and a series of experiments are conducted to validate the proposed framework. Experimental results show that the proposed model outperforms traditional MLP-based models as well as other competitive algorithms in response to two real benchmark data sets. © 2017 Springer Science+Business Media, LLC","Memory; Multi-layer perceptron; Neural network; Prior knowledge; Recognition","Data storage equipment; Learning algorithms; Learning systems; Neural networks; Cognitive mechanisms; Competitive algorithms; Convolutional neural network; Multi layer perceptron; Neural network (nn); Prior knowledge; Recognition; Training algorithms; Pattern recognition",2-s2.0-85032029937
"Zhang S., Zhang S., Huang T., Gao W.","Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching",2017,"IEEE Transactions on Multimedia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032447590&doi=10.1109%2fTMM.2017.2766843&partnerID=40&md5=34165333edad52e4015ef2be63edc80b","Speech emotion recognition is challenging because of the affective gap between the subjective emotions and low-level features. Integrating multi-level feature learning and model training, Deep Convolutional Neural Networks (DCNN) has exhibited remarkable success in bridging the semantic gap in visual tasks like image classification, object detection. This paper explores how to utilize a DCNN to bridge the affective gap in speech signals. To this end, we firstly extract three channels of log Mel-spectrograms (static, delta and delta-delta) similar to the RGB image representation as the DCNN input. Then the AlexNet DCNN model pre-trained on the large ImageNet dataset is employed to learn high-level feature representations on each segment divided from an utterance. The learned segment-level features are aggregated by a Discriminant Temporal Pyramid Matching (DTPM) strategy. DTPM combines temporal pyramid matching and optimal Lp-norm pooling to form a global utterance-level feature representation, followed by the linear Support Vector Machines (SVM) for emotion classification. Experimental results on four public datasets, i.e., EMO-DB, RML, eNTERFACE05 and BAUM-1s, show the promising performance of our DCNN model and the DTPM strategy. Another interesting finding is that the DCNN model pre-trained for image applications performs reasonably good in affective speech feature extraction. A further fine-tuning on the target emotional speech datasets substantially promotes the recognition performance. IEEE","Deep Convolutional Neural Network; Discriminant Temporal Pyramid Matching; Feature Learning; Lp-norm Pooling; Speech Emotion Recognition","Convolution; Deep neural networks; Feature extraction; Image processing; Neural networks; Object detection; Semantics; Speech; Support vector machines; Vision; Convolutional neural network; Discriminant Temporal Pyramid Matching; Feature learning; Lp-norm; Speech emotion recognition; Speech recognition",2-s2.0-85032447590
"Ghasemi S., Meybodi M.R., Fooladi M.D.T., Rahmani A.M.","A cost-aware mechanism for optimized resource provisioning in cloud computing",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032187968&doi=10.1007%2fs10586-017-1271-z&partnerID=40&md5=e2ee6fecd579e0572b900e0c2dbed049","Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based approach which selects the most proper resources for hosting each service of the demanded application; our approach considers both cost and service requirements together for deploying applications. Thirdly, a comprehensive evaluation is performed for three typical workloads: data-intensive, process-intensive and normal applications. The experimental results show that our method adapts most of the requirements efficiently, and furthermore the resulting performance meets our design goals. © 2017 Springer Science+Business Media, LLC","Cloud computing; Cost; Learning automata; Resource provisioning; Services; Virtual machine","Automata theory; Cloud computing; Cost effectiveness; Costs; Virtual machine; Comprehensive evaluation; Computational resources; Cost-effective methods; Learning Automata; Optimized resources; Resource provisioning; Service requirements; Services; Cost reduction",2-s2.0-85032187968
"Pecli A., Cavalcanti M.C., Goldschmidt R.","Automatic feature selection for supervised learning in link prediction applications: a comparative study",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032204164&doi=10.1007%2fs10115-017-1121-6&partnerID=40&md5=6122a93288698a9bff49c72a9f3d6ad8","For the last years, a considerable amount of attention has been devoted to the research about the link prediction (LP) problem in complex networks. This problem tries to predict the likelihood of an association between two not interconnected nodes in a network to appear in the future. One of the most important approaches to the LP problem is based on supervised machine learning (ML) techniques for classification. Although many works have presented promising results with this approach, choosing the set of features (variables) to train the classifiers is still a major challenge. In this article, we report on the effects of three different automatic variable selection strategies (Forward, Backward and Evolutionary) applied to the feature-based supervised learning approach in LP applications. The results of the experiments show that the use of these strategies does lead to better classification models than classifiers built with the complete set of variables. Such experiments were performed over three datasets (Microsoft Academic Network, Amazon and Flickr) that contained more than twenty different features each, including topological and domain-specific ones. We also describe the specification and implementation of the process used to support the experiments. It combines the use of the feature selection strategies, six different classification algorithms (SVM, K-NN, naïve Bayes, CART, random forest and multilayer perceptron) and three evaluation metrics (Precision, F-Measure and Area Under the Curve). Moreover, this process includes a novel ML voting committee inspired approach that suggests sets of features to represent data in LP applications. It mines the log of the experiments in order to identify sets of features frequently selected to produce classification models with high performance. The experiments showed interesting correlations between frequently selected features and datasets. © 2017 Springer-Verlag London Ltd.","Binary classification; Complex network analysis; Feature selection; Link prediction",,2-s2.0-85032204164
"Wahlmüller-Schiller C.","Artificial intelligence—where are we heading? [Künstliche Intelligenz – wohin geht die Reise?]",2017,"Elektrotechnik und Informationstechnik",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032199070&doi=10.1007%2fs00502-017-0529-8&partnerID=40&md5=977a1f7722bd9a1cdd399484f9d097d8","Artificial intelligence (AI) is booming. Will chatbots, self-learning machines and robots dominate our lives, change our society and the working world? This article takes a look inside the latest developments in the field of AI. Starting with an analysis of the present situation the paper presents applications (e.g. customer services), latest research results, services and products of IT companies like IBM, Microsoft and SAP as well as research work of Austria’s academic institutions. The second part deals with consequences of AI development. The recent position paper on AI by Bitcom, Germany’s digital association, is discussed as well as expected changes on the labour market and in our daily lives. Digital competences (for which the European Computer Driving Licence EDCL is a good basis) are a central factor for the success of an AI supported digital transformation. © 2017 Springer-Verlag GmbH Austria","artificial intelligence; companies; consequences; research; trends; working world","Education; Employment; Industry; Learning systems; Research; Academic institutions; consequences; Customer services; Digital transformation; Latest development; Present situation; trends; working world; Artificial intelligence",2-s2.0-85032199070
"Liu B., Ramsundar B., Kawthekar P., Shi J., Gomes J., Luu Nguyen Q., Ho S., Sloane J., Wender P., Pande V.","Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models",2017,"ACS Central Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032259185&doi=10.1021%2facscentsci.7b00303&partnerID=40&md5=130b5a011d03fd0d226ffe2ad3e90480","We describe a fully data driven model that learns to perform a retrosynthetic reaction prediction task, which is treated as a sequence-to-sequence mapping problem. The end-to-end trained model has an encoder-decoder architecture that consists of two recurrent neural networks, which has previously shown great success in solving other sequence-to-sequence prediction tasks such as machine translation. The model is trained on 50,000 experimental reaction examples from the United States patent literature, which span 10 broad reaction types that are commonly used by medicinal chemists. We find that our model performs comparably with a rule-based expert system baseline model, and also overcomes certain limitations associated with rule-based expert systems and with any machine learning approach that contains a rule-based expert system component. Our model provides an important first step toward solving the challenging problem of computational retrosynthetic analysis. © 2017 American Chemical Society.",,,2-s2.0-85032259185
"Eyassu F., Angione C.","Modelling pyruvate dehydrogenase under hypoxia and its role in cancer metabolism",2017,"Royal Society Open Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032389833&doi=10.1098%2frsos.170360&partnerID=40&md5=f248f2ec5e672b04c99c2bdacc2956d1","Metabolism is the only biological system that can be fully modelled at genome scale. As a result, metabolic models have been increasingly used to study the molecular mechanisms of various diseases. Hypoxia, a low-oxygen tension, is a well-known characteristic of many cancer cells. Pyruvate dehydrogenase (PDH) controls the flux of metabolites between glycolysis and the tricarboxylic acid cycle and is a key enzyme in metabolic reprogramming in cancer metabolism. Here, we develop and manually curate a constraint-based metabolic model to investigate the mechanism of pyruvate dehydrogenase under hypoxia. Our results characterize the activity of pyruvate dehydrogenase and its decline during hypoxia. This results in lactate accumulation, consistent with recent hypoxia studies and a well-known feature in cancer metabolism. We apply machine-learning techniques on the flux datasets to identify reactions that drive these variations. We also identify distinct features on the structure of the variables and individual metabolic components in the switch from normoxia to hypoxia. Our results provide a framework for future studies by integrating multi-omics data to predict condition-specific metabolic phenotypes under hypoxia. © 2017 The Authors.","Cancer metabolism; Constraint-based models; Flux balance analysis; Hypoxia; Pyruvate dehydrogenase",,2-s2.0-85032389833
"Cho D., Ham J., Oh J., Park J., Kim S., Lee N.-K., Lee B.","Detection of stress levels from biosignals measured in virtual reality environments using a kernel-based extreme learning machine",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032574814&doi=10.3390%2fs17102435&partnerID=40&md5=5094fcc565f911db790333ef95370086","Virtual reality (VR) is a computer technique that creates an artificial environment composed of realistic images, sounds, and other sensations. Many researchers have used VR devices to generate various stimuli, and have utilized them to perform experiments or to provide treatment. In this study, the participants performed mental tasks using a VR device while physiological signals were measured: a photoplethysmogram (PPG), electrodermal activity (EDA), and skin temperature (SKT). In general, stress is an important factor that can influence the autonomic nervous system (ANS). Heart-rate variability (HRV) is known to be related to ANS activity, so we used an HRV derived from the PPG peak interval. In addition, the peak characteristics of the skin conductance (SC) from EDA and SKT variation can also reflect ANS activity; we utilized them as well. Then, we applied a kernel-based extreme-learning machine (K-ELM) to correctly classify the stress levels induced by the VR task to reflect five different levels of stress situations: baseline, mild stress, moderate stress, severe stress, and recovery. Twelve healthy subjects voluntarily participated in the study. Three physiological signals were measured in stress environment generated by VR device. As a result, the average classification accuracy was over 95% using K-ELM and the integrated feature (IT = HRV + SC + SKT). In addition, the proposed algorithm can embed a microcontroller chip since K-ELM algorithm have very short computation time. Therefore, a compact wearable device classifying stress levels using physiological signals can be developed. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Autonomic nervous system (ANS); Heart rate variability (HRV); Kernel-based extreme learning machine (K-ELM); Virtual reality (VR)","E-learning; Electronic design automation; Heart; Knowledge acquisition; Learning systems; Physiology; Virtual reality; Artificial environments; Autonomic nervous system; Classification accuracy; Electrodermal activity; Extreme learning machine; Heart rate variability; Physiological signals; Virtual-reality environment; Biomedical signal processing",2-s2.0-85032574814
"Kang Q., Shi L., Zhou M., Wang X., Wu Q., Wei Z.","A Distance-Based Weighted Undersampling Scheme for Support Vector Machines and its Application to Imbalanced Classification",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450330&doi=10.1109%2fTNNLS.2017.2755595&partnerID=40&md5=025c24b7d3df2bde844d3cd817decea1","A support vector machine (SVM) plays a prominent role in classic machine learning, especially classification and regression. Through its structural risk minimization, it has enjoyed a good reputation in effectively reducing overfitting, avoiding dimensional disaster, and not falling into local minima. Nevertheless, existing SVMs do not perform well when facing class imbalance and large-scale samples. Undersampling is a plausible alternative to solve imbalanced problems in some way, but suffers from soaring computational complexity and reduced accuracy because of its enormous iterations and random sampling process. To improve their classification performance in dealing with data imbalance problems, this work proposes a weighted undersampling (WU) scheme for SVM based on space geometry distance, and thus produces an improved algorithm named WU-SVM. In WU-SVM, majority samples are grouped into some subregions (SRs) and assigned different weights according to their Euclidean distance to the hyper plane. The samples in an SR with higher weight have more chance to be sampled and put to use in each learning iteration, so as to retain the data distribution information of original data sets as much as possible. Comprehensive experiments are performed to test WU-SVM via 21 binary-class and six multiclass publically available data sets. The results show that it well outperforms the state-of-the-art methods in terms of three popular metrics for imbalanced classification, i.e., area under the curve, F-Measure, and G-Mean. IEEE","Class imbalance; data distribution; Euclidean distance; Euclidean distance; Kernel; Learning systems; Machine learning algorithms; Optimization; support vector machine (SVM); Support vector machines; Training; undersampling.","Artificial intelligence; Disaster prevention; Iterative methods; Learning algorithms; Learning systems; Optimization; Personnel training; Vectors; Class imbalance; Data distribution; Euclidean distance; Kernel; Under-sampling; Support vector machines",2-s2.0-85032450330
"Fan S., Fei J., Shen L.","Accelerating Deep Learning with a Parallel Mechanism Using CPU + MIC",2017,"International Journal of Parallel Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032024950&doi=10.1007%2fs10766-017-0535-9&partnerID=40&md5=bab05872f72e593b7fde41fbcbc49b1e","Deep neural networks (DNNs) is one of the most popular machine learning methods and is widely used in many modern applications. The training process of DNNs is a time-consuming process. Accelerating the training of DNNs has been the focus of many research works. In this paper, we speed up the training of DNNs applied for automatic speech recognition and the target architecture is heterogeneous (CPU + MIC). We apply asynchronous methods for I/O and communication operations and propose an adaptive load balancing method. Besides, we also employ a momentum idea to speed up the convergence of the gradient descent algorithm. Experimental results show that our optimized algorithm is able to acquire a 20-fold speedup on a CPU + MIC platform compared with the original sequential algorithm on a single-core CPU. © 2017 Springer Science+Business Media, LLC","CPU + MIC; Deep learning; DNNs; Parallel mechanism","Deep neural networks; Learning systems; Mechanisms; Microwave integrated circuits; Speech recognition; Adaptive load balancing; Automatic speech recognition; Communication operation; DNNs; Gradient descent algorithms; Machine learning methods; Parallel mechanisms; Sequential algorithm; Deep learning",2-s2.0-85032024950
"Deng S., Wang B., Huang S., Yue C., Zhou J., Wang G.","Self-Adaptive Framework for Efficient Stream Data Classification on Storm",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440452&doi=10.1109%2fTSMC.2017.2757029&partnerID=40&md5=b37f8252a40210e9649a441fcf9ac9f5","In this era of big data, stream data classification which is one of typical data stream applications has become more and more significant and challengeable. In these applications, it is obvious that data classification is much more frequent than model training. The ratio of stream data to be classified is rapid and time-varying, so it is an important problem to classify the stream data efficiently with high throughput. In this paper, we first analyze and categorize the current data stream machine learning algorithms according to their data structures. Then, we propose stream data classification topology (SDC-Topology) on Storm. For the classification algorithms based on the matrix, we propose self-adaptive stream data classification framework (SASDC-Framework) for efficient stream data classification on Storm. In SASDC-Framework, all the data sets arriving at the same unit time are partitioned into subsets with the nearly best partition size and processed in parallel. To select the nearly best partition size for the stream data sets efficiently, we adopt bisection method strategy and inverse distance weighted strategy. Extreme learning machine, which is a fast and accurate machine learning method based on matrix calculating, is used to test the efficiency of our proposals. According to evaluation results, the throughputs based on SASDC-Framework are 8-35 times higher than those based on SDC-Topology and the best throughput is more than 40,000 prediction requests per second in our environment. IEEE","Artificial neural networks; Classification; extreme learning machine (ELM); Learning systems; partition strategy; Proposals; Real-time systems; Storm; Storms; stream data; Throughput; Training","Artificial intelligence; Big data; Data mining; Interactive computer systems; Inverse problems; Knowledge acquisition; Learning algorithms; Learning systems; Matrix algebra; Network function virtualization; Neural networks; Personnel training; Real time systems; Storms; Throughput; Topology; Classification algorithm; Data classification; Extreme learning machine; Inverse distance weighted; Machine learning methods; Proposals; Stream data; Stream data classifications; Classification (of information)",2-s2.0-85032440452
"Liu W., Zhang M., Luo Z., Cai Y.","An Ensemble Deep Learning Method for Vehicle Type Classification on Visual Traffic Surveillance Sensors",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032446248&doi=10.1109%2fACCESS.2017.2766203&partnerID=40&md5=9e8f5f2e802bced59c9680fa8b051cb0","Visual traffic surveillance systems play important roles in intelligent transport systems nowadays. The first step of a visual traffic surveillance system usually needs to correctly detect objects from images or videos and classify them into different categories (e.g. car, truck, bus). This paper aims to introduce a new vehicle type classification scheme on the images acquired from multi-view visual traffic surveillance sensors. Most image classification algorithms focus on maximizing the percentage of the correct predictions, which have a deficiency that images from minority categories are prone to be misclassified as the dominant categories. To address this challenge of classifying imbalanced data acquired from visual traffic surveillance sensors, we propose a method which integrates deep neural networks with balanced sampling in this paper. The proposed method consists of two main stages. In the first stage, data augmentation with balanced sampling is applied to alleviate the unbalanced data set problem. In the second stage, an ensemble of convolutional neural network models with different architectures is constructed with parameters learned on the augmented training data set. Experiments on the MIO-TCD classification challenge dataset demonstrate that the proposed method is able to enhance the mean precision of all categories, in the condition of high overall accuracy, compared with the baseline algorithms. OAPA","ensemble learning; image classification; imbalanced data; intelligent transport systems; Machine learning; Neural networks; Sensors; Surveillance; Traffic data; traffic surveillance systems; Training; Visualization","Data visualization; Deep learning; Deep neural networks; Flow visualization; Image classification; Intelligent systems; Intelligent vehicle highway systems; Learning systems; Monitoring; Neural networks; Object detection; Personnel training; Security systems; Sensors; Space surveillance; Traffic control; Transportation; Ensemble learning; Imbalanced data; Intelligent transport systems; Traffic data; Traffic surveillance; Classification (of information)",2-s2.0-85032446248
"Zhang Z., Zhang L., Tan Y., Zhang L., Liu F., Zhong R.","Joint Discriminative Dictionary and Classifier Learning for ALS Point Cloud Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032437600&doi=10.1109%2fTGRS.2017.2751061&partnerID=40&md5=5e31338f7e202ca286da7c517816e5bc","To efficiently recognize on-ground objects in airborne laser scanning (ALS) point clouds, we design a method that jointly learns a discriminative dictionary and a classifier. In the method, the point cloud is segmented into hierarchical point clusters, which are organized by a tree structure. Then, the feature of each point cluster is extracted. The feature of a leaf node is obtained by aggregating the features of all its parent nodes. The feature of the leaf node is called the hierarchical aggregation feature. The hierarchical aggregation features are encoded by sparse coding. We introduce a new label consistency constraint called ''discriminative sparse-code error,'' and combine it with the reconstruction error, the classification error, and L&#x2081;-norm sparsity constraint to form a unified objective function. The objective function is efficiently solved by using the proposed label consistency feature sign method. We obtain an overcomplete discriminative dictionary and an optimal linear classifier. Experiments performed on different ALS point cloud scenes have shown that the hierarchical aggregation features combined with the learned classifier can significantly enhance the classification results, and also demonstrated the superior performance of our method over other techniques in point cloud classification. IEEE","Airborne laser scanning (ALS) point clouds; classification; Dictionaries; discriminative dictionary learning; Eigenvalues and eigenfunctions; Feature extraction; hierarchical aggregation feature; Linear programming; Machine learning; point clusters; sparse coding.; Tensile stress; Three-dimensional displays","Classification (of information); Codes (symbols); Errors; Feature extraction; Glossaries; Laser applications; Learning systems; Linear programming; Surface analysis; Tensile stress; Trees (mathematics); Discriminative dictionaries; Hierarchical aggregation; Point cloud; Point clusters; Sparse coding; Three-dimensional display; Eigenvalues and eigenfunctions",2-s2.0-85032437600
"Zhang W.E., Sheng Q.Z., Qin Y., Taylor K., Yao L.","Learning-based SPARQL query performance modeling and prediction",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032015124&doi=10.1007%2fs11280-017-0498-1&partnerID=40&md5=781a83a1a376c934c171a8affb632038","One of the challenges of managing an RDF database is predicting performance of SPARQL queries before they are executed. Performance characteristics, such as the execution time and memory usage, can help data consumers identify unexpected long-running queries before they start and estimate the system workload for query scheduling. Extensive works address such performance prediction problem in traditional SQL queries but they are not directly applicable to SPARQL queries. In this paper, we adopt machine learning techniques to predict the performance of SPARQL queries. Our work focuses on modeling features of a SPARQL query to a vector representation. Our feature modeling method does not depend on the knowledge of underlying systems and the structure of the underlying data, but only on the nature of SPARQL queries. Then we use these features to train prediction models. We propose a two-step prediction process and consider performances in both cold and warm stages. Evaluations are performed on real world SPRAQL queries, whose execution time ranges from milliseconds to hours. The results demonstrate that the proposed approach can effectively predict SPARQL query performance and outperforms state-of-the-art approaches. © 2017 Springer Science+Business Media, LLC","Feature modeling; Prediction; Query performance; SPARQL","Forecasting; Learning systems; Query languages; Search engines; Feature modeling; Machine learning techniques; Performance characteristics; Performance prediction; Query performance; SPARQL; State-of-the-art approach; Vector representations; Query processing",2-s2.0-85032015124
"Mou L., Ghamisi P., Zhu X.X.","Unsupervised Spectral-Spatial Feature Learning via Deep Residual Conv-Deconv Network for Hyperspectral Image Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032438871&doi=10.1109%2fTGRS.2017.2748160&partnerID=40&md5=5a6dd936ed281c4edc413156b5567b1b","Supervised approaches classify input data using a set of representative samples for each class, known as training samples. The collection of such samples is expensive and time demanding. Hence, unsupervised feature learning, which has a quick access to arbitrary amounts of unlabeled data, is conceptually of high interest. In this paper, we propose a novel network architecture, fully Conv-Deconv network, for unsupervised spectral-spatial feature learning of hyperspectral images, which is able to be trained in an end-to-end manner. Specifically, our network is based on the so-called encoder-decoder paradigm, i.e., the input 3-D hyperspectral patch is first transformed into a typically lower dimensional space via a convolutional subnetwork (encoder), and then expanded to reproduce the initial data by a deconvolutional subnetwork (decoder). However, during the experiment, we found that such a network is not easy to be optimized. To address this problem, we refine the proposed network architecture by incorporating: 1) residual learning and 2) a new unpooling operation that can use memorized max-pooling indexes. Moreover, to understand the ''black box,'' we make an in-depth study of the learned feature maps in the experimental analysis. A very interesting discovery is that some specific ''neurons'' in the first residual block of the proposed network own good description power for semantic visual patterns in the object level, which provide an opportunity to achieve ''free'' object detection. This paper, for the first time in the remote sensing community, proposes an end-to-end fully Conv-Deconv network for unsupervised spectral-spatial feature learning. Moreover, this paper also introduces an in-depth investigation of learned features. Experimental results on two widely used hyperspectral data, Indian Pines and Pavia University, demonstrate competitive performance obtained by the proposed methodology compared with other studied approaches. OAPA","Convolutional network; deconvolutional network; Feature extraction; hyperspectral image classification; Hyperspectral imaging; Network architecture; residual learning; Support vector machines; Training; unsupervised spectral-spatial feature learning.","Classification (of information); Convolution; Decoding; Feature extraction; Hyperspectral imaging; Image classification; Independent component analysis; Object detection; Personnel training; Remote sensing; Semantics; Signal encoding; Spectroscopy; Support vector machines; Competitive performance; Convolutional networks; Dimensional spaces; Experimental analysis; Representative sample; residual learning; Spatial features; Unsupervised feature learning; Network architecture",2-s2.0-85032438871
"Kaveri V.V., Maheswari V.","A framework for recommending health-related topics based on topic modeling in conversational data (Twitter)",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032018749&doi=10.1007%2fs10586-017-1263-z&partnerID=40&md5=89b0a8981bc0aaa7bffdf6f56d9e2a16","Social media is an environment where it enables the user’s to create and share content or participate in all social networking activities; it has given rise to information overloading problem and knowledge starvation. Generally, in large conversational dataset like Twitter it is very difficult to identifying public health-related topics. Machine learning is the area which helps us to overcome this situation. By applying probabilistic unsupervised machine learning we have identified latent topics from the text corpora. Topic modeling algorithms are probabilistic generative models which is used to analyze the words of the original texts to identify themes that run through them, how connected to each other by themes, and how the topics change over (Blei in Commun ACM 55(4):77–84, 2012). In this paper we have proposed a framework for recommending health-related topics by applying an enhanced topic model hierarchical latent Dirichlet allocation for identifying public health-related topics and themes in tweets. We have tested our model by exploring tobacco usage across conversational data, as well as tested with the subset of data generated by tobacco related queries. Our test results indicate most of the public health related issues are uncovered in our model. However, this model provides relevant topic across the tobacco subset. © 2017 Springer Science+Business Media, LLC","Data mining; HLDA; Public health; Social media; Social networks; Tobacco use; Topic modeling","Artificial intelligence; Health; Learning systems; Public health; Recommender systems; Social networking (online); Statistics; Tobacco; Generative model; HLDA; Information overloading; Latent Dirichlet allocation; Social media; Topic Modeling; Topic modeling algorithms; Unsupervised machine learning; Data mining",2-s2.0-85032018749
"Syed K., Abdelzaher A., Mayo M., Ghosh P.","Similar Feed-forward Loop Crosstalk Patterns may Impact Robust Information Transport Across E. coli and S. Cerevisiae Transcriptional Networks",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031994996&doi=10.1007%2fs11036-017-0944-4&partnerID=40&md5=0a2a4ca8f694646bfee2919ba661c67c","Evolved biological network topologies may resist perturbances to allow for more robust information transport across larger networks in which their network motifs may play a complex role. Although the abundance of individual motifs correlate with some metrics of biological robustness, the extent to which redundant regulatory interactions affect motif connectivity and how this connectivity affects robustness is unknown. To address this problem, we applied machine learning based regression modeling to evaluate how feed-forward loops interlinked by crosstalk altered information transport across a network in terms of packets successfully routed over networks of noisy channels via NS-2 simulation. The sample networks were extracted from the complete transcriptional regulatory networks of two well-studied bacteria: E.coli and Yeast. We developed 233 topological features for the E.coli subnetworks and 842 topological features for the Yeast subnetworks which distinctly account for the opportunities in which two feed-forward loops may exhibit crosstalk. Random forest regression modeling was used to infer significant features from this modest configuration space. The coefficient of determination was used as a primary performance metric to rank features within our regression models. Although only a handful of features were highly ranked, we observed that, in particular, feed-forward loop crosstalk patterns correlated substantially with an improved chance for successful information transmission. Additionally, both E.coli and Yeast subnetworks demonstrate very similar FFL crosstalk patterns that were considered significant in their contribution to information transport robustness in these two organisms. This finding may potentially allude to common design principles in transcriptomic networks from different organisms. © 2017 Springer Science+Business Media, LLC","Complex networks; Crosstalk; Edge-connected motif; Motif connectivity; Transcriptional networks","Crosstalk; Decision trees; Escherichia coli; Learning systems; Regression analysis; Topology; Yeast; Applied machine learning; Coefficient of determination; Edge-connected motif; Information transmission; Motif connectivity; Regulatory interactions; Transcriptional networks; Transcriptional regulatory networks; Complex networks",2-s2.0-85031994996
"Wang X.-H., Jiao Y., Li L.","Predicting clinical symptoms of attention deficit hyperactivity disorder based on temporal patterns between and within intrinsic connectivity networks",2017,"Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028913146&doi=10.1016%2fj.neuroscience.2017.08.038&partnerID=40&md5=f08fe0c370c84aa9ad9fe08a349fcd1e","Attention deficit hyperactivity disorder (ADHD) is a common brain disorder with high prevalence in school-age children. Previously developed machine learning-based methods have discriminated patients with ADHD from normal controls by providing label information of the disease for individuals. Inattention and impulsivity are the two most significant clinical symptoms of ADHD. However, predicting clinical symptoms (i.e., inattention and impulsivity) is a challenging task based on neuroimaging data. The goal of this study is twofold: to build predictive models for clinical symptoms of ADHD based on resting-state fMRI and to mine brain networks for predictive patterns of inattention and impulsivity. To achieve this goal, a cohort of 74 boys with ADHD and a cohort of 69 age-matched normal controls were recruited from the ADHD-200 Consortium. Both structural and resting-state fMRI images were obtained for each participant. Temporal patterns between and within intrinsic connectivity networks (ICNs) were applied as raw features in the predictive models. Specifically, sample entropy was taken as an intra-ICN feature, and phase synchronization (PS) was used as an inter-ICN feature. The predictive models were based on the least absolute shrinkage and selectionator operator (LASSO) algorithm. The performance of the predictive model for inattention is r = 0.79 (p &lt; 10−8), and the performance of the predictive model for impulsivity is r = 0.48 (p &lt; 10−8). The ICN-related predictive patterns may provide valuable information for investigating the brain network mechanisms of ADHD. In summary, the predictive models for clinical symptoms could be beneficial for personalizing ADHD medications. © 2017 IBRO","ADHD; clinical symptoms; intrinsic connectivity networks; machine learning; temporal patterns","Article; attention deficit disorder; child; cohort analysis; controlled study; cortical synchronization; entropy; female; functional magnetic resonance imaging; human; impulsiveness; intrinsic connectivity network; major clinical study; male; nerve cell network; prediction; priority journal; school child; symptom",2-s2.0-85028913146
"Duan M., Li K., Lia K.","An Ensemble CNN2ELM for Age Estimation",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032446833&doi=10.1109%2fTIFS.2017.2766583&partnerID=40&md5=7c7d6d20d606f2c9f9283b53a434021a","Age estimation is a challenging task because it can be easily affected by gender, race, and other intrinsic and extrinsic attributes. At the same time, performing age estimation for a narrow age range may lead to better results. In this paper, to achieve robust age estimation, an ensemble structure referred to as CNN2ELM, which includes convolutional neural network (CNN) and extreme learning machine (ELM), is proposed for age estimation. The three-level system includes feature extraction and fusion, age grouping via an ELM classifier, and age estimation via an ELM regressor. Age-Net, Gender-Net, and Race-Net are trained using different targets, such as age class, gender class, and race class, respectively, and the three networks are used to extract features corresponding to age, gender, and race from the same image of a person during validation and test stages. Features related to the age property are enhanced by fusing these of race and gender properties. Then, to achieve a narrow age range, the ELM classifies the fusion results into one of the age groups. Afterward, an age decision is made using an ELM regressor. Our network is pretrained on an ImageNet database and then fine-tuned on the IMDB-WIKI database. The recently released Adience benchmark, ChaLearn Looking at People 2016 (LAP-2016), and MORPH-II are used to verify the performance of &#x0022;Race-Net &#x002B; Age-Net &#x002B; Gender-Net &#x002B; ELM classifier &#x002B; ELM regressor (RAGN)&#x0022;. RAGN outperforms the existing state-of-theart age estimation methods. The mean absolute error of the age estimation of RAGN for MORPH-II is determined to be 2.61 years; the accuracy of the age estimation for the Adience benchmark is 0.6649; and the normal score (&#x03B5;) for the sequestered test set of the LAP-2016 dataset is 0.3679. IEEE","Age Estimation; Benchmark testing; Convolutional Neural Network; Ensemble; Estimation; Extreme Learning Machine; Face; Feature extraction; Neural networks; Robustness; Support vector machines","Benchmarking; Convolution; Estimation; Extraction; Feature extraction; Knowledge acquisition; Learning systems; Neural networks; Robustness (control systems); Social sciences; Statistical tests; Support vector machines; Age estimation; Benchmark testing; Convolutional neural network; Ensemble; Extreme learning machine; Face; Artificial intelligence",2-s2.0-85032446833
"Boroumand M., Fridrich J.","Applications of Explicit Non-Linear Feature Maps in Steganalysis",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032442002&doi=10.1109%2fTIFS.2017.2766580&partnerID=40&md5=be14c108df60f6af8af43ffeabf21ecc","Currently, the most popular detectors of content-adaptive image steganography are built using machine learning with images represented with rich features. Such high-dimensional descriptors, however, prevent utilization of more complex and potentially more accurate machine learning paradigms, such as kernelized support vector machines, due to infeasibly expensive training. In this paper, we demonstrate that explicit non-linear feature maps coupled with simple classifiers improve the accuracy of current steganalysis detectors built as binary classifiers as well as quantitative detectors in the form of payload regressors. The non-linear map is obtained by approximating a symmetric positive semi-definite kernel on selected pairs of cover features. Exponential forms of kernels derived from symmetrized Ali-Silvey distances improve the detection accuracy of binary detectors and lower the error of quantitative detectors across all tested steganographic schemes on grayscale and color images. The learned non-linear map only weakly depends on the cover source and its learning has a low computational complexity. The technique can also be used for unsupervised feature dimensionality reduction. For payload regressors, the dimensionality can be significantly reduced while simultaneously decreasing the estimation error. IEEE","adaptive steganography; Additives; Detectors; explicit transformation; Feature extraction; Kernel; Nystr&#x00F6;m approximation; Payloads; Steganalysis; support vector machine; Support vector machines; Training","Additives; Artificial intelligence; Bins; Detectors; Image enhancement; Learning systems; Personnel training; Steganography; Support vector machines; Dimensionality reduction; explicit transformation; Kernel; Low computational complexity; Payloads; Quantitative detectors; Steganalysis; Steganographic schemes; Feature extraction",2-s2.0-85032442002
"Tatsch C., Ahmadi A., Bottega F., Tani J., da Silva Guerra R.","Dimitri: an Open-Source Humanoid Robot with Compliant Joint",2017,"Journal of Intelligent and Robotic Systems: Theory and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032015971&doi=10.1007%2fs10846-017-0727-y&partnerID=40&md5=e13eca809f64be47381fd99167be5fcd","We introduce Dimitri, an open-software & open-hardware humanoid robot with 31 DOFs, fitted with cost-effective modular compliant joints and parallel link legs, designed for advanced human-robot interaction research, force-informed object handling and intelligent environment discovery. Our main innovation is in the design of a robust full-body biped humanoid robot equipped with very low-cost polyurethane torsional spring fixed to traditional servo motors and a circuit to measure angular displacement, transforming the system into a series elastic actuator (SEA). In order to illustrate the robot’s qualities in the field of machine learning applied to robotics and manipulation, a multiple timescale recurrent neural network (MTRNN) is implemented, allowing the robot to replicate combined movement sequences earlier taught via interactive demonstration. © 2017 Springer Science+Business Media B.V.","Compliant joints; Humanoid robot; MTRNN; Neural network","Anthropomorphic robots; Cost effectiveness; Intelligent robots; Learning systems; Machine design; Neural networks; Open source software; Recurrent neural networks; Robots; Universal joints; Angular displacement; Biped humanoid robot; Compliant joints; Humanoid robot; Intelligent environment; MTRNN; Multiple timescale recurrent neural networks; Series elastic actuators; Human robot interaction",2-s2.0-85032015971
"Morales-Alvarez P., Perez-Suay A., Molina R., Camps-Valls G.","Remote Sensing Image Classification With Large-Scale Gaussian Processes",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032447623&doi=10.1109%2fTGRS.2017.2758922&partnerID=40&md5=e4c962a88d36c634ee09df2f1da11d54","Current remote sensing image classification problems have to deal with an unprecedented amount of heterogeneous and complex data sources. Upcoming missions will soon provide large data streams that will make land cover/use classification difficult. Machine-learning classifiers can help at this, and many methods are currently available. A popular kernel classifier is the Gaussian process classifier (GPC), since it approaches the classification problem with a solid probabilistic treatment, thus yielding confidence intervals for the predictions as well as very competitive results to the state-of-the-art neural networks and support vector machines. However, its computational cost is prohibitive for large-scale applications, and constitutes the main obstacle precluding wide adoption. This paper tackles this problem by introducing two novel efficient methodologies for GP classification. We first include the standard random Fourier features approximation into GPC, which largely decreases its computational cost and permits large-scale remote sensing image classification. In addition, we propose a model which avoids randomly sampling a number of Fourier frequencies and alternatively learns the optimal ones within a variational Bayes approach. The performance of the proposed methods is illustrated in complex problems of cloud detection from multispectral imagery and infrared sounding data. Excellent empirical results support the proposal in both computational cost and accuracy. IEEE","Cloud detection; Computational efficiency; Gaussian process classification (GPC); Gaussian processes; IAVISA; Image resolution; Infrared Atmospheric Sounding Interferometer (IASI)/Advanced Very High Resolution Radiometer (AVHRR); Kernel; random Fourier features (RFFs); Remote sensing; Spinning Enhanced Visible and Infrared Imager/Meteosat Second Generation (SEVIRI/MSG); Standards; Support vector machines; variational inference.","Complex networks; Computational efficiency; Costs; Fourier transforms; Gaussian distribution; Gaussian noise (electronic); Image enhancement; Image processing; Image resolution; Learning systems; Neural networks; Remote sensing; Standards; Support vector machines; Cloud detection; Fourier features; Gaussian process classifications; Gaussian Processes; IAVISA; Kernel; Second generation; Variational inference; Very high resolution; Image classification",2-s2.0-85032447623
"Pham M., Lefevre S., Aptoula E.","Local Feature-Based Attribute Profiles for Optical Remote Sensing Image Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032447318&doi=10.1109%2fTGRS.2017.2761402&partnerID=40&md5=c42bec7fe7b96dd2c106445b705998bd","This paper introduces an extension of morphological attribute profiles (APs) by extracting their local features. The so-called local feature-based APs (LFAPs) are expected to provide a better characterization of each APs' filtered pixel (i.e., APs' sample) within its neighborhood, and hence better deal with local texture information from the image content. In this paper, LFAPs are constructed by extracting some simple first-order statistical features of the local patch around each APs' sample such as mean, standard deviation, and range. Then, the final feature vector characterizing each image pixel is formed by combining all local features extracted from APs of that pixel. In addition, since the self-dual APs (SDAPs) have been proved to outperform the APs in recent years, a similar process will be applied to form the local feature-based SDAPs (LFSDAPs). In order to evaluate the effectiveness of LFAPs and LFSDAPs, supervised classification using both the random forest and the support vector machine classifiers is performed on the very high resolution Reykjavik image as well as the hyperspectral Pavia University data. Experimental results show that LFAPs (respectively, LFSDAPs) can considerably improve the classification accuracy of the standard APs (respectively, SDAPs) and the recently proposed histogram-based APs. IEEE","Attribute profiles (APs); Feature extraction; Histograms; Hyperspectral imaging; local feature-based APs (LFAPs); local feature-based self-dual APs (LFSDAPs); optical remote sensing imagery; self-dual APs (SDAPs); Standards; supervised classification.","Codes (symbols); Decision trees; Feature extraction; Graphic methods; Hyperspectral imaging; Image classification; Image retrieval; Information filtering; Pixels; Remote sensing; Spectroscopy; Standards; Supervised learning; Support vector machines; Attribute profiles (APs); Histograms; Local feature; Optical remote-sensing imagery; Self-dual; Supervised classification; Image processing",2-s2.0-85032447318
"Chen S.-H., Venkatachalam R.","Agent-based modelling as a foundation for big data",2017,"Journal of Economic Methodology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007408&doi=10.1080%2f1350178X.2017.1388964&partnerID=40&md5=3c7d5b8ea9574d5c29f58d538cdb38e2","In this article, we propose a process-based definition of big data, as opposed to the size- and technology-based definitions. We argue that big data should be perceived as a continuous, unstructured and unprocessed dynamics of primitives, rather than as points (snapshots) or summaries (aggregates) of an underlying phenomenon. Given this, we show that big data can be generated through agent-based models but not by equation-based models. Though statistical and machine learning tools can be used to analyse big data, they do not constitute a big data-generation mechanism. Furthermore, agent-based models can aid in evaluating the quality (interpreted as information aggregation efficiency) of big data. Based on this, we argue that agent-based modelling can serve as a possible foundation for big data. We substantiate this interpretation through some pioneering studies from the 1980s on swarm intelligence and several prototypical agent-based models developed around the 2000s. © 2017 Informa UK Limited, trading as Taylor & Francis Group","abduction; agent-based models; Big data; information aggregation; prediction markets; swarm",,2-s2.0-85032007408
"Micucci D., Mobilio M., Napoletano P.","UniMiB SHAR: A dataset for human activity recognition using acceleration data from smartphones",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032299017&doi=10.3390%2fapp7101101&partnerID=40&md5=d066575f17a0f12259019efead8ca3fb","Smartphones, smartwatches, fitness trackers, and ad-hoc wearable devices are being increasingly used to monitor human activities. Data acquired by the hosted sensors are usually processed by machine-learning-based algorithms to classify human activities. The success of those algorithms mostly depends on the availability of training (labeled) data that, if made publicly available, would allow researchers to make objective comparisons between techniques. Nowadays, there are only a few publicly available data sets, which often contain samples from subjects with too similar characteristics, and very often lack specific information so that is not possible to select subsets of samples according to specific criteria. In this article, we present a new dataset of acceleration samples acquired with an Android smartphone designed for human activity recognition and fall detection. The dataset includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL performed, the age, the gender, and so on. Finally, the dataset has been benchmarked with four different classifiers and with two different feature vectors. We evaluated four different classification tasks: fall vs. no fall, 9 activities, 8 falls, 17 activities and falls. For each classification task, we performed a 5-fold cross-validation (i.e., including samples from all the subjects in both the training and the test dataset) and a leave-one-subject-out cross-validation (i.e., the test data include the samples of a subject only, and the training data, the samples of all the other subjects). Regarding the classification tasks, the major findings can be summarized as follows: (i) it is quite easy to distinguish between falls and ADLs, regardless of the classifier and the feature vector selected. Indeed, these classes of activities present quite different acceleration shapes that simplify the recognition task; (ii) on average, it is more difficult to distinguish between types of falls than between types of activities, regardless of the classifier and the feature vector selected. This is due to the similarity between the acceleration shapes of different kinds of falls. On the contrary, ADLs acceleration shapes present differences except for a small group. Finally, the evaluation shows that the presence of samples of the same subject both in the training and in the test datasets, increases the performance of the classifiers regardless of the feature vector used. This happens because each human subject differs from other subjects in performing activities even if she shares with them the same physical characteristics. © 2017 by the authors.","Activity of daily living recognition; Dataset; Fall detection; Human activity recognition; Smartphone accelerometers",,2-s2.0-85032299017
"Zine W., Makni Z., Monmasson E., Idkhajine L., Condamin B.","Interests and limits of machine learning-based neural networks for rotor position estimation in EV traction drives",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032454857&doi=10.1109%2fTII.2017.2765398&partnerID=40&md5=761a65fb46d3922734b404f7fe9fe371","In this paper, a novel rotor position estimator for an interior permanent magnet synchronous motor is presented and evaluated. The proposed estimator lies on one of the most popular methods in the field of artificial intelligence : Machine learning-based neural networks algorithm. The main interest is to introduce a cost-efficient position estimator which is comparable to classic methods in terms of functional performances. The estimator model is built by learning from a dataset that associates phase currents and voltages to the rotor position. Learning signals are generated using a simulation model. This is primarily intended to save the resources invested in testbench trials. In this work, off-line training steps and results are described and commented. The efficiency of the proposed position estimator is first verified by functional simulations. Second, real-time experiments are conducted on an actual scale testbench. The NN-based estimator covers a wide speed range and is implemented in the context of IPMSM-based EV traction drives. More broadly, these findings can also be applicable to the AC-based electric drives for the position estimation purpose. IEEE","Artificial neural networks; Data models; Electric Vehicle (EV); Estimation; Interior Permanent Magnet Synchronous Machine (IPMSM); Machine Learning (ML); Machine learning algorithms; Neural networks (NN); position estimation; Rotors; Torque","Artificial intelligence; Data structures; Digital storage; Drives; Electric drives; Estimation; Learning systems; Magnets; Neural networks; Permanent magnets; Power transmission; Rotors; Synchronous motors; Torque; Traction (friction); Traction motors; Functional performance; Functional simulations; Interior permanent magnet synchronous machine; Interior permanent magnet synchronous motor; Neural network (nn); Neural networks algorithms; Position estimation; Rotor position estimation; Learning algorithms",2-s2.0-85032454857
"Orsini F., Frasconi P., de Raedt L.","kProbLog: an algebraic Prolog for machine learning",2017,"Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919664&doi=10.1007%2fs10994-017-5668-y&partnerID=40&md5=a9ffc5472db86f93d2ccf0a731dcfd8e","We introduce kProbLog as a declarative logical language for machine learning. kProbLog is a simple algebraic extension of Prolog with facts and rules annotated by semi-ring labels. It allows to elegantly combine algebraic expressions with logic programs. We introduce the semantics of kProbLog, its inference algorithm, its implementation and provide convergence guarantees. We provide several code examples to illustrate its potential for a wide range of machine learning techniques. In particular, we show the encodings of state-of-the-art graph kernels such as Weisfeiler-Lehman graph kernels, propagation kernels and an instance of graph invariant kernels, a recent framework for graph kernels with continuous attributes. However, kProbLog is not limited to kernel methods and it can concisely express declarative formulations of tensor-based algorithms such as matrix factorization and energy-based models, and it can exploit semirings of dual numbers to perform algorithmic differentiation. Furthermore, experiments show that kProbLog is not only of theoretical interest, but can also be applied to real-world datasets. At the technical level, kProbLog extends aProbLog (an algebraic Prolog) by allowing multiple semirings to coexist in a single program and by introducing meta-functions for manipulating algebraic values. © 2017 The Author(s)","Algebraic Prolog; Graph kernels; Kernel programming; Machine learning","Algebra; Artificial intelligence; Factorization; Inference engines; Learning systems; Logic programming; Semantics; Algebraic expression; Algebraic Prolog; Algorithmic differentiations; Continuous attribute; Graph kernels; Machine learning techniques; Matrix factorizations; Real-world datasets; PROLOG (programming language)",2-s2.0-85031919664
"Anbar M., Abdullah R., Al-Tamimi B.N., Hussain A.","A Machine Learning Approach to Detect Router Advertisement Flooding Attacks in Next-Generation IPv6 Networks",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031915216&doi=10.1007%2fs12559-017-9519-8&partnerID=40&md5=3d3e46d6747b1875c3e1be5ac6364a9c","Router advertisement (RA) flooding attack aims to exhaust all node resources, such as CPU and memory, attached to routers on the same link. A biologically inspired machine learning-based approach is proposed in this study to detect RA flooding attacks. The proposed technique exploits information gain ratio (IGR) and principal component analysis (PCA) for feature selection and a support vector machine (SVM)-based predictor model, which can also detect input traffic anomaly. A real benchmark dataset obtained from National Advanced IPv6 Center of Excellence laboratory is used to evaluate the proposed technique. The evaluation process is conducted with two experiments. The first experiment investigates the effect of IGR and PCA feature selection methods to identify the most contributed features for the SVM training model. The second experiment evaluates the capability of SVM to detect RA flooding attacks. The results show that the proposed technique demonstrates excellent detection accuracy and is thus an effective choice for detecting RA flooding attacks. The main contribution of this study is identification of a set of new features that are related to RA flooding attack by utilizing IGR and PCA algorithms. The proposed technique in this paper can effectively detect the presence of RA flooding attack in IPv6 network. © 2017 Springer Science+Business Media, LLC","IGR; IPv6 security; Network security; PCA; RA flooding attack; SVM","Artificial intelligence; Floods; Internet protocols; Learning algorithms; Learning systems; Network security; Next generation networks; Principal component analysis; Routers; Support vector machines; Benchmark datasets; Biologically inspired; Feature selection methods; Flooding attacks; Information gain ratio; IPv6 security; Machine learning approaches; Router Advertisements; Feature extraction",2-s2.0-85031915216
"Sun B., Fernandez M., Barnard A.S.","Machine Learning for Silver Nanoparticle Electron Transfer Property Prediction",2017,"Journal of Chemical Information and Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031994675&doi=10.1021%2facs.jcim.7b00272&partnerID=40&md5=5f4cc2216550d901af5b6c1268263833","Nanoparticles exhibit diverse structural and morphological features that are often interconnected, making the correlation of structure/property relationships challenging. In this study a multi-structure/single-property relationship of silver nanoparticles is developed for the energy of Fermi level, which can be tuned to improve the transfer of electrons in a variety of applications. By combining different machine learning analytical algorithms, including k-mean, logistic regression, and random forest with electronic structure simulations, we find that the degree of twinning (characterized by the fraction of hexagonal closed packed atoms) and the population of the {111} facet (characterized by a surface coordination number of nine) are strongly correlated to the Fermi energy of silver nanoparticles. A concise three layer artificial neural network together with principal component analysis is built to predict this property, with reduced geometrical, structural, and topological features, making the method ideal for efficient and accurate high-throughput screening of large-scale virtual nanoparticle libraries and the creation of single-structure/single-property, multi-structure/single-property, and single-structure/multi-property relationships in the near future. © 2017 American Chemical Society.",,"Artificial intelligence; Decision trees; Digital libraries; Electronic structure; Fermi level; Learning systems; Metal nanoparticles; Nanoparticles; Network layers; Neural networks; Principal component analysis; Stress corrosion cracking; Analytical algorithms; High throughput screening; Logistic regressions; Morphological features; Silver nanoparticles; Structure simulations; Structure/property relationships; Topological features; Silver",2-s2.0-85031994675
"Diez-Olivan A., Pagan J.A., Khoa N.L.D., Sanz R., Sierra B.","Kernel-based support vector machines for automated health status assessment in monitoring sensor data",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031941572&doi=10.1007%2fs00170-017-1204-2&partnerID=40&md5=a4c12bede387def01c8e72500e150576","This paper presents a novel algorithm to assess the health status in monitoring sensor data using a kernel-based support vector machine (SVM) approach. Today, accurate fault prediction is a key issue raised by maintenance. In particular, automatically modelling the normal behaviour from condition monitoring data is probably one of the most challenging problems, specially when there is limited information of real faults. To overcome this difficulty, a data-driven learning framework based on nonparametric density estimation for outlier detection and ν-SVM for normality modelling, with optimal bandwidth selection, is proposed. A health score based on the log-normalisation of the distance to the separating hyperplane is also provided. Experimental results obtained when analysing the propagation of a critical fault over time in a marine diesel engine demonstrate the validity of the algorithm. The predictions of normality models learned were compared to those of the k-nearest neighbours (kNN) method. Low false positive rates on healthy data and improved prediction capacities are achieved. © 2017 Springer-Verlag London Ltd.","Bandwidth selection; Condition monitoring; Fault prediction; Health status assessment; Kernel density estimator; Machine learning; Normality modelling; Support vector machines","Bandwidth; Condition monitoring; Diesel engines; Forecasting; Health; Learning systems; Marine engineering; Maximum likelihood estimation; Monitoring; Nearest neighbor search; Bandwidth selections; Condition-monitoring data; Fault prediction; Health status; K nearest neighbours (k-NN); Kernel density estimators; Marine Diesel Engines; Nonparametric density estimation; Support vector machines",2-s2.0-85031941572
"Arsenault L.-F., Neuberg R., Hannah L.A., Millis A.J.","Projected regression method for solving Fredholm integral equations arising in the analytic continuation problem of quantum physics",2017,"Inverse Problems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032745219&doi=10.1088%2f1361-6420%2faa8d93&partnerID=40&md5=36c88a0b817bb33bfd0495cd1c582e21","We present a supervised machine learning approach to the inversion of Fredholm integrals of the first kind as they arise, for example, in the analytic continuation problem of quantum many-body physics. The approach provides a natural regularization for the ill-conditioned inverse of the Fredholm kernel, as well as an efficient and stable treatment of constraints. The key observation is that the stability of the forward problem permits the construction of a large database of outputs for physically meaningful inputs. Applying machine learning to this database generates a regression function of controlled complexity, which returns approximate solutions for previously unseen inputs; the approximate solutions are then projected onto the subspace of functions satisfying relevant constraints. Under standard error metrics the method performs as well or better than the Maximum Entropy method for low input noise and is substantially more robust to increased input noise. We suggest that the methodology will be similarly effective for other problems involving a formally ill-conditioned inversion of an integral operator, provided that the forward problem can be efficiently solved. © 2017 IOP Publishing Ltd.","analytical continuation; integral equations; machine learning; regularization","Artificial intelligence; Functional analysis; Integral equations; Learning systems; Maximum entropy methods; Regression analysis; Supervised learning; Analytic continuation; Analytical continuation; Approximate solution; Fredholm integral equations; Integral operators; Regression function; regularization; Supervised machine learning; Inverse problems",2-s2.0-85032745219
"Pang S., Del Coz J.J., Yu Z., Luaces O., Díez J.","Deep Learning and Preference Learning for Object Tracking: A Combined Approach",2017,"Neural Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031915843&doi=10.1007%2fs11063-017-9720-5&partnerID=40&md5=6ab5ac8ce99070900966acf4165ec867","Object tracking is one of the most important processes for object recognition in the field of computer vision. The aim is to find accurately a target object in every frame of a video sequence. In this paper we propose a combination technique of two algorithms well-known among machine learning practitioners. Firstly, we propose a deep learning approach to automatically extract the features that will be used to represent the original images. Deep learning has been successfully applied in different computer vision applications. Secondly, object tracking can be seen as a ranking problem, since the regions of an image can be ranked according to their level of overlapping with the target object (ground truth in each video frame). During object tracking, the target position and size can change, so the algorithms have to propose several candidate regions in which the target can be found. We propose to use a preference learning approach to build a ranking function which will be used to select the bounding box that ranks higher, i.e., that will likely enclose the target object. The experimental results obtained by our method, called (Formula presented.) (Deep and Preference Learning), are competitive with respect to other algorithms. © 2017 Springer Science+Business Media, LLC","Deep learning; Object tracking; Preference learning","Computer vision; Deep learning; Image processing; Learning systems; Object recognition; Target tracking; Computer vision applications; Learning approach; Object Tracking; Preference learning; Ranking functions; Ranking problems; Target position; Video sequences; Learning algorithms",2-s2.0-85031915843
"Stacey R.G., Skinnider M.A., Scott N.E., Foster L.J.","A rapid and accurate approach for prediction of interactomes from co-elution data (PrInCE)",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031900758&doi=10.1186%2fs12859-017-1865-8&partnerID=40&md5=673cb14e9f6fa15a77ecee9f0e9005d8","Background: An organism's protein interactome, or complete network of protein-protein interactions, defines the protein complexes that drive cellular processes. Techniques for studying protein complexes have traditionally applied targeted strategies such as yeast two-hybrid or affinity purification-mass spectrometry to assess protein interactions. However, given the vast number of protein complexes, more scalable methods are necessary to accelerate interaction discovery and to construct whole interactomes. We recently developed a complementary technique based on the use of protein correlation profiling (PCP) and stable isotope labeling in amino acids in cell culture (SILAC) to assess chromatographic co-elution as evidence of interacting proteins. Importantly, PCP-SILAC is also capable of measuring protein interactions simultaneously under multiple biological conditions, allowing the detection of treatment-specific changes to an interactome. Given the uniqueness and high dimensionality of co-elution data, new tools are needed to compare protein elution profiles, control false discovery rates, and construct an accurate interactome. Results: Here we describe a freely available bioinformatics pipeline, PrInCE, for the analysis of co-elution data. PrInCE is a modular, open-source library that is computationally inexpensive, able to use label and label-free data, and capable of detecting tens of thousands of protein-protein interactions. Using a machine learning approach, PrInCE offers greatly reduced run time, more predicted interactions at the same stringency, prediction of protein complexes, and greater ease of use over previous bioinformatics tools for co-elution data. PrInCE is implemented in Matlab (version R2017a). Source code and standalone executable programs for Windows and Mac OSX are available at https://github.com/fosterlab/PrInCE , where usage instructions can be found. An example dataset and output are also provided for testing purposes. Conclusions: PrInCE is the first fast and easy-to-use data analysis pipeline that predicts interactomes and protein complexes from co-elution data. PrInCE allows researchers without bioinformatics expertise to analyze high-throughput co-elution datasets. © 2017 The Author(s).","Co-elution; Co-fractionation; Data analysis; Interactome; Protein correlation profiling; Protein-protein interaction; Proteomics; Software; System biology","Bioinformatics; Cell culture; Complex networks; Complexation; Computer software; Data handling; Data reduction; Digital storage; Information analysis; Learning systems; Mass spectrometry; MATLAB; Molecular biology; Open source software; Pipelines; Scalability; Statistical tests; Biological conditions; Complementary techniques; Interactome; Machine learning approaches; Protein-protein interactions; Proteomics; Stable-isotope labeling; System biology; Proteins",2-s2.0-85031900758
"Li Y., Yang Z.","Application of EOS-ELM with Binary Jaya- Based Feature Selection to Real-Time Transient Stability Assessment Using PMU Data",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032435506&doi=10.1109%2fACCESS.2017.2765626&partnerID=40&md5=813ad1e23153f582da997ba909cd1766","Recent researches show that pattern recognitionbased transient stability assessment (PRTSA) is a promising approach for predicting the transient stability status of power systems. However, many of the current well-known PRTSA methods suffer from excessive training time and complex tuning of parameters, resulting in inefficiency for real-time implementation and lacking the on-line model updating ability. In this paper, a novel PRTSA approach based on an ensemble of OSELM (EOS-ELM) with binary Jaya (BinJaya)-based feature selection is proposed with the use of PMU data. After briefly describing the principles of OS-ELM, an EOS-ELM-based PRTSA model is built to predict the post-fault transient stability status of power systems in real time by integrating OS-ELM and an online boosting algorithm respectively as a weak classifier and an ensemble learning algorithm. Furthermore, a BinJaya-based feature selection approach is put forward for selecting an optimal feature subset from the entire feature space constituted by a group of system-level classification features extracted from PMU data. The application results on the IEEE 39-bus system and a real provincial system show that the proposal has superior computation speed and prediction accuracy than other state-ofthe- art sequential learning algorithms. In addition, without sacrificing the classification performance, the dimension of the input space has been reduced to about one-third of its initial value. OAPA","binary Jaya algorithm; ensemble learning; extreme learning machine; Feature extraction; feature selection; Power system stability; Stability criteria; Training; Transient analysis; transient stability","Bins; Classification (of information); Feature extraction; Forecasting; Learning algorithms; Learning systems; Online systems; Pattern recognition; Personnel training; Real time control; Real time systems; Stability; System stability; Transient analysis; Classification performance; Ensemble learning; Ensemble learning algorithm; Extreme learning machine; Power system stability; Real-time implementations; Sequential learning algorithm; Transient stability assessment; Stability criteria",2-s2.0-85032435506
"Brondeel R., Kestens Y., Chaix B.","An evaluation of transport mode shift policies on transport-related physical activity through simulations based on random forests",2017,"International Journal of Behavioral Nutrition and Physical Activity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031922752&doi=10.1186%2fs12966-017-0600-1&partnerID=40&md5=735323968df1b0586a044f11bd781125","Background: Physical inactivity is widely recognized as one of the leading causes of mortality, and transport accounts for a large part of people's daily physical activity. This study develops a simulation approach to evaluate the impact of the Ile-de-France Urban Mobility Plan (2010-2020) on physical activity, under the hypothesis that the intended transport mode shifts are realized. Methods: Based on the Global Transport Survey (2010, n = 21,332) and on the RECORD GPS Study (2012-2013, n = 229) from the French capital region of Paris (Ile-de-France), a simulation method was designed and tested. The simulation method used accelerometer data and random forest models to predict the impact of the transport mode shifts anticipated in the Mobility Plan on transport-related moderate-to-vigorous physical activity (T-MVPA). The transport mode shifts include less private motorized trips in favor of more public transport, walking, and biking trips. Results: The simulation model indicated a mean predicted increase of 2 min per day of T-MVPA, in case the intended transport mode shifts in the Ile-de-France Urban Mobility Plan were realized. The positive effect of the transport mode shifts on T-MVPA would, however, be larger for people with a higher level of education. This heterogeneity in the positive effect would further increase the existing inequality in transport-related physical activity by educational level. Conclusions: The method presented in this paper showed a significant increase in transport-related physical activity in case the intended mode shifts in the Ile-de-France Urban Mobility Plan were realized. This simulation method could be applied on other important health outcomes, such as exposure to noise or air pollution, making it a useful tool to anticipate the health impact of transport interventions or policies. © 2017 The Author(s).","Active transport; Data integration; France; Health inequalities; Machine learning; RECORD cohort study; Simulation study; Transport","adult; aged; Article; clinical effectiveness; educational status; France; health care policy; health program; health status; health survey; human; management; motor vehicle; physical activity; random forest; simulation; traffic and transport; walking",2-s2.0-85031922752
"Çelik E., Öztürk N.","First application of symbiotic organisms search algorithm to off-line optimization of PI parameters for DSP-based DC motor drives",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031922717&doi=10.1007%2fs00521-017-3256-5&partnerID=40&md5=8a6cb9f8320bac81eff06438393f8e1b","To enhance the performance and dynamics of a direct current (DC) motor drive, this paper proposes a new alternative based on recently introduced powerful symbiotic organisms search (SOS) algorithm for tuning proportional integral parameters. While imitating the symbiotic behavior that is seen among organisms in an ecosystem, SOS has important features such that it does not require tuning parameters, and its implementation is very easy with efficient three phases. After obtaining the optimized values of Kp − Ki pair within the accurately prepared simulation software, they are used in real time. By managing the DC motor speed-controlled system with DSP of TMS320F28335, several simulations and experimental results confirming the performance of our proposal are presented along with comparisons against those of particle swarm optimization (PSO), genetic algorithm (GA), and Ziegler–Nichols (Z–N) tuning method. Results explicitly show that SOS is the pioneer in yielding better tracking performance and load disturbance rejection capability of the concerned drive system, which is followed by PSO, GA, and Z–N method, respectively. This has been achieved due to the fact that the gains obtained by SOS are more performant than those obtained by other applied methods. © 2017 The Natural Computing Applications Forum","DC motor; Genetic algorithm; Optimized PI control; Particle swarm optimization; Symbiotic organisms search algorithm; Ziegler–Nichols","Biology; Computer software; Digital signal processing; Disturbance rejection; Electric drives; Electric machine control; Electric motors; Genetic algorithms; Learning algorithms; Optimization; Parameter estimation; Particle swarm optimization (PSO); Systems engineering; Two term control systems; Important features; Load disturbance rejection capabilities; Off-line optimization; PI control; Proportional-integral parameters; Search Algorithms; Simulation software; Tracking performance; DC motors",2-s2.0-85031922717
"Bosc N., Kuenemann M.A., Bécot J., Vavrusa M., Cerdan A.H., Sperandio O.","Privileged Substructures to Modulate Protein-Protein Interactions",2017,"Journal of Chemical Information and Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032022746&doi=10.1021%2facs.jcim.7b00435&partnerID=40&md5=012419f1784ec210c33f4a38caac8ec4","Given the difficulties to identify chemical probes that can modulate protein-protein interactions (PPIs), actors in the field have started to agree on the necessity to use PPI-tailored screening chemical collections. However, which type of scaffolds may promote the binding of compounds to PPI targets remains unclear. In this big data analysis, we have identified a list of privileged chemical substructures that are most often observed within inhibitors of PPIs. Using molecular frameworks as a way to perceive chemical substructures with the combination of an experimental and a machine-learning based predicted data set of iPPI compounds, we propose a list of privileged substructures in the form of scaffolds and chemical moieties that can be substantially chemically functionalized and do not present any toxicophore nor pan-assay interference (PAINS) alerts. We think that such chemical guidance will be valuable for medicinal chemists in their attempt to identify initial quality chemical probes on PPI targets. © 2017 American Chemical Society.",,"Big data; Chemical analysis; Learning systems; Probes; Proteins; Chemical moieties; Chemical probes; Data set; Functionalized; Molecular frameworks; Protein-protein interactions; Chemical compounds",2-s2.0-85032022746
"Huang S., Wang H., Li T., Li T., Xu Z.","Robust graph regularized nonnegative matrix factorization for clustering",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031936297&doi=10.1007%2fs10618-017-0543-9&partnerID=40&md5=89a79e04da91f0e4b6efef7e33306362","Nonnegative matrix factorization and its graph regularized extensions have received significant attention in machine learning and data mining. However, existing approaches are sensitive to outliers and noise due to the utilization of the squared loss function in measuring the quality of graph regularization and data reconstruction. In this paper, we present a novel robust graph regularized NMF model (RGNMF) to approximate the data matrix for clustering. Our assumption is that there may exist some entries of the data corrupted arbitrarily, but the corruption is sparse. To address this problem, an error matrix is introduced to capture the sparse corruption. With this sparse outlier matrix, a robust factorization result could be obtained since a much cleaned data could be reconstructed. Moreover, the (Formula presented.)-norm function is used to alleviate the influence of unreliable regularization which is incurred by unexpected graphs. That is, the sparse error matrix alleviates the impact of noise and outliers, and the (Formula presented.)-norm function leads to a faithful regularization since the influence of the unreliable regularization errors can be reduced. Thus, RGNMF is robust to unreliable graphs and noisy data. In order to solve the optimization problem of our method, an iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Experimental results show that the proposed method consistently outperforms many state-of-the-art methods. © 2017 The Author(s)","$$\ell _{1}$$ℓ1-norm function; Clustering; Nonnegative matrix factorization; Robust regularization","Crime; Data mining; Errors; Factorization; Iterative methods; Learning systems; Optimization; Statistics; Clustering; Data reconstruction; Nonnegative matrix factorization; Optimization problems; Regularization errors; Robust regularization; State-of-the-art methods; Updating algorithm; Matrix algebra",2-s2.0-85031936297
"Zuo R., Xiong Y.","Big Data Analytics of Identifying Geochemical Anomalies Supported by Machine Learning Methods",2017,"Natural Resources Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031925628&doi=10.1007%2fs11053-017-9357-0&partnerID=40&md5=7f5d13ae9855367873092355a5eddc2e","Big data analytics brings a novel way for identifying geochemical anomalies in mineral exploration because it involves processing of the whole geochemical dataset to reveal statistical correlations between geochemical patterns and known mineralization. Traditional methods of processing exploration geochemical data mainly involve the identification of positive geochemical anomalies related to mineralization, but ignore negative geochemical anomalies. Therefore, the identified geochemical anomalies do not completely reflect the sought geochemical signature of mineralization, leading to uncertainty in geochemical prospecting. In this study, data for 39 geochemical variables from a regional stream sediment geochemical survey of southwest Fujian Province of China were subjected to big data analytics for identifying geochemical anomalies related to skarn-type Fe polymetallic mineralization through deep autoencoder network. The receiver operating characteristic (ROC) and areas under curve (AUC) were applied to evaluate the performance of big data analytics. The AUC of the anomaly map obtained using all the geochemical variables is larger than the AUC of the anomaly map obtained using only five selected elements known to be associated with the mineralization (i.e., Fe2O3, Cu, Pb, Zn, Mn). This indicates that big data analytics, with the support of machine learning methods, is a powerful tool for identifying multivariate geochemical anomalies related to mineralization. © 2017 International Association for Mathematical Geosciences","Big data analytics; Fujian Province; Geochemical anomalies; GIS; Machine learning; Mineral exploration",,2-s2.0-85031925628
"Mandal I.","Machine learning algorithms for the creation of clinical healthcare enterprise systems",2017,"Enterprise Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994156958&doi=10.1080%2f17517575.2016.1251617&partnerID=40&md5=dee57e11fc29e6184c4ccd62153c015a","Clinical recommender systems are increasingly becoming popular for improving modern healthcare systems. Enterprise systems are persuasively used for creating effective nurse care plans to provide nurse training, clinical recommendations and clinical quality control. A novel design of a reliable clinical recommender system based on multiple classifier system (MCS) is implemented. A hybrid machine learning (ML) ensemble based on random subspace method and random forest is presented. The performance accuracy and robustness of proposed enterprise architecture are quantitatively estimated to be above 99% and 97%, respectively (above 95% confidence interval). The study then extends to experimental analysis of the clinical recommender system with respect to the noisy data environment. The ranking of items in nurse care plan is demonstrated using machine learning algorithms (MLAs) to overcome the drawback of the traditional association rule method. The promising experimental results are compared against the sate-of-the-art approaches to highlight the advancement in recommendation technology. The proposed recommender system is experimentally validated using five benchmark clinical data to reinforce the research findings. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","Clinical recommender systems; enterprise systems; healthcare; hybrid machine learning; ranking; robustness","Artificial intelligence; Decision trees; Health care; Learning systems; Nursing; Quality control; Recommender systems; Robustness (control systems); Enterprise Architecture; Enterprise system; Hybrid machine learning; Machine learning algorithm (MLAs); Multiple classifier systems; Random subspace method; ranking; Recommendation technologies; Learning algorithms",2-s2.0-84994156958
"Zarkogianni K., Athanasiou M., Thanopoulou A.C., Nikita K.S.","Comparison of machine learning approaches towards assessing the risk of developing Cardiovascular disease as a long-term diabetes complication",2017,"IEEE Journal of Biomedical and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440793&doi=10.1109%2fJBHI.2017.2765639&partnerID=40&md5=4a827f05374f288973262ccb6b641b0c","The estimation of long-term diabetes complications risk is essential in the process of medical decision making. Guidelines for the management of Type 2 Diabetes Mellitus (T2DM) advocate calculating the Cardiovascular Disease (CVD) risk to initiate appropriate treatment. The objective of this study is to investigate the use of sophisticated machine learning techniques towards the development of personalized models able to predict the risk of fatal or non-fatal CVD incidence in T2DM patients. The important challenge of handling the unbalanced nature of the available dataset is addressed, by applying novel ensemble strategies. Hybrid Wavelet Neural Networks (HWNNs) and Self-Organizing Maps (SOMs) constitute the primary models for building ensembles following a sub-sampling approach. Different methods for combining the decisions of the primary models are applied and comparatively assessed. Data from the 5-year follow up of 560 patients with T2DM are used for development and evaluation purposes. The highest discrimination performance (Area Under the Curve (AUC): 71.48%) is achieved by taking into account both the HWNN- and SOM- based primary models&#x0027; outputs. The proposed method is superior to the Binomial Linear Regression (BLR) model justifying the need to apply more sophisticated techniques in order to produce reliable CVD risk scores. IEEE","Biological system modeling; Cardiovascular Disease; Computational modeling; Diabetes; Diabetes; Diseases; machine learning; Predictive models; Self-organizing feature maps; UKPDS; unbalanced data","Artificial intelligence; Biological systems; Cardiology; Conformal mapping; Decision making; Learning systems; Medical problems; Risk assessment; Risk perception; Self organizing maps; Biological system modeling; Cardio-vascular disease; Computational model; Predictive models; UKPDS; Unbalanced data; Diseases",2-s2.0-85032440793
"Grazioli G., Butts C.T., Andricioaei I.","Automated placement of interfaces in conformational kinetics calculations using machine learning",2017,"Journal of Chemical Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031816219&doi=10.1063%2f1.4989857&partnerID=40&md5=8655575d0995ce633a38dd97ad8efad6","Several recent implementations of algorithms for sampling reaction pathways employ a strategy for placing interfaces or milestones across the reaction coordinate manifold. Interfaces can be introduced such that the full feature space describing the dynamics of a macromolecule is divided into Voronoi (or other) cells, and the global kinetics of the molecular motions can be calculated from the set of fluxes through the interfaces between the cells. Although some methods of this type are exact for an arbitrary set of cells, in practice, the calculations will converge fastest when the interfaces are placed in regions where they can best capture transitions between configurations corresponding to local minima. The aim of this paper is to introduce a fully automated machine-learning algorithm for defining a set of cells for use in kinetic sampling methodologies based on subdividing the dynamical feature space; the algorithm requires no intuition about the system or input from the user and scales to high-dimensional systems. © 2017 Author(s).",,"Artificial intelligence; Cells; Cytology; Kinetics; Learning systems; Automated placements; Dynamical features; Fully automated; High-dimensional systems; Kinetics calculations; Molecular motions; Reaction coordinates; Reaction pathways; Learning algorithms",2-s2.0-85031816219
"Li G., Zeng L., Zhang L., Wu Q.M.J.","State Identification of Duffing Oscillator Based on Extreme Learning Machine",2017,"IEEE Signal Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441174&doi=10.1109%2fLSP.2017.2765895&partnerID=40&md5=80881eed04ac9b828c58dc3335f0fe82","As an important weak target detection method, Duffing oscillator is very effective in detecting signals with very low signal-to-noise ratio (SNR). However, the accurate discrimination between chaotic and periodic states is a crucial problem and that is the prerequisite for using the Duffing oscillator. Conventionally, the Lyapunov exponent is used as an index to identify different states, but as this indicator has the problem of heavy computation cost, slow convergence rate and requires a mass of data, its application becomes seriously limits. To solve this problem, a novel method for state identification of the Duffing oscillator based on extreme learning machine (ELM) is proposed. The feature data, as the input of ELM, is extracted from the phase diagram and the time series of the Duffing oscillator. Three effective features are extracted in this paper, i.e., ratio of points in and out of the closed region, average distance, and power spectrum. Computer simulations are presented to validate the proposed method and demonstrate that the state classification performance is superior to other related methods with higher computation efficiency, faster convergence rate and better accuracy. IEEE","Bifurcation; Duffing oscillator; extreme learning machine; Feature extraction; feature extraction; Mathematical model; Orbits; Oscillators; Signal to noise ratio; state identification; Time series analysis","Bifurcation (mathematics); Extraction; Feature extraction; Knowledge acquisition; Learning systems; Lyapunov methods; Mathematical models; Orbits; Oscillators (electronic); Oscillators (mechanical); Problem solving; Signal detection; Time series analysis; Computation efficiency; Duffing oscillator; Extreme learning machine; Faster convergence; Low signal-to-noise ratio; State classification; State identification; Weak target detection; Signal to noise ratio",2-s2.0-85032441174
"Jordan P.L., Peterson G.L., Lin A.C., Mendenhall M.J., Sellers A.J.","Narrowing the scope of failure prediction using targeted fault load injection",2017,"Enterprise Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031778357&doi=10.1080%2f17517575.2017.1390167&partnerID=40&md5=c2e37295fc507b4a893bd7a93c11b36b","As society becomes more dependent upon computer systems to perform increasingly critical tasks, ensuring that those systems do not fail becomes increasingly important. Many organizations depend heavily on desktop computers for day-to-day operations. Unfortunately, the software that runs on these computers is written by humans and, as such, is still subject to human error and consequent failure. A natural solution is to use statistical machine learning to predict failure. However, since failure is still a relatively rare event, obtaining labelled training data to train these models is not a trivial task. This work presents new simulated fault-inducing loads that extend the focus of traditional fault injection techniques to predict failure in the Microsoft enterprise authentication service and Apache web server. These new fault loads were successful in creating failure conditions that were identifiable using statistical learning methods, with fewer irrelevant faults being created. © 2017 Informa UK Limited, trading as Taylor & Francis Group","enterprise architecture; fault injection; machine learning; Online failure prediction","Artificial intelligence; Forecasting; Personal computers; Software testing; Web services; Windows operating system; Authentication services; Day-to-day operations; Enterprise Architecture; Failure prediction; Fault injection; Fault Injection techniques; Statistical learning methods; Statistical machine learning; Learning systems",2-s2.0-85031778357
"Wang J., Zhong H., Lai X., Xia Q., Wang Y., Kang C.","Exploring Key Weather Factors from Analytical Modeling toward Improved Solar Power Forecasting",2017,"IEEE Transactions on Smart Grid",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032442916&doi=10.1109%2fTSG.2017.2766022&partnerID=40&md5=b87023c82bbe1fdfa0da6d6614674b56","Accurate solar power forecasting plays a critical role in ensuring the reliable and economic operation of power grids. Most of existing literature directly uses available weather conditions as input features, which might ignore some key weather factors and the coupling among weather conditions. Therefore, a novel solar power forecasting approach is proposed in this paper by exploring key weather factors from PV analytical modeling. The proposed approach is composed of three engines: i) analytical modeling of PV systems; ii) machine learning methods for mapping weather features with solar power; and iii) a deviation analysis for solar power forecast adjustment. In contrast to the existing research that directly uses available weather conditions, this paper explores the physical knowledge from PV models. Different irradiance components and PV cell temperatures are derived from PV analytical modeling. These weather features are used to reformulate the input of machine learning methods, which helps achieve a better forecasting performance. Moreover, based on the historical forecasting deviations, a compensation term is presented to adjust the solar power forecast. Case studies based on measured datasets from PV systems in Australia demonstrate that the forecasting performance can be highly improved by taking advantage of the key weather features derived from PV models. IEEE","Analytical modeling; Analytical models; deviation analysis; Engines; Forecasting; Predictive models; solar power forecasting; weather knowledge.; Wind forecasting","Analytical models; Artificial intelligence; Couplings; Electric power system economics; Electric power transmission networks; Engines; Forecasting; Learning systems; Meteorology; Photovoltaic cells; Solar energy; Deviation analysis; Economic operations; Forecasting performance; Machine learning methods; Modeling of PV systems; Power forecasting; Predictive models; Wind forecasting; Weather forecasting",2-s2.0-85032442916
"Yang X., Wang M., Tao D.","Person Re-identification with Metric Learning using Privileged Information",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436440&doi=10.1109%2fTIP.2017.2765836&partnerID=40&md5=f48eac2bff42dd579bb4587ff75be214","Despite the promising progress made in recent years, person re-identification remains a challenging task due to complex variations in human appearances from different camera views. This paper presents a logistic discriminant metric learning method for this challenging problem. Different with most existing metric learning algorithms, it exploits both original data and auxiliary data during training, which is motivated by the new machine learning paradigm - Learning Using Privileged Information. Such privileged information is a kind of auxiliary knowledge which is only available during training. Our goal is to learn an optimal distance function by constructing a locally adaptive decision rule with the help of privileged information. We jointly learn two distance metrics by minimizing the empirical loss penalizing the difference between the distance in the original space and that in the privileged space. In our setting, the distance in the privileged space functions as a local decision threshold, which guides the decision making in the original space like a teacher. The metric learned from the original space is used to compute the distance between a probe image and a gallery image during testing. In addition, we extend the proposed approach to a multi-view setting which is able to explore the complementation of multiple feature representations. In the multi-view setting, multiple metrics corresponding to different original features are jointly learned, guided by the same privileged information. Besides, an effective iterative optimization scheme is introduced to simultaneously optimize the metrics and the assigned metric weights. Experiment results on several widely-used datasets demonstrate that the proposed approach is superior to global decision threshold based methods and outperforms most state-of-the-art results. IEEE","Cameras; Computer Vision; Learning systems; Learning using Privileged Information; Logistics; Measurement; Metric Learning; Person Re-identification; Probes; Testing; Training","Cameras; Computer vision; Decision making; Iterative methods; Learning algorithms; Learning systems; Logistics; Measurements; Personnel training; Probes; Teaching; Testing; Decision threshold; Distance functions; Distance metrics; Iterative Optimization; Learning using Privileged Information; Metric learning; Multiple features; Person re identifications; Education",2-s2.0-85032436440
"Jiang H., Mei C., Chen Q.","Rapid identification of fermentation stages of bioethanol solid-state fermentation (SSF) using FT-NIR spectroscopy: Comparisons of linear and non-linear algorithms for multiple classification issues",2017,"Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031675622&doi=10.1039%2fc7ay01861d&partnerID=40&md5=cc159a3f2f838aec4e8a94dbdd828a74","Solid-state fermentation (SSF) is a critical step in bioethanol production, and a means for the effective monitoring of the SSF process is urgently needed due to the rapid changes in the SSF industry, which demands fast tools that could provide real time information to ensure the quality of the final product. The aim of the present study was to investigate the FT-NIR spectroscopy technique associated with supervised pattern recognition methods in order to develop a means to monitor the time-related molecular changes that occur during the SSF of bioethanol. Principal component analysis as an exploratory tool was employed to uncover details on the molecular modifications of the spectral data during the SSF process. Furthermore, identification models were constructed using partial least squares discriminant analysis (PLS-DA), back propagation neural network (BPNN), support vector machine (SVM), and extreme learning machine (ELM) algorithms. The parameters of the four algorithms were optimized by leave-one-out cross-validation (LOOCV) for the calibration of the identification models. The experimental results showed that the nonlinear identification models achieved strong classification performance to identify the fermentation stages in the SSF of bioethanol. Moreover, compared with the BPNN and SVM models, the ELM model achieved a slightly better generalization performance with an identification rate of 92.60% in the validation process. The overall results show that the ELM-FT-NIR methodology was efficient in accurately identifying the fermentation stages during the SSF of bioethanol, thus demonstrating its potential for application in the in situ monitoring and control of large-scale industrial processes. © 2017 The Royal Society of Chemistry.",,"Backpropagation; Backpropagation algorithms; Bioethanol; Data handling; Discriminant analysis; Ethanol; Identification (control systems); Infrared devices; Learning systems; Least squares approximations; Near infrared spectroscopy; Neural networks; Pattern recognition; Principal component analysis; Statistical methods; Support vector machines; Back-propagation neural networks; Classification performance; Generalization performance; Large-scale industrial process; Leave-one-out cross-validation (LOOCV); Nonlinear identifications; Partial least squares discriminant analyses (PLSDA); Supervised pattern recognition; Fermentation",2-s2.0-85031675622
"Wang J., Wang G., Zhou M.","Bimodal Vein Data Mining via Cross-Selected-Domain Knowledge Transfer",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450388&doi=10.1109%2fTIFS.2017.2766039&partnerID=40&md5=4b8740fb5fecf869e671e50d7e9f857d","Recent success in large-scale image recognition challenge (i.e. ImageNet) fully demonstrates the capability of Deep Neural Network (DNN) in learning complex and semantic representation, and this also motivates the generation of transfer learning model, which fine-tunes state-of-the-art DNN models with other small-scale database for better performance. Driven by such idea, task-specific DNN model fine-tuned from VGG-face is constructed for both gender and identity recognition with hand vein information. Unlike the traditional transfer learning models which fine-tune directly from source to target, we leverage the coarse-to-fine scheme to train the task-specific models in a step-aware way such that the inherent correlation between the neighboring databases could serve as initialization base to relieve the problem of over-fitting, which is inevitable with the small-scaled hand vein database, and also speed up the convergence. Besides, the task-driven network training idea, which involves joint optimization of linear regression classifier and network parameters, is also adopted during training of each model to obtain more discriminative representation for specified tasks. Instead of adopting the trained linear regression classifier for gender and identity classification, the large margin distribution machine (LDM) is introduced to ensure the discriminative and generalization performance of the model simultaneously, and it should be noted that before feeding the gender feature vector into the LDM, a supervised feature selection step is incorporated to improve the classification performance by discarding the redundant feature and highlighting the important ones for gender classification. Rigorous experiments using the lab-made database are conducted to demonstrate the effectiveness and feasibility of the proposed model. What is more, additional experiment with subset of PolyU database illustrates its generalization ability and robustness. IEEE","coarse-to-fine; Databases; Face; Feature extraction; gender classification; Hand vein information; LDM; personal identification; Robustness; supervised feature selection; task-driven; Training; transfer learning; Veins","Data mining; Database systems; Deep learning; Deep neural networks; Feature extraction; Image recognition; Knowledge management; Palmprint recognition; Personnel training; Robustness (control systems); Semantics; Social sciences; Coarse to fine; Face; Gender classification; Hand vein; Personal identification; Task-driven; Transfer learning; Veins; Classification (of information)",2-s2.0-85032450388
"Janalipour M., Mohammadzadeh A.","Evaluation of effectiveness of three fuzzy systems and three texture extraction methods for building damage detection from post-event LiDAR data",2017,"International Journal of Digital Earth",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031811397&doi=10.1080%2f17538947.2017.1387818&partnerID=40&md5=79e4841f8ff3b02a609e870c1199d8b9","Building damage maps after disasters can help us to better manage the rescue operations. Researchers have used Light Detection and Ranging (LiDAR) data for extracting the building damage maps. For producing building damage maps from LiDAR data in a rapid manner, it is necessary to understand the effectiveness of features and classifiers. However, there is no comprehensive study on the performance of features and classifiers in identifying damaged areas. In this study, the effectiveness of three texture extraction methods and three fuzzy systems for producing the building damage maps was investigated. In the proposed method, at first, a pre-processing stage was utilized to apply essential processes on post-event LiDAR data. Second, textural features were extracted from the pre-processed LiDAR data. Third, fuzzy inference systems were generated to make a relation between the extracted textural features of buildings and their damage extents. The proposed method was tested across three areas over the 2010 Haiti earthquake. Three building damage maps with overall accuracies of 75.0%, 78.1% and 61.4% were achieved. Based on outcomes, the fuzzy inference systems were stronger than random forest, bagging, boosting and support vector machine classifiers for detecting damaged buildings. © 2017 Informa UK Limited, trading as Taylor & Francis Group","ANFIS model; backpropagation learning; building damage detection; fuzzy system generation strategies; LiDAR; texture analysis",,2-s2.0-85031811397
"Mutter S., Casey A.E., Zhen S., Shi Z., Mäkinen V.-P.","Multivariable analysis of nutritional and socio-economic profiles shows differences in incident anemia for northern and southern Jiangsu in China",2017,"Nutrients",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032255648&doi=10.3390%2fnu9101153&partnerID=40&md5=0a3a14129dde57984a546589f98cf285","Anemia is a prevalent public health problem associated with nutritional and socio-economic factors that contribute to iron deficiency. To understand the complex interplay of risk factors, we investigated a prospective population sample from the Jiangsu province in China. At baseline, three-day food intake was measured for 2849 individuals (20 to 87 years of age, mean age 47 ± 14, range 20-87 years, 64% women). At a five-year follow-up, anemia status was re-assessed for 1262 individuals. The dataset was split and age-matched to accommodate cross-sectional (n = 2526), prospective (n = 837), and subgroup designs (n = 1844). We applied a machine learning framework (self-organizing map) to define four subgroups. The first two subgroups were primarily from the less affluent North: the High Fibre subgroup had a higher iron intake (35 vs. 21 mg/day) and lower anemia incidence (10% vs. 25%) compared to the Low Vegetable subgroup. However, the predominantly Southern subgroups were surprising: the Low Fibre subgroup showed a lower anemia incidence (10% vs. 27%), yet also a lower iron intake (20 vs. 28 mg/day) compared to the High Rice subgroup. These results suggest that interventions and iron intake guidelines should be tailored to regional, nutritional, and socio-economic subgroups. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Anemia; China; Geographical divide; Grains; Iron; Jiangsu; Rice; Subgroups; Subtypes; Wheat",,2-s2.0-85032255648
"Pajouh H.H., Dehghantanha A., Khayami R., Choo K.-K.R.","Intelligent OS X malware threat detection with code inspection",2017,"Journal of Computer Virology and Hacking Techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031910004&doi=10.1007%2fs11416-017-0307-5&partnerID=40&md5=0e8867727965f5ed3a1ecaf05fb8fee9","With the increasing market share of Mac OS X operating system, there is a corresponding increase in the number of malicious programs (malware) designed to exploit vulnerabilities on Mac OS X platforms. However, existing manual and heuristic OS X malware detection techniques are not capable of coping with such a high rate of malware. While machine learning techniques offer promising results in automated detection of Windows and Android malware, there have been limited efforts in extending them to OS X malware detection. In this paper, we propose a supervised machine learning model. The model applies kernel base Support Vector Machine and a novel weighting measure based on application library calls to detect OS X malware. For training and evaluating the model, a dataset with a combination of 152 malware and 450 benign were created. Using common supervised Machine Learning algorithm on the dataset, we obtain over 91% detection accuracy with 3.9% false alarm rate. We also utilize Synthetic Minority Over-sampling Technique (SMOTE) to create three synthetic datasets with different distributions based on the refined version of collected dataset to investigate impact of different sample sizes on accuracy of malware detection. Using SMOTE datasets we could achieve over 96% detection accuracy and false alarm of less than 4%. All malware classification experiments are tested using cross validation technique. Our results reflect that increasing sample size in synthetic datasets has direct positive effect on detection accuracy while increases false alarm rate in compare to the original dataset. © 2017 The Author(s)","Cyber threat intelligence; Mach-O; OS X malware detection; RBF–SVM; Supervised classification","Alarm systems; Artificial intelligence; Competition; Computer crime; Errors; Image resolution; Learning algorithms; Learning systems; Sampling; Supervised learning; Support vector machines; Cross-validation technique; Cyber threats; MAC Os X operating systems; Machine learning techniques; Malware detection; Supervised classification; Supervised machine learning; Synthetic minority over-sampling techniques; Malware",2-s2.0-85031910004
"Zhu X., Jing X., You X., Zuo W., Shan S., Zheng W.","Image to Video Person Re-identification by Learning Heterogeneous Dictionary Pair with Feature Projection Matrix",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032682505&doi=10.1109%2fTIFS.2017.2765524&partnerID=40&md5=d1a942a937ce92b129ee641ca3419eea","Person re-identification plays an important role in video surveillance and forensics applications. In many cases, person re-identification needs to be conducted between image and video clip, e.g., re-identifying a suspect from large quantities of pedestrian videos given a single image of the suspect. We call re-identification in this scenario as image to video person reidentification (IVPR). In practice, image and video are usually represented with different features, and there usually exist large variations between frames within each video. These factors make matching between image and video become a very challenging task. In this paper, we propose a joint feature projection matrix and heterogeneous dictionary pair learning (PHDL) approach for IVPR. Specifically, PHDL jointly learns an intra-video projection matrix and a pair of heterogeneous image and video dictionaries. With the learned projection matrix, the influence caused by variations within each video on the matching can be reduced. With the learned dictionary pair, the heterogeneous image and video features can be transformed into coding coefficients with the same dimension, such that the matching can be conducted by using the coding coefficients. Furthermore, to ensure that the obtained coding coefficients own favorable discriminability, PHDL designs a point-to-set coefficient discriminant term. To make better use of the complementary spatial-temporal and visual appearance information contained in pedestrian video data, we further propose a multi-view PHDL approach, which can fuse different video information effectively in the dictionary learning process. Experiments on four publicly available person sequence datasets demonstrate the effectiveness of the proposed approaches. IEEE","Dictionaries; Electronic mail; Feature extraction; feature projection matrix; heterogeneous dictionary pair learning; Image to video person re-identification; Legged locomotion; Machine learning; Measurement; multi-view learning; Person re-identification; Visualization","Codes (symbols); Electronic mail; Feature extraction; Flow visualization; Glossaries; Image coding; Learning systems; Measurements; Security systems; Visualization; Feature projection; heterogeneous dictionary pair learning; Legged locomotion; Multi-view learning; Person re identifications; Image processing",2-s2.0-85032682505
"Sun Y., Wang Y., Liu X., Yang C., Zhang Z., Gui W., Chen X., Zhu B.","A novel Bayesian inference soft sensor for real-time statistic learning modeling for industrial polypropylene melt index prediction",2017,"Journal of Applied Polymer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021013044&doi=10.1002%2fapp.45384&partnerID=40&md5=bc5a08bd5acb3bd39905b1c306d7fbce","A novel real-time soft sensor based on a sparse Bayesian probabilistic inference framework is proposed for the prediction of melt index in industrial polypropylene process. The Bayesian framework consists of a relevance vector machine for predicting melt index and a particle filtering algorithm for soft sensor optimization. An online correcting strategy is also developed for improving the performance of real-time melt index prediction. The method takes advantages of the probabilistic inference and using prior statistical knowledge of polymerization process. Developed soft sensors are validated with ten public databases from UCI machine learning repository and real data from industrial polypropylene process. Experimental results indicate the effectiveness of proposed method and show the improvement in both prediction precision and generalization capability compared with the reported models in literatures. © 2017 Wiley Periodicals, Inc. J. Appl. Polym. Sci. 2017, 134, 45384. © 2017 Wiley Periodicals, Inc.","sensors and actuators; synthesis and processing techniques; theory and modeling","Bayesian networks; Education; Forecasting; Inference engines; Signal filtering and prediction; Generalization capability; Melt index predictions; Polymerization process; Polypropylene process; Probabilistic inference; Relevance Vector Machine; Statistical knowledge; UCI machine learning repository; Polypropylenes",2-s2.0-85021013044
"Xie X.","Improvement on projection twin support vector machine",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031910137&doi=10.1007%2fs00521-017-3237-8&partnerID=40&md5=b09b6443e988ab4c0a950d44bb1f9cfc","Traditional projection twin support vector machines (SVMs) ignore the differences between the categories when establishing the objective functions, which would lessen their generalization performance. To solve the issue, an improved projection twin SVM (abbreviated as IPTSVM) is proposed in this paper, which aims to find two projected directions via a single quadratic programming problem. In their respective subspace, the projected sample points belonging to each category are far from those of the other class. Meanwhile, to enhance the performance, the recursive arithmetic seeks for more than one projection directions for each class. Besides, an effective clipping dual coordinate descent model is adopted to solve the dual problem to accelerate the training process. The linear IPTSVM model could be changed into the nonlinear model by using the kernel metric. Furthermore, the multi-label version of IPTSVM model is developed to deal with the multi-label learning problems. Experiments on a set of public datasets show that the IPTSVM model has significant advantages over the other models in terms of generalization performance. © 2017 The Natural Computing Applications Forum","Clipping DCD; Multi-label learning; Projected direction; Support vector machine","Learning systems; Problem solving; Quadratic programming; Clipping DCD; Generalization performance; Multi-label learning; Objective functions; Projected direction; Projection direction; Quadratic programming problems; Twin support vector machines; Support vector machines",2-s2.0-85031910137
"Lee D., Kang S., Shin J.","Using deep learning techniques to forecast environmental consumption level",2017,"Sustainability (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032859366&doi=10.3390%2fsu9101894&partnerID=40&md5=e0bb0c6ced743af686815cba5524cc61","Artificial intelligence is a promising futuristic concept in the field of science and technology, and is widely used in new industries. The deep-learning technology leads to performance enhancement and generalization of artificial intelligence technology. The global leader in the field of information technology has declared its intention to utilize the deep-learning technology to solve environmental problems such as climate change, but few environmental applications have so far been developed. This study uses deep-learning technologies in the environmental field to predict the status of pro-environmental consumption. We predicted the proenvironmental consumption index based on Google search query data, using a recurrent neural network (RNN) model. To verify the accuracy of the index, we compared the prediction accuracy of the RNN model with that of the ordinary least square and artificial neural network models. The RNN model predicts the pro-environmental consumption index better than any other model. We expect the RNN model to perform still better in a big data environment because the deep-learning technologies would be increasingly sophisticated as the volume of data grows. Moreover, the framework of this study could be useful in environmental forecasting to prevent damage caused by climate change. © 2017 by the authors.","Artificial intelligence; Artificial neural network; Consumption index; Deep-learning technology; Pro-environment","accuracy assessment; artificial intelligence; artificial neural network; environmental issue; environmentalism; index method; information technology; Internet; machine learning; prediction; resource use",2-s2.0-85032859366
"Silva G.R., Medeiros R.R., Jaimes B.R.A., Takahashi C.C., Vieira D.A., Braga A.P.","CUDA-based parallelization of Power Iteration Clustering for Large Datasets",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032699929&doi=10.1109%2fACCESS.2017.2765380&partnerID=40&md5=b20eb86262a34e5f00f2774a79472fca","This work presents a new clustering algorithm, the GPIC, a Graphics Processing Unit (GPU) accelerated algorithm for Power Iteration Clustering (PIC). Our algorithm is based on the original PIC proposal, adapted to take advantage of the GPU architecture, maintaining the algorithm&#x2019;s original properties. The proposed method was compared against the serial implementation, achieving a considerable speedup in tests with synthetic and real datasets. A significant volume of real data application (&#x003E;10 7 records) was used, and we identified that GPIC implementation has good scalability to handle datasets with millions of data points. Our implementation efforts are directed towards two aspects: to process large datasets in less time and to maintain the same quality of the clusters results generated by the original PIC version. OAPA","Clustering algorithms; Clustering methods; Eigenvalues and eigenfunctions; GPU; Graphics processing units; Instruction sets; Kernel; Power Iteration Clustering; Scalable Machine Learning Algorithms; Symmetric matrices","Cluster analysis; Computer graphics; Computer graphics equipment; Eigenvalues and eigenfunctions; Graphics processing unit; Image coding; Iterative methods; Learning algorithms; Learning systems; Program processors; Clustering methods; Instruction set; Kernel; Power Iteration Clustering; Scalable machine learning; Symmetric matrices; Clustering algorithms",2-s2.0-85032699929
"Zhao W., Xu L., Bai J., Ji M., Runge T.","Sensor-based risk perception ability network design for drivers in snow and ice environmental freeway: a deep learning and rough sets approach",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031893547&doi=10.1007%2fs00500-017-2850-x&partnerID=40&md5=c340b34047429c1f7f0b2faf7dec0e3e","Due to factors such as snow and ice impeding drivers’ vision, the number of automobile crashes significantly rises during winter months. This study sets forth an automatic evaluation network of the risk perceived ability for motorists driving on the freeway in snow and ice environments, using a deep learning approach and the rough sets technique. First, a naturalistic driving experiment involving thirteen licensed drivers was conducted on a freeway in Jilin, China, with a crash hot spot set prior to the start of the experiment. Then multi-sensor (eye-trackers, mini-cameras, and speed detectors) apparatuses, collecting both images and numerical data, were utilized. Afterward, restricted Boltzmann machine was used to develop a deep belief network (DBN) along with training procedures. Rough sets technique was added as judgment in output layer of the DBN. Finally, fixation duration, pupil size, changes in speed, etc., were used as input impact factors and the perception conditions were used as output variables to train the network. Furthermore, after comparing the DBN-based risk perception ability network with Naïve Bayes and BP-ANN (artificial neural networks with back propagations), the results indicate that the DBN-FS not only outperforms both Naïve Bayes and BP-ANN, but also improves the accuracy of perceiving risky conditions. This approach can provide reference for the design of hazard detection systems of partially automated vehicles. © 2017 Springer-Verlag GmbH Germany","DBN; Deep learning; Freeway curves; Fuzzy sets; Risk perception ability","Accidents; Automobile drivers; Backpropagation; Deep learning; Fuzzy sets; Highway accidents; Ice; Neural networks; Rough set theory; Snow; Sodium; Automated vehicles; Automatic evaluation; Deep belief network (DBN); Fixation duration; Learning approach; Output variables; Restricted boltzmann machine; Training procedures; Risk perception",2-s2.0-85031893547
"Moschetti A., Fiorini L., Esposito D., Dario P., Cavallo F.","Toward an Unsupervised Approach for Daily Gesture Recognition in Assisted Living Applications",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032670794&doi=10.1109%2fJSEN.2017.2764323&partnerID=40&md5=7704e2dd63af61d535fcc47e24f51571","Activity Recognition is important in assisted living applications to monitor people at home. Over the past, inertial sensors have been used to recognize different activities, spanning from physical activities to eating ones. Over the last years, supervised methods have been widely used, but they require an extensive labeled dataset to train the algorithms and this may represent a limitation of concrete approaches. This paper presents a comparison of unsupervised and supervised methods in recognizing nine gestures by means of two inertial sensors placed on the index finger and on the wrist. Three supervised classification techniques, namely Random Forest, Support Vector Machine, and Multilayer Perceptron, as well as three unsupervised classification techniques, namely k-Means, Hierarchical Clustering, and Self-Organized Maps, were compared in the recognition of gestures made by 20 subjects. The obtained results show that the Support Vector Machine classifier provided the best performances (0.94 accuracy) compared to the other supervised algorithms. However, the outcomes show that even in an unsupervised context, the system is able to recognize the gestures with an average accuracy of &#x007E;0.81. The proposed system may be therefore involved in future telecare services that could monitor the activities of daily living, allowing an unsupervised approach that does not require labelled data. OAPA","Accelerometers; Accelerometers; Gesture Recognition; Legged locomotion; Sensors; Support vector machines; Thumb; Unsupervised Analysis; Wearable Sensors; Wrist","Accelerometers; Assisted living; Decision trees; Inertial navigation systems; Sensors; Supervised learning; Support vector machines; Wearable sensors; Activities of Daily Living; Legged locomotion; Supervised classification; Support vector machine classifiers; Thumb; Unsupervised analysis; Unsupervised classification; Wrist; Gesture recognition",2-s2.0-85032670794
"Sun M., Sun H.","Improved proximal ADMM with partially parallel splitting for multi-block separable convex programming",2017,"Journal of Applied Mathematics and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031814650&doi=10.1007%2fs12190-017-1138-8&partnerID=40&md5=f56011b3dc45f9186327962a03c3337c","For a type of multi-block separable convex programming raised in machine learning and statistical inference, we propose a proximal alternating direction method of multiplier with partially parallel splitting, which has the following nice properties: (1) to alleviate the weight of the proximal terms, the restrictions imposed on the proximal parameters are relaxed substantively; (2) to maintain the inherent structure of the primal variables (Formula presented.), the relaxation parameter (Formula presented.) is only attached to the update formula of the dual variable (Formula presented.). For the resulted method, we establish its global convergence and worst-case (Formula presented.) convergence rate in an ergodic sense, where t is the iteration counter. Finally, three numerical examples are given to illustrate the theoretical results obtained. © 2017 Korean Society for Computational and Applied Mathematics","Alternating direction method of multipliers; Global convergence; Multi-block separable convex programming","Convex optimization; Learning systems; Alternating direction method of multipliers; Convergence rates; Global conver-gence; Iteration counters; Multi blocks; Parallel splitting; Relaxation parameter; Statistical inference; Iterative methods",2-s2.0-85031814650
"Zhukovskiy Y., Koteleva N.","Method of Data storing, collection and aggregation for definition of life-cycle resources of electromechanical equipment",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032434390&doi=10.1088%2f1755-1315%2f87%2f3%2f032057&partnerID=40&md5=17e5f1685964567b2163d0dd2fe73277","Analysis of technical and technological conditions for the emergence of emergency situations during the operation of electromechanical equipment of enterprises of the mineral and raw materials complex shows that when developing the basis for ensuring safe operation, it is necessary to take into account not only the technical condition, but also the non-stationary operation of the operating conditions of equipment, and the nonstationarity of operational operating parameters of technological processes. Violations of the operation of individual parts of the machine, not detected in time, can lead to severe accidents at work, as well as to unplanned downtime and loss of profits. That is why, the issues of obtaining and processing Big data obtained during the life cycle of electromechanical equipment, for assessing the current state of the electromechanical equipment used, timely diagnostics of emergency and pre-emergency modes of its operation, estimating the residual resource, as well as prediction the technical state on the basis of machine learning are very important. This article is dedicated to developing the special method of data storing, collection and aggregation for definition of life-cycle resources of electromechanical equipment. This method can be used in working with big data and can allow extracting the knowledge from different data types: the plants' historical data and the factory historical data. The data of the plants contains the information about electromechanical equipment operation and the data of the factory contains the information about a production of electromechanical equipment. © Published under licence by IOP Publishing Ltd.",,"Data acquisition; Data handling; Data mining; Electric power systems; Equipment; Learning systems; Life cycle; Mining machinery; Electromechanical equipments; Emergency situation; Mineral and raw materials complexes; Operating condition; Operating parameters; Technical conditions; Technological conditions; Technological process; Big data",2-s2.0-85032434390
"Pardoe H.R., Kuzniecky R.","NAPR: a Cloud-Based Framework for Neuroanatomical Age Prediction",2017,"Neuroinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031939622&doi=10.1007%2fs12021-017-9346-9&partnerID=40&md5=2b37881c3f891f72df2d561178f11cde","The availability of cloud computing services has enabled the widespread adoption of the “software as a service” (SaaS) approach for software distribution, which utilizes network-based access to applications running on centralized servers. In this paper we apply the SaaS approach to neuroimaging-based age prediction. Our system, named “NAPR” (Neuroanatomical Age Prediction using R), provides access to predictive modeling software running on a persistent cloud-based Amazon Web Services (AWS) compute instance. The NAPR framework allows external users to estimate the age of individual subjects using cortical thickness maps derived from their own locally processed T1-weighted whole brain MRI scans. As a demonstration of the NAPR approach, we have developed two age prediction models that were trained using healthy control data from the ABIDE, CoRR, DLBS and NKI Rockland neuroimaging datasets (total N = 2367, age range 6–89 years). The provided age prediction models were trained using (i) relevance vector machines and (ii) Gaussian processes machine learning methods applied to cortical thickness surfaces obtained using Freesurfer v5.3. We believe that this transparent approach to out-of-sample evaluation and comparison of neuroimaging age prediction models will facilitate the development of improved age prediction models and allow for robust evaluation of the clinical utility of these methods. © 2017 Springer Science+Business Media, LLC","Age prediction; Cloud computing; Morphometry; Software as a service",,2-s2.0-85031939622
"Timoshenko J., Lu D., Lin Y., Frenkel A.I.","Supervised Machine-Learning-Based Determination of Three-Dimensional Structure of Metallic Nanoparticles",2017,"Journal of Physical Chemistry Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031802014&doi=10.1021%2facs.jpclett.7b02364&partnerID=40&md5=a723daf5b8a75ddd2796e033d51f9484","Tracking the structure of heterogeneous catalysts under operando conditions remains a challenge due to the paucity of experimental techniques that can provide atomic-level information for catalytic metal species. Here we report on the use of X-ray absorption near-edge structure (XANES) spectroscopy and supervised machine learning (SML) for refining the 3D geometry of metal catalysts. SML is used to unravel the hidden relationship between the XANES features and catalyst geometry. To train our SML method, we rely on ab initio XANES simulations. Our approach allows one to solve the structure of a metal catalyst from its experimental XANES, as demonstrated here by reconstructing the average size, shape, and morphology of well-defined platinum nanoparticles. This method is applicable to the determination of the nanoparticle structure in operando studies and can be generalized to other nanoscale systems. It also allows on-the-fly XANES analysis and is a promising approach for high-throughput and time-dependent studies. © 2017 American Chemical Society.",,"Artificial intelligence; Catalysts; Learning systems; Metal refining; Metals; Nanoparticles; Supervised learning; X ray absorption; X ray absorption near edge structure spectroscopy; Experimental techniques; Heterogeneous catalyst; In-operando studies; Metallic nanoparticles; Nanoparticle structures; Platinum nano-particles; Supervised machine learning; Three-dimensional structure; Metal nanoparticles",2-s2.0-85031802014
"Li G.-Q., Qi X.-B., Chan K.C.C., Chen B.","Deep Bidirectional Learning Machine for Predicting NOx Emissions and Boiler Efficiency from a Coal-Fired Boiler",2017,"Energy and Fuels",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031937949&doi=10.1021%2facs.energyfuels.7b01415&partnerID=40&md5=6320ac9a358bdd35e17162757535037f","Combustion optimization is one of the effective techniques to enhance boiler efficiency and reduce nitrogen oxide (NOx) emissions from coal-fired boilers. A precise NOx emission model and a boiler efficiency model are the basis of implementing real-time combustion optimization and are required. In this study, to obtain very precise models and make full use of abundant real-time operational data easily collected from supervisory information systems (SIS), a novel deep learning algorithm called a deep bidirectional learning machine (DBLM) is proposed to set up the correlation between NOx emissions, boiler efficiency, and operational parameters from a 300 MW circulating fluidized bed boiler (CFBB). Experimental results indicate that, in comparison to other recently published state-of-the-art modeling methods, the models built by DBLM could own much better generalization performance and high repeatability, which may be a better choice for modeling NOx emissions and efficiency in achieving boiler combustion optimization and improving power plant performance. © 2017 American Chemical Society.",,"Boilers; Coal combustion; Combustion; Efficiency; Fluidized bed combustion; Fluidized bed process; Fluidized beds; Fossil fuel power plants; Gas emissions; Learning algorithms; Learning systems; Nitrogen oxides; Pulverized fuel fired boilers; Real time systems; Boiler combustion optimization; Circulating fluidized bed boiler; Combustion optimization; Generalization performance; Learning machines; Operational parameters; Real time operational data; Supervisory information system; Coal fired boilers",2-s2.0-85031937949
"Wang S., Ding Z., Fu Y.","Marginalized Denoising Dictionary Learning with Locality Constraint",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032680570&doi=10.1109%2fTIP.2017.2764622&partnerID=40&md5=460c1a9d5fa6e11318e48685104b0889","Learning good representation for images is always a hot topic in machine learning and pattern recognition fields. Among the numerous algorithms, dictionary learning is a well-known strategy for effective feature extraction. Recently, more discriminative sub-dictionaries have been built by Fisher discriminative dictionary learning with specific class labels. Different types of constraints, such as sparsity, low-rankness and locality, are also exploited to make use of global and local information. On the other hand, as the basic building block of deep structure, the auto-encoder has demonstrated its promising performance in extracting new feature representation. To this end, we develop a unified feature learning framework by incorporating the marginalized denoising auto-encoder into a localityconstrained dictionary learning scheme, named Marginalized Denoising Dictionary Learning (MDDL). Overall, we deploy lowrank constraint on each sub-dictionary and locality constraint instead of sparsity on coefficients, in order to learn a more concise and pure feature spaces meanwhile inheriting the discrimination from sub-dictionary learning. Finally, we evaluate our algorithm on several face and object datasets. Experimental results have demonstrated the effectiveness and efficiency of our proposed algorithm by comparing with several state-of-the-art methods. IEEE","Dictionaries; dictionary learning; Encoding; Feature extraction; locality constraint; Machine learning; marginalized denoising auto-encoder; Noise measurement; Noise reduction; Training","Artificial intelligence; Encoding (symbols); Extraction; Feature extraction; Glossaries; Noise abatement; Pattern recognition; Personnel training; Signal encoding; Auto encoders; Dictionary learning; Discriminative dictionaries; Effectiveness and efficiencies; Global and local informations; locality constraint; Noise measurements; State-of-the-art methods; Learning systems",2-s2.0-85032680570
"Xiao S., Yu H., Wu Y., Peng Z., Zhang Y.","Self-Evolving Trading Strategy Integrating Internet of Things and Big Data",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032661703&doi=10.1109%2fJIOT.2017.2764957&partnerID=40&md5=2606382180fe2ee12eabaef8b0d08177","In the era of Internet of things and big data, data has increased dramatically. Computers have been used in various fields. Algorithmic trading is beginning to develop rapidly in the trading market, more and more algorithms begin to be used in the transaction market. As a form of machine learning, neural network can fully reveal the complex trading market. Based on the characteristics of commodity futures market, this paper chooses BP neural network to establish price forecasting model. And then, according to the rules of futures market, a self-evolving commodity futures trading strategy is proposed. We also use the data of the Shanghai Futures Exchange and the Dalian Futures Exchange to back-testing the strategy. Finally, we compare the proposed strategies and traditional strategies, and illustrate the evolution of our strategy. Experiments show that our strategies are superior to other compared strategies in the proposed evaluation indicator. Our strategy has a good performance both in yield and risk. It also proves the feasibility of the model and the strategy. The research of this paper is significant to the research of the futures market, and it also provides a new idea for the application of machine learning in algorithmic trading. IEEE","Algorithm design and analysis; Big Data; Big Data.; futures market; Hidden Markov models; Internet of Things; Internet of Things; Machine learning algorithms; neural network; Neural networks; Predictive models; trading strategy","Artificial intelligence; Big data; Commerce; Financial markets; Hidden Markov models; Internet of things; Learning algorithms; Learning systems; Markov processes; Neural networks; Algorithm design and analysis; Algorithmic trading; BP neural networks; Commodity futures; Evaluation indicators; Predictive models; Price forecasting; Trading strategies; Electronic trading",2-s2.0-85032661703
"Bonin-Font F., Burguera A., Lisani J.","Visual Discrimination and Large Area Mapping of Posidonia Oceanica Using a Lightweight AUV",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032696042&doi=10.1109%2fACCESS.2017.2764998&partnerID=40&md5=24b4f00f86c4ae64ba1da9624b01ca6f","Controlling and quantifying the presence of Posidonia Oceanica (P.O.) in the Mediterranean sea is crucial for the conservation of these endemic ecosystems and to underscore the negative impact of many anthropogenic activities. These activities, which include uncontrolled leisure anchoring or illegal drag fishing, directly affect the tourism and fishing industries. Nowadays, the control and quantification of P.O. is done by divers, in a slow and imprecise process achieved in successive missions of a duration limited by the capacity of the oxygen scuba tanks. This paper proposes the application of robotic and computer vision technologies to upgrade the current P.O. control methods, building large scale coverage maps using the imagery provided by an AUV endowed with a bottom-looking camera. The process includes four main steps: a) training a classifier based on two different Gabor filter image patch descriptors and a Support Vector Machine (SVM); b) detecting P.O. autonomously, both on-line and off-line, in each individual image; c) color photomosaicking the area explored by the vehicle to obtain a global view of the meadow structure; these mosaics are extremely useful to analyze the structure and extension of the meadow and to calculate some of the biological descriptors needed to diagnose its state, and d) building a binary coverage map in which the classification results of areas with image overlap are fused according to four different strategies. The experiments, performed in coastal areas of Mallorca and Girona, evaluate and compare the proposed descriptors and fusion techniques, showing, in some cases, accuracies and precisions above 90&#x0025; in the detection of different patterns of P.O., from video sequences at different locations, in different seasons and with different environmental conditions. OAPA","Autonomous Underwater Vehicles; Biology; Cameras; Gabor Filters; Image color analysis; Indexes; Machine Learning; Photo-Mosaicing; Posidonia Oceanica; Sea measurements; Two dimensional displays; Visualization","Biology; Cameras; Computer vision; Fisheries; Flow visualization; Gabor filters; Image analysis; Learning systems; Pattern recognition; Support vector machines; Image color analysis; Indexes; Mosaicing; Posidonia oceanica; Sea measurements; Two-dimensional displays; Autonomous underwater vehicles",2-s2.0-85032696042
"Atas M.","Hand Tremor Based Biometric Recognition Using Leap Motion Device",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032658809&doi=10.1109%2fACCESS.2017.2764471&partnerID=40&md5=f93c9bb2f747604829cca83dfc7153d6","In this study, applicability of hand tremor based biometric recognition via leap motion device is investigated. The hypothesis is that the hand tremor is unique for humans and can be utilized as a biometric identification. In order to verify our hypothesis, spatiotemporal hand tremor signals are acquired from subjects. The objective is to establish a live and secure identification system to avoid mimic and cloning of password by attackers. Various feature extraction methods including statistical, fast Fourier transform, discrete wavelet transform, and 1D local binary pattern are used. For evaluating recognition performance, Na&#x00EF;ve Bayes and Multi-Layer Perceptron are utilized as linear-simple and nonlinear-complex classifiers, respectively. Since the conducted experiments produced promising results (above 95% of classification accuracy rate), it is considered that the proposed approach has the potential to be used as a new biometric identification manner in the field of security. OAPA","Authentication; Authentication; Biometrics; Biometrics (access control); Data acquisition; Discrete wavelet transforms; Feature extraction; Hand Tremors; Human Computer Interaction; Leap Motion; Machine Learning; Neurophysiology; Neuroscience; Object recognition; Recognition; Security; Software","Access control; Anthropometry; Authentication; Biometrics; Computer software; Data acquisition; Discrete wavelet transforms; Extraction; Fast Fourier transforms; Feature extraction; Human computer interaction; Learning systems; Neurophysiology; Object recognition; Signal reconstruction; Wavelet transforms; Hand tremors; Leap Motion; Neuroscience; Recognition; Security; Palmprint recognition",2-s2.0-85032658809
"Geissmann Q., Garcia Rodriguez L., Beckwith E.J., French A.S., Jamasb A.R., Gilestro G.F.","Ethoscopes: An open platform for high-throughput ethomics",2017,"PLoS Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032162639&doi=10.1371%2fjournal.pbio.2003026&partnerID=40&md5=d09d76fae3992c1693605c873dee4188","Here, we present the use of ethoscopes, which are machines for high-throughput analysis of behavior in Drosophila and other animals. Ethoscopes provide a software and hardware solution that is reproducible and easily scalable. They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source. Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware. All software and construction specifications are available at http://lab.gilest.ro/ethoscope. © 2017 Geissmann et al.",,"algorithm; animal; animal behavior; devices; Drosophila melanogaster; ethology; machine learning; microcomputer; physiology; procedures; reproducibility; software; three dimensional printing; Algorithms; Animals; Behavior, Animal; Drosophila melanogaster; Ethology; Machine Learning; Microcomputers; Printing, Three-Dimensional; Reproducibility of Results; Software",2-s2.0-85032162639
"Ramírez-Morales I., Fernández-Blanco E., Rivero D., Pazos A.","Automated early detection of drops in commercial egg production using neural networks",2017,"British Poultry Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031507902&doi=10.1080%2f00071668.2017.1379051&partnerID=40&md5=62ca7a42356b6e19277e0667a8a71c5b","1. The purpose of this work was to support decision-making in poultry farms by performing automatic early detection of anomalies in egg production. 2. Unprocessed data were collected from a commercial egg farm on a daily basis over 7 years. Records from a total of 24 flocks, each with approximately 20 000 laying hens, were studied. 3. Other similar works have required a prior feature extraction by a poultry expert, and this method is dependent on time and expert knowledge. 4. The present approach reduces the dependency on time and expert knowledge because of the automatic selection of relevant features and the use of artificial neural networks capable of cost-sensitive learning. 5. The optimum configuration of features and parameters in the proposed model was evaluated on unseen test data obtained by a repeated cross-validation technique. 6. The accuracy, sensitivity, specificity and positive predictive value are presented and discussed at 5 forecasting intervals. The accuracy of the proposed model was 0.9896 for the day before a problem occurs. © 2017 British Poultry Science Ltd","Farming systems; laying hens; machine learning; modelling; production drops",,2-s2.0-85031507902
"Ma Y., Wu H., Zhu M., Ren P., Zheng N., Chen B.","Reconstruction of Visual Image from Functional Magnetic Resonance Imaging Using Spiking Neuron Model",2017,"IEEE Transactions on Cognitive and Developmental Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032711966&doi=10.1109%2fTCDS.2017.2764948&partnerID=40&md5=6b91703ad2314bf157a3054c80b77b9f","Spikes are the basic elements of information dissemination in the organic brain. A spike train, a series of discrete action potentials, incorporates the conception of time. Nowadays, the majority of existing artificial neural networks use numerical information to realize machine learning and cognition tasks. The advantage of numerical computation is its precision and remarkable performance on well-structured problems. However, cognition tasks have semi-structured or ill-structured information, even worse, some cannot reach a conclusion. According to biological experiments, the spiking neuron model is potentially more suitable for dealing with undetermined and unstructured problems. Researchers are trying to decode functional magnetic resonance imaging (fMRI) data, and this is a typical task to extract meaningful information from relatively undetermined and unstructured data. In this paper, we selected the Tempotron neuron model to analyze and reconstruct visual images from fMRI data when subjects received several kinds of visual stimuli; the preliminary results of pattern reconstruction were unpolished, but somewhat effective. Thus, a new structure for the spiking neural network was built to achieve better results. Several important aspects of the proposed model were discussed in this paper: the encoding of external stimulating signals, the extraction of effective features, the relative position of spike trains on the timeline, the back propagation of error, and the rationality of parameter selection. These aspects are crucial in the spiking neuron model&#x2019;s implementation, and are worth further investigation in future studies. IEEE","Biological neural networks; Biological system modeling; Computational modeling; Functional magnetic resonance imaging; functional magnetic resonance imaging (fMRI); Neurons; postsynaptic potential (PSP).; Spiking neuron model; synapse; Synapses; Tempotron; Visualization","Backpropagation; Biological systems; Cognitive systems; Electrophysiology; Flow visualization; Image processing; Image reconstruction; Information dissemination; Learning algorithms; Learning systems; Magnetic levitation vehicles; Magnetic resonance imaging; Magnetism; Neural networks; Neurons; Resonance; Biological neural networks; Biological system modeling; Computational model; Functional magnetic resonance imaging; Post-synaptic potentials; Spiking neuron models; synapse; Synapses; Tempotron; Functional neuroimaging",2-s2.0-85032711966
"Saxena S., Pandey S., Khanna P.","A semi-supervised domain adaptation assembling approach for image classification",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031821542&doi=10.1007%2fs10044-017-0664-1&partnerID=40&md5=703f54cad60d4d0b0017a31ea5230b13","Automatic annotation of images is one of the fundamental problems in computer vision applications. With the increasing amount of freely available images, it is quite possible that the training data used to learn a classifier has different distribution from the data which is used for testing. This results in degradation of the classifier performance and highlights the problem known as domain adaptation. Framework for domain adaptation typically requires a classification model which can utilize several classifiers by combining their results to get the desired accuracy. This work proposes depth-based and iterative depth-based fusion methods which are basically rank-based fusion methods and utilize rank of the predicted labels from different classifiers. Two frameworks are also proposed for domain adaptation. The first framework uses traditional machine learning algorithms, while the other works with metric learning as well as transfer learning algorithm. Motivated from ImageCLEF’s 2014 domain adaptation task, these frameworks with the proposed fusion methods are validated and verified by conducting experiments on the images from five domains having varied distributions. Bing, Caltech, ImageNet, and PASCAL are used as source domains and the target domain is SUN. Twelve object categories are chosen from these domains. The experimental results show the performance improvement not only over the baseline system, but also over the winner of the ImageCLEF’s 2014 domain adaptation challenge. © 2017 Springer-Verlag London Ltd.","Domain adaptation; Domain adaptation frameworks; Fusion methods; Image classification",,2-s2.0-85031821542
"Espejel-Trujillo A., Iwamoto M., Nakano-Miyatake M.","A proactive secret image sharing scheme with resistance to machine learning based steganalysis",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031494966&doi=10.1007%2fs11042-017-5097-8&partnerID=40&md5=76c955ef75eed47bc9b5a395ce22cc77","In secret image sharing (SIS) schemes, a secret image is shared among a set of n images called stego-images. Each stego-image is preserved by a participant. In the recovery stage, at least k out of n stego-images are required to obtain the secret image, while k − 1 cannot reveal the secret in the sense of perfect secrecy. Hence, SIS guarantees long-term security. However, as the longer the stego-images remain stored, the higher is the probability of being vulnerable against steganalysis. To resolve this issue, this paper proposes the use of proactive secret sharing in an SIS scheme (P-SIS). P-SIS allows the stego-images to be renewed frequently while these are stored, without changing both cover and secret images. However, direct implementation of a proactive SIS requires more embedding rate (ER), causing high steganalysis accuracy detection and loss of quality in the stego-images. Our proposal addresses this issue and presents the combination of a (k, L, n)-threshold ramp secret sharing scheme and least significant bit matching (LSBM) steganography to reduce the steganalysis accuracy detection. The results of the evaluation show effectiveness of the proposal in terms of good quality of the stego-images, accurate recovery of the secret, and reduce the ER. Note that, despite the extensive research of SIS presented until now, only a few previous work is found on steganalysis in SIS. Not only constructing P-SIS scheme, but we also experimented the tolerance of the proposed P-SIS scheme against stganalysis in this paper. As a result, it is shown that the proposed scheme can withstand steganalysis based on machine learning (i.e., based on subtractive pixel adjacency matrix, SPAM). © 2017 Springer Science+Business Media, LLC","Machine learning; Proactive secret sharing scheme; Ramp secret sharing scheme; Secret image sharing scheme; Steganalysis; Support vector machine","Artificial intelligence; Electric resistance; Learning systems; Steganography; Support vector machines; Adjacency matrices; Least significant bit matching; Perfect secrecy; Proactive secret sharing; Proactive Secret Sharing Schemes; Secret image sharing; Secret sharing schemes; Steganalysis; Image processing",2-s2.0-85031494966
"Luo F., Guo W., Yu Y., Chen G.","A multi-label classification algorithm based on kernel extreme learning machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019937554&doi=10.1016%2fj.neucom.2017.04.052&partnerID=40&md5=6a3c4705d2949f1dd164721a991b4490","Multi-label classification learning provides a multi-dimensional perspective for polysemic object, and becomes a new research hotspot in machine learning in recent years. In the big data environment, it is urgent to obtain a fast and efficient multi-label classification algorithm. Kernel extreme learning machine was applied to multi-label classification problem (ML-KELM) in this paper, so the iterative learning operations can be avoided. Meanwhile, a dynamic, self-adaptive threshold function was designed to solve the transformation from ML-KELM network's real-value outputs to binary multi-label vector. ML-KELM has the least square optimal solution of ELM, and less parameters that needs adjustment, stable running, faster convergence speed and better generalization performance. Extensive multi-label classification experiments were conducted on data sets of different scale. Comparison results show that ML-KELM outperformance in large scale dataset with high dimension instance feature. © 2017 Elsevier B.V.","Extreme learning machine; Kernel extreme learning machine; Multi-label learning; Threshold selection","Big data; Iterative methods; Knowledge acquisition; Learning algorithms; Learning systems; Neural networks; Adaptive thresholds; Extreme learning machine; Generalization performance; Iterative learning; Large-scale dataset; Multi label classification; Multi-label learning; Threshold selection; Classification (of information); accuracy; Article; calculation; classification algorithm; kernel method; linear system; machine learning; mathematical analysis; mathematical model; nonlinear system; prediction; priority journal; probability; sensitivity analysis",2-s2.0-85019937554
"Wan Y., Song S., Huang G., Li S.","Twin extreme learning machines for pattern classification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018265569&doi=10.1016%2fj.neucom.2017.04.036&partnerID=40&md5=d5296a5ff86b65fdf8e5f98cd18a3a64","Extreme learning machine (ELM) is an efficient and effective learning algorithm for pattern classification. For binary classification problem, traditional ELM learns only one hyperplane to separate different classes in the feature space. In this paper, we propose a novel twin extreme learning machine (TELM) to simultaneously train two ELMs with two nonparallel classification hyperplanes. Specifically, TELM first utilizes the random feature mapping mechanism to construct the feature space, and then two nonparallel separating hyperplanes are learned for the final classification. For each hyperplane, TELM jointly minimizes its distance to one class and requires it to be far away from the other class. TELM incorporates the idea of twin support vector machine (TSVM) into the basic framework of ELM, thus TELM could have the advantages of the both algorithms. Moreover, compared to TSVM, TELM has fewer optimization constraint variables but with better classification performance. We also introduce a successive over-relaxation technique to speed up the training of our algorithm. Comprehensive experimental results on a large number of datasets verify the effectiveness and efficiency of TELM. © 2017","Extreme learning machine; Nonparallel separating hyperplane; Pattern classification; Twin extreme learning machine; Twin support vector machine","Geometry; Knowledge acquisition; Learning algorithms; Optimization; Pattern recognition; Support vector machines; Binary classification problems; Classification performance; Effective learning; Effectiveness and efficiencies; Extreme learning machine; Separating hyperplane; Successive over relaxation; Twin support vector machines; Learning systems; algorithm; Article; controlled study; extreme learning machine; intermethod comparison; kernel twin extreme learning machine; linear twin extreme learning machine; priority journal; support vector machine; twin support vector machine",2-s2.0-85018265569
"Nguyen T.V., Mirza B.","Dual-layer kernel extreme learning machine for action recognition",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017564311&doi=10.1016%2fj.neucom.2017.04.007&partnerID=40&md5=a554e6b974240a4edf278b5636ebfd13","In this paper, we propose a simple yet effective method for video based action recognition referred to as dual-layer kernel extreme learning machine (DKELM). Our approach takes advantages of both early and late fusion techniques into a unified framework. In particular, the first layer in DKELM adopts linear kernel extreme learning machine (KELM) on handcrafted feature kernel, deep-learned feature kernel, and the fused kernel to provide various perspectives about the video. The second layer trains a radial basis function based KELM classifier on different fusion scores obtained from the first layer to predict the final action class label. Finally, we empirically show the superior performance of DKELM, both in terms of accuracy and computational time, over some state-of-the-art human action recognition methods on two large-scale datasets. © 2017","Action recognition; Dual-layer kernel learning; Extreme learning machine","Knowledge acquisition; Neural networks; Radial basis function networks; Action recognition; Computational time; Extreme learning machine; Human-action recognition; Kernel learning; Large-scale datasets; Radial basis functions; Unified framework; Learning systems; Article; brain function; classifier; dual-layer kernel extreme learning machine; extreme learning machine; human; machine learning; priority journal; support vector machine",2-s2.0-85017564311
"Mott A., Job J., Vlimant J.-R., Lidar D., Spiropulu M.","Solving a Higgs optimization problem with quantum annealing for machine learning",2017,"Nature",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919241&doi=10.1038%2fnature24047&partnerID=40&md5=eb29e097892861df51693fbfc6f9d989","The discovery of Higgs-boson decays in a background of standard-model processes was assisted by machine learning methods. The classifiers used to separate signals such as these from background are trained using highly unerring but not completely perfect simulations of the physical processes involved, often resulting in incorrect labelling of background processes or signals (label noise) and systematic errors. Here we use quantum and classical annealing (probabilistic techniques for approximating the global maximum or minimum of a given function) to solve a Higgs-signal-versus-background machine learning optimization problem, mapped to a problem of finding the ground state of a corresponding Ising spin model. We build a set of weak classifiers based on the kinematic observables of the Higgs decay photons, which we then use to construct a strong classifier. This strong classifier is highly resilient against overtraining and against errors in the correlations of the physical observables in the training data. We show that the resulting quantum and classical annealing-based classifier systems perform comparably to the state-of-the-art machine learning methods that are currently used in particle physics. However, in contrast to these methods, the annealing-based classifiers are simple functions of directly interpretable experimental parameters with clear physical meaning. The annealer-trained classifiers use the excited states in the vicinity of the ground state and demonstrate some advantage over traditional machine learning methods for small training datasets. Given the relative simplicity of the algorithm and its robustness to error, this technique may find application in other areas of experimental particle physics, such as real-time decision making in event-selection problems and classification in neutrino physics. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,"quantum dot; algorithm; data set; experimental study; fundamental particle; machine learning; optimization; physical method; quantum mechanics; simulated annealing; algorithm; Article; boson; classifier; decision making; decision tree; photon; physics; priority journal; process optimization; quantum mechanics; receiver operating characteristic; simulation; systematic error",2-s2.0-85031919241
"Savage N.","Machine learning: Calculating disease",2017,"Nature",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031762464&doi=10.1038%2f550S115a&partnerID=40&md5=735e82382f644d560cba1d0d122cd56d",[No abstract available],,,2-s2.0-85031762464
"Wu Y., Hoi S.C.H., Liu C., Lu J., Sahoo D., Yu N.","SOL: A library for scalable online learning algorithms",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018699843&doi=10.1016%2fj.neucom.2017.03.077&partnerID=40&md5=e9a2d5d3a2c34636560a585463cf9866","SOL is an open-source library for scalable online learning with high-dimensional data. The library provides a family of regular and sparse online learning algorithms for large-scale classification tasks with high efficiency, scalability, portability, and extensibility. We provide easy-to-use command-line tools, python wrappers and library calls for users and developers, and comprehensive documents for both beginners and advanced users. SOL is not only a machine learning toolbox, but also a comprehensive experimental platform for online learning research. Experiments demonstrate that SOL is highly efficient and scalable for large-scale learning with high-dimensional data. © 2017 Elsevier B.V.","High dimensionality; Online learning; Scalable machine learning; Sparse learning","Artificial intelligence; Clustering algorithms; Data mining; E-learning; Learning systems; Sols; High dimensional data; High dimensionality; Large scale classifications; Online learning; Online learning algorithms; Open-source libraries; Scalable machine learning; Sparse learning; Learning algorithms",2-s2.0-85018699843
"Tan Q., Yu Y., Yu G., Wang J.","Semi-supervised multi-label classification using incomplete label information",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018979647&doi=10.1016%2fj.neucom.2017.04.033&partnerID=40&md5=96420136e1ba45d2e2fbc9d67f89b52d","Classifying multi-label instances using incompletely labeled instances is one of the fundamental tasks in multi-label learning. Most existing methods regard this task as supervised weak-label learning problem and assume sufficient partially labeled instances are available. However, collecting or annotating such instances is expensive and time-consuming. In contrast, abundant unlabeled instances are easy to accumulate. Recently, some methods move toward exploiting unlabeled instances and performing transductive multi-label classification. However, these methods can not directly apply to new instances, which are not available during training process. In this paper, we proposed an approach called Semi-supervised multi-label classification using incomplete label information (SMILE for short). SMILE first estimates label correlation from partially labeled instances and replenishes missing labels of these instances. Then, it takes advantage of labeled and unlabeled instances to construct a neighborhood graph. Next, the known labels and replenished ones of labeled instances, along with unlabeled instances are exploited to train a graph based semi-supervised linear classifier. SMILE can further replenish the missing labels of training instances based on the adopted neighborhood graph. In addition, it can directly predict the labels of completely unlabeled new instances. The empirical study on multi-label datasets shows that SMILE performs significantly better than other related methods across various evaluation criteria and it is important to leverage unlabeled data with label correlation for multi-label classification. © 2017 Elsevier B.V.","Incomplete labels; Label correlation; Multi-label learning; Semi-supervised learning","Graphic methods; Learning algorithms; Learning systems; Supervised learning; Evaluation criteria; Label correlations; Linear classifiers; Multi label classification; Multi-label instances; Multi-label learning; Neighborhood graphs; Semi- supervised learning; Classification (of information); Article; classification algorithm; classifier; correlation function; empiricism; information system; intermethod comparison; learning algorithm; logistic regression analysis; measurement accuracy; priority journal; problem based learning; supervised machine learning; support vector machine; variance",2-s2.0-85018979647
"Buza K., Peška L.","Drug–target interaction prediction with Bipartite Local Models and hubness-aware regression",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019889766&doi=10.1016%2fj.neucom.2017.04.055&partnerID=40&md5=58d8221feeab08ec07c34769af805199","Computational prediction of drug–target interactions is an essential task with various applications in the pharmaceutical industry, such as adverse effect prediction or drug repositioning. Recently, expert systems based on machine learning have been applied to drug–target interaction prediction. Although hubness-aware machine learning techniques are among the most promising approaches, their potential to enhance drug–target interaction prediction methods has not been exploited yet. In this paper, we extend the Bipartite Local Model (BLM), one of the most prominent interaction prediction methods. In particular, we use BLM with a hubness-aware regression technique, ECkNN. We represent drugs and targets in the similarity space with rich set of features (i.e., chemical, genomic and interaction features), and build a projection-based ensemble of BLMs. In order to assist reproducibility of our work as well as comparison to published results, we perform experiments on widely used publicly available drug–target interaction datasets. The results show that our approach outperforms state-of-the-art drug–target prediction techniques. Additionally, we demonstrate the feasibility of predictions from the point of view of applications. © 2017 Elsevier B.V.","Bipartite local models; Drug-target interaction prediction; Hubness-aware machine learning; Regression","Artificial intelligence; Expert systems; Forecasting; Learning systems; Regression analysis; Computational predictions; Drug-target interactions; Hubness; Interaction prediction; Local model; Machine learning techniques; Pharmaceutical industry; Regression; Drug interactions; cell nucleus receptor; enzyme; G protein coupled receptor; ion channel; phosphotransferase; analytical error; Article; bipartite local model; chemical interaction; drug target interaction; hubness aware regression; k nearest neighbor; mathematical analysis; prediction; priority journal; regression analysis; statistical model; support vector machine",2-s2.0-85019889766
"Xiang Z., Xiao Z., Wang D., Xiao J.","Gaussian kernel smooth regression with topology learning neural networks and Python implementation",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012902761&doi=10.1016%2fj.neucom.2017.01.051&partnerID=40&md5=272bfc29aa27c8908fda4915e5d17455","Topology learning neural networks such as Growing Neural Gas (GNG) and Self-Organizing Incremental Neural Network (SOINN) are online clustering methods. With GNG and SOINN implemented as basic learners, this software completes two machine learning tasks, namely density estimation and regression. A kernel density estimation framework is implemented to transform the topology learning neural networks into density estimation methods. Besides, a kernel smoother to implement supervised and semi-supervised regression is devised. Moreover, the implemented frameworks can be used to transform other clustering methods into density estimation, supervised regression and semi-supervised regression. © 2017 Elsevier B.V.","Kernel density estimation; Python; Semi-supervised regression","Cluster analysis; High level languages; Regression analysis; Statistics; Topology; Clustering methods; Density estimation; Density estimation methods; Kernel Density Estimation; Online clustering method; Python; Self-organizing incremental neural network; Semi-supervised; Learning systems; Article; artificial neural network; cluster analysis; computer language; computer prediction; controlled study; information processing; kernel method; machine learning; online analysis; priority journal; regression analysis; software",2-s2.0-85012902761
"Schneider H.W., Raiol T., Brigido M.M., Walter M.E.M.T., Stadler P.F.","A Support Vector Machine based method to distinguish long non-coding RNAs from protein coding transcripts",2017,"BMC Genomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031682325&doi=10.1186%2fs12864-017-4178-4&partnerID=40&md5=d1d0e6eb77f3a0c2b8b9bdea7dfd217c","Background: In recent years, a rapidly increasing number of RNA transcripts has been generated by thousands of sequencing projects around the world, creating enormous volumes of transcript data to be analyzed. An important problem to be addressed when analyzing this data is distinguishing between long non-coding RNAs (lncRNAs) and protein coding transcripts (PCTs). Thus, we present a Support Vector Machine (SVM) based method to distinguish lncRNAs from PCTs, using features based on frequencies of nucleotide patterns and ORF lengths, in transcripts. Methods: The proposed method is based on SVM and uses the first ORF relative length and frequencies of nucleotide patterns selected by PCA as features. FASTA files were used as input to calculate all possible features. These features were divided in two sets: (i) 336 frequencies of nucleotide patterns; and (ii) 4 features derived from ORFs. PCA were applied to the first set to identify 6 groups of frequencies that could most contribute to the distinction. Twenty-four experiments using the 6 groups from the first set and the features from the second set where built to create the best model to distinguish lncRNAs from PCTs. Results: This method was trained and tested with human (Homo sapiens), mouse (Mus musculus) and zebrafish (Danio rerio) data, achieving 98.21%, 98.03% and 96.09%, accuracy, respectively. Our method was compared to other tools available in the literature (CPAT, CPC, iSeeRNA, lncRNApred, lncRScan-SVM and FEELnc), and showed an improvement in accuracy by ≈3.00%. In addition, to validate our model, the mouse data was classified with the human model, and vice-versa, achieving ≈97.80% accuracy in both cases, showing that the model is not overfit. The SVM models were validated with data from rat (Rattus norvegicus), pig (Sus scrofa) and fruit fly (Drosophila melanogaster), and obtained more than 84.00% accuracy in all these organisms. Our results also showed that 81.2% of human pseudogenes and 91.7% of mouse pseudogenes were classified as non-coding. Moreover, our method was capable of re-annotating two uncharacterized sequences of Swiss-Prot database with high probability of being lncRNAs. Finally, in order to use the method to annotate transcripts derived from RNA-seq, previously identified lncRNAs of human, gorilla (Gorilla gorilla) and rhesus macaque (Macaca mulatta) were analyzed, having successfully classified 98.62%, 80.8% and 91.9%, respectively. Conclusions: The SVM method proposed in this work presents high performance to distinguish lncRNAs from PCTs, as shown in the results. To build the model, besides using features known in the literature regarding ORFs, we used PCA to identify features among nucleotide pattern frequencies that contribute the most in distinguishing lncRNAs from PCTs, in reference data sets. Interestingly, models created with two evolutionary distant species could distinguish lncRNAs of even more distant species. © 2017 The Author(s).","LncRNA prediction with nucleotide pattern frequencies and ORF length; Long non-coding RNA (lncRNA); Machine learning; Principal component analysis (PCA); Support vector machine (SVM)",,2-s2.0-85031682325
"Zamora E., Sossa H.","Dendrite morphological neurons trained by stochastic gradient descent",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020098306&doi=10.1016%2fj.neucom.2017.04.044&partnerID=40&md5=a5f10e4cbc726faddf86c093f13539be","Dendrite morphological neurons are a type of artificial neural network that works with min and max operators instead of algebraic products. These morphological operators build hyperboxes in N-dimensional space. These hyperboxes allow the proposal of training methods based on heuristics without using an optimisation method. In literature, it has been claimed that these heuristic-based trainings have advantages: there are no convergence problems, perfect classification can always be reached and training is performed in only one epoch. In this paper, we show that these assumed advantages come with a cost: these heuristics increase classification errors in the test set because they are not optimal and learning generalisation is poor. To solve these problems, we introduce a novel method to train dendrite morphological neurons based on stochastic gradient descent for classification tasks, using these heuristics just for initialisation of learning parameters. Experiments show that we can enhance the testing error in comparison with solely heuristic-based training methods. This approach can reach competitive performance with respect to other popular machine learning algorithms. © 2017 Elsevier B.V.","Dendrite morphological neural network; Gradient descent; Machine learning; Morphological perceptron; Neural network","Artificial intelligence; Classification (of information); Learning algorithms; Learning systems; Neural networks; Neurons; Optimization; Stochastic systems; Classification errors; Classification tasks; Competitive performance; Convergence problems; Gradient descent; Morphological neural networks; Morphological operator; Stochastic gradient descent; Heuristic methods; Article; artificial neural network; cell structure; controlled study; dendrite; dendrite morphological neuron; dendritic cell; heuristics; increasing morphological perceptron; intermethod comparison; kernel method; lattice perceptron; machine learning; Markov chain; perceptron; priority journal; process optimization; radial based function; single-layer morphological perceptron; stochastic gradient descent; support vector machine",2-s2.0-85020098306
"Burdick J., Marques O., Weinthal J., Furht B.","Rethinking Skin Lesion Segmentation in a Convolutional Classifier",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031745245&doi=10.1007%2fs10278-017-0026-y&partnerID=40&md5=730309f20f68889563c32692481eb057","Melanoma is a fatal form of skin cancer when left undiagnosed. Computer-aided diagnosis systems powered by convolutional neural networks (CNNs) can improve diagnostic accuracy and save lives. CNNs have been successfully used in both skin lesion segmentation and classification. For reasons heretofore unclear, previous works have found image segmentation to be, conflictingly, both detrimental and beneficial to skin lesion classification. We investigate the effect of expanding the segmentation border to include pixels surrounding the target lesion. Ostensibly, segmenting a target skin lesion will remove inessential information, non-lesion skin, and artifacts to aid in classification. Our results indicate that segmentation border enlargement produces, to a certain degree, better results across all metrics of interest when using a convolutional based classifier built using the transfer learning paradigm. Consequently, preprocessing methods which produce borders larger than the actual lesion can potentially improve classifier performance, more than both perfect segmentation, using dermatologist created ground truth masks, and no segmentation altogether. © 2017 Society for Imaging Informatics in Medicine","Convolutional neural networks; Deep learning; Machine learning; Medical decision support systems; Medical image analysis; Skin lesions","Artificial intelligence; Classification (of information); Computer aided diagnosis; Convolution; Decision support systems; Deep learning; Dermatology; Diagnosis; Learning systems; Medical imaging; Neural networks; Classifier performance; Computer aided diagnosis systems; Convolutional neural network; Diagnostic accuracy; Medical decision support system; Pre-processing method; Skin lesion; Transfer learning; Image segmentation",2-s2.0-85031745245
"Anderson J.P., Icten Z., Alas V., Benson C., Joshi K.","Comparison and predictors of treatment adherence and remission among patients with schizophrenia treated with paliperidone palmitate or atypical oral antipsychotics in community behavioral health organizations",2017,"BMC Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031688445&doi=10.1186%2fs12888-017-1507-8&partnerID=40&md5=40b8da96afc882193ff049e3bd9e1e78","Background: Nonadherence to antipsychotic treatment increases the likelihood of relapse and progressive symptomatology in patients with schizophrenia. Atypical long-acting injectables, including paliperidone palmitate (PP), may increase adherence and improve symptoms. This study compared and assessed predictors of treatment patterns and symptom remission among schizophrenia patients treated with PP versus atypical oral antipsychotic therapy (OAT) in community behavioral health organizations (CBHOs). Methods: This retrospective cohort analysis evaluated 763 patients with schizophrenia and new (PP-N; N=174) or continuing (PP-C; N=308) users of PP, or new users of OAT (N=281) at enrollment in the REACH-OUT study (2010-2013). Treatment outcomes assessed at 1year were discontinuation, and adherence, measured by proportion of days covered (PDC) or medication possession ratio (MPR). Remission status was assessed using the Structured Clinical Interview for Symptoms of Remission (SCI-SR). A machine learning platform, Reverse Engineering and Forward Simulation (REFS™), was used to identify predictors of study outcomes. Multivariate Cox and generalized linear regressions estimated the adjusted hazard ratios (HRs) or odds ratios (ORs) with 95% confidence intervals. Results: Among PP-N users, 27% discontinued their initial treatment regimen versus 51% (p<0.001) of OAT users. PP-N (vs OAT; HR=0.49 [0.31-0.76]) users and males (HR=0.65 [0.46-0.92]) had significantly lower rates of discontinuation. Relative to OAT, PP-N had a 36% [31%-42%] higher MPR and a 10-fold increased achievement of PDC ≥80% (OR=10.46 [5.72-19.76]). PP users were significantly more likely to achieve remission in follow-up (PP-N vs OAT: OR=2.65 [1.39-5.05]; PP-C vs OAT: OR=1.83 [1.03-3.25]). Conclusions: Relative to OAT, PP was associated with improved adherence, less frequent treatment discontinuation, and improved symptom remission in this CBHO study population. © 2017 The Author(s).","Adherence; Community behavioral health organization; Oral antipsychotics; Paliperidone palmitate; Remission; Schizophrenia","atypical antipsychotic agent; paliperidone; adult; alcohol abuse; Article; cohort analysis; comorbidity; comparative study; drug efficacy; female; follow up; health care organization; hospitalization; human; machine learning; major clinical study; male; medication compliance; patient compliance; questionnaire; remission; retrospective study; risk factor; schizophrenia; simulation; Structured Clinical Interview for Symptoms of Remission questionnaire; substance abuse; treatment outcome",2-s2.0-85031688445
"Mafarja M.M., Mirjalili S.","Hybrid Whale Optimization Algorithm with simulated annealing for feature selection",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019385058&doi=10.1016%2fj.neucom.2017.04.053&partnerID=40&md5=88d923d86241234229d57c65aa735d17","Hybrid metaheuristics are of the most interesting recent trends in optimization and memetic algorithms. In this paper, two hybridization models are used to design different feature selection techniques based on Whale Optimization Algorithm (WOA). In the first model, Simulated Annealing (SA) algorithm is embedded in WOA algorithm, while it is used to improve the best solution found after each iteration of WOA algorithm in the second model. The goal of using SA here is to enhance the exploitation by searching the most promising regions located by WOA algorithm. The performance of the proposed approaches is evaluated on 18 standard benchmark datasets from UCI repository and compared with three well-known wrapper feature selection methods in the literature. The experimental results confirm the efficiency of the proposed approaches in improving the classification accuracy compared to other wrapper-based algorithms, which insures the ability of WOA algorithm in searching the feature space and selecting the most informative attributes for classification tasks. © 2017 Elsevier B.V.","Classification; Feature selection; Hybrid optimization; Optimization; Simulated annealing; Whale Optimization Algorithm; WOA","Benchmarking; Classification (of information); Feature extraction; Iterative methods; Simulated annealing; Classification accuracy; Classification tasks; Feature selection methods; Hybrid metaheuristics; Hybrid optimization; Informative attributes; Optimization algorithms; Simulated annealing algorithms; Optimization; algorithm; Article; benchmarking; classification; computer heuristics; controlled study; feature selection; information system; machine learning; priority journal; simulated annealing algorithm; whale optimization algorithm",2-s2.0-85019385058
"Rakkiyappan R., Maheswari K., Sivaranjani K.","Non-weighted H∞ state estimation for discrete-time switched neural networks with persistent dwell time switching regularities based on Finsler's lemma",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019350329&doi=10.1016%2fj.neucom.2017.04.006&partnerID=40&md5=82f3b7a60f646431d46f6a647eebc911","In this study, the state estimation and H∞ control problem for discrete-time switched neural networks with mode-dependent time-varying delays has been studied with persistent dwell time (PDT) switching regularities. The phenomenon of PDT, existing for the designed estimator of underlying switched neural networks are characterized by introducing a Bernoulli distributed white sequence. The main aim of the addressed problem is to design mode dependent state estimators such that the dynamics of the estimation error is exponentially stable with an expected decay rate and satisfies the prescribed H∞ performance constraint. Sufficient conditions are established for the occurence of the desired filter to ensure the mean-square exponential stability of the augmented system by using the generalized Finsler's lemma and then the full-order filter parameters are presented in terms of solutions to a set of linear matrix inequality (LMI) conditions. Finally, simulation results are given to explain the usefulness of the proposed design procedure. © 2017 Elsevier B.V.","Exponential stability; H∞ state estimation; Persistent dwell time; Switched neural network","Asymptotic stability; Decay (organic); Estimation; Linear matrix inequalities; Time delay; Time varying control systems; Augmented systems; Dwell time; Dwell-time switching; Exponentially stable; Mean square exponential stability; Performance constraints; Switched neural networks; Time varying- delays; State estimation; Article; artificial neural network; control system; digital filtering; discrete time switched neural network; filter design; machine learning; persistent dwell time switching regularity; priority journal; simulation",2-s2.0-85019350329
"Shinde S., Kulkarni U.","Extended fuzzy hyperline-segment neural network with classification rule extraction",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020222717&doi=10.1016%2fj.neucom.2017.03.036&partnerID=40&md5=ec7de255bb8c3b81440c30ec3bcaaf92","The hyperline segment layer of fuzzy hyperline segment neural network (FHLSNN) consists of number of n- dimensional hyperline segments. Each hyperline segment has the two end points defined in terms of continuous attribute values and an associated membership function. The membership function processes the continuous attribute values and gives the membership of each input pattern to the hyperline segments. These end points and membership function are defined only in terms of continuous attributes and cannot process discrete attributes. But the real world data comprises both continuous and discrete attributes. Due to this original FHLSNN and its ancestor versions cannot be applied to these types of data. Also, justification of the classification decision given by these fuzzy min–max neural networks is required to be obtained to make them more applicable to the real world applications. The objective of the proposed extended fuzzy hyperline segment neural network (EFHLSNN) is to solve these two problems. In the EFHLSNN, each hyperline segment has two end points defined not only in terms of continuous attributes but also the set of binary strings defined for discrete attributes. The membership function and expansion condition of the original FHLSNN are modified to process both continuous and discrete attribute values. The proposed EFHLSNN model is applied to eight different benchmark datasets. The experimental results show that the proposed model gives very good accuracy and the compact rule set that justifies the classification decision of the EFHLSNN and thus, giving rise to good rule fidelity. © 2017","Classification rules; Fuzzy membership function; Hyperline segment; Mixed attributes; Neural network; Neuro-fuzzy system","Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Neural networks; Point contacts; Classification rules; Fuzzy membership function; Hyperline segments; Mixed attributes; Neurofuzzy system; Membership functions; rain; algorithm; Article; artificial neural network; classifier; data base; extended fuzzy hyperline segment neural network; fuzzy system; machine learning; priority journal; weather",2-s2.0-85020222717
"Liu H.-R., Hu Y.-L., Yin R.-R., Deng Y.-J.","Cascading failure model of scale-free topology for avoiding node failure",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020395569&doi=10.1016%2fj.neucom.2017.04.042&partnerID=40&md5=d57ae31b81a44dc9a55e971b3a71a443","In order to efficiently fortify the robustness of a scale-free network against cascading failure, a model of this phenomenon is proposed based on defining the load of a node with respect to both degree and betweenness centrality. Simultaneously, when the load is redistributed, an allocation proportion is ascertained according to both the loads and the energies of the nodes. After that, we obtain the relationship between the model parameters and the robustness of the network against the cascading failure with theoretical analysis, which also helps us determine the influence of the average network degree on cascading failure. Finally, the results of theoretical analysis are verified with simulation experiments. © 2017 Elsevier B.V.","Cascading failure; Load parameter; Node energy; Scale-free network","Complex networks; Computer applications; Neural networks; Betweenness centrality; Cascading failure models; Cascading failures; Load parameters; Model parameters; Network degree; Node energy; Scale-free topologies; Topology; Article; cascading failure model; energy; information processing; load carrying capacity; machine learning; model; node energy; priority journal; scale free network; simulation; theoretical study",2-s2.0-85020395569
"Fu Y., Wu X., Wen Y., Xiang Y.","Efficient locality-constrained occlusion coding for face recognition",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018409904&doi=10.1016%2fj.neucom.2017.04.001&partnerID=40&md5=1222e5c1e79605ae02893f718fba2292","Occlusion is a common yet challenging problem in face recognition. Most of the existing approaches cannot achieve the accuracy of the recognition with high efficiency in the occlusion case. To address this problem, this paper proposes a novel algorithm, called efficient locality-constrained occlusion coding (ELOC), improving the previous sparse error correction with Markov random fields (SEC_MRF) algorithm. The proposed approach estimates and excludes occluded region by locality-constrained linear coding (LLC), which avoids the time-consuming l1-minimization and exhaustive subject-by-subject search during the occlusion estimation, and greatly reduces the running time of recognition. Moreover, by simplifying the regularization, the ELOC can be further accelerated. Experimental results on several face databases show that our algorithms significantly improve the previous algorithms in efficiency without losing too much accuracy. © 2017 Elsevier B.V.","Efficiency; Face recognition; Locality-constrained Linear Coding; Occlusion estimating; Sparse Error Correction with Markov Random Fields","Codes (symbols); Efficiency; Error correction; Markov processes; Face database; High-efficiency; Linear coding; Markov Random Fields; Novel algorithm; Occlusion estimating; Running time; Face recognition; Article; efficient locality constrained occlusion coding algorithm; facial recognition; human; machine learning; Markov chain; mathematical computing; mathematical model; mathematical parameters; measurement accuracy; measurement error; priority journal",2-s2.0-85018409904
"Cui Y., Liu Y., Zhang W., Hayat T., Alsaedi A.","Sampled-data state estimation for a class of delayed complex networks via intermittent transmission",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018973546&doi=10.1016%2fj.neucom.2017.04.031&partnerID=40&md5=1c4647b22f785867c996d2523c27e104","This paper investigates the sampled-data state estimation problem for a class of delayed complex networks. At certain sampling times, transmission of sampled-data through communication network may fail, which means the considered estimator can only intermittently receive sampled-data. The main objective of this paper is to design a sampled-data state estimator subjected to intermittent transmission such that the error system is exponentially stable. Specifically, the error system is first transformed into time-varying delayed switched systems, including both stable and unstable subsystems. And then, to analyze the stability of the error system, a modified Halanay inequality is presented. In view of the modified Halanay inequality and switched systems methodology, a sufficient condition for globally exponential stability of the error system is established. Meanwhile, the upper bound of transmission failure rate is given, which reflects to be closely related to sampling period and the upper bound of node delays. Furthermore, the desired estimator gain of each node is explicitly provided by solving a set of matrix inequalities. Finally, a numerical simulation is carried out to verify the effectiveness of the inferred results. © 2017 Elsevier B.V.","Complex networks; Halanay inequality; Sampled-data intermittent transmission; State estimation","Errors; Failure analysis; State estimation; Switching systems; System stability; Time delay; Estimation problem; Exponentially stable; Globally exponential stability; Halanay inequality; Intermittent transmission; Matrix inequality; Time-varying delayed; Transmission failures; Complex networks; Article; complex network; machine learning; mathematical analysis; priority journal; problem solving; sampled data state estimation; sampling error; simulation; statistical concepts",2-s2.0-85018973546
"Fan Y., Levine M.D., Wen G., Qiu S.","A deep neural network for real-time detection of falling humans in naturally occurring scenes",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018714168&doi=10.1016%2fj.neucom.2017.02.082&partnerID=40&md5=1432388759b5599091cc519b26cacaff","We introduce a novel approach to the problem of human fall detection in naturally occurring scenes. This is important because falling incidents cause thousands of deaths every year and vision-based approaches offer a promising and effective way to detect falls. To address this challenging issue, we regard it as an example of action detection and propose to also locate its temporal extent. We achieve this by exploiting the effectiveness of deep networks. In the training stage, the trimmed video clips of four phases (standing, falling, fallen and not moving) in a fall are converted to four categories of so-called dynamic image to train a deep ConvNet that scores and predicts the label of each dynamic image. In the testing stage, a set of sub-videos is generated using a sliding window on an untrimmed video that converts it to multiple dynamic images. Based on the predicted label of each dynamic image by the trained deep ConvNet, the videos are classified as falling or not by a “standing watch” for a situation consisting of the four sequential phases. In order to localize the temporal extent of the event, we propose a difference score method (DSM) based on adjacent dynamic images in the temporal sequence. We collect a new dataset, called the YouTube Fall Dataset (YTFD), which contains 430 falling incidents and 176 normal activities and use it to learn the deep network to detect falling humans. We perform experiments on datasets of varying complexity: Le2i fall detection dataset, multiple cameras fall dataset, high quality fall simulation dataset and our own YouTube Fall Dataset. The results demonstrate the effectiveness and efficiency of our approach. © 2017 Elsevier B.V.","Action detection; Convolutional neural network; Deep learning; Dynamic image; Fall detection; Temporal location","Communication channels (information theory); Deep learning; Neural networks; Signal detection; Video cameras; Convolutional neural network; Dynamic images; Effectiveness and efficiencies; Fall detection; Human fall detection; Naturally occurring; Real-time detection; Vision-based approaches; Deep neural networks; Article; artificial neural network; deep learning; falling; false negative result; false positive result; human; illumination; information processing; learning; long short term memory; long term memory; prediction; priority journal; recurrent neural network; sensitivity and specificity; short term memory; simulation; sudden cardiac death; support vector machine; terrorism",2-s2.0-85018714168
"Zhang Y., Chu G., Li P., Hu X., Wu X.","Three-layer concept drifting detection in text data streams",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019345026&doi=10.1016%2fj.neucom.2017.04.047&partnerID=40&md5=8ba0e90ae4c682885b6118c84b48198e","Text data streams have widely appeared in real-world applications, in which, concept drifts owe a significant challenge for classification. Compared with relational data streams, concept drifts hidden in text streams usually reflect in the relationship between the feature vector and the instance labels. Meanwhile, existing concept drifting detection methods are mainly based on error rates of classification. When applying these methods in text streams, they perform poorly in the evaluations of false alarms and missing detections, etc. Motivated by this, we firstly give a systematic analysis of the concept drifts in text data streams. Then, we propose a three-layer concept drifting detection approach, where the three layers indicate the layer of label space, the layer of feature space and the layer of the mapping relationships between labels and features, respectively. In this approach, the latter two layers are based on the values of WoE (Weight of Evidence) and the IV (Information Value) index. Experimental results show that our approach can improve the performance of concept drifting detection and the accuracy of classification, especially when concept drifts in text data streams are frequent. © 2017 Elsevier B.V.","Classification; Concept drift; IV value; Text data streams","Classification (of information); Data communication systems; Accuracy of classifications; Concept drifts; IV value; Mapping relationships; Relational data streams; Systematic analysis; Text data; Weight of evidences; Text processing; algorithm; Article; Bayesian learning; controlled study; data processing; decision tree; dext data stream; information value; priority journal; statistical parameters; support vector machine; theory; three layer concept drifting detection; weight of evidence",2-s2.0-85019345026
"Ferreira J.R., Jr, Oliveira M.C., de Azevedo-Marques P.M.","Characterization of Pulmonary Nodules Based on Features of Margin Sharpness and Texture",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732294&doi=10.1007%2fs10278-017-0029-8&partnerID=40&md5=5ed8c7cd376e450ef517ebb1814ffd87","Lung cancer is the leading cause of cancer-related deaths in the world, and one of its manifestations occurs with the appearance of pulmonary nodules. The classification of pulmonary nodules may be a complex task to specialists due to temporal, subjective, and qualitative aspects. Therefore, it is important to integrate computational tools to the early pulmonary nodule classification process, since they have the potential to characterize objectively and quantitatively the lesions. In this context, the goal of this work is to perform the classification of pulmonary nodules based on image features of texture and margin sharpness. Computed tomography scans were obtained from a publicly available image database. Texture attributes were extracted from a co-occurrence matrix obtained from the nodule volume. Margin sharpness attributes were extracted from perpendicular lines drawn over the borders on all nodule slices. Feature selection was performed by different algorithms. Classification was performed by several machine learning classifiers and assessed by the area under the receiver operating characteristic curve, sensitivity, specificity, and accuracy. Highest classification performance was obtained by a random forest algorithm with all 48 extracted features. However, a decision tree using only two selected features obtained statistically equivalent performance on sensitivity and specificity. © 2017 Society for Imaging Informatics in Medicine","Image classification; Lung cancer; Pattern recognition; Pulmonary nodule","Biological organs; Computerized tomography; Decision trees; Diseases; Image classification; Learning algorithms; Learning systems; Pattern recognition; Positron emission tomography; Classification performance; Classification process; Computed tomography scan; Lung Cancer; Pulmonary nodules; Random forest algorithm; Receiver operating characteristic curves; Sensitivity and specificity; Data mining",2-s2.0-85031732294
"Liu Y., Xie Y., Bao C., Srivastava A.","A Combined Optimization-Theoretic and Side-Channel Approach for Attacking Strong Physical Unclonable Functions",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032278617&doi=10.1109%2fTVLSI.2017.2759731&partnerID=40&md5=e12c38618e2a5fb4758644c6eebf402f","The promise of strong physical unclonable functions (PUF) is to utilize the manufacturing variations of circuit elements to produce an independent and unpredictable response to any input challenge vector. Attacks on PUFs that predict the responses to input challenge vectors offer an interesting research problem. An attacking approach based on the optimization theory and side-channel information is proposed where we estimate the manufacturing variations of the circuit elements and predict the PUF's responses to challenge vectors whose actual responses are not known. We apply this attacking approach on some popular PUF designs, including the Arbiter PUFs, the Memristor Crossbar PUFs, and the XOR Arbiter PUFs. Simulations show a substantial reduction in attack complexity compared with previously proposed machine-learning (ML)-based attacks: we achieve an average reduction of 66&#x0025; in attack time compared with the ML approach. Despite some overhead, our approach is also applicable when the PUF responses are noisy. IEEE","Cutting-plane method; optimization theoretic; physical unclonable functions (PUFs); PUF attack; side channel; strong PUF.","Cryptography; Hardware security; Learning systems; Manufacture; Optimization; Site selection; Cutting plane methods; Manufacturing Variation; Physical unclonable functions (PUF); PUF attack; Side-channel; Side-channel information; strong PUF; Substantial reduction; Side channel attack",2-s2.0-85032278617
"Chen L., Wu M., Zhou M., Liu Z., She J., Hirota K.","Dynamic Emotion Understanding in Human-Robot Interaction Based on Two-Layer Fuzzy SVR-TS Model",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032278026&doi=10.1109%2fTSMC.2017.2756447&partnerID=40&md5=0c7c371937561eff0ad2f9d16fc158c0","Two-layer fuzzy support vector regression-Takagi-Sugeno (TLFSVR-TS) model is proposed for emotion understanding in human-robot interaction (HRI), where the real-time dynamic emotion is recognized according to facial expression, and emotional intention understanding is obtained mainly based on human emotions and identification information. It aims to make robots capable of recognizing and understanding human emotions, in such a way that make HRI run smoothly. TLFSVR-TS considers about the priori knowledge inferred from human personal preference to reduce the uncertainty of various people, and multiple support vector regression (SVR) corresponding to different genders/provinces/ages of human to guarantee the local learning ability. Preliminary application experiments are performed in the developing emotional social robot system, where 30 volunteers experience the scenario of ``drinking in the bar.'' Results show that the proposal receives higher understanding accuracy than that of TLFSVR, kernel fuzzy c-means clustering is fused with SVR, and SVR. IEEE","Dynamic recognition; emotion understanding; human-robot interaction (HRI); two-layer fuzzy support vector regression-Takagi-Sugeno (TLFSVR-TS) model","Man machine systems; Regression analysis; Robots; Dynamic recognition; Emotion understanding; Fuzzy C means clustering; Fuzzy support vector regressions; Human robot Interaction (HRI); Intention understanding; Personal preferences; Support vector regression (SVR); Human robot interaction",2-s2.0-85032278026
"Pedemonte S., Pierce L., Van Leemput K.","A machine learning method for fast and accurate characterization of depth-of-interaction gamma cameras",2017,"Physics in Medicine and Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032686169&doi=10.1088%2f1361-6560%2faa6ee5&partnerID=40&md5=7cf7328c008b8473ebb02d9d7573b4b8","Measuring the depth-of-interaction (DOI) of gamma photons enables increasing the resolution of emission imaging systems. Several design variants of DOI-sensitive detectors have been recently introduced to improve the performance of scanners for positron emission tomography (PET). However, the accurate characterization of the response of DOI detectors, necessary to accurately measure the DOI, remains an unsolved problem. Numerical simulations are, at the state of the art, imprecise, while measuring directly the characteristics of DOI detectors experimentally is hindered by the impossibility to impose the depth-of-interaction in an experimental set-up. In this article we introduce a machine learning approach for extracting accurate forward models of gamma imaging devices from simple pencil-beam measurements, using a nonlinear dimensionality reduction technique in combination with a finite mixture model. The method is purely data-driven, not requiring simulations, and is applicable to a wide range of detector types. The proposed method was evaluated both in a simulation study and with data acquired using a monolithic gamma camera designed for PET (the cMiCE detector), demonstrating the accurate recovery of the DOI characteristics. The combination of the proposed calibration technique with maximum- a posteriori estimation of the coordinates of interaction provided a depth resolution of ≈1.14 mm for the simulated PET detector and ≈1.74 mm for the cMiCE detector. The software and experimental data are made available at http://occiput.mgh.harvard.edu/depthembedding/. © 2017 Institute of Physics and Engineering in Medicine.","depth-of-interaction (DOI); gamma camera; locally linear embedding; manifold learning; maximum-likelihood estimation; position sensitive gamma detectors; scintillator characterization","Artificial intelligence; Cameras; Characterization; Image resolution; Learning algorithms; Learning systems; Maximum likelihood; Maximum likelihood estimation; Depth of interactions; Gamma cameras; Gamma detectors; Locally linear embedding; Manifold learning; Positron emission tomography",2-s2.0-85032686169
"Brown A.I., Sivak D.A.","Allocating dissipation across a molecular machine cycle to maximize flux",2017,"Proceedings of the National Academy of Sciences of the United States of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497136&doi=10.1073%2fpnas.1707534114&partnerID=40&md5=722f4b0816237d8774788ff4409fff05","Biomolecular machines consume free energy to break symmetry and make directed progress. Nonequilibrium ATP concentrations are the typical free energy source, with one cycle of a molecular machine consuming a certain number of ATP, providing a fixed free energy budget. Since evolution is expected to favor rapid-turnover machines that operate efficiently, we investigate how this free energy budget can be allocated to maximize flux. Unconstrained optimization eliminates intermediate metastable states, indicating that flux is enhanced in molecular machines with fewer states. When maintaining a set number of states, we show that—in contrast to previous findings—the flux-maximizing allocation of dissipation is not even. This result is consistent with the coexistence of both “irreversible” and reversible transitions in molecular machine models that successfully describe experimental data, which suggests that, in evolved machines, different transitions differ significantly in their dissipation. © 2017, National Academy of Sciences. All rights reserved.","Dissipation; Molecular machines; Nonequilibrium steady state","Article; budget; cell cycle; dynamics; energy; energy resource; hydrolysis; kinetics; landscape; machine; machine learning; priority journal; rate constant; steady state; velocity",2-s2.0-85031497136
"Chen Y., Jiao L., Li Y., Zhao J.","Multilayer Projective Dictionary Pair Learning and Sparse Autoencoder for PolSAR Image Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032220488&doi=10.1109%2fTGRS.2017.2727067&partnerID=40&md5=0da323cc8cbeea47616a0b76b2485e4a","Polarimetric synthetic aperture radar (PolSAR) image classification is a vital application in remote sensing image processing. In general, PolSAR image classification is actually a high-dimensional nonlinear mapping problem. The methods based on sparse representation and deep learning have shown a great potential for PolSAR image classification. Therefore, a novel PolSAR image classification method based on multilayer projective dictionary pair learning (MDPL) and sparse autoencoder (SAE) is proposed in this paper. First, MDPL is used to extract features, and the abstract degree of the extracted features is high. Second, in order to get the nonlinear relationship between elements of feature vectors in an adaptive way, SAE is also used in this paper. Three PolSAR images are used to test the effectiveness of our method. Compared with several state-of-the-art methods, our method achieves very competitive results in PolSAR image classification. IEEE","Dictionaries; Encoding; Feature extraction; Machine learning; Multilayer projective dictionary pair learning (MDPL); Nonhomogeneous media; polarimetric synthetic aperture radar (PolSAR); Scattering; sparse autoencoder (SAE); sparse representation.","Encoding (symbols); Feature extraction; Glossaries; Image processing; Learning systems; Multilayers; Polarimeters; Radar; Remote sensing; Scattering; Synthetic aperture radar; Auto encoders; Classification methods; Non-linear relationships; Nonhomogeneous media; Polarimetric synthetic aperture radars; Remote sensing image processing; Sparse representation; State-of-the-art methods; Image classification",2-s2.0-85032220488
"Chen D., Tian Y.","Large-scale structural learning and predicting via hashing approximation",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031490877&doi=10.1007%2fs00521-017-3238-7&partnerID=40&md5=9c560a65c485f864e25328700ad9f204","By combining the structural information with nonparallel support vector machine, structural nonparallel support vector machine (SNPSVM) can fully exploit prior knowledge to directly improve the algorithm’s generalization capacity. However, the scalability issue how to train SNPSVM efficiently on data with huge dimensions has not been studied. In this paper, we integrate linear SNPSVM with b-bit minwise hashing scheme to speedup the training phase for large-scale and high-dimensional statistical learning, and then we address the problem of speeding-up its prediction phase via locality-sensitive hashing. For one-against-one multi-class classification problems, a two-stage strategy is put forward: a series of hash-based classifiers are built in order to approximate the exact results and filter the hypothesis space in the first stage and then the classification can be refined by solving a multi-class SNPSVM on the remaining classes in the second stage. The proposed method can deal with large-scale classification problems with a huge number of features. Experimental results on two large-scale datasets (i.e., news20 and webspam) demonstrate the efficiency of structural learning via b-bit minwise hashing. Experimental results on the ImageNet-BOF dataset, and several large-scale UCI datasets show that the proposed hash-based prediction can be more than two orders of magnitude faster than the exact classifier with minor losses in quality. © 2017 The Natural Computing Applications Forum","Locality-sensitive hashing; Minwise hashing; Nonparallel support vector machine; Structural information","Forecasting; Image retrieval; Learning systems; Support vector machines; Generalization capacity; Large scale classifications; Large-scale datasets; Locality sensitive hashing; Minwise hashing; Multiclass classification problems; Statistical learning; Structural information; Classification (of information)",2-s2.0-85031490877
"Kim G.B., Jung K.-H., Lee Y., Kim H.-J., Kim N., Jun S., Seo J.B., Lynch D.A.","Comparison of Shallow and Deep Learning Methods on Classifying the Regional Pattern of Diffuse Lung Disease",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031507028&doi=10.1007%2fs10278-017-0028-9&partnerID=40&md5=d12c96b14f74aafab34f8571b08c00a5","This study aimed to compare shallow and deep learning of classifying the patterns of interstitial lung diseases (ILDs). Using high-resolution computed tomography images, two experienced radiologists marked 1200 regions of interest (ROIs), in which 600 ROIs were each acquired using a GE or Siemens scanner and each group of 600 ROIs consisted of 100 ROIs for subregions that included normal and five regional pulmonary disease patterns (ground-glass opacity, consolidation, reticular opacity, emphysema, and honeycombing). We employed the convolution neural network (CNN) with six learnable layers that consisted of four convolution layers and two fully connected layers. The classification results were compared with the results classified by a shallow learning of a support vector machine (SVM). The CNN classifier showed significantly better performance for accuracy compared with that of the SVM classifier by 6–9%. As the convolution layer increases, the classification accuracy of the CNN showed better performance from 81.27 to 95.12%. Especially in the cases showing pathological ambiguity such as between normal and emphysema cases or between honeycombing and reticular opacity cases, the increment of the convolution layer greatly drops the misclassification rate between each case. Conclusively, the CNN classifier showed significantly greater accuracy than the SVM classifier, and the results implied structural characteristics that are inherent to the specific ILD patterns. © 2017 Society for Imaging Informatics in Medicine","Convolution neural network; Deep architecture; Interscanner variation; Interstitial lung disease; Support vector machine","Biological organs; Computerized tomography; Convolution; Deep learning; Opacity; Pulmonary diseases; Classification accuracy; Convolution neural network; Deep architectures; High-resolution computed tomography; Interscanner variation; Interstitial lung disease; Misclassification rates; Structural characteristics; Support vector machines",2-s2.0-85031507028
"Lepot M., Aubin J.-B., Clemens F.H.L.R.","Interpolation in time series: An introductive overview of existing methods, their performance criteria and uncertainty assessment",2017,"Water (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031899194&doi=10.3390%2fw9100796&partnerID=40&md5=beb1a1425f6ef0a0eac4a97aecb56a60","A thorough review has been performed on interpolation methods to fill gaps in time-series, efficiency criteria, and uncertainty quantifications. On one hand, there are numerous available methods: interpolation, regression, autoregressive, machine learning methods, etc. On the other hand, there are many methods and criteria to estimate efficiencies of these methods, but uncertainties on the interpolated values are rarely calculated. Furthermore, while they are estimated according to standard methods, the prediction uncertainty is not taken into account: a discussion is thus presented on the uncertainty estimation of interpolated/extrapolated data. Finally, some suggestions for further research and a new method are proposed. © 2017 by the authors.","Comparison; Criteria; Interpolation; Methods; Review; Uncertainty","Efficiency; Interpolation; Learning systems; Reviews; Time series; Comparison; Criteria; Machine learning methods; Methods; Uncertainty; Uncertainty assessment; Uncertainty estimation; Uncertainty quantifications; Uncertainty analysis; assessment method; comparative study; estimation method; interpolation; literature review; numerical method; performance assessment; prediction; time series analysis; uncertainty analysis",2-s2.0-85031899194
"Audus D.J., De Pablo J.J.","Polymer Informatics: Opportunities and Challenges",2017,"ACS Macro Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031706538&doi=10.1021%2facsmacrolett.7b00228&partnerID=40&md5=764908a8423a66d7f662b82fb2378f6e","We are entering an era where large volumes of scientific data, coupled with algorithmic and computational advances, can reduce both the time and cost of developing new materials. This emerging field known as materials informatics has gained acceptance for a number of classes of materials, including metals and oxides. In the particular case of polymer science, however, there are important challenges that must be addressed before one can start to deploy advanced machine learning approaches for designing new materials. These challenges are primarily related to the manner in which polymeric systems and their properties are reported. In this viewpoint, we discuss the opportunities and challenges for making materials informatics as applied to polymers, or equivalently polymer informatics, a reality. © 2017 American Chemical Society.",,"Learning systems; Informatics; Large volumes; Machine learning approaches; Materials informatics; Number of class; Polymer science; Polymeric systems; Scientific data; Polymers",2-s2.0-85031706538
"Yuan T., Deng W., Hu J.","Distortion Minimization Hashing",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032277250&doi=10.1109%2fACCESS.2017.2763600&partnerID=40&md5=9d48e45da4913bf74ff1fc8dd13cf869","Application of the hashing method to large-scale image retrieval has drawn much attention because of the high efficiency and favorable accuracy of the method. Its related research generally involves two basic problems: similarity-preserving projection and information-preserving quantization. Most previous works focused on learning projection approaches, while the importance of quantization strategies was ignored. Although several hashing quantization models have been recently proposed to improve retrieval performance by assigning multiple bits to projected directions, these models still suffer from suboptimal results, as the critical information loss that occurs in the quantization procedure is not considered. In this paper, to construct an effective quantization model, we utilize rate-distortion theory in the hashing quantization procedure and minimize the distortion to reduce the information loss. Furthermore, combining principal component analysis (PCA) with our quantization strategy, we present a quantization-based hashing method named distortion minimization hashing (DMH). Extensive experiments involving one synthetic dataset and three image datasets demonstrate the superior performance of our proposed methods over several quantization techniques and state-of-the-art hashing methods. OAPA","Binary codes; Bit rate; Distortion; Hashing Quantization; Image retrieval; Image Retrieval; Iterative Optimization; Loss measurement; Machine Learning; Quantization (signal); Rate-distortion; Rate-distortion Theory","Binary codes; Distortion (waves); Electric distortion; Image coding; Image retrieval; Iterative methods; Learning systems; Principal component analysis; Signal distortion; Bit rates; Hashing Quantization; Iterative Optimization; Loss measurement; Rate distortion-theory; Rate distortions; Quantization (signal)",2-s2.0-85032277250
"Mohammed A., Biegert G., Adamec J., Helikar T.","Identification of potential tissue-specific cancer biomarkers and development of cancer versus normal genomic classifiers",2017,"Oncotarget",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031498642&doi=10.18632%2foncotarget.21127&partnerID=40&md5=8fa3c27e30cbe6c5aa980b208958dc8e","Machine learning techniques for cancer prediction and biomarker discovery can hasten cancer detection and significantly improve prognosis. Recent ""OMICS"" studies which include a variety of cancer and normal tissue samples along with machine learning approaches have the potential to further accelerate such discovery. To demonstrate this potential, 2,175 gene expression samples from nine tissue types were obtained to identify gene sets whose expression is characteristic of each cancer class. Using random forests classification and ten-fold cross-validation, we developed nine single-tissue classifiers, two multi-tissue cancer-versus-normal classifiers, and one multi-tissue normal classifier. Given a sample of a specified tissue type, the single-tissue models classified samples as cancer or normal with a testing accuracy between 85.29% and 100%. Given a sample of non-specific tissue type, the multitissue bi-class model classified the sample as cancer versus normal with a testing accuracy of 97.89%. Given a sample of non-specific tissue type, the multi-tissue multiclass model classified the sample as cancer versus normal and as a specific tissue type with a testing accuracy of 97.43%. Given a normal sample of any of the nine tissue types, the multi-tissue normal model classified the sample as a particular tissue type with a testing accuracy of 97.35%. The machine learning classifiers developed in this study identify potential cancer biomarkers with sensitivity and specificity that exceed those of existing biomarkers and pointed to pathways that are critical to tissuespecific tumor development. This study demonstrates the feasibility of predicting the tissue origin of carcinoma in the context of multiple cancer classes. © Mohammed et al.","Biomarker identification; Cancer biomarker; Cancer classification; Machine learning; Microarray gene expression","tumor marker; Article; cancer classification; cancer diagnosis; cancer prognosis; cancer tissue; carcinogenesis; classification; classifier; diagnostic accuracy; feasibility study; gene expression; gene identification; genetic algorithm; genetic database; genomics; machine learning; prediction; random forest; sensitivity and specificity; statistical model; tissue specificity; validation process",2-s2.0-85031498642
"Andreadis I., Sevastianos C., George S., Konstantina N.","Fused man-machine classification schemes to enhance diagnosis of breast microcalcifications",2017,"Measurement Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032196944&doi=10.1088%2f1361-6501%2faa884e&partnerID=40&md5=dd5506f2f1ec1f3ec9288970017ae857","Computer aided diagnosis (CADx) approaches are developed towards the effective discrimination between benign and malignant clusters of microcalcifications. Different sources of information are exploited, such as features extracted from the image analysis of the region of interest, features related to the location of the cluster inside the breast, age of the patient and descriptors provided by the radiologists while performing their diagnostic task. A series of different CADx schemes are implemented, each of which uses a different category of features and adopts a variety of machine learning algorithms and alternative image processing techniques. A novel framework is introduced where these independent diagnostic components are properly combined according to features critical to a radiologist in an attempt to identify the most appropriate CADx schemes for the case under consideration. An open access database (Digital Database of Screening Mammography (DDSM)) has been elaborated to construct a large dataset with cases of varying subtlety, in order to ensure the development of schemes with high generalization ability, as well as extensive evaluation of their performance. The obtained results indicate that the proposed framework succeeds in improving the diagnostic procedure, as the achieved overall classification performance outperforms all the independent single diagnostic components, as well as the radiologists that assessed the same cases, in terms of accuracy, sensitivity, specificity and area under the curve following receiver operating characteristic analysis. © 2017 IOP Publishing Ltd.","BIRADS descriptors; computer aided diagnosis; interaction with radiologist; microcalcifications","Image processing; Image segmentation; Learning algorithms; Learning systems; Medical imaging; Breast microcalcifications; Computeraided diagnoses (CADx); Descriptors; Digital database of screening mammographies; Image processing technique; interaction with radiologist; Microcalcifications; Receiver operating characteristic analysis; Computer aided diagnosis",2-s2.0-85032196944
"Zeiler F.A., Donnelly J., Cardim D., Menon D.K., Smielewski P., Czosnyka M.","ICP Versus Laser Doppler Cerebrovascular Reactivity Indices to Assess Brain Autoregulatory Capacity",2017,"Neurocritical Care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031495083&doi=10.1007%2fs12028-017-0472-x&partnerID=40&md5=7cdd56e026035d4da92e52f230d70834","Background: To explore the relationship between various autoregulatory indices in order to determine which approximate small vessel/microvascular (MV) autoregulatory capacity most accurately. Methods: Utilizing a retrospective cohort of traumatic brain injury patients (N = 41) with: transcranial Doppler (TCD), intracranial pressure (ICP) and cortical laser Doppler flowmetry (LDF), we calculated various continuous indices of autoregulation and cerebrovascular responsiveness: A. ICP derived [pressure reactivity index (PRx)—correlation between ICP and mean arterial pressure (MAP), PAx—correlation between pulse amplitude of ICP (AMP) and MAP, RAC—correlation between AMP and cerebral perfusion pressure (CPP)], B. TCD derived (Mx—correlation between mean flow velocity (FVm) and CPP, Mx_a—correlation between FVm and MAP, Sx—correlation between systolic flow velocity (FVs) and CPP, Sx_a—correlation between FVs and MAP, Dx—correlation between diastolic flow index (FVd) and CPP, Dx_a—correlation between FVd and MAP], and LDF derived (Lx—correlation between LDF cerebral blood flow [CBF] and CPP, Lx_a—correlation between LDF-CBF and MAP). We assessed the relationship between these indices via Pearson correlation, Friedman test, principal component analysis (PCA), agglomerative hierarchal clustering (AHC), and k-means cluster analysis (KMCA). Results: LDF-based autoregulatory index (Lx) was most associated with TCD-based Mx/Mx_a and Dx/Dx_a across Pearson correlation, PCA, AHC, and KMCA. Lx was only remotely associated with ICP-based indices (PRx, PAx, RAC). TCD-based Sx/Sx_a was more closely associated with ICP-derived PRx, PAx and RAC. This indicates that vascular-derived indices of autoregulatory capacity (i.e., TCD and LDF based) covary, with Sx/Sx_a being the exception, whereas indices of cerebrovascular reactivity derived from pulsatile CBV (i.e., ICP indices) appear to not be closely related to those of vascular origin. Conclusions: Transcranial Doppler Mx is the most closely associated with LDF-based Lx/Lx_a. Both Sx/Sx-a and the ICP-derived indices appear to be dissociated with LDF-based cerebrovascular reactivity, leaving Mx/Mx-a as a better surrogate for the assessment of cortical small vessel/MV cerebrovascular reactivity. Sx/Sx_a cocluster/covary with ICP-derived indices, as seen in our previous work. © 2017 The Author(s)","Autoregulation; Cerebrovascular reactivity; Covariance; ICP index; Laser Doppler; Machine learning",,2-s2.0-85031495083
"Paulo J., Peixoto P., Nunes U.J.","ISR-AIWALKER: Robotic Walker for Intuitive and Safe Mobility Assistance and Gait Analysis",2017,"IEEE Transactions on Human-Machine Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032255071&doi=10.1109%2fTHMS.2017.2759807&partnerID=40&md5=50a640a373a04b69875bf0e5c3d2f576","Robotic walkers are assistive robotic devices that provide mobility assistance, in a domestic or clinical scenario, to individuals suffering from a gait disorder, being age related or due to injuries, surgery, or diseases. Walkers also provide a significant potential for lower limb rehabilitation. In this paper, we present a novel multimodal robotic walker platform, the ISR-AIWALKER, where innovative contributions were made both in the human&#x2013;machine interface (HMI) and in a gait analysis system placed on board the platform. Taking into account the application potential of these devices, an effort was made to use low-cost sensors without sacrificing the overall performance of the system. A change was made in the HMI paradigm, moving from a force-sensing to a vision-based approach, while maintaining a natural user interaction and adding complementary safety features like correct gripping enforcement. To cope with the close proximity of the user&#x0027;s body, a multimodal sensor setup was considered. Using both RGB and depth map data, a kinematic model of the user&#x0027;s lower limbs is obtained, allowing the identification of a set of features that are used in a machine learning approach to discriminate gait asymmetries. Experiments made with several subjects revealed that the proposed HMI is able to correctly estimate the user intention in a natural and intuitive way. The gait analysis system was also evaluated and evidenced a good discrimination capability to distinguish between different gait patterns. IEEE","Assistive robots; Cameras; Force; gait analysis; human&#x2013;machine interface (HMI); Legged locomotion; Robot sensing systems; robotic walker; Safety; user intention","Accident prevention; Biological organs; Cameras; Kinematics; Learning systems; Robotics; Walking aids; Assistive robots; Force; Legged locomotion; Machine interfaces; Robot sensing system; Robotic walkers; User intention; Gait analysis",2-s2.0-85032255071
"Zhang X.","Melanoma segmentation based on deep learning",2017,"Computer Assisted Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031748547&doi=10.1080%2f24699322.2017.1389405&partnerID=40&md5=85f990bc01b91e550bf05734489611c7","Malignant melanoma is one of the most deadly forms of skin cancer, which is one of the world's fastest-growing cancers. Early diagnosis and treatment is critical. In this study, a neural network structure is utilized to construct a broad and accurate basis for the diagnosis of skin cancer, thereby reducing screening errors. The technique is able to improve the efficacy for identification of normally indistinguishable lesions (such as pigment spots) versus clinically unknown lesions, and to ultimately improve the diagnostic accuracy. In the field of medical imaging, in general, using neural networks for image segmentation is relatively rare. The existing traditional machine-learning neural network algorithms still cannot completely solve the problem of information loss, nor detect the precise division of the boundary area. We use an improved neural network framework, described herein, to achieve efficacious feature learning, and satisfactory segmentation of melanoma images. The architecture of the network includes multiple convolution layers, dropout layers, softmax layers, multiple filters, and activation functions. The number of data sets can be increased via rotation of the training set. A non-linear activation function (such as ReLU and ELU) is employed to alleviate the problem of gradient disappearance, and RMSprop/Adam are incorporated to optimize the loss algorithm. A batch normalization layer is added between the convolution layer and the activation layer to solve the problem of gradient disappearance and explosion. Experiments, described herein, show that our improved neural network architecture achieves higher accuracy for segmentation of melanoma images as compared with existing processes. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","convolutional neural networks; deep learning; image segmentation; Melanoma",,2-s2.0-85031748547
"Tzirakis P., Trigeorgis G., Nicolaou M.A., Schuller B., Zafeiriou S.","End-to-End Multimodal Emotion Recognition using Deep Neural Networks",2017,"IEEE Journal on Selected Topics in Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032261257&doi=10.1109%2fJSTSP.2017.2764438&partnerID=40&md5=d5196f12bbd360e3e351591200c32c95","Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where - by also taking advantage of the correlations of the each of the streams - we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition. IEEE","affective computing; deep learning; Emotion recognition; emotion recognition; end-to-end learning; Feature extraction; Hidden Markov models; Neural networks; Speech; Visualization","Deep learning; Deep neural networks; Extraction; Feature extraction; Flow visualization; Hidden Markov models; Human computer interaction; Learning algorithms; Learning systems; Long short-term memory; Markov processes; Neural networks; Speech; Affective Computing; Convolutional neural network; Emotion recognition; End to end; Multimedia Retrieval; Multimodal emotion recognition; Research challenges; Traditional approaches; Speech recognition",2-s2.0-85032261257
"White M.M., Zhang W., Winslow A.T., Zahabi M., Zhang F., Huang H., Kaber D.B.","Usability Comparison of Conventional Direct Control Versus Pattern Recognition Control of Transradial Prostheses",2017,"IEEE Transactions on Human-Machine Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032257100&doi=10.1109%2fTHMS.2017.2759762&partnerID=40&md5=e6342bbd2b8e98e2494c83604bffc84b","The goal of this study was to compare the usability of two control schemes for a transradial myoelectric prosthesis, including conventional direct control (DC) and pattern recognition (PR) control, when used by able-bodied individuals. Three types of response measures were captured to assess the control schemes, including learnability, performance, and cognitive workload. Prior research has applied performance and cognitive workload metrics for evaluation of prosthetics; however, workload measures applied in these studies (e.g., heart rate, electroencephalography, and respiration rate) have many limitations. This study used eye tracking to compare cognitive load implications of the different control schemes for a two degrees-of-freedom myoelectric prosthesis. In total, 12 participants were assigned to either control condition (six persons each) or perform a clothespin relocation task. Results revealed the PR scheme to be more intuitive for users and superior to DC across all response measures. We observed a lower learning percentage (i.e., greater learning potential), lower cognitive load, and greater productivity in task performance. This preliminary study illustrates efficacy of using eye-tracking-based measures of cognitive load and standardize test paradigms for assessment of upper limb prosthetic usability and supports PR prosthetic device control as an intuitive alternative to DC. IEEE","Atmospheric measurements; Biomedical signal; Electroencephalography; Human factors; Man-machine systems; Particle measurements; Pollution measurement; Prosthetics; prosthetics; Usability","Biomedical signal processing; Degrees of freedom (mechanics); Electroencephalography; Electrophysiology; Human engineering; Interactive computer systems; Man machine systems; Pattern recognition; Prosthetics; Usability engineering; Atmospheric measurement; Biomedical signal; Particle measurement; Pollution measurement; Usability; Myoelectrically controlled prosthetics",2-s2.0-85032257100
"Neeba E.A., Koteeswaran S., Malarvizhi N.","Swarm-based clustering algorithm for efficient web blog and data classification",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031503457&doi=10.1007%2fs11227-017-2162-z&partnerID=40&md5=35d47811843979bc35eba770531f44d9","Data classification and the weblog classification have become the most regular approach for people to express themselves. Data classification is another type of problem for classifying a feature set into several feature subsets, and those are further clustered into different classes on the basis of binary or multiclassification. Many problems in science and technology, industry and commercial business and medicine and health care can be treated as classification problems. In recent years, many methods are existing to build a classification model based on many statistical concepts and optimization methods. One major issue of building statistical model will have the principle to provide good accuracy simply when the principal assumptions are correct. The classification decision made on accuracy only justifies the performance of the particular model. Before applying the model to the particular application, it requires good perceptive of data utilized. In order to provide an effective learning algorithm to refine such complexity in handling the data and to minimize output errors and to provide the hands to improve the efficiency of the model, this research article is framed. In this work, a novel algorithm named ‘swarm-based cluster algorithm’ is proposed to complete the feature selection task in order to produce optimized feature-based clusters for effective data and weblogs classification. © 2017 Springer Science+Business Media, LLC","Blog classification; Data classification; k-means clustering and support vector machine (SVM); Particle swarm optimization (PSO); Swarm-based cluster algorithm (SBCA)","Blogs; Classification (of information); Data handling; Optimization; Particle swarm optimization (PSO); Support vector machines; Blog classifications; Classification decision; Classification models; Cluster algorithms; Data classification; K-means clustering; Multi-classification; Science and Technology; Clustering algorithms",2-s2.0-85031503457
"Rosa S., Toscana G., Bona B.","Q-PSO: Fast Quaternion-Based Pose Estimation from RGB-D Images",2017,"Journal of Intelligent and Robotic Systems: Theory and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496793&doi=10.1007%2fs10846-017-0714-3&partnerID=40&md5=2f9a7349d50a4b285a1198e9888bab88","We present a pipeline for fast object pose estimation using RGB-D images, which does not rely on image features or machine learning. We are interested in segmenting objects with large variety in app.earance, from lack of texture to presence of strong textures, with a focus on the task of robotic grasping. The proposed pipeline is divided into an object segmentation part and a pose estimation part. We first find candidate object clusters using a graph-based image segmentation technique. A modified Canny edge detector is introduced for extracting robust graph edges by fusing RGB and depth information. A suitable cost function is used for building the graph, which is then partitioned using the concept of internal and external differences between graph regions. The extracted object regions are then used to initialize the 3D position of a quaternion-based Particle Swarm Optimization algorithm (Q-PSO), that fits a 3D model of the object to the depth image. The fitness function is based on depth information only and the quaternion formulation avoids singularities and the need for conversions between rotation representations. In this work we focus on the details of the GPU implementation of Q-PSO, in order to fully exploit the highly parallelizable nature of the particular implementation of the particle swarm algorithm, and discuss critic implementation details. We then test the app.roach on different publicly available RGB-D object datasets, and provide numeric comparisons with other state-of-the-art methods, as well as a discussion on robustness and an extension to the case of articulated objects. We show how Q-PSO offers comparable performances to current learning-based app.roaches, while not suffering from the problems of lack of features in objects or issues related to training, such as the need for a large training set and long training times. © 2017 Springer Science+Business Media B.V.","GPU; Object pose estimation; Particle swarm optimization; Robotic grasping","Cost functions; End effectors; Graphic methods; Graphics processing unit; Image segmentation; Learning systems; Optimization; Pipelines; Robotics; Three dimensional computer graphics; Canny edge detectors; Graph-based image segmentations; Object pose; Object segmentation; Particle swarm algorithm; Particle swarm optimization algorithm; Robotic grasping; State-of-the-art methods; Particle swarm optimization (PSO)",2-s2.0-85031496793
"Ma L., Song D., Liao L., Ni Y.","A joint deep model of entities and documents for cumulative citation recommendation",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031494350&doi=10.1007%2fs10586-017-1273-x&partnerID=40&md5=08fc3b655a90c89f1da5e2b3b2e1614b","Knowledge bases (such as Wikipedia) are valuable resources of human knowledge which have contributed to various of applications. However, their manual maintenance makes a big lag between their contents and the up-to-date information of entities. Cumulative citation recommendation (CCR) concentrates on identifying worthy-citation documents from a large volume of stream data for a given target entity in knowledge bases. Most previous approaches first carefully extract human-designed features from entities and documents, and then leverage machine learning methods such as SVM and Random Forests to filter worthy-citation documents for target entities. There are some problems in handcraft features for entities and documents: (1) It is an empirical process that requires expert knowledge, thus cannot be easily generalized; (2) The effectiveness of humanly designed features has great effect on the performance; (3) The implementation of the feature extraction process is resource dependent and time-consuming. In this paper, we present a Joint Deep Neural Network Model of Entities and Documents for CCR, termed as DeepJoED, to identify highly related documents for given entities with several layers of neurons, by automatically learn feature extraction of the entities and documents, and train the networks in an end-to-end fashion.An extensive set of experiments have been conducted on the benchmark dataset provided in the Text REtrieval Conference (TREC) Knowledge base acceleration (KBA) task in 2012. The results show the model can bring a significant improvement relative to the state-of-the-art results on this dataset in CCR. © 2017 Springer Science+Business Media, LLC","Convolution Neural Networks; Cumulative citation recommendation; Knowledge base acceleration; Latent semantic representations; Word embedding","Decision trees; Extraction; Feature extraction; Knowledge based systems; Learning systems; Semantics; Convolution neural network; Cumulative citation recommendation; Knowledge base; Latent semantics; Word embedding; Deep neural networks",2-s2.0-85031494350
"Ren P., Chen W., Dai H., Zhang H.","Distributed Cooperative Learning over Networks via Fuzzy Logic Systems: Performance Analysis and Comparison",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032282537&doi=10.1109%2fTFUZZ.2017.2762285&partnerID=40&md5=d34f5c3d68ca0827446ef78dd3e41666","This paper studies a distributed machine learning problem by applying a distributed optimization algorithm over an undirected and connected communication network. Each node has its own fuzzy logic system (FLS) based machine whose weights are trained by the proposed FLS-based distributed cooperative learning (DCL) algorithm to reach the optimum of the global cost function. The training process utilizes the data that is distributed among different nodes and can not be gathered at any node in the network. The main advantages of the FLS-based DCL algorithm are: 1) it has an exponential convergence; 2) it requires a small amount of computation and communication at each iteration step; 3) the private and confidential information is protected without exchanging raw data between neighboring nodes. These advantages are verified by performing simulation experiments to compare the FLS-based DCL algorithm with the distributed average consensus (DAC) based learning algorithm, the alternating direction method of multipliers (ADMM) based learning algorithm and the diffusion least-mean square (LMS) algorithms. IEEE","Approximation algorithms; Communication networks; Consensus; Convergence; Distributed cooperative learning; Distributed machine learning; Eigenvalues and eigenfunctions; Exponential convergence; Fuzzy logic; Fuzzy logic systems; Lyapunov method; Machine learning algorithms; Peer-to-peer computing","Approximation algorithms; Artificial intelligence; Computation theory; Computer circuits; Cooperative communication; Cost functions; Distributed computer systems; Eigenvalues and eigenfunctions; Fuzzy logic; Iterative methods; Learning systems; Least squares approximations; Lyapunov methods; Optimization; Peer to peer networks; Telecommunication networks; Consensus; Convergence; Cooperative learning; Distributed machine learning; Exponential convergence; Fuzzy logic system; Peer-to-peer computing; Learning algorithms",2-s2.0-85032282537
"Sarkar R., Acton S.T.","SDL: Saliency based dictionary learning framework for image similarity",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032296125&doi=10.1109%2fTIP.2017.2763829&partnerID=40&md5=324f38e767391265514346c90d9aca33","In image classification, obtaining adequate data to learn a robust classifier has often proven to be difficult in several scenarios. Classification of histological tissue images for health care analysis is a notable application in this context due to the necessity of surgery, biopsy or autopsy. To adequately exploit limited training data in classification, we propose a saliency guided dictionary learning method and subsequently an image similarity technique for histo-pathological image classification. Salient object detection from images aids in the identification of discriminative image features. We leverage the saliency values for the local image regions to learn a dictionary and respective sparse codes for an image, such that the more salient features are reconstructed with smaller error. The dictionary learned from an image gives a compact representation of the image itself and is capable of representing images with similar content, with comparable sparse codes. We employ this idea to design a similarity measure between a pair of images, where local image features of one image, are encoded with the dictionary learned from the other and vice versa. To effectively utilize the learned dictionary, we take into account the contribution of each dictionary atom in the sparse codes to generate a global image representation for image comparison. The efficacy of the proposed method was evaluated using three tissue datasets that consist of mammalian kidney, lung and spleen tissue, breast cancer and colon cancer tissue images. From the experiments, we observe that our methods outperform the state of the art with an increase of 14.2&#x0025; in the average classification accuracy over all datasets. IEEE","Algorithm design and analysis; Dictionaries; dictionary learning; Feature extraction; Image coding; Image segmentation; image similarity; Machine learning; Object detection; saliency; sparse representation; tissue image classification","Classification (of information); Codes (symbols); Diseases; Feature extraction; Glossaries; Image analysis; Image coding; Image processing; Image segmentation; Learning systems; Mammals; Medical imaging; Object detection; Object recognition; Tissue; Algorithm design and analysis; Dictionary learning; Image similarity; saliency; Sparse representation; Tissue images; Image classification",2-s2.0-85032296125
"Kléma J., Malinka F., železný F.","Semantic biclustering for finding local, interpretable and predictive expression patterns",2017,"BMC Genomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031494977&doi=10.1186%2fs12864-017-4132-5&partnerID=40&md5=4b1f475f01e3b5509656c8509cf7237e","Background: One of the major challenges in the analysis of gene expression data is to identify local patterns composed of genes showing coherent expression across subsets of experimental conditions. Such patterns may provide an understanding of underlying biological processes related to these conditions. This understanding can further be improved by providing concise characterizations of the genes and situations delimiting the pattern. Results: We propose a method called semantic biclustering with the aim to detect interpretable rectangular patterns in binary data matrices. As usual in biclustering, we seek homogeneous submatrices, however, we also require that the included elements can be jointly described in terms of semantic annotations pertaining to both rows (genes) and columns (samples). To find such interpretable biclusters, we explore two strategies. The first endows an existing biclustering algorithm with the semantic ingredients. The other is based on rule and tree learning known from machine learning. Conclusions: The two alternatives are tested in experiments with two Drosophila melanogaster gene expression datasets. Both strategies are shown to detect sets of compact biclusters with semantic descriptions that also remain largely valid for unseen (testing) data. This desirable generalization aspect is more emphasized in the strategy stemming from conventional biclustering although this is traded off by the complexity of the descriptions (number of ontology terms employed), which, on the other hand, is lower for the alternative strategy. © 2017 The Author(s).","Biclustering; Enrichment analysis; Gene expression; Ontology; Symbolic machine learning","Drosophila melanogaster; gene expression; machine learning; nonhuman; ontology",2-s2.0-85031494977
"Ardakani A., Condo C., Ahmadi M., Gross W.J.","An Architecture to Accelerate Convolution in Deep Neural Networks",2017,"IEEE Transactions on Circuits and Systems I: Regular Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032305448&doi=10.1109%2fTCSI.2017.2757036&partnerID=40&md5=9b34a90fb9f137e59700f07946b39044","In the past few years, the demand for real-time hardware implementations of deep neural networks (DNNs), especially convolutional neural networks (CNNs), has dramatically increased, thanks to their excellent performance on a wide range of recognition and classification tasks. When considering real-time action recognition and video/image classification systems, latency is of paramount importance. Therefore, applications strive to maximize the accuracy while keeping the latency under a given application-specific maximum: in most cases, this threshold cannot exceed a few hundred milliseconds. Until now, the research on DNNs has mainly focused on achieving a better classification or recognition accuracy, whereas very few works in literature take in account the computational complexity of the model. In this paper, we propose an efficient computational method, which is inspired by a computational core of fully connected neural networks, to process convolutional layers of state-of-the-art deep CNNs within strict latency requirements. To this end, we implemented our method customized for VGG and VGG-based networks which have shown state-of-the-art performance on different classification/recognition data sets. The implementation results in 65-nm CMOS technology show that the proposed accelerator can process convolutional layers of VGGNet up to 9.5 times faster than state-of-the-art accelerators reported to-date while occupying 3.5 mm&#x00B2;. IEEE","Complexity theory; Computer architecture; Convolution; Convolutional neural networks; deep neural network; Hardware; hardware implementation; machine learning; Neural networks; Neurons; pattern recognition; Three-dimensional displays; very large scale integration (VLSI)","Classification (of information); Complex networks; Computational complexity; Computer architecture; Computer hardware; Convolution; Hardware; Learning systems; Network architecture; Neural networks; Neurons; Pattern recognition; Real time systems; VLSI circuits; Application specific; Classification system; Complexity theory; Convolutional neural network; Fully connected neural network; Hardware implementations; State-of-the-art performance; Three-dimensional display; Deep neural networks",2-s2.0-85032305448
"Vijayan V., Yiu S.-M., Zhang L.","Improving somatic variant identification through integration of genome and exome data",2017,"BMC Genomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497103&doi=10.1186%2fs12864-017-4134-3&partnerID=40&md5=b1158f47779091b576f9855cabfae6bc","Background: Cost-effective high-throughput sequencing technologies, together with efficient mapping and variant calling tools, have made it possible to identify somatic variants for cancer study. However, integrating somatic variants from whole exome and whole genome studies poses a challenge to researchers as the variants identified by whole genome analysis may not be identified by whole exome analysis and vice versa. Simply taking the union or intersection of the results may lead to too many false positives or too many false negatives. Results: To tackle this problem, we use machine learning models to integrate whole exome and whole genome calling results from two representative tools, VCMM (with the highest sensitivity but very low precision) and MuTect (with the highest precision). The evaluation results, based on both simulated and real data, show that our framework improves somatic variant calling, and is more accurate in identifying somatic variants than either individual method used alone or using variants identified from only whole genome data or only whole exome data. Conclusion: Using machine learning approach to combine results from multiple calling methods on multiple data platforms (e.g., genome and exome) enables more accurate identification of somatic variants. © 2017 The Author(s).","Framework for combining results from tools; Genome and exome analysis; Somatic variants","exome; human; machine learning; simulation",2-s2.0-85031497103
"Wei L., Tang J., Zou Q.","SkipCPP-Pred: An improved and promising sequence-based predictor for predicting cell-penetrating peptides",2017,"BMC Genomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031509034&doi=10.1186%2fs12864-017-4128-1&partnerID=40&md5=f8ccbc25008311080d7f324e31109202","Background: Cell-penetrating peptides (CPPs) are short peptides (5-30 amino acids) that can enter almost any cell without significant damage. On account of their high delivery efficiency, CPPs are promising candidates for gene therapy and cancer treatment. Accordingly, techniques that correctly predict CPPs are anticipated to accelerate CPP applications in future therapeutics. Recently, computational methods have been reportedly successful in predicting CPPs. Unfortunately, the predictive performance of existing methods is not satisfactory and reliable so as to accurately identify CPPs. Results: In this study, we propose a novel computational predictor called SkipCPP-Pred to further improve the predictive performance. The novelty of the proposed predictor is that we present a sequence-based feature representation algorithm called adaptive k-skip-n-gram that sufficiently captures the intrinsic correlation information of residues. By fusing the proposed adaptive skip features with a random forest (RF) classifier, we successfully construct the prediction model of SkipCPP-Pred. The various jackknife results demonstrate that the proposed SkipCPP-Pred is 3.6% higher than state-of-the-art CPP predictors in terms of accuracy. Moreover, we construct a high-quality benchmark dataset by reducing the data redundancy and enhancing the similarity between the positive and negative classes. Using this dataset to build prediction models, we can successfully avoid the performance bias lying in existing methods and yield a promising predictive model. Conclusions: The proposed SkipCPP-Pred is a simple and fast sequence-based predictor featured with the adaptive k-skip-n-gram model for the improved prediction of CPPs. Currently, SkipCPP-Pred is publicly available from an online webserver (http://server.malab.cn/SkipCPP-Pred/Index.html). © 2017 The Author(s).","Adaptive k-skip-n-gram features; Cell-penetrating peptide; Machine learning","cell penetrating peptide; Article; benchmarking; classifier; computer prediction; factual database; information processing; mathematical parameters; measurement accuracy; online system; predictive value; process optimization; random forest; SkipCPP Pred database",2-s2.0-85031509034
"Rouze C., Datta N.","Finite blocklength and moderate deviation analysis of hypothesis testing of correlated quantum states and application to classical-quantum channels with memory",2017,"IEEE Transactions on Information Theory",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032172199&doi=10.1109%2fTIT.2017.2763975&partnerID=40&md5=e1748a718ed1fc4be701879cac22d661","Martingale concentration inequalities constitute a powerful mathematical tool in the analysis of problems in a wide variety of fields ranging from probability and statistics to information theory and machine learning. Here we apply techniques borrowed from this field to quantum hypothesis testing, which is the problem of discriminating quantum states belonging to two different sequences, &#x007B; &#x03C1; n&#x007D;n and &#x007B; &#x03C3; n&#x007D;n. We obtain achievability bounds on the finite blocklength type II Stein- and Hoeffding errors which, for i.i.d. states, are in general tighter than the corresponding bounds obtained by Audenaert, Mosonyi and Verstraete in &#x005B;5&#x005D;. We also derive finite blocklength bounds and moderate deviation results for pairs of sequences of correlated states satisfying a (non-homogeneous) factorization property. Examples of such sequences include Gibbs states of spin chains with translationinvariant finite range interaction, as well as finitely correlated quantum states. We apply our results to find bounds on the capacity of a certain class of classical-quantum channels with memory, which satisfy a so-called channel factorization property - both in the finite blocklength and moderate deviation regimes. IEEE","Entropy; Error probability; Hilbert space; Information theory; Quantum mechanics; Random variables; Testing","Communication channels (information theory); Entropy; Factorization; Hilbert spaces; Information theory; Learning systems; Quantum entanglement; Quantum optics; Random variables; Testing; Classical-quantum channels; Concentration inequality; Error probabilities; Mathematical tools; Moderate-deviations; Probability and statistics; Quantum hypothesis; Translation invariants; Quantum theory",2-s2.0-85032172199
"Dey R., Roy A., Chakraborty T., Ghosh S.","Sleeping beauties in Computer Science: characterization and early identification",2017,"Scientometrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031938375&doi=10.1007%2fs11192-017-2543-3&partnerID=40&md5=7668c9e8e8e0497897e04720de934a69","While a large majority of scientific publications get most of their citations within the initial few years after publication, there is an interesting number of papers—termed as sleeping beauties—which do not get much cited for several years after being published, but then suddenly start getting cited heavily. In this work, we focus on sleeping beauties (SBs) in the domain of Computer Science. We identify more than 5,000 sleeping beauties in Computer Science, and characterise them based on their sub-field and their citation profile after awakening. We also reveal some interesting factors which led to their awakening long after publication. Furthermore, we also propose a methodology for early identification of sleeping beauties, and develop a machine learning-based classification approach that attempts to classify publications based on whether they are likely to be SBs. The classifier achieves a precision of 0.73 and a recall of 0.45 in identifying SBs immediately after their year of publications, and the performance significantly improves with time. To our knowledge, this is the first study on sleeping beauties in Computer Science. © 2017 Akadémiai Kiadó, Budapest, Hungary","Citation networks; Classification; Prince of sleeping beauty; Sleeping beauties",,2-s2.0-85031938375
"Kiluk S.","Diagnostic information system dynamics in the evaluation of machine learning algorithms for the supervision of energy efficiency of district heating-supplied buildings",2017,"Energy Conversion and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019049599&doi=10.1016%2fj.enconman.2017.05.006&partnerID=40&md5=fdfd2770d971af06aa612eda3edd1808","Modern ways of exploring the diagnostic knowledge provided by data mining and machine learning raise some concern about the ways of evaluating the quality of output knowledge, usually represented by information systems. Especially in district heating, the stationarity of efficiency models, and thus the relevance of diagnostic classification system, cannot be ensured due to the impact of social, economic or technological changes, which are hard to identify or predict. Therefore, data mining and machine learning have become an attractive strategy for automatically and continuously absorbing such dynamics. This paper presents a new method of evaluation and comparison of diagnostic information systems gathered algorithmically in district heating efficiency supervision based on exploring the evolution of information system and analyzing its dynamic features. The process of data mining and knowledge discovery was applied to the data acquired from district heating substations’ energy meters to provide the automated discovery of diagnostic knowledge base necessary for the efficiency supervision of district heating-supplied buildings. The implemented algorithm consists of several steps of processing the billing data, including preparation, segmentation, aggregation and knowledge discovery stage, where classes of abstract models representing energy efficiency constitute an information system representing diagnostic knowledge about the energy efficiency of buildings favorably operating under similar climate conditions and supplied from the same district heating network. The authors analyzed the evolution of a series of information systems originating from the same knowledge discovery algorithm applied to a sequence of energy consumption-related data. Specifically, the rough sets theory was applied to describe the knowledge base and measure the uncertainty of machine learning predictions of current classification based on a past knowledge base. Fluctuations of diagnostic class membership were identified and provided for the differentiation between returning and novel fault detections, thus introducing the qualities of information system uncertainty and its sustainability. The usability of the new method was demonstrated in the comparison of results for exemplary data mining algorithms implemented on real data from over one thousand buildings. © 2017","District heating; Energy efficiency; Entropy; Information system dynamics; Machine learning","Artificial intelligence; Data handling; District heating; Energy efficiency; Energy utilization; Entropy; Heating; Information systems; Knowledge based systems; Learning algorithms; Learning systems; System theory; Attractive strategies; Classification system; Data mining algorithm; Data mining and knowledge discovery; Diagnostic knowledge; Diagnostic knowledge base; Heating efficiencies; Technological change; Data mining",2-s2.0-85019049599
"Barboza F., Kimura H., Altman E.","Machine learning models and bankruptcy prediction",2017,"Expert Systems with Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019113920&doi=10.1016%2fj.eswa.2017.04.006&partnerID=40&md5=0b208261ae80f3c61c490025aefb6a0d","There has been intensive research from academics and practitioners regarding models for predicting bankruptcy and default events, for credit risk management. Seminal academic research has evaluated bankruptcy using traditional statistics techniques (e.g. discriminant analysis and logistic regression) and early artificial intelligence models (e.g. artificial neural networks). In this study, we test machine learning models (support vector machines, bagging, boosting, and random forest) to predict bankruptcy one year prior to the event, and compare their performance with results from discriminant analysis, logistic regression, and neural networks. We use data from 1985 to 2013 on North American firms, integrating information from the Salomon Center database and Compustat, analysing more than 10,000 firm-year observations. The key insight of the study is a substantial improvement in prediction accuracy using machine learning techniques especially when, in addition to the original Altman's Z-score variables, we include six complementary financial indicators. Based on Carton and Hofer (2006), we use new variables, such as the operating margin, change in return-on-equity, change in price-to-book, and growth measures related to assets, sales, and number of employees, as predictive variables. Machine learning models show, on average, approximately 10% more accuracy in relation to traditional models. Comparing the best models, with all predictive variables, the machine learning technique related to random forest led to 87% accuracy, whereas logistic regression and linear discriminant analysis led to 69% and 50% accuracy, respectively, in the testing sample. We find that bagging, boosting, and random forest models outperform the others techniques, and that all prediction accuracy in the testing sample improves when the additional variables are included. Our research adds to the discussion of the continuing debate about superiority of computational methods over statistical techniques such as in Tsai, Hsu, and Yen (2014) and Yeh, Chi, and Lin (2014). In particular, for machine learning mechanisms, we do not find SVM to lead to higher accuracy rates than other models. This result contradicts outcomes from Danenas and Garsva (2015) and Cleofas-Sanchez, Garcia, Marques, and Senchez (2016), but corroborates, for instance, Wang, Ma, and Yang (2014), Liang, Lu, Tsai, and Shih (2016), and Cano et al. (2017). Our study supports the applicability of the expert systems by practitioners as in Heo and Yang (2014), Kim, Kang, and Kim (2015) and Xiao, Xiao, and Wang (2016). © 2017 Elsevier Ltd","Bagging; Bankruptcy prediction; Boosting; Machine learning; Random forest; Support vector machines",,2-s2.0-85019113920
"Duguay T.M., Tesche C., Vliegenthart R., De Cecco C.N., Lin H., Albrecht M.H., Varga-Szemes A., De Santis D., Ebersberger U., Bayer R.R., 2nd, Litwin S.E., Hoffmann E., Steinberg D.H., Schoepf U.J.","Coronary Computed Tomographic Angiography-Derived Fractional Flow Reserve Based on Machine Learning for Risk Stratification of Non-Culprit Coronary Narrowings in Patients with Acute Coronary Syndrome",2017,"American Journal of Cardiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028328241&doi=10.1016%2fj.amjcard.2017.07.008&partnerID=40&md5=0d68e47d0b7d61adcde64e0c3158ea08","This study investigated the prognostic value of coronary computed tomography angiography (cCTA)-derived fractional flow reserve (CT-FFR) in patients with acute coronary syndrome (ACS) and multivessel disease to gauge significance and guide management of non-culprit lesions. We retrospectively analyzed data of 48 patients (56 ± 10 years, 60% men) who were admitted for symptoms suggestive of ACS and underwent dual-source cCTA followed by invasive coronary angiography with culprit lesion intervention. Culprit lesions were retrospectively identified on cCTA using images obtained during invasive coronary angiography. Non-culprit lesions with ≥25% luminal stenosis and deferred intervention were evaluated using a machine learning CT-FFR algorithm to determine lesion-specific ischemia (CT-FFR ≤0.80). Follow-up was performed. CT-FFR identified lesion-specific ischemia in 23 of 81 non-culprit lesions. After a median follow-up of 19.5 months, 14 patients (29%) had major adverse cardiac events (MACE). Univariate Cox regression analysis revealed that CT-FFR ≤0.80 (hazard ratio [HR] 3.77 [95% confidence interval 1.16 to 12.29], p = 0.027), Framingham risk score (FRS) (HR 2.96 [1.01 to 7.63], p = 0.038), and a CAD-RADS classification ≥3 (HR 3.12 [1.03 to 10.17], p = 0.051) were predictors of MACE. In a risk-adjusted model controlling for FRS and CAD-RADS ≥3, CT-FFR ≤0.80 remained a predictor of MACE (1.56 [1.01 to 2.83], p = 0.048). Receiver operating characteristics analysis including FRS, CAD-RADS ≥ 3, and CT-FFR ≤0.80 (area under the curve 0.78) showed incremental discriminatory power over FRS alone (area under the curve 0.66, p = 0.032). CT-FFR of non-culprit lesions in patients with ACS and multivessel disease adds prognostic value to identify risk of future MACE. © 2017 Elsevier Inc.",,"acute coronary syndrome; adult; Article; cardiovascular disease assessment; clinical article; computed tomographic angiography; computed tomography scanner; coronary angiography; coronary artery disease reporting and data system; coronary artery obstruction; coronary computed tomography angiography; diagnostic test accuracy study; female; follow up; fractional flow reserve; Framingham risk score; heart muscle ischemia; human; machine learning; major adverse cardiac event; male; priority journal; prognosis; receiver operating characteristic; retrospective study; risk assessment; risk factor; sensitivity and specificity; acute coronary syndrome; complication; computed tomographic angiography; coronary angiography; Coronary Stenosis; epidemiology; fractional flow reserve; middle aged; mortality; pathophysiology; physiology; procedures; survival rate; trends; United States; Acute Coronary Syndrome; Computed Tomography Angiography; Coronary Angiography; Coronary Stenosis; Female; Fractional Flow Reserve, Myocardial; Humans; Machine Learning; Male; Middle Aged; Prognosis; Retrospective Studies; Risk Assessment; ROC Curve; Survival Rate; United States",2-s2.0-85028328241
"Lin L., Wang F., Xie X., Zhong S.","Random forests-based extreme learning machine ensemble for multi-regime time series prediction",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018887458&doi=10.1016%2fj.eswa.2017.04.013&partnerID=40&md5=d31cb5365befc505cdb2a5e6cde33a95","Accurate and timely predicting values of performance parameters are currently strongly needed for important complex equipment in engineering. In time series prediction, two problems are urgent to be solved. One problem is how to achieve the accuracy, stability and efficiency together, and the other is how to handle time series with multiple regimes. To solve these two problems, random forests-based extreme learning machine ensemble model and a novel multi-regime approach are proposed respectively, and these two approaches can be integrated to achieve better performance. First, the extreme learning machine (ELM) is used in the proposed model because of its efficiency. Then the regularized ELM and ensemble learning strategy are used to improve generalization performance and prediction accuracy. The bootstrap sampling technique is used to generate training sample sets for multiple base-level ELM models, and then the random forests (RF) model is used as the combiner to aggregate these ELM models to achieve more accurate and stable performance. Next, based on the specific properties of turbofan engine time series, a multi-regime approach is proposed to handle it. Regimes are first separated, then the proposed RF-based ELM ensemble model is used to learn models of all regimes, individually, and last, all the learned regime models are aggregated to predict performance parameter at the future timestamp. The proposed RF-based ELM ensemble model and multi-regime approaches are evaluated by using NN3 time series and NASA turbofan engine time series, and then the proposed model is applied to the exhaust gas temperature prediction of CFM engine. The results demonstrate that the proposed RF-based ELM ensemble model and multi-regime approach can be accurate, stable and efficient in predicting multi-regime time series, and it can be robust against overfitting. © 2017 Elsevier Ltd","Ensemble learning; Extreme learning machine; Multi-regime time series; Random forests; Time series prediction","Decision trees; Efficiency; Engines; Knowledge acquisition; Learning systems; NASA; Problem solving; Time series; Ensemble learning; Exhaust gas temperature prediction; Extreme learning machine; Generalization performance; Performance parameters; Prediction accuracy; Random forests; Time series prediction; Forecasting",2-s2.0-85018887458
"Dozmorov M.G.","Epigenomic annotation-based interpretation of genomic data: From enrichment analysis to machine learning",2017,"Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031821115&doi=10.1093%2fbioinformatics%2fbtx414&partnerID=40&md5=e57fa4ac20e9b0ef5315508444c1e864","Motivation One of the goals of functional genomics is to understand the regulatory implications of experimentally obtained genomic regions of interest (ROIs). Most sequencing technologies now generate ROIs distributed across the whole genome. The interpretation of these genome-wide ROIs represents a challenge as the majority of them lie outside of functionally well-defined protein coding regions. Recent efforts by the members of the International Human Epigenome Consortium have generated volumes of functional/regulatory data (reference epigenomic datasets), effectively annotating the genome with epigenomic properties. Consequently, a wide variety of computational tools has been developed utilizing these epigenomic datasets for the interpretation of genomic data. Results The purpose of this review is to provide a structured overview of practical solutions for the interpretation of ROIs with the help of epigenomic data. Starting with epigenomic enrichment analysis, we discuss leading tools and machine learning methods utilizing epigenomic and 3D genome structure data. The hierarchy of tools and methods reviewed here presents a practical guide for the interpretation of genome-wide ROIs within an epigenomic context. Contact mikhail.dozmorov@vcuhealth.org Supplementary informationSupplementary dataare available at Bioinformatics online. © The Author 2017. Published by Oxford University Press. All rights reserved.",,,2-s2.0-85031821115
"Beliakov G., Gómez D., James S., Montero J., Rodríguez J.T.","Approaches to learning strictly-stable weights for data with missing values",2017,"Fuzzy Sets and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013665918&doi=10.1016%2fj.fss.2017.02.003&partnerID=40&md5=5d7d16640a1484886d89e03fe3487fe6","The problem of missing data is common in real-world applications of supervised machine learning such as classification and regression. Such data often gives rise to the need for functions defined for varying dimension. Here we propose optimization methods for learning the weights of quasi-arithmetic means in the context of data with missing values. We investigate some alternative approaches depending on the number of variables that have missing values and show results for several numerical experiments. © 2017 Elsevier B.V.","Aggregation functions; Linear programming; Missing data; Strict stability; Weight learning","Learning systems; Supervised learning; Aggregation functions; Approaches to learning; Missing data; Numerical experiments; Quasiarithmetic means; Strict stabilities; Supervised machine learning; Weight learning; Linear programming",2-s2.0-85013665918
"Hess M., Lenz S., Blätte T.J., Bullinger L., Binder H.","Partitioned learning of deep Boltzmann machines for SNP data",2017,"Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031794551&doi=10.1093%2fbioinformatics%2fbtx408&partnerID=40&md5=36c6dbf68dadf98bc7e266566ff6ada7","Motivation Learning the joint distributions of measurements, and in particular identification of an appropriate low-dimensional manifold, has been found to be a powerful ingredient of deep leaning approaches. Yet, such approaches have hardly been applied to single nucleotide polymorphism (SNP) data, probably due to the high number of features typically exceeding the number of studied individuals. Results After a brief overview of how deep Boltzmann machines (DBMs), a deep learning approach, can be adapted to SNP data in principle, we specifically present a way to alleviate the dimensionality problem by partitioned learning. We propose a sparse regression approach to coarsely screen the joint distribution of SNPs, followed by training several DBMs on SNP partitions that were identified by the screening. Aggregate features representing SNP patterns and the corresponding SNPs are extracted from the DBMs by a combination of statistical tests and sparse regression. In simulated case-control data, we show how this can uncover complex SNP patterns and augment results from univariate approaches, while maintaining type 1 error control. Time-to-event endpoints are considered in an application with acute myeloid leukemia patients, where SNP patterns are modeled after a pre-screening based on gene expression data. The proposed approach identified three SNPs that seem to jointly influence survival in a validation dataset. This indicates the added value of jointly investigating SNPs compared to standard univariate analyses and makes partitioned learning of DBMs an interesting complementary approach when analyzing SNP data. Availability and implementation A Julia package is provided at 'http://github.com/binderh/BoltzmannMachines.jl'. Contact binderh@imbi.uni-freiburg.de Supplementary informationSupplementary dataare available at Bioinformatics online. © The Author 2017. Published by Oxford University Press. All rights reserved.",,,2-s2.0-85031794551
"Martins R.G., Martins A.S., Neves L.A., Lima L.V., Flores E.L., do Nascimento M.Z.","Exploring polynomial classifier to predict match results in football championships",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018636158&doi=10.1016%2fj.eswa.2017.04.040&partnerID=40&md5=8effbfd84512954f9412e19d4a06f085","Football is the team sport that mostly attracts great mass audience. Because of the detailed information about all football matches of championships over almost a century, matches build a huge and valuable database to test prediction of matches results. The problem of modeling football data has become increasingly popular in the last years and learning machine have been used to predict football matches results in many studies. Our present work brings a new approach to predict matches results of championships. This approach investigates data of matches in order to predict the results, which are win, draw and defeat. The investigated groups were different type of combinations of two by two pairs, win-draw, win-defeat and draw-defeat, of the possible matches results of each championship. In this study we employed the features obtained by scouts during a football match. The proposed system applies a polynomial algorithm to analyse and define matches results. Some machine-learning algorithms were compared with our approach, which includes experiments with information obtained from the football championships. The association between polynomial algorithm and machine learning techniques allowed a significant increase of the accuracy values. Our polynomial algorithm provided an accuracy superior to 96%, selecting the relevant features from the training and testing set. © 2017 Elsevier Ltd","Feature selection; Football championship; Machine learning; Polynomial classifier; Prediction","Artificial intelligence; Feature extraction; Forecasting; Learning systems; Polynomials; Sports; Football championship; Learning machines; Machine learning techniques; New approaches; Polynomial algorithm; Polynomial classifier; Relevant features; Training and testing; Learning algorithms",2-s2.0-85018636158
"Silva R.M., Alberto T.C., Almeida T.A., Yamakami A.","Towards filtering undesired short text messages using an online learning approach with semantic indexing",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018431948&doi=10.1016%2fj.eswa.2017.04.055&partnerID=40&md5=96085c7c3fa8fb2ae68a8502b9412bdb","The popularity and reach of short text messages commonly used in electronic communication have led spammers to use them to propagate undesired content. This is often composed by misleading information, advertisements, viruses, and malwares that can be harmful and annoying to users. The dynamic nature of spam messages demands for knowledge-based systems with online learning and, therefore, the most traditional text categorization techniques can not be used. In this study, we introduce the MDLText, a text classifier based on the minimum description length principle, to the context of filtering undesired short text messages. The proposed approach supports incremental learning and, therefore, its predictive model is scalable and can adapt to continuously evolving spamming techniques. It is also fast, with computational cost increasing linearly with the number of samples and features, which is very desirable for expert systems applied to real-time electronic communication. In addition to the dynamic nature of these messages, they are also short and usually poorly written, rife with slangs, symbols, and abbreviations that difficult text representation, learning, and filtering. In this scenario, we also investigated the benefits of using text normalization and semantic indexing techniques. We showed these techniques can improve the text content quality and, consequently, enhance the performance of the expert systems for spamming detection. Based on these findings, we propose a new hybrid ensemble approach that combines the predictions obtained by the classifiers using the original text samples along with their variations created by applying text normalization and semantic indexing techniques. It has the advantages of being independent of the classification method and the results indicated it is efficient to filter undesired short text messages. © 2017 Elsevier Ltd","Machine learning; Minimum description length; Semantic indexing; Short text messages; Text categorization","Classification (of information); E-learning; Electronic mail filters; Expert systems; Indexing (of information); Information dissemination; Knowledge based systems; Learning systems; Online systems; Real time systems; Semantics; Spamming; Viruses; Electronic communications; Minimum description length; Minimum description length principle; Misleading informations; Semantic indexing; Semantic indexing techniques; Short text messages; Text categorization; Text processing",2-s2.0-85018431948
"Gauthama Raman M.R., Somu N., Kirthivasan K., Liscano R., Shankar Sriram V.S.","An efficient intrusion detection system based on hypergraph - Genetic algorithm for parameter optimization and feature selection in support vector machine",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028026731&doi=10.1016%2fj.knosys.2017.07.005&partnerID=40&md5=981ce810eb2b700a0c3f2632a33bef49","Realization of the importance for advanced tool and techniques to secure the network infrastructure from the security risks has led to the development of many machine learning based intrusion detection techniques. However, the benefits and limitations of these techniques make the development of an efficient Intrusion Detection System (IDS), an open challenge. This paper presents an adaptive, and a robust intrusion detection technique using Hypergraph based Genetic Algorithm (HG - GA) for parameter setting and feature selection in Support Vector Machine (SVM). Hyper – clique property of Hypergraph was exploited for the generation of initial population to fasten the search for the optimal solution and to prevent the trap at the local minima. HG-GA uses a weighted objective function to maintain the trade-off between maximizing the detection rate and minimizing the false alarm rate, along with the optimal number of features. The performance of HG-GA SVM was evaluated using NSL-KDD intrusion dataset under two scenarios (i) All features and (ii) informative features obtained from HG – GA. Experimental results show the prominence of HG-GA SVM over the existing techniques in terms of classifier accuracy, detection rate, false alarm rate, and runtime analysis. © 2017","Feature subset; Genetic algorithm; Hypergraph; Intrusion detection system; Kernel parameters; Support vector machine","Computer crime; Economic and social effects; Errors; Feature extraction; Genetic algorithms; Image resolution; Learning systems; Mercury (metal); Network security; Optimization; Parameter estimation; Support vector machines; Feature subset; Hypergraph; Initial population; Intrusion Detection Systems; Kernel parameter; Network infrastructure; Parameter optimization; Weighted objective function; Intrusion detection",2-s2.0-85028026731
"Chong E., Han C., Park F.C.","Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018926461&doi=10.1016%2fj.eswa.2017.04.030&partnerID=40&md5=6e92b5ac83c8b98a0e092654700c9fef","We offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction. Its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies. Deep learning algorithms vary considerably in the choice of network structure, activation function, and other model parameters, and their performance is known to depend heavily on the method of data representation. Our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction. Using high-frequency intraday stock returns as input data, we examine the effects of three unsupervised feature extraction methods—principal component analysis, autoencoder, and the restricted Boltzmann machine—on the network's overall ability to predict future market behavior. Empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance; the same cannot be said when the autoregressive model is applied to the residuals of the network. Covariance estimation is also noticeably improved when the predictive network is applied to covariance-based market structure analysis. Our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction. © 2017 Elsevier Ltd","Covariance estimation; Deep learning; Multilayer neural network; Stock market prediction","Commerce; Deep learning; Deep neural networks; Electronic trading; Feature extraction; Finance; Financial data processing; Financial markets; Forecasting; Investments; Learning systems; Multilayer neural networks; Principal component analysis; Auto regressive models; Covariance estimation; Feature extraction methods; Intraday stock returns; Prediction performance; Restricted boltzmann machine; Stock market analysis; Stock market prediction; Learning algorithms",2-s2.0-85018926461
"Maggu J., Majumdar A.","Kernel transform learning",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029411949&doi=10.1016%2fj.patrec.2017.09.002&partnerID=40&md5=471cb971fec8e2e7a344f94b569fe88a","This work proposes kernel transform learning. The idea of dictionary learning is well known; it is a synthesis formulation where a basis is learnt along with the coefficients so as to generate/synthesize the data. Transform learning is its analysis equivalent; the transforms operates/analyses on the data to generate the coefficients. The concept of kernel dictionary learning has been introduced in the recent past, where the dictionary is represented as a linear combination of non-linear version of the data. Its success has been showcased in feature extraction. In this work we propose to kernelize transform learning on line similar to kernel dictionary learning. An efficient solution for kernel transform learning has been proposed – especially for problems where the number of samples is much larger than the dimensionality of the input samples making the kernel matrix very high dimensional. Kernel transform learning has been compared with other representation learning tools like autoencoder, restricted Boltzmann machine as well as with dictionary learning (and its kernelized version). Our proposed kernel transform learning yields better results than all the aforesaid techniques; experiments have been carried out on benchmark databases. © 2017 Elsevier B.V.",,"Software engineering; Benchmark database; Dictionary learning; High-dimensional; Kernel matrices; Linear combinations; Nonlinear versions; Number of samples; Restricted boltzmann machine; Pattern recognition",2-s2.0-85029411949
"Milošević M., Živić N., Andjelković I.","Early churn prediction with personalized targeting in mobile social games",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018722857&doi=10.1016%2fj.eswa.2017.04.056&partnerID=40&md5=37d151ede46d70284160bdd6600b279a","Customer churn is a widely known term in many industries, including banking, telecommunications and gaming. By definition, churn represents the act of a customer leaving a product for good. Most commonly, late customer churn is addressed. In the dynamics of free to play games, most of newly registered users abandon the game in the first few days, so the main focus is on early customer churn. Therefore, successful early churn prevention methodology is vital to having a successful business in free to play gaming industry. To tackle this problem, we introduce a two stage intelligent system. It employs early churn prediction, formulated as a binary classification task, followed by a churn prevention technique using personalized push notifications. For early churn prediction, common machine learning models are trained and compared using a data set obtained from two million players of Top Eleven - Be A Football Manager online mobile game. To prevent churn, we track user activity, identify the game features that are potentially interesting to the user and then use that data to tailor personalized push notifications with a purpose to attract users back into the game. Using this approach, we are able to reduce churn up to 28%, which, at the scale of millions of users, represents a significant positive impact to business. © 2017 Elsevier Ltd","Churn prediction; Churn prevention; Free to play; Machine learning; Push notifications; Targeting","Artificial intelligence; Computer games; Intelligent systems; Learning systems; Sales; Churn predictions; Churn prevention; Free to plays; Push notifications; Targeting; Forecasting",2-s2.0-85018722857
"Kumar D., Thakur M., Dubey C.S., Shukla D.P.","Landslide susceptibility mapping & prediction using Support Vector Machine for Mandakini River Basin, Garhwal Himalaya, India",2017,"Geomorphology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032695826&doi=10.1016%2fj.geomorph.2017.06.013&partnerID=40&md5=5f4c98b9bdb52e4317ed9d5143ede711","In recent years, various machine learning techniques have been applied for landslide susceptibility mapping. In this study, three different variants of support vector machine viz., SVM, Proximal Support Vector Machine (PSVM) and L2-Support Vector Machine - Modified Finite Newton (L2-SVM-MFN) have been applied on the Mandakini River Basin in Uttarakhand, India to carry out the landslide susceptibility mapping. Eight thematic layers such as elevation, slope, aspect, drainages, geology/lithology, buffer of thrusts/faults, buffer of streams and soil along with the past landslide data were mapped in GIS environment and used for landslide susceptibility mapping in MATLAB. The study area covering 1625 km2 has merely 0.11% of area under landslides. There are 2009 pixels for past landslides out of which 50% (1000) landslides were considered as training set while remaining 50% as testing set. The performance of these techniques has been evaluated and the computational results show that L2-SVM-MFN obtains higher prediction values (0.829) of receiver operating characteristic curve (AUC-area under the curve) as compared to 0.807 for PSVM model and 0.79 for SVM. The results obtained from L2-SVM-MFN model are found to be superior than other SVM prediction models and suggest the usefulness of this technique to problem of landslide susceptibility mapping where training data is very less. However, these techniques can be used for satisfactory determination of susceptible zones with these inputs. © 2017 Elsevier B.V.","GIS; Landslide susceptibility mapping; Mandakini Basin; Remote sensing; Support Vector Machine (SVM); Uttarakhand","GIS; landslide; mapping method; prediction; remote sensing; support vector machine; Garhwal Himalayas; Himalayas; India; Mandakini River; Uttarakhand",2-s2.0-85032695826
"Ono T., Sakamoto C., Nakao M., Saitoh N., Hirano T.","Condensin II plays an essential role in reversible assembly of mitotic chromosomes in situ",2017,"Molecular Biology of the Cell",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031299203&doi=10.1091%2fmbc.E17-04-0252&partnerID=40&md5=95f69ee38237eeb2590414add9ad6c4f","Condensins I and II are multisubunit complexes that play a central role in mitotic chromosome assembly. Although both complexes become concentrated along the axial region of each chromatid by metaphase, it remains unclear exactly how such axes might assemble and contribute to chromosome shaping. To address these questions from a physicochemical point of view, we have established a set of two-step protocols for inducing reversible assembly of chromosome structure in situ, namely within a whole cell. In this assay, mitotic chromosomes are first expanded in a hypotonic buffer containing a Mg2+-chelating agent and then converted into different shapes in a NaCl concentration-dependent manner. Both chromatin and condensin-positive chromosome axes are converted into near-original shapes at 100 mM NaCl. This assay combined with small interfering RNA depletion demonstrates that the recovery of chromatin shapes and the reorganization of axes are highly sensitive to depletion of condensin II but less sensitive to depletion of condensin I or topoisomerase IIα. Furthermore, quantitative morphological analyses using the machine-learning algorithm wndchrm support the notion that chromosome shaping is tightly coupled to the reorganization of condensin II-based axes. We propose that condensin II makes a primary contribution to mitotic chromosome architecture and maintenance in human cells. © 2017 Schvartz et al.",,"buffer; centromere protein A; chelating agent; condensin; condensin ii; DNA topoisomerase (ATP hydrolysing); histone H3; magnesium; phosphate buffered saline; small interfering RNA; sodium chloride; topoisomerase iialpha; unclassified drug; algorithm; antibody labeling; Article; assay; cell structure; centromere; chromatid; chromatin; chromatin structure; chromosome; chromosome marker; chromosome structure; controlled study; histone methylation; human; human cell; immunofluorescence test; in situ hybridization; mitosis; morphology; phylogeny; priority journal; protein depletion; quantitative analysis; rosette formation; supervised machine learning",2-s2.0-85031299203
"Xia M., He Y.","Functional connectomics from a “big data” perspective",2017,"NeuroImage",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013471853&doi=10.1016%2fj.neuroimage.2017.02.031&partnerID=40&md5=52476f0990caa6abe4e147af14be0fad","In the last decade, explosive growth regarding functional connectome studies has been observed. Accumulating knowledge has significantly contributed to our understanding of the brain's functional network architectures in health and disease. With the development of innovative neuroimaging techniques, the establishment of large brain datasets and the increasing accumulation of published findings, functional connectomic research has begun to move into the era of “big data”, which generates unprecedented opportunities for discovery in brain science and simultaneously encounters various challenging issues, such as data acquisition, management and analyses. Big data on the functional connectome exhibits several critical features: high spatial and/or temporal precision, large sample sizes, long-term recording of brain activity, multidimensional biological variables (e.g., imaging, genetic, demographic, cognitive and clinic) and/or vast quantities of existing findings. We review studies regarding functional connectomics from a big data perspective, with a focus on recent methodological advances in state-of-the-art image acquisition (e.g., multiband imaging), analysis approaches and statistical strategies (e.g., graph theoretical analysis, dynamic network analysis, independent component analysis, multivariate pattern analysis and machine learning), as well as reliability and reproducibility validations. We highlight the novel findings in the application of functional connectomic big data to the exploration of the biological mechanisms of cognitive functions, normal development and aging and of neurological and psychiatric disorders. We advocate the urgent need to expand efforts directed at the methodological challenges and discuss the direction of applications in this field. © 2017 Elsevier Inc.","Big data; Brain networks; Connectome; Dynamics; Fingerprint; Graph theory","aging; Article; behavior; brain disease; brain function; cognition; connectome; data analysis; data base; development; functional connectivity; functional magnetic resonance imaging; graph theory; human; image processing; independent component analysis; information processing; machine learning; mathematical model; mental disease; nerve cell network; neuroimaging; priority journal; reproducibility; statistical analysis; test retest reliability",2-s2.0-85013471853
"Linnell M.A., Davis R.J., Lesmeister D.B., Swingle J.K.","Conservation and relative habitat suitability for an arboreal mammal associated with old forest",2017,"Forest Ecology and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025647618&doi=10.1016%2fj.foreco.2017.07.004&partnerID=40&md5=0fab4d6c5409f397c2d4ca26b8177177","Contraction of native old forest can limit occurrence of old forest associated species, especially species with limited vagility. Patterns of size and distribution of remaining patches of old forest along with forest disturbance and what replaces old forest can influence whether species adapt or perish after forest loss. The arboreal red tree vole (Arborimus longicaudus) is a small arvicoline rodent that is associated with old coniferous forest and typically emigrates short distances. Since 1911, old forest (≥80 years old) in the northern half of the Oregon Coast Range has been reduced by >80%, primarily due to large stand replacing wildfires, timber harvest, and subsequent conversion to young forest (<80 years old). In 2011, the tree vole population in the northern half of the Oregon Coast Range was listed as a candidate species as a distinct population segment under the United States Endangered Species Act, primarily due to habitat loss. We examined the contribution of current and historical (early 20th century) old forest cover, and recent disturbances (1984–2012) on relative habitat suitability for tree voles using light detecting and ranging (LiDAR) data, Landsat imagery, and machine learning. We used a step-wise variable removal procedure to build a parsimonious model and to rank contribution of variables in our final model. We further described the configuration of large patches of old forest using metrics of amount and distance from patch for historical, current, two forest loss scenarios, and two forest restoration scenarios within our study area. Red tree vole relative habitat suitability was positively correlated with current old forest cover at the local-scale and negatively correlated with distance from large patches of current old forest. Landscape context, specifically proximity to old forest and absence of recent disturbance contributed most to relative habitat suitability of young forest matrix. If old forest contracted to only reserves on federal lands, amount would decrease from 10.9% to 9.5% and be spatially clumped with an increase in average distance to nearest patch from 3.1 km to 11.1 km. Alternatively, a random addition of patches equivalent to a 1.4% increase in amount, would reduce distance to nearest patch to 1.8 km. Given the history of large historical wildfires in the Oregon Coast Range, restoration of even a small amount of old forest throughout the study area would likely enhance connectivity and resiliency of red tree vole populations in the event of large-scale loss of old forest. © 2017","Conservation planning; Forest loss; Forest matrix; LiDAR; Red tree vole; Relative habitat suitability modeling","Conservation; Ecosystems; Fires; Optical radar; Restoration; Conservation planning; Forest loss; Forest matrix; Habitat suitability model; Red tree vole; Forestry; arboreal species; coniferous forest; conservation planning; forest cover; habitat loss; Landsat; lidar; machine learning; matrix; old-growth forest; patch size; rodent; size distribution; species occurrence; Oregon; United States; Arborimus; Arborimus longicaudus; Arvicolinae; Mammalia",2-s2.0-85025647618
"Lee S., Kevrekidis I.G., Karniadakis G.E.","A general CFD framework for fault-resilient simulations based on multi-resolution information fusion",2017,"Journal of Computational Physics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019386590&doi=10.1016%2fj.jcp.2017.06.044&partnerID=40&md5=78d47e69389f62411d080d80ee901fa8","We develop a general CFD framework for multi-resolution simulations to target multiscale problems but also resilience in exascale simulations, where faulty processors may lead to gappy, in space-time, simulated fields. We combine approximation theory and domain decomposition together with statistical learning techniques, e.g. coKriging, to estimate boundary conditions and minimize communications by performing independent parallel runs. To demonstrate this new simulation approach, we consider two benchmark problems. First, we solve the heat equation (a) on a small number of spatial “patches” distributed across the domain, simulated by finite differences at fine resolution and (b) on the entire domain simulated at very low resolution, thus fusing multi-resolution models to obtain the final answer. Second, we simulate the flow in a lid-driven cavity in an analogous fashion, by fusing finite difference solutions obtained with fine and low resolution assuming gappy data sets. We investigate the influence of various parameters for this framework, including the correlation kernel, the size of a buffer employed in estimating boundary conditions, the coarseness of the resolution of auxiliary data, and the communication frequency across different patches in fusing the information at different resolution levels. In addition to its robustness and resilience, the new framework can be employed to generalize previous multiscale approaches involving heterogeneous discretizations or even fundamentally different flow descriptions, e.g. in continuum-atomistic simulations. © 2017 Elsevier Inc.","coKriging; Domain decomposition; Exascale computing; Gap-tooth algorithm; Gappy data; Machine learning; Multi-resolution simulation; Resilience",,2-s2.0-85019386590
"Greiff V., Weber C.R., Palme J., Bodenhofer U., Miho E., Menzel U., Reddy S.T.","Learning the high-dimensional immunogenomic features that predict public and private antibody repertoires",2017,"Journal of Immunology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031505313&doi=10.4049%2fjimmunol.1700594&partnerID=40&md5=b4dfbd883166750a926bc14329d9b024","Recent studies have revealed that immune repertoires contain a substantial fraction of public clones, which may be defined as Ab or TCR clonal sequences shared across individuals. It has remained unclear whether public clones possess predictable sequence features that differentiate them from private clones, which are believed to be generated largely stochastically. This knowledge gap represents a lack of insight into the shaping of immune repertoire diversity. Leveraging a machine learning approach capable of capturing the high-dimensional compositional information of each clonal sequence (defined by CDR3), we detected predictive public clone and private clone-specific immunogenomic differences concentrated in CDR3?s N1-D-N2 region, which allowed the prediction of public and private status with 80% accuracy in humans and mice. Our results unexpectedly demonstrate that public, as well as private, clones possess predictable high-dimensional immunogenomic features. Our support vector machine model could be trained effectively on large published datasets (3 million clonal sequences) and was sufficiently robust for public clone prediction across individuals and studies prepared with different library preparation and high-throughput sequencing protocols. In summary, we have uncovered the existence of high-dimensional immunogenomic rules that shape immune repertoire diversity in a predictable fashion. Our approach may pave the way for the construction of a comprehensive atlas of public mouse and human immune repertoires with potential applications in rational vaccine design and immunotherapeutics. Copyright © 2017 by The American Association of Immunologists, Inc.",,"antibody; immunoglobulin G; immunoglobulin M; complementarity determining region; lymphocyte antigen receptor; vaccine; animal cell; Article; B lymphocyte; cell differentiation; cell expansion; classification; clone; controlled study; gene sequence; genomics; high dimensional immunogenomic feature; human; human cell; measurement accuracy; mouse; nonhuman; nucleotide sequence; prediction; priority journal; private clone; public clone; sequence composition; support vector machine; VDJ exon; animal; antibody specificity; antigen mediated clonal selection; Bagg albino mouse; C57BL mouse; cell clone; complementarity determining region; genetics; high throughput sequencing; immunology; immunotherapy; information processing; physiology; procedures; T lymphocyte; Animals; Antibody Diversity; B-Lymphocytes; Clonal Selection, Antigen-Mediated; Clone Cells; Complementarity Determining Regions; Datasets as Topic; High-Throughput Nucleotide Sequencing; Humans; Immunotherapy; Mice; Mice, Inbred BALB C; Mice, Inbred C57BL; Receptors, Antigen, B-Cell; Receptors, Antigen, T-Cell; T-Lymphocytes; Vaccines",2-s2.0-85031505313
"Kaneda Y., Shibata S., Mineno H.","Multi-modal sliding window-based support vector regression for predicting plant water stress",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026301078&doi=10.1016%2fj.knosys.2017.07.028&partnerID=40&md5=969816b38927a6e6c727285e7bbd3e89","Information communication technology (ICT) is required in the field of agriculture to solve problems arising because of the aging of farmers and shortage of heirs. In particular, environmental sensors and cameras are widely used in existing agricultural support systems for easy data collection. Although the traditional purpose of these systems is naive monitoring and controlling of the environment, the propagation of advanced cultivation is now expected by applying the data to machine learning and data mining technologies. Therefore, we propose a novel multi-modal sliding window-based support vector regression (multi-modal SW-SVR) method for accurate prediction of complicated water stress, which is a plant status, from two data types, namely environmental and plant image data. The proposed method includes two methodologies, SW-SVR and deep neural network (DNN) as a multi-modal feature extractor for SW-SVR. SW-SVR, which we proposed previously, is a suitable learning method for data with time-dependent characteristics, such as plant status. Moreover, we propose a new image feature, remarkable moving objects detected by adjacent optical flow (ROAF), to enable DNN to extract essential features easily for predicting water stress. Compared with existing regression models and features, the proposed multi-modal SW-SVR with ROAF demonstrates more precise and stable water stress prediction. © 2017 The Authors","Deep neural network; Ensemble learning; Image processing; Support vector regression; Water stress","Agricultural machinery; Agriculture; Cultivation; Deep neural networks; Education; Forecasting; Image processing; Learning systems; Object detection; Optical data processing; Plants (botany); Regression analysis; Data mining technology; Ensemble learning; Environmental sensor; Information communication technology; Monitoring and controlling; Support vector regression (SVR); Time-dependent characteristics; Water stress; Deep learning",2-s2.0-85026301078
"Song G., Dai Q.","A novel double deep ELMs ensemble system for time series forecasting",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028711091&doi=10.1016%2fj.knosys.2017.07.014&partnerID=40&md5=283eca45a16437f616f4c75fe4836f9f","Extreme Learning Machine (ELM) has proved to be well suited to different kinds of classification and regression problems. However, failing to seek deep representation of raw data completely brought by shallow architecture has made a plenty of research work stagnant, when ELM was chosen as the basic model. Recent years, deep ELM models like Hierarchical ELM (H-ELM), deep representations learning via ELM (Dr-ELM) have been proposed to be applied in multiple applications in machine learning. In this paper, a novel double deep ELMs ensemble system (DD-ELMs-ES) is proposed to focus on the problem of time series forecasting. In the proposed system, besides H-ELM and Dr-ELM are utilized as the basic models, a novel Constrained H-ELM (CH-ELM) is presented and serves as another basic model as well. CH-ELM intends to constrain the hidden neurons’ input connection weights, so that they could be consistent with the directions of sample vectors. Whats more, a self-adaptive ReTSP-Trend pruning technique is proposed to implement ensemble pruning in DD-ELMs-ES. Benefited from the merits of combining deep learning scheme with ensemble pruning paradigm, in the empirical results, DD-ELMs-ES demonstrates better generalization performance than the basic deep ELM models and some other state-of-the-art algorithms in tackling with time series forecasting tasks. © 2017 Elsevier B.V.","Constrained H-ELM (CH-ELM); Deep representations learning via ELM (dr-ELM); Double deep ELMs ensemble system (DD-ELMs-ES); Hierarchical ELM (H-ELM); Self-adaptive ReTSP-Trend (SA-ReTSP-Trend); Time series forecasting (TSF)","Learning systems; Time series; Constrained H-ELM (CH-ELM); Deep representations learning via ELM (dr-ELM); Ensemble systems; Hierarchical ELM (H-ELM); Self-adaptive ReTSP-Trend (SA-ReTSP-Trend); Time series forecasting; Forecasting",2-s2.0-85028711091
"Al-Dhamari A., Sudirman R., Mahmood N.H.","Abnormal behavior detection in automated surveillance videos: A review",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732480&partnerID=40&md5=7e3dd9f484aeeaec277d3de6f7733e28","Abnormal detection refers to infrequent data instances that come from a diverse cluster or distribution than the majority normal instances. Owing to the increasing demand for safety and security, discovery abnormalities from video streams has attracted significant research interest during recent years. By automatically finding abnormal actions, it significantly decreases the cost to label and annotate the videos of a huge number of hours. The current advancements in computer vision and machine learning have a remarkable role in enabling such intelligent frameworks. Different algorithms that are specially designed for building smart vision frameworks seek to scene understanding and building correct semantic inference from observed dynamic motions caused by moving targets. Unfortunately, although there are many algorithms have been proposed in this interesting topic, the research in this area still lacks strongly to two important things: comparative general assessment and public-accessible datasets. This study addresses these inadequacies by presenting an overview of most recent research algorithms that concentrate significantly on abnormal behavior detection in surveillance applications. This study extensively presents state-of-the-art algorithms in a way that enables those interested to know all the key issues and challenges relevant to the abnormal behavior detection topic and their applications as well as their specific features. Additionally, there are five important evaluation benchmarks from 2007 to 2017. The performance and limitations of those benchmarks are discussed, which will help largely research in this area. © 2005 – ongoing JATIT & LLS.","Abnormal detection; Clustering; Feature extraction; Learning methods; Sparsity; Spatio-temporal compositions; Video surveillance",,2-s2.0-85031732480
"Xia B., Ni Z., Li T., Li Q., Zhou Q.","VRer: Context-Based Venue Recommendation using embedded space ranking SVM in location-based social network",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018469655&doi=10.1016%2fj.eswa.2017.04.020&partnerID=40&md5=786f49f880393af405b31146771397f4","Venue recommendation has attracted a lot of research attention with the rapid development of Location-Based Social Networks. The effectiveness of venue recommendation largely depends on how well it captures users’ contexts or preferences. However, it is quite difficult, if not impossible, to capture the whole information about users’ preferences. In addition, users’ preferences are often heterogeneous (i.e., some preferences are static and common to all users while some preferences are dynamic and diverse). Existing venue recommendation does not well address the aforementioned issues and often recommends the most popular, the cheapest, or the closest venues based on simple contexts. In this paper, we cast the venue recommendation as a ranking problem and propose a recommendation framework named VRer (Context-Based Venue Recommendation using embedded space ranking SVM) employing an embedded space ranking SVM model to separate the venues in terms of different characteristics. Our proposed approach makes use of ‘check-in’ data to capture users’ preferences and utilizes a machine learning model to tune the importance of different factors in ranking. The major contribution of this paper are: (1) VRer combines various contexts (e.g., the temporal influence and the category of locations) with the check-in records to capture individual heterogeneous preferences; (2) we propose an embedded space ranking SVM optimizing the learning function to reduce the time consumption of training the personalized recommendation model for each group or user; (3) we evaluate our proposed approach against a real world LBSN and compare it with other baseline methods. Experimental results demonstrate the benefits of our proposed approach. © 2017 Elsevier Ltd","Check-in; Context-based; LBSNs; Preference; Ranking SVM","Learning systems; Location; Check-in; Context-based; LBSNs; Preference; Ranking SVM; Recommender systems",2-s2.0-85018469655
"Khalaf M.H., Al-Khateeb B., Farhan R.N.","Hand written charcter recognition using neural network and deep belief network",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031752939&partnerID=40&md5=2d82d0beca4a87244af5e6c2adece255","In this paper a comparison is done between two classification architectures, those are Standard Neural Networks (NN) that contain one hidden layer and Deep Learning concept using Deep Belief Networks (DBN). Both algorithms are applied on Capital English Character with same architectures and parameter for comparison purpose. The Standard Neural Network was trained as a supervised learning using Back Propagation (BP) algorithms while Deep Belief Network was trained using two phases of learning, the first phase as unsupervised learning using Contrastive Divergence (CD) algorithm and the second phase as a supervised learning using Back Propagation algorithms for fine tuning the network. Each character represented as an image in grayscale pixels. The features are extracted depending on the intensity of pixel in image that white color represents as a 0’s and black color represent as a 1’s. DBN is represented as a stack of Restricted Boltzmann Machines (RBM). The DBN learning procedure undergoes a pre-training stage and a fine-tuning stage. DBN gave a higher performance as compared with the Standard neural networks with an accuracy of approximately 92.3% for a classification of Capital English handwritten characters. © 2005 – ongoing JATIT & LLS.","Backpropagation; Character recognition; Contrastive divergence; Deep belief networks; Supervised learning",,2-s2.0-85031752939
"Serrano J.I., Lambrecht S., del Castillo M.D., Romero J.P., Benito-León J., Rocon E.","Identification of activities of daily living in tremorous patients using inertial sensors",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018563641&doi=10.1016%2fj.eswa.2017.04.032&partnerID=40&md5=3106d039eabb6f3fe83b3a94c4712b36","BACKGROUND Much attention has been given to the use of inertial sensors for remote monitoring of individuals suffering from neurological pathologies. However, the focus has been mostly on the detection of symptoms like tremor or dyskinesia, not on identifying specific activities carried out by subjects. The objective of this study was to develop an automated segmentation and recognition methodology, from inertial sensor data, to identify tasks and motor patterns during activities of daily living. This will enable clinicians to contextualize the symptoms of these diseases and improve their treatment. METHODS We designed and tested a methodology to automatically label continuous upper-limb activity from IMUs (Inertial Measurement Units). Three classification problems are considered: the task itself, the precision level required by the task with respect to movement (fine or gross) and the trajectory of the task (distal or proximal). These problems were identified by tremor experts as clinically important aspects to be monitored in tremor patients while performing daily activities. The proposed methodology reveals the relation between the functional context or activity and the patient's on-going tremor to clinicians. RESULTS Overall task identification rate was 86%. Task precision and task trajectory are classified with 79% and 89% accuracy, respectively. Aligning the semantic nature of the activity with the tremor location and intensity can also provide novel and relevant information for clinical monitoring of tremor and help clinicians and researchers as a useful tool to develop new therapies or strategies and novel anti-tremor medications to combat context dependent tremor. CONCLUSIONS The present study describes the development of a comprehensive methodology based on machine learning techniques to segment and detect activities of daily living in people with tremor using inertial sensors, which aims at facilitating detailed interpretation of tremor movements by neurologists. © 2017","Daily activity identification; Essential tremor; Inertial sensors; Parkinson; Time series classification","Learning systems; Semantics; Units of measurement; Daily activity; Essential tremor; Inertial sensor; Parkinson; Time series classifications; Inertial navigation systems",2-s2.0-85018563641
"Daood A., Salman I., Ghneim N.","Comparison study of automatic classifiers performance in emotion recognition of Arabic social media users",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031755098&partnerID=40&md5=4b9e522c96a419706379f911933dc05f","Emotion recognition from text gained a lot of interest in the last years, but some languages such as Arabic (with its different spoken dialects) have not been given such attention. In this paper, we present our work in the Emotion detection of Arabic texts, with a focus on Levantine Twitter Messages. We have constructed a corpus of Arabic Levantine tweets, and annotated it with correspondent emotions. We implemented different methods to automatically classify text messages of individuals to infer their emotional states. We compared the results of different machine learning algorithms, and the inclusion of different features, to determine the best configuration of the emotion recognition system. © 2005 – ongoing JATIT & LLS.","Data mining; Emotion detection from arabic text; Emotional analysis; Syrian dialects; Twitter",,2-s2.0-85031755098
"Yao X., Yan J., Liu K., Kim S., Nho K., Risacher S.L., Greene C.S., Moore J.H., Saykin A.J., Shen L.","Tissue-specific network-based genome wide study of amygdala imaging phenotypes to identify functional interaction modules",2017,"Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031782790&doi=10.1093%2fbioinformatics%2fbtx344&partnerID=40&md5=0e91c0ca174991ccddc3d2d4cd0b565c","Motivation Network-based genome-wide association studies (GWAS) aim to identify functional modules from biological networks that are enriched by top GWAS findings. Although gene functions are relevant to tissue context, most existing methods analyze tissue-free networks without reflecting phenotypic specificity. Results We propose a novel module identification framework for imaging genetic studies using the tissue-specific functional interaction network. Our method includes three steps: (i) re-prioritize imaging GWAS findings by applying machine learning methods to incorporate network topological information and enhance the connectivity among top genes; (ii) detect densely connected modules based on interactions among top re-prioritized genes; and (iii) identify phenotype-relevant modules enriched by top GWAS findings. We demonstrate our method on the GWAS of [ 18 F]FDG-PET measures in the amygdala region using the imaging genetic data from the Alzheimer's Disease Neuroimaging Initiative, and map the GWAS results onto the amygdala-specific functional interaction network. The proposed network-based GWAS method can effectively detect densely connected modules enriched by top GWAS findings. Tissue-specific functional network can provide precise context to help explore the collective effects of genes with biologically meaningful interactions specific to the studied phenotype. Availability and implementation The R code and sample data are freely available at http://www.iu.edu/shenlab/tools/gwasmodule/ Contact shenli@iu.edu Supplementary informationSupplementary dataare available at Bioinformatics online. © The Author 2017. Published by Oxford University Press. All rights reserved.",,,2-s2.0-85031782790
"He G.-W., Wang T.-Y., Chiang A.-S., Ching Y.-T.","Soma Detection in 3D Images of Neurons using Machine Learning Technique",2017,"Neuroinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031407247&doi=10.1007%2fs12021-017-9342-0&partnerID=40&md5=606f013906239d98d30aee7632d664fc","Computing and analyzing the neuronal structure is essential to studying connectome. Two important tasks for such analysis are finding the soma and constructing the neuronal structure. Finding the soma is considered more important because it is required for some neuron tracing algorithms. We describe a robust automatic soma detection method developed based on the machine learning technique. Images of neurons were three-dimensional confocal microscopic images in the FlyCircuit database. The testing data were randomly selected raw images that contained noises and partial neuronal structures. The number of somas in the images was not known in advance. Our method tries to identify all the somas in the images. Experimental results showed that the method is efficient and robust. © 2017 Springer Science+Business Media, LLC","Drosophila; Machine learning method; Soma detection",,2-s2.0-85031407247
"Zhang H.-G., Wu L., Song Y., Su C.-W., Wang Q., Su F.","An Online Sequential Learning Non-parametric Value-at-Risk Model for High-Dimensional Time Series",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031944453&doi=10.1007%2fs12559-017-9516-y&partnerID=40&md5=2b1fc1574588f5256cbaf4d99bfb819d","Online Value-at-Risk (VaR) analysis in high-dimensional space remains a challenge in the era of big data. In this paper, we propose an online sequential learning non-parametric VaR model called OS-GELM which is an autonomous cognitive system. This model uses a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) process and an online sequential extreme learning machine (OS-ELM) to cognitively calculate VaR, which can be used for online risk analysis. The proposed model not only learns the data one-by-one or chunk-by-chunk but also calculates VaR in real time by extending OS-ELM from machine learning to the non-parametric GARCH process. The GARCH process is also extended to one-by-one and chunk-by-chunk mode. In OS-GELM, the parameters of hidden nodes are randomly selected. The output weights are analytically determined based on the sequentially arriving data. In addition, the generalization performance of the OS-GELM model attains a small training error and generates the smallest norm of weights. Experimentally obtained VaRs are compared with those given by GARCH-type models and conventional OS-ELM. The computational results demonstrate that the OS-GELM model obtains more accurate results and is better at forecasting the online VaR. OS-GELM model is an autonomous cognitive system to dynamically calculate Value-at-Risk, which can be used for online financial risk assessment about human being’s behavior. The OS-GELM model can calculate VaR in real time, which can be used as a tool for online risk management. OS-GELM can handle any bounded, non-constant, piecewise-continuous membership function to realize real-time VaR monitoring. © 2017 Springer Science+Business Media, LLC","GARCH models; High-dimensional space; OS-ELM; Time series; Value-at-Risk","Big data; Cognitive systems; Learning systems; Membership functions; Risk analysis; Risk assessment; Risk management; Time series; Value engineering; Computational results; GARCH models; Generalization performance; Generalized autoregressive conditional heteroskedasticity; High dimensional spaces; Online sequential extreme learning machine; Piecewise-continuous; Value at Risk; E-learning",2-s2.0-85031944453
"Xiao J., Xiang Z., Wang D., Xiao Z.","Nonparametric kernel smoother on topology learning neural networks for incremental and ensemble regression",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031423192&doi=10.1007%2fs00521-017-3218-y&partnerID=40&md5=b8c76bd21b721327243efbcc0c565d24","Incremental learning is a technique which is effective to increase the space efficiency of machine learning algorithms. Ensemble learning can combine different algorithms to form more accurate ones. The parameter selection of incremental methods is difficult because no retraining is allowed, and the combination of incremental and ensemble learning has not been fully explored. In this paper, we propose a parameter-free regression framework and it combines incremental learning and ensemble learning. First, the topology learning neural networks such as growing neural gas (GNG) and self-organizing incremental neural network (SOINN) are employed as solutions to nonlinearity. Then, the vector quantizations of GNG and SOINN are transformed into a feed-forward neural network by an improved Nadaraya–Watson estimator. A maximum likelihood process is devised for adaptive parameter selection of the estimator. Finally, a weighted training strategy is incorporated to enable the topology learning regressors for ensemble learning by AdaBoost. Experiments are carried out on 5 UCI datasets, and an application study of short-term traffic flow prediction is given. The results show that the proposed method gives comparable results to mainstream incremental and non-incremental regression methods, and better performances in the short-term traffic flow prediction. © 2017 The Natural Computing Applications Forum","Competitive learning; Ensemble learning; Incremental learning; Nadaraya–Watson estimator; Nonparametric regression","Adaptive boosting; Learning systems; Maximum likelihood; Regression analysis; Street traffic control; Topology; Competitive learning; Ensemble learning; Incremental learning; Incremental regressions; Non-parametric regression; Parameter selection; Self-organizing incremental neural network; Short-term traffic flow; Learning algorithms",2-s2.0-85031423192
"Artetxe A., Graña M., Beristain A., Ríos S.","Balanced training of a hybrid ensemble method for imbalanced datasets: a case of emergency department readmission prediction",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402164&doi=10.1007%2fs00521-017-3242-y&partnerID=40&md5=bd1dfb0b187dfb6fe9cea20cb4ec0c56","Dealing with imbalanced datasets is a recurrent issue in health-care data processing. Most literature deals with small academic datasets, so that results often do not extrapolate to the large real-life datasets, or have little real-life validity. When minority class sample generation by interpolation is meaningless, the recourse to undersampling the majority class is mandatory in order to reach some acceptable results. Ensembles of classifiers provide the advantage of the diversity of their members, which may allow adaptation to the imbalanced distribution. In this paper, we present a pipeline method combining random undersampling with bootstrap aggregation (bagging) for a hybrid ensemble of extreme learning machines and decision trees, whose diversity improves adaptation to the imbalanced class dataset. The approach is demonstrated on a realistic greatly imbalanced dataset of emergency department patients from a Chilean hospital targeted to predict patient readmission. Computational experiments show that our approach outperforms other well-known classification algorithms. © 2017 The Natural Computing Applications Forum","Class imbalance; Ensemble learning; Extreme learning machine; Hospital readmission","Data handling; Decision trees; Emergency rooms; Hospitals; Knowledge acquisition; Learning systems; Statistical methods; Class imbalance; Classification algorithm; Computational experiment; Emergency departments; Ensemble learning; Ensembles of classifiers; Extreme learning machine; Random under samplings; Education",2-s2.0-85031402164
"Núñez H., Gonzalez-Abril L., Angulo C.","Improving SVM Classification on Imbalanced Datasets by Introducing a New Bias",2017,"Journal of Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031399870&doi=10.1007%2fs00357-017-9242-x&partnerID=40&md5=b8738a8b64b68e472e2ee71c642d8f9e","Support Vector Machine (SVM) learning from imbalanced datasets, as well as most learning machines, can show poor performance on the minority class because SVMs were designed to induce a model based on the overall error. To improve their performance in these kind of problems, a low-cost post-processing strategy is proposed based on calculating a new bias to adjust the function learned by the SVM. The proposed bias will consider the proportional size between classes in order to improve performance on the minority class. This solution avoids not only introducing and tuning new parameters, but also modifying the standard optimization problem for SVM training. Experimental results on 34 datasets, with different degrees of imbalance, show that the proposed method actually improves the classification on imbalanced datasets, by using standardized error measures based on sensitivity and g-means. Furthermore, its performance is comparable to well-known cost-sensitive and Synthetic Minority Over-sampling Technique (SMOTE) schemes, without adding complexity or computational costs. © 2017 Classification Society of North America","Bias; Cost-sensitive strategy; Post-processing; SMOTE; Support Vector Machine",,2-s2.0-85031399870
"Bruchansky C.","Machine learning: A structuralist discipline?",2017,"AI and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031395979&doi=10.1007%2fs00146-017-0764-x&partnerID=40&md5=cfc96c6397d00c9288d553668aded40c","Advances in machine learning and natural language processing are revolutionizing the way we live, work, and think. As for any science, they are based on assumptions about what the world is, and how humans interact with it. In this paper, I discuss what is potentially one of these assumptions: structuralism, which states that all cultures share a hidden structure. I illustrate this assumption with political footprints: a machine-learning technique using pre-trained word vectors for political discourse analysis. I introduce some of the benefits and limitations of structuralism when applied to machine learning, and the risks of exploiting a technology before establishing the validity of all its hypotheses. I consider how machine-learning techniques could evolve towards hybrid structuralism or post-structuralism, and how deeply these developments would impact cultural studies. © 2017 Springer-Verlag London Ltd.","Culture; Human mind; Knowledge; Linguistic; Machine learning; Natural language processing; Political discourse; Post-structuralism; Semantics; Structuralism","Artificial intelligence; Cell culture; Learning algorithms; Linguistics; Natural language processing systems; Semantics; Human mind; Knowledge; Political discourse; Post-structuralism; Structuralism; Learning systems",2-s2.0-85031395979
"Ambale-Venkatesh B., Yang X., Wu C.O., Liu K., Hundley W.G., McClelland R., Gomes A.S., Folsom A.R., Shea S., Guallar E., Bluemke D.A., Lima J.A.C.","Cardiovascular Event Prediction by Machine Learning: The Multi-Ethnic Study of Atherosclerosis",2017,"Circulation research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031820117&doi=10.1161%2fCIRCRESAHA.117.311312&partnerID=40&md5=5b95284f6726f54e11b4579998925886","RATIONALE: Machine learning may be useful to characterize cardiovascular risk, predict outcomes, and identify biomarkers in population studies.OBJECTIVE: To test the ability of random survival forests, a machine learning technique, to predict 6 cardiovascular outcomes in comparison to standard cardiovascular risk scores.METHODS AND RESULTS: We included participants from the MESA (Multi-Ethnic Study of Atherosclerosis). Baseline measurements were used to predict cardiovascular outcomes over 12 years of follow-up. MESA was designed to study progression of subclinical disease to cardiovascular events where participants were initially free of cardiovascular disease. All 6814 participants from MESA, aged 45 to 84 years, from 4 ethnicities, and 6 centers across the United States were included. Seven-hundred thirty-five variables from imaging and noninvasive tests, questionnaires, and biomarker panels were obtained. We used the random survival forests technique to identify the top-20 predictors of each outcome. Imaging, electrocardiography, and serum biomarkers featured heavily on the top-20 lists as opposed to traditional cardiovascular risk factors. Age was the most important predictor for all-cause mortality. Fasting glucose levels and carotid ultrasonography measures were important predictors of stroke. Coronary Artery Calcium score was the most important predictor of coronary heart disease and all atherosclerotic cardiovascular disease combined outcomes. Left ventricular structure and function and cardiac troponin-T were among the top predictors for incident heart failure. Creatinine, age, and ankle-brachial index were among the top predictors of atrial fibrillation. TNF-α (tissue necrosis factor-α) and IL (interleukin)-2 soluble receptors and NT-proBNP (N-Terminal Pro-B-Type Natriuretic Peptide) levels were important across all outcomes. The random survival forests technique performed better than established risk scores with increased prediction accuracy (decreased Brier score by 10%-25%).CONCLUSIONS: Machine learning in conjunction with deep phenotyping improves prediction accuracy in cardiovascular event prediction in an initially asymptomatic population. These methods may lead to greater insights on subclinical disease markers without apriori assumptions of causality.CLINICAL TRIAL REGISTRATION: URL: http://www.clinicaltrials.gov. Unique identifier: NCT00005487. © 2017 American Heart Association, Inc.","atrial fibrillation; cardiovascular disease; coronary heart disease; heart failure; machine learning; mortality; stroke","aged; atherosclerosis; cardiovascular disease; clinical trial; cohort analysis; controlled study; diagnostic imaging; ethnic group; ethnology; female; follow up; human; machine learning; male; middle aged; mortality; multicenter study; predictive value; randomized controlled trial; survival rate; trends; very elderly; Aged; Aged, 80 and over; Atherosclerosis; Cardiovascular Diseases; Cohort Studies; Ethnic Groups; Female; Follow-Up Studies; Humans; Machine Learning; Male; Middle Aged; Predictive Value of Tests; Survival Rate",2-s2.0-85031820117
"Zhang Q., Zhou D.","Deep Arm/Ear-ECG Image Learning for Highly Wearable Biometric Human Identification",2017,"Annals of Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031413559&doi=10.1007%2fs10439-017-1944-z&partnerID=40&md5=98586b145e258bf7cccf0634450e0ef2","In this study, to advance smart health applications which have increasing security/privacy requirements, we propose a novel highly wearable ECG-based user identification system, empowered by both non-standard convenient ECG lead configurations and deep learning techniques. Specifically, to achieve a super wearability, we suggest situating all the ECG electrodes on the left upper-arm, or behind the ears, and successfully obtain weak but distinguishable ECG waveforms. Afterwards, to identify individuals from weak ECG, we further present a two-stage framework, including ECG imaging and deep feature learning/identification. In the former stage, the ECG heartbeats are projected to a 2D state space, to reveal heartbeats’ trajectory behaviors and produce 2D images by a split-then-hit method. In the second stage, a convolutional neural network is introduced to automatically learn the intricate patterns directly from the ECG image representations without heavy feature engineering, and then perform user identification. Experimental results on two acquired datasets using our wearable prototype, show a promising identification rate of 98.4% (single-arm-ECG) and 91.1% (ear-ECG), respectively. To the best of our knowledge, it is the first study on the feasibility of using single-arm-ECG/ear-ECG for user identification purpose, which is expected to contribute to pervasive ECG-based user identification in smart health applications. © 2017 Biomedical Engineering Society","Biometric; Convolutional neural network; Deep learning; ECG; Machine learning; Representation learning; Smart health; User identification; Wearable computers","Biometrics; Convolution; Deep learning; Health; Image processing; Learning systems; Neural networks; Wearable computers; Wearable technology; Convolutional neural network; Deep feature learning; Feature engineerings; Human identification; Identification rates; Image representations; Representation learning; User identification; Electrocardiography",2-s2.0-85031413559
"Okawa H., Suefusa K., Tanaka T.","Neural entrainment to auditory imagery of rhythms",2017,"Frontiers in Human Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032006681&doi=10.3389%2ffnhum.2017.00493&partnerID=40&md5=53a2a23aac42baecab7a8327d5d47e12","A method of reconstructing perceived or imagined music by analyzing brain activity has not yet been established. As a first step toward developing such a method, we aimed to reconstruct the imagery of rhythm, which is one element of music. It has been reported that a periodic electroencephalogram (EEG) response is elicited while a human imagines a binary or ternary meter on a musical beat. However, it is not clear whether or not brain activity synchronizes with fully imagined beat and meter without auditory stimuli. To investigate neural entrainment to imagined rhythm during auditory imagery of beat and meter, we recorded EEG while nine participants (eight males and one female) imagined three types of rhythm without auditory stimuli but with visual timing, and then we analyzed the amplitude spectra of the EEG. We also recorded EEG while the participants only gazed at the visual timing as a control condition to confirm the visual effect. Furthermore, we derived features of the EEG using canonical correlation analysis (CCA) and conducted an experiment to individually classify the three types of imagined rhythm from the EEG. The results showed that classification accuracies exceeded the chance level in all participants. These results suggest that auditory imagery of meter elicits a periodic EEG response that changes at the imagined beat and meter frequency even in the fully imagined conditions. This study represents the first step toward the realization of a method for reconstructing the imagined music from brain activity. © 2017 Okawa, Suefusa and Tanaka.","Canonical correlation analysis; Discrete fourier transform; EEG analysis; Machine learning; Rhythm perception","adult; brain function; classification; clinical article; controlled study; correlation analysis; electroencephalogram; female; Fourier transformation; human; imagery; machine learning; male; music; nervous system; perception; rhythm; stimulus",2-s2.0-85032006681
"Yousefnezhad M., Zhang D.","Anatomical Pattern Analysis for Decoding Visual Stimuli in Human Brains",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031405349&doi=10.1007%2fs12559-017-9518-9&partnerID=40&md5=9f075546c136e4c3f97b5c2184e53f0c","A universal unanswered question in neuroscience and machine learning is whether computers can decode the patterns of the human brain. Multi-Voxel Pattern Analysis (MVPA) is a critical tool for addressing this question. However, there are two challenges in the previous MVPA methods, which include decreasing sparsity and noise in the extracted features and increasing the performance of prediction. In overcoming mentioned challenges, this paper proposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the human brain. This framework develops a novel anatomical feature extraction method and a new imbalance AdaBoost algorithm for binary classification. Further, it utilizes an Error-Correcting Output Codes (ECOC) method for multiclass prediction. APA can automatically detect active regions for each category of the visual stimuli. Moreover, it enables us to combine homogeneous datasets for applying advanced classification. Experimental studies on four visual categories (words, consonants, objects, and scrambled photos) demonstrate that the proposed approach achieves superior performance to state-of-the-art methods. © 2017 Springer Science+Business Media, LLC","Anatomical feature extraction; Brain decoding; Multi-voxel pattern analysis; Visual object recognition","Adaptive boosting; Brain; Classification (of information); Extraction; Feature extraction; Learning systems; Object recognition; Anatomical features; Binary classification; Brain decoding; Error correcting output code; Multi-voxel pattern analysis; Multiclass prediction; State-of-the-art methods; Visual object recognition; Decoding",2-s2.0-85031405349
"Dumpert F., Beck M.","Use of machine learning in official business statistics [Einsatz von Machine-Learning-Verfahren in amtlichen Unternehmensstatistiken]",2017,"AStA Wirtschafts- und Sozialstatistisches Archiv",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031747984&doi=10.1007%2fs11943-017-0208-6&partnerID=40&md5=6f8e9b29e904d8e42d18576addb2900a","The task of the official business statistics is to provide information on the structure and development of the economy, which is gained through surveys, the use of administrative data, the purchase of commercial data and the linking of micro data. Recently, the use of machine learning methods in official business statistics has also been experimentally tested in the case of classification decisions and the generation of new data. This article provides an overview of the proceeding. To this end, the methodology of machine learning is first presented in the basic principles, previous fields of application are described outside and in official statistics, and the methods used experimentally in the business statistics are explained. Subsequently, the practical application of Support Vector Machines and Random Forests is presented in five concrete tasks in selected business statistics. Finally, the experience gained so far is summarized and potential further tasks as well as foreseeable further developments of the machine learning methods are presented. © 2017 Springer-Verlag GmbH Deutschland","Business statistics; Machine learning; Random Forest; Support Vector Machine",,2-s2.0-85031747984
"Stratton T.P., Perryman A.L., Vilchèze C., Russo R., Li S.-G., Patel J.S., Singleton E., Ekins S., Connell N., Jacobs W.R., Freundlich J.S.","Addressing the Metabolic Stability of Antituberculars through Machine Learning",2017,"ACS Medicinal Chemistry Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031311169&doi=10.1021%2facsmedchemlett.7b00299&partnerID=40&md5=d6c6347a7f44c61c0f77a2bf53dd1841","We present the first prospective application of our mouse liver microsomal (MLM) stability Bayesian model. CD117, an antitubercular thienopyrimidine tool compound that suffers from metabolic instability (MLM t1/2 &lt; 1 min), was utilized to assess the predictive power of our new MLM stability model. The S-substituent was removed, a set of commercial reagents was utilized to construct a virtual library of 411 analogues, and our MLM stability model was applied to prioritize 13 analogues for synthesis and biological profiling. In MLM stability assays, all 13 analogues had superior metabolic stability to the parent compound, and six new analogues had acceptable MLM t1/2 values greater than or equal to 60 min. It is noteworthy that whole-cell efficacy and lack of relative mammalian cell cytotoxicity could not be predicted simultaneously. These results support the utility of our new MLM stability model in chemical tool and drug discovery optimization efforts. © 2017 American Chemical Society.","antitubercular; Bayesian; chemical tool optimization; computer-aided analogue design; machine learning; mouse liver microsomal stability","cd 117; pyrimidine derivative; tuberculostatic agent; unclassified drug; animal cell; Article; Bayesian learning; computer aided design; controlled study; drug cytotoxicity; drug efficacy; drug half life; drug stability; drug synthesis; intrinsic clearance; liver microsome; machine learning; mammal cell; metabolic stability; metabolomics; mouse; Mycobacterium tuberculosis; nonhuman; priority journal; tuberculosis",2-s2.0-85031311169
"Mesquita D.P.P., Gomes J.P.P., Junior A.H.S.","Epanechnikov kernel for incomplete data",2017,"Electronics Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032660725&doi=10.1049%2fel.2017.0507&partnerID=40&md5=ca3bbea63639762939c168d79b7c85b8","The Epanechnikov kernel (EK) is a popular kernel function that has achieved promising results in many machine learning applications. Although the EK is widely used, its basic formulation requires fully observed input feature vectors. A method is proposed to estimate the EK when these input vectors are only partially observed, i.e. some of its features are missing. In the proposed method, named expected EK, the expected value of the kernel function is estimated given the distribution of the data and the observed values of the feature vectors. © The Institution of Engineering and Technology 2017.",,"Electronics engineering; Technology; Basic formulation; Expected values; Feature vectors; Incomplete data; Input features; Kernel function; Machine learning applications; Observed values; Learning systems",2-s2.0-85032660725
"Liao K.-W., Muto Y., Lin J.-Y.","Scour depth evaluation of a bridge with a complex pier foundation",2017,"KSCE Journal of Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328759&doi=10.1007%2fs12205-017-1769-1&partnerID=40&md5=b9358dbebddfa25e0f7bb00462731298","A scour depth prediction formula for a river bridge is established using experimental data in which the effects of the pier, pile-cap and pile group are considered. More than 170 experimental data entries, including different pier structural sizes, flow depths and soil covering depths, are collected and verified by existing formulae, which failed to deliver a promising prediction. A machine learning prediction model was then developed to enhance the accuracy. For application purpose, a sequential quadratic programming optimization was adopted to construct an explicit prediction formula. The MAPE was significantly improved from 102.8 to 28.9. The results indicate that the proposed formula can simultaneously satisfy the requirements of accuracy and simplicity. The proposed formula has the advantages of being conceptually consistent with observed scour behaviors and provides a solid scour depth prediction, which is an important and critical step in the bridge safety evaluation if floods are considered. © 2017 Korean Society of Civil Engineers and Springer-Verlag GmbH Germany","complicated foundation; flood; machine learning; optimization; scour","Artificial intelligence; Bridge piers; Floods; Forecasting; Learning systems; Optimization; Piers; Quadratic programming; Scour; Bridge safety; Critical steps; Pier foundation; Prediction model; River bridges; Sequential quadratic programming; Soil coverings; Structural size; Piles",2-s2.0-85030328759
"Chang J., Ryoo S.","Implementation of an improved facial emotion retrieval method in multimedia system",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031406478&doi=10.1007%2fs11042-017-5241-5&partnerID=40&md5=5f4f240b4340b37eb6b83bc9419a9d5e","The interaction between machine and human with the development of information technology is growing, and therefore human friendly system is more increasing in actual circumstances. The most important thing in communication between human and machine is the understanding of each other’s thought and the knowing of each other’s emotion. This paper proposes a new method of grasping human emotion with the emotion recognition through the human’s facial image. This new approach consists of two categories, which include the combination of principal component analysis, linear discriminant analysis for pattern recognition problem, and support vector machine for the emotion retrieval from the images. © 2017 Springer Science+Business Media, LLC","Emotion retrieval; Facial features; Fisherface; Machine learning","Discriminant analysis; Image retrieval; Learning systems; Multimedia systems; Pattern recognition; Emotion recognition; Emotion retrieval; Facial emotions; Facial feature; Fisherface; Linear discriminant analysis; Pattern recognition problems; Retrieval methods; Principal component analysis",2-s2.0-85031406478
"Kwon S.-K., Jung H.-S., Baek W.-K., Kim D.","Classification of forest vertical structure in South Korea from aerial orthophoto and lidar data using an artificial neural network",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031395649&doi=10.3390%2fapp7101046&partnerID=40&md5=dfc50e792b9057af31a2acaf63d287cd","Every vegetation colony has its own vertical structure. Forest vertical structure is considered as an important indicator of a forest's diversity and vitality. The vertical structure of a forest has typically been investigated by field survey, which is the traditional method of forest inventory. However, this method is very time- and cost-consuming due to poor accessibility. Remote sensing data such as satellite imagery, aerial photography, and lidar data can be a viable alternative to the traditional field-based forestry survey. In this study, we classified forest vertical structures from red-green-blue (RGB) aerial orthophotos and lidar data using an artificial neural network (ANN), which is a powerful machine learning technique. The test site was Gongju province in South Korea, which contains single-, double-, and triple-layered forest structures. The performance of the proposed method was evaluated by comparing the results with field survey data. The overall accuracy achieved was about 70%. It means that the proposed approach can classify the forest vertical structures from the aerial orthophotos and lidar data. © 2017 by the authors.","Aerial orthophoto; ANN (Artificial Neural Network); Forest inventory; Forestry vertical structure; Lidar (light detection and ranging); Machine learning; Stratification",,2-s2.0-85031395649
"Lin W.-C., Wu C.-C., Yu K., Zhuang Y.-H., Liu S.-C.","On the Use of Genetic Algorithm for Solving Re-entrant Flowshop Scheduling with Sum-of-processing-times-based Learning Effect to Minimize Total Tardiness",2017,"Intelligent Automation and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032694411&doi=10.1080%2f10798587.2017.1302711&partnerID=40&md5=36d535ebff3f455ee8154a5ad9d6df4e","Most research studies on scheduling problems assume that a job visits certain machines only one time. However, this assumption is invalid in some real-life situations. For example, a job may be processed by the same machine more than once in semiconductor wafer manufacturing or in a printed circuit board manufacturing machine. Such a setting is known as the “re-entrant flowshop”. On the other hand, the importance of learning effect present in many practical situations such as machine shop, in different branches of industry and for a variety of corporate activities, in shortening life cycles, and in an increasing diversity of products in the manufacturing environment. Inspired by these observations, this paper addresses a re-entrant m-machine flowshop scheduling problems with time-dependent learning effect to minimize the total tardiness. The complexity of the proposed problem is very difficult. Therefore, in this paper we first present four heuristic algorithms, which are modified from existing algorithms to solve the problem. Then, we use the solutions as four initials to a genetic algorithm. Finally, we report experimental performances of all the proposed methods for the small and big numbers of jobs, respectively. © 2017 TSI® Press","Genetic algorithm; Learning effect; re-entrant flowshop; Total tardiness",,2-s2.0-85032694411
"Lewinski N.A., Jimenez I., McInnes B.T.","An annotated corpus with nanomedicine and pharmacokinetic parameters",2017,"International Journal of Nanomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032855045&doi=10.2147%2fIJN.S137117&partnerID=40&md5=f0349709a0c46c1d1ed752a443f487f7","A vast amount of data on nanomedicines is being generated and published, and natural language processing (NLP) approaches can automate the extraction of unstructured text-based data. Annotated corpora are a key resource for NLP and information extraction methods which employ machine learning. Although corpora are available for pharmaceuticals, resources for nanomedicines and nanotechnology are still limited. To foster nanotechnology text mining (NanoNLP) efforts, we have constructed a corpus of annotated drug product inserts taken from the US Food and Drug Administration’s Drugs@FDA online database. In this work, we present the development of the Engineered Nanomedicine Database corpus to support the evaluation of nanomedicine entity extraction. The data were manually annotated for 21 entity mentions consisting of nanomedicine physicochemical characterization, exposure, and biologic response information of 41 Food and Drug Administration-approved nanomedicines. We evaluate the reliability of the manual annotations and demonstrate the use of the corpus by evaluating two state-of-the-art named entity extraction systems, OpenNLP and Stanford NER. The annotated corpus is available open source and, based on these results, guidelines and suggestions for future development of additional nanomedicine corpora are provided. © 2017 Lewinski et al.","Corpora; Informatics; Nanotechnology; Natural language processing; Text mining","Article; data mining; drug database; drug exposure; drug labeling; drug response; engineered nanomedicine database; information processing; medical informatics; named entity recognition; nanomedicine; nanotechnology; natural language processing; pharmacokinetic parameters; physical chemistry; reliability",2-s2.0-85032855045
"Jochems A., El-Naqa I., Kessler M., Mayo C.S., Jolly S., Matuszak M., Faivre-Finn C., Price G., Holloway L., Vinod S., Field M., Barakat M.S., Thwaites D., de Ruysscher D., Dekker A., Lambin P.","A prediction model for early death in non-small cell lung cancer patients following curative-intent chemoradiotherapy",2017,"Acta Oncologica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031433467&doi=10.1080%2f0284186X.2017.1385842&partnerID=40&md5=22993faa84e49dd1986fe98c84ffc97d","Background: Early death after a treatment can be seen as a therapeutic failure. Accurate prediction of patients at risk for early mortality is crucial to avoid unnecessary harm and reducing costs. The goal of our work is two-fold: first, to evaluate the performance of a previously published model for early death in our cohorts. Second, to develop a prognostic model for early death prediction following radiotherapy. Material and methods: Patients with NSCLC treated with chemoradiotherapy or radiotherapy alone were included in this study. Four different cohorts from different countries were available for this work (N = 1540). The previous model used age, gender, performance status, tumor stage, income deprivation, no previous treatment given (yes/no) and body mass index to make predictions. A random forest model was developed by learning on the Maastro cohort (N = 698). The new model used performance status, age, gender, T and N stage, total tumor volume (cc), total tumor dose (Gy) and chemotherapy timing (none, sequential, concurrent) to make predictions. Death within 4 months of receiving the first radiotherapy fraction was used as the outcome. Results: Early death rates ranged from 6 to 11% within the four cohorts. The previous model performed with AUC values ranging from 0.54 to 0.64 on the validation cohorts. Our newly developed model had improved AUC values ranging from 0.62 to 0.71 on the validation cohorts. Conclusions: Using advanced machine learning methods and informative variables, prognostic models for early mortality can be developed. Development of accurate prognostic tools for early mortality is important to inform patients about treatment options and optimize care. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group",,,2-s2.0-85031433467
"Massey G., Ehrensberger-Dow M.","Machine learning: Implications for translator education",2017,"Lebende Sprachen",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032201522&doi=10.1515%2fles-2017-0021&partnerID=40&md5=0fc606298c9a6632b182ff5ce73678fd","Machines are learning fast, and human translators must keep pace by learning with, from and about them. Deep learning (DL) and neural machine translation (NMT) are set to change the reality of translation and the distributions of tasks. Although theoretical and practical courses on computer-aided and/or machine translation abound, less attention has been paid to DL and NMT in most translation programmes. The challenge for translation education is to give students the knowledge and toolkits to learn when and how to embrace the new technologies, and to exploit how and when the added value of human intuition, creativity and ethics can and should be deployed. © 2017 Walter de Gruyter GmbH, Berlin/Boston 2017.","creativity; ethics; human translation; NMT; Translator education",,2-s2.0-85032201522
"Calumby R.T., Gonçalves M.A., Torres R.D.S.","Diversity-based interactive learning meets multimodality",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014086621&doi=10.1016%2fj.neucom.2016.08.129&partnerID=40&md5=a4a36eb4e854c3de0918e997c3081cff","In interactive retrieval tasks, one of the main objectives is to maximize the user information gain throughout search sessions. Retrieving many relevant items is quite important, but it does not necessarily completely satisfy the user needs. When only relevant near-duplicate items are retrieved, the amount of different concepts users are able to extract from the target collection is very limited. Therefore, broadening the number of concepts present in a result set may improve the overall search experience. Diversifying concepts present in the retrieved set is one possibility for increasing the information gain in a single search iteration, maximizing the likelihood of including at least some relevant items for each possible intent of ambiguous or underspecified queries. Relevance feedback approaches may also take advantage of diverse results to improve internal machine learning models. In this context, this work proposes and analyses several multimodal image retrieval approaches built over a learning framework for relevance feedback on diversified results. Our experimental analysis shows that different retrieval modalities are positively impacted by diversity, but achieve best retrieval effectiveness with diversification applied at different moments of a search session. Moreover, the best results are achieved with a query-by-example approach using multimodal information obtained from feedback. In summary, we demonstrate that learning with diversity is an effective alternative for boosting multimodal interactive learning approaches. © 2017 Elsevier B.V.","Diversity; Machine learning; Multimodal retrieval; Relevance feedback","Artificial intelligence; Content based retrieval; Educational technology; Image retrieval; Diversity; Experimental analysis; Machine learning models; Multi-modal; Multi-modal information; Query-by example approach; Relevance feedback; Retrieval effectiveness; Learning systems; Article; controlled study; image analysis; image processing; image retrieval; information retrieval; learning algorithm; multimedia; priority journal; process optimization; simulation",2-s2.0-85014086621
"Xi X., Xu H., Shi H., Zhang C., Ding H.Y., Zhang G., Tang Y., Yin Y.","Robust texture analysis of multi-modal images using Local Structure Preserving Ranklet and multi-task learning for breast tumor diagnosis",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013175462&doi=10.1016%2fj.neucom.2016.06.082&partnerID=40&md5=ddb77c8f3cf1020ea4867da67206bda2","Robust texture analysis of multi-modal images is important for practical breast tumor diagnosis applications. Texture features based on ranklet transform were proposed for breast tumor classification of multi-modal images and improved diagnostic performance. However, two limitations still exist in these features. Ranklet transform ignores local characteristics of images which are important for texture feature extraction. In addition, due to application of multi-resolution analysis of ranklet transform, some noises or redundant information may be introduced. These issues may result in performance degradation. To solve these problems, this paper proposes a robust texture feature based on Local Structure Preserving Ranklet (LSP-Ranklet) transform and multi-task learning. First of all, multiple LSP-Ranklet images are generated via LSP-Ranklet transform. In this procedure, the distance-based weighting method is proposed to preserve local structure of images by learning local relevance between pixels. Based on LSP-Ranklet images, texture features based on Gray-Level Co-occurrence Matrix (GLCM) are extracted. To eliminate noises of extracted features, multi-task feature learning is employed to select common feature subsets which are robust for tumor classification of multi-modal images. At last, SVM model is used for tumor classification. Experimental results on our multi-modal breast ultrasound images database demonstrate the effectiveness and robustness of the proposed feature. © 2017 Elsevier B.V.","Breast tumor diagnosis; LSP-Ranklet; Multi-modal images; Multi-task learning","Diagnosis; Feature extraction; Image analysis; Image classification; Image processing; Image texture; Learning systems; Medical imaging; Tumors; Breast tumor; Breast tumor classifications; Breast ultrasound images; Gray level co occurrence matrix(GLCM); LSP-Ranklet; Multi-modal image; Multitask learning; Texture feature extraction; Modal analysis; Article; breast tumor; controlled study; data extraction; diagnostic accuracy; human; image analysis; intermethod comparison; local structure preserving blanket; machine learning; major clinical study; multi task learning; multimodal imaging; noise reduction; predictive value; priority journal; receiver operating characteristic; robust texture analysis; sensitivity and specificity; support vector machine; tumor classification",2-s2.0-85013175462
"Yu Y., Ji Z., Guo J., Pang Y.","Zero-shot learning with regularized cross-modality ranking",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012929248&doi=10.1016%2fj.neucom.2016.06.085&partnerID=40&md5=b30a69d47697296033de950d39c5dceb","Zero-Shot Learning tries to predict the novel class samples that do not have any labeled instances in the training stage. This is typically achieved by exploring intermediate side information to transfer knowledge from seen classes to unseen testing ones. Different approaches vary in the usage of the side information and embedding methods. However, most methods only concern the relationships among different modalities but ignore to preserve the consistency among different samples in the same modality. In this paper, we propose an approach called Regularized Cross-Modality Ranking (ReCMR) to capture the semantic information from heterogeneous sources by taking both intra-modal and inter-modal semantics into consideration. Specifically, we employ the hinge ranking loss to exploit the structures among different modalities and devise efficient regularizers to constrain the variation of the samples in the identical modality. Experimental results on the popular AwA and CUB datasets show that ReCMR significantly outperforms the state-of-the-art methods. © 2017 Elsevier B.V.","Consistency preserving; Cross-modality; Image classification; Regularization; Zero-shot learning","Computer applications; Image classification; Neural networks; Consistency preserving; Cross modality; Embedding method; Heterogeneous sources; nocv1; Regularization; Semantic information; State-of-the-art methods; Zero-shot learning; Semantics; algorithm; Article; classifier; controlled study; information processing; kernel method; machine learning; measurement accuracy; priority journal; process optimization; regularized cross modality ranking; support vector machine; zero shot learning",2-s2.0-85012929248
"Bu S., Wang L., Han P., Liu Z., Li K.","3D shape recognition and retrieval based on multi-modality deep learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014534738&doi=10.1016%2fj.neucom.2016.06.088&partnerID=40&md5=b1f5e1d79de91f8ef46b2f877396ad5c","For 3D shape analysis, an effective and efficient feature is the key to popularize its applications in 3D domain where the major challenge lies in designing an effective high-level feature. The three-dimensional shape contains various useful information including visual information, geometric relationships, and other type properties. Thus the strategy of exploring these characteristics is the core of extracting effective 3D shape features. In this paper, we propose a novel 3D feature learning framework which combines different modality data effectively to promote the discriminability of uni-modal feature by using deep learning. The geometric information and visual information are extracted by Convolutional Neural Networks (CNNs) and Convolutional Deep Belief Networks (CDBNs), respectively, and then two independent Deep Belief Networks (DBNs) are employed to learn high-level features from geometric and visual features. Finally, a Restricted Boltzmann Machine (RBM) is trained for mining the deep correlations between different modalities. Extensive experiments demonstrate that the proposed framework achieves better performance. © 2017 Elsevier B.V.","3D shape; Deep learning; Multi modality; Recognition; Retrieval","Convolution; Deep neural networks; Geometry; Neural networks; 3-D shape; Convolutional neural network; Geometric relationships; Multi modality; Recognition; Restricted boltzmann machine; Retrieval; Three-dimensional shape; Deep learning; Article; artificial neural network; automated pattern recognition; classifier; convolutional deep belief network; convolutional neural network; data extraction; data mining; deep learning; image analysis; image retrieval; machine learning; multimedia; priority journal; restricted Boltzmann machine",2-s2.0-85014534738
"Li K., Wu Y., Nan Y., Li P., Li Y.","Hierarchical multi-class classification in multimodal spacecraft data using DNN and weighted support vector machine",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013477443&doi=10.1016%2fj.neucom.2016.08.131&partnerID=40&md5=b902b55807537310df8d658229ead99c","Prognostics and health management (PHM) is widely applied to assess the reliability, safety and operation of systems particularly in spacecraft systems. However, spacecraft systems are very complex with intangibility and uncertainty, and it is difficult to model and analyze the complex degradation process, and thus there is no single prognostic method for solving the critical and complicated problem. This paper presents a novel hierarchical multi-class classification method using deep neural networks (DNN) and weighted support vector machine (WSVM) in order to achieve a highly discriminative feature representation for classifying the multimodal spacecraft data. First, the stack auto-Encoder (SAE) or deep belief network is adopted to initialize the initial weights and offsets of the hierarchical multi-layer neural network in order to reduce the dimension of the original multimodal data, and the optimal depth of multi-layer neural network and the discriminative features are also obtained. Second, in order to make the high dimensional spacecraft data more separable, the initialization parameters are online monitored by using a gradient descent method. Finally, a flexible hierarchical estimation method of a multi-class weighted support vector machines (MCWSVM) is applied to classify the multimodal spacecraft data. The performance of the proposed work is evaluated by the classification accuracy, sensitivity, specificity and execution time, respectively. The results demonstrate that the proposed DNN with MCWSVM is efficient in terms of better classification accuracy at a lesser execution time when compared to K-nearest neighbors (KNN), SVM and naive Bayes method (NBM). © 2017 Elsevier B.V.","Deep belief network; Deep neural network (DNN); Multi-modal spacecraft data; Prognostics and health management (PHM); Weighted support vector machine (WSVM)","Classifiers; Complex networks; Deep neural networks; Information management; Learning systems; Nearest neighbor search; Network layers; Spacecraft; Support vector machines; Systems engineering; Uncertainty analysis; Vectors; Classification accuracy; Deep belief networks; Hierarchical estimation; K nearest neighbor (KNN); Multi-class classification; Prognostics and health managements; Spacecraft data; Weighted support vector machine; Classification (of information); Article; Bayesian learning; computer network; controlled study; data processing; deep belief network; default mode network; k nearest neighbor; measurement accuracy; multimodal spacecraft data; priority journal; sensitivity and specificity; stack auto encoder; support vector machine; weighted support vector machine",2-s2.0-85013477443
"Zhang D., Zhu Q., Zhang D.","Multi-modal dimensionality reduction using effective distance",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012927259&doi=10.1016%2fj.neucom.2016.07.075&partnerID=40&md5=509f3207f330d586fd4e6ac7bee0ae78","By providing complementary information, multi-modal data is usually helpful for obtaining good performance in the identification or classification tasks. As an important way to deal with high-dimensional features in multi-modal data, multi-modal dimensionality reduction has caused extensive concern in the machine learning domain. Most of the existing dimensionality reduction methods adopt a similarity matrix to capture the structure of data, and then this matrix is computed by using conventional distances (e.g. Euclidean distance) in most cases. However, Euclidean distance can only model the static structure of data, and the intrinsic dynamic structure information is usually ignored. For overcoming this problem, we develop two novel dimensionality reduction methods based on effective distance for multi-modal data, by using a probabilistically motivated effective distance rather than conventional Euclidean distance. Specifically, we first develop two approaches to compute the effective distance. Then, we propose two novel effective distance-based dimensionality reduction methods, including Effective Distance-based Locality Preserving Projections (EDLPP) and Effective Distance-based Sparsity Preserving Projections (EDSPP). Experiments on varied data sets from UCI machine learning repository and the Alzheimer's disease Neuroimaging Initiative (ADNI) database demonstrate that the effective distance-based dimensionality reduction methods are superior to other state-of-art methods which employ only Euclidean distance. © 2017","Dimensionality reduction; Dynamic structure; Effective distance; Multi-modal","Artificial intelligence; Classification (of information); Learning systems; Matrix algebra; Modal analysis; Neurodegenerative diseases; Neuroimaging; Dimensionality reduction; Dimensionality reduction method; Dynamic structure; Effective distance; Locality preserving projections; Multi-modal; Sparsity preserving projections; UCI machine learning repository; Data reduction; algorithm; Alzheimer disease; Article; classifier; effective distance; Euclidean distance; k nearest neighbor; locality preserving projection; machine learning; mathematical computing; multimedia; neuroimaging; priority journal; sparse preserving projection",2-s2.0-85012927259
"Gigoni L., Betti A., Crisostomi E., Franco A., Tucci M., Bizzarri F., Mucci D.","Day-Ahead Hourly Forecasting of Power Generation from Photovoltaic Plants",2017,"IEEE Transactions on Sustainable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765425&doi=10.1109%2fTSTE.2017.2762435&partnerID=40&md5=1899bffc13a29dc127432ae7aa5f6f15","The ability to accurately forecast power generation from renewable sources is nowadays recognised as a fundamental skill to improve the operation of power systems. Despite the general interest of the power community in this topic, it is not always simple to compare different forecasting methodologies, and infer the impact of single components in providing accurate predictions. In this paper we extensively compare simple forecasting methodologies with more sophisticated ones over 32 photovoltaic plants of different size and technology over a whole year. Also, we try to evaluate the impact of weather forecasts on the prediction of PV power generation. IEEE","Machine Learning algorithms; power generation forecasts; PV plants","Electric power systems; Forecasting; Learning algorithms; Learning systems; Accurate prediction; Operation of power system; PhotoVoltaic plant; Power generation forecasts; Power generation from renewable; PV plants; PV power generation; Single components; Weather forecasting",2-s2.0-85031765425
"An L., Chen X., Yang S.","Multi-graph feature level fusion for person re-identification",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012876545&doi=10.1016%2fj.neucom.2016.08.127&partnerID=40&md5=9af3dec4df58cc8f653b1ba676a6a16b","Person re-identification refers to the task of matching people in non-overlapping cameras. As the concerns for public safety keep rising, the ability to accurately identify a subject in surveillance cameras is a highly demanded technique. In practice, person re-identification is challenging due to the substantial appearance shift caused by view change. Many factors, such as illumination, pose, and image quality, can affect the matching accuracy. In the past, many feature descriptors have been engineered for more robust matching in certain cases. In this paper, we propose a graph-based feature fusion scheme to effectively leverage different feature descriptors. Moreover, instead of determining the matching results by computing pairwise distance between a unknown probe and a gallery subject in the database, we learn the similarity scores between a probe and all the gallery subjects simultaneously in a graph learning framework. We use off-the-shelf features and test our method on popular benchmark datasets for person re-identification. Experimental results show that different feature descriptors can be effectively combined through this graph learning scheme and superior results are achieved as compared with the rival approaches. © 2017 Elsevier B.V.","Feature fusion; Graph learning; Multi-camera; Person re-identification","Cameras; Graphic methods; Probes; Feature descriptors; Feature fusion; Graph learning; Graph-based features; Multi-cameras; Non-overlapping cameras; Person re identifications; Surveillance cameras; Security systems; Article; benchmarking; camera; controlled study; data extraction; identity recognition; image processing; intermethod comparison; machine learning; measurement accuracy; multi graph feature level fusion; priority journal",2-s2.0-85012876545
"Liu M., Cao L., Lu F., Zhen Y.","Multimodal media data understanding and analysis",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006857047&doi=10.1016%2fj.neucom.2016.10.067&partnerID=40&md5=45c7fe301324d2ee07c8e3350e6be5a7",[No abstract available],,"algorithm; automated pattern recognition; breast tumor; classifier; data mining; Editorial; gaze; global positioning system; human; image retrieval; image segmentation; machine learning; multimedia; neuroimaging; priority journal; robotics; sensor; sign language; sleep; support vector machine",2-s2.0-85006857047
"Jiang X., Zhang H., Duan F., Quan X.","Identify Huntington's disease associated genes based on restricted Boltzmann machine with RNA-seq data",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030853005&doi=10.1186%2fs12859-017-1859-6&partnerID=40&md5=8f459a806a9a326a6e8fe374d825d38a","Background: Predicting disease-associated genes is helpful for understanding the molecular mechanisms during the disease progression. Since the pathological mechanisms of neurodegenerative diseases are very complex, traditional statistic-based methods are not suitable for identifying key genes related to the disease development. Recent studies have shown that the computational models with deep structure can learn automatically the features of biological data, which is useful for exploring the characteristics of gene expression during the disease progression. Results: In this paper, we propose a deep learning approach based on the restricted Boltzmann machine to analyze the RNA-seq data of Huntington's disease, namely stacked restricted Boltzmann machine (SRBM). According to the SRBM, we also design a novel framework to screen the key genes during the Huntington's disease development. In this work, we assume that the effects of regulatory factors can be captured by the hierarchical structure and narrow hidden layers of the SRBM. First, we select disease-associated factors with different time period datasets according to the differentially activated neurons in hidden layers. Then, we select disease-associated genes according to the changes of the gene energy in SRBM at different time periods. Conclusions: The experimental results demonstrate that SRBM can detect the important information for differential analysis of time series gene expression datasets. The identification accuracy of the disease-associated genes is improved to some extent using the novel framework. Moreover, the prediction precision of disease-associated genes for top ranking genes using SRBM is effectively improved compared with that of the state of the art methods. © 2017 The Author(s).","Huntington's disease; Key genes associated to the disease progression; Restricted Boltzmann machine; RNA-seq data","Gene expression; Genes; RNA; Time series analysis; Disease progression; Hierarchical structures; Huntington's disease; Identification accuracy; Restricted boltzmann machine; RNA-Seq datum; State-of-the-art methods; Time series gene expressions; Neurodegenerative diseases",2-s2.0-85030853005
"Zu C., Zhu L., Zhang D.","Iterative sparsity score for feature selection and its extension for multimodal data",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014110335&doi=10.1016%2fj.neucom.2016.08.124&partnerID=40&md5=da320b6c21ba3e63f11d986ed6430a4a","As a key dimensionality reduction technique in pattern recognition, feature selection has been widely used in information retrieval, text classification and genetic data analysis. In recent years, structural information contained in samples for guiding feature selection has become a new hot spot in machine learning field. Although tremendous feature selection methods have been developed, less important features are still used to construct the structure in those conventional structure based feature selection approaches. In this paper, we propose a new filter-type feature selection method called iterative sparsity score, which is independent of any learning algorithm. The proposed method can preserve the structural information by sparse representation, which can be efficiently solved by a ℓ1-norm minimization problem. To exclude data noise, at one time we discard last m features and iteratively optimize the ℓ1-norm minimization problem. We perform clustering and classification experiments on numerous bench mark datasets. Furthermore, its extension for multimodal data is also developed. We adopt the multi-modality alzheimer's disease data for classification to evaluate the extended method. The experimental results show the effectiveness of our proposed methods compared with several popular feature selection approaches. © 2017 Elsevier B.V.","Alzheimer's disease; Classification; Clustering; Feature selection; Iterative sparse representation; Multi-modality","Character recognition; Feature extraction; Iterative methods; Learning algorithms; Learning systems; Neurodegenerative diseases; Pattern recognition; Text processing; 00-01; 99-00; Alzheimer's disease; Clustering; Multi modality; Sparse representation; Classification (of information); algorithm; Alzheimer disease; Article; automated pattern recognition; classifier; cluster analysis; digital filtering; feature selection; human; iterative sparsity score; multimedia; nuclear magnetic resonance imaging; positron emission tomography; priority journal",2-s2.0-85014110335
"Bachoc F., Gamboa F., Loubes J., Venet N.","A Gaussian Process Regression Model for Distribution Inputs",2017,"IEEE Transactions on Information Theory",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031796856&doi=10.1109%2fTIT.2017.2762322&partnerID=40&md5=69b47ddffb052e7ddda7fd0a18e38f24","Monge-Kantorovich distances, otherwise known as Wasserstein distances, have received a growing attention in statistics and machine learning as a powerful discrepancy measure for probability distributions. In this paper, we focus on forecasting a Gaussian process indexed by probability distributions. For this, we provide a family of positive definite kernels built using transportation based distances. We provide a probabilistic understanding of these kernels and characterize the corresponding stochastic processes. We prove that the Gaussian processes indexed by distributions corresponding to these kernels can be efficiently forecast, opening new perspectives in Gaussian process modeling. IEEE","Fractional Brownian motion; Gaussian process; Kriging; Monge-Kantorovich distance; Positive definite kernel","Brownian movement; Gaussian distribution; Gaussian noise (electronic); Learning systems; Random processes; Regression analysis; Stochastic systems; Fractional brownian motion; Gaussian Processes; Kriging; Monge-Kantorovich distance; Positive definite kernels; Probability distributions",2-s2.0-85031796856
"Nossek R.Z., Gilboa G.","Flows Generating Nonlinear Eigenfunctions",2017,"Journal of Scientific Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031088715&doi=10.1007%2fs10915-017-0577-6&partnerID=40&md5=a3c6d8f7e2fc46b3afd4049a9ffd790b","Linear eigenvalue analysis has provided a fundamental framework for many scientific and engineering disciplines. Consequently, vast research was devoted to numerical schemes for computing eigenfunctions. In recent years, new research in image processing and machine-learning has shown the applicability of nonlinear eigenvalue analysis, specifically based on operators induced by convex functionals. This has provided new insights, better theoretical understanding and improved image-processing, clustering and classification algorithms. However, the theory of nonlinear eigenvalue problems is still very preliminary. We present a new class of nonlinear flows that can generate nonlinear eigenfunctions of the form (Formula presented.), where T(u) is a nonlinear operator and (Formula presented.) is the eigenvalue. We develop the theory where T(u) is a subgradient element of a regularizing one-homogeneous functional, such as total-variation or total-generalized-variation. We focus on a forward flow which simultaneously smooths the solution (with respect to the regularizer) while increasing the 2-norm. An analog discrete flow and its normalized version are formulated and analyzed. The flows translate to a series of convex minimization steps. In addition we suggest an indicator to measure the affinity of a function to an eigenfunction and relate it to pseudo-eigenfunctions in the linear case. © 2017 Springer Science+Business Media, LLC","Nonlinear eigenfunctions; Nonlinear flows; Nonlinear spectral theory; One-homogeneous functionals; Total-variation; Variational methods","Clustering algorithms; Computation theory; Image enhancement; Image processing; Learning systems; Mathematical operators; Functionals; Nonlinear flow; Spectral theory; Total variation; Variational methods; Eigenvalues and eigenfunctions",2-s2.0-85031088715
"Forstner M.","The translators' new clothes. Translation economy in times of digitalization, datafication, and big data management [Des translators neue kleider. Die translationwirtschaft in zeiten von digitalisierung, datafizierung und big data management]",2017,"Lebende Sprachen",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032228879&doi=10.1515%2fles-2017-0027&partnerID=40&md5=26d055abc1912e6ab3ecf78613abd828","The Internet of things will influence all professional environments, including translation services. Advances in machine learning, supported by accelerating improvements in computer linguistics, have enabled new systems that can learn from their own experience and will have repercussions on the workflow processes of translators or even put their services at risk in the expected digitalized society. Outsourcing has become a common practice and working in the cloud and in the crowd tend to enable translating on a very low-cost level. Confronted with promising new labels like Industry 4.0 and Work 4.0, professional freelance translators will have to organize themselves as smart offices by integrating digitalization and datafication and Big Data Management. Most probably, those developments will change translation training and services in the professional world of translation industry. © 2017 Walter de Gruyter GmbH, Berlin/Boston 2017.","artificial intelligence; Big Data Management; computer-assisted translation; Digitalization; translator's smart office",,2-s2.0-85032228879
"Cheng W.","Translation and big data technology: Challenges and implications",2017,"Lebende Sprachen",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032195530&doi=10.1515%2fles-2017-0019&partnerID=40&md5=955b310316010f9d48a6feb0dd627601","In China, a growing market for language service along and rapidly-developing technology have ushered in a new wave of T&I software development. Despite translation tools' wide range of applications and the current explosion of AI, translation memory and translation tools' machine learning algorithms are far from satisfactory in providing language solutions. Unfortunately, an overreliance on technology has reduced some translators to machine operators or post-editors, as their traditional skills of critical thinking, analysis, and aesthetic pursuit have declined. By analyzing cases where machine translation gave incorrect suggestions, this article aims to explore the relationship between emerging technology and traditional professionalism in T&I. The author believes that technology and professionalism are not in a zero-sum game; technology does not deprive translators of job opportunities, but rather their analytical skills, critical thinking and aesthetic pursuits. While technology is going to streamline the more basic and repetitive aspects of translation work, in-depth cultural exchanges and communication will only be achievable by professional translators. © 2017 Walter de Gruyter GmbH, Berlin/Boston 2017.","artificial intelligence; big data; intercultural competence; translation",,2-s2.0-85032195530
"Prati R.C., Said-Hung E.","Predicting the ideological orientation during the Spanish 24M elections in Twitter using machine learning",2017,"AI and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030838466&doi=10.1007%2fs00146-017-0761-0&partnerID=40&md5=aa5ec47389d3fdbb61ff6bdef18f162a","Through the application of machine learning techniques, this paper aims to estimate the importance of messages with ideological load during the elections held in Spain on May 24th, 2015 posted by Twitter’s users, as well as other variables associated with the publication of these types of messages. Our study collected and analysed 24,900 tweets associated to two of the main trending topics’ hashtags (#24M and #Elections2015) used in the election day and build a predictive model to infer the ideological orientation for the messages which made use of these hashtags during Election Day. This approach allows us to classify the ideological orientation of all collected tweets, instead of only tweets that explicitly express their ideological or partisan preferences in the messages. Using the ideological orientation for all tweets predicted by our model, it was possible to identify how messages with a defined ideological load were pushed forward by users with leftist tendencies. We also observed a relationship between these messages and the partisan orientation of those who published them. © 2017 Springer-Verlag London Ltd.","Elections; Ideology; Machine learning; Political participation; Social media; Spain","Artificial intelligence; Social networking (online); Elections; Ideology; Political participations; Social media; Spain; Learning systems",2-s2.0-85030838466
"Zhang C., Liu Q., Wu Q., Zheng Y., Zhou J., Tu Z., Chan S.H.","Modelling of solid oxide electrolyser cell using extreme learning machine",2017,"Electrochimica Acta",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028329640&doi=10.1016%2fj.electacta.2017.08.113&partnerID=40&md5=e8df7c03eeb29b582d396bf366823c43","Solid Oxide Electrolyzer Cell (SOEC) can covert H2O and/or CO2 into usable fuel by consuming the excess electricity of renewable resource or off-peak grid power. The SOEC is a promising device for the sustainable development of energy and hydrogen economy. In this work, the steady-state performance of SOEC is tested under different gas compositions and modelled by extreme learning machine (ELM) algorithm. According to the experimental results, the concentrations of H2O and CO2 influence the performance of SOEC. For the model, the inputs are the operating voltage and volume percentage of H2, CO2, and H2O, while the output is the performance (current) of SOEC. The obtained model has correlation coefficients of higher than 0.999 and root mean square error less than 0.018, which means that the predicted data by the model well matches the experimental results. Then, the obtained ELM model is used to analyse the performances of SOEC under different concentrations of feedstock. Thus, this data driven ELM model is suitable for many instances of fast modelling for individual group and may be helpful to save the cost, time and effort to build a model for the purpose of performance analysis and system level design. © 2017 Elsevier Ltd","Electrolysis; Extreme learning machine; Modelling; SOEC","Carbon dioxide; Electrolysis; Hydrogen fuels; Knowledge acquisition; Learning systems; Mean square error; Models; Regenerative fuel cells; Correlation coefficient; Extreme learning machine; Performance analysis; Root mean square errors; SOEC; Solid oxide electrolyzer cells (SOEC); Steady state performance; System level design; Solid oxide fuel cells (SOFC)",2-s2.0-85028329640
"Li X., Xie Y., Hu D., Lan Z.","Analysis of the Geometrical Evolution in On-the-Fly Surface-Hopping Nonadiabatic Dynamics with Machine Learning Dimensionality Reduction Approaches: Classical Multidimensional Scaling and Isometric Feature Mapping",2017,"Journal of Chemical Theory and Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030983606&doi=10.1021%2facs.jctc.7b00394&partnerID=40&md5=cc68b44b77e76bea43d1b442f0453a1a","On-the-fly trajectory-based nonadiabatic dynamics simulation has become an important approach to study ultrafast photochemical and photophysical processes in recent years. Because a large number of trajectories are generated from the dynamics simulation of polyatomic molecular systems with many degrees of freedom, the analysis of simulation results often suffers from the large amount of high-dimensional data. It is very challenging but meaningful to find dominating active coordinates from very complicated molecular motions. Dimensionality reduction techniques provide ideal tools to realize this purpose. We apply two dimensionality reduction approaches (classical multidimensional scaling and isometric feature mapping) to analyze the results of the on-the-fly surface-hopping nonadiabatic dynamics simulation. Two representative model systems, CH2NH2 + and the phytochromobilin chromophore model, are chosen to examine the performance of these dimensionality reduction approaches. The results show that these approaches are very promising, because they can extract the major molecular motion from complicated time-dependent molecular evolution without preknown knowledge. © 2017 American Chemical Society.",,,2-s2.0-85030983606
"Mathew J., Pang C.K., Luo M., Leong W.H.","Classification of Imbalanced Data by Oversampling in Kernel Space of Support Vector Machines",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031798179&doi=10.1109%2fTNNLS.2017.2751612&partnerID=40&md5=91dc91273edf14f43156fc192cb763fd","Historical data sets for fault stage diagnosis in industrial machines are often imbalanced and consist of multiple categories or classes. Learning discriminative models from such data sets is challenging due to the lack of representative data and the bias of traditional classifiers toward the majority class. Sampling methods like synthetic minority oversampling technique (SMOTE) have been traditionally used for such problems to artificially balance the data set before being trained by a classifier. This paper proposes a weighted kernel-based SMOTE (WK-SMOTE) that overcomes the limitation of SMOTE for nonlinear problems by oversampling in the feature space of support vector machine (SVM) classifier. The proposed oversampling algorithm along with a cost-sensitive SVM formulation is shown to improve performance when compared to other baseline methods on multiple benchmark imbalanced data sets. In addition, a hierarchical framework is developed for multiclass imbalanced problems that have a progressive class order. The proposed WK-SMOTE and hierarchical framework are validated on a real-world industrial fault detection problem to identify deterioration in insulation of high-voltage equipments. IEEE","Classification algorithms; Cost-sensitive learning; fault detection; Fault diagnosis; imbalanced classification; Kernel; kernel methods; Learning systems; multiclass; Sampling methods; support vector machine; Support vector machines; Training","Benchmarking; Computer aided diagnosis; Failure analysis; Fault detection; Learning systems; Personnel training; Support vector machines; Vector spaces; Vectors; Classification algorithm; Cost-sensitive learning; Imbalanced classification; Kernel; Kernel methods; multiclass; Sampling method; Classification (of information)",2-s2.0-85031798179
"Ruffalo M., Stojanov P., Pillutla V.K., Varma R., Bar-Joseph Z.","Reconstructing cancer drug response networks using multitask learning",2017,"BMC Systems Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030861280&doi=10.1186%2fs12918-017-0471-8&partnerID=40&md5=5e3dd42fceb5e54a2f3592531e6300e4","Background: Translating in vitro results to clinical tests is a major challenge in systems biology. Here we present a new Multi-Task learning framework which integrates thousands of cell line expression experiments to reconstruct drug specific response networks in cancer. Results: The reconstructed networks correctly identify several shared key proteins and pathways while simultaneously highlighting many cell type specific proteins. We used top proteins from each drug network to predict survival for patients prescribed the drug. Conclusions: Predictions based on proteins from the in-vitro derived networks significantly outperformed predictions based on known cancer genes indicating that Multi-Task learning can indeed identify accurate drug response networks. © 2017 The Author(s).","LINCS; Machine learning; TCGA",,2-s2.0-85030861280
"Wu T., Wen S., Liu S., Zhang J., Xiang Y., Alrubaian M., Hassan M.M.","Detecting spamming activities in twitter based on deep-learning technique",2017,"Concurrency Computation ",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021148844&doi=10.1002%2fcpe.4209&partnerID=40&md5=4a75c1a3777636e85710e7006ae3a06b","Twitter spam has long been a critical but difficult problem to be addressed. So far, researchers have developed a series of machine learning–based methods and blacklisting techniques to detect spamming activities on Twitter. According to our investigation, current methods and techniques have achieved the accuracy of around 87%. However, because of the problems of spam drift and information fabrication, these machine learning–based methods cannot efficiently detect spam activities in real-life scenarios. Meanwhile, the blacklisting method also cannot catch up with the variations of spamming activities, as manually inspecting suspicious URLs is extremely timeconsuming. In this paper, we proposed a novel technique based on deep-learning technique to address the above challenges. The syntax of each tweet will be learned through WordVector and trained by deep learning. We then constructed a binary classifier to differentiate spam and regular tweets. In experiments, we collected and labeled a 10-day real tweet dataset as ground truth to evaluate our proposed method. We first went for empirical analysis with a series of comparisons to other methods: (1) performance of different classifiers, (2) other existing text-based methods, and (3) nontext-based detection techniques. According to the experiment results, our proposed method largely outperformed previous methods. We further conducted principle component analysis on typical methods to theoretically justify the outperformance of our method. We extracted all kinds of features via dimensionality reduction. It was found that our features were most distinct among all the detection methods. This well demonstrated the outperformance of our method. Copyright © 2017 John Wiley & Sons, Ltd.","deep learning; social media security; twitter spam detection","Artificial intelligence; Education; Learning algorithms; Learning systems; Principal component analysis; Social networking (online); Spamming; Binary classifiers; Dimensionality reduction; Empirical analysis; Learning techniques; Principle component analysis; Social media; Spam detection; Text-based methods; Deep learning",2-s2.0-85021148844
"Li S., Chen J., Liu B.","Protein remote homology detection based on bidirectional long short-term memory",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030849925&doi=10.1186%2fs12859-017-1842-2&partnerID=40&md5=3abc6bc97fd656a54d04b94a55c708f6","Background: Protein remote homology detection plays a vital role in studies of protein structures and functions. Almost all of the traditional machine leaning methods require fixed length features to represent the protein sequences. However, it is never an easy task to extract the discriminative features with limited knowledge of proteins. On the other hand, deep learning technique has demonstrated its advantage in automatically learning representations. It is worthwhile to explore the applications of deep learning techniques to the protein remote homology detection. Results: In this study, we employ the Bidirectional Long Short-Term Memory (BLSTM) to learn effective features from pseudo proteins, also propose a predictor called ProDec-BLSTM: it includes input layer, bidirectional LSTM, time distributed dense layer and output layer. This neural network can automatically extract the discriminative features by using bidirectional LSTM and the time distributed dense layer. Conclusion: Experimental results on a widely-used benchmark dataset show that ProDec-BLSTM outperforms other related methods in terms of both the mean ROC and mean ROC50 scores. This promising result shows that ProDec-BLSTM is a useful tool for protein remote homology detection. Furthermore, the hidden patterns learnt by ProDec-BLSTM can be interpreted and visualized, and therefore, additional useful information can be obtained. © 2017 The Author(s).","Bidirectional Long Short-Term Memory; Neural network; Protein remote homology detection; Protein sequence analysis","Brain; Deep learning; Learning algorithms; Learning systems; Neural networks; Proteins; Benchmark datasets; Discriminative features; Learning techniques; Machine leaning; Protein sequence analysis; Protein sequences; Protein structures; Remote homology detections; Long short-term memory",2-s2.0-85030849925
"Zeng J., Liu Y., Leng B., Xiong Z., Cheung Y.","Dimensionality Reduction in Multiple Ordinal Regression",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774002&doi=10.1109%2fTNNLS.2017.2752003&partnerID=40&md5=9fe9de506b563bcca84f1f4ae6045cd1","Supervised dimensionality reduction (DR) plays an important role in learning systems with high-dimensional data. It projects the data into a low-dimensional subspace and keeps the projected data distinguishable in different classes. In addition to preserving the discriminant information for binary or multiple classes, some real-world applications also require keeping the preference degrees of assigning the data to multiple aspects, e.g., to keep the different intensities for co-occurring facial expressions or the product ratings in different aspects. To address this issue, we propose a novel supervised DR method for DR in multiple ordinal regression (DRMOR), whose projected subspace preserves all the ordinal information in multiple aspects or labels. We formulate this problem as a joint optimization framework to simultaneously perform DR and ordinal regression. In contrast to most existing DR methods, which are conducted independently of the subsequent classification or ordinal regression, the proposed framework fully benefits from both of the procedures. We experimentally demonstrate that the proposed DRMOR method (DRMOR-M) well preserves the ordinal information from all the aspects or labels in the learned subspace. Moreover, DRMOR-M exhibits advantages compared with representative DR or ordinal regression algorithms on three standard data sets. IEEE","Algorithm design and analysis; Computer science; Dimensionality reduction (DR); Learning systems; multiple labels; ordinal regression; Standards; supervised; Support vector machines; Transforms","Clustering algorithms; Computer science; Learning systems; Mathematical transformations; Optimization; Standards; Support vector machines; Vectors; Algorithm design and analysis; Dimensionality reduction; Multiple labels; Ordinal regression; supervised; Regression analysis",2-s2.0-85031774002
"Tan C., Ji G.","Semisupervised local preserving embedding algorithm based on maximum margin criterion for large-scale data streams",2017,"Concurrency Computation ",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412176&doi=10.1002%2fcpe.4246&partnerID=40&md5=c75ca5e94af36544cde9870afa258cf3","In the field of machine learning, feature extraction is one of the most important preprocessing in data classification for its effectiveness, and now it has attracted much extensive attention for large-scale data stream preprocessing step, especially in the era of big data. Motivated by the advantages of unsupervised and supervised feature extraction, which are two desirable and promising characteristics for dimension reduction, a new semisupervised local preserving embedding algorithm based on maximum margin criterion (SLPE/MMC) is proposed in this paper. First, the objective functions of maximum margin criterion (MMC) and neighborhood preserving embedding (NPE) are combined to get the first objective function of SLPE/MMC. Then, in order to overcome the out-of-sample problem, a linear transformation is introduced to construct the second objective function. At last, the whole optimal objective function is constructed by combing the two objective functions together. The proposed algorithm has effectively taken advantage of the sample's supervised information and keeps the geometry structure and the class discrimination information of the manifold. Experiments on face datasets Yale, CMU PIE, and AR datasets are performed to evaluate the classification accuracy of SLPE/MMC. The experimental results and time complexity comparisons have demonstrated the effectiveness of the proposed method. Copyright © 2017 John Wiley & Sons, Ltd.","class discrimination information; feature extraction; large-scale data stream; neighborhood geometry relationship","Big data; Classification (of information); Data communication systems; Data reduction; Extraction; Feature extraction; Learning algorithms; Learning systems; Linear transformations; Mathematical transformations; Classification accuracy; Discrimination informations; Embedding algorithms; Geometry relationships; Large scale data; Maximum margin criterions; Neighborhood preserving embedding; Supervised feature extractions; Data mining",2-s2.0-85028412176
"Fan J., Guan C., Ren K., Cui Y., Qiao C.","SPABox: Safeguarding Privacy During Deep Packet Inspection at a MiddleBox",2017,"IEEE/ACM Transactions on Networking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031915563&doi=10.1109%2fTNET.2017.2753044&partnerID=40&md5=657ca55456952f1d23e25b02eec7c3d6","Widely used over the Internet to encrypt traffic, HTTPS provides secure and private data communication between clients and servers. However, to cope with rapidly changing and sophisticated security attacks, network operators often deploy middleboxes to perform deep packet inspection (DPI) to detect attacks and potential security breaches, using techniques ranging from simple keyword matching to more advanced machine learning and data mining analysis. But this creates a problem: how can middleboxes, which employ DPI, work over HTTPS connections with encrypted traffic while preserving privacy? In this paper, we present SPABox, a middlebox-based system that supports both keyword-based and data analysis-based DPI functions over encrypted traffic. SPABox preserves privacy by using a novel protocol with a limited connection setup overhead. We implement SPABox on a standard server and show that SPABox is practical for both long-lived and short-lived connection. Compared with the state-of-the-art Blindbox system, SPABox is more than five orders of magnitude faster and requires seven orders of magnitude less bandwidth for connection setup while SPABox can achieve a higher security level. IEEE","Cryptography; Data privacy; DPI; Malware; middlebox; Middleboxes; Privacy; privacy preserving.; Protocols","Computer crime; Cryptography; Data mining; HTTP; Learning systems; Malware; Network protocols; Deep packet inspection; Deep packet inspection (DPI); Encrypted traffic; Key word matching; Middleboxes; Orders of magnitude; Privacy preserving; Security breaches; Data privacy",2-s2.0-85031915563
"Kern N.S., Liu A., Parsons A.R., Mesinger A., Greig B.","Emulating Simulations of Cosmic Dawn for 21 cm Power Spectrum Constraints on Cosmology, Reionization, and X-Ray Heating",2017,"Astrophysical Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031930707&doi=10.3847%2f1538-4357%2faa8bb4&partnerID=40&md5=32fe81cae606b44dde01f44092114fa6","Current and upcoming radio interferometric experiments are aiming to make a statistical characterization of the high-redshift 21 cm fluctuation signal spanning the hydrogen reionization and X-ray heating epochs of the universe. However, connecting 21 cm statistics to the underlying physical parameters is complicated by the theoretical challenge of modeling the relevant physics at computational speeds quick enough to enable exploration of the high-dimensional and weakly constrained parameter space. In this work, we use machine learning algorithms to build a fast emulator that can accurately mimic an expensive simulation of the 21 cm signal across a wide parameter space. We embed our emulator within a Markov Chain Monte Carlo framework in order to perform Bayesian parameter constraints over a large number of model parameters, including those that govern the Epoch of Reionization, the Epoch of X-ray Heating, and cosmology. As a worked example, we use our emulator to present an updated parameter constraint forecast for the Hydrogen Epoch of Reionization Array experiment, showing that its characterization of a fiducial 21 cm power spectrum will considerably narrow the allowed parameter space of reionization and heating parameters, and could help strengthen Planck's constraints on σg. We provide both our generalized emulator code and its implementation specifically for 21 cm parameter constraints as publicly available software. © 2017. The American Astronomical Society. All rights reserved.","dark ages, reionization, first stars; methods: numerical; methods: statistical",,2-s2.0-85031930707
"Pardakhti M., Moharreri E., Wanik D., Suib S.L., Srivastava R.","Machine Learning Using Combined Structural and Chemical Descriptors for Prediction of Methane Adsorption Performance of Metal Organic Frameworks (MOFs)",2017,"ACS Combinatorial Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030786303&doi=10.1021%2facscombsci.7b00056&partnerID=40&md5=c09857b1a770d5c675e0bf1ddfb27265","Using molecular simulation for adsorbent screening is computationally expensive and thus prohibitive to materials discovery. Machine learning (ML) algorithms trained on fundamental material properties can potentially provide quick and accurate methods for screening purposes. Prior efforts have focused on structural descriptors for use with ML. In this work, the use of chemical descriptors, in addition to structural descriptors, was introduced for adsorption analysis. Evaluation of structural and chemical descriptors coupled with various ML algorithms, including decision tree, Poisson regression, support vector machine and random forest, were carried out to predict methane uptake on hypothetical metal organic frameworks. To highlight their predictive capabilities, ML models were trained on 8% of a data set consisting of 130,398 MOFs and then tested on the remaining 92% to predict methane adsorption capacities. When structural and chemical descriptors were jointly used as ML input, the random forest model with 10-fold cross validation proved to be superior to the other ML approaches, with an R2 of 0.98 and a mean absolute percent error of about 7%. The training and prediction using the random forest algorithm for adsorption capacity estimation of all 130,398 MOFs took approximately 2 h on a single personal computer, several orders of magnitude faster than actual molecular simulations on high-performance computing clusters. © 2017 American Chemical Society.","computational screening; machine learning; metal-organic frameworks; methane adsorption; predictive modeling","metal; organometallic compound; adsorption; algorithm; chemistry; computer simulation; machine learning; software; Adsorption; Algorithms; Computer Simulation; Machine Learning; Metals; Organometallic Compounds; Software",2-s2.0-85030786303
"Gilanie G., Bajwa U.I., Waraich M.M., Habib Z., Ullah H., Nasir M.","Classification of normal and abnormal brain MRI slices using Gabor texture and support vector machines",2017,"Signal, Image and Video Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030873627&doi=10.1007%2fs11760-017-1182-8&partnerID=40&md5=016237e7ce758128530d521efb4a1c8b","In computational and clinical environments, autoclassification of brain magnetic resonance image (MRI) slices as normal and abnormal is challenging. The purpose of this study is to investigate the computer vision and machine learning methods for classification of brain magnetic resonance (MR) slices. In routine health-care units, MR scanners are being used to generate a massive number of brain slices, underlying the anatomical details. Pathological assessment from this medical data is being carried out manually by the radiologists or neuro-oncologists. It is almost impossible to analyze each slice manually due to the large amount of data produced by MRI devices at each moment. Irrefutably, if an automated protocol performing this task is executed, not only the radiologist will be assisted, but a better pathological assessment process can also be expected. Numerous schemes have been reported to address the issue of autoclassification of brain MRI slices as normal and abnormal, but accuracy, robustness and optimization are still an open issue. The proposed method, using Gabor filter and support vector machines, classifies brain MRI slices as normal or abnormal. Accuracy, sensitivity, specificity and ROC-curve have been used as standard quantitative measures to evaluate the proposed algorithm. To the best of our knowledge, this is the first study in which experiments have been performed on Whole Brain Atlas-Harvard Medical School (HMS) dataset, achieving an accuracy of 97.5%, sensitivity of 99%, specificity of 92% and ROC-curve as 0.99. To test the robustness against medical traits based on ethnicity and to achieve optimization, a locally developed dataset has also been used for experiments and remarkable results with accuracy (96.5%), sensitivity (98%), specificity (92%) and ROC-curve (0.97) were achieved. Comparison with state-of-the-art methods proved the overall efficacy of the proposed method. © 2017 Springer-Verlag London Ltd.","Brain; Classification; MRI; Normal and abnormal brain slices","Brain; Classification (of information); Computer vision; Curve fitting; Gabor filters; Learning systems; Statistical tests; Support vector machines; Assessment process; Brain magnetic resonance images; Brain slices; Clinical environments; Harvard Medical School; Machine learning methods; Quantitative measures; State-of-the-art methods; Magnetic resonance imaging",2-s2.0-85030873627
"Mai L., Noh D.K.","Cluster Ensemble with Link-Based Approach for Botnet Detection",2017,"Journal of Network and Systems Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030837957&doi=10.1007%2fs10922-017-9436-x&partnerID=40&md5=0acc3fa1581eb33fbbbe7c02694aeaae","Botnet detection is one of the most imminent tasks for cyber security. Among popular botnet countermeasures, an intrusion detection system is the prominent mechanism. In the past, packet-based intrusion detection systems were popular. However, flow-based intrusion detection systems have been preferred in recent years due to their ability to adapt to modern high-speed networks. A collection of flows from an enterprise network usually contains both botnet traffic and normal traffic. To classify this traffic, supervised machine learning algorithms, i.e., classifications, have been applied and achieved a high accuracy. In an effort to improve the ability of intrusion detection systems against botnets, some studies have suggested partitioning flows into clusters before applying the classifications and this step could significantly reduce the complexity of a flow set. However, the instability of individual clustering algorithms is still a constraint for botnet detection.To overcome this bottleneck, we propose a novel method that combines individual partitions to become a strong learner through the use of a link-based algorithm. Our experiments show that our cluster ensemble model outperforms existing botnet detection mechanisms with a high reliability. We also determine the balance between accuracy and computer resources for botnet detection, and thereby propose a range for the maximum duration time of flows in botnet research. © 2017 Springer Science+Business Media, LLC","Classification; Command and control; Cyber crime; Intrusion detection system; Machine learning; Network flow","Artificial intelligence; Classification (of information); Clustering algorithms; Command and control systems; Computer crime; Data flow analysis; HIgh speed networks; Intrusion detection; Learning algorithms; Learning systems; Mercury (metal); Supervised learning; Command and control; Computer resources; Cyber-crimes; Enterprise networks; Intrusion Detection Systems; Link-based approach; Network flows; Supervised machine learning; Botnet",2-s2.0-85030837957
"Salawu S., He Y., Lumsden J.","Approaches to Automated Detection of Cyberbullying: A Survey",2017,"IEEE Transactions on Affective Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765182&doi=10.1109%2fTAFFC.2017.2761757&partnerID=40&md5=6dc63ff1c99bd06022b96705b1cbf3d1","Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely; supervised learning, lexicon based, rule based and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Na&#x00EF;ve Bayes to develop predictive models for cyberbullying detection. Lexicon based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rules-based approaches match text to predefined rules to identify bullying and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of quality representative labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field. IEEE","Abuse and crime involving computers; Computers; data mining; Electronic mail; machine learning; natural language processing; Sentiment analysis; sentiment analysis; Social network services; social networking; Supervised learning","Artificial intelligence; Computers; Data mining; Electronic mail; Information services; Learning algorithms; Learning systems; Mobile devices; Natural language processing systems; Social networking (online); Supervised learning; Abuse and crime involving computers; Automated detection; Bibliographic database; Detection approach; Learning-based approach; Literature reviews; Sentiment analysis; Social network services; Computer crime",2-s2.0-85031765182
"Lima R., Espinasse B., Freitas F.","OntoILPER: an ontology- and inductive logic programming-based system to extract entities and relations from text",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030849258&doi=10.1007%2fs10115-017-1108-3&partnerID=40&md5=547a569fba2b1e4253593f5230953d91","Named entity recognition (NER) and relation extraction (RE) are two important subtasks in information extraction (IE). Most of the current learning methods for NER and RE rely on supervised machine learning techniques with more accurate results for NER than RE. This paper presents OntoILPER a system for extracting entity and relation instances from unstructured texts using ontology and inductive logic programming, a symbolic machine learning technique. OntoILPER uses the domain ontology and takes advantage of a higher expressive relational hypothesis space for representing examples whose structure is relevant to IE. It induces extraction rules that subsume examples of entities and relation instances from a specific graph-based model of sentence representation. Furthermore, OntoILPER enables the exploitation of the domain ontology and further background knowledge in the form of relational features. To evaluate OntoILPER, several experiments over the TREC corpus for both NER and RE tasks were conducted and the yielded results demonstrate its effectiveness in both tasks. This paper also provides a comparative assessment among OntoILPER and other NER and RE systems, showing that OntoILPER is very competitive on NER and outperforms the selected systems on RE. © 2017 Springer-Verlag London Ltd.","Named entity recognition; Ontology population; Ontology-based information extraction; Relation extraction; Relational learning; Supervised machine learning",,2-s2.0-85030849258
"Borgmann K., Ghorpade A.","Methamphetamine Augments Concurrent Astrocyte Mitochondrial Stress, Oxidative Burden, and Antioxidant Capacity: Tipping the Balance in HIV-Associated Neurodegeneration",2017,"Neurotoxicity Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030850049&doi=10.1007%2fs12640-017-9812-z&partnerID=40&md5=7c9d758765d0fb0cdb65cf2c34d080cc","Methamphetamine (METH) use, with and without human immunodeficiency virus (HIV)-1 comorbidity, exacerbates neurocognitive decline. Oxidative stress is a probable neurotoxic mechanism during HIV-1 central nervous system infection and METH abuse, as viral proteins, antiretroviral therapy and METH have each been shown to induce mitochondrial dysfunction. However, the mechanisms regulating mitochondrial homeostasis and overall oxidative burden in astrocytes are not well understood in the context of HIV-1 infection and METH abuse. Here, we report METH-mediated dysregulation of astrocyte mitochondrial morphology and function during prolonged exposure to low levels of METH. Mitochondria became larger and more rod shaped with METH when assessed by machine learning, segmentation analyses. These changes may be mediated by elevated mitofusin expression coupled with inhibitory phosphorylation of dynamin-related protein-1, which regulate mitochondrial fusion and fission, respectively. While METH decreased oxygen consumption and ATP levels during acute exposure, chronic treatment of 1 to 2 weeks significantly enhanced both when tested in the absence of METH. Together, these changes significantly increased not only expression of antioxidant proteins, augmenting the astrocyte’s oxidative capacity, but also oxidative damage. We propose that targeting astrocytes to reduce their overall oxidative burden and expand their antioxidant capacity could ultimately tip the balance from neurotoxicity towards neuroprotection. © 2017 Springer Science+Business Media, LLC","Astroglia; Dynamin-related protein; Extracellular flux; Machine learning; Mitochondria; Mitofusin; Neurotoxicity; Oxidative stress",,2-s2.0-85030850049
"Pahari S., Chatterjee D., Negi S., Kaur J., Singh B., Agrewala J.N.","Morbid sequences suggest molecular mimicry between microbial peptides and self-antigens: A possibility of inciting autoimmunity",2017,"Frontiers in Microbiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030680690&doi=10.3389%2ffmicb.2017.01938&partnerID=40&md5=eb58acf4e4746ee03bd35cdaaca3d1f9","Understanding etiology of autoimmune diseases has been a great challenge for designing drugs and vaccines. The pathophysiology of many autoimmune diseases may be attributed to molecular mimicry provoked by microbes. Molecular mimicry hypothesizes that a sequence homology between foreign and self-peptides leads to cross-activation of autoreactive T cells. Different microbial proteins are implicated in various autoimmune diseases, including multiple sclerosis, human type 1 diabetes, primary biliary cirrhosis and rheumatoid arthritis. It may be imperative to identify the microbial epitopes that initiate the activation of autoreactive T cells. Consequently, in the present study, we employed immunoinformatics tools to delineate homologous antigenic regions between microbes and human proteins at not only the sequence level but at the structural level too. Interestingly, many cross-reactive MHC class II binding epitopes were detected from an array of microbes. Further, these peptides possess a potential to skew immune response toward Th1-like patterns. The present study divulges many microbial target proteins, their putative MHC-binding epitopes, and predicted structures to establish the fact that both sequence and structure are two important aspects for understanding the relationship between molecular mimicry and autoimmune diseases. Such findings may enable us in designing potential immunotherapies to tolerize autoreactive T cells. © 2017 Pahari, Chatterjee, Negi, Kaur, Singh and Agrewala.","Autoantigens; Autoimmunity; Cytokines; HLA binders; Immunoinformatics; Microbes; Molecular mimicry; Sequence and structural mimicry","gamma interferon; interleukin 4; major histocompatibility antigen class 2; microorganism protein; algorithm; Article; autoimmunity; binding affinity; cross reaction; cytokine release; environmental factor; gene frequency; genetic variability; IC50; immune response; immunotherapy; insulin dependent diabetes mellitus; machine learning; molecular docking; molecular mimicry; multiple sclerosis; nerve cell network; nonhuman; primary biliary cirrhosis; protein analysis; protein expression; proteomics; rheumatoid arthritis; sequence analysis",2-s2.0-85030680690
"Li X., Zhao Z., Ma J., Cui S., Yi M., Guo H., Wan Y.","Extracting neural oscillation signatures of laser-induced nociception in pain-related regions in rats",2017,"Frontiers in Neural Circuits",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032270427&doi=10.3389%2ffncir.2017.00071&partnerID=40&md5=cdd0147ddd8c20b604fcbbf8d89060cc","Previous studies have shown that multiple brain regions are involved in pain perception and pain-related neural processes by forming a functionally connected pain network. It is still unclear how these pain-related brain areas actively work together to generate the experience of pain. To get a better insight into the pain network, we implanted electrodes in four pain-related areas of rats including the anterior cingulate cortex (ACC), orbitofrontal cortex (OFC), primary somatosensory cortex (S1) and periaqueductal gray (PAG). We analyzed the pattern of local field potential (LFP) oscillations under noxious laser stimulations and innoxious laser stimulations. A high-dimensional feature matrix was built based on the LFP characters for both experimental conditions. Generalized linear models (GLMs) were trained to classify recorded LFPs under noxious vs. innoxious condition. We found a general power decrease in a and b bands and power increase in γ band in the recorded areas under noxious condition. After noxious laser stimulation, there was a consistent change in LFP power and correlation in all four brain areas among all 13 rats. With GLM classifiers, noxious laser trials were distinguished from innoxious laser trials with high accuracy (86%) using high-dimensional LFP features. This work provides a basis for further research to examine which aspects (e.g., sensory, motor or affective processes) of noxious stimulation should drive distinct neural activity across the pain network. © 2017 Hudson.","Acute pain; Electroencephalogram; Machine learning; Neural oscillation; Pain network","adult; affect; animal experiment; animal model; animal tissue; anterior cingulate; Article; controlled study; local field potential; male; motor performance; nervous system function; neural oscillation; nociception; nociceptive stimulation; nonhuman; orbital cortex; pain; periaqueductal gray matter; primary somatosensory cortex; rat; sensory system; statistical model",2-s2.0-85032270427
"Madani K., Kachurka V., Sabourin C., Amarger V., Golovko V., Rossi L.","A human-like visual-attention-based artificial vision system for wildland firefighting assistance",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030870715&doi=10.1007%2fs10489-017-1053-6&partnerID=40&md5=e28bfca6ad1fbafdf9b4a373ad5a24d7","In this work we contribute to development of a “Human-like Visual-Attention-based Artificial Vision” system for boosting firefighters’ awareness about the hostile environment in which they are supposed to move along. Taking advantage from artificial visual-attention, the investigated system’s conduct may be adapted to firefighter’s way of gazing by acquiring some kind of human-like artificial visual neatness supporting firefighters in interventional conditions’ evaluation or in their appraisal of the rescue conditions of people in distress dying out within the disaster. We achieve such a challenging goal by combining a statistically-founded bio-inspired saliency detection model with a Machine-Learning-based human-eye-fixation model. Hybridization of the two above-mentioned models leads to a system able to tune its parameters in order to fit human-like gazing of the inspected environment. It opens appealing perspectives in computer-aided firefighters’ assistance boosting their awareness about the hostile environment in which they are supposed to evolve. Using as well various available wildland fires images’ databases as an implementation of the investigated concept on a 6-wheeled mobile robot equipped with communication facilities, we provide experimental results showing the plausibility as well as the efficiency of the proposed system. © 2017 Springer Science+Business Media, LLC","Artificial visual attention; Fire region detection; Firefighters’ assistance; Human-like artificial vision; Implementation; Robot","Fire extinguishers; Fire fighting equipment; Learning systems; Robots; Vision; Artificial vision system; Communication facilities; Hostile environments; Human like; Implementation; Region detection; Saliency detection; Visual Attention; Behavioral research",2-s2.0-85030870715
"CHURCH K.W.","Emerging trends: A tribute to Charles Wayne",2017,"Natural Language Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030871507&doi=10.1017%2fS1351324917000389&partnerID=40&md5=b25ba6871e1c849f8cd2089f311c277a","Charles Wayne restarted funding in speech and language in the mid-1980s after a funding winter brought on by Pierce’s glamour-and-deceit criticisms in the ALPAC report and ‘Whither Speech Recognition’. Wayne introduced a new glamour-and-deceit-proof idea, an emphasis on evaluation. No other sort of program could have been funded at the time, at least in America. One could argue that Wayne has been so successful that the program no longer needs him to continue on. These days, shared tasks and leaderboards have become common place in speech and language (and vision and machine learning) research. That said, I am concerned that the community may not appreciate what it has got until it’s gone. Wayne has been doing much more than merely running competitions, but he did what he did in such a subtle Columbo-like way. Going forward, government funding is being eclipsed by consumer markets. Those of us with research to sell need to find more and more ways to be relevant to potential sponsors given this new world order. Copyright © Cambridge University Press 2017 This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.",,"Finance; International cooperation; Learning systems; Consumer market; Emerging trends; Government funding; New world order; Speech recognition",2-s2.0-85030871507
"Behler J.","First Principles Neural Network Potentials for Reactive Simulations of Large Molecular and Condensed Systems",2017,"Angewandte Chemie - International Edition",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030626089&doi=10.1002%2fanie.201703114&partnerID=40&md5=50f7ce8f4fa9c98e442184517f9adea4","Modern simulation techniques have reached a level of maturity which allows a wide range of problems in chemistry and materials science to be addressed. Unfortunately, the application of first principles methods with predictive power is still limited to rather small systems, and despite the rapid evolution of computer hardware no fundamental change in this situation can be expected. Consequently, the development of more efficient but equally reliable atomistic potentials to reach an atomic level understanding of complex systems has received considerable attention in recent years. A promising new development has been the introduction of machine learning (ML) methods to describe the atomic interactions. Once trained with electronic structure data, ML potentials can accelerate computer simulations by several orders of magnitude, while preserving quantum mechanical accuracy. This Review considers the methodology of an important class of ML potentials that employs artificial neural networks. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","computational chemistry; density functional calculations; molecular dynamics; neural networks; potential energy surfaces",,2-s2.0-85030626089
"Kutsanedzie F.Y.H., Chen Q., Sun H., Cheng W.","In situ cocoa beans quality grading by near-infrared-chemodyes systems",2017,"Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030315323&doi=10.1039%2fc7ay01751k&partnerID=40&md5=b4e58c6ba7b5d2dd017509043ec86938","Fermentation level is a key bean quality indicator in the cocoa industry. A colorimetric sensor e-nose (CS e-nose) and an innovatively designed near infrared chemo-intermediary-dyes spectra technique (NIR-CDS) combined with four chemometric algorithms-extreme machine learning (ELM), support vector machine (SVM), linear discriminant analysis (LDA) and k-nearest neighbors (k-NN)-were applied to classify 90 sampled cocoa beans into three quality grades-fully fermented, partially fermented and non-fermented. The CS e-nose (89% ≤ Rp ≤ 94%) and NIR-CDS (85% ≤ Rp ≤ 94%) achieved comparable classification rates, with the systems' data cluster analysis yielding cophenetic correlation coefficients of 0.85-0.89. Both systems combined with SVM and ELM achieved a high classification rate (Rp = 94%) and could be applied to cocoa bean quality classification on an in situ and nondestructive basis. This novel NIR-CDS technique proved a pragmatic approach for the selection of sensitive chemo-dyes used in the fabrication of e-nose colorimetric sensor arrays compared with the hitherto trial-and-error method, which is time-consuming and dye-wasteful. The technique could also be deployed in near-infrared systems for the detection of volatile (gaseous) compounds, which previously had been a limitation. © 2017 The Royal Society of Chemistry.",,"Cluster analysis; Cocoa; Colorimetric analysis; Colorimetry; Discriminant analysis; Electronic nose; Grading; Image retrieval; Learning systems; Nearest neighbor search; Quality control; Support vector machines; Classification rates; Colorimetric sensor arrays; Colorimetric sensors; Correlation coefficient; Extreme machine learning; Linear discriminant analysis; Quality classification; Trial-and-error method; Infrared devices",2-s2.0-85030315323
"Luo T., Yang Y., Yi D., Ye J.","Robust discriminative feature learning with calibrated data reconstruction and sparse low-rank model",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030663538&doi=10.1007%2fs10489-017-1060-7&partnerID=40&md5=a845b710bd7ca24e505ec2b98d5c8143","Since large amounts of labeled high-dimensional data needed to be processed, supervised feature learning has become an important and challenging problem in machine learning. Conventional supervised methods often adopt ℓ2-norm loss function, which is sensitive to the outliers. However, real world data always contain lots of outliers that make traditional supervised methods fail to achieve the optimal performance. In addition, these methods can not reconstruct the original complex structured data well, since the dimensions of their learned projection matrices are often limited to the number of classes and are sub-optimal. To address these challenges, we propose a novel robust discriminative feature learning (RDFL) method via calibrated data reconstruction and sparse low-rank model. Specifically, RDFL preserves the discriminant information and simultaneously reconstructs the complex low-rank structure by minimizing joint ℓ2,1-norm reconstruction error and within-class distance. To solve the proposed non-smooth problem, we derive an efficient optimization algorithm to soften the contributions of outliers. Meanwhile, we adopt the general power iteration method (GPIM) to accelerate our algorithm to make it scalable to large scale problem and theoretically analyze the convergence and computational complexity of the proposed algorithm. Extensive experimental results present that our proposed RDFL outperforms other compared methods in most cases and significantly improve the robust performance to noise and outliers. © 2017 Springer Science+Business Media, LLC","Data reconstruction; General power iteration method (GPIM); Low-rank model; Robust discriminative feature learning; Sparse learning","Calibration; Clustering algorithms; Learning systems; Optimization; Statistics; Supervised learning; Data reconstruction; Discriminative features; Iteration method; Rank modeling; Sparse learning; Iterative methods",2-s2.0-85030663538
"Hao W., Fan J., Zhang Z., Zhu G.","End-to-End Lifelong Learning: a Framework to Achieve Plasticities of both the Feature and Classifier Constructions",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030720304&doi=10.1007%2fs12559-017-9514-0&partnerID=40&md5=9b346db63523bcf5e1b5cf6537964563","Plasticity in our brain offers us promising ability to learn and know the world. Although great successes have been achieved in many fields, few bio-inspired machine learning methods have mimicked this ability. Consequently, when meeting large-scale or time-varying data, these bio-inspired methods are infeasible, due to the reasons that they lack plasticity and need all training data loaded into memory. Furthermore, even the popular deep convolutional neural network (CNN) models have relatively fixed structures and cannot process time varying data well. Through incremental methodologies, this paper aims at exploring an end-to-end lifelong learning framework to achieve plasticities of both the feature and classifier constructions. The proposed model mainly comprises of three parts: Gabor filters followed by max pooling layer offering shift and scale tolerance to input samples, incremental unsupervised feature extraction, and incremental SVM trying to achieve plasticities of both the feature learning and classifier construction. Different from CNN, plasticity in our model has no back propogation (BP) process and does not need huge parameters. Our incremental models, including IncPCANet and IncKmeansNet, have achieved better results than PCANet and KmeansNet on minist and Caltech101 datasets respectively. Meanwhile, IncPCANet and IncKmeansNet show promising plasticity of feature extraction and classifier construction when the distribution of data changes. Lots of experiments have validated the performance of our model and verified a physiological hypothesis that plasticity exists in high level layer better than that in low level layer. © 2017 Springer Science+Business Media, LLC","End-to-end; Incremental KMeansNet; Incremental PCANet; Incremental SVM; Lifelong learning; Plasticity","Deep neural networks; Extraction; Feature extraction; Gabor filters; Learning systems; Neural networks; Physiological models; Plasticity; End to end; Incremental KMeansNet; Incremental PCANet; Incremental SVM; Life long learning; Classification (of information)",2-s2.0-85030720304
"Jamil A., Bayram B.","Tree Species Extraction and Land Use&#x002F;Cover Classification From High-Resolution Digital Orthophoto Maps",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031786668&doi=10.1109%2fJSTARS.2017.2756864&partnerID=40&md5=83d5ae8fe357baa45ee813a3c5f91277","Understanding tree species distribution and land use&#x002F;cover classes plays a key role for developing environmental monitoring and decision support systems. This study investigates a method based on integration of multiple classifiers to improve the classification accuracy for extraction of tree species and land use&#x002F;cover classes from large scale data. First, a diverse set of classifiers from different families of statistical learning was selected as base classifiers namely: support vector machine, artificial neural network, and random forest. Both spectral and spatial features were, then, extracted and fed into individual classifiers to classify data into four classes (tea gardens, other trees, impervious surfaces, and bare land). Finally, the results obtained from each classifier were combined to obtain final output by maximum voting. The proposed method was evaluated by using an area-based accuracy assessment on a dataset consisting of ten high-resolution digital orthophoto maps. Experimental results showed that integrating the outputs of individual classifiers improved (4&#x0025;&#x2013;7&#x0025;) overall classification accuracy. IEEE","Artificial neural networks (ANN); Feature extraction; Land surface; land use&#x002F;cover classification; Radio frequency; random forest (RF); Remote sensing; support vector machine (SVM); Support vector machines; Training; tree species classification; Vegetation","Artificial intelligence; Data reduction; Decision support systems; Decision trees; Extraction; Forestry; Land use; Neural networks; Support vector machines; Accuracy assessment; Classification accuracy; Environmental Monitoring; Individual classifiers; Multiple classifiers; Random forests; Statistical learning; Tree species; Classification (of information)",2-s2.0-85031786668
"Alharthi R., Alharthi R., Guthier B., El Saddik A.","CASP: context-aware stress prediction system",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030718541&doi=10.1007%2fs11042-017-5246-0&partnerID=40&md5=cd7a02e467e3cac3ede30c3e2fb138a5","In this paper, we propose a mobile-based context-aware acute stress prediction system (CASP) that predicts a user’s stress status based on their current contextual data. The system consists of a context-aware stress prediction algorithm, and an early stage stress intervention method. In the learning phase, the context-aware stress detection algorithm uses ECG signals to identify the user’s stress status. With the aid of machine learning algorithms and cloud computing services, the stress prediction algorithm produces adaptive and personalized prediction models based on the user’s context gathered from their smartphone. The prediction models are able to adapt the changing nature of both the user’s stress status and the surrounding environment. Our evaluation results show that the CASP system is able to predict the stress status of a user using the current contextual data with an average accuracy of 78.3% as measured from ground truth data collected using biofeedback sensors. © 2017 Springer Science+Business Media, LLC","Context-aware; Naive bayes; Stress prediction","Biofeedback; Forecasting; Learning systems; Cloud computing services; Context-Aware; Evaluation results; Ground truth data; Intervention methods; Naive bayes; Stress prediction; Surrounding environment; Learning algorithms",2-s2.0-85030718541
"Zhao Z., Schiller E., Kalogeiton E., Braun T., Stiller B., Garip M.T., Joy J., Gerla M., Akhtar N., Matta I.","Autonomic Communications in Software-driven Networks",2017,"IEEE Journal on Selected Areas in Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031768524&doi=10.1109%2fJSAC.2017.2760354&partnerID=40&md5=8ce79ee355a00bb283527d985195e29e","Autonomic Communications aims to provide Quality-of-Service (QoS) in networks using self-management mechanisms. It inherits many characteristics from Autonomic Computing, in particular, when communication systems are running as specialized applications in Software-Defined Networking (SDN) and Network Function Virtualization (NFV) enabled cloud environments. This paper surveys Autonomic Computing and Communications in the context of softwaredriven networks, i.e. networks based on SDN/NFV concepts. Autonomic Communications creates new challenges in terms of security, operations, and business support. We discuss several goals, research challenges, and development issues on selfmanagement mechanisms and architectures in software-driven networks. The paper covers multiple perspectives of Autonomic Communications in software-driven networks, such as automatic testing, integration, and deployment of network functions. We also focus on self-management and optimization, which make use of machine learning techniques. IEEE","Autonomic Communications; Autonomic Computing; Autonomic Security; Network Function Virtualization (NFV); Operation and Business Support System (OSS/BSS); Self-Management; Self-Optimization; Software-Defined Networking (SDN); Testing","Application programs; Automatic testing; Distributed computer systems; Integration testing; Learning systems; Quality of service; Software defined networking; Software testing; Testing; Transfer functions; Virtual reality; Virtualization; Autonomic communications; Autonomic Computing; Autonomic Security; Business support systems; Self management; Self-optimization; Software defined networking (SDN); Network function virtualization",2-s2.0-85031768524
"Kulkarni A., Page A., Attaran N., Jafari A., Malik M., Homayoun H., Mohsenin T.","An Energy-Efficient Programmable Manycore Accelerator for Personalized Biomedical Applications",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031814807&doi=10.1109%2fTVLSI.2017.2754272&partnerID=40&md5=424666eed7d8139463cf9ee622c0d403","Wearable personalized health monitoring systems can offer a cost-effective solution for human health care. These systems must constantly monitor patients' physiological signals and provide highly accurate, and quick processing and delivery of the vast amount of data within a limited power and area footprint. These personalized biomedical applications require sampling and processing multiple streams of physiological signals with a varying number of channels and sampling rates. The processing typically consists of feature extraction, data fusion, and classification stages that require a large number of digital signal processing (DSP) and machine learning (ML) kernels. In response to these requirements, in this paper, a tiny, energy-efficient, and domain-specific manycore accelerator referred to as power-efficient nanoclusters (PENC) is proposed to map and execute the kernels of these applications. Simulation results show that the PENC is able to reduce energy consumption by up to 80&#x0025; and 25&#x0025; for DSP and ML kernels, respectively, when optimally parallelized. In addition, we fully implemented three compute-intensive personalized biomedical applications, namely, multichannel seizure detection, multiphysiological stress detection, and standalone tongue drive system (sTDS), to evaluate the proposed manycore performance relative to commodity embedded CPU, graphical processing unit (GPU), and field-programmable gate array (FPGA)-based implementations. For these three case studies, the energy consumption and the performance of the proposed PENC manycore, when acting as an accelerator along with an Intel Atom processor as a host, are compared with the existing commercial off-the-shelf general-purpose, customizable, and programmable embedded platforms, including Intel Atom, Xilinx Artix-7 FPGA, and NVIDIA TK1 advanced RISC machine -A15 and K1 GPU system on a chip. For these applications, the PENC manycore is able to significantly improve throughput and energy efficiency by up to 1872x and 276x, respectively. For the most computational intensive application of seizure detection, the PENC manycore is able to achieve a throughput of 15.22 giga-operations-per-second (GOPs), which is a 14x improvement in throughput over custom FPGA solution. For stress detection, the PENC achieves a throughput of 21.36 GOPs and an energy efficiency of 4.23 GOP/J, which is 14.87x and 2.28x better over FPGA implementation, respectively. For the sTDS application, the PENC improves a throughput by 5.45x and an energy efficiency by 2.37x over FPGA implementation. IEEE","Low-power manycore accelerator; personalized biomedical applications; seizure detection; stress detection; tongue drive system (TDS)","Biomedical signal processing; Cost effectiveness; Data fusion; Digital signal processing; Digital storage; Energy utilization; Field programmable gate arrays (FPGA); Graphics processing unit; Learning systems; Medical applications; Monitoring; Patient monitoring; Physiology; Signal processing; Stresses; System-on-chip; Throughput; Biomedical applications; Drive systems; Many-core accelerators; Seizure detection; Stress detection; Energy efficiency",2-s2.0-85031814807
"Huang G., Yuan M., Chen M., Li L., You W., Li H., Cai J.J., Ji G.","Integrating multiple fitting regression and Bayes decision for cancer diagnosis with transcriptomic data from tumor-educated blood platelets",2017,"Analyst",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029935909&doi=10.1039%2fc7an00944e&partnerID=40&md5=d507527f0b472c3d9bbf5713ac5f833a","The application of machine learning in cancer diagnostics has shown great promise and is of importance in clinic settings. Here we consider applying machine learning methods to transcriptomic data derived from tumor-educated platelets (TEPs) from individuals with different types of cancer. We aim to define a reliability measure for diagnostic purposes to increase the potential for facilitating personalized treatments. To this end, we present a novel classification method called MFRB (for Multiple Fitting Regression and Bayes decision), which integrates the process of multiple fitting regression (MFR) with Bayes decision theory. MFR is first used to map multidimensional features of the transcriptomic data into a one-dimensional feature. The probability density function of each class in the mapped space is then adjusted using the Gaussian probability density function. Finally, the Bayes decision theory is used to build a probabilistic classifier with the estimated probability density functions. The output of MFRB can be used to determine which class a sample belongs to, as well as to assign a reliability measure for a given class. The classical support vector machine (SVM) and probabilistic SVM (PSVM) are used to evaluate the performance of the proposed method with simulated and real TEP datasets. Our results indicate that the proposed MFRB method achieves the best performance compared to SVM and PSVM, mainly due to its strong generalization ability for limited, imbalanced, and noisy data. © 2017 The Royal Society of Chemistry.",,,2-s2.0-85029935909
"Siciliano R., D’Ambrosio A., Aria M., Amodio S.","Analysis of Web Visit Histories, Part II: Predicting Navigation by Nested STUMP Regression Trees",2017,"Journal of Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030720376&doi=10.1007%2fs00357-017-9239-5&partnerID=40&md5=9cbd5243014e8dc6e9ae99586356dac3","This paper constitutes part II of the contribution to the analysis of web visit histories through a new methodological framework for web usage-structure mining considering association rules theory. The aim is to explore through a tree structure the sequence of direct rules (i.e. paths) that characterize a web navigator who keeps standing longer on a web page with respect to the path characterizing navigators who leave the web earlier. A novel tree-based structure is introduced to take into account that the learning sample changes click by click leaving out navigators who drop off from the web after any click. The response variable at each time point is the remaining number of clicks before leaving the web. The split is induced by the predictors that describe the preferred web sections. The methodology introduced results in a Nested Stump Regression Tree that is an hierarchy of stump trees, where a stump is a tree with only one split or, equivalently, with only two terminal nodes. Suitable properties are outlined. As in first part of the contribution to the analysis of the web visit histories, a methodological description is provided by considering a web portal with a fixed set of web sections, i.e. a data set coming from the UCI Machine Learning Repository. © 2017 Classification Society of North America","Recursive partitioning; Sequence rules; Web path; Web Usage-Structure Mining",,2-s2.0-85030720376
"Cockrell C., An G.","Sepsis reconsidered: Identifying novel metrics for behavioral landscape characterization with a high-performance computing implementation of an agent-based model",2017,"Journal of Theoretical Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027722597&doi=10.1016%2fj.jtbi.2017.07.016&partnerID=40&md5=49267b0e662cee755e1695a5c8357d7e","Objectives Sepsis affects nearly 1 million people in the United States per year, has a mortality rate of 28–50% and requires more than $20 billion a year in hospital costs. Over a quarter century of research has not yielded a single reliable diagnostic test or a directed therapeutic agent for sepsis. Central to this insufficiency is the fact that sepsis remains a clinical/physiological diagnosis representing a multitude of molecularly heterogeneous pathological trajectories. Advances in computational capabilities offered by High Performance Computing (HPC) platforms call for an evolution in the investigation of sepsis to attempt to define the boundaries of traditional research (bench, clinical and computational) through the use of computational proxy models. We present a novel investigatory and analytical approach, derived from how HPC resources and simulation are used in the physical sciences, to identify the epistemic boundary conditions of the study of clinical sepsis via the use of a proxy agent-based model of systemic inflammation. Design Current predictive models for sepsis use correlative methods that are limited by patient heterogeneity and data sparseness. We address this issue by using an HPC version of a system-level validated agent-based model of sepsis, the Innate Immune Response ABM (IIRBM), as a proxy system in order to identify boundary conditions for the possible behavioral space for sepsis. We then apply advanced analysis derived from the study of Random Dynamical Systems (RDS) to identify novel means for characterizing system behavior and providing insight into the tractability of traditional investigatory methods. Results The behavior space of the IIRABM was examined by simulating over 70 million sepsis patients for up to 90 days in a sweep across the following parameters: cardio-respiratory-metabolic resilience; microbial invasiveness; microbial toxigenesis; and degree of nosocomial exposure. In addition to using established methods for describing parameter space, we developed two novel methods for characterizing the behavior of a RDS: Probabilistic Basins of Attraction (PBoA) and Stochastic Trajectory Analysis (STA). Computationally generated behavioral landscapes demonstrated attractor structures around stochastic regions of behavior that could be described in a complementary fashion through use of PBoA and STA. The stochasticity of the boundaries of the attractors highlights the challenge for correlative attempts to characterize and classify clinical sepsis. Conclusions HPC simulations of models like the IIRABM can be used to generate approximations of the behavior space of sepsis to both establish “boundaries of futility” with respect to existing investigatory approaches and apply system engineering principles to investigate the general dynamic properties of sepsis to provide a pathway for developing control strategies. The issues that bedevil the study and treatment of sepsis, namely clinical data sparseness and inadequate experimental sampling of system behavior space, are fundamental to nearly all biomedical research, manifesting in the “Crisis of Reproducibility” at all levels. HPC-augmented simulation-based research offers an investigatory strategy more consistent with that seen in the physical sciences (which combine experiment, theory and simulation), and an opportunity to utilize the leading advances in HPC, namely deep machine learning and evolutionary computing, to form the basis of an iterative scientific process to meet the full promise of Precision Medicine (right drug, right patient, right time). © 2017","Attractors; Cytokines; Parameter space; Personalized medicine; Precision medicine; Random dynamical systems; Stochastic dynamical systems",,2-s2.0-85027722597
"Liang J., Liu X., Liao K.","Soil Moisture Retrieval using UWB Echoes via Fuzzy Logic and Machine Learning",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031780207&doi=10.1109%2fJIOT.2017.2760338&partnerID=40&md5=f99362b30797e9d6c372b46376f0c44d","Soil moisture (SM) retrieval using wireless signals has become a research focus with the development of sensor devices in internet of things (IOT). Studies applying ground-penetrating radar (GPR) have improved the accuracy of SM retrieval, however the field-scaled data are hardly satisfactory mainly due to the frequency response of the antenna, and it&#x2019;s not cost-effective for farmers to monitor the soil conditions. In this paper, we compare two fuzzy logic systems (FLS) -type-1 fuzzy logic system (T1FLS) and adaptive network-based fuzzy inference system (ANFIS) to extract fuzzy parameters of soil. Moreover, two machine learning (ML) algorithms - random forest (RF) and artificial neural network (ANN) with principal component analysis (PCA) are applied in the SM classifications. 9 types of UWB soil echoes of different texture and volume water content (VWC) are collected and investigated using our approaches. Final analysis shows that ANFIS with RF provides the best VWC correct recognition rate (CRR) compared to other algorithms. IEEE","artificial neural network; Feature extraction; Fuzzy logic; fuzzy logic system; Ground penetrating radar; Internet of Things; principal component analysis.; Radio frequency; random forest; Soil; Soil measurements; soil moisture retrieval; UWB","Artificial intelligence; Computer circuits; Cost effectiveness; Decision trees; Feature extraction; Frequency response; Fuzzy inference; Fuzzy neural networks; Geological surveys; Ground penetrating radar systems; Internet of things; Learning systems; Moisture; Neural networks; Principal component analysis; Radar; Radar measurement; Soil moisture; Soil surveys; Soils; Fuzzy logic system; Ground Penetrating Radar; Radio frequencies; Random forests; Soil measurement; Soil moisture retrievals; Fuzzy logic",2-s2.0-85031780207
"Zhong Z., Li J., Luo Z., Chapman M.","Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031794275&doi=10.1109%2fTGRS.2017.2755542&partnerID=40&md5=39704b8384d0ce88fa8fa91c4b8a2671","In this paper, we designed an end-to-end spectral-spatial residual network (SSRN) that takes raw 3-D cubes as input data without feature engineering for hyperspectral image classification. In this network, the spectral and spatial residual blocks consecutively learn discriminative features from abundant spectral signatures and spatial contexts in hyperspectral imagery (HSI). The proposed SSRN is a supervised deep learning framework that alleviates the declining-accuracy phenomenon of other deep learning models. Specifically, the residual blocks connect every other 3-D convolutional layer through identity mapping, which facilitates the backpropagation of gradients. Furthermore, we impose batch normalization on every convolutional layer to regularize the learning process and improve the classification performance of trained models. Quantitative and qualitative results demonstrate that the SSRN achieved the state-of-the-art HSI classification accuracy in agricultural, rural-urban, and urban data sets: Indian Pines, Kennedy Space Center, and University of Pavia. IEEE","3-D deep learning; Feature extraction; hyperspectral image classification; Hyperspectral imaging; Machine learning; Robustness; spectral-spatial feature extraction; spectral-spatial residual network (SSRN); Testing; Training","Agricultural machinery; Convolution; Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Learning systems; Personnel training; Robustness (control systems); Space platforms; Spectroscopy; Testing; Classification accuracy; Classification performance; Discriminative features; Feature engineerings; Hyperspectral imagery; Kennedy space centers; Learning frameworks; spectral-spatial residual network (SSRN); Classification (of information)",2-s2.0-85031794275
"Elangovan K., Tamilselvam Y.K., Mohan R.E., Iwase M., Nemoto T., Wood K.","Fault diagnosis of a reconfigurable crawling-rolling robot based on support vector machines",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030974249&doi=10.3390%2fapp7101025&partnerID=40&md5=79280c2e8dd26156288ac334f2d9002f","As robots begin to perform jobs autonomously, with minimal or no human intervention, a new challenge arises: robots also need to autonomously detect errors and recover from faults. In this paper, we present a Support Vector Machine (SVM)-based fault diagnosis system for a bioinspired reconfigurable robot named Scorpio. The diagnosis system needs to detect and classify faults while Scorpio uses its crawling and rolling locomotion modes. Specifically, we classify between faulty and non-faulty conditions by analyzing onboard Inertial Measurement Unit (IMU) sensor data. The data capture nine different locomotion gaits, which include rolling and crawling modes, at three different speeds. Statistical methods are applied to extract features and to reduce the dimensionality of original IMU sensor data features. These statistical features were given as inputs for training and testing. Additionally, the c-Support Vector Classification (c-SVC) and nu-SVC models of SVM, and their fault classification accuracies, were compared. The results show that the proposed SVM approach can be used to autonomously diagnose locomotion gait faults while the reconfigurable robot is in operation. © 2017 by the authors.","Fault diagnosis; Machine learning; Reconfigurable robotics; Statistical features; Support vector machines",,2-s2.0-85030974249
"Bzdok D.","Classical statistics and statistical learning in imaging neuroscience",2017,"Frontiers in Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030691538&doi=10.3389%2ffnins.2017.00543&partnerID=40&md5=fdcf39df3337ddd55c4aae15839edcdf","Brain-imaging research has predominantly generated insight by means of classical statistics, including regression-type analyses and null-hypothesis testing using t-test and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity especially for applications in rich and complex data, including cross-validated out-of-sample prediction using pattern classification and sparsity-inducing regression. This concept paper discusses the implications of inferential justifications and algorithmic methodologies in common data analysis scenarios in neuroimaging. It is retraced how classical statistics and statistical learning originated from different historical contexts, build on different theoretical foundations, make different assumptions, and evaluate different outcome metrics to permit differently nuanced conclusions. The present considerations should help reduce current confusion between model-driven classical hypothesis testing and data-driven learning algorithms for investigating the brain with imaging techniques. © 2017 Bzdok.","Data science; Epistemology; Machine learning; Neuroimaging; P-value; Rosetta stone; Statistical inference","analysis of variance; brain; classification; data analysis; epistemology; learning algorithm; neuroimaging; neuroscience; null hypothesis; outcome assessment; prediction; regression analysis; statistical significance; statistics; Student t test; theoretical study",2-s2.0-85030691538
"Bolivar-Cime A., Cordova-Rodriguez L.M.","Binary discrimination methods for high-dimensional data with a geometric representation",2017,"Communications in Statistics - Theory and Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031101246&doi=10.1080%2f03610926.2017.1342838&partnerID=40&md5=1a114b4244b47fd8a5607cb1228405f2","Four binary discrimination methods are studied in the context of high-dimension, low sample size data with an asymptotic geometric representation, when the dimension increases while the sample sizes of the classes are fixed. We show that the methods support vector machine, mean difference, distance-weighted discrimination, and maximal data piling have the same asymptotic behavior as the dimension increases. We study the consistent, inconsistent, and strongly inconsistent cases in terms of angles between the normal vectors of the separating hyperplanes of the methods and the optimal direction for classification. A simulation study is done to assess the theoretical results. © 2017 Taylor & Francis Group, LLC","Asymptotic analysis; binary discrimination; geometric representation; high dimensional data; machine learning.","Asymptotic analysis; Bins; Geometry; Learning systems; Sampling; Asymptotic behaviors; Binary discrimination; Geometric representation; High dimensional data; High dimensions; Optimal direction; Separating hyperplane; Simulation studies; Clustering algorithms",2-s2.0-85031101246
"Sarica A., Cerasa A., Quattrone A.","Random forest algorithm for the classification of neuroimaging data in Alzheimer's disease: A systematic review",2017,"Frontiers in Aging Neuroscience",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031098588&doi=10.3389%2ffnagi.2017.00329&partnerID=40&md5=c0273359fcbfb2840e4bdfc7875d9c2c","Objective: Machine learning classification has been the most important computational development in the last years to satisfy the primary need of clinicians for automatic early diagnosis and prognosis. Nowadays, Random Forest (RF) algorithm has been successfully applied for reducing high dimensional and multi-source data in many scientific realms. Our aim was to explore the state of the art of the application of RF on single and multi-modal neuroimaging data for the prediction of Alzheimer's disease. Methods: A systematic review following PRISMA guidelines was conducted on this field of study. In particular, we constructed an advanced query using boolean operators as follows: (""random forest"" OR ""random forests"") AND neuroimaging AND (""alzheimer's disease"" OR alzheimer's OR alzheimer) AND (prediction OR classification). The query was then searched in four well-known scientific databases: Pubmed, Scopus, Google Scholar and Web of Science. Results: Twelve articles-published between the 2007 and 2017-have been included in this systematic review after a quantitative and qualitative selection. The lesson learnt from these works suggest that when RF was applied on multi-modal data for prediction of Alzheimer's disease (AD) conversion from the Mild Cognitive Impairment (MCI), it produces one of the best accuracies to date. Moreover, the RF has important advantages in terms of robustness to overfitting, ability to handle highly non-linear data, stability in the presence of outliers and opportunity for efficient parallel processing mainly when applied on multi-modality neuroimaging data, such as, MRI morphometric, diffusion tensor imaging, and PET images. Conclusions: We discussed the strengths of RF, considering also possible limitations and by encouraging further studies on the comparisons of this algorithm with other commonly used classification approaches, particularly in the early prediction of the progression from MCI to AD. © 2017 Sarica, Cerasa and Quattrone.","Alzheimer's disease; Classification; Mild cognitive impairment; Neuroimaging; Random forest","Alzheimer disease; classification algorithm; diffusion tensor imaging; disease course; human; machine learning; mild cognitive impairment; multimodal imaging; neuroimaging; positron emission tomography; practice guideline; prediction; random forest algorithm; Review; systematic review",2-s2.0-85031098588
"Lass M., Kuhne T.D., Plessl C.","Using Approximate Computing for the Calculation of Inverse Matrix p-th Roots",2017,"IEEE Embedded Systems Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031827760&doi=10.1109%2fLES.2017.2760923&partnerID=40&md5=e95aa2127eaaa3a42ddc0a7635686742","Approximate computing has shown to provide new ways to improve performance and power consumption of error-resilient applications. While many of these applications can be found in image processing, data classification or machine learning, we demonstrate its suitability to a problem from scientific computing. Utilizing the self-correcting behavior of iterative algorithms, we show that approximate computing can be applied to the calculation of inverse matrix p-th roots which are required in many applications in scientific computing. Results show great opportunities to reduce the computational effort and bandwidth required for the execution of the discussed algorithm, especially when targeting special accelerator hardware. IEEE","Approximate computing; Approximate computing; Approximation algorithms; Bandwidth; Convergence; Hardware; Iterative methods; Iterative methods; Linear algebra; Scientific computing.; Symmetric matrices","Approximation algorithms; Bandwidth; Computer hardware; Data handling; Green computing; Hardware; Image processing; Inverse problems; Iterative methods; Learning systems; Linear algebra; Natural sciences computing; Approximate computing; Computational effort; Convergence; Data classification; Error-resilient; Improve performance; Iterative algorithm; Symmetric matrices; Matrix algebra",2-s2.0-85031827760
"Lausser L., Szekely R., Schirra L.-R., Kestler H.A.","The Influence of Multi-class Feature Selection on the Prediction of Diagnostic Phenotypes",2017,"Neural Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030710019&doi=10.1007%2fs11063-017-9706-3&partnerID=40&md5=08b9d22257fa4e45c9ebbe1bf5bf99f4","In this work, we evaluate two schemes for incorporating feature selection processes in multi-class classifier systems on high-dimensional data of low cardinality. These schemes operate on the level of the systems’ individual base classifiers and therefore do not perfectly fit in the traditional categories of filter, wrapper and embedded feature selection strategies. They can be seen as two examples of feature selection networks that are only loosely related to the structure of the multi-class classifier system. The architectures are tested for their application in predicting diagnostic phenotypes from gene expression profiles. Their selection stability and the overall generalization ability are evaluated in (Formula presented.) cross-validation experiments with support vector machines, random forests and nearest neighbor classifiers on eight publicly available multi-class microarray datasets. Overall the feature selecting multi-class classifier systems were able to outperform their counterparts on at least five of eight datasets. © 2017 Springer Science+Business Media, LLC","Classifier fusion; Feature selection; High-dimensional data; Low cardinality; Multi-class classification","Clustering algorithms; Decision trees; Embedded systems; Feature extraction; Gene expression; Learning systems; Cardinalities; Classifier fusion; Embedded feature selections; Gene expression profiles; Generalization ability; High dimensional data; Multi-class classification; Nearest Neighbor classifier; Classification (of information)",2-s2.0-85030710019
"Kayastha S., Kunimoto R., Horvath D., Varnek A., Bajorath J.","From bird’s eye views to molecular communities: two-layered visualization of structure–activity relationships in large compound data sets",2017,"Journal of Computer-Aided Molecular Design",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030662739&doi=10.1007%2fs10822-017-0070-1&partnerID=40&md5=b89904427fb980cf5cb712df06ef3782","The analysis of structure–activity relationships (SARs) becomes rather challenging when large and heterogeneous compound data sets are studied. In such cases, many different compounds and their activities need to be compared, which quickly goes beyond the capacity of subjective assessments. For a comprehensive large-scale exploration of SARs, computational analysis and visualization methods are required. Herein, we introduce a two-layered SAR visualization scheme specifically designed for increasingly large compound data sets. The approach combines a new compound pair-based variant of generative topographic mapping (GTM), a machine learning approach for nonlinear mapping, with chemical space networks (CSNs). The GTM component provides a global view of the activity landscapes of large compound data sets, in which informative local SAR environments are identified, augmented by a numerical SAR scoring scheme. Prioritized local SAR regions are then projected into CSNs that resolve these regions at the level of individual compounds and their relationships. Analysis of CSNs makes it possible to distinguish between regions having different SAR characteristics and select compound subsets that are rich in SAR information. © 2017 Springer International Publishing AG","Chemical space networks; Generative topographic mapping; Matched molecular pair; Structure–activity relationships",,2-s2.0-85030662739
"Pahlevan A., Qu X., Zapater M., Atienza D.","Integrating Heuristic and Machine-Learning Methods for Efficient Virtual Machine Allocation in Data Centers",2017,"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774157&doi=10.1109%2fTCAD.2017.2760517&partnerID=40&md5=ed06c75529660919029ceb200d260493","Modern cloud data centers (DCs) need to tackle efficiently the increasing demand for computing resources and address the energy efficiency challenge. Therefore, it is essential to develop resource provisioning policies that are aware of virtual machine (VM) characteristics, such as CPU utilization and data communication, and applicable in dynamic scenarios. Traditional approaches fall short in terms of flexibility and applicability for large-scale DC scenarios. In this paper we propose a heuristic-and a machine learning (ML)-based VM allocation method and compare them in terms of energy, quality of service (QoS), network traffic, migrations, and scalability for various DC scenarios. Then, we present a novel hyper-heuristic algorithm that exploits the benefits of both methods by dynamically finding the best algorithm, according to a user-defined metric. For optimality assessment, we formulate an integer linear programming (ILP)-based VM allocation method to minimize energy consumption and data communication, which obtains optimal results, but is impractical at runtime. Our results demonstrate that the ML approach provides up to 24% server-to-server network traffic improvement and reduces execution time by up to 480x compared to conventional approaches, for large-scale scenarios. On the contrary, the heuristic outperforms the ML method in terms of energy and network traffic for reduced scenarios. We also show that the heuristic and ML approaches have up to 6% energy consumption overhead compared to ILP-based optimal solution. Our hyper-heuristic integrates the strengths of both the heuristic and the ML methods by selecting the best one during runtime. IEEE","cloud data centers; Correlation; Data communication; Energy consumption; energy-network traffic trade-offs; greedy heuristic; hyper heuristic; integer linear programming; machine learning; QoS; Quality of service; Resource management; scalability assessment.; Servers","Artificial intelligence; Convolutional codes; Correlation methods; Distributed computer systems; E-learning; Economic and social effects; Energy efficiency; Energy utilization; Green computing; Heuristic algorithms; Heuristic programming; Inductive logic programming (ILP); Information management; Integer programming; Learning systems; Network security; Optimization; Quality of service; Scalability; Servers; Virtual machine; Cloud data centers; Data-communication; Greedy heuristics; Hyperheuristic; Integer Linear Programming; Resource management; Trade off; Heuristic methods",2-s2.0-85031774157
"Quinodoz M., Royer-Bertrand B., Cisarova K., Di Gioia S.A., Superti-Furga A., Rivolta C.","DOMINO: Using Machine Learning to Predict Genes Associated with Dominant Disorders",2017,"American Journal of Human Genetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031694356&doi=10.1016%2fj.ajhg.2017.09.001&partnerID=40&md5=e9ab34ef1d41e913fcc5ea99eb5fd539","In contrast to recessive conditions with biallelic inheritance, identification of dominant (monoallelic) mutations for Mendelian disorders is more difficult, because of the abundance of benign heterozygous variants that act as massive background noise (typically, in a 400:1 excess ratio). To reduce this overflow of false positives in next-generation sequencing (NGS) screens, we developed DOMINO, a tool assessing the likelihood for a gene to harbor dominant changes. Unlike commonly-used predictors of pathogenicity, DOMINO takes into consideration features that are the properties of genes, rather than of variants. It uses a machine-learning approach to extract discriminant information from a broad array of features (N = 432), including: genomic data, intra-, and interspecies conservation, gene expression, protein-protein interactions, protein structure, etc. DOMINO's iterative architecture includes a training process on 985 genes with well-established inheritance patterns for Mendelian conditions, and repeated cross-validation that optimizes its discriminant power. When validated on 99 newly-discovered genes with pathogenic mutations, the algorithm displays an excellent final performance, with an area under the curve (AUC) of 0.92. Furthermore, unsupervised analysis by DOMINO of real sets of NGS data from individuals with intellectual disability or epilepsy correctly recognizes known genes and predicts 9 new candidates, with very high confidence. In summary, DOMINO is a robust and reliable tool that can infer dominance of candidate genes with high sensitivity and specificity, making it a useful complement to any NGS pipeline dealing with the analysis of the morbid human genome. © 2017 American Society of Human Genetics",,"dominant gene; genetic database; genetic disorder; genetics; genomics; high throughput sequencing; human; human genome; machine learning; mutation; procedures; software; Databases, Genetic; Genes, Dominant; Genetic Diseases, Inborn; Genome, Human; Genomics; High-Throughput Nucleotide Sequencing; Humans; Machine Learning; Mutation; Software",2-s2.0-85031694356
"Hu A., Wu S., Wang X., Wang Y., Norman R., He C., Cai H., Zhang K.","Improvement of Reflection Detection Success Rate of GNSS RO Measurements Using Artificial Neural Network",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031813003&doi=10.1109%2fTGRS.2017.2754512&partnerID=40&md5=17645cc838c1b6d3a2582cfd3498f3ec","Global Navigation Satellite System (GNSS) radio occultation (RO) has been widely used in the prediction of weather, climate, and space weather, particularly in the area of tropospheric analyses. However, one of the issues with GNSS RO measurements is that they are interfered with by the signals reflected from the earth's surface. Many RO events are subject to such interfered GNSS measurements, which are considerably difficult to extract from the GNSS RO measurements. To precisely identify interfered RO events, an improved machine learning approach--a gradient descent artificial neural network (ANN)-aided radio-holography method--is proposed in this paper. Since this method is more complex than most other machine learning methods, for improving its efficiency through the reduction in computational time for near-real-time applications, a scale factor and a regularization factor are also adjusted in the ANN approach. This approach was validated using Constellation Observing System for Meteorology, Ionosphere, and Climate/FC-3 atmPhs (level 1b) data during the period of day of year 172-202, 2015, and its detection results were compared with the flag data set provided by Radio Occultation Meteorology Satellite Application Facilities for the performance assessment and validation of the new approach. The results were also compared with those of the support vector machine method for improvement assessment. The comparison results showed that the proposed method can considerably improve both the success rate of GNSS RO reflection detection and the computational efficiency. IEEE","Cost function; Earth; Global navigation satellite system; Global navigation satellite system; holography; Meteorology; reflectivity; remote sensing.; Satellite broadcasting; Satellites; Training data","Artificial intelligence; Communication satellites; Computational efficiency; Cost functions; Earth (planet); Efficiency; Holography; Ionosphere; Learning systems; Meteorology; Navigation; Neural networks; Radio; Radio navigation; Reflection; Remote sensing; Satellites; Constellation observing system for meteorology , ionosphere , and climates; Global Navigation Satellite Systems; Machine learning approaches; Machine learning methods; Meteorology satellites; Satellite broadcasting; Support vector machine method; Training data; Global positioning system",2-s2.0-85031813003
"Lee H., Mansouri M., Tajmir S., Lev M.H., Do S.","A Deep-Learning System for Fully-Automated Peripherally Inserted Central Catheter (PICC) Tip Detection",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030710900&doi=10.1007%2fs10278-017-0025-z&partnerID=40&md5=7a2dc1dc54402cb466da72c101479330","A peripherally inserted central catheter (PICC) is a thin catheter that is inserted via arm veins and threaded near the heart, providing intravenous access. The final catheter tip position is always confirmed on a chest radiograph (CXR) immediately after insertion since malpositioned PICCs can cause potentially life-threatening complications. Although radiologists interpret PICC tip location with high accuracy, delays in interpretation can be significant. In this study, we proposed a fully-automated, deep-learning system with a cascading segmentation AI system containing two fully convolutional neural networks for detecting a PICC line and its tip location. A preprocessing module performed image quality and dimension normalization, and a post-processing module found the PICC tip accurately by pruning false positives. Our best model, trained on 400 training cases and selectively tuned on 50 validation cases, obtained absolute distances from ground truth with a mean of 3.10 mm, a standard deviation of 2.03 mm, and a root mean squares error (RMSE) of 3.71 mm on 150 held-out test cases. This system could help speed confirmation of PICC position and further be generalized to include other types of vascular access and therapeutic support devices. © 2017 Society for Imaging Informatics in Medicine","Chest radiograph; Computer-aided detection; Deep learning; Machine learning; PICC; Radiology workflow","Catheters; Deep learning; Learning systems; Neural networks; Radiography; Chest radiographs; Computer aided detection; Convolutional neural network; Peripherally inserted central catheters; PICC; Preprocessing modules; Radiology workflow; Standard deviation; Computer aided instruction",2-s2.0-85030710900
"Nunes I., Jannach D.","A systematic review and taxonomy of explanations in decision support and recommender systems",2017,"User Modeling and User-Adapted Interaction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030671275&doi=10.1007%2fs11257-017-9195-0&partnerID=40&md5=405b5ed6d347a5388fe1e8bdec93fb4f","With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today’s increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advice-giving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated. © 2017 Springer Science+Business Media B.V.","Artificial intelligence; Decision support system; Expert system; Explanation; Knowledge-based system; Machine learning; Recommender system; Systematic review; Trust","Artificial intelligence; Decision making; Expert systems; Knowledge based systems; Learning algorithms; Learning systems; Recommender systems; Taxonomies; Decision supports; Explanation; Fully automated; Software systems; Sophisticated machines; Systematic Review; Trust; Trust systems; Decision support systems",2-s2.0-85030671275
"Zhong Y., Dutkiewicz E., Yang Y., Zhu X., Zhou Z., Jiang T.","Internet of Mission-Critical Things: Human and Animal Classification&#x2014;A Device-Free Sensing Approach",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031781784&doi=10.1109%2fJIOT.2017.2760322&partnerID=40&md5=e95033f1a2f2fcc54cda1c2ebda4e060","The well-known Internet-of-things is recently being considered for critical missions, such as search and rescue, surveillance and border patrol. One of the most critical issues that these applications are currently facing is how to correctly distinguish between human and animal targets in a cost-effective way. In this work, we present a relatively low-cost but robust approach that uses a combination of device-free sensing and machine-learning technologies to tackle this issue. In order to validate the feasibility of the presented approach, a variety of data is collected in a cornfield using impulse-radio ultra-wideband transceivers. These data are then used to investigate the influence of different statistical properties of the RF signal on the accuracy of human/animal target classification. Based on the probability density function of different statistical properties, two distinguishing features for target classification are found, namely standard deviation and root mean spread delay spread. Using them, the impact on the classification accuracy due to different classifiers, number of training samples and different values of signal-to-noise ratio is extensively verified. Even with the worst case, the classification accuracy of the system is still better than 91targets (including goats and dogs), which indicates that the presented approach has a great potential to be deployed in the near future. IEEE","Classifier; device-free sensing; Dogs; feature extraction; impulse-radio ultra-wideband (IR-UWB); Internet of Things; pattern recognition.; Receivers; RF signals; Sensors; Transceivers","Animals; Channel capacity; Classifiers; Cost effectiveness; Feature extraction; Impulse noise; Internet of things; Learning systems; Network security; Pattern recognition; Probability density function; Radio; Radio communication; Radio transceivers; Receivers (containers); Sensors; Signal to noise ratio; Transceivers; Classification accuracy; Device-free; Dogs; Impulse radio ultra wideband (IR-UWB); Impulse radio ultra-wideband; Machine learning technology; RF signal; Statistical properties; Ultra-wideband (UWB)",2-s2.0-85031781784
"Shimomura Y., Nemoto Y., Ishii T., Nakamura T.","A method for identifying customer orientations and requirements for product–service systems design",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030694437&doi=10.1080%2f00207543.2017.1384581&partnerID=40&md5=9afc171caf232be1b322067b36c12f28","For manufacturers, developing product–service systems (PSSs) is getting more important because of the trends of servitisation and creating social value. A PSS is a social system where multiple actors mutually provide products and services. A PSS design, therefore, must take into account various actors as customers. However, existing methods provide an insufficient solution as to how various customers should be handled in an analysis to identify and accommodate various customer preferences and requirements. To tackle this issue, this article proposes a new method of identifying customers’ orientations and requirements for PSS design. The proposed method employs a combination of topic analysis, persona and scenario approaches. The effectiveness of the method is demonstrated with its application to an urban development case. Through the demonstration, its practical benefits are concluded as follows: consistent and logical results of requirement analysis and insights into a new market for manufacturers. © 2017 Informa UK Limited, trading as Taylor & Francis Group","big data; clustering; customer preference; data mining; design for service; machine learning; product–service systems","Big data; Data mining; Learning systems; Manufacture; Sales; Urban growth; clustering; Customer orientation; Customer preferences; Design for services; Developing product; Products and services; Requirement analysis; Service systems; Product design",2-s2.0-85030694437
"Koyamada K., Onoue Y., Kioka M., Uetsuji T., Baba K.","Visualization of JOV abstracts",2017,"Journal of Visualization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030706585&doi=10.1007%2fs12650-017-0451-5&partnerID=40&md5=df7274ff0a24b196f7ab3c3d879d91c1","Abstract: Since the abstract can be found at the beginning of most scientific articles and is an essential part of the article, several attempts have been made to explore the rhetorical moves of abstracts in various research fields. These studies dealt only with accepted articles since they can be easily accessed. Although the findings of such works have some pedagogical implications for academic writing courses for young researchers who are relatively new to their fields, they do not contribute enough to the transparency of the peer review processes conducted in research fields. Increasing transparency requires considering rejected articles since they help to clarify the decision criteria in the peer review. Based on 591 abstracts of accepted or rejected articles submitted to Journal of Visualization (JOV), the present study aimed at exploring the differences between the accepted and rejected abstracts. The results show that there are significant differences in the structures of the abstracts. Since we also successfully develop a classification model for the decision using a machine-learning technique, the findings of this study have some implications for developing a semi-automatic reviewing system that can reduce the reviewer’s burden and increase the review quality. Graphical abstract: [Figure not available: see fulltext.] © 2017 The Author(s)","Machine learning; Move analysis; Peer review; Review crisis; Text visualization",,2-s2.0-85030706585
"Zafar S.F., Postma E.N., Biswal S., Fleuren L., Boyle E.J., Bechek S., O’Connor K., Shenoy A., Jonnalagadda D., Kim J., Shafi M.S., Patel A.B., Rosenthal E.S., Westover M.B.","Electronic Health Data Predict Outcomes After Aneurysmal Subarachnoid Hemorrhage",2017,"Neurocritical Care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030724089&doi=10.1007%2fs12028-017-0466-8&partnerID=40&md5=9f24db3c3d61600c09650e0e91ffbe1e","Backgroud: Using electronic health data, we sought to identify clinical and physiological parameters that in combination predict neurologic outcomes after aneurysmal subarachnoid hemorrhage (aSAH). Methods: We conducted a single-center retrospective cohort study of patients admitted with aSAH between 2011 and 2016. A set of 473 predictor variables was evaluated. Our outcome measure was discharge Glasgow Outcome Scale (GOS). For laboratory and physiological data, we computed the minimum, maximum, median, and variance for the first three admission days. We created a penalized logistic regression model to determine predictors of outcome and a multivariate multilevel prediction model to predict poor (GOS 1–2), intermediate (GOS 3), or good (GOS 4–5) outcomes. Results: One hundred and fifty-three patients met inclusion criteria; most were discharged with a GOS of 3. Multivariate analysis predictors of mortality (AUC 0.9198) included APACHE II score, Glasgow Come Scale (GCS), white blood cell (WBC) count, mean arterial pressure, variance of serum glucose, intracranial pressure (ICP), and serum sodium. Predictors of death/dependence versus independence (GOS 4–5)(AUC 0.9456) were levetiracetam, mechanical ventilation, WBC count, heart rate, ICP variance, GCS, APACHE II, and epileptiform discharges. The multiclass prediction model selected GCS, admission APACHE II, periodic discharges, lacosamide, and rebleeding as significant predictors; model performance exceeded 80% accuracy in predicting poor or good outcome and exceeded 70% accuracy for predicting intermediate outcome. Conclusions: Variance in early physiologic data can impact patient outcomes and may serve as targets for early goal-directed therapy. Electronically retrievable features such as ICP, glucose levels, and electroencephalography patterns should be considered in disease severity and risk stratification scores. © 2017 Springer Science+Business Media, LLC","EEG; Machine learning; Neurologic outcomes; Predictive analytics; Subarachnoid hemorrhage",,2-s2.0-85030724089
"López-de-Ipiña K., Calvo P., Faundez-Zanuy M., Clavé P., Nascimento W., Martinez de Lizarduy U., Alvarez D., Arreola V., Ortega O., Mekyska J., Sanz-Cartagena P.","Automatic voice analysis for dysphagia detection",2017,"Speech, Language and Hearing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031404349&doi=10.1080%2f2050571X.2017.1369017&partnerID=40&md5=5f3ef95719d0ef165330bc25e8e9ee6d","Purpose: A videofluoroscopic analysis is the gold standard approach to determine whether a dysphagia problem exists. This procedure is invasive as it involves radiation but also provides the most direct physical evidence of swallowing problems. The main goal of this study was to evaluate an automatic tool based on voice analysis to support medical detection of dysphagia. Methods: An automatic voice analysis system has been developed. Prior to (basal) and immediately following (viscosity) swallowing liquids of varying viscosity and volume, individuals with Parkinson Disease were required to produce the same test word. The acoustic features (linear and non-linear) of this word were then analyzed with regard to specific situations by standard and Machine Learning methods. Results: The results indicated a high degree of accuracy in detecting voice associated with basal and viscosity states. Conclusion: Thus, while the gold standard of dysphagia diagnosis continues to involve video-fluoroscopy analysis, the consideration of voice analysis may prove to be a far simpler and less invasive approach to diagnosis by advanced voice features. © 2017 Informa UK Limited, trading as Taylor & Francis Group","detection; dysphagia; Parkinson disease; Voice analysis",,2-s2.0-85031404349
"Sug H.","Using Machine Learning Methods Jointly to Find Better Set of Rules in Data Mining",2017,"MATEC Web of Conferences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032873302&doi=10.1051%2fmatecconf%2f201712504017&partnerID=40&md5=86aaefc0277034517aa0d71db1227be2","Rough set-based data mining algorithms are one of widely accepted machine learning technologies because of their strong mathematical background and capability of finding optimal rules based on given data sets only without room for prejudiced views to be inserted on the data. But, because the algorithms find rules very precisely, we may confront with the overfitting problem. On the other hand, association rule algorithms find rules of association, where the association resides between sets of items in database. The algorithms find itemsets that occur more than given minimum support, so that they can find the itemsets practically in reasonable time even for very large databases by supplying the minimum support appropriately. In order to overcome the problem of the overfitting problem in rough set-based algorithms, first we find large itemsets, after that we select attributes that cover the large itemsets. By using the selected attributes only, we may find better set of rules based on rough set theory. Results from experiments support our suggested method. © The Authors, published by EDP Sciences, 2017.",,"Artificial intelligence; Computer circuits; Data mining; Learning systems; Optical variables measurement; Association rule algorithm; Data mining algorithm; Machine learning methods; Machine learning technology; Minimum support; Over fitting problem; Rough-set based; Very large database; Rough set theory",2-s2.0-85032873302
"Xiao L.-Y., Shao W., Liang T.-L., Wang B.-Z.","Efficient extreme learning machine with transfer functions for filter design",2017,"IEEE MTT-S International Microwave Symposium Digest",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032463287&doi=10.1109%2fMWSYM.2017.8058624&partnerID=40&md5=c31777727f54d8d37d406b94f1fd665a","This paper proposes a model based on a machine learning algorithm, extreme learning machine (ELM), and the pole-residue-based transfer function (TF) for parametric modeling of electromagnetic behavior of microwave components. Compared with the model based on the artificial neural network, the proposed ELM model can obtain the accurate results for microwave passive component design with the small training datasets due to its good iterative learning ability. The validity and efficiency of this proposed model is confirmed by a triple-mode filter. © 2017 IEEE.","Extreme learning machine (ELM); Filter design; Pole-residue-based transfer function (TF)","Bandpass filters; Iterative methods; Knowledge acquisition; Learning algorithms; Machine components; Microwaves; Neural networks; Poles; Transfer functions; Electromagnetic behavior; Extreme learning machine; Filter designs; Iterative learning; Microwave components; Microwave passive component; Parametric modeling; Triple-mode filters; Learning systems",2-s2.0-85032463287
"Zhang C.-Y., Zhu Y.-Y., Cheng Q.-F., Fu H.-P., Ma J.-G., Zhang Q.-J.","Extreme learning machine for the behavioral modeling of RF power amplifiers",2017,"IEEE MTT-S International Microwave Symposium Digest",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032499882&doi=10.1109%2fMWSYM.2017.8058626&partnerID=40&md5=acf83c0f1b3e86c03c7fceb24263e0ab","In this brief, an efficient approach using extreme learning machine (ELM) is first proposed for the behavioral modeling of radio frequency power amplifiers (RF PAs). As a single-hidden layer feedforward neural network algorithm, ELM offers significant speed advantages over conventional neural network learning algorithms. Compared to the existing behavioral modeling based on ANN, the proposed method also requires minimal human intervention. A Class-E PA is taken as an example for comparing ELM against traditional neural network learning algorithm. The modeling results of ELM for AM/AM and IMD3 agree well with the simulation results, and the speed advantage of the proposed method has also been confirmed. © 2017 IEEE.","Behavioral modeling; Computer-aided design; Extreme learning machine; Nonlinearity; Radio frequency power amplifiers","Behavioral research; Computer aided design; Computer aided instruction; Feedforward neural networks; Knowledge acquisition; Learning systems; Network layers; Power amplifiers; Radio frequency amplifiers; Radio waves; Behavioral model; Extreme learning machine; Human intervention; Neural network learning algorithm; Nonlinearity; Radio frequency power amplifiers; RF power amplifiers; Single-hidden layer feedforward neural networks; Learning algorithms",2-s2.0-85032499882
"Nie Y., Tang Z., Liu F., Chang J., Zhang J.","A data-driven dynamics simulation framework for railway vehicles",2017,"Vehicle System Dynamics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030547753&doi=10.1080%2f00423114.2017.1381981&partnerID=40&md5=8061b708c95fbcc638bf5f2eeb14b067","The finite element (FE) method is essential for simulating vehicle dynamics with fine details, especially for train crash simulations. However, factors such as the complexity of meshes and the distortion involved in a large deformation would undermine its calculation efficiency. An alternative method, the multi-body (MB) dynamics simulation provides satisfying time efficiency but limited accuracy when highly nonlinear dynamic process is involved. To maintain the advantages of both methods, this paper proposes a data-driven simulation framework for dynamics simulation of railway vehicles. This framework uses machine learning techniques to extract nonlinear features from training data generated by FE simulations so that specific mesh structures can be formulated by a surrogate element (or surrogate elements) to replace the original mechanical elements, and the dynamics simulation can be implemented by co-simulation with the surrogate element(s) embedded into a MB model. This framework consists of a series of techniques including data collection, feature extraction, training data sampling, surrogate element building, and model evaluation and selection. To verify the feasibility of this framework, we present two case studies, a vertical dynamics simulation and a longitudinal dynamics simulation, based on co-simulation with MATLAB/Simulink and Simpack, and a further comparison with a popular data-driven model (the Kriging model) is provided. The simulation result shows that using the legendre polynomial regression model in building surrogate elements can largely cut down the simulation time without sacrifice in accuracy. © 2017 Informa UK Limited, trading as Taylor & Francis Group","co-simulation; data-driven modelling; Dynamics simulation; machine learning; surrogate element","Accidents; Artificial intelligence; Dynamics; Efficiency; Feature extraction; Learning systems; Locomotives; MATLAB; Network function virtualization; Railroad rolling stock; Railroads; Regression analysis; Vehicles; Calculation efficiency; Cosimulation; Data driven modelling; Data-driven simulation; Dynamics simulation; Longitudinal dynamics; Machine learning techniques; surrogate element; Finite element method",2-s2.0-85030547753
"Bai T., Yan H., Jia X., Jiang S., Wang G., Mou X.","Z-Index Parameterization (ZIP) for Volumetric CT Image Reconstruction via 3D Dictionary Learning",2017,"IEEE Transactions on Medical Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031825646&doi=10.1109%2fTMI.2017.2759819&partnerID=40&md5=272ad6cce5b48478ad3ae1b303b3c4ae","Despite the rapid developments of x-ray cone-beam CT (CBCT), image noise still remains a major issue for the low dose CBCT. To suppress the noise effectively while retain the structures well for low dose CBCT image, in this work, a sparse constraint based on the 3D dictionary is incorporated into a regularized iterative reconstruction framework, defining the 3DDL method. In addition, by analyzing the sparsity level curve associated with different regularization parameters, a new adaptive parameter selection strategy is proposed to facilitate our 3DDL method. To justify the proposed method, we first analyze the distributions of the representation coefficients associated with the 3D dictionary and the conventional 2D dictionary to compare their efficiencies in representing volumetric images. Then, multiple real data experiments are conducted for performance validation. Based on these results, we found: (1) the 3D dictionary based sparse coefficients have three orders narrower Laplacian distribution compared to the 2D dictionary, suggesting the higher representation efficiencies of the 3D dictionary; (2) the sparsity level curve demonstrates a clear Z-shape, and hence referred to as Z-curve in this paper; (3) the parameter associated with the maximum curvature point of the Z-curve suggests a nice parameter choice, which could be adaptively located with the proposed Z-index parameterization (ZIP) method; (4) the proposed 3DDL algorithm equipped with the ZIP method could deliver reconstructions with the lowest root mean squared errors (RMSE) and the highest structural similarity (SSIM) index compared to the competing methods; (5) similar noise performance as the regular dose FDK reconstruction regarding the standard deviation metric could be achieved with the proposed method using 1/2 / 1/4 / 1/8 dose level projections. The contrast-noise ratio (CNR) is improved by &#x007E; 2.5&#x002F;3.5 times with respect to two different cases under the 1/8 dose level compared to the low dose FDK reconstruction. The proposed method is expected to reduce the radiation dose by a factor of 8 for CBCT, considering the voted strongly discriminated low contrast tissues. IEEE","Computed tomography; cone-beam CT; Dictionaries; Dictionary learning; Image reconstruction; Machine learning; Matching pursuit algorithms; noise suppression; regularization parameter; sparse representation; Three-dimensional displays; Two dimensional displays","Efficiency; Image processing; Image reconstruction; Iterative methods; Mean square error; Parameterization; Cone-beam CT; Dictionary learning; Noise suppression; Regularization parameters; Sparse representation; Computerized tomography",2-s2.0-85031825646
"Geis C., Thoma M., Pittore M., Wieland M., Dech S.W., Taubenbock H.","Multitask Active Learning for Characterization of Built Environments With Multisensor Earth Observation Data",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031803004&doi=10.1109%2fJSTARS.2017.2748339&partnerID=40&md5=91ddf44f3875814979d4ae55791ab7dd","In this paper, we propose a multitask active learning (AL) framework for an efficient characterization of buildings using features from multisensor earth observation data. Conventional AL methods establish query functions based on a preliminary trained learning machine to guide the selection of additional prior knowledge (i.e., labeled samples) for model improvement with respect to a single target variable. In contrast to that, here, we follow three multitask AL metaprotocols to select unlabeled samples from a learning set which can be considered relevant with respect to multiple target variables. In particular, multitask AL methods based on multivariable criterion, alternating selection, rank combination, as well as hybrid approaches, which internalize multiple principles from the different metaprotocols, are introduced. Thereby, the alternating selection strategies implement a so-called one-sided selection (i.e., single-task AL selection for a reference target variable with simultaneous labeling of the residual target variables) with a changing leading variable in an iterative selection process. The multivariable criterion-based methods and rank combination approaches aim to select unlabeled samples based on combined single-task selection decisions. Experimental results are obtained from two application scenarios for the city of Cologne, Germany. Thereby, the target variables to be predicted comprise building material type, building occupancy, urban typology, building type, and roof type. Comparative model accuracy evaluations underline the capability of the introduced methods to provide superior solutions with respect to one-sided selection and random sampling strategies. IEEE","Building material type; building occupancy; building type; Buildings; Data models; Earth; Geometry; LiDAR; multitask active learning (AL); Remote sensing; roof type; Support vector machines; support vector machines (SVM); Training; urban typology; very high resolution imagery","Aluminum; Artificial intelligence; Building materials; Characterization; Data structures; Earth (planet); Geometry; Iterative methods; Learning systems; Multivariable systems; Observatories; Optical radar; Personnel training; Remote sensing; Roofs; Support vector machines; Active Learning; Building occupancy; Building types; urban typology; Very high resolution; Buildings",2-s2.0-85031803004
"Felix E.A., Lee S.P.","Integrated Approach to Software Defect Prediction",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030760696&doi=10.1109%2fACCESS.2017.2759180&partnerID=40&md5=efe792869538454281844b63aad52d94","Software defect prediction provides actionable outputs to software teams while contributing to industrial success. Empirical studies have been conducted on software defect prediction for both cross-project and within-project defect prediction. However, existing studies have yet to demonstrate a method of predicting the number of defects in an upcoming product release. This study presents such a method using predictor variables derived from the defect acceleration, namely, the defect density, defect velocity and defect introduction time, and determines the correlation of each predictor variable with the number of defects. We report the application of an integrated machine learning approach based on regression models constructed from these predictor variables. An experiment was conducted on 10 different datasets collected from the PROMISE repository, containing 22,838 instances. The regression model constructed as a function of the average defect velocity achieved an adjusted R-square of 98.6&#x0025;, with a p-value of &#x003C;0.001. The average defect velocity is strongly positively correlated with the number of defects, with a correlation coefficient of 0.98. Thus, it is demonstrated that this technique can provide a blueprint for program testing to enhance the effectiveness of software development activities. OAPA","class imbalance; defect velocity; machine learning; number of defects; software defect prediction","Artificial intelligence; Defect density; Forecasting; Learning systems; Regression analysis; Software design; Software testing; Velocity; Class imbalance; Correlation coefficient; Defect prediction; Development activity; Integrated approach; Integrated machines; Predictor variables; Software defect prediction; Defects",2-s2.0-85030760696
"Eck A., Zintgraf L.M., de Groot E.F.J., de Meij T.G.J., Cohen T.S., Savelkoul P.H.M., Welling M., Budding A.E.","Interpretation of microbiota-based diagnostics by explaining individual classifier decisions",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030316587&doi=10.1186%2fs12859-017-1843-1&partnerID=40&md5=c5deb4b3ce24d40256c8feb0cefb66ab","Background: The human microbiota is associated with various disease states and holds a great promise for non-invasive diagnostics. However, microbiota data is challenging for traditional diagnostic approaches: It is high-dimensional, sparse and comprises of high inter-personal variation. State of the art machine learning tools are therefore needed to achieve this goal. While these tools have the ability to learn from complex data and interpret patterns therein that cannot be identified by humans, they often operate as black boxes, offering no insight into their decision-making process. In most cases, it is difficult to represent the learning of a classifier in a comprehensible way, which makes them prone to be mistrusted, or even misused, in a clinical environment. In this study, we aim to elucidate microbiota-based classifier decisions in a biologically meaningful context to allow their interpretation. Results: We applied a method for explanation of classifier decisions on two microbiota datasets of increasing complexity: gut versus skin microbiota samples, and inflammatory bowel disease versus healthy gut microbiota samples. The algorithm simulates bacterial species as being unknown to a pre-trained classifier, and measures its effect on the outcome. Consequently, each patient is assigned a unique quantitative estimation of which species in their microbiota defined the classification of their sample. The algorithm was able to explain the classifier decisions well, demonstrated by our validation method, and the explanations were biologically consistent with recent microbiota findings. Conclusions: Application of a method for explaining individual classifier decisions for complex microbiota analysis proved feasible and opens perspectives on personalized therapy. Providing an explanation to support a microbiota-based diagnosis could guide decisions of clinical microbiologists, and has the potential to increase their confidence in the outcome of such decision support systems. This may facilitate the development of new diagnostic applications. © 2017 The Author(s).","Inflammatory bowel disease (IBD); IS-pro; Machine learning; Microbiota; Supervised classification","Artificial intelligence; Decision making; Decision support systems; Diagnosis; Learning systems; Supervised learning; Decision making process; Diagnostic applications; Inflammatory bowel disease; Microbiotas; Non-invasive diagnostics; Personalized therapies; Quantitative estimation; Supervised classification; Classification (of information)",2-s2.0-85030316587
"Hossain M.S., Muhammad G., Al Qurishi M.","Verifying the Images Authenticity in Cognitive Internet of Things (CIoT)-Oriented Cyber Physical System",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030327342&doi=10.1007%2fs11036-017-0928-4&partnerID=40&md5=302bfbdd1568fcc4ac219c3a8b2c9055","With the recent development of Cognitive Internet of Things (CIoT) and the potential of Cyber Physical System (CPS), people’s daily activities become smarter, and intelligent. The combination of CIoT and CPS can greatly enhance the quality of people’s life. To this end, this article proposes CIoT-CPS that comprises of two main models: user activity cognitive model and image authentication model.The user activity cognitive model (UACM) is a machine-learning model to have the meaningful data. The image authentication model is to verify the authenticity of images captured by various devices, such as smart phones, digital cameras, and other camera-embedded portable devices. The authenticity of an image is breached when parts of images are assembled to produce a new image (known as a splicing forgery), or a part of an image is copied or pasted into another part of the same image (known as a copy-move forgery). In the proposed verification method, an opposite color local binary pattern (OC-LBP) texture descriptor is applied to a questioned image. The image is first decomposed into an RGB (red, green, blue) and a luminance and chroma color spaces. The OC-LBP measures the interrelation between pixels of different color components. The intensive computation involving six color components and a gray version is performed in the cloud, where a server can be dedicated to doing this job. The histograms of the OC-LBP are concatenated with weights to produce a final feature vector of the image. A support vector machine is applied as a classifier, which classifies the image as authentic or forged. Several experiments were performed to verify the suitability of those models or approaches. The proposed approaches show a good accuracy compared to other competing approaches. © 2017 Springer Science+Business Media, LLC","Cloud-based cyber physical system; Image authenticity verification; Image forgery; Opposite color local binary pattern","Color; Cyber Physical System; Digital devices; Embedded systems; Image retrieval; Internet of things; Learning systems; Smartphones; Authenticity verification; Cloud-based; Cyber-physical systems (CPS); Embedded portable devices; Image authentication; Image forgery; Local binary patterns; Machine learning models; Authentication",2-s2.0-85030327342
"Luo Y., Wen Y., Tao D.","Heterogeneous Multitask Metric Learning Across Multiple Domains",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030779435&doi=10.1109%2fTNNLS.2017.2750321&partnerID=40&md5=fee74fcf3a60ea920f4d2552af99c3e7","Distance metric learning plays a crucial role in diverse machine learning algorithms and applications. When the labeled information in a target domain is limited, transfer metric learning (TML) helps to learn the metric by leveraging the sufficient information from other related domains. Multitask metric learning (MTML), which can be regarded as a special case of TML, performs transfer across all related domains. Current TML tools usually assume that the same feature representation is exploited for different domains. However, in real-world applications, data may be drawn from heterogeneous domains. Heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains. However, they are often limited in that only two domains can be handled. To appropriately handle multiple domains, we develop a novel heterogeneous MTML (HMTML) framework. In HMTML, the metrics of all different domains are learned together. The transformations derived from the metrics are utilized to induce a common subspace, and the high-order covariance among the predictive structures of these domains is maximized in this subspace. There do exist a few heterogeneous transfer learning approaches that deal with multiple domains, but the high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, is ignored in these approaches. Compared with them, the proposed HMTML can effectively explore such high-order information, thus obtaining more reliable feature transformations and metrics. Effectiveness of our method is validated by the extensive and intensive experiments on text categorization, scene classification, and social image annotation. IEEE","Distance metric learning (DML); heterogeneous domain; high-order statistics; multitask; tensor.","Learning systems; Multitasking; Tensors; Text processing; Distance Metric Learning; Feature representation; Feature transformations; Heterogeneous domains; High order statistics; Predictive structures; Scene classification; Social image annotations; Learning algorithms",2-s2.0-85030779435
"Wen L., Gao L., Li X.","A New Deep Transfer Learning Based on Sparse Auto-Encoder for Fault Diagnosis",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030792766&doi=10.1109%2fTSMC.2017.2754287&partnerID=40&md5=b7bcbce556d2028b56309d5986b759db","Fault diagnosis plays an important role in modern industry. With the development of smart manufacturing, the data-driven fault diagnosis becomes hot. However, traditional methods have two shortcomings: 1) their performances depend on the good design of handcrafted features of data, but it is difficult to predesign these features and 2) they work well under a general assumption: the training data and testing data should be drawn from the same distribution, but this assumption fails in many engineering applications. Since deep learning (DL) can extract the hierarchical representation features of raw data, and transfer learning provides a good way to perform a learning task on the different but related distribution datasets, deep transfer learning (DTL) has been developed for fault diagnosis. In this paper, a new DTL method is proposed. It uses a three-layer sparse auto-encoder to extract the features of raw data, and applies the maximum mean discrepancy term to minimizing the discrepancy penalty between the features from training data and testing data. The proposed DTL is tested on the famous motor bearing dataset from the Case Western Reserve University. The results show a good improvement, and DTL achieves higher prediction accuracies on most experiments than DL. The prediction accuracy of DTL, which is as high as 99.82&#x0025;, is better than the results of other algorithms, including deep belief network, sparse filter, artificial neural network, support vector machine and some other traditional methods. What is more, two additional analytical experiments are conducted. The results show that a good unlabeled third dataset may be helpful to DTL, and a good linear relationship between the final prediction accuracies and their standard deviations have been observed. IEEE","Deep learning (DL); fault diagnosis; sparse auto-encoder (SAE); transfer learning","Deep learning; Failure analysis; Forecasting; Learning systems; Neural networks; Signal encoding; Auto encoders; Case Western Reserve University; Data-driven fault diagnosis; Deep belief networks; Engineering applications; Hierarchical representation; Linear relationships; Transfer learning; Fault detection",2-s2.0-85030792766
"Leung C.S.","Special issue on ACML 2015",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017131289&doi=10.1016%2fj.neucom.2017.02.076&partnerID=40&md5=9fffe0862c8885753a5833b2cbb93863",[No abstract available],,"Bayes theorem; Bayesian learning; Editorial; finite budget analysis; linear regression analysis; machine learning; mathematical analysis; priority journal; stochastic model",2-s2.0-85017131289
"Muñoz-Organero M., Ruiz-Blázquez R.","Detecting steps walking at very low speeds combining outlier detection, transition matrices and autoencoders from acceleration patterns",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031301950&doi=10.3390%2fs17102274&partnerID=40&md5=f5e16475212ca362629f2f86b54c7330","In this paper, we develop and validate a new algorithm to detect steps while walking at speeds between 30 and 40 steps per minute based on the data sensed from a single tri-axial accelerometer. The algorithm concatenates three consecutive phases. First, an outlier detection is performed on the sensed data based on the Mahalanobis distance to pre-detect candidate points in the acceleration time series that may contain a ground contact segment of data while walking. Second, the acceleration segment around the pre-detected point is used to calculate the transition matrix in order to capture the time dependencies. Finally, autoencoders, trained with data segments containing ground contact transition matrices from acceleration series from labeled steps are used to reconstruct the computed transition matrices at each pre-detected point. A similarity index is used to assess if the pre-selected point contains a true step in the 30–40 steps per minute speed range. Our experimental results, based on a database from three different participants performing similar activities to the target one, are able to achieve a recall = 0.88 with precision = 0.50 improving the results when directly applying the autoencoders to acceleration patterns (recall = 0.77 with precision = 0.50). © 2017 by the authors; Licensee MDPI, Basel, Switzerland.","Autoencoders; Machine learning; Outlier detection; Step detection; Transition matrices","Acceleration; Data handling; Learning systems; Statistics; Walking aids; Acceleration pattern; Acceleration time series; Autoencoders; Mahalanobis distances; Outlier Detection; Step detection; Transition matrices; Triaxial accelerometer; Matrix algebra",2-s2.0-85031301950
"Boly M., Massimini M., Tsuchiya N., Postle B.R., Koch C., Tononi G.","Are the neural correlates of consciousness in the front or in the back of the cerebral cortex? Clinical and neuroimaging evidence",2017,"Journal of Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030719157&doi=10.1523%2fJNEUROSCI.3218-16.2017&partnerID=40&md5=9d92af2bea9a45e34b738dc8d19fb549","The role of the frontal cortex in consciousness remains a matter of debate. In this Perspective, we will critically review the clinical and neuroimaging evidence for the involvement of the front versus the back of the cortex in specifying conscious contents and discuss promising research avenues. ©2017 the authors.","Consciousness; Frontal cortex; Lesion studies; Neuroimaging; Stimulation studies","Article; blood flow; brain cortex; cognition; consciousness; diffusion tensor imaging; electrostimulation; functional magnetic resonance imaging; human; machine learning; nerve injury; neuroimaging; neuromodulation; nonREM sleep; occipital cortex; oxygen supply; parahippocampal place area; primary motor cortex; priority journal; reliability; REM sleep; reproducibility; sensory stimulation; task performance; transcranial magnetic stimulation; visual cortex; working memory; animal; brain cortex; brain mapping; consciousness; nerve cell network; neuroimaging; physiology; procedures; Animals; Brain Mapping; Cerebral Cortex; Consciousness; Humans; Nerve Net; Neuroimaging",2-s2.0-85030719157
"Yu Y., Wang J., Tan Q., Jia L., Yu G.","Semi-supervised Multi-label Dimensionality Reduction based on Dependence Maximization",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030772968&doi=10.1109%2fACCESS.2017.2760141&partnerID=40&md5=081713723e5e9bdbcb63f809c16da2b1","Alike other machine learning paradigms, multilabel learning also suffers from the curse of dimensionality problem. Multi-label dimensionality reduction can alleviate the problem but they generally ask for sufficient labeled samples. Nevertheless, we often may only have scarce labeled samples and abundant unlabeled samples. In this paper, we propose a Semi-supervised Multi-label Dimensionality Reduction based on dependence maximization approach (SMDRdm in short). SDMRdm assumes the semantic similarity and feature similarity of multi-label samples are inter-depended. SMDRdm firstly applies label propagation on a neighborhood graph composed with labeled and unlabeled samples to obtain the soft labels of unlabeled samples, and then measures the semantic similarity between all the training samples (including unlabeled ones) based on these soft labels and available labels of labeled samples. Next, it measures the feature similarity between samples in the subspace projected by the target projective matrix, instead of the original high-dimensional feature space. After that, it maximizes the dependence between these two types of similarities and incorporates the dependence into linear discriminant analysis to optimize the target projective matrix. Experiments on publicly accessible multi-label datasets demonstrate that SMDRdm achieves more prominent results than other related approaches across various evaluation metrics. In addition, the empirical study also shows the semantic similarity between samples derived from soft labels works better than that derived from scarce available labels. OAPA","dependence maximization; dimensionality reduction; Multi-label learning; semantic similarity; semi-supervised learning","Discriminant analysis; Learning algorithms; Learning systems; Supervised learning; Curse of dimensionality; Dimensionality reduction; Feature similarities; High-dimensional feature space; Linear discriminant analysis; Multi-label learning; Semantic similarity; Semi- supervised learning; Semantics",2-s2.0-85030772968
"Beltrán Prieto L.A., Komínkova-Oplatková Z.","A performance comparison of two emotion-recognition implementations using OpenCV and Cognitive Services API",2017,"MATEC Web of Conferences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032874818&doi=10.1051%2fmatecconf%2f201712502067&partnerID=40&md5=da3c20a9f30cf9b32d6242301e81d75b","Emotions represent feelings about people in several situations. Various machine learning algorithms have been developed for emotion detection in a multimedia element, such as an image or a video. These techniques can be measured by comparing their accuracy with a given dataset in order to determine which algorithm can be selected among others. This paper deals with the comparison of two implementations of emotion recognition in faces, each implemented with specific technology. OpenCV is an open-source library of functions and packages mostly used for computer-vision analysis and applications. Cognitive services is a set of APIs containing artificial intelligence algorithms for computer-vision, speech, knowledge, and language processing. Two Android mobile applications were developed in order to test the performance between an OpenCV algorithm for emotion recognition and an implementation of Emotion cognitive service. For this research, one thousand tests were carried out per experiment. Our findings show that the OpenCV implementation got a better performance than the Cognitive services application. In both cases, performance can be improved by increasing the sample size per emotion during the training step. © The Authors, published by EDP Sciences, 2017.",,"Artificial intelligence; Computer circuits; Learning algorithms; Learning systems; Speech recognition; Artificial intelligence algorithms; Emotion recognition; Language processing; Mobile applications; Multimedia elements; Open-source libraries; Performance comparison; Services applications; Computer vision",2-s2.0-85032874818
"Abid A., Khan M.T., de Silva C.W.","Layered and Real-Valued Negative Selection Algorithm for Fault Detection",2017,"IEEE Systems Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030779275&doi=10.1109%2fJSYST.2017.2753851&partnerID=40&md5=606ed939595027d2a5aef9d968743d96","In this paper, the challenging task of the development of a generic fault detection (FD) method is addressed. While the past FD research has primarily focused on modeling and signal-processing methods that are problem specific and require complete knowledge of the system model and fault types, this paper presents a novel layered and real-valued negative-selection algorithm (LRNSA)-based FD method independent of prior knowledge of fault types and patterns. Specifically, in the training phase, the nonself-space is divided into different layers for effective generation and distribution of detectors using normal (self) data. The major accomplishments of the proposed method are improved nonself-space coverage of the uncovered gaps (holes), followed by the formation of cluster detector with large radius. To test the capabilities of the developed method, the generated specialized detector distribution is studied using bearing fault modeling in a three-phase induction motor. The proposed method is subsequently investigated and validated by applying it to an actual induction motor for different types of bearing faults. Finally, the comparative results on the benchmark dataset demonstrate the superiority of the proposed method compared to the state-of-the-art machine learning algorithms in terms of higher FD accuracy and quick detection with reduced online detection time. IEEE","Fault detection (FD); induction motor; layered algorithm; negative selection; nonself-coverage","Finite difference method; Induction motors; Learning algorithms; Learning systems; Phase comparators; Signal processing; Benchmark datasets; Different layers; Negative selection; nonself-coverage; On-line detection; Real-valued negative selection algorithms; State of the art; Three phase induction motor; Fault detection",2-s2.0-85030779275
"Cai J., King J., Pedro J.C.","A new nonlinear behavioral modeling technique for RF power transistors based on Bayesian inference",2017,"IEEE MTT-S International Microwave Symposium Digest",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032499884&doi=10.1109%2fMWSYM.2017.8058645&partnerID=40&md5=2a0a53455a9bedcada4e7d2759e06f9d","A novel nonlinear behavioral modeling technique, for transistor behavioral modeling, is presented in this paper. Compared with existing modeling techniques, the new approach is based on a fundamentally different theory, Bayesian inference (one of the core methods of machine learning). The new technique not only good at handling multidimensional modeling problem, it could also greatly alleviated the notorious overfitting issue through corresponded model extraction method. Both simulation and experimental test examples for a 10W Cree GaN transistor are provided. The new model provides accurate prediction throughout the Smith chart at different input power levels. © 2017 IEEE.","Bayesian inference; Device modeling; Nonlinear behavioral modeling; RF power transistor","Bayesian networks; Behavioral research; Gallium compounds; Inference engines; Learning systems; Power transistors; Accurate prediction; Bayesian inference; Behavioral model; Device modeling; Experimental test; Modeling technique; Multi-dimensional model; RF power transistors; Transistors",2-s2.0-85032499884
"Özel T., Altay A., Donmez A., Leach R.","Surface topography investigations on nickel alloy 625 fabricated via laser powder bed fusion",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541854&doi=10.1007%2fs00170-017-1187-z&partnerID=40&md5=8eabe195f1465fc1821dcdaab4fecd69","Laser powder bed fusion as an additive manufacturing process produces complex surface topography at multiple scales through rapid heating, melting, directional cooling, and solidification that are often governed by laser path and layer-to-layer scanning strategies and influenced by process parameters such as power, scan velocity, hatch distance, and resultant energy density. Investigations on manufactured surfaces, as-built and after applying electropolishing, are performed using stylus profilometry, digital optical microscopy, and scanning electron microscopy techniques to reveal the complex surface texture of the nickel alloy 625 test cubes that are produced by following an experimental design. Surface texture is further explored using image processing together with machine learning-based algorithms. Measurement uncertainty is also discussed briefly. The results reveal a complex nature of laser powder bed fusion created surface topography and textures as exposed with electropolishing that may further lead to a quantitative understanding of such textures and their formations influenced by different scanning strategies and process parameters. © 2017 Springer-Verlag London Ltd.","Nickel alloy; Powder bed fusion; Surface texture; Surface topography","Electrolytic polishing; Image processing; Learning systems; Nickel; Nickel alloys; Optical data processing; Scanning electron microscopy; Topography; Uncertainty analysis; Additive manufacturing process; Complex surface; Measurement uncertainty; Powder bed; Process parameters; Scanning strategies; Stylus profilometry; Surface textures; Surface topography",2-s2.0-85030541854
"Philips S., Wu H.-Y., Li L.","Using machine learning algorithms to identify genes essential for cell survival",2017,"BMC Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030331000&doi=10.1186%2fs12859-017-1799-1&partnerID=40&md5=3d120fc6be61dd9e769aab4dcff8a721","Background: With the explosion of data comes a proportional opportunity to identify novel knowledge with the potential for application in targeted therapies. In spite of this huge amounts of data, the solutions to treating complex disease is elusive. One reason being that these diseases are driven by a network of genes that need to be targeted in order to understand and treat them effectively. Part of the solution lies in mining and integrating information from various disciplines. Here we propose a machine learning method to mining through publicly available literature on RNA interference with the goal of identifying genes essential for cell survival. Results: A total of 32,164 RNA interference abstracts were identified from 10.5 million pubmed abstracts (2001 - 2015). These abstracts spanned over 1467 cancer cell lines and 4373 genes representing a total of 25,891 cell gene associations. Among the 1467 cell lines 88% of them had at least 1 or up to 25 genes studied in a given cell line. Among the 4373 genes 96% of them were studied in at least 1 or up to 25 different cell lines. Conclusions: Identifying genes that are crucial for cell survival can be a critical piece of information especially in treating complex diseases, such as cancer. The efficacy of a therapeutic intervention is multifactorial in nature and in many cases the source of therapeutic disruption could be from an unsuspected source. Machine learning algorithms helps to narrow down the search and provides information about essential genes in different cancer types. It also provides the building blocks to generate a network of interconnected genes and processes. The information thus gained can be used to generate hypothesis which can be experimentally validated to improve our understanding of what triggers and maintains the growth of cancerous cells. © 2017 The Author(s).","Gene essentiality; Literature mining; Machine learning","Abstracting; Artificial intelligence; Cell culture; Cells; Complex networks; Cytology; Diseases; Genes; Learning systems; RNA; Building blockes; Cancer cell lines; Gene associations; Integrating information; Literature mining; Machine learning methods; RNA interference; Therapeutic intervention; Learning algorithms",2-s2.0-85030331000
"Jin B., Jing Z., Zhao H.","Incremental and Decremental Extreme Learning Machine based on Generalized Inverse",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030776592&doi=10.1109%2fACCESS.2017.2758645&partnerID=40&md5=5c7af10c0f835babbbd66e719bf8381f","In online sequential applications, a machine learning model needs to have a self-updating ability to handle the situation, that the training set is changing. Conventional incremental extreme learning machine (ELM) and online sequential ELM are usually achieved in two approaches: directly updating the output weight and recursively computing the left pseudo inverse of the hidden layer output matrix. In this paper, we develop a novel solution for incremental and decremental ELM, via recursively updating and downdating the generalized inverse of the hidden layer output matrix. By preserving the global optimality and best generalization performance, our approach implements node incremental ELM (N-IELM) and sample incremental ELM (S-IELM) in a universal form, and overcomes the problem of self-starting and numerical instability in the conventional online sequential ELM. We also propose sample decremental ELM (S-DELM), which is the first decremental version of ELM. The experiments on regression and classification problems with real-world datasets demonstrate the feasibility and effectiveness of the proposed algorithms with encouraging performances. OAPA","Approximation error; Classification algorithms; Computational complexity; Computational modeling; decremental ELM; Extreme learning machine; generalized inverse; incremental ELM; Machine learning algorithms; Neurons; online sequential ELM; Training","Approximation algorithms; Artificial intelligence; Classification (of information); Computational complexity; Inverse problems; Knowledge acquisition; Learning algorithms; Matrix algebra; Neurons; Personnel training; Approximation errors; Classification algorithm; Computational model; decremental ELM; Extreme learning machine; Generalized inverse; incremental ELM; online sequential ELM; Learning systems",2-s2.0-85030776592
"Havugimana P.C., Hu P., Emili A.","Protein complexes, big data, machine learning and integrative proteomics: lessons learned over a decade of systematic analysis of protein interaction networks",2017,"Expert Review of Proteomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029596048&doi=10.1080%2f14789450.2017.1374179&partnerID=40&md5=fd744944c49253fca4dfec308a3359f2","Overview: Elucidation of the networks of physical (functional) interactions present in cells and tissues is fundamental for understanding the molecular organization of biological systems, the mechanistic basis of essential and disease-related processes, and for functional annotation of previously uncharacterized proteins (via guilt-by-association or -correlation). After a decade in the field, we felt it timely to document our own experiences in the systematic analysis of protein interaction networks. Areas covered: Researchers worldwide have contributed innovative experimental and computational approaches that have driven the rapidly evolving field of ‘functional proteomics’. These include mass spectrometry-based methods to characterize macromolecular complexes on a global-scale and sophisticated data analysis tools–most notably machine learning–that allow for the generation of high-quality protein association maps. Expert commentary: Here, we recount some key lessons learned, with an emphasis on successful workflows, and challenges, arising from our own and other groups’ ongoing efforts to generate, interpret and report proteome-scale interaction networks in increasingly diverse biological contexts. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","functional proteomics; interactome; machine learning; Macromolecular complex; mass spectrometry; network; prediction; protein interaction; scoring; systems biology","protein; Article; biochemistry; Escherichia coli; eukaryote; functional proteomics; human; machine learning; nonhuman; protein analysis; protein function; protein interaction; protein purification; proteomics; retrospective study; tandem mass spectrometry",2-s2.0-85029596048
"Zhu J., Song Y., Jiang D., Song H.","A New Deep-Q-Learning-Based Transmission Scheduling Mechanism for the Cognitive Internet of Things",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030756296&doi=10.1109%2fJIOT.2017.2759728&partnerID=40&md5=259f95debadbce33e00473d88eb5d555","Cognitive networks (CNs) are one of the key enablers for the Internet of Things (IoT), where CNs will play an important role in the future internet in several application scenarios, such as healthcare, agriculture, environment monitoring, and smart metering. However, the current low packet transmission efficiency of IoT faces a problem of the crowded spectrum for the rapidly increasing popularities of various wireless applications. Hence, the IoT that uses the advantages of cognitive technology, namely the cognitive radio-based Internet of Things (CIoT), is a promising solution for IoT applications. A major challenge in CIoT is the packet transmission efficiency using CNs. Therefore, a new Q-learning-based transmission scheduling mechanism using deep learning for the CIoT is proposed to solve the problem of how to achieve the appropriate strategy to transmit packets of different buffers through multiple channels to maximize the system throughput. A Markov decision process based model is formulated to describe the state transformation of the system. A relay is used to transmit packets to the sink for the other nodes. To maximize the system utility in different system states, the reinforcement learning method, i.e., the Q learning algorithm, is introduced to help the relay to find the optimal strategy. In addition, the stacked auto-encoders deep learning model is used to establish the mapping between the state and the action to accelerate the solution of the problem. Finally, the experimental results demonstrate that the new action selection method can converge after a certain number of iterations. Compared with other algorithms, the proposed method can better transmit packets with less power consumption and packet loss. IEEE","Bit error rate; cognitive networks; deep learning.; Internet of Things; Internet of Tings; Machine learning; Markov decision process; Q learning; Relays; Scheduling; Throughput; Wireless sensor networks","Behavioral research; Bit error rate; Cognitive radio; Deep learning; Efficiency; Learning algorithms; Learning systems; Markov processes; Packet networks; Problem solving; Reinforcement learning; Scheduling; Throughput; Wireless sensor networks; Wireless telecommunication systems; Cognitive network; Internet of thing (IOT); Markov Decision Processes; Packet transmission efficiencies; Q-learning; Reinforcement learning method; Relays; Transmission scheduling; Internet of things",2-s2.0-85030756296
"Bu F.","A High-order Clustering Algorithm Based on Dropout Deep Learning for Heterogeneous Data in Cyber-Physical-Social Systems",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773719&doi=10.1109%2fACCESS.2017.2759509&partnerID=40&md5=6a7261ae0841511a868d5c3733d996a2","An explosive growth of cyber-physical-social systems has been witnessed owing to the wide use of various mobile devices recently. A large volume of heterogeneous data has been collected from cyber-physical-social systems in the past few years. Each object in the heterogeneous dataset is typically multi-modal, posing a remarkable challenge on heterogeneous data clustering. In this paper, we propose a high-order k-means algorithm based on the dropout deep learning model for clustering heterogeneous objects in cyber-physical-social systems.We first build three dropout stacked auto-encoders, each with three hidden layers to learn the features for the different modalities of each object. Furthermore, we establish a feature tensor for each object by using the vector outer product to fuse the learned features. At last, we devise a tensor k-means algorithm to cluster the heterogeneous objects based on the tensor distance. We evaluate the proposed high-order k-means algorithm on two representative heterogeneous datasets and results imply that the proposed high-order k-means algorithm can achieve more accurate clustering results than other heterogeneous data clustering methods. OAPA","Algorithm design and analysis; Clustering algorithms; Computer architecture; Cyber-physical-social systems; Data mining; Dropout deep learning model; Feature extraction; Heterogeneous data; High-order clustering; Machine learning; Tensile stress","Cluster analysis; Computer architecture; Cyber Physical System; Data mining; Deep learning; Feature extraction; Learning algorithms; Learning systems; Tensile stress; Tensors; Algorithm design and analysis; Heterogeneous data; High-order; Learning models; Social systems; Clustering algorithms",2-s2.0-85030773719
"Akhtar N., Mian A.","Nonparametric, Coupled ,Bayesian ,Dictionary ,and Classifier Learning for Hyperspectral Classification",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782530&doi=10.1109%2fTNNLS.2017.2742528&partnerID=40&md5=5893b587ebd9f5010ca20a1779fcca06","We present a principled approach to learn a discriminative dictionary along a linear classifier for hyperspectral classification. Our approach places Gaussian Process priors over the dictionary to account for the relative smoothness of the natural spectra, whereas the classifier parameters are sampled from multivariate Gaussians. We employ two Beta-Bernoulli processes to jointly infer the dictionary and the classifier. These processes are coupled under the same sets of Bernoulli distributions. In our approach, these distributions signify the frequency of the dictionary atom usage in representing class-specific training spectra, which also makes the dictionary discriminative. Due to the coupling between the dictionary and the classifier, the popularity of the atoms for representing different classes gets encoded into the classifier. This helps in predicting the class labels of test spectra that are first represented over the dictionary by solving a simultaneous sparse optimization problem. The labels of the spectra are predicted by feeding the resulting representations to the classifier. Our approach exploits the nonparametric Bayesian framework to automatically infer the dictionary size--the key parameter in discriminative dictionary learning. Moreover, it also has the desirable property of adaptively learning the association between the dictionary atoms and the class labels by itself. We use Gibbs sampling to infer the posterior probability distributions over the dictionary and the classifier under the proposed model, for which, we derive analytical expressions. To establish the effectiveness of our approach, we test it on benchmark hyperspectral images. The classification performance is compared with the state-of-the-art dictionary learning-based classification methods. IEEE","Bayes methods; Beta-Bernoulli process; coupled Bayesian dictionary learning; Dictionaries; discriminative dictionary learning; Gaussian process; hyperspectral classification.; Hyperspectral imaging; Machine learning; Training; Training data","Atoms; Gaussian distribution; Gaussian noise (electronic); Glossaries; Hyperspectral imaging; Learning systems; Optimization; Personnel training; Probability distributions; Spectroscopy; Bayes method; Bernoulli process; Dictionary learning; Discriminative dictionaries; Gaussian Processes; Hyper-spectral classification; Training data; Classification (of information)",2-s2.0-85030782530
"Nabih-Ali M., El-Dahshan E.-S.A., Yahia A.S.","A review of intelligent systems for heart sound signal analysis",2017,"Journal of Medical Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031709378&doi=10.1080%2f03091902.2017.1382584&partnerID=40&md5=35021510a6bcf4bbc1ac86e5fb8cd1db","Intelligent computer-aided diagnosis (CAD) systems can enhance the diagnostic capabilities of physicians and reduce the time required for accurate diagnosis. CAD systems could provide physicians with a suggestion about the diagnostic of heart diseases. The objective of this paper is to review the recent published preprocessing, feature extraction and classification techniques and their state of the art of phonocardiogram (PCG) signal analysis. Published literature reviewed in this paper shows the potential of machine learning techniques as a design tool in PCG CAD systems and reveals that the CAD systems for PCG signal analysis are still an open problem. Related studies are compared to their datasets, feature extraction techniques and the classifiers they used. Current achievements and limitations in developing CAD systems for PCG signal analysis using machine learning techniques are presented and discussed. In the light of this review, a number of future research directions for PCG signal analysis are provided. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","classification; feature extraction; intelligent computer-aided diagnosis (CAD) systems; machine learning; Phonocardiogram (PCG)","Acoustic signal processing; Artificial intelligence; Biomedical signal processing; Cardiology; Classification (of information); Computer aided instruction; Diagnosis; Extraction; Feature extraction; Intelligent systems; Learning algorithms; Learning systems; Phonocardiography; Signal analysis; Computer Aided Diagnosis(CAD); Diagnostic capabilities; Feature extraction and classification; Feature extraction techniques; Future research directions; Heart sound signal; Machine learning techniques; Phonocardiograms; Computer aided diagnosis",2-s2.0-85031709378
"Miner A.S., Milstein A., Hancock J.T.","Talking to machines about personal mental health problems",2017,"JAMA - Journal of the American Medical Association",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031321388&doi=10.1001%2fjama.2017.14151&partnerID=40&md5=b6b8f1957e3ddc2a52d66dfdb0c62e7a",[No abstract available],,"artificial intelligence; chronic pain; clinical assessment; depression; human; language processing; machine learning; malpractice; medical device; medical information; medical practice; mental disease; mental health care; mental health service; patient care; priority journal; Short Survey; smartphone; social media; software; artificial intelligence; Mental Disorders; self disclosure; software; Artificial Intelligence; Humans; Mental Disorders; Self Disclosure; Software",2-s2.0-85031321388
"Sher G., Zhi D., Zhang S.","DRREP: Deep ridge regressed epitope predictor",2017,"BMC Genomics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030313529&doi=10.1186%2fs12864-017-4024-8&partnerID=40&md5=e8370bdccdc9627edb2319758693d55a","Introduction: The ability to predict epitopes plays an enormous role in vaccine development in terms of our ability to zero in on where to do a more thorough in-vivo analysis of the protein in question. Though for the past decade there have been numerous advancements and improvements in epitope prediction, on average the best benchmark prediction accuracies are still only around 60%. New machine learning algorithms have arisen within the domain of deep learning, text mining, and convolutional networks. This paper presents a novel analytically trained and string kernel using deep neural network, which is tailored for continuous epitope prediction, called: Deep Ridge Regressed Epitope Predictor (DRREP). Results: DRREP was tested on long protein sequences from the following datasets: SARS, Pellequer, HIV, AntiJen, and SEQ194. DRREP was compared to numerous state of the art epitope predictors, including the most recently published predictors called LBtope and DMNLBE. Using area under ROC curve (AUC), DRREP achieved a performance improvement over the best performing predictors on SARS (13.7%), HIV (8.9%), Pellequer (1.5%), and SEQ194 (3.1%), with its performance being matched only on the AntiJen dataset, by the LBtope predictor, where both DRREP and LBtope achieved an AUC of 0.702. Conclusion: DRREP is an analytically trained deep neural network, thus capable of learning in a single step through regression. By combining the features of deep learning, string kernels, and convolutional networks, the system is able to perform residue-by-residue prediction of continues epitopes with higher accuracy than the current state of the art predictors. © 2017 The Author(s).","Analytical learning; Continuous epitope; Convolutional network; Deep network; Epitope prediction; Linear epitope; Neural network; String kernel","epitope; algorithm; amino acid sequence; AntiJen database; Article; artificial neural network; data base; deep ridge regressed epitope predictor; Human immunodeficiency virus infection; kernel method; learning; machine learning; mining; nerve cell; Pellequer database; prediction; predictor variable; SEQ194 database; severe acute respiratory syndrome",2-s2.0-85030313529
"Crippa A., Salvatore C., Molteni E., Mauri M., Salandi A., Trabattoni S., Agostoni C., Molteni M., Nobile M., Castiglioni I.","The utility of a computerized algorithm based on a multi-domain profile of measures for the diagnosis of attention deficit/hyperactivity disorder",2017,"Frontiers in Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032720196&doi=10.3389%2ffpsyt.2017.00189&partnerID=40&md5=565207c79e0a2aaa3e37ae60bb4e8eba","The current gold standard for diagnosis of attention deficit/hyperactivity disorder (ADHD) includes subjective measures, such as clinical interview, observation, and rating scales. The significant heterogeneity of ADHD symptoms represents a challenge for this assessment and could prevent an accurate diagnosis. The aim of this work was to investigate the ability of a multi-domain profile of measures, including blood fatty acid (FA) profiles, neuropsychological measures, and functional measures from near-infrared spectroscopy (fNIRS), to correctly recognize school-aged children with ADHD. To answer this question, we elaborated a supervised machine-learning method to accurately discriminate 22 children with ADHD from 22 children with typical development by means of the proposed profile of measures. To assess the performance of our classifier, we adopted a nested 10-fold cross validation, where the original dataset was split into 10 subsets of equal size, which were used repeatedly for training and testing. Each subset was used once for performance validation. Our method reached a maximum diagnostic accuracy of 81% through the combining of the predictive models trained on neuropsychological, FA profiles, and deoxygenated-hemoglobin features. With respect to the analysis of a single-domain dataset per time, the most discriminant neuropsychological features were measures of vigilance, focused and sustained attention, and cognitive flexibility; the most discriminating blood FAs were linoleic acid and the total amount of polyunsaturated fatty acids. Finally, with respect to the fNIRS data, we found a significant advantage of the deoxygenated-hemoglobin over the oxygenated-hemoglobin data in terms of predictive accuracy. These preliminary findings show the feasibility and applicability of our machine-learning method in correctly identifying children with ADHD based on multi-domain data. The present machine-learning classification approach might be helpful for supporting the clinical practice of diagnosing ADHD, even fostering a computer-aided diagnosis perspective. © 2017 Crippa, Salvatore, Molteni, Mauri, Salandi, Trabattoni, Agostoni, Molteni, Nobile and Castiglioni.","Attention deficit/hyperactivity disorder; Fatty acids; Machine learning; Near-infrared spectroscopy; Support vector machines",,2-s2.0-85032720196
"Rahman M.A., LaPierre N., Rangwala H.","Phenotype Prediction from Metagenomic Data Using Clustering and Assembly with Multiple Instance Learning (CAMIL)",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030749336&doi=10.1109%2fTCBB.2017.2758782&partnerID=40&md5=6833bd5995b3e3668e75d536de8a4985","The recent advent of Metagenome Wide Association Studies (MGWAS) provides insight into the role of microbes on human health and disease. However, the studies present several computational challenges. In this paper we demonstrate a novel, efficient, and effective Multiple Instance Learning (MIL) based computational pipeline to predict patient phenotype from metagenomic data. MIL methods have the advantage that besides predicting the clinical phenotype, we can infer the instance level label or role of microbial sequence reads in the specific disease. Specifically, we use a Bag of Words method, which has been shown to be one of the most effective and efficient MIL methods. This involves assembly of the metagenomic sequence data, clustering of the assembled contigs, extracting features from the contigs, and using an SVM classifier to predict patient labels and identify the most relevant sequence clusters. With the exception of the given labels for the patients, this entire process is de novo (unsupervised). We call our pipeline &#x201C;CAMIL&#x201D;, which stands for Clustering and Assembly with Multiple Instance Learning. We use multiple state-of-the-art clustering methods for feature extraction, evaluate and comparison of the performance of our proposed approach for each of these clustering methods. We also present a fast and scalable pre-clustering algorithm as a preprocessing step for our proposed pipeline. Our approach achieves efficiency by partitioning the large number of sequence reads into groups (called canopies) using locality sensitive hashing (LSH). These canopies are then refined by using state-of-the-art sequence clustering algorithms. We use data from a well-known MGWAS study of patients with Type-2 Diabetes and show that our pipeline significantly outperforms the classifier used in that paper, as well as other common MIL methods. IEEE","Assembly; Canopy; Clustering; Diseases; Feature extraction; Genomics; LSH; Metagenome; Multiple Instance Learning; Pipelines; Sequential analysis; Standards; Support vector machines","Assembly; Cluster analysis; Diseases; Extraction; Feature extraction; Forecasting; Learning systems; Pipelines; Standards; Support vector machines; Canopy; Clustering; Genomics; Metagenomes; Multiple instance learning; Sequential analysis; Clustering algorithms",2-s2.0-85030749336
"Sussman D.M., Schoenholz S.S., Cubuk E.D., Liu A.J.","Disconnecting structure and dynamics in glassy thin films",2017,"Proceedings of the National Academy of Sciences of the United States of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030253618&doi=10.1073%2fpnas.1703927114&partnerID=40&md5=3df50f4fa866792a2424c0f5f3551d3e","Nanometrically thin glassy films depart strikingly from the behavior of their bulk counterparts. We investigate whether the dynamical differences between a bulk and thin film polymeric glass former can be understood by differences in local microscopic structure. Machine learning methods have shown that local structure can serve as the foundation for successful, predictive models of particle rearrangement dynamics in bulk systems. By contrast, in thin glassy films, we find that particles at the center of the film and those near the surface are structurally indistinguishable despite exhibiting very different dynamics. Next, we show that structure-independent processes, already present in bulk systems and demonstrably different from simple facilitated dynamics, are crucial for understanding glassy dynamics in thin films. Our analysis suggests a picture of glassy dynamics in which two dynamical processes coexist, with relative strengths that depend on the distance from an interface. One of these processes depends on local structure and is unchanged throughout most of the film, while the other is purely Arrhenius, does not depend on local structure, and is strongly enhanced near the free surface of a film. © 2017, National Academy of Sciences. All rights reserved.","Glass; Machine learning; Thin film","polymer; algorithm; Article; film; machine learning; mathematical model; molecular dynamics; polymerization; priority journal; relaxation time; support vector machine; transition temperature",2-s2.0-85030253618
"Zhou J., Chen F.","DecisionMind: revealing human cognition states in data analytics-driven decision making with a multimodal interface",2017,"Journal on Multimodal User Interfaces",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030315348&doi=10.1007%2fs12193-017-0249-8&partnerID=40&md5=e6a85841fb2689fae90e40a5699e020a","Despite the recognized value of machine learning (ML) techniques and high expectation of applying ML techniques within various applications, significant barriers to widespread adoption and local implementation of ML approaches still exist in the areas of trust (of ML results), comprehension (of ML processes) and related workload, as well as confidence (in decision making based on ML results) by users. This paper argues that the revealing of human cognition states with a multimodal interface during ML-based data analytics-driven decision making could provide a rich view for both ML researchers and domain experts to learn the effectiveness of ML technologies in applications. On the one hand, human cognition states could help understand to what degree users accept innovative technologies. On the other hand, through understanding human cognition states during data analytics-driven decision making, ML-based decision attributes and even ML models can be adaptively refined in order to make ML transparent. The paper also identifies examples of impact challenges and obstacles, as well as high-demand research directions in making ML transparent. © 2017 Springer International Publishing AG","Decision making; DecisionMind; Human cognition states; Multimodal interface; Transparent machine learning","Artificial intelligence; Decision making; Interactive computer systems; Learning systems; Data analytics; Decision attribute; DecisionMind; Domain experts; High demand; Human cognition; Innovative technology; Multi-modal interfaces; Interface states",2-s2.0-85030315348
"Young J.D., Cai C., Lu X.","Unsupervised deep learning reveals prognostically relevant subtypes of glioblastoma",2017,"BMC Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030319132&doi=10.1186%2fs12859-017-1798-2&partnerID=40&md5=b25406d39ebcf2206bb7dc5a5d3a8b20","Background: One approach to improving the personalized treatment of cancer is to understand the cellular signaling transduction pathways that cause cancer at the level of the individual patient. In this study, we used unsupervised deep learning to learn the hierarchical structure within cancer gene expression data. Deep learning is a group of machine learning algorithms that use multiple layers of hidden units to capture hierarchically related, alternative representations of the input data. We hypothesize that this hierarchical structure learned by deep learning will be related to the cellular signaling system. Results: Robust deep learning model selection identified a network architecture that is biologically plausible. Our model selection results indicated that the 1st hidden layer of our deep learning model should contain about 1300 hidden units to most effectively capture the covariance structure of the input data. This agrees with the estimated number of human transcription factors, which is approximately 1400. This result lends support to our hypothesis that the 1st hidden layer of a deep learning model trained on gene expression data may represent signals related to transcription factor activation. Using the 3rd hidden layer representation of each tumor as learned by our unsupervised deep learning model, we performed consensus clustering on all tumor samples-leading to the discovery of clusters of glioblastoma multiforme with differential survival. One of these clusters contained all of the glioblastoma samples with G-CIMP, a known methylation phenotype driven by the IDH1 mutation and associated with favorable prognosis, suggesting that the hidden units in the 3rd hidden layer representations captured a methylation signal without explicitly using methylation data as input. We also found differentially expressed genes and well-known mutations (NF1, IDH1, EGFR) that were uniquely correlated with each of these clusters. Exploring these unique genes and mutations will allow us to further investigate the disease mechanisms underlying each of these clusters. Conclusions: In summary, we show that a deep learning model can be trained to represent biologically and clinically meaningful abstractions of cancer gene expression data. Understanding what additional relationships these hidden layer abstractions have with the cancer cellular signaling system could have a significant impact on the understanding and treatment of cancer. © 2017 The Author(s).","Cancer; Deep belief network; Deep learning; Gene expression; Glioblastoma multiforme; Model selection; Unsupervised learning","Abstracting; Alkylation; Cluster analysis; Diagnosis; Diseases; Gene expression; Genes; Input output programs; Learning algorithms; Learning systems; Methylation; Network architecture; Patient treatment; Transcription; Transcription factors; Tumors; Unsupervised learning; Cancer; Cancer gene expression; Deep belief networks; Differentially expressed gene; Glioblastoma multiforme; Hierarchical structures; Human transcription factors; Model Selection; Deep learning",2-s2.0-85030319132
"Gao M., Zöllner J.M.","Sparse Contextual Task Learning and Classification to Assist Mobile Robot Teleoperation with Introspective Estimation",2017,"Journal of Intelligent and Robotic Systems: Theory and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030322279&doi=10.1007%2fs10846-017-0681-8&partnerID=40&md5=7134d86751c57e1ac9db7d3ea7ab7c44","This report proposes a novel approach to learn from demonstrations and classify contextual tasks the human operator executes by remotely controlling a mobile robot with joystick, aiming to assist mobile robot teleoperation within a shared autonomy system in a task-appropriate manner. The proposed classifier is implemented with the Gaussian Process (GP). GP is superior in uncertainty estimation when predicting class labels (i.e. the introspective capability) over other state-of-art classification methods, such as Support Vector Machine (SVM), which is probably the most widely used approach on this topic to date. Moreover, to keep the learned model sparse to limit the amount of storage and computation required, full GP is approximated with a state-of-art Sparse Online Gaussian Process (SOGP) algorithm, to maintain scalability to large datasets without compromising classification performance. The proposed approach is extensively evaluated on real data and verified to outperform the baseline classifiers both in classification accuracy and uncertainty estimation in predicting class labels, while maintaining sparsity and real-time property to scale with large datasets. This demonstrates the feasibility of the proposed approach for online use in real applications. © 2017 Springer Science+Business Media B.V.","Assisted teleoperation; Learning from demonstration; Mobile robot; Shared autonomy; SOGP classifier","Digital storage; Gaussian distribution; Gaussian noise (electronic); Mobile robots; Remote control; Robots; Support vector machines; Classification accuracy; Classification methods; Classification performance; Learning from demonstration; Real-time properties; Remotely controlling; Shared autonomy; Uncertainty estimation; Classification (of information)",2-s2.0-85030322279
"Liberti M.V., Dai Z., Wardell S.E., Baccile J.A., Liu X., Gao X., Baldi R., Mehrmohamadi M., Johnson M.O., Madhukar N.S., Shestov A.A., Chio I.I.C., Elemento O., Rathmell J.C., Schroeder F.C., McDonnell D.P., Locasale J.W.","A Predictive Model for Selective Targeting of the Warburg Effect through GAPDH Inhibition with a Natural Product",2017,"Cell Metabolism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029429365&doi=10.1016%2fj.cmet.2017.08.017&partnerID=40&md5=429f93bfbac4bff04976e1c8f97c1333","Targeted cancer therapies that use genetics are successful, but principles for selectively targeting tumor metabolism that is also dependent on the environment remain unknown. We now show that differences in rate-controlling enzymes during the Warburg effect (WE), the most prominent hallmark of cancer cell metabolism, can be used to predict a response to targeting glucose metabolism. We establish a natural product, koningic acid (KA), to be a selective inhibitor of GAPDH, an enzyme we characterize to have differential control properties over metabolism during the WE. With machine learning and integrated pharmacogenomics and metabolomics, we demonstrate that KA efficacy is not determined by the status of individual genes, but by the quantitative extent of the WE, leading to a therapeutic window in vivo. Thus, the basis of targeting the WE can be encoded by molecular principles that extend beyond the status of individual genes. Liberti et al. use metabolic control analysis and multi-omics approaches to show that the enzyme GAPDH is rate limiting for the Warburg effect in cancer cells. They identify a therapeutic window where partial GAPDH inhibition is more selective for highly glycolytic tumors, highlighting how metabolism is an integral part of precision medicine. © 2017 Elsevier Inc.","cancer metabolism; glucose metabolism; metabolic control analysis; metabolic flux analysis; metabolomics; natural product; pharmacogenomics; precision medicine; systems biology; Warburg effect","glucose; glyceraldehyde 3 phosphate dehydrogenase; K ras protein; koningic acid; Myc protein; natural product; phosphatidylinositol 3 kinase; protein p53; Article; bioavailability; cancer cell; cell metabolism; cytotoxicity; disease marker; enzyme inhibition; glucose metabolism; glycolysis; human; IC50; in vivo study; kinetics; machine learning; metabolic regulation; metabolomics; model; pharmacogenomics; priority journal; protein expression; thermodynamics; treatment response; Warburg effect",2-s2.0-85029429365
"Hampton T.","Early brain imaging in infants may help predict autism",2017,"JAMA - Journal of the American Medical Association",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031330236&doi=10.1001%2fjama.2017.13706&partnerID=40&md5=b0a6f8a5c112c36334f42eb855ada696",[No abstract available],,"algorithm; asymptomatic disease; autism; brain size; diagnostic accuracy; disease association; disease predisposition; DNA determination; early diagnosis; functional connectivity; functional magnetic resonance imaging; genetic risk; high risk patient; human; machine learning; neuroanatomy; neuroimaging; nuclear magnetic resonance imaging; pathological anatomy; predictive value; priority journal; radiodiagnosis; sensitivity analysis; Short Survey; autism; brain; diagnostic imaging; infant; nuclear magnetic resonance imaging; procedures; Autism Spectrum Disorder; Brain; Humans; Infant; Machine Learning; Magnetic Resonance Imaging; Neuroimaging",2-s2.0-85031330236
"Liu X., Li Y., Shen Q.","Real-time action detection and temporal segmentation in continuous video",2017,"Imaging Science Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027514148&doi=10.1080%2f13682199.2017.1361664&partnerID=40&md5=4d48c32972464a41dc179fda0c550ade","Temporal segmentation of actions has been under intensive focus in the field of computer vision for a prolonged period. The present study proposed a template-based framework to resolve the issues concerning timeliness and real-time performance in the temporal segmentation in a continuous video. A complete action can be detected, based on the previous frames, and the action can be segmented immediately without waiting for the follow-up frames. Herein, characteristic frames are selected by a martingale-based method, followed by the formation of the corresponding motion history through backtracking along the characteristic frames, and the final segmentation is determined according to the recognition model trained by the extreme learning machine. In the experiment on the IXMAS database, the average rate of the detection of action reached 91%, and the accuracy in the frame level reached 83.5%. In the experiment on the 3D skeleton data based on Kinect, the detection rate reached 94%. © 2017 The Royal Photographic Society.","Action segmentation; characteristic frames; extreme learning machine; motion history image","Knowledge acquisition; Learning systems; Action segmentation; Characteristic frames; Detection rates; Extreme learning machine; Motion history images; Real time performance; Recognition models; Temporal segmentations; Image segmentation",2-s2.0-85027514148
"Rahimi A., Tchouprina A., Kanerva P., Millán J.D.R., Rabaey J.M.","Hyperdimensional Computing for Blind and One-Shot Classification of EEG Error-Related Potentials",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328005&doi=10.1007%2fs11036-017-0942-6&partnerID=40&md5=6578451a944b1fdb76780f85a0fca79c","The mathematical properties of high-dimensional (HD) spaces show remarkable agreement with behaviors controlled by the brain. Computing with HD vectors, referred to as “hypervectors,” is a brain-inspired alternative to computing with numbers. HD computing is characterized by generality, scalability, robustness, and fast learning, making it a prime candidate for utilization in application domains such as brain–computer interfaces. We describe the use of HD computing to classify electroencephalography (EEG) error-related potentials for noninvasive brain–computer interfaces. Our algorithm naturally encodes neural activity recorded from 64 EEG electrodes to a single temporal–spatial hypervector without requiring any electrode selection process. This hypervector represents the event of interest, can be analyzed to identify the most discriminative electrodes, and is used for recognition of the subject’s intentions. Using the full set of training trials, HD computing achieves on average 5% higher single-trial classification accuracy compared to a conventional machine learning method on this task (74.5% vs. 69.5%) and offers further advantages: (1) Our algorithm learns fast: using only 34% of training trials it achieves an average accuracy of 70.5%, surpassing the conventional method. (2) Conventional method requires prior domain expert knowledge, or a separate process, to carefully select a subset of electrodes for a subsequent preprocessor and classifier, whereas our algorithm blindly uses all 64 electrodes, tolerates noises in data, and the resulting hypervector is intrinsically clustered into HD space; in addition, most preprocessing of the electrode signal can be eliminated while maintaining an average accuracy of 71.7%. © 2017 Springer Science+Business Media, LLC","Classification; Electroencephalogram (EEG); Error-related potentials (ERP); Hyperdimensional computing","Brain; Classification (of information); Electrodes; Electrophysiology; Errors; Learning systems; Neurons; Conventional machines; Conventional methods; Domain expert knowledge; Electro-encephalogram (EEG); Error related potentials; Hyperdimensional computing; Mathematical properties; Single-trial classifications; Electroencephalography",2-s2.0-85030328005
"Jalali A., Farsi H., Ghaemmaghami S.","A Universal Image Steganalysis System Based On Double Sparse Representation Classification (DSRC)",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030314572&doi=10.1007%2fs11042-017-5201-0&partnerID=40&md5=005bc1983e618bf10d78e41e86ac0138","Achieving high rates of detection in low rates of embedding is still a challenging problem in many steganalysis systems. The newly proposed steganalysis system based on sparse representation classifier has shown remarkable detection rates in low embedding rate. In this paper, we propose a new steganalysis system based on double sparse representation classifier. We compare our proposed method with other steganalysis systems which use different classifier (including nearest neighbor, support vector machine, ensemble support vector machine and sparse representation). In all of our experiments, input features to the classifier are fixed and the ability of classifier is examined. Also we provide a complexity analysis in terms of execution time for different classifier. In most of experiments, our proposed method shows superior performance in terms of detection rate and complexity for low embedding rates. © 2017 Springer Science+Business Media, LLC","Dictionary learning; Double sparse representation; Steganalysis; Steganography","Image classification; Support vector machines; Complexity analysis; Detection rates; Dictionary learning; Embedding rates; Image steganalysis; Nearest neighbors; Sparse representation; Steganalysis; Steganography",2-s2.0-85030314572
"Ye Q., Zhao H., Li Z., Yang X., Gao S., Yin T., Ye N.","L1-Norm Distance Minimization-Based Fast Robust Twin Support Vector k-Plane Clustering",2017,"IEEE Transactions on Neural Networks and Learning Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030769454&doi=10.1109%2fTNNLS.2017.2749428&partnerID=40&md5=b270df5ffb9f50884706d70d043d3613","Twin support vector clustering (TWSVC) is a recently proposed powerful k-plane clustering method. It, however, is prone to outliers due to the utilization of squared L2-norm distance. Besides, TWSVC is computationally expensive, attributing to the need of solving a series of constrained quadratic programming problems (CQPPs) in learning each clustering plane. To address these problems, this brief first develops a new k-plane clustering method called L1-norm distance minimization-based robust TWSVC by using robust L1-norm distance. To achieve this objective, we propose a novel iterative algorithm. In each iteration of the algorithm, one CQPP is solved. To speed up the computation of TWSVC and simultaneously inherit the merit of robustness, we further propose Fast RTWSVC and design an effective iterative algorithm to optimize it. Only a system of linear equations needs to be computed in each iteration. These characteristics make our methods more powerful and efficient than TWSVC. We also conduct some insightful analysis on the existence of local minimum and the convergence of the proposed algorithms. Theoretical insights and effectiveness of our methods are further supported by promising experimental results. IEEE","Algorithm design and analysis; Clustering methods; Electronic mail; Iterative algorithm; Iterative methods; k-plane clustering; L1-norm distance; Learning systems; linear equations; Robustness; Support vector machines; twin support vector clustering (TWSVC).","Cluster analysis; Clustering algorithms; Electronic mail; Learning systems; Linear equations; Quadratic programming; Robustness (control systems); Support vector machines; Vectors; Algorithm design and analysis; Clustering methods; Iterative algorithm; K-plane clustering; L1 norm; Support vector clustering; Iterative methods",2-s2.0-85030769454
"Glorieux E., Svensson B., Danielsson F., Lennartson B.","Multi-objective constructive cooperative coevolutionary optimization of robotic press-line tending",2017,"Engineering Optimization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006124128&doi=10.1080%2f0305215X.2016.1264220&partnerID=40&md5=9426711119e68d48397c39c84cd6f041","This article investigates multi-objective optimization of the robot trajectories and position-based operation-coordination of complex multi-robot systems, such as press lines, to improve the production rate and obtaining smooth motions to avoid excessive wear of the robots' components. Different functions for handling the multiple objectives are evaluated on real-world press lines, including both scalarizing single-objective functions and Pareto-based multi-objective functions. Additionally, the Multi-Objective Constructive Cooperative Coevolutionary (moC3) algorithm is proposed, for Pareto-based optimization, which uses a novel constructive initialization of the subpopulations in a co-adaptive fashion. It was found that Pareto-based optimization performs better than the scalarizing single-objective functions. Furthermore, (moC3) gives substantially better results compared to manual online tuning, as currently used in the industry. Optimizing robot trajectories and operation-coordination of complex multi-robot systems using the proposed method with (moC3) significantly improves productivity and reduces maintenance. This article hereby addresses the lack of systematic methods for effectively improving the productivity of press lines. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","coevolutionary optimization; multi-objective optimization; multi-robot coordination; press tending","Coordination reactions; Industrial robots; Multipurpose robots; Presses (machine tools); Printing; Productivity; Robot learning; Robots; Co-evolutionary; Cooperative-coevolutionary optimization; Multi-objective functions; Multi-robot coordination; Multi-robot systems; Multiple-objectives; Single objective; Systematic method; Multiobjective optimization",2-s2.0-85006124128
"Teo J., Hou C.L., Mountstephens J.","Deep learning for EEG-Based preference classification",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031285544&doi=10.1063%2f1.5005474&partnerID=40&md5=a20b26479601820f9bb3bdfc9a871e44","Electroencephalogram (EEG)-based emotion classification is rapidly becoming one of the most intensely studied areas of brain-computer interfacing (BCI). The ability to passively identify yet accurately correlate brainwaves with our immediate emotions opens up truly meaningful and previously unattainable human-computer interactions such as in forensic neuroscience, rehabilitative medicine, affective entertainment and neuro-marketing. One particularly useful yet rarely explored areas of EEG-based emotion classification is preference recognition [1], which is simply the detection of like versus dislike. Within the limited investigations into preference classification, all reported studies were based on musically-induced stimuli except for a single study which used 2D images. The main objective of this study is to apply deep learning, which has been shown to produce state-of-the-art results in diverse hard problems such as in computer vision, natural language processing and audio recognition, to 3D object preference classification over a larger group of test subjects. A cohort of 16 users was shown 60 bracelet-like objects as rotating visual stimuli on a computer display while their preferences and EEGs were recorded. After training a variety of machine learning approaches which included deep neural networks, we then attempted to classify the users' preferences for the 3D visual stimuli based on their EEGs. Here, we show that that deep learning outperforms a variety of other machine learning classifiers for this EEG-based preference classification task particularly in a highly challenging dataset with large inter- and intra-subject variability. © 2017 Author(s).",,,2-s2.0-85031285544
"Uddin M.A., Joolee J.B., Alam A., Lee Y.","Human Action Recognition using Adaptive Local Motion Descriptor in Spark",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030780957&doi=10.1109%2fACCESS.2017.2759225&partnerID=40&md5=92402bd368e2097ba4b73aae84518cd2","Human action recognition plays a significant part in the computer vision and multimedia research society due to its numerous applications. However, despite different approaches proposed to address this problem, some issues regarding the robustness and efficiency of the action recognition still need to be solved. Moreover, due to the speedy development of multimedia applications from numerous origins, e.g. CCTV or video surveillance, there is an increasing demand for parallel processing of the large-scale video data. In this paper, we introduce a novel approach to recognize the human actions. Firstly, we explore Apache Spark with in-memory computing, to resolve the task of human action recognition in the distributed environment. Secondly, we introduce a novel feature descriptor, namely Adaptive Local Motion Descriptor (ALMD) by considering motion and appearance, which is an extension of Local Ternary Pattern used for static texture analysis, and ALMD also generate persistent codes to describe the localtextures. Finally, the Spark MLlib (Machine Learning Library) Random Forest is employed to recognize the human actions. Experimental results show the superiority of the proposed approach over other state-of-the-arts. OAPA","Adaptive Local Motion Descriptor; Data mining; Distributed databases; Feature extraction; Human action recognition; Libraries; Multimedia communication; Random Forest; Spark; Spark MLlib; Sparks; Streaming media","Data mining; Decision trees; Electric sparks; Feature extraction; Learning systems; Libraries; Media streaming; Motion analysis; Multimedia systems; Security systems; Distributed database; Human-action recognition; Local motions; Multi-media communications; Random forests; Streaming media; Image recognition",2-s2.0-85030780957
"Yu Z., Xiong W., Eeckhout L., Bei Z., Avi M., Xu C.","MIA: Metric Importance Analysis for Big Data Workload Characterization",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782120&doi=10.1109%2fTPDS.2017.2758781&partnerID=40&md5=364a156d39ec619b22a651954931becb","Data analytics is at the foundation of both high-quality products and services in modern economies and societies. Big data workloads run on complex large-scale computing clusters, which implies significant challenges for deeply understanding and characterizing overall system performance. In general, performance is affected by many factors at multiple layers in the system stack, hence it is challenging to identify the key metrics when characterizing and understanding big data workload performance. In this paper, we propose a novel workload characterization methodology using ensemble learning, called Metric Importance Analysis (MIA), to quantify the respective importance of workload metrics. By focusing on the most important metrics, MIA reduces the complexity of the analysis without losing information. Moreover, we develop the MIA-based Kiviat Plot (MKP) and Benchmark Similarity Matrix (BSM) which provide more insightful information than the traditional linkage clustering based dendrogram to visualize program behavior (dis)similarity. To demonstrate the applicability of MIA, we use it to characterize three big data benchmark suites: HiBench, CloudRank-D and SZTS. The results show that MIA is able to characterize complex big data workloads in a simple, intuitive manner, and reveal interesting insights. Moreover, through a case study, we demonstrate that tuning the configuration parameters related to the important metrics found by MIA results in higher performance improvements than through tuning the parameters related to the less important ones. IEEE","Benchmark testing; Benchmarking; Big Data; Big Data; Hardware; MapReduce/Hadoop; Measurement; Performance Measurement; Software; Support vector machines; Workload Characterization","Benchmarking; Characterization; Computer hardware; Computer software; Measurements; Software testing; Support vector machines; Benchmark testing; Configuration parameters; High-quality products; Importance analysis; Large-scale computing; Map-reduce; Performance measurements; Workload characterization; Big data",2-s2.0-85030782120
"Arlitsch K., Newell B.","Thriving in the Age of Accelerations: A Brief Look at the Societal Effects of Artificial Intelligence and the Opportunities for Libraries",2017,"Journal of Library Administration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031803908&doi=10.1080%2f01930826.2017.1362912&partnerID=40&md5=40cab9aae505524f6056e55ec45b2b25","Fifty years of the compounding effects of Moore's Law have led to enormous advances in computer processing power. Increased network speeds, the availability of big data, and machine learning techniques have accelerated the development of artificial intelligence; this promises to dramatically change many industries, including libraries. This article offers some thoughts on the effects of automation on employment, the social and political fallout, and the threats and opportunities for academic and public libraries. © 2017, Published with license by Taylor & Francis © 2017, © Kenning Arlitsch and Bruce Newell.","academic libraries; algorithms; artificial intelligence; employment; professional development; public libraries; robots",,2-s2.0-85031803908
"Eembijamil N.C., Ishak I., Sidi F.","Deception detection approach for data veracity in online digital news: Headlines vs contents",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031299922&doi=10.1063%2f1.5005369&partnerID=40&md5=3f770434ff11c9515bda5bbfce9c6b71","Veracity is a way to find the truthfulness, availability, accountability and authenticity while deception refers to the way of identifying whether verbal expressions or the overall content is truthful or not. Among the issue in data veracity is the use of deception element in digital news content. Many research have been conducted to address the issue of deception especially in news content they proposed machine learning-based approaches to detect deception in news content. In this paper we compare available deception detection model to improve deception detection accuracy for online digital news veracity. We also proposed a framework to improve deception detection accuracy over digital news portal focusing on headlines. Furthermore, this paper also discussed potential directions for future research in deception of online news. © 2017 Author(s).",,,2-s2.0-85031299922
"Zin H.M., Mustapha N., Murad M.A.A., Sharef N.M.","The effects of pre-processing strategies in sentiment analysis of online movie reviews",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031295861&doi=10.1063%2f1.5005422&partnerID=40&md5=e2fb3240b6acc3e882051262822afee6","With the ever increasing of internet applications and social networking sites, people nowadays can easily express their feelings towards any products and services. These online reviews act as an important source for further analysis and improved decision making. These reviews are mostly unstructured by nature and thus, need processing like sentiment analysis and classification to provide a meaningful information for future uses. In text analysis tasks, the appropriate selection of words/features will have a huge impact on the effectiveness of the classifier. Thus, this paper explores the effect of the pre-processing strategies in the sentiment analysis of online movie reviews. In this paper, supervised machine learning method was used to classify the reviews. The support vector machine (SVM) with linear and non-linear kernel has been considered as classifier for the classification of the reviews. The performance of the classifier is critically examined based on the results of precision, recall, f-measure, and accuracy. Two different features representations were used which are term frequency and term frequency-inverse document frequency. Results show that the pre-processing strategies give a significant impact on the classification process. © 2017 Author(s).",,,2-s2.0-85031295861
"Rahman N.A.A., Tan K.L., Lim C.K.","Predictive analysis and data mining among the employment of fresh graduate students in HEI",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031286586&doi=10.1063%2f1.5005340&partnerID=40&md5=530487858b9e11176883d4a277666263","Management of higher education have a problem in producing 100% of graduates who can meet the needs of industry while industry is also facing the problem of finding skilled graduates who suit their needs partly due to the lack of an effective method in assessing problem solving skills as well as weaknesses in the assessment of problem-solving skills. The purpose of this paper is to propose a suitable classification model that can be used in making prediction and assessment of the attributes of the student's dataset to meet the selection criteria of work demanded by the industry of the graduates in the academic field. Supervised and unsupervised Machine Learning Algorithms were used in this research where; K-Nearest Neighbor, Naïve Bayes, Decision Tree, Neural Network, Logistic Regression and Support Vector Machine. The proposed model will help the university management to make a better long-term plans for producing graduates who are skilled, knowledgeable and fulfill the industry needs as well. © 2017 Author(s).",,,2-s2.0-85031286586
"Ibrahim N., Akhir N.S.M., Hassan F.H.","Predictive analysis effectiveness in determining the epidemic disease infected area",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031311147&doi=10.1063%2f1.5005397&partnerID=40&md5=89e2bc40aa3b0ce712f95cff3071eea2","Epidemic disease outbreak had caused nowadays community to raise their great concern over the infectious disease controlling, preventing and handling methods to diminish the disease dissemination percentage and infected area. Backpropagation method was used for the counter measure and prediction analysis of the epidemic disease. The predictive analysis based on the backpropagation method can be determine via machine learning process that promotes the artificial intelligent in pattern recognition, statistics and features selection. This computational learning process will be integrated with data mining by measuring the score output as the classifier to the given set of input features through classification technique. The classification technique is the features selection of the disease dissemination factors that likely have strong interconnection between each other in causing infectious disease outbreaks. The predictive analysis of epidemic disease in determining the infected area was introduced in this preliminary study by using the backpropagation method in observation of other's findings. This study will classify the epidemic disease dissemination factors as the features for weight adjustment on the prediction of epidemic disease outbreaks. Through this preliminary study, the predictive analysis is proven to be effective method in determining the epidemic disease infected area by minimizing the error value through the features classification. © 2017 Author(s).",,,2-s2.0-85031311147
"Manaf S.A., Mustapha N., Sulaiman M.N., Husin N.A., Shafri H.Z.M.","Validation assessment of shoreline extraction on medium resolution satellite image",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031295370&doi=10.1063%2f1.5005334&partnerID=40&md5=a2612af656f35dea7a6b0bd83c76f087","Monitoring coastal zones helps provide information about the conditions of the coastal zones, such as erosion or accretion. Moreover, monitoring the shorelines can help measure the severity of such conditions. Such measurement can be performed accurately by using Earth observation satellite images rather than by using traditional ground survey. To date, shorelines can be extracted from satellite images with a high degree of accuracy by using satellite image classification techniques based on machine learning to identify the land and water classes of the shorelines. In this study, the researchers validated the results of extracted shorelines of 11 classifiers using a reference shoreline provided by the local authority. Specifically, the validation assessment was performed to examine the difference between the extracted shorelines and the reference shorelines. The research findings showed that the SVM Linear was the most effective image classification technique, as evidenced from the lowest mean distance between the extracted shoreline and the reference shoreline. Furthermore, the findings showed that the accuracy of the extracted shoreline was not directly proportional to the accuracy of the image classification. © 2017 Author(s).",,,2-s2.0-85031295370
"Pohjankukka J., Pahikkala T., Nevalainen P., Heikkonen J.","Estimating the prediction performance of spatial models via spatial k-fold cross validation",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022063285&doi=10.1080%2f13658816.2017.1346255&partnerID=40&md5=c26f5985728bb9e4697d3edd5109c19f","In machine learning, one often assumes the data are independent when evaluating model performance. However, this rarely holds in practice. Geographic information datasets are an example where the data points have stronger dependencies among each other the closer they are geographically. This phenomenon known as spatial autocorrelation (SAC) causes the standard cross validation (CV) methods to produce optimistically biased prediction performance estimates for spatial models, which can result in increased costs and accidents in practical applications. To overcome this problem, we propose a modified version of the CV method called spatial k-fold cross validation (SKCV), which provides a useful estimate for model prediction performance without optimistic bias due to SAC. We test SKCV with three real-world cases involving open natural data showing that the estimates produced by the ordinary CV are up to 40% more optimistic than those of SKCV. Both regression and classification cases are considered in our experiments. In addition, we will show how the SKCV method can be applied as a criterion for selecting data sampling density for new research area. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","geographic information science; geographic information systems; spatial data mining; Spatio-temporal data modelling","data mining; GIS; modeling; spatial data",2-s2.0-85022063285
"Shultz R.H., Jr.","U.S. counterterrorism operations during the Iraq war: A case study of task force 714",2017,"Studies in Conflict and Terrorism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003977159&doi=10.1080%2f1057610X.2016.1239990&partnerID=40&md5=3eacb51a59823e73e06529406c758c97","U.S. counterterrorism (CT) forces that deployed to Iraq in 2003 as Task Force 714 (TF 714) faced an ugly surprise. Tasked to dismantle the al-Qaeda in Iraq (AQI) dominated insurgency, the organization could not achieve that mission. General Stanley McChrystal, who commanded TF 714 concluded, “we were losing to an enemy … we should have dominated.” But TF 714 transformed in the midst of war and during 2006-2009 was able to largely dismantle AQI’s clandestine networks to a degree that they could no longer function in a cohesive manner. By developing the capacity to operate inside those networks, TF 714 was able, in the words of General McChrystal, to “claw the guts out of AQI.” This transformation runs counter to what organizational experts identify as barriers inhibiting militaries from learning, innovating, and changing, especially in wartime. To decipher the puzzle of how TF 714 overcame these barriers, two questions are addressed in this study: 1) How did TF 714 transform from a specialized and compartmented unit customized for executing infrequent CT missions in peacetime to a wartime industrial-strength CT machine that by 2009 dismantled AQI’s networks that operated across Iraq; and 2) Why was TF 714 able to achieve this remarkable transformation?. © 2017 Taylor & Francis Group, LLC.",,,2-s2.0-85003977159
"Miao Y.Z., Ma X.P., Bu S.P.","Research on the Learning Method Based on PCA-ELM",2017,"Intelligent Automation and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023171189&doi=10.1080%2f10798587.2017.1316071&partnerID=40&md5=61cf24737cc855f449c454aca8075eaf","The Single-hidden Layer Feed-forward Neural Network has been widely applied in the fields such as pattern recognition, automatic control and data mining. However, the speed of the traditional learning method, since it is far from enough to satisfy the actual demand has become the main bottleneck, which restricts its development. As one of the new learning methods, the extreme learning machine (ELM) has its own remarkable characteristics, but the fact that ELM is based on the Empirical Risk Minimization may lead to over fitting. In addition, ELM does not consider the weight of error, so its performance will be severely affected when there are outliers in data integration. To solve the above problems, this paper referred to the two algorithms including PCA (Principal Component Analysis) and ELM, and put forward a learning method and prediction model, which combined PCA and ELM. From the results of simulation analysis, as combining advantages of PCA and ELM algorithms, the network structure can be simplified to improve the learning ability and its prediction precision. © 2017 TSI® Press.","Extreme learning machine (ELM); Feed-forward neural network; Principal component analysis (PCA)",,2-s2.0-85023171189
"Wei Z., Feng Y., Hong Z., Qu R., Tan J.","Product quality improvement method in manufacturing process based on kernel optimisation algorithm",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018256267&doi=10.1080%2f00207543.2017.1324223&partnerID=40&md5=678b9e2fe5e1ab1aab654530acbdf2a3","Quality data in manufacture process has the features of mixed type, uneven distribution, dimension curse and data coupling. To apply the massive manufacturing quality data effectively to the quality analysis of the manufacture enterprise, the data pre-processing algorithm based on equivalence relation is employed to select the characteristic of hybrid data and preprocess data. KML-SVM (Optimised kernel-based hybrid manifold learning and support vector machines algorithm) is proposed. KML is adopted to solve the problems of manufacturing process quality data dimension curse. SVM is adopted to classify and predict low-dimensional embedded data, as well as to optimise support vector machine kernel function so that the classification accuracy can be maximised. The actual manufacturing process data of AVIC Shenyang Liming Aero-Engine Group Corporation Ltd is demonstrated to simulate and verify the proposed algorithm. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","hybrid manifold learning; kernel function; manufacturing process quality; optimisation algorithm; support vector machines","Aircraft engines; Automobile engine manifolds; Couplings; Data handling; Learning algorithms; Manufacture; Optimization; Processing; Support vector machines; Classification accuracy; Kernel function; Manifold learning; Manufacture enterprise; Manufacturing process; Manufacturing quality; Optimisations; Support vector machines algorithms; Manufacturing data processing",2-s2.0-85018256267
"Perna G., Grassi M., Caldirola D., Nemeroff C.B.","The revolution of personalized psychiatry: will technology make it happen sooner?",2017,"Psychological Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030862300&doi=10.1017%2fS0033291717002859&partnerID=40&md5=51931df01c3aaf9c10e44a0196c06d55","Personalized medicine (PM) aims to establish a new approach in clinical decision-making, based upon a patient's individual profile in order to tailor treatment to each patient's characteristics. Although this has become a focus of the discussion also in the psychiatric field, with evidence of its high potential coming from several proof-of-concept studies, nearly no tools have been developed by now that are ready to be applied in clinical practice. In this paper, we discuss recent technological advances that can make a shift toward a clinical application of the PM paradigm. We focus specifically on those technologies that allow both the collection of massive as much as real-time data, i.e., electronic medical records and smart wearable devices, and to achieve relevant predictions using these data, i.e. the application of machine learning techniques. Copyright © Cambridge University Press 2017","Big data; electronic medical records; machine learning; personalized medicine; precision medicine; wearable devices",,2-s2.0-85030862300
"Montoye A.H.K., Conger S.A., Connolly C.P., Imboden M.T., Nelson M.B., Bock J.M., Kaminsky L.A.","Validation of Accelerometer-Based Energy Expenditure Prediction Models in Structured and Simulated Free-Living Settings",2017,"Measurement in Physical Education and Exercise Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021120411&doi=10.1080%2f1091367X.2017.1337638&partnerID=40&md5=71195bb51223a79a17d62efe65e18560","This study compared accuracy of energy expenditure (EE) prediction models from accelerometer data collected in structured and simulated free-living settings. Twenty-four adults (mean age 45.8 years, 50% female) performed two sessions of 11 to 21 activities, wearing four ActiGraph GT9X Link activity monitors (right hip, ankle, both wrists) and a metabolic analyzer (EE criterion). Visit 1 (V1) involved structured, 5-min activities dictated by researchers; Visit 2 (V2) allowed participants activity choice and duration (simulated free-living). EE prediction models were developed incorporating data from one setting (V1/V2; V2/V2) or both settings (V1V2/V2). The V1V2/V2 method had the lowest root mean square error (RMSE) for EE prediction (1.04–1.23 vs. 1.10–1.34 METs for V1/V2, V2/V2), and the ankle-worn accelerometer had the lowest RMSE of all accelerometers (1.04–1.18 vs. 1.17–1.34 METs for other placements). The ankle-worn accelerometer and associated EE prediction models developed using data from both structured and simulated free-living settings should be considered for optimal EE prediction accuracy. © 2017 Taylor & Francis.","ActiGraph; artificial neural network; machine learning; physical activity; validity","adult; aged; ankle; Article; energy expenditure; female; hip; human; human experiment; male; measurement accuracy; normal human; physical activity; prediction; sedentary lifestyle; simulation; validation study; wrist",2-s2.0-85021120411
"Coolen-Maturi T.","Three-group ROC predictive analysis for ordinal outcomes",2017,"Communications in Statistics - Theory and Methods",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020285913&doi=10.1080%2f03610926.2016.1212074&partnerID=40&md5=5a7eea2fc607ce66e7f8034038eab6a0","Measuring the accuracy of diagnostic tests is crucial in many application areas including medicine, machine learning, and credit scoring. The receiver operating characteristic (ROC) surface is a useful tool to assess the ability of a diagnostic test to discriminate among three-ordered classes or groups. In this article, nonparametric predictive inference (NPI) for three-group ROC analysis for ordinal outcomes is presented. NPI is a frequentist statistical method that is explicitly aimed at using few modeling assumptions, enabled through the use of lower and upper probabilities to quantify uncertainty. This article also includes results on the volumes under the ROC surfaces and consideration of the choice of decision thresholds for the diagnosis. Two examples are provided to illustrate our method. © 2017 Taylor & Francis Group, LLC.","Accuracy of diagnostic tests; Lower and upper probability; Nonparametric predictive inference; Ordinal data; ROC surface","Diagnosis; Learning algorithms; Learning systems; Accuracy of diagnostic tests; Application area; Decision threshold; Lower and upper probabilities; Model assumptions; Nonparametric predictive inference; Ordinal data; Receiver operating characteristics; Uncertainty analysis",2-s2.0-85020285913
"Křen T., Pilát M., Neruda R.","Automatic Creation of Machine Learning Workflows with Strongly Typed Genetic Programming",2017,"International Journal on Artificial Intelligence Tools",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032862100&doi=10.1142%2fS021821301760020X&partnerID=40&md5=7bc6806ae29b99f378917872d41a09b2","Manual creation of machine learning ensembles is a hard and tedious task which requires an expert and a lot of time. In this work we describe a new version of the GP-ML algorithm which uses genetic programming to create machine learning workows (combinations of preprocessing, classification, and ensembles) automatically, using strongly typed genetic programming and asynchronous evolution. The current version improves the way in which the individuals in the genetic programming are created and allows for much larger workows. Additionally, we added new machine learning methods. The algorithm is compared to the grid search of the base methods and to its previous versions on a set of problems from the UCI machine learning repository. © 2017 World Scientific Publishing Company.","asynchronous evolutionary algorithm; Genetic programming; machine learning workows","Artificial intelligence; Evolutionary algorithms; Genetic algorithms; Learning algorithms; Learning systems; Automatic creations; Grid search; Machine learning methods; Ml algorithms; Strongly-typed genetic programming; UCI machine learning repository; Work-flows; Genetic programming",2-s2.0-85032862100
"Zhang J., Tu H., Ren Y., Wan J., Zhou L., Li M., Wang J., Yu L., Zhao C., Zhang L.","A parameter communication optimization strategy for distributed machine learning in sensors",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030116098&doi=10.3390%2fs17102172&partnerID=40&md5=714a77253db09ec89153777f6ff8d844","In order to utilize the distributed characteristic of sensors, distributed machine learning has become the mainstream approach, but the different computing capability of sensors and network delays greatly influence the accuracy and the convergence rate of the machine learning model. Our paper describes a reasonable parameter communication optimization strategy to balance the training overhead and the communication overhead. We extend the fault tolerance of iterative-convergent machine learning algorithms and propose the Dynamic Finite Fault Tolerance (DFFT). Based on the DFFT, we implement a parameter communication optimization strategy for distributed machine learning, named Dynamic Synchronous Parallel Strategy (DSP), which uses the performance monitoring model to dynamically adjust the parameter synchronization strategy between worker nodes and the Parameter Server (PS). This strategy makes full use of the computing power of each sensor, ensures the accuracy of the machine learning model, and avoids the situation that the model training is disturbed by any tasks unrelated to the sensors. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Disturbed machine learning; Dynamic synchronous parallel strategy (DSP); Parameter server (PS); Sensors","Artificial intelligence; Distributed computer systems; Fault tolerance; Iterative methods; Learning systems; Sensors; Communication optimization; Communication overheads; Distributed characteristics; Distributed machine learning; Machine learning models; Parallel strategies; Parameter server (PS); Synchronization strategies; Learning algorithms",2-s2.0-85030116098
"Liang L., Liu M., Martin C., Elefteriades J.A., Sun W.","A machine learning approach to investigate the relationship between shape features and numerically predicted risk of ascending aortic aneurysm",2017,"Biomechanics and Modeling in Mechanobiology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017118283&doi=10.1007%2fs10237-017-0903-9&partnerID=40&md5=82fd7de594e93b001a84a8e9678afd5b","Geometric features of the aorta are linked to patient risk of rupture in the clinical decision to electively repair an ascending aortic aneurysm (AsAA). Previous approaches have focused on relationship between intuitive geometric features (e.g., diameter and curvature) and wall stress. This work investigates the feasibility of a machine learning approach to establish the linkages between shape features and FEA-predicted AsAA rupture risk, and it may serve as a faster surrogate for FEA associated with long simulation time and numerical convergence issues. This method consists of four main steps: (1) constructing a statistical shape model (SSM) from clinical 3D CT images of AsAA patients; (2) generating a dataset of representative aneurysm shapes and obtaining FEA-predicted risk scores defined as systolic pressure divided by rupture pressure (rupture is determined by a threshold criterion); (3) establishing relationship between shape features and risk by using classifiers and regressors; and (4) evaluating such relationship in cross-validation. The results show that SSM parameters can be used as strong shape features to make predictions of risk scores consistent with FEA, which lead to an average risk classification accuracy of 95.58% by using support vector machine and an average regression error of 0.0332 by using support vector regression, while intuitive geometric features have relatively weak performance. Compared to FEA, this machine learning approach is magnitudes faster. In our future studies, material properties and inhomogeneous thickness will be incorporated into the models and learning algorithms, which may lead to a practical system for clinical applications. © 2017, Springer-Verlag Berlin Heidelberg.","Ascending aortic aneurysm; Computer-aided diagnosis; Finite element analysis; Machine learning","Artificial intelligence; Blood vessels; Classification (of information); Computer aided analysis; Computer aided diagnosis; Computer aided instruction; Computerized tomography; Diagnosis; Geometry; Learning algorithms; Learning systems; Aortic aneurysms; Clinical application; Geometric feature; Machine learning approaches; Numerical convergence; Statistical shape model; Support vector regression (SVR); Threshold criterion; Finite element method; aortic aneurysm; Article; ascending aorta; finite element analysis; machine learning; model; principal component analysis; priority journal; risk assessment; statistical analysis; support vector machine; systolic blood pressure",2-s2.0-85017118283
"Wang J., Yang D., Jiang W., Zhou J.","Semisupervised Incremental Support Vector Machine Learning Based on Neighborhood Kernel Estimation",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029946984&doi=10.1109%2fTSMC.2017.2667703&partnerID=40&md5=5d119a7831c3389e3f1a56fd6d20485a","Semisupervised scheme has emerged as a popular strategy in the machine learning community due to the expensiveness of getting enough labeled data. In this paper, a semisupervised incremental support vector machine (SE-INC-SVM) algorithm based on neighborhood kernel estimation is proposed. First, kernel regression is constructed to estimate the unlabeled data from the labeled neighbors and its estimation accuracy is discussed from the analogy with tradition RBF neural network. The incremental scheme is derived to improve the learning efficiency and reduce the computing time. Simulations for manual data set and industrial benchmark-penicillin fermentation process demonstrate the effectiveness of the proposed SE-INC-SVM method. © 2013 IEEE.","Incremental training; neighborhood kernel estimation (KE); semisupervised scheme; support vector machine (SVM)","Artificial intelligence; Learning systems; Incremental support vector machine; Incremental training; Kernel estimation; Learning efficiency; Machine learning communities; Penicillin fermentation process; RBF Neural Network; Semi-supervised; Support vector machines",2-s2.0-85029946984
"Jiang Y., Gou Y., Zhang T., Wang K., Hu C.","A machine learning approach to argo data analysis in a thermocline",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030534742&doi=10.3390%2fs17102225&partnerID=40&md5=da979ac31674c4eab339b5b8722e4ff8","With the rapid development of sensor networks, big marine data arises. To efficiently use these data to predict thermoclines, we propose a machine learning approach. We firstly focus on analyzing how temperature, salinity, and geographic location features affect the formation of thermocline. Then, an improved model based on entropy value method for the thermocline selection is demonstrated. The experiments adopt BOA Argo data sets and the experimental results show that our novel model can predict thermoclines and related data effectively. © 2017 by the authors.","Entropy value calculation; Machine learning; Statistical learning; Thermocline","Artificial intelligence; Entropy; Sensor networks; Stream flow; Temperature distribution; Entropy value; Entropy value methods; Geographic location; Machine learning approaches; Statistical learning; Learning systems",2-s2.0-85030534742
"Büsch S., Nissen V., Wünscher A.","Automatic classification of data-warehouse-data for information lifecycle management using machine learning techniques",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979652117&doi=10.1007%2fs10796-016-9680-8&partnerID=40&md5=bbaf4bf392cd24cb361c0eea9520e037","The aim of Information Lifecycle Management (ILM) is to govern data throughout its lifecycle as efficiently as possible and effectively from technical points of view. A core aspect is the question, where the data should be stored, since different costs and access times are entailed. For this purpose data have to be classified, which presently is either done manually in an elaborate way, or with recourse to only a few data attributes, in particular access frequency. In the context of Data-Warehouse-Systems this article introduces an automated and therefore speedy and cost-effective data classification for ILM. Machine learning techniques, in particular an artificial neural network (multilayer perceptron), a support vector machine and a decision tree approach are compared on an SAP-based real-world data set from the automotive industry. This data classification considers a large number of data attributes and thus attains similar results akin to human experts. In this comparison of machine learning techniques, besides the accuracy of classification, also the types of misclassification that appear, are included, since this is important in ILM. © 2016, Springer Science+Business Media New York.","Artificial neural net; Automatic classification; Business intelligence; Computational intelligence; Data warehouse; Information lifecycle management; Machine learning; Multilayer perceptron","Artificial intelligence; Automotive industry; Competitive intelligence; Cost effectiveness; Data warehouses; Decision trees; Learning algorithms; Learning systems; Life cycle; Multilayer neural networks; Multilayers; Neural networks; Trees (mathematics); Virtual reality; Accuracy of classifications; Artificial neural net; Automatic classification; Data classification; Data warehouse systems; Information life cycle management; Machine learning techniques; Misclassifications; Classification (of information)",2-s2.0-84979652117
"Fischell E.M., Schmidt H.","Supervised Machine Learning for Estimation of Target Aspect Angle from Bistatic Acoustic Scattering",2017,"IEEE Journal of Oceanic Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012993350&doi=10.1109%2fJOE.2017.2650759&partnerID=40&md5=a22cf310ec15b6d9f2925e0ef6aecce3","When an aspect-dependent target is insonified by an acoustic source, distinct features are produced in the resulting bistatic scattered field. These features change as the aspect between the source and the target is varied. This paper describes the use of these features for estimation of the target aspect angle using data collected by an autonomous underwater vehicle (AUV). An experiment was conducted in November 2014 in Massachusetts Bay to collect data using a ship-based acoustic source producing 7-9-kHz linear frequency modulation (LFM) chirps insonifying a steel pipe. The true target orientation was unknown, as the target was dropped from the ship with no rotation control. The AUV Unicorn, fitted with a 16-element nose array, was deployed in data collection behaviors around the target, and the ship was moved to create two target aspects. A support vector machine regression model was trained using simulated scattering bistatic field data. This model was then used to estimate the target aspect angle from the data collected during the experiment. The difference between the estimates was consistent with experimental observations of relative source positioning. The simulation-based model appeared successful in estimating the target aspect angle despite uncertainties in target and source location and mismatch between true environment and simulation parameters. © 2017 IEEE.","Machine learning; swimming robots; underwater acoustics","Acoustic noise measurement; Acoustics; Autonomous underwater vehicles; Chirp modulation; Data acquisition; Frequency modulation; Learning systems; Regression analysis; Ships; Supervised learning; Uncertainty analysis; Acoustic Scattering; Autonomous underwater vehicles (AUV); Linear frequency modulation; Simulated scattering; Simulation parameters; Simulation-based modeling; Supervised machine learning; Support vector machine regressions; Acoustic wave scattering",2-s2.0-85012993350
"Kakihata E.M., Sapia H.M., Oiakawa R.T., Pereira D.R., Papa J.P., De Albuquerque V.H.C., Da Silva F.A.","Intrusion Detection System Based on Flows Using Machine Learning Algorithms",2017,"IEEE Latin America Transactions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032616666&doi=10.1109%2fTLA.2017.8071245&partnerID=40&md5=737bdd9e46aa1f43719840827f3b4b28","The use of technology information and communication by different types of devices generates a large quantity of data packets that contains of confidential and personal information. The traffic of data packet can be summarized in network flow. Due this reason, it is necessary to use computer security tools, such as Intrusion Detection Systems (IDS). This work presents an IDS that can perform the flow- based analysis (netflow). This research conducted an analysis on flows previously collected and properly detected of three different types of attacks. The flows were organized to be processed by machine learning methods. The results obtained by proposed approach were very promising. Also, this work aimed at building a public dataset to be used by researchers worldwide in order to foster IDS-related research. © 2003-2012 IEEE.","Bayes Classifier; Intrusion Detection System; KNN; Machine Learning; Netflow; OPF; SVM","Artificial intelligence; Computer crime; Learning algorithms; Learning systems; Mercury (metal); Network security; Security of data; Bayes Classifier; Flow-based analysis; Information and communication; Intrusion Detection Systems; Machine learning methods; NetFlows; Personal information; Public dataset; Intrusion detection",2-s2.0-85032616666
"Shirzadi A., Shahabi H., Chapi K., Bui D.T., Pham B.T., Shahedi K., Ahmad B.B.","A comparative study between popular statistical and machine learning methods for simulating volume of landslides",2017,"Catena",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019592596&doi=10.1016%2fj.catena.2017.05.016&partnerID=40&md5=4be52de8e5abe5d90fc96e576d34d064","This study attempts to compare popular statistical methods (linear, logarithmic, quadratic, power and exponential functions) with machine learning methods (multi-layer perceptron (MLP), radial base function (RBF), adaptive neural-based fuzzy inference system (ANFIS) and support vector machine (SVM)) for simulating the volume of landslides based on their surface area (VL ~ AL) in the Kurdistan province, Iran. Performances of the models were validated using some commonly error functions including the Adjusted R2, F-test and AIC (Akaike Information Criteria). The results showed that the power model demonstrates the best performance compared to other statistical methods whereas the ANFIS model outperforms other machine learning approaches. Furthermore, the comparative results showed that machine learning methods indicate better performances than simple statistical methods for simulating the volume of landslides in the study area. In practice, the outputs of this research can help managers and investigators decrease the cost of field surveys and measurements of volumes of landslides in landslide hazard management projects. © 2017 Elsevier B.V.","ANFIS; Iran; Kurdistan province; Landslide; Machine learning algorithms; Simple statistical models","algorithm; comparative study; fuzzy mathematics; landslide; machine learning; model validation; numerical model; simulation; statistical analysis; support vector machine; Iran; Kordestan [Iran]",2-s2.0-85019592596
"Zhang Y.C., Kagen A.C.","Machine Learning Interface for Medical Image Analysis",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991063736&doi=10.1007%2fs10278-016-9910-0&partnerID=40&md5=a659b17e25cd7fb291c0b0d5d64b17a5","TensorFlow is a second-generation open-source machine learning software library with a built-in framework for implementing neural networks in wide variety of perceptual tasks. Although TensorFlow usage is well established with computer vision datasets, the TensorFlow interface with DICOM formats for medical imaging remains to be established. Our goal is to extend the TensorFlow API to accept raw DICOM images as input; 1513 DaTscan DICOM images were obtained from the Parkinson’s Progression Markers Initiative (PPMI) database. DICOM pixel intensities were extracted and shaped into tensors, or n-dimensional arrays, to populate the training, validation, and test input datasets for machine learning. A simple neural network was constructed in TensorFlow to classify images into normal or Parkinson’s disease groups. Training was executed over 1000 iterations for each cross-validation set. The gradient descent optimization and Adagrad optimization algorithms were used to minimize cross-entropy between the predicted and ground-truth labels. Cross-validation was performed ten times to produce a mean accuracy of 0.938 ± 0.047 (95 % CI 0.908–0.967). The mean sensitivity was 0.974 ± 0.043 (95 % CI 0.947–1.00) and mean specificity was 0.822 ± 0.207 (95 % CI 0.694–0.950). We extended the TensorFlow API to enable DICOM compatibility in the context of DaTscan image analysis. We implemented a neural network classifier that produces diagnostic accuracies on par with excellent results from previous machine learning models. These results indicate the potential role of TensorFlow as a useful adjunct diagnostic tool in the clinical setting. © 2016, Society for Imaging Informatics in Medicine.","Artificial intelligence; Classification; Computer vision; Image analysis","Artificial intelligence; Classification (of information); Computer vision; Diagnosis; Learning systems; Medical imaging; Open source software; Open systems; Optimization; Clinical settings; Diagnostic accuracy; Dimensional arrays; Gradient descent optimization; Machine learning models; Machine learning software; Neural network classifier; Optimization algorithms; Image analysis",2-s2.0-84991063736
"Elahian B., Yeasin M., Mudigoudar B., Wheless J.W., Babajani-Feremi A.","Identifying seizure onset zone from electrocorticographic recordings: A machine learning approach based on phase locking value",2017,"Seizure",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026458309&doi=10.1016%2fj.seizure.2017.07.010&partnerID=40&md5=e5486f067b0a2efad9b27edd58a93bd1","Purpose Using a novel technique based on phase locking value (PLV), we investigated the potential for features extracted from electrocorticographic (ECoG) recordings to serve as biomarkers to identify the seizure onset zone (SOZ). Methods We computed the PLV between the phase of the amplitude of high gamma activity (80–150 Hz) and the phase of lower frequency rhythms (4–30 Hz) from ECoG recordings obtained from 10 patients with epilepsy (21 seizures). We extracted five features from the PLV and used a machine learning approach based on logistic regression to build a model that classifies electrodes as SOZ or non-SOZ. Results More than 96% of electrodes identified as the SOZ by our algorithm were within the resected area in six seizure-free patients. In four non-seizure-free patients, more than 31% of the identified SOZ electrodes by our algorithm were outside the resected area. In addition, we observed that the seizure outcome in non-seizure-free patients correlated with the number of non-resected SOZ electrodes identified by our algorithm. Conclusion This machine learning approach, based on features extracted from the PLV, effectively identified electrodes within the SOZ. The approach has the potential to assist clinicians in surgical decision-making when pre-surgical intracranial recordings are utilized. © 2017 British Epilepsy Association","Electrocorticographic (ECoG) recording; Epilepsy surgery; Intracranial EEG; Machine learning approach; Phase locking value (PLV); Seizure onset zone (SOZ); Seizure outcome","adolescent; adult; Article; child; clinical article; clinical study; comparative study; controlled study; cortical electrode; electrocorticography; electroencephalograph; electroencephalography; female; follow up; gamma rhythm; human; intractable epilepsy; machine learning; male; neuroimaging; neurosurgery; nuclear magnetic resonance imaging; phase locking value; preschool child; priority journal; retrospective study; seizure; temporal lobe epilepsy; young adult",2-s2.0-85026458309
"Lötsch J., Ultsch A., Kalso E.","Prediction of persistent post-surgery pain by preoperative cold pain sensitivity: Biomarker development with machine-learning-derived analysis",2017,"British Journal of Anaesthesia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032863237&doi=10.1093%2fbja%2faex236&partnerID=40&md5=f4c1097305cadc53f629e00d1cbcfaf6","Background To prevent persistent post-surgery pain, early identification of patients at high risk is a clinical need. Supervised machine-learning techniques were used to test how accurately the patients' performance in a preoperatively performed tonic cold pain test could predict persistent post-surgery pain. Methods We analysed 763 patients from a cohort of 900 women who were treated for breast cancer, of whom 61 patients had developed signs of persistent pain during three yr of follow-up. Preoperatively, all patients underwent a cold pain test (immersion of the hand into a water bath at 2-4 °C). The patients rated the pain intensity using a numerical ratings scale (NRS) from 0 to 10. Supervised machine-learning techniques were used to construct a classifier that could predict patients at risk of persistent pain. Results Whether or not a patient rated the pain intensity at NRS=10 within less than 45 s during the cold water immersion test provided a negative predictive value of 94.4% to assign a patient to the ""persistent pain"" group. If NRS=10 was never reached during the cold test, the predictive value for not developing persistent pain was almost 97%. However, a low negative predictive value of 10% implied a high false positive rate. Conclusions Results provide a robust exclusion of persistent pain in women with an accuracy of 94.4%. Moreover, results provide further support for the hypothesis that the endogenous pain inhibitory system may play an important role in the process of pain becoming persistent. © The Author 2017. Published by Oxford University Press on behalf of the British Journal of Anaesthesia.","cold induced pain; human experimental pain; Post surgery pain; supervised machine-learning","biological marker; Article; breast cancer; cancer patient; cancer surgery; cohort analysis; cold sensitivity; controlled study; female; follow up; human; major clinical study; mastectomy; nociception; numeric rating scale; pain assessment; pain intensity; partial mastectomy; postoperative pain; predictive value; preoperative period; priority journal; sensitivity and specificity; supervised machine learning; water immersion",2-s2.0-85032863237
"Vranas K.C., Jopling J.K., Sweeney T.E., Ramsey M.C., Milstein A.S., Slatore C.G., Escobar G.J., Liu V.X.","Identifying distinct subgroups of ICU patients: A machine learning approach",2017,"Critical Care Medicine",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021057838&doi=10.1097%2fCCM.0000000000002548&partnerID=40&md5=3047abab45e8355db33f018796d333fc","Objectives: Identifying subgroups of ICU patients with similar clinical needs and trajectories may provide a framework for more efficient ICU care through the design of care platforms tailored around patients' shared needs. However, objective methods for identifying these ICU patient subgroups are lacking. We used a machine learning approach to empirically identify ICU patient subgroups through clustering analysis and evaluate whether these groups might represent appropriate targets for care redesign efforts. Design: We performed clustering analysis using data from patients' hospital stays to retrospectively identify patient subgroups from a large, heterogeneous ICU population. Setting: Kaiser Permanente Northern California, a healthcare delivery system serving 3.9 million members. Patients: ICU patients 18 years old or older with an ICU admission between January 1, 2012, and December 31, 2012, at one of 21 Kaiser Permanente Northern California hospitals. Interventions: None. Measurements and Main Results: We used clustering analysis to identify putative clusters among 5,000 patients randomly selected from 24,884 ICU patients. To assess cluster validity, we evaluated the distribution and frequency of patient characteristics and the need for invasive therapies. We then applied a classifier built from the sample cohort to the remaining 19,884 patients to compare the derivation and validation clusters. Clustering analysis successfully identified six clinically recognizable subgroups that differed significantly in all baseline characteristics and clinical trajectories, despite sharing common diagnoses. In the validation cohort, the proportion of patients assigned to each cluster was similar and demonstrated significant differences across clusters for all variables. Conclusions: A machine learning approach revealed important differences between empirically derived subgroups of ICU patients that are not typically revealed by admitting diagnosis or severity of illness alone. Similar data-driven approaches may provide a framework for future organizational innovations in ICU care tailored around patients' shared needs.","clustering analysis; critical care; intensive care units; patient care management; unsupervised machine learning","adult; aged; Article; cluster analysis; cohort analysis; female; health care delivery; health care system; hospitalization; human; intensive care unit; invasive procedure; machine learning; major clinical study; male; priority journal; retrospective study; California; cluster analysis; intensive care; middle aged; needs assessment; Aged; California; Cluster Analysis; Critical Care; Female; Humans; Intensive Care Units; Machine Learning; Male; Middle Aged; Needs Assessment",2-s2.0-85021057838
"Lim D.K., Long N.P., Mo C., Dong Z., Cui L., Kim G., Kwon S.W.","Combination of mass spectrometry-based targeted lipidomics and supervised machine learning algorithms in detecting adulterated admixtures of white rice",2017,"Food Research International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027249955&doi=10.1016%2fj.foodres.2017.08.006&partnerID=40&md5=35336719a5f268e7d0614082f8b83b1e","The mixing of extraneous ingredients with original products is a common adulteration practice in food and herbal medicines. In particular, authenticity of white rice and its corresponding blended products has become a key issue in food industry. Accordingly, our current study aimed to develop and evaluate a novel discrimination method by combining targeted lipidomics with powerful supervised learning methods, and eventually introduce a platform to verify the authenticity of white rice. A total of 30 cultivars were collected, and 330 representative samples of white rice from Korea and China as well as seven mixing ratios were examined. Random forests (RF), support vector machines (SVM) with a radial basis function kernel, C5.0, model averaged neural network, and k-nearest neighbor classifiers were used for the classification. We achieved desired results, and the classifiers effectively differentiated white rice from Korea to blended samples with high prediction accuracy for the contamination ratio as low as five percent. In addition, RF and SVM classifiers were generally superior to and more robust than the other techniques. Our approach demonstrated that the relative differences in lysoGPLs can be successfully utilized to detect the adulterated mixing of white rice originating from different countries. In conclusion, the present study introduces a novel and high-throughput platform that can be applied to authenticate adulterated admixtures from original white rice samples. © 2017 Elsevier Ltd","Adulteration; Discrimination; Lysophospholipids; Machine learning; Targeted lipidomics; White rice","Artificial intelligence; Authentication; Decision trees; Learning systems; Mass spectrometry; Mixing; Nearest neighbor search; Radial basis function networks; Supervised learning; Support vector machines; Adulteration; Discrimination; Lipidomics; Lysophospholipids; White rice; Learning algorithms",2-s2.0-85027249955
"Chen Y., Luo Y., Huang W., Hu D., Zheng R.-Q., Cong S.-Z., Meng F.-K., Yang H., Lin H.-J., Sun Y., Wang X.-Y., Wu T., Ren J., Pei S.-F., Zheng Y., He Y., Hu Y., Yang N., Yan H.","Machine-learning-based classification of real-time tissue elastography for hepatic fibrosis in patients with chronic hepatitis B",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026541999&doi=10.1016%2fj.compbiomed.2017.07.012&partnerID=40&md5=b96567319574ed612c173cde9cd913d9","Hepatic fibrosis is a common middle stage of the pathological processes of chronic liver diseases. Clinical intervention during the early stages of hepatic fibrosis can slow the development of liver cirrhosis and reduce the risk of developing liver cancer. Performing a liver biopsy, the gold standard for viral liver disease management, has drawbacks such as invasiveness and a relatively high sampling error rate. Real-time tissue elastography (RTE), one of the most recently developed technologies, might be promising imaging technology because it is both noninvasive and provides accurate assessments of hepatic fibrosis. However, determining the stage of liver fibrosis from RTE images in a clinic is a challenging task. In this study, in contrast to the previous liver fibrosis index (LFI) method, which predicts the stage of diagnosis using RTE images and multiple regression analysis, we employed four classical classifiers (i.e., Support Vector Machine, Naïve Bayes, Random Forest and K-Nearest Neighbor) to build a decision-support system to improve the hepatitis B stage diagnosis performance. Eleven RTE image features were obtained from 513 subjects who underwent liver biopsies in this multicenter collaborative research. The experimental results showed that the adopted classifiers significantly outperformed the LFI method and that the Random Forest(RF) classifier provided the highest average accuracy among the four machine algorithms. This result suggests that sophisticated machine-learning methods can be powerful tools for evaluating the stage of hepatic fibrosis and show promise for clinical applications. © 2017 Elsevier Ltd","Chronic hepatitis B; Hepatic fibrosis; Machine learning; Real-time tissue elastography","Artificial intelligence; Biopsy; Decision support systems; Decision trees; Diagnosis; Diseases; Imaging techniques; Medical imaging; Nearest neighbor search; Regression analysis; Tissue; Chronic hepatitis b; Chronic liver disease; Clinical interventions; Collaborative research; Elastography; Hepatic fibrosis; Multiple regression analysis; Sophisticated machines; Learning systems; adult; Article; chronic hepatitis B; cross-sectional study; diagnostic accuracy; diagnostic test accuracy study; disease classification; elastography; female; human; human tissue; k nearest neighbor; liver biopsy; liver fibrosis; machine learning; major clinical study; male; multicenter study; priority journal; prospective study; random forest; real time ultrasound scanner; sensitivity and specificity; support vector machine",2-s2.0-85026541999
"Sun F., Huang G.-B., Jonathan Wu Q.M., Song S., Wunsch D.C., II","Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029167518&doi=10.1109%2fTSMC.2017.2741558&partnerID=40&md5=546a54288ad38508019420e09161b565","With the exponential growth of data and complexity of systems, fast machine learning/artificial intelligence and computational intelligence techniques are highly required. Many conventional computational intelligence techniques face bottlenecks in learning (e.g., intensive human intervention and convergence time) [item 1) in the Appendix]. However, efficient learning algorithms alternatively offer significant benefits including fast learning speed, ease of implementation, and minimal human intervention. The need for efficient and fast implementation of machine learning techniques in big data and dynamic varying systems poses many research challenges. This special issue highlights some latest development in the related areas. © 2013 IEEE.",,"Artificial intelligence; Big data; Learning systems; Computational intelligence techniques; Convergence time; Exponential growth; Fast implementation; Human intervention; Latest development; Machine learning techniques; Research challenges; Learning algorithms",2-s2.0-85029167518
"Buckley S.","Combining broadband spectra and machine learning to derive material properties",2017,"Spectroscopy (Santa Monica)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032297347&partnerID=40&md5=823be4e59c1c589f5832a992405527c4","A quiet but interesting trend has been occurring in material analysis, coincident with the rise of artificial intelligence (AI) and so-called ""deep"" machine learning methods. Astute spectroscopists have always known that there is more information in the spectra that they obtain than simply the molecular or atomic peaks that are directly measured. Particularly with methods such as infrared, Raman, and laser-induced breakdown spectroscopy (LIBS), the spectral background contains a wealth of information about the sample, and analytical combinations of the peaks can provide material properties. Traditionally, such analytical combinations of peaks were performed explicitly by analysts, but now information about material properties embedded in the spectra can be derived implicitly by AI and machine learning algorithms. This column introduces these ideas and touches on recent results indicative of what more may be coming in this direction.",,"Artificial intelligence; Atomic emission spectroscopy; Laser induced breakdown spectroscopy; Learning systems; Broadband spectra; Laserinduced breakdown spectroscopy (LIBS); Machine learning methods; Material analysis; Wealth of information; Learning algorithms",2-s2.0-85032297347
"Granata F., de Marinis G.","Machine learning methods for wastewater hydraulics",2017,"Flow Measurement and Instrumentation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027563854&doi=10.1016%2fj.flowmeasinst.2017.08.004&partnerID=40&md5=a7edb66c71731706a5ebc6d49444e9c4","Wastewater hydraulics problems are frequently addressed by investigation on physical models. Dimensional analysis is a powerful tool that allows discovering essential information about the investigated phenomenon, but in some cases it is affected by significant limitations. In such cases, many issues can be addressed by means of machine learning algorithms, resulting from the theories on pattern recognition and computational learning. In order to show the potential of such an approach, in this study Regression Tree M5P model, Bagging algorithm and Random Forest algorithm were applied to the solution of some complex problems of wastewater engineering: the prediction of energy loss, the pool depth, the air entrainment in a drop manhole, and the forecasting of the lateral outflow in a low crested side weir. The algorithms were trained and tested on data obtained from experimental tests that were carried out at the Water Engineering Laboratory of the University of Cassino and Southern Lazio. In most of the considered cases, regression trees and ensemble methods were able to provide very accurate predictions. © 2017 Elsevier Ltd","Bagging; Drop manhole; Experimental research; Machine learning; Random Forest; Side weir; Tree model; Wastewater hydraulics","Air entrainment; Artificial intelligence; Computation theory; Decision trees; Drops; Energy dissipation; Forecasting; Forestry; Hydraulic structures; Hydraulics; Laboratories; Learning systems; Pattern recognition; Weirs; Bagging; Experimental research; Random forests; Side- weirs; Tree modeling; Learning algorithms",2-s2.0-85027563854
"Rahman A., Smith A.D.","Predicting fuel consumption for commercial buildings with machine learning algorithms",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026487043&doi=10.1016%2fj.enbuild.2017.07.017&partnerID=40&md5=ab3bed1d43fec6161cc0ee5b81365a3d","This paper presents a modeling framework that uses machine learning algorithms to make long-term, i.e. one year-ahead predictions, of fuel consumption in multiple types of commercial prototype buildings at one-hour resolutions. Weather and schedule variables were used as model inputs, and the hourly fuel consumption simulated with EnergyPlus provided target values. The data was partitioned on a monthly basis, and a feature selection method was incorporated as part of the model to select the best subset of input variables for a given month. Neural networks (NN) and Gaussian process (GP) regression were shown to perform better than multivariate linear regression and ridge regression, and as such, were included as part of the model. The modeling framework was applied to make predictions about fuel consumption in a small office, supermarket, and restaurant in multiple climate zone. It was shown that for all climate zones for all months, the maximum errors pertaining to one year-ahead forecasts of fuel consumption made by the ML model are 15.7 MJ (14,880 Btu), 284.3 MJ (268,516 Btu) and 74.0 MJ (70,138 Btu) respectively. The methods and results from this study can be used to estimate on-site fuel consumption and emissions from buildings, thereby enabling improved decisions pertaining to building efficiency with respect to fuel use. © 2017 Elsevier B.V.","Building energy modeling; Data-driven modeling; Heating load; Machine learning; Prediction","Artificial intelligence; Buildings; Climate models; Education; Forecasting; Fuels; Learning systems; Office buildings; Regression analysis; Building efficiency; Building energy model; Commercial building; Data-driven model; Feature selection methods; Heating load; Multivariate linear regressions; Neural network (nn); Learning algorithms",2-s2.0-85026487043
"Zhu Y., Liu K., Liu L., Myint S.W., Wang S., Liu H., He Z.","Exploring the potential of world view-2 red-edge band-based vegetation indices for estimation of mangrove leaf area index with machine learning algorithms",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032880259&doi=10.3390%2frs9101060&partnerID=40&md5=b3e836e9092e9b7989e86f8294e0ef06","To accurately estimate leaf area index (LAI) in mangrove areas, the selection of appropriate models and predictor variables is critical. However, there is a major challenge in quantifying and mapping LAI using multi-spectral sensors due to the saturation effects of traditional vegetation indices (VIs) for mangrove forests. WorldView-2 (WV2) imagery has proven to be effective to estimate LAI of grasslands and forests, but the sensitivity of its vegetation indices (VIs) has been uncertain for mangrove forests. Furthermore, the single model may exhibit certain randomness and instability in model calibration and estimation accuracy. Therefore, this study aims to explore the sensitivity of WV2 VIs for estimating mangrove LAI by comparing artificial neural network regression (ANNR), support vector regression (SVR) and random forest regression (RFR). The results suggest that the RFR algorithm yields the best results (RMSE = 0.45, 14.55% of the average LAI), followed by ANNR (RMSE = 0.49, 16.04% of the average LAI), and then SVR (RMSE = 0.51, 16.56% of the average LAI) algorithms using 5-fold cross validation (CV) using all VIs. Quantification of the variable importance shows that the VIs derived from the red-edge band consistently remain the most important contributor to LAI estimation. When the red-edge band-derived VIs are removed from the models, estimation accuracies measured in relative RMSE (RMSEr) decrease by 3.79%, 2.70% and 4.47% for ANNR, SVR and RFR models respectively. VIs derived from red-edge band also yield better accuracy compared with other traditional bands of WV2, such as near-infrared-1 and near-infrared-2 band. Furthermore, the estimated LAI values vary significantly across different mangrove species. The study demonstrates the utility of VIs of WV2 imagery and the selected machine-learning algorithms in developing LAI models in mangrove forests. The results indicate that the red-edge band of WV2 imagery can help alleviate the saturation problem and improve the accuracy of LAI estimation in a mangrove area. © 2017 by the authors.","Leaf area index; Machine learning; Mangrove forests; Red-edge band; Variable importance; Vegetation index; WorldView-2 imagery","Artificial intelligence; Calibration; Decision trees; Forestry; Image enhancement; Infrared devices; Learning systems; Neural networks; Photomapping; Plants (botany); Regression analysis; Vegetation; Leaf Area Index; Mangrove forest; Red edge; Variable importances; Vegetation index; Worldview-2; Learning algorithms",2-s2.0-85032880259
"Gagne D.J., II, McGovern A., Haupt S.E., Sobash R.A., Williams J.K., Xue M.","Storm-based probabilistic hail forecasting with machine learning applied to convection-allowing ensembles",2017,"Weather and Forecasting",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032296055&doi=10.1175%2fWAF-D-17-0010.1&partnerID=40&md5=1ce1a6b74fb491e89dac7dcc1d214612","Forecasting severe hail accurately requires predicting how well atmospheric conditions support the development of thunderstorms, the growth of large hail, and the minimal loss of hail mass to melting before reaching the surface. Existing hail forecasting techniques incorporate information about these processes from proximity soundings and numerical weather prediction models, but they make many simplifying assumptions, are sensitive to differences in numerical model configuration, and are often not calibrated to observations. In this paper a storm-based probabilistic machine learning hail forecasting method is developed to overcome the deficiencies of existing methods. An object identification and tracking algorithm locates potential hailstorms in convection-allowing model output and gridded radar data. Forecast storms are matched with observed storms to determine hail occurrence and the parameters of the radar-estimated hail size distribution. The database of forecast storms contains information about storm properties and the conditions of the prestorm environment. Machine learning models are used to synthesize that information to predict the probability of a storm producing hail and the radar-estimated hail size distribution parameters for each forecast storm. Forecasts from the machine learning models are produced using two convection-allowing ensemble systems and the results are compared to other hail forecasting methods. The machine learning forecasts have a higher critical success index (CSI) at most probability thresholds and greater reliability for predicting both severe and significant hail. © 2017 American Meteorological Society.","Convective storms; Ensembles; Forecast verification/skill; Hail; Probability forecasts/models/distribution; Statistical forecasting","Artificial intelligence; Forecasting; Learning systems; Numerical models; Parameter estimation; Precipitation (meteorology); Probability; Probability distributions; Radar; Size distribution; Storms; Convective storms; Ensembles; Forecast verification/skill; Probability forecasts/models/distribution; Statistical forecasting; Weather forecasting; convective system; ensemble forecasting; hail; machine learning; numerical model; probability; statistical analysis; thunderstorm; weather forecasting",2-s2.0-85032296055
"Dumpert F., Beck M.","Use of machine learning in official business statistics [Einsatz von Machine-Learning-Verfahren in amtlichen Unternehmensstatistiken]",2017,"AStA Wirtschafts- und Sozialstatistisches Archiv",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032026299&doi=10.1007%2fs11943-017-0208-6&partnerID=40&md5=56b5e668bbad8059aa7dc6f0b255ce43","The task of the official business statistics is to provide information on the structure and development of the economy, which is gained through surveys, the use of administrative data, the purchase of commercial data and the linking of micro data. Recently, the use of machine learning methods in official business statistics has also been experimentally tested in the case of classification decisions and the generation of new data. This article provides an overview of the proceeding. To this end, the methodology of machine learning is first presented in the basic principles, previous fields of application are described outside and in official statistics, and the methods used experimentally in the business statistics are explained. Subsequently, the practical application of Support Vector Machines and Random Forests is presented in five concrete tasks in selected business statistics. Finally, the experience gained so far is summarized and potential further tasks as well as foreseeable further developments of the machine learning methods are presented. © 2017, Springer-Verlag GmbH Deutschland.","Business statistics; Machine learning; Random Forest; Support Vector Machine",,2-s2.0-85032026299
"Ashinsky B.G., Bouhrara M., Coletta C.E., Lehallier B., Urish K.L., Lin P.-C., Goldberg I.G., Spencer R.G.","Predicting early symptomatic osteoarthritis in the human knee using machine learning classification of magnetic resonance images from the osteoarthritis initiative",2017,"Journal of Orthopaedic Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016411648&doi=10.1002%2fjor.23519&partnerID=40&md5=5299d8ab2afd5ccc952623a21753994f","The purpose of this study is to evaluate the ability of a machine learning algorithm to classify in vivo magnetic resonance images (MRI) of human articular cartilage for development of osteoarthritis (OA). Sixty-eight subjects were selected from the osteoarthritis initiative (OAI) control and incidence cohorts. Progression to clinical OA was defined by the development of symptoms as quantified by the Western Ontario and McMaster Universities Arthritis (WOMAC) questionnaire 3 years after baseline evaluation. Multi-slice T2-weighted knee images, obtained through the OAI, of these subjects were registered using a nonlinear image registration algorithm. T2 maps of cartilage from the central weight bearing slices of the medial femoral condyle were derived from the registered images using the multiple available echo times and were classified for “progression to symptomatic OA” using the machine learning tool, weighted neighbor distance using compound hierarchy of algorithms representing morphology (WND-CHRM). WND-CHRM classified the isolated T2 maps for the progression to symptomatic OA with 75% accuracy. Clinical significance: Machine learning algorithms applied to T2 maps have the potential to provide important prognostic information for the development of OA. © 2017 Orthopaedic Research Society. Published by Wiley Periodicals, Inc. J Orthop Res 35:2243–2250, 2017. © 2017 Orthopaedic Research Society. Published by Wiley Periodicals, Inc.","classification; MRI; osteoarthritis; pattern recognition; registration; segmentation","adult; Article; articular cartilage; case control study; clinical evaluation; cohort analysis; controlled study; diagnostic accuracy; diagnostic test accuracy study; femoral condyle; human; image analysis; incidence; knee; knee osteoarthritis; knee radiography; learning algorithm; machine learning; major clinical study; middle aged; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; priority journal; registration; sensitivity and specificity; symptom; weight bearing; Western Ontario and McMaster Universities Osteoarthritis Index; WND CHRM algorithm; algorithm; diagnostic imaging; knee osteoarthritis; regression analysis; Algorithms; Cartilage, Articular; Cohort Studies; Humans; Machine Learning; Magnetic Resonance Imaging; Middle Aged; Osteoarthritis, Knee; Regression Analysis",2-s2.0-85016411648
"Wang J., Wu C.-J., Bao M.-L., Zhang J., Wang X.-N., Zhang Y.-D.","Machine learning-based analysis of MR radiomics can help to improve the diagnostic performance of PI-RADS v2 in clinically relevant prostate cancer",2017,"European Radiology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016965514&doi=10.1007%2fs00330-017-4800-5&partnerID=40&md5=9658b86391f99242fc86757fa58ddf7f","Objective: To investigate whether machine learning-based analysis of MR radiomics can help improve the performance PI-RADS v2 in clinically relevant prostate cancer (PCa). Methods: This IRB-approved study included 54 patients with PCa undergoing multi-parametric (mp) MRI before prostatectomy. Imaging analysis was performed on 54 tumours, 47 normal peripheral (PZ) and 48 normal transitional (TZ) zone based on histological-radiological correlation. Mp-MRI was scored via PI-RADS, and quantified by measuring radiomic features. Predictive model was developed using a novel support vector machine trained with: (i) radiomics, (ii) PI-RADS scores, (iii) radiomics and PI-RADS scores. Paired comparison was made via ROC analysis. Results: For PCa versus normal TZ, the model trained with radiomics had a significantly higher area under the ROC curve (Az) (0.955 [95% CI 0.923–0.976]) than PI-RADS (Az: 0.878 [0.834–0.914], p < 0.001). The Az between them was insignificant for PCa versus PZ (0.972 [0.945–0.988] vs. 0.940 [0.905–0.965], p = 0.097). When radiomics was added, performance of PI-RADS was significantly improved for PCa versus PZ (Az: 0.983 [0.960–0.995]) and PCa versus TZ (Az: 0.968 [0.940–0.985]). Conclusion: Machine learning analysis of MR radiomics can help improve the performance of PI-RADS in clinically relevant PCa. Key Points: • Machine-based analysis of MR radiomics outperformed in TZ cancer against PI-RADS. • Adding MR radiomics significantly improved the performance of PI-RADS. • DKI-derived Dapp and Kapp were two strong markers for the diagnosis of PCa. © 2017, European Society of Radiology.","Machine learning; Multi-parametric MRI; Prostate cancer; Prostate Imaging Reporting and Data System v2; Support vector machine","adult; aged; Article; histopathology; human; human tissue; machine learning; major clinical study; male; nuclear magnetic resonance; nuclear magnetic resonance radiomic; nuclear magnetic resonance scanner; predictive value; priority journal; prostate cancer; Prostate Imaging Reporting and Data System v2 score; receiver operating characteristic; retrospective study; scoring system",2-s2.0-85016965514
"Wang J., Yang X., Zeng Z., Zhang X., Zhao X., Wang Z.","New methods for prediction of elastic constants based on density functional theory combined with machine learning",2017,"Computational Materials Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021679863&doi=10.1016%2fj.commatsci.2017.06.015&partnerID=40&md5=b66b78abd6c8230829afd46ca4ef9543","Elastic constants play critical roles in researching mechanical properties, but they are usually difficult to be measured. While density functional theory (DFT) calculations provide a reliable method to meet this challenge, the results contain inherent errors caused by various approximations. The data-driven approach of machine learning also laid a foundation for predicting material properties. In order to increase the accuracy of theoretical calculations results, in this paper we investigate using machine learning methods to both correct the elastic constants by DFT calculation, and to directly predict elastic constants. The single-hidden layer feedforward neural network trained by back propagation algorithm (SLFN), general regression neural network (GRNN) and support vector machine for regression (SVR) techniques are employed to build regression models to correct the elastic constants by DFT calculation for metal or metallic binary alloys. We also build regression models to predict the elastic constants of metallic binary alloys with cubic crystal system rather than using DFT calculations. It has been demonstrated that the elastic constants corrected by regression models has higher accuracy than those calculated by DFT, and the elastic constants of binary alloys directly predicted by model using the outperformed SLFN technique is prospective. © 2017 Elsevier B.V.","DFT calculation; General regression neural network; Materials informatics; Neural network; Prediction of elastic constants; Support vector regression","Artificial intelligence; Binary alloys; Bins; Education; Elastic constants; Feedforward neural networks; Forecasting; Intelligent systems; Learning systems; Network layers; Neural networks; Regression analysis; Cubic crystal system; DFT calculation; General regression neural network; Machine learning methods; Materials informatics; Single-hidden layer feedforward neural networks; Support vector regression (SVR); Theoretical calculations; Density functional theory",2-s2.0-85021679863
"Garrido A.L., Sangiao S., Cardiel O.","Improving the Generation of Infoboxes from Data Silos through Machine Learning and the Use of Semantic Repositories",2017,"International Journal on Artificial Intelligence Tools",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032869275&doi=10.1142%2fS0218213017600223&partnerID=40&md5=f417174803fd4420fd202c5584183bba","Nowadays, both public and private organizations own large private text-based data repositories with critical information. The information stored in these data silos is usually queried through information retrieval systems based on indexes, which yield hundreds or thousands of results when interrogated using keywords. In order to improve data accessibility when searching for specific information, the use of infoboxes can be very useful. The generation such infoboxes is by itself a complex problem, but in this type of isolated environments, it becomes even harder as the selection of the entities and their attributes can be conditioned by local and very specific parameters. In this work, we propose a methodology to tackle this special problem, combining classical approaches with machine learning, and leveraging the resources provided by the Semantic Web. The working methodology has been applied to two well-known datasets, and also it has been tested on a real environment scenario, showing the feasibility of our approach. © 2017 World Scientific Publishing Company.","Infoboxes; information extraction; machine learning; named entity disambiguation","Artificial intelligence; Information retrieval; Information retrieval systems; Learning systems; Classical approach; Data accessibility; Infoboxes; Named entity disambiguations; Private organizations; Real environments; Semantic repository; Specific information; Search engines",2-s2.0-85032869275
"Tamuli D., Godiyal A.K., Kaur M., Jaryal A.K., Srivastava A.K., Deepak K.K.","Autonomic function based classification of spinocerebellar ataxia type 1 and 2 using machine learning classifiers",2017,"Indian Journal of Physiology and Pharmacology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030754251&partnerID=40&md5=7c865786ae8b76bcb7acb38b71637cb0","Spinocerebellar ataxia (SCA) is a progressive neurodegenerative disorder characterized by autonomic dysfunction. SCA has multiple genetically classified subtypes, amongst which SCA1 and SCA2 are most prevalent in India. Autonomic function based characterization of SCA patients into respective subtypes has not been done. We have evaluated autonomic function - heart rate variability (HRV), systolic blood pressure variability (BPV), systolic baroreflex sensitivity (BRS) and composite autonomic severity score (CASS) in SCA patients (SCA1 = 31; SCA2 = 40). To evaluate the classification performance of the battery of autonomic function tests (AFT), linear discriminant analysis (LDA) and support vector machine (SVM) classifiers were used. The average classification accuracy for SCA subtypes were 80% by LDA and 70% by SVM. Interestingly, individually the autonomic function tests do not differ between SCA1 and SCA2 but when they are used together by classifier - a conclusive pattern to characterize the SCA subtypes emerges. This is the first study to classify SCA patients into their respective subtypes using a novel machine learning approach on their autonomic function profile. © 2017, Association of Physiologists and Pharmacologists of India. All rights reserved.",,"adult; Article; autonomic nervous system function; cerebellar ataxia; composite autonomic severity score; controlled study; diagnostic accuracy; disease classification; female; functional assessment; heart rate variability; human; India; linear discriminant analysis; machine learning; major clinical study; male; neurologic examination; performance; pressoreceptor reflex; scoring system; spinocerebellar ataxia; support vector machine; systolic baroreflex sensitivity; systolic blood pressure",2-s2.0-85030754251
"Nakai Y., Takiguchi T., Matsui G., Yamaoka N., Takada S.","Detecting Abnormal Word Utterances in Children With Autism Spectrum Disorders: Machine-Learning-Based Voice Analysis Versus Speech Therapists",2017,"Perceptual and Motor Skills",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029505555&doi=10.1177%2f0031512517716855&partnerID=40&md5=bb96a8fa09595d771bc035451e0504bf","Abnormal prosody is often evident in the voice intonations of individuals with autism spectrum disorders. We compared a machine-learning-based voice analysis with human hearing judgments made by 10 speech therapists for classifying children with autism spectrum disorders (n = 30) and typical development (n = 51). Using stimuli limited to single-word utterances, machine-learning-based voice analysis was superior to speech therapist judgments. There was a significantly higher true-positive than false-negative rate for machine-learning-based voice analysis but not for speech therapists. Results are discussed in terms of some artificiality of clinician judgments based on single-word utterances, and the objectivity machine-learning-based voice analysis adds to judging abnormal prosody. © 2017, © The Author(s) 2017.","abnormal prosody; autism spectrum disorder; F-measure; machine-learning-based voice analysis; speech therapy",,2-s2.0-85029505555
"Venkatraman V., Alsberg B.K.","Predicting CO2 capture of ionic liquids using machine learning",2017,"Journal of CO2 Utilization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025091455&doi=10.1016%2fj.jcou.2017.06.012&partnerID=40&md5=03dee3007779545889068930074bac26","Ionic liquid (IL) based CO2 capture is currently seen as a promising alternative to conventional amine-based solvents. While the possible combinations of cations and anions are numerous, it is time consuming and expensive to carry out experimental measurements for CO2 solubilities for each new IL. Therefore, as a means to rapidly screen suitable ILs as potential solvents for CO2 absorption, we investigate the use of machine learning (ML) based models to establish structure-property relationships between molecular structures of cations and anions and their CO2 solubilities. Over 10,000 IL-CO2 solubility data of 185 ILs measured at different operating temperatures and pressures were extracted from the literature. Using semi-empirically derived geometrical and charge-based molecular descriptors, good agreement with the available experimental measurements was obtained for both single decision tree (mean absolute error of 0.10) and ensemble random forest (mean absolute error of 0.04) approaches. The results were found to be more accurate than those obtained with the quantum chemistry based COSMOtherm predictions. © 2017 Elsevier Ltd.","CO2 capture; Ionic liquids; Machine learning; QSPR","Artificial intelligence; Data mining; Decision trees; Education; Ionic liquids; Learning systems; Liquids; Positive ions; Quantum chemistry; Solubility; CO2 absorption; Mean absolute error; Molecular descriptors; Operating temperature; QSPR; Random forests; Single decision; Structure property relationships; Carbon dioxide",2-s2.0-85025091455
"Joel S., Eastwick P.W., Finkel E.J.","Is Romantic Desire Predictable? Machine Learning Applied to Initial Romantic Attraction",2017,"Psychological Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031106158&doi=10.1177%2f0956797617714580&partnerID=40&md5=5422235b7604b9def069f137680cf47b","Matchmaking companies and theoretical perspectives on close relationships suggest that initial attraction is, to some extent, a product of two people’s self-reported traits and preferences. We used machine learning to test how well such measures predict people’s overall tendencies to romantically desire other people (actor variance) and to be desired by other people (partner variance), as well as people’s desire for specific partners above and beyond actor and partner variance (relationship variance). In two speed-dating studies, romantically unattached individuals completed more than 100 self-report measures about traits and preferences that past researchers have identified as being relevant to mate selection. Each participant met each opposite-sex participant attending a speed-dating event for a 4-min speed date. Random forests models predicted 4% to 18% of actor variance and 7% to 27% of partner variance; crucially, however, they were unable to predict relationship variance using any combination of traits and preferences reported before the dates. These results suggest that compatibility elements of human mating are challenging to predict before two people meet. © 2017, © The Author(s) 2017.","attraction; dating; ensemble methods; machine learning; open data; open materials; random forests; romantic desire; romantic relationships; speed dating; statistical learning",,2-s2.0-85031106158
"Bandaragoda T.R., De Silva D., Alahakoon D.","Automatic event detection in microblogs using incremental machine learning",2017,"Journal of the Association for Information Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021831438&doi=10.1002%2fasi.23896&partnerID=40&md5=11dcfa26f3a6a9d81415b4c1d30bed80","The global popularity of microblogs has led to an increasing accumulation of large volumes of text data on microblogging platforms such as Twitter. These corpora are untapped resources to understand social expressions on diverse subjects. Microblog analysis aims to unlock the value of such expressions by discovering insights and events of significance hidden among swathes of text. Besides velocity; diversity of content, brevity, absence of structure and time-sensitivity are key challenges in microblog analysis. In this paper, we propose an unsupervised incremental machine learning and event detection technique to address these challenges. The proposed technique separates a microblog discussion into topics to address the key problem of diversity. It maintains a record of the evolution of each topic over time. Brevity, time-sensitivity and unstructured nature are addressed by these individual topic pathways which contribute to generate a temporal, topic-driven structure of a microblog discussion. The proposed event detection method continuously monitors these topic pathways using multiple domain-independent event indicators for events of significance. The autonomous nature of topic separation, topic pathway generation, new topic identification and event detection, appropriates the proposed technique for extensive applications in microblog analysis. We demonstrate these capabilities on tweets containing #microsoft and tweets containing #obama. © 2017 ASIS&T",,"Artificial intelligence; Learning systems; Event detection; Large volumes; Micro-blog; Micro-blogging platforms; Microblogs; Multiple domains; Time sensitivity; Topic identification; Education; human; machine learning; velocity",2-s2.0-85021831438
"Schwartz I.M., York P., Nowakowski-Sims E., Ramos-Hernandez A.","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience",2017,"Children and Youth Services Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028534860&doi=10.1016%2fj.childyouth.2017.08.020&partnerID=40&md5=2d6c6ccf256e9a5ee0627b0d1fed414f","This paper presents the findings from a study designed to explore whether predictive analytics and machine learning could improve the accuracy and utility of the child welfare risk assessment instrument used in Broward County (Ft. Lauderdale, Florida). The findings from this study indicate that, indeed, predictive analytics and machine learning would significantly improve the accuracy and utility of the child welfare risk assessment instrument being used. If the predictive analytic and machine learning algorithms developed in this study would be deployed, there would be improved accuracy in identifying low, moderate and high risk cases, better matching between the needs of children and families and available services and improved child and family outcomes. This paper also identifies further areas of research and study. © 2017 Elsevier Ltd","Analytics; Child Welfare; Machine Learning","child; child welfare; Florida; human; machine learning; outcome assessment; risk assessment",2-s2.0-85028534860
"Henglin M., Stein G., Hushcha P.V., Snoek J., Wiltschko A.B., Cheng S.","Machine Learning Approaches in Cardiovascular Imaging",2017,"Circulation. Cardiovascular imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031011688&doi=10.1161%2fCIRCIMAGING.117.005614&partnerID=40&md5=6811fe31b4686bdae2e46a714d70911c","Cardiovascular imaging technologies continue to increase in their capacity to capture and store large quantities of data. Modern computational methods, developed in the field of machine learning, offer new approaches to leveraging the growing volume of imaging data available for analyses. Machine learning methods can now address data-related problems ranging from simple analytic queries of existing measurement data to the more complex challenges involved in analyzing raw images. To date, machine learning has been used in 2 broad and highly interconnected areas: automation of tasks that might otherwise be performed by a human and generation of clinically important new knowledge. Most cardiovascular imaging studies have focused on task-oriented problems, but more studies involving algorithms aimed at generating new clinical insights are emerging. Continued expansion in the size and dimensionality of cardiovascular imaging databases is driving strong interest in applying powerful deep learning methods, in particular, to analyze these data. Overall, the most effective approaches will require an investment in the resources needed to appropriately prepare such large data sets for analyses. Notwithstanding current technical and logistical challenges, machine learning and especially deep learning methods have much to offer and will substantially impact the future practice and science of cardiovascular imaging. © 2017 American Heart Association, Inc.","algorithms; artificial intelligence; automation; workflow","algorithm; automation; Cardiovascular Diseases; computer assisted diagnosis; diagnostic imaging; human; machine learning; predictive value; procedures; prognosis; reproducibility; severity of illness index; workflow; Algorithms; Automation; Cardiovascular Diseases; Diagnostic Imaging; Humans; Image Interpretation, Computer-Assisted; Machine Learning; Predictive Value of Tests; Prognosis; Reproducibility of Results; Severity of Illness Index; Workflow",2-s2.0-85031011688
"Adam E., Mureriwa N., Newete S.","Mapping Prosopis glandulosa (mesquite) in the semi-arid environment of South Africa using high-resolution WorldView-2 imagery and machine learning classifiers",2017,"Journal of Arid Environments",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018765771&doi=10.1016%2fj.jaridenv.2017.05.001&partnerID=40&md5=c248e638893b298545fff97418a21a8b","The rapid spread of the Prosopis species has caused considerable negative impacts to biodiversity across different landscapes. The invasive taxa of Prosopis is currently rated the world's top 100 unwanted species. However, the lack of up-to-date information about the spatial and temporal distribution of mesquite invasion has made the current control and monitoring methods unsuccessful. Consequently, detection and monitoring of Prosopis species is essential to provide reliable and accurate information about the spatial distribution and the level of invasive species dynamism into the native eco-community. This study investigates the ability of WorldView-2 imagery for mapping the invasion of P. glandulosa and coexistent indigenous species in the semi-arid region of Northern Cape Province, South Africa, using the random forest and support vector machines as classifiers. Our results show that the eight-band multispectral WV-2 imagery is able to detect and distinguish P. glandulosa effectively from the three coexisting indigenous species of acacia, with an overall accuracy of 86% at 2 m spatial resolution. This result shows that high-accuracy can be achieved with the multispectral WV-2 sensor. This high-accuracy provides the possibility for economically-feasible mapping of the distribution and spread of invasive alien plants and assists with the restoration and conservation process. © 2017 Elsevier Ltd","Image classification; Invasive species; Prosopis glandulosa; Random forest; Support Vector Machine; WorldView 2","biodiversity; biological invasion; image classification; invasive species; machine learning; mapping; multispectral image; native species; satellite imagery; semiarid region; shrub; spatial distribution; spatial resolution; support vector machine; WorldView; Northern Cape; South Africa; Acacia; Prosopis; Prosopis glandulosa",2-s2.0-85018765771
"Chang N.-B., Bai K., Chen C.-F.","Integrating multisensor satellite data merging and image reconstruction in support of machine learning for better water quality management",2017,"Journal of Environmental Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021280402&doi=10.1016%2fj.jenvman.2017.06.045&partnerID=40&md5=435861d3218065c139a0316bb38199e5","Monitoring water quality changes in lakes, reservoirs, estuaries, and coastal waters is critical in response to the needs for sustainable development. This study develops a remote sensing-based multiscale modeling system by integrating multi-sensor satellite data merging and image reconstruction algorithms in support of feature extraction with machine learning leading to automate continuous water quality monitoring in environmentally sensitive regions. This new Earth observation platform, termed “cross-mission data merging and image reconstruction with machine learning” (CDMIM), is capable of merging multiple satellite imageries to provide daily water quality monitoring through a series of image processing, enhancement, reconstruction, and data mining/machine learning techniques. Two existing key algorithms, including Spectral Information Adaptation and Synthesis Scheme (SIASS) and SMart Information Reconstruction (SMIR), are highlighted to support feature extraction and content-based mapping. Whereas SIASS can support various data merging efforts to merge images collected from cross-mission satellite sensors, SMIR can overcome data gaps by reconstructing the information of value-missing pixels due to impacts such as cloud obstruction. Practical implementation of CDMIM was assessed by predicting the water quality over seasons in terms of the concentrations of nutrients and chlorophyll-a, as well as water clarity in Lake Nicaragua, providing synergistic efforts to better monitor the aquatic environment and offer insightful lake watershed management strategies. © 2017 Elsevier Ltd","Enabling technology; Machine learning; Remote sensing; Water quality; Watershed management","chlorophyll a; nitrogen; phosphorus; image analysis; machine learning; reconstruction; remote sensing; satellite data; water management; water quality; watershed; algorithm; aquatic environment; Article; controlled study; data mining; entropy; image enhancement; image processing; image reconstruction; lake; machine learning; multisensor satellite data merging; Nicaragua; quality control; remote sensing; satellite imagery; sea; season; SMart Information Reconstruction; Spectral Information Adaptation and Synthesis Scheme; water management; water quality; water supply; watershed; watershed management",2-s2.0-85021280402
"Elmas A., Wang X., Dresch J.M.","The folded k-spectrum kernel: A machine learning approach to detecting transcription factor binding sites with gapped nucleotide dependencies",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030687343&doi=10.1371%2fjournal.pone.0185570&partnerID=40&md5=bc15905489c170484c5b9b9fe2860082","Understanding the molecular machinery involved in transcriptional regulation is central to improving our knowledge of an organism’s development, disease, and evolution. The building blocks of this complex molecular machinery are an organism’s genomic DNA sequence and transcription factor proteins. Despite the vast amount of sequence data now available for many model organisms, predicting where transcription factors bind, often referred to as ‘motif detection’ is still incredibly challenging. In this study, we develop a novel bioinformatic approach to binding site prediction. We do this by extending pre-existing SVM approaches in an unbiased way to include all possible gapped k-mers, representing different combinations of complex nucleotide dependencies within binding sites. We show the advantages of this new approach when compared to existing SVM approaches, through a rigorous set of cross-validation experiments. We also demonstrate the effectiveness of our new approach by reporting on its improved performance on a set of 127 genomic regions known to regulate gene expression along the anterio-posterior axis in early Drosophila embryos. © 2017 Elmas et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"nucleotide; transcription factor; nucleotide; transcription factor; anterior posterior axis; Article; binding site; bioinformatics; Drosophila; embryo; gene expression regulation; kernel method; machine learning; nonhuman; prediction; support vector machine; binding site; metabolism; Binding Sites; Machine Learning; Nucleotides; Support Vector Machine; Transcription Factors",2-s2.0-85030687343
"Pinto J.V., Passos I.C., Gomes F., Reckziegel R., Kapczinski F., Mwangi B., Kauer-Sant'Anna M.","Peripheral biomarker signatures of bipolar disorder and schizophrenia: A machine learning approach",2017,"Schizophrenia Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009739652&doi=10.1016%2fj.schres.2017.01.018&partnerID=40&md5=f1bd5388527fde86966794da4c8ca09b",[No abstract available],,"biological marker; brain derived neurotrophic factor; eotaxin; glutathione peroxidase; glutathione transferase; interleukin 10; interleukin 6; neurotrophin; bipolar disorder; clinical article; controlled study; diagnostic accuracy; DSM-IV; feasibility study; human; informed consent; Letter; machine learning; pathophysiology; predictive value; priority journal; schizophrenia; Structured Clinical Interview for DSM Disorders; support vector machine",2-s2.0-85009739652
"Rodrigues É.O., Pinheiro V.H.A., Liatsis P., Conci A.","Machine learning in the prediction of cardiac epicardial and mediastinal fat volumes",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015312884&doi=10.1016%2fj.compbiomed.2017.02.010&partnerID=40&md5=a03e8943f4f948efd41f10c7e7862775","We propose a methodology to predict the cardiac epicardial and mediastinal fat volumes in computed tomography images using regression algorithms. The obtained results indicate that it is feasible to predict these fats with a high degree of correlation, thus alleviating the requirement for manual or automatic segmentation of both fat volumes. Instead, segmenting just one of them suffices, while the volume of the other may be predicted fairly precisely. The correlation coefficient obtained by the Rotation Forest algorithm using MLP Regressor for predicting the mediastinal fat based on the epicardial fat was 0.9876, with a relative absolute error of 14.4% and a root relative squared error of 15.7%. The best correlation coefficient obtained in the prediction of the epicardial fat based on the mediastinal was 0.9683 with a relative absolute error of 19.6% and a relative squared error of 24.9%. Moreover, we analysed the feasibility of using linear regressors, which provide an intuitive interpretation of the underlying approximations. In this case, the obtained correlation coefficient was 0.9534 for predicting the mediastinal fat based on the epicardial, with a relative absolute error of 31.6% and a root relative squared error of 30.1%. On the prediction of the epicardial fat based on the mediastinal fat, the correlation coefficient was 0.8531, with a relative absolute error of 50.43% and a root relative squared error of 52.06%. In summary, it is possible to speed up general medical analyses and some segmentation and quantification methods that are currently employed in the state-of-the-art by using this prediction approach, which consequently reduces costs and therefore enables preventive treatments that may lead to a reduction of health problems. © 2017 Elsevier Ltd","Adipose tissue; Cardiac fat segmentation; Correlation; Epicardial; Mediastinal; Prediction; Quantification; Regression; Volume Estimation","Computerized tomography; Correlation methods; Errors; Learning systems; Medical problems; Adipose tissue; Epicardial; Mediastinal; Quantification; Regression; Volume estimations; Forecasting; adipose tissue; Article; clinical article; computer assisted tomography; correlation coefficient; epicardial fat; fat mass; feasibility study; human; image segmentation; intermethod comparison; k nearest neighbor; limit of quantitation; linear regression analysis; machine learning; measurement error; mediastinal fat; perceptron; prediction; predictive value; priority journal; random forest; rotation forest",2-s2.0-85015312884
"Murata M., Abe Y.","Using machine learning for automatic estimation of emphases in Japanese documents",2017,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030258282&doi=10.1587%2ftransinf.2016EDL8247&partnerID=40&md5=79a81f902e2145a0bab01297923d65a1","We propose a method for automatic emphasis estimation using conditional random fields. In our experiments, the value of Fmeasure obtained using our proposed method (0.31) was higher than that obtained using a random emphasis method (0.20), a method using TF-IDF (0.21), and a method based on LexRank (0.26). On the contrary, the value of F-measure of obtained using our proposed method (0.28) was slightly worse as compared with that obtained using manual estimation (0.26-0.40, with an average of 0.35). Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","Automatic estimation; Bold; Conditional random fields; Emphasis; Machine learning","Artificial intelligence; Random processes; Automatic estimation; Bold; Conditional random field; Emphasis; F measure; Japanese document; Lexrank; Learning systems",2-s2.0-85030258282
"Zuo R.","Machine Learning of Mineralization-Related Geochemical Anomalies: A Review of Potential Methods",2017,"Natural Resources Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019550384&doi=10.1007%2fs11053-017-9345-4&partnerID=40&md5=26f748c6d1c9ec944e20fa36aff330ba","Research on processing geochemical data and identifying geochemical anomalies has made important progress in recent decades. Fractal/multi-fractal models, compositional data analysis, and machine learning (ML) are three widely used techniques in the field of geochemical data processing. In recent years, ML has been applied to model the complex and unknown multivariate geochemical distribution and extract meaningful elemental associations related to mineralization or environmental pollution. It is expected that ML will have a more significant role in geochemical mapping with the development of big data science and artificial intelligence in the near future. In this study, state-of-the-art applications of ML in identifying geochemical anomalies were reviewed, and the advantages and disadvantages of ML for geochemical prospecting were investigated. More applications are needed to demonstrate the advantage of ML in solving complex problems in the geosciences. © 2017, International Association for Mathematical Geosciences.","Compositional data analysis; Fractal model; Geochemical anomalies; Geochemical prospecting; Machine learning",,2-s2.0-85019550384
"Hornbrook M.C., Goshen R., Choman E., O’Keeffe-Rosetti M., Kinar Y., Liles E.G., Rust K.C.","Early Colorectal Cancer Detected by Machine Learning Model Using Gender, Age, and Complete Blood Count Data",2017,"Digestive Diseases and Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027979337&doi=10.1007%2fs10620-017-4722-8&partnerID=40&md5=19e516f5f4e23e7e44dc30aa309ab99d","Background: Machine learning tools identify patients with blood counts indicating greater likelihood of colorectal cancer and warranting colonoscopy referral. Aims: To validate a machine learning colorectal cancer detection model on a US community-based insured adult population. Methods: Eligible colorectal cancer cases (439 females, 461 males) with complete blood counts before diagnosis were identified from Kaiser Permanente Northwest Region’s Tumor Registry. Control patients (n = 9108) were randomly selected from KPNW’s population who had no cancers, received at ≥1 blood count, had continuous enrollment from 180 days prior to the blood count through 24 months after the count, and were aged 40–89. For each control, one blood count was randomly selected as the pseudo-colorectal cancer diagnosis date for matching to cases, and assigned a “calendar year” based on the count date. For each calendar year, 18 controls were randomly selected to match the general enrollment’s 10-year age groups and lengths of continuous enrollment. Prediction performance was evaluated by area under the curve, specificity, and odds ratios. Results: Area under the receiver operating characteristics curve for detecting colorectal cancer was 0.80 ± 0.01. At 99% specificity, the odds ratio for association of a high-risk detection score with colorectal cancer was 34.7 (95% CI 28.9–40.4). The detection model had the highest accuracy in identifying right-sided colorectal cancers. Conclusions: ColonFlag® identifies individuals with tenfold higher risk of undiagnosed colorectal cancer at curable stages (0/I/II), flags colorectal tumors 180–360 days prior to usual clinical diagnosis, and is more accurate at identifying right-sided (compared to left-sided) colorectal cancers. © 2017, Springer Science+Business Media, LLC.","Area under receiver operating characteristics curve; Blood cell count; Colonoscopy; Colorectal neoplasms; Hemoglobin; Medical informatics computing","adult; aged; Article; blood cell count; colorectal cancer; controlled study; diagnostic accuracy; diagnostic equipment; diagnostic test accuracy study; early cancer; early cancer diagnosis; female; human; machine learning; major clinical study; male; prediction; priority journal; receiver operating characteristic; sex difference; United States; age; algorithm; area under the curve; blood; blood cell count; colonoscopy; Colorectal Neoplasms; computer assisted diagnosis; data mining; early cancer diagnosis; middle aged; odds ratio; pathology; patient referral; predictive value; procedures; register; reproducibility; risk factor; validation study; very elderly; Adult; Age Factors; Aged; Aged, 80 and over; Algorithms; Area Under Curve; Blood Cell Count; Colonoscopy; Colorectal Neoplasms; Data Mining; Diagnosis, Computer-Assisted; Early Detection of Cancer; Female; Humans; Machine Learning; Male; Middle Aged; Odds Ratio; Predictive Value of Tests; Referral and Consultation; Registries; Reproducibility of Results; Risk Factors; ROC Curve; Sex Factors",2-s2.0-85027979337
"Guo P., Liu T., Zhang Q., Wang L., Xiao J., Zhang Q., Luo G., Li Z., He J., Zhang Y., Ma W.","Developing a dengue forecast model using machine learning: A case study in China",2017,"PLoS neglected tropical diseases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032635968&doi=10.1371%2fjournal.pntd.0005973&partnerID=40&md5=f2dcc5a94eaea2b28dfc039c2092672e","BACKGROUND: In China, dengue remains an important public health issue with expanded areas and increased incidence recently. Accurate and timely forecasts of dengue incidence in China are still lacking. We aimed to use the state-of-the-art machine learning algorithms to develop an accurate predictive model of dengue.METHODOLOGY/PRINCIPAL FINDINGS: Weekly dengue cases, Baidu search queries and climate factors (mean temperature, relative humidity and rainfall) during 2011-2014 in Guangdong were gathered. A dengue search index was constructed for developing the predictive models in combination with climate factors. The observed year and week were also included in the models to control for the long-term trend and seasonality. Several machine learning algorithms, including the support vector regression (SVR) algorithm, step-down linear regression model, gradient boosted regression tree algorithm (GBM), negative binomial regression model (NBM), least absolute shrinkage and selection operator (LASSO) linear regression model and generalized additive model (GAM), were used as candidate models to predict dengue incidence. Performance and goodness of fit of the models were assessed using the root-mean-square error (RMSE) and R-squared measures. The residuals of the models were examined using the autocorrelation and partial autocorrelation function analyses to check the validity of the models. The models were further validated using dengue surveillance data from five other provinces. The epidemics during the last 12 weeks and the peak of the 2014 large outbreak were accurately forecasted by the SVR model selected by a cross-validation technique. Moreover, the SVR model had the consistently smallest prediction error rates for tracking the dynamics of dengue and forecasting the outbreaks in other areas in China.CONCLUSION AND SIGNIFICANCE: The proposed SVR model achieved a superior performance in comparison with other forecasting techniques assessed in this study. The findings can help the government and community respond early to dengue epidemics.",,"algorithm; China; climate; dengue; epidemic; forecasting; human; incidence; machine learning; predictive value; procedures; public health; statistical model; temperature; virology; Algorithms; China; Climate; Dengue; Disease Outbreaks; Forecasting; Humans; Incidence; Linear Models; Machine Learning; Predictive Value of Tests; Public Health; Temperature",2-s2.0-85032635968
"Abadi S., Yan W.X., Amar D., Mayrose I.","A machine learning approach for predicting CRISPR-Cas9 cleavage efficiencies and patterns underlying its mechanism of action",2017,"PLoS Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032645372&doi=10.1371%2fjournal.pcbi.1005807&partnerID=40&md5=0b5c5dae75fbd52da257904fee1bb4e2","The adaptation of the CRISPR-Cas9 system as a genome editing technique has generated much excitement in recent years owing to its ability to manipulate targeted genes and genomic regions that are complementary to a programmed single guide RNA (sgRNA). However, the efficacy of a specific sgRNA is not uniquely defined by exact sequence homology to the target site, thus unintended off-targets might additionally be cleaved. Current methods for sgRNA design are mainly concerned with predicting off-targets for a given sgRNA using basic sequence features and employ elementary rules for ranking possible sgRNAs. Here, we introduce CRISTA (CRISPR Target Assessment), a novel algorithm within the machine learning framework that determines the propensity of a genomic site to be cleaved by a given sgRNA. We show that the predictions made with CRISTA are more accurate than other available methodologies. We further demonstrate that the occurrence of bulges is not a rare phenomenon and should be accounted for in the prediction process. Beyond predicting cleavage efficiencies, the learning process provides inferences regarding patterns that underlie the mechanism of action of the CRISPR-Cas9 system. We discover that attributes that describe the spatial structure and rigidity of the entire genomic site as well as those surrounding the PAM region are a major component of the prediction capabilities. © 2017 Abadi et al.",,"guide RNA; algorithm; biology; CRISPR Cas system; gene editing; genetics; human; machine learning; procedures; receiver operating characteristic; Algorithms; Computational Biology; CRISPR-Cas Systems; Gene Editing; Humans; Machine Learning; RNA, Guide; ROC Curve",2-s2.0-85032645372
"Corral-Corral R., Beltrán J.A., Brizuela C.A., Del Rio G.","Systematic identification of machine-learning models aimed to classify critical residues for protein function from protein structure",2017,"Molecules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032654437&doi=10.3390%2fmolecules22101673&partnerID=40&md5=c5ec30aa6306d418a9cf03f448c4b7bc","Protein structure and protein function should be related, yet the nature of this relationship remains unsolved. Mapping the critical residues for protein function with protein structure features represents an opportunity to explore this relationship, yet two important limitations have precluded a proper analysis of the structure-function relationship of proteins: (i) the lack of a formal definition of what critical residues are and (ii) the lack of a systematic evaluation of methods and protein structure features. To address this problem, here we introduce an index to quantify the protein-function criticality of a residue based on experimental data and a strategy aimed to optimize both, descriptors of protein structure (physicochemical and centrality descriptors) and machine learning algorithms, to minimize the error in the classification of critical residues. We observed that both physicochemical and centrality descriptors of residues effectively relate protein structure and protein function, and that physicochemical descriptors better describe critical residues. We also show that critical residues are better classified when residue criticality is considered as a binary attribute (i.e., residues are considered critical or not critical). Using this binary annotation for critical residues 8 models rendered accurate and non-overlapping classification of critical residues, confirming the multi-factorial character of the structure-function relationship of proteins. © 2017 by The Authors. Licensee MDPI, Basel, Switzerland.","Functional residues; Machine learning; Protein structure",,2-s2.0-85032654437
"Tiwari P., Kutum R., Sethi T., Shrivastava A., Girase B., Aggarwal S., Patil R., Agarwal D., Gautam P., Agrawal A., Dash D., Ghosh S., Juvekar S., Mukerji M., Prasher B.","Recapitulation of Ayurveda constitution types by machine learning of phenotypic traits",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030657691&doi=10.1371%2fjournal.pone.0185380&partnerID=40&md5=280479fd79ec8b6dd693aa8f25d118a8","In Ayurveda system of medicine individuals are classified into seven constitution types, “Prakriti”, for assessing disease susceptibility and drug responsiveness. Prakriti evaluation involves clinical examination including questions about physiological and behavioural traits. A need was felt to develop models for accurately predicting Prakriti classes that have been shown to exhibit molecular differences. The present study was carried out on data of phenotypic attributes in 147 healthy individuals of three extreme Prakriti types, from a genetically homogeneous population of Western India. Unsupervised and supervised machine learning approaches were used to infer inherent structure of the data, and for feature selection and building classification models for Prakriti respectively. These models were validated in a North Indian population. Unsupervised clustering led to emergence of three natural clusters corresponding to three extreme Prakriti classes. The supervised modelling approaches could classify individuals, with distinct Prakriti types, in the training and validation sets. This study is the first to demonstrate that Prakriti types are distinct verifiable clusters within a multidimensional space of multiple interrelated phenotypic traits. It also provides a computational framework for predicting Prakriti classes from phenotypic attributes. This approach may be useful in precision medicine for stratification of endophenotypes in healthy and diseased populations. © 2017 Tiwari et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Article; Ayurveda; biological trait; clinical examination; cohort analysis; conceptual framework; controlled study; endophenotype; female; genetic heterogeneity; human; human experiment; India; information processing; machine learning; male; normal human; personalized medicine; phonetics; Prakriti evaluation; social stratification; disease predisposition; phenotype; questionnaire; Disease Susceptibility; Humans; India; Machine Learning; Medicine, Ayurvedic; Phenotype; Precision Medicine; Surveys and Questionnaires",2-s2.0-85030657691
"Maxwell P.I., Popelier P.L.A.","Accurate prediction of the energetics of weakly bound complexes using the machine learning method kriging",2017,"Structural Chemistry",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014025047&doi=10.1007%2fs11224-017-0928-9&partnerID=40&md5=1ef1946b464e2af0808c231a843685de","Here, we extend the system energy prediction approach used in the force field FFLUX (Maxwell et al. Theor Chem Acc 135:195, 2016) to complexes bound by weak intermolecular interactions. The investigation features the first application of the approach to bound complex systems, additionally challenged by investigating complexes held together only weakly, through either a predominant dispersion contribution, or through mixed dispersion and hydrogen-bonding. Our approach uses the interacting quantum atoms (IQA) energy partitioning scheme to obtain the intra-atomic, EintraA, and interatomic, VinterAA', energies, which when summed, compose the molecular energy, EIQAsystem. The EintraA and VinterAA' energies are mapped to the positions of the nuclear coordinates through the machine learning method kriging to build atomic energy models. A model’s quality is established through its ability to accurately predict the atomic and molecular energies of atoms in an external test set. Mean absolute error percentages (MAE%) of 1.5, 1.5, 1.6, 1.0, 2.6 and 1.7% are obtained in recovering the molecular energy for ammonia…benzene, water…benzene, HCN…benzene, methane…benzene, stacked-benzene (C2h) dimer and T-benzene (C2v) dimer complexes, respectively. © 2017, The Author(s).","Complexes; Dispersion; Force field development; IQA; Kriging; Machine learning; QTAIM; Quantum chemical topology (QCT); S22",,2-s2.0-85014025047
"Chemura A., Mutanga O., Dube T.","Separability of coffee leaf rust infection levels with machine learning methods at Sentinel-2 MSI spectral resolutions",2017,"Precision Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007494472&doi=10.1007%2fs11119-016-9495-0&partnerID=40&md5=4da1ae1929cb05f5a933721f35ce8932","Coffee leaf rust (CLR) caused by the fungus Hemileia vastarix is a devastating disease in almost all coffee producing countries and remote sensing approaches have the potential to monitor the disease. This study evaluated the potential of Sentinel-2 band settings for discriminating CLR infection levels at leaf levels. Field spectra were resampled to the band settings of the Sentinel-2, and evaluated using the random forest (RF) and partial least squares discriminant analysis (PLS-DA) algorithms with and without variable optimization. Using all variables, Sentinel-2 Multispectral Imager (MSI)-derived vegetation indices achieved higher overall accuracy of 76.2% when compared to 69.8% obtained using raw spectral bands. Using the RF out-of-bag (OOB) scores, 4 spectral bands and 7 vegetation indices were identified as important variables in CLR discrimination. Using the PLS-DA Variable Importance in Projection (VIP) score, 3 Sentinel-2 spectral bands (B4, B6 and B5) and 5 vegetation indices were found to be important variables. Use of the identified variables improved the CLR discrimination accuracies to 79.4 and 82.5% for spectral bands and indices respectively when discriminated with the RF. Discrimination accuracy slightly increased through variable optimization for PLS-DA using spectral bands (63.5%) and vegetation indices (71.4%). Overall, this study showed the potential of the Sentinel 2 MSI band settings for CLR discrimination as part of crop condition assessment. Nevertheless further studies are required under field conditions. © 2016, Springer Science+Business Media New York.","Disease discrimination; Hemileia vastatrix; Random forest; Red edge; Variable optimization","algorithm; coffee; disease incidence; edge effect; fungal disease; infectious disease; machine learning; optimization; remote sensing; Sentinel; spectral resolution; Fungi; Hemileia; Hemileia vastatrix",2-s2.0-85007494472
"Andreatta M., Jurtz V.I., Kaever T., Sette A., Peters B., Nielsen M.","Machine learning reveals a non-canonical mode of peptide binding to MHC class II molecules",2017,"Immunology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020519372&doi=10.1111%2fimm.12763&partnerID=40&md5=ee68027a1c5e18c204ce5e83eecbc804","MHC class II molecules play a fundamental role in the cellular immune system: they load short peptide fragments derived from extracellular proteins and present them on the cell surface. It is currently thought that the peptide binds lying more or less flat in the MHC groove, with a fixed distance of nine amino acids between the first and last residue in contact with the MHCII. While confirming that the great majority of peptides bind to the MHC using this canonical mode, we report evidence for an alternative, less common mode of interaction. A fraction of observed ligands were shown to have an unconventional spacing of the anchor residues that directly interact with the MHC, which could only be accommodated to the canonical MHC motif either by imposing a more stretched out peptide backbone (an 8mer core) or by the peptide bulging out of the MHC groove (a 10mer core). We estimated that on average 2% of peptides bind with a core deletion, and 0·45% with a core insertion, but the frequency of such non-canonical cores was as high as 10% for certain MHCII molecules. A mutational analysis and experimental validation of a number of these anomalous ligands demonstrated that they could only fit to their MHC binding motif with a non-canonical binding core of length different from nine. This previously undescribed mode of peptide binding to MHCII molecules gives a more complete picture of peptide presentation by MHCII and allows us to model more accurately this event. © 2017 John Wiley & Sons Ltd","deletions; insertions; machine learning; MHC class II; non-canonical binding","major histocompatibility antigen class 2; epitope; HLA antigen class 2; ligand; monoclonal antibody; peptide; protein binding; Article; binding affinity; controlled study; indel mutation; ligand binding; machine learning; molecular model; mutational analysis; priority journal; protein binding; protein motif; validation process; artificial neural network; binding site; biology; chemistry; genetics; human; immunology; metabolism; mutation; protein database; protein domain; structure activity relation; Antibodies, Monoclonal; Binding Sites; Computational Biology; Databases, Protein; Epitopes; Histocompatibility Antigens Class II; Humans; Ligands; Machine Learning; Mutation; Neural Networks (Computer); Peptides; Protein Binding; Protein Interaction Domains and Motifs; Structure-Activity Relationship",2-s2.0-85020519372
"Ganglberger W., Gritsch G., Hartmann M.M., Fürbass F., Perko H., Skupch A., Kluge T.","A comparison of rule-based and machine learning methods for classification of spikes in EEG",2017,"Journal of Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032343447&doi=10.12720%2fjcm.12.10.589-595&partnerID=40&md5=76089dd222b2ce88ecccacbf59e7de49","Diagnosis of epilepsy is based on the analysis of electroencephalogram (EEG) recordings. Essential epileptiform transients in the EEG are spikes, which are commonly marked manually by biomedical technical assistants which is very time-consuming and error-prone. Automatic spike detectors already exist but still have to be improved to better meet the needs of clinical experts. In this paper we discuss different automatic spike detection methods in order to improve the detection performance and to establish a user adjustable sensitivity parameter. The performances of a rule-based system, artificial neural networks (ANN) and random forests are investigated. For this retrospective study, data from an epilepsy-monitoring unit, including 12 patients comprising 130 hours recording time, were collected. The recordings were annotated by medical experts leading to a total of 5582 spikes. An artificial neural network exceeds the alternative methods in classifying the data set and achieves an average detection sensitivity of 44.1% and positive predictive value of 56.2% at a false detection rate of 19.8 per hour. Furthermore, the ANN also performs well in different sensitivity settings, enabling a user adjustable sensitivity parameter which helps the clinical experts to adjust the classifier to handle different application scenarios. © 2017 Journal of Communications.","Artificial neural networks; Automatic; Classification; EEG; Epilepsy; Machine learning; Minority class oversampling; Random forests; Rule-based; Spike detection",,2-s2.0-85032343447
"Chen C., Li K., Ouyang A., Tang Z., Li K.","GPU-accelerated parallel hierarchical extreme learning machine on flink for big data",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018957128&doi=10.1109%2fTSMC.2017.2690673&partnerID=40&md5=03bfcaa1e1db523bfaf8862f15f47896","The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster. © 2013 IEEE.","Big data; deep learning (DL); flink; GPGPU; hierarchical extreme learning machine (H-ELM); parallel","Approximation algorithms; Artificial intelligence; Cache memory; Cluster computing; Clustering algorithms; Computer architecture; Computer graphics; Data handling; Graphics processing unit; Knowledge acquisition; Learning algorithms; Learning systems; Mapping; Network layers; Optimization; Program processors; Computing platform; Computing power; Extreme learning machine; Feed-forward network; Java virtual machines; Large-scale datasets; Partitioning strategies; Universal approximation; Big data",2-s2.0-85018957128
"Yang Z., Zhang T., Lu J., Su Y., Zhang D., Duan Y.","Extreme learning machines for regression based on V-matrix method",2017,"Cognitive Neurodynamics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020647319&doi=10.1007%2fs11571-017-9444-2&partnerID=40&md5=97f8a360c662475afc68a77f248e47e2","This paper studies the joint effect of V-matrix, a recently proposed framework for statistical inferences, and extreme learning machine (ELM) on regression problems. First of all, a novel algorithm is proposed to efficiently evaluate the V-matrix. Secondly, a novel weighted ELM algorithm called V-ELM is proposed based on the explicit kernel mapping of ELM and the V-matrix method. Though V-matrix method could capture the geometrical structure of training data, it tends to assign a higher weight to instance with smaller input value. In order to avoid this bias, a novel method called VI-ELM is proposed by minimizing both the regression error and the V-matrix weighted error simultaneously. Finally, experiment results on 12 real world benchmark datasets show the effectiveness of our proposed methods. © 2017, Springer Science+Business Media B.V.","Extreme learning machine; Regression; V matrix","experimental model; joint; machine learning; statistical model",2-s2.0-85020647319
"Bendtsen C., Degasperi A., Ahlberg E., Carlsson L.","Improving machine learning in early drug discovery",2017,"Annals of Mathematics and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015700336&doi=10.1007%2fs10472-017-9541-2&partnerID=40&md5=e066c9f14197fe14227828f9f39fc5b5","The high cost for new medicines is hindering their development and machine learning is therefore being used to avoid carrying out physical experiments. Here, we present a comparison between three different machine learning approaches in a classification setting where learning and prediction follow a teaching schedule to mimic the drug discovery process. The approaches are standard SVM classification, SVM based multi-kernel classification and SVM classification based on learning using privileged information. Our two main conclusions are derived using experimental in-vitro data and compound structure descriptors. The in-vitro data is assumed to i) be completely absent in the standard SVM setting, ii) be available at all times when applying multi-kernel learning, or iii) be available as privileged information during training only. The structure descriptors are always available. One conclusion is that multi-kernel learning has higher odds than standard SVM in producing higher accuracy. The second is that learning using privileged information does not have higher odds than the standard SVM, although it may improve accuracy when the training sets are small. © 2017, Springer International Publishing Switzerland.","Human microsome clearance; Multi-kernel learning; Privileged information; Support vector machine; SVM+",,2-s2.0-85015700336
"Oneto L., Fumeo E., Clerico G., Canepa R., Papa F., Dambra C., Mazzino N., Anguita D.","Dynamic delay predictions for large-scale railway networks: Deep and shallow extreme learning machines tuned via thresholdout",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019023963&doi=10.1109%2fTSMC.2017.2693209&partnerID=40&md5=1209cfebc30e0d0b3d1c62d53f71b655","Current train delay (TD) prediction systems do not take advantage of state-of-the-art tools and techniques for handling and extracting useful and actionable information from the large amount of endogenous (i.e., generated by the railway system itself) and exogenous (i.e., related to railway operation but generated by external phenomena) data available. Additionally, they are not designed in order to deal with the intrinsic time varying nature of the problem (e.g., regular changes in the nominal timetable, etc.). The purpose of this paper is to build a dynamic data-driven TD prediction system that exploits the most recent tools and techniques in the field of time varying big data analysis. In particular, we map the TD prediction problem into a time varying multivariate regression problem that allows exploiting both historical data about the train movements and exogenous data about the weather provided by the national weather services. The performance of these methods have been tuned through the state-of-the-art thresholdout technique, a very powerful procedure which relies on the differential privacy theory. Finally, the performance of two efficient implementations of shallow and deep extreme learning machines that fully exploit the recent in-memory large-scale data processing technologies have been compared with the current state-of-the-art TD prediction systems. Results on real-world data coming from the Italian railway network show that the proposal of this paper is able to remarkably improve the state-of-the-art systems. © 2013 IEEE.","Apache Spark; big data; deep extreme learning machine (DELM); delay prediction; dynamic varying systems; in-memory computing; intelligent transportation systems; model selection (MS); railway; shallow extreme learning machine (SELM); thresholdout","Data handling; Forecasting; Knowledge acquisition; Learning systems; Railroad transportation; Railroads; Regression analysis; Transportation; Differential privacies; Efficient implementation; Extreme learning machine; Large-scale data processing; Multivariate regression; National Weather Services; State-of-the-art system; Tools and techniques; Big data",2-s2.0-85019023963
"Wang S., Liu Q., Zhu E., Yin J., Zhao W.","MST-GEN: An Efficient Parameter Selection Method for One-Class Extreme Learning Machine",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020417169&doi=10.1109%2fTCYB.2017.2707463&partnerID=40&md5=1e177b2a1f100ed6c949e7cb8f184679","One-class classification (OCC) models a set of target data from one class to detect outliers. OCC approaches like one-class support vector machine (OCSVM) and support vector data description (SVDD) have wide practical applications. Recently, one-class extreme learning machine (OCELM), which inherits the fast learning speed of original ELM and achieves equivalent or higher data description performance than OCSVM and SVDD, is proposed as a promising alternative. However, OCELM faces the same thorny parameter selection problem as OCSVM and SVDD. It significantly affects the performance of OCELM and remains under-explored. This paper proposes minimal spanning tree (MST)-GEN, an automatic way to select proper parameters for OCELM. Specifically, we first build a n-round MST to model the structure and distribution of the given target set. With information from n-round MST, a controllable number of pseudo outliers are generated by edge pattern detection and a novel 'repelling' process, which readily overcomes two fundamental problems in previous outlier generation methods: where and how many pseudo outliers should be generated. Unlike previous methods that only generate pseudo outliers, we further exploit n-round MST to generate pseudo target data, so as to avoid the time-consuming cross-validation process and accelerate the parameter selection. Extensive experiments on various datasets suggest that the proposed method can select parameters for OCELM in a highly efficient and accurate manner when compared with existing methods, which enables OCELM to achieve better OCC performance in OCC applications. Furthermore, our experiments show that MST-GEN can also be favorably applied to other prevalent OCC methods like OCSVM and SVDD. © 2013 IEEE.","Extreme learning machine (ELM); one-class classification (OCC); parameter selection","Data description; Image retrieval; Image segmentation; Knowledge acquisition; Learning systems; Pattern recognition; Statistics; Cross validation; Extreme learning machine; Minimal spanning tree; One-class Classification; One-class support vector machines (OCSVM); Outlier generations; Parameter selection; Support vector data description; Equivalence classes",2-s2.0-85020417169
"Cervellera C., Maccio D.","An Extreme Learning Machine Approach to Density Estimation Problems",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009959399&doi=10.1109%2fTCYB.2017.2648261&partnerID=40&md5=5853dd0e29820afb133b6637a6abff43","In this paper, we discuss how the extreme learning machine (ELM) framework can be effectively employed in the unsupervised context of multivariate density estimation. In particular, two algorithms are introduced, one for the estimation of the cumulative distribution function underlying the observed data, and one for the estimation of the probability density function. The algorithms rely on the concept of F-discrepancy, which is closely related to the Kolmogorov-Smirnov criterion for goodness of fit. Both methods retain the key feature of the ELM of providing the solution through random assignment of the hidden feature map and a very light computational burden. A theoretical analysis is provided, discussing convergence under proper hypotheses on the chosen activation functions. Simulation tests show how ELMs can be successfully employed in the density estimation framework, as a possible alternative to other standard methods. © 2013 IEEE.","Density estimation; extreme learning machine (ELM); F-discrepancy; unsupervised learning","Activation analysis; Distribution functions; Knowledge acquisition; Learning systems; Probability distributions; Activation functions; Computational burden; Cumulative distribution function; Density estimation; Extreme learning machine; Kolmogorov-Smirnov; Multivariate density estimation; Random assignment; Probability density function",2-s2.0-85009959399
"Yang L., Yang S., Li S., Liu Z., Jiao L.","Incremental laplacian regularization extreme learning machine for online learning",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021253255&doi=10.1016%2fj.asoc.2017.05.051&partnerID=40&md5=5218ac671ae70a1656279ceb83e8762f","The past decade has witnessed an explosive growth of data streams in Internet, biometrics, remote sensing and other fields. Nowadays many supervised incremental/online learning approaches have been developed, to avoid retraining and reduce the computational complexity when data come chunk by chunk. However, these methods can't obtain satisfied performance when the labeled samples are limited. In this paper, we propose an Incremental Laplacian Regularization Extreme Learning Machine (ILR-ELM) for semi-supervised online learning, by utilizing both labeled and unlabeled samples. Unlike most of the existing semi-supervised incremental/online learning algorithms, this paper not only proposes incremental/online learning mechanism for data chunk containing both labeled and unlabeled samples but also proposes incremental/online learning mechanism for data chunk containing only unlabeled samples. The latter case is more common in practical applications because there is usually no enough time to label the samples for continuously arriving data stream. The alternative analytical solutions of ILR-ELM for the two incremental/online learning mechanisms are also presented. The performance of ILR-ELM is evaluated on three benchmark machine learning data, and the results show that it can achieve near accurate and robust classification/regression with a small number of labeled data, and outperforms the incremental/online learning approaches. Compared with the supervised and the comparative semi-supervised incremental/online learning methods, the generalization accuracy of ILR-ELM is increased by nearly 9% and 2% respectively for the classification problem. For the regression problem, the generalization error of ILR-ELM is reduced by nearly 14% than the supervised incremental/online learning methods and 1% than compared semi-supervised incremental/online learning methods Furthermore, ILR-ELM can achieve comparable prediction to semi-supervised batch learning, using less time and Random Access Memory (RAM). © 2017 Elsevier B.V.","Extreme learning machine; Graph laplacian; Incremental laplacian regularization; Semi-supervised incremental/online learning","Benchmarking; Data communication systems; E-learning; Knowledge acquisition; Laplace transforms; Learning algorithms; Learning systems; Random access storage; Extreme learning machine; Generalization accuracy; Generalization Error; Graph Laplacian; Laplacian regularizations; Random access memory; Robust classification; Semi-supervised; Education",2-s2.0-85021253255
"Qian K., Zhang Z., Baird A., Schuller B.","Active learning for bird sound classification via a kernel-based extreme learning machine",2017,"Journal of the Acoustical Society of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031797962&doi=10.1121%2f1.5004570&partnerID=40&md5=bc1dd937c0d6d069e2f96d323e150b08","In recent years, research fields, including ecology, bioacoustics, signal processing, and machine learning, have made bird sound recognition a part of their focus. This has led to significant advancements within the field of ornithology, such as improved understanding of evolution, local biodiversity, mating rituals, and even the implications and realities associated to climate change. The volume of unlabeled bird sound data is now overwhelming, and comparatively little exploration is being made into methods for how best to handle them. In this study, two active learning (AL) methods are proposed, sparse-instance-based active learning (SI-AL), and least-confidence-score-based active learning (LCS-AL), both effectively reducing the need for expert human annotation. To both of these AL paradigms, a kernel-based extreme learning machine (KELM) is then integrated, and a comparison is made to the conventional support vector machine (SVM). Experimental results demonstrate that, when the classifier capacity is improved from an unweighted average recall of 60%-80%, KELM can outperform SVM even when a limited proportion of human annotations are used from the pool of data in both cases of SI-AL (minimum 34.5% vs minimum 59.0%) and LCS-AL (minimum 17.3% vs minimum 28.4%). © 2017 Acoustical Society of America.",,"Aluminum; Artificial intelligence; Biodiversity; Birds; Climate change; Knowledge acquisition; Signal processing; Support vector machines; Active Learning; Confidence score; Extreme learning machine; Human annotations; Research fields; Sound classification; Sound data; Sound recognition; Learning systems",2-s2.0-85031797962
"Shen Q., Ban X., Liu R., Wang Y.","Decay-weighted extreme learning machine for balance and optimization learning",2017,"Machine Vision and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014104728&doi=10.1007%2fs00138-017-0828-4&partnerID=40&md5=8d9f1caeebc90f696630333fd2db4d66","The original extreme learning machine (ELM) was designed for the balanced data, and it balanced misclassification cost of every sample to get the solution. Weighted extreme learning machine assumed that the balance can be achieved through the equality of misclassification costs. This paper improves previous weighted ELM with decay-weight matrix setting for balance and optimization learning. The decay-weight matrix is based on the sample number of each class, but the weight sum values of each class are not necessarily equal. When the number of samples is reduced, the weight sum is also reduced. By adjusting the decaying velocity, classifier could achieve more appropriate boundary position. From the experimental results, the decay-weighted ELM obtains the better effects in solving the imbalance classification tasks, particularly in multiclass tasks. This method was successfully applied to build the prediction model in the urban traffic congestion prediction system. © 2017, Springer-Verlag Berlin Heidelberg.","Extreme learning machine; Multiclass classification; Weighted extreme learning machine","Classification (of information); Decay (organic); Knowledge acquisition; Matrix algebra; Motor transportation; Traffic congestion; Boundary positions; Classification tasks; Extreme learning machine; Misclassification costs; Multi-class classification; Multi-class tasks; Number of samples; Urban traffic congestion; Learning systems",2-s2.0-85014104728
"Arevalillo J.M., Sztein M.B., Kotloff K.L., Levine M.M., Simon J.K.","Identification of immune correlates of protection in Shigella infection by application of machine learning",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028443503&doi=10.1016%2fj.jbi.2017.08.005&partnerID=40&md5=8074152c0d769e0035de5d3de96e8d3f","Background Immunologic correlates of protection are important in vaccine development because they give insight into mechanisms of protection, assist in the identification of promising vaccine candidates, and serve as endpoints in bridging clinical vaccine studies. Our goal is the development of a methodology to identify immunologic correlates of protection using the Shigella challenge as a model. Methods The proposed methodology utilizes the Random Forests (RF) machine learning algorithm as well as Classification and Regression Trees (CART) to detect immune markers that predict protection, identify interactions between variables, and define optimal cutoffs. Logistic regression modeling is applied to estimate the probability of protection and the confidence interval (CI) for such a probability is computed by bootstrapping the logistic regression models. Results The results demonstrate that the combination of Classification and Regression Trees and Random Forests complements the standard logistic regression and uncovers subtle immune interactions. Specific levels of immunoglobulin IgG antibody in blood on the day of challenge predicted protection in 75% (95% CI 67–86). Of those subjects that did not have blood IgG at or above a defined threshold, 100% were protected if they had IgA antibody secreting cells above a defined threshold. Comparison with the results obtained by applying only logistic regression modeling with standard Akaike Information Criterion for model selection shows the usefulness of the proposed method. Conclusion Given the complexity of the immune system, the use of machine learning methods may enhance traditional statistical approaches. When applied together, they offer a novel way to quantify important immune correlates of protection that may help the development of vaccines. © 2017 Elsevier Inc.","Classification and Regression Trees; Correlate of protection; Logistic regression; Random Forests algorithm; Shigella","Antibodies; Artificial intelligence; Blood; Decision trees; Forestry; Learning algorithms; Learning systems; Statistical tests; Vaccines; Classification and regression tree; Correlate of protection; Logistic regressions; Random forests; Shigella; Regression analysis",2-s2.0-85028443503
"Thampi B.V., Wong T., Lukashin C., Loeb N.G.","Determination of CERES TOA fluxes using machine learning algorithms. Part I: Classification and retrieval of CERES cloudy and clear scenes",2017,"Journal of Atmospheric and Oceanic Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032746166&doi=10.1175%2fJTECH-D-16-0183.1&partnerID=40&md5=ecf69b5675c0551235ccb0203aca6fc7","Continuous monitoring of the earth radiation budget (ERB) is critical to the understanding of Earth's climate and its variability with time. The Clouds and the Earth's Radiant Energy System (CERES) instrument is able to provide a long record of ERB for such scientific studies. This manuscript, which is the first of a two-part paper, describes the new CERES algorithm for improving the clear/cloudy scene classification without the use of coincident cloud imager data. This new CERES algorithm is based on a subset of the modern artificial intelligence (AI) paradigm called machine learning (ML) algorithms. This paper describes the development and application of the ML algorithm known as random forests (RF), which is used to classify CERES broadband footprint measurements into clear and cloudy scenes. Results from the RF analysis carried using the CERES Single Scanner Footprint (SSF) data for January and July are presented in the manuscript. The daytime RF misclassification rate (MCR) shows relatively large values (> 30%) for snow, sea ice, and bright desert surface types, while lower values (< 10%) for the forest surface type. MCR values observed for the nighttime data in general show relatively larger values for most of the surface types compared to the daytime MCR values. The modified MCR values show lower values (< 4%) for most surface types after thin cloud data are excluded from the analysis. Sensitivity analysis shows that the number of input variables and decision trees used in the RF analysis has a substantial influence on determining the classification error. © 2017 American Meteorological Society.","Atmosphere; Clouds; Radiative fluxes; Remote sensing; Statistical technique","Artificial intelligence; Benchmarking; Budget control; Clouds; Decision trees; Earth (planet); Earth atmosphere; Image enhancement; Learning systems; Remote sensing; Sea ice; Sensitivity analysis; Classification errors; Clouds and the Earth's radiant energy systems; Development and applications; Earth radiation Budget; Misclassification rates; Radiative fluxes; Single scanner footprints; Statistical techniques; Learning algorithms",2-s2.0-85032746166
"Roy K., Choudhary A., Jayapradha J.","Product recommendations using data mining and machine learning algorithms",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031755768&partnerID=40&md5=56630d04eba586c6a5dd057fb7ea3215","Data Mining is a cross-disciplinary field that concentrates on discovering properties of data sets. There are different approaches to discovering properties of data sets and Machine Learning is one of them. Machine Learning is a sub-field of data science that focuses on designing algorithms that can learn from and make predictions on the data. With the increase in the demand for the e-commerce websites, lots of information arises due to which the users face difficulty in finding the relevant information matching their preferences. Thus, we represent a system which will recommend similar food products to the user based on his purchase. The Food Product will be recommended based on the day to day health diseases of the user. The user profile is formed in which health complication of the user is there. The dataset for Recommendation System comprises of 2075 food items. We will apply K-nutrient algorithm to realize the Recommendation System. We will also implement Machine Learning algorithms such as Support Vector Machine (SVM) and Random Forest. In addition to this, the comparison between SVM and Random Forest is performed and SVM outperforms Random Forest algorithm as it shows an increase in the performance. © 2006-2017 Asian Research Publishing Network (ARPN).","Collaborative filtering; Health hazard; Random forest; Recommendation system; Support vector machine; User profile",,2-s2.0-85031755768
"Zuo W., Wang F., Zhang D., Lin L., Huang Y., Meng D., Zhang L.","Distance Metric Learning via Iterated Support Vector Machines",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023625275&doi=10.1109%2fTIP.2017.2725578&partnerID=40&md5=142cbc139579d020854b8ef0979d3323","Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while most existing methods are based on customized optimizers and become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem with the positive semi-definite constraint, and solve it by iterated training of support vector machines (SVMs). The new formulation is easy to implement and efficient in training with the off-the-shelf SVM solvers. Two novel metric learning models, namely positive-semidefinite constrained metric learning (PCML) and nonnegative-coefficient constrained metric learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experiments are conducted on general classification, face verification, and person re-identification to evaluate our methods. Compared with the state-of-the-art approaches, our methods can achieve comparable classification accuracy and are efficient in training. © 1992-2012 IEEE.","alternating minimization; kernel method; Lagrange duality; Metric learning; support vector machine","Classification (of information); Learning systems; Measurements; Optimization; Personnel training; Support vector machines; Vectors; Alternating minimization; Face; Kernel; Kernel methods; Lagrange duality; Metric learning; Education",2-s2.0-85023625275
"Uçar M.K., Bozkurt M.R., Bilgin C., Polat K.","Automatic detection of respiratory arrests in OSA patients using PPG and machine learning techniques",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991619029&doi=10.1007%2fs00521-016-2617-9&partnerID=40&md5=df5f86a0a70a0eadb90537985109a71a","Obstructive sleep apnea is a syndrome which is characterized by the decrease in air flow or respiratory arrest depending on upper respiratory tract obstructions recurring during sleep and often observed with the decrease in the oxygen saturation. The aim of this study was to determine the connection between the respiratory arrests and the photoplethysmography (PPG) signal in obstructive sleep apnea patients. Determination of this connection is important for the suggestion of using a new signal in diagnosis of the disease. Thirty-four time-domain features were extracted from the PPG signal in the study. The relation between these features and respiratory arrests was statistically investigated. The Mann–Whitney U test was applied to reveal whether this relation was incidental or statistically significant, and 32 out of 34 features were found statistically significant. After this stage, the features of the PPG signal were classified with k-nearest neighbors classification algorithm, radial basis function neural network, probabilistic neural network, multilayer feedforward neural network (MLFFNN) and ensemble classification method. The output of the classifiers was considered as apnea and control (normal). When the classifier results were compared, the best performance was obtained with MLFFNN. Test accuracy rate is 97.07 % and kappa value is 0.93 for MLFFNN. It has been concluded with the results obtained that respiratory arrests can be recognized through the PPG signal and the PPG signal can be used for the diagnosis of OSA. © 2016, The Natural Computing Applications Forum.","Biomedical signal classification; Digital signal processing; Ensemble classification methods; Mann–Whitney U test; Neural network; Obstructive sleep apnea; Photoplethysmography; Statistical signal processing","Artificial intelligence; Diagnosis; Digital signal processing; Feedforward neural networks; Learning algorithms; Learning systems; Nearest neighbor search; Neural networks; Photoplethysmography; Radial basis function networks; Signal processing; Time domain analysis; Two phase flow; Biomedical signal; Ensemble classification; Obstructive sleep apnea; Statistical signal processing; Whitney; Sleep research",2-s2.0-84991619029
"Aldwairi M., Hasan M., Balbahaith Z.","Detection of drive-by download attacks using machine learning approach",2017,"International Journal of Information Security and Privacy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028690822&doi=10.4018%2fIJISP.2017100102&partnerID=40&md5=b85354d237b1352be8e0dfe209804cae","Drive-by download refers to attacks that automatically download malwares to user's computer without his knowledge or consent. This type of attack is accomplished by exploiting web browsers and plugins vulnerabilities. The damage may include data leakage leading to financial loss. Traditional antivirus and intrusion detection systems are not efficient against such attacks. Researchers proposed plenty of detection approaches mostly passive blacklisting. However, a few proposed dynamic classification techniques, which suffer from clear shortcomings. In this paper, we propose a novel approach to detect drive-by download infected web pages based on extracted features from their source code. We test 23 different machine learning classifiers using data set of 5435 webpages and based on the detection accuracy we selected the top five to build our detection model. The approach is expected to serve as a base for implementing and developing anti drive-by download programs. We develop a graphical user interface program to allow the end user to examine the URL before visiting the website. The Bagged Trees classifier exhibited the highest accuracy of 90.1% and reported 96.24% true positive and 26.07% false positive rate. Copyright © 2017, IGI Global.","Browser Exploits; Drive-by Downloads; Malware Detection; Plugin Exploits; URL Classification; Web Client Exploits","Artificial intelligence; Classification (of information); Computer crime; Digital storage; Graphical user interfaces; Learning systems; Losses; Malware; Statistical tests; User interfaces; Web browsers; Websites; Browser Exploits; Drive-by downloads; Malware detection; Plug-ins; Web clients; Intrusion detection",2-s2.0-85028690822
"Than J.C.M., Saba L., Noor N.M., Rijal O.M., Kassim R.M., Yunus A., Suri H.S., Porcu M., Suri J.S.","Lung disease stratification using amalgamation of Riesz and Gabor transforms in machine learning framework",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027526771&doi=10.1016%2fj.compbiomed.2017.08.014&partnerID=40&md5=c199067fd1793034c2d5f18eafaee780","Lung disease risk stratification is important for both diagnosis and treatment planning, particularly in biopsies and radiation therapy. Manual lung disease risk stratification is challenging because of: (a) large lung data sizes, (b) inter- and intra-observer variability of the lung delineation and (c) lack of feature amalgamation during machine learning paradigm. This paper presents a two stage CADx cascaded system consisting of: (a) semi-automated lung delineation subsystem (LDS) for lung region extraction in CT slices followed by (b) morphology-based lung tissue characterization, thereby addressing the above shortcomings. LDS primarily uses entropy-based region extraction while ML-based lung characterization is mainly based on an amalgamation of directional transforms such as Riesz and Gabor along with texture-based features comprising of 100 greyscale features using the K-fold cross-validation protocol (K = 2, 3, 5 and 10). The lung database consisted of 96 patients: 15 normal and 81 diseased. We use five high resolution Computed Tomography (HRCT) levels representing different anatomy landmarks where disease is commonly seen. We demonstrate the amalgamated ML stratification accuracy of 99.53%, an increase of 2% against the conventional non-amalgamation ML system that uses alone Riesz-based feature embedded with feature selection based on feature strength. The robustness of the system was determined based on the reliability and stability that showed a reliability index of 0.99 and the deviation in risk stratification accuracies less than 5%. Our CADx system shows 10% better performance when compared against the mean of five other prominent studies available in the current literature covering over one decade. © 2017","Amalgamation; CADx cascaded system; Computer tomography; Gabor transforms; Lung cancer; Performance; Riesz transforms; Risk stratification","Artificial intelligence; Biological organs; Computer aided diagnosis; Computerized tomography; Diagnosis; Disease control; Extraction; Learning systems; Metals; Radiotherapy; Tomography; Amalgamation; Cascaded system; Gabor transform; Lung Cancer; Performance; Riesz transform; Risk stratification; Diseases",2-s2.0-85027526771
"Kuppili V., Biswas M., Sreekumar A., Suri H.S., Saba L., Edla D.R., Marinhoe R.T., Sanches J.M., Suri J.S.","Extreme Learning Machine Framework for Risk Stratification of Fatty Liver Disease Using Ultrasound Tissue Characterization",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028048475&doi=10.1007%2fs10916-017-0797-1&partnerID=40&md5=9d903475340226c16a82ea15a57ad29d","Fatty Liver Disease (FLD) is caused by the deposition of fat in liver cells and leads to deadly diseases such as liver cancer. Several FLD detection and characterization systems using machine learning (ML) based on Support Vector Machines (SVM) have been applied. These ML systems utilize large number of ultrasonic grayscale features, pooling strategy for selecting the best features and several combinations of training/testing. As result, they are computationally intensive, slow and do not guarantee high performance due to mismatch between grayscale features and classifier type. This study proposes a reliable and fast Extreme Learning Machine (ELM)-based tissue characterization system (a class of Symtosis) for risk stratification of ultrasound liver images. ELM is used to train single layer feed forward neural network (SLFFNN). The input-to-hidden layer weights are randomly generated reducing computational cost. The only weights to be trained are hidden-to-output layer which is done in a single pass (without any iteration) making ELM faster than conventional ML methods. Adapting four types of K-fold cross-validation (K = 2, 3, 5 and 10) protocols on three kinds of data sizes: S0-original, S4-four splits, S8-sixty four splits (a total of 12 cases) and 46 types of grayscale features, we stratify the FLD US images using ELM and benchmark against SVM. Using the US liver database of 63 patients (27 normal/36 abnormal), our results demonstrate superior performance of ELM compared to SVM, for all cross-validation protocols (K2, K3, K5 and K10) and all types of US data sets (S0, S4, and S8) in terms of sensitivity, specificity, accuracy and area under the curve (AUC). Using the K10 cross-validation protocol on S8 data set, ELM showed an accuracy of 96.75% compared to 89.01% for SVM, and correspondingly, the AUC: 0.97 and 0.91, respectively. Further experiments also showed the mean reliability of 99% for ELM classifier, along with the mean speed improvement of 40% using ELM against SVM. We validated the symtosis system using two class biometric facial public data demonstrating an accuracy of 100%. © 2017, Springer Science+Business Media, LLC.","Extreme learning machine; Fatty liver disease; Grayscale features; Neural network; Performance; Reliability; Support vector machine",,2-s2.0-85028048475
"Zhang Z., Zhao X., Wang G.","FE-ELM: A New Friend Recommendation Model with Extreme Learning Machine",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020284213&doi=10.1007%2fs12559-017-9484-2&partnerID=40&md5=d63ca8c31ffbaa821c0afe9efbdc22f5","Friend recommendation is one of the most popular services in location-based social network (LBSN) platforms, which recommends interested or familiar people to users. Except for the original social property and textual property in social networks, LBSN specially owns the spatial-temporal property. However, none of the existing methods fully utilized all the three properties (i.e., just one or two), which may lead to the low recommendation accuracy. Moreover, these existing methods are usually inefficient. In this paper, we propose a new friend recommendation model to solve the above shortcomings of the existing methods, called feature extraction-extreme learning machine (FE-ELM), where friend recommendation is regarded as a binary classification problem. Classification is an important task in cognitive computation community. First, we use new strategies in our FE-ELM model to extract the spatial-temporal feature, social feature, and textual feature. These features make full use of all above properties of LBSN and ensure the recommendation accuracy. Second, our FE-ELM model also takes advantage of the extreme learning machine (ELM) classifier. ELM has fast learning speed and ensures the recommendation efficiency. Extensive experiments verify the accuracy and efficiency of FE-ELM model. © 2017, Springer Science+Business Media New York.","Classification; Extreme learning machine; Friend recommendation; Location-based social networks","Classification (of information); Efficiency; Feature extraction; Knowledge acquisition; Learning systems; Location based services; Binary classification problems; Extreme learning machine; Friend recommendations; Location-based social networks; Recommendation accuracy; Recommendation efficiency; Social properties; Spatial-temporal features; Recommender systems",2-s2.0-85020284213
"Ke H.","Designing extreme learning machine network structure based on tolerance rough set",2017,"International Journal of Intelligent Information Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028069921&doi=10.4018%2fIJIIT.2017100103&partnerID=40&md5=899f3bb99eeffb5ac255fae72b4876d0","In this paper, we present a new extreme learning machine network structure on the basis of tolerance rough set. The purpose of this paper is to realize the high-efficiency and multi-dimensional ELM network structure. Various published algorithms have been applied to breast cancer datasets, but rough set is a fairly new intelligent technique that applies to predict breast cancer recurrence. We analyze Ljubljana Breast Cancer Dataset, firstly, obtain lower and upper approximations and calculate the accuracy and quality of the classification. The high values of the quality of classification and accuracy prove that the attributes selected can well approximate the classification. Rough sets approach is established to solve the prolem of tolerance. © 2017, IGI Global.","Data Processing; Extreme Learning Machine; Ljubljana Breast Cancer Dataset; Network Structure; Tolerance Rough Set","Classification (of information); Data handling; Data processing; Diseases; Knowledge acquisition; Learning systems; Breast Cancer; Extreme learning machine; High-efficiency; Intelligent techniques; Lower and upper approximations; Multi dimensional; Network structures; Tolerance rough sets; Rough set theory",2-s2.0-85028069921
"Li X., Niu P., Li G., Liu J.","An Adaptive Extreme Learning Machine for Modeling NOx Emission of a 300 MW Circulating Fluidized Bed Boiler",2017,"Neural Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016649555&doi=10.1007%2fs11063-017-9611-9&partnerID=40&md5=8896ac56ded643a8c37b08577ca01cf8","Extreme learning machine (ELM) provides high learning speed, but generalization performance needs to be further improved. Therefore, we propose an adaptive ELM with a relaxation factor λ (A-ELM). In A-ELM, according to the nonlinear degree of actual data, the output layer obtains adaptively 1 - λ rate information through the hidden layer and λ rate information through the input layer. Since the relaxation factor λ is bound up with the input weights and hidden biases of A-ELM, in order to obtain the optimal λ, λ, input weights and hidden biases are obtained together by teaching–learning-based optimization (A-ELM-TLBO). Then, 15 benchmark regression data sets verify the performance of A-ELM-TLBO. Finally, A-ELM-TLBO is applied to set up the mapping relation between NOx emission and operational conditions of a 300 MW circulating fluidized bed (CFB) boiler. Compared with six other models, experimental results show that A-ELM-TLBO has good approximation ability and generalization performance. So, A-ELM-TLBO provides a good basis for tuning CFB boiler operating parameters to reduce NOx emission. © 2017, Springer Science+Business Media New York.","Circulating fluidized bed boiler; Extreme learning machine; NOx emission; Relaxation factor; Teaching–learning-based optimization","Benchmarking; Boilers; Fluidized bed process; Knowledge acquisition; Learning systems; Nitrogen oxides; Pulverized fuel fired boilers; Approximation ability; Circulating fluidized bed boiler; Extreme learning machine; Generalization performance; NOx emissions; Operating parameters; Operational conditions; Relaxation factors; Fluidized beds",2-s2.0-85016649555
"Kang F., Liu J., Li J., Li S.","Concrete dam deformation prediction model for health monitoring based on extreme learning machine",2017,"Structural Control and Health Monitoring",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011705780&doi=10.1002%2fstc.1997&partnerID=40&md5=a46c5e5811a42b90822b516340afb6f5","Structural health monitoring via quantities that can reflect behaviors of concrete dams, like horizontal and vertical displacements, rotations, stresses and strains, seepage, and so forth, is an important method to evaluate operational states of concrete dams correctly and predict the future structural behaviors accurately. Traditionally, statistical model is widely applied in practical engineering for structural health monitoring. In this paper, an extreme learning machine (ELM)-based health monitoring model is proposed for displacement prediction of gravity dams. ELM is one type of feedforward neural networks with a single layer of hidden nodes, where the weights connecting inputs to hidden nodes are randomly assigned. The model can produce good generalization performance and learns faster than networks trained using the back propagation algorithm. The advantages such as easy operating, high prediction accuracy, and fast training speed of the ELM health monitoring model are verified by monitoring data of a real concrete dam. Results are also compared with that of the back propagation neural networks, multiple linear regression, and stepwise regression models for dam health monitoring. Copyright © 2017 John Wiley & Sons, Ltd.","artificial neural networks; dam health monitoring; extreme learning machine; multiple linear regression; stepwise regression","Backpropagation; Backpropagation algorithms; Concrete dams; Concretes; Dams; Elasticity; Feedforward neural networks; Forecasting; Knowledge acquisition; Learning systems; Linear regression; Monitoring; Neural networks; Regression analysis; Structural design; Back propagation neural networks; Dam deformation prediction model; Extreme learning machine; Generalization performance; Health monitoring; Horizontal and vertical displacement; Multiple linear regressions; Stepwise regression; Structural health monitoring",2-s2.0-85011705780
"Alipour M., Harris D.K., Barnes L.E., Ozbulut O.E., Carroll J.","Load-Capacity Rating of Bridge Populations through Machine Learning: Application of Decision Trees and Random Forests",2017,"Journal of Bridge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027266886&doi=10.1061%2f%28ASCE%29BE.1943-5592.0001103&partnerID=40&md5=e591c0e9641e9b252d05d26258b1005c","The functionality of the U.S. transportation infrastructure system is dependent upon the health of an aging network of over 600,000 bridges, and agencies responsible for maintaining these bridges rely on the process of load rating to assess the adequacy of individual structures. This paper presents a new approach for safety screening and load-capacity evaluation of large bridge populations that seeks to uncover heretofore unseen patterns within the National Bridge Inventory database and establish relationships between select bridge attributes and their load-capacity status. Decision-tree and random-forest classification models were trained on the national concrete slab bridge data set of over 40,000 structures. The resulting models were validated on an independent data set and then compared with a number of existing judgment-based schemes found in an extensive survey of the current state of practice in the United States. The proposed approach offers a method that provides guidance for improved allocation of resources by informing maintenance decisions through rapid identification of candidate bridges that require further scrutiny for either possible load restriction or restriction removal. © 2017 American Society of Civil Engineers.","Data-driven; Decision trees; Load posting; Load rating; National Bridge Inventory (NBI); Random forests","Air navigation; Classification (of information); Concrete slabs; Learning systems; Rating; Data driven; Load capacity evaluation; Load ratings; National Bridge Inventory (NBI); National Bridge Inventory database; Random forest classification; Random forests; Transportation infrastructures; Decision trees",2-s2.0-85027266886
"Applin S.","Amazon?s Echo Look: Harnessing the Power of Machine Learning or Subtle Exploitation of Human Vulnerability?",2017,"IEEE Consumer Electronics Magazine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031100592&doi=10.1109%2fMCE.2017.2714273&partnerID=40&md5=89ec34db8375c83fe147e2b2d477fa49","Here we are in 2017, but, at times, it feels as though we are back in the 1950s. Apparently, how women look and present themselves to the world is so crucial that they must sacrifice their privacy, security, and trust to Amazon's algorithms, just to gain societal acceptance for their fashion choices. Or at least, this is how it appears in the Amazon Echo Look video. © 2012 IEEE.",,"Electronics engineering; Electronics industry; Learning systems",2-s2.0-85031100592
"Notomista G., Botsch M.","A machine learning approach for the segmentation of driving maneuvers and its application in autonomous parking",2017,"Journal of Artificial Intelligence and Soft Computing Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019112885&doi=10.1515%2fjaiscr-2017-0017&partnerID=40&md5=7a52a021a24b2163aad3a27d6b4a71f2","A classification system for the segmentation of driving maneuvers and its validation in autonomous parking using a small-scale vehicle are presented in this work. The classifiers are designed to detect points that are crucial for the path-planning task, thus enabling the implementation of efficient autonomous parking maneuvers. The training data set is generated by simulations using appropriate vehicle-dynamics models and the resulting classifiers are validated with the small-scale autonomous vehicle. To achieve both a high classification performance and a classification system that can be implemented on a microcontroller with limited computational resources, a two-stage design process is applied. In a first step an ensemble classifier, the Random Forest (RF) algorithm, is constructed and based on the RF-kernel a General Radial Basis Function (GRBF) classifier is generated. The GRBF-classifier is integrated into the small-scale autonomous vehicle leading to excellent performance in parallel-, cross- and oblique-parking maneuvers. The work shows that segmentation using classifies and open-loop control are an efficient approach in autonomous driving for the implementation of driving maneuvers. © 2017.","Autonomous parking; Ensemble learning; Maneuver segmentation",,2-s2.0-85019112885
"Zhao C., Fang J., Cheng T.C.E., Ji M.","A note on the time complexity of machine scheduling with DeJong's learning effect",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027720544&doi=10.1016%2fj.cie.2017.08.010&partnerID=40&md5=88f4aef4eb60ceee171c8de2408f8ffc","In a recent paper (Ji et al., 2015), the authors provided a fully polynomial-time approximation scheme (FPTAS) for the considered NP-hard problem. Extending this research, the authors in another paper (Ji et al., 2016) gave an FPTAS for a related NP-hard problem. In this note we show that the time complexity of the two FPTASes can be improved. © 2017 Elsevier Ltd","DeJong's learning effect; FPTAS; Scheduling; Time complexity","Computational complexity; Scheduling; FPTAS; Fully polynomial time approximation schemes; Learning effects; Machine scheduling; Time complexity; Polynomial approximation",2-s2.0-85027720544
"Bedbrook C.N., Yang K.K., Rice A.J., Gradinaru V., Arnold F.H.","Machine learning to design integral membrane channelrhodopsins for efficient eukaryotic expression and plasma membrane localization",2017,"PLoS Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032724402&doi=10.1371%2fjournal.pcbi.1005786&partnerID=40&md5=d6c4852f2956f1110288dc25874619f0","There is growing interest in studying and engineering integral membrane proteins (MPs) that play key roles in sensing and regulating cellular response to diverse external signals. A MP must be expressed, correctly inserted and folded in a lipid bilayer, and trafficked to the proper cellular location in order to function. The sequence and structural determinants of these processes are complex and highly constrained. Here we describe a predictive, machine-learning approach that captures this complexity to facilitate successful MP engineering and design. Machine learning on carefully-chosen training sequences made by structure-guided SCHEMA recombination has enabled us to accurately predict the rare sequences in a diverse library of channelrhodopsins (ChRs) that express and localize to the plasma membrane of mammalian cells. These light-gated channel proteins of microbial origin are of interest for neuroscience applications, where expression and localization to the plasma membrane is a prerequisite for function. We trained Gaussian process (GP) classification and regression models with expression and localization data from 218 ChR chimeras chosen from a 118,098-variant library designed by SCHEMA recombination of three parent ChRs. We use these GP models to identify ChRs that express and localize well and show that our models can elucidate sequence and structure elements important for these processes. We also used the predictive models to convert a naturally occurring ChR incapable of mammalian localization into one that localizes well. © 2017 Bedbrook et al.",,,2-s2.0-85032724402
"Takkar S., Singh A., Pandey B.","Application of machine learning algorithms to a well defined clinical problem: Liver disease",2017,"International Journal of E-Health and Medical Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028712421&doi=10.4018%2fIJEHMC.2017100103&partnerID=40&md5=0a9f97e8b3d41f3041473d57880fee3e","Liver diseases represent a major health burden worldwide. Machine learning (ML) algorithms have been extensively used to diagnose liver disease. This study accordingly aims to employ various individual and integrated ML algorithms on distinct liver disease datasets for evaluating the diagnostic performances, to integrate dimensionality reduction method with the ML algorithms for analyzing variation in results, to find the best classification model and to analyze the merits and demerits of these algorithms. KNN and PCA-KNN emerged to be the top individual and integrated models. The study also concluded that one specific algorithm can't show best results for all types of datasets and integrated models not always perform better than the individuals. It is observed that no algorithm is perfect and performance of an algorithm totally depends on the dataset type and structure, its number of observations, its dimensions and the decision boundary. Copyright © 2017, IGI Global.","Classification; Discriminant Analysis; Feature Extraction; Integrated Models; KNN; Liver Diagnosis; PCA; SVM",,2-s2.0-85028712421
"Allende-Cid H.","Distributed machine learning with context-awareness for the regression task",2017,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992388877&doi=10.1007%2f978-3-319-48317-7_19&partnerID=40&md5=985f7233cd28389bdd2c2a225cfb3068","The amount of information available nowadays is almost incalculable, presenting new opportunities to gain insight from this data. In this chapter we present some of the work done in field of Distributed Machine Learning and discuss a problem not often mentioned in the literature. The problem is related when the distributed information comes from different contexts. Different contexts can be defined as the different underlying laws of probability governing the data. This is a problem not always addressed, where the majority of the contributions assume that between distributed sources, there is no difference in the underlying law of probability. In this chapter a distributed regression model is presented that addresses this problem. © Springer International Publishing AG 2017.",,,2-s2.0-84992388877
"Zhu L., Lu C., Dong Z.Y., Hong C.","Imbalance Learning Machine-Based Power System Short-Term Voltage Stability Assessment",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031673422&doi=10.1109%2fTII.2017.2696534&partnerID=40&md5=8649418be72687a194f23b6b94b8a0bf","In terms of machine learning-based power system dynamic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a challenging class imbalance problem. Besides, short-term feature extraction from scarce instability seems extremely difficult for conventional learning machines. Faced with such a dilemma, this paper develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is embedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribution. Cost-sensitive learning is employed to intensify bias toward those scarce yet valuable unstable cases. Furthermore, an incremental learning strategy is put forward for online monitoring, contributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the assessment scheme. © 2005-2012 IEEE.","Class imbalance; cost-sensitive; incremental learning; phasor measurements; shapelets; short-term voltage stability","Electric power system stability; Learning systems; Phasor measurement units; Stability; Class imbalance; Cost-sensitive; Incremental learning; Shapelets; Short-term voltage stability; System stability",2-s2.0-85031673422
"Huang C., Mezencev R., McDonald J.F., Vannberg F.","Open source machine-learning algorithms for the prediction of optimal cancer drug therapies",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032486392&doi=10.1371%2fjournal.pone.0186906&partnerID=40&md5=a9effb8634b1834b663f41bd456ae400","Precision medicine is a rapidly growing area of modern medical science and open source machine-learning codes promise to be a critical component for the successful development of standardized and automated analysis of patient data. One important goal of precision cancer medicine is the accurate prediction of optimal drug therapies from the genomic profiles of individual patient tumors. We introduce here an open source software platform that employs a highly versatile support vector machine (SVM) algorithm combined with a standard recursive feature elimination (RFE) approach to predict personalized drug responses from gene expression profiles. Drug specific models were built using gene expression and drug response data from the National Cancer Institute panel of 60 human cancer cell lines (NCI-60). The models are highly accurate in predicting the drug responsiveness of a variety of cancer cell lines including those comprising the recent NCI-DREAM Challenge. We demonstrate that predictive accuracy is optimized when the learning dataset utilizes all probe-set expression values from a diversity of cancer cell types without pre-filtering for genes generally considered to be “drivers” of cancer onset/progression. Application of our models to publically available ovarian cancer (OC) patient gene expression datasets generated predictions consistent with observed responses previously reported in the literature. By making our algorithm “open source”, we hope to facilitate its testing in a variety of cancer types and contexts leading to community-driven improvements and refinements in subsequent applications. © 2017 Huang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,2-s2.0-85032486392
"Wen L., Bowen C.R., Hartman G.L.","Prediction of short-distance aerial movement of Phakopsora pachyrhizi urediniospores using machine learning",2017,"Phytopathology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032624988&doi=10.1094%2fPHYTO-04-17-0138-FI&partnerID=40&md5=9a9fd23ee8b827c8ddffaece7f7c1d55","Dispersal of urediniospores by wind is the primary means of spread for Phakopsora pachyrhizi, the cause of soybean rust. Our research focused on the short-distance movement of urediniospores from within the soybean canopy and up to 61 m from field-grown rust-infected soybean plants. Environmental variables were used to develop and compare models including the least absolute shrinkage and selection operator regression, zero-inflated Poisson/regular Poisson regression, random forest, and neural network to describe deposition of urediniospores collected in passive and active traps. All four models identified distance of trap from source, humidity, temperature, wind direction, and wind speed as the five most important variables influencing short-distance movement of urediniospores. The random forest model provided the best predictions, explaining 76.1 and 86.8% of the total variation in the passive- and active-trap datasets, respectively. The prediction accuracy based on the correlation coefficient (r) between predicted values and the true values were 0.83 (P < 0.0001) and 0.94 (P < 0.0001) for the passive and active trap datasets, respectively. Overall, multiple machine learning techniques identified the most important variables to make the most accurate predictions of movement of P. pachyrhizi urediniospores short-distance. © 2017, American Phytopathological Society. All rights reserved.",,,2-s2.0-85032624988
"Hu L., Li H., Cai Z., Lin F., Hong G., Chen H., Lu Z.","A new machine-learning method to prognosticate paraquat poisoned patients by combining coagulation, liver, and kidney indices",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031751565&doi=10.1371%2fjournal.pone.0186427&partnerID=40&md5=94c1886d00e78daf6d7ed10b1f0e333a","The prognosis of paraquat (PQ) poisoning is highly correlated to plasma PQ concentration, which has been identified as the most important index in PQ poisoning. This study investigated the predictive value of coagulation, liver, and kidney indices in prognosticating PQ-poisoning patients, when aligned with plasma PQ concentrations. Coagulation, liver, and kidney indices were first analyzed by variance analysis, receiver operating characteristic curves, and Fisher discriminant analysis. Then, a new, intelligent, machine learning-based system was established to effectively provide prognostic analysis of PQ-poisoning patients based on a combination of the aforementioned indices. In the proposed system, an enhanced extreme learning machine wrapped with a grey wolf-optimization strategy was developed to predict the risk status from a pool of 103 patients (56 males and 47 females); of these, 52 subjects were deceased and 51 alive. The proposed method was rigorously evaluated against this real-life dataset, in terms of accuracy, Matthews correlation coefficients, sensitivity, and specificity. Additionally, the feature selection was investigated to identify correlating factors for risk status. The results demonstrated that there were significant differences in the coagulation, liver, and kidney indices between deceased and surviving subjects (p<0.05). Aspartate aminotransferase, prothrombin time, prothrombin activity, total bilirubin, direct bilirubin, indirect bilirubin, alanine aminotransferase, urea nitrogen, and creatinine were the most highly correlated indices in PQ poisoning and showed statistical significance (p<0.05) in predicting PQ-poisoning prognoses. According to the feature selection, the most important correlated indices were found to be associated with aspartate aminotransferase, the aspartate aminotransferase to alanine ratio, creatinine, prothrombin time, and prothrombin activity. The method proposed here showed excellent results that were better than that produced based on blood-PQ concentration alone. These promising results indicated that the combination of these indices can provide a new avenue for prognosticating the outcome of PQ poisoning. © 2017 Hu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,2-s2.0-85031751565
"Han Z., Liu Z., Han J., Vong C.-M., Bu S., Chen C.L.P.","Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features with Structure Preservation on 3-D Meshes",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978906098&doi=10.1109%2fTNNLS.2016.2582532&partnerID=40&md5=c9827bb7ed37366ec7fbd42d6cb90b9b","Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: Global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features. © 2012 IEEE.","3-D mesh; Laplace-Beltrami operator; mesh convolutional deep belief networks (MCDBNs); mesh convolutional restricted Boltzmann machines (MCRBMs)","Convolution; Convolution; Distribution functions; Distribution functions; Mesh generation; Mesh generation; Three dimensional computer graphics; Three dimensional computer graphics; Deep belief networks; Deep belief networks; Discriminative features; Discriminative features; Energy distributions; Energy distributions; Non-rigid transformation; Non-rigid transformation; Restricted boltzmann machine; Restricted boltzmann machine; Shape correspondences; Shape correspondences; Structure preservation; Structure preservation; Structure-preserving; Structure-preserving; MESH networking; MESH networking",2-s2.0-84978906098
"Iwata K., Nakashima T., Anan Y., Ishii N.","Machine learning classification to effort estimation for embedded software development projects",2017,"International Journal of Software Innovation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028091396&doi=10.4018%2fIJSI.2017100102&partnerID=40&md5=534204b2539e7539992a314a3b9fb08e","This paper discusses the effect of classification in estimating the amount of effort (in man-days) associated with code development. Estimating the effort requirements for new software projects is especially important. As outliers are harmful to the estimation, they are excluded from many estimation models. However, such outliers can be identified in practice once the projects are completed, and so they should not be excluded during the creation of models and when estimating the required effort. This paper presents classifications for embedded software development projects using an artificial neural network (ANN) and a support vector machine. After defining the classifications, effort estimation models are created for each class using linear regression, an ANN, and a form of support vector regression. Evaluation experiments are carried out to compare the estimation accuracy of the model both with and without the classifications using 10-fold cross-validation. In addition, the Games-Howell test with one-way analysis of variance is performed to consider statistically significant evidence. Copyright © 2017, IGI Global.","Artificial Neural Network; Classification; Embedded Software; Software Development Process Improvement; Support Vector Regression",,2-s2.0-85028091396
"Toksan M.D., Ank O.A.","Single machine scheduling problems under position-dependent fuzzy learning effect with fuzzy processing times",2017,"Journal of Manufacturing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030111456&doi=10.1016%2fj.jmsy.2017.08.006&partnerID=40&md5=6b7eb1725282fa115923c8e9ead572c0","In this paper, we consider single machine scheduling problems under position-dependent fuzzy learning effect with fuzzy processing times. We study three objectives which are to minimize makespan, total completion time and total weighted completion time. Furthermore, we show that these three problems are polynomially solvable under position-dependent fuzzy learning effects with fuzzy processing times. In order to model the uncertainty of fuzzy model parameters such as processing time and learning effect, we use an approach called likelihood profile that depends on the possibility and necessity measures of fuzzy parameters. For three objective functions, we build Fuzzy Mixed Integer Nonlinear Programming (FMINP) models using dependent chance constrained programming techniques for the same predetermined confidence levels. Furthermore, we present polynomially solvable algorithms for different confidence levels for these problems. © 2017","Chance constrained programming; Fuzzy learning effect; Fuzzy mixed integer nonlinear programming; Fuzzy processing time; Likelihood profile; Scheduling","Computer programming; Integer programming; Machinery; Nonlinear programming; Scheduling; Scheduling algorithms; Chance-constrained programming; Fuzzy learning; Fuzzy processing time; Likelihood profile; Mixed-integer nonlinear programming; Problem solving",2-s2.0-85030111456
"Byrne M.D.","Machine Learning in Health Care",2017,"Journal of Perianesthesia Nursing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029742438&doi=10.1016%2fj.jopan.2017.07.004&partnerID=40&md5=d337b6357edab4d07bd215afba48faa9",[No abstract available],,,2-s2.0-85029742438
"Kruk C., Devercelli M., Huszar V.L.M., Hernández E., Beamud G., Diaz M., Silva L.H.S., Segura A.M.","Classification of Reynolds phytoplankton functional groups using individual traits and machine learning techniques",2017,"Freshwater Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026379891&doi=10.1111%2ffwb.12968&partnerID=40&md5=789271006df39c339916529dab733e5b","The Reynolds Functional Groups (RFG) classification scheme is an informative and widely used method in ecological studies of freshwater phytoplankton. It clusters species with similar traits, as well as common environmental sensitivities and tolerances. However, researchers face the difficulty to classify species into RFG because it relies in expert opinion, taxonomical knowledge and environmental information, which are not always accessible. Thus, a step forward is to build general statistical models to classify species into RFG. Under the hypothesis that an organism's response to environmental conditions determines their functional traits, here represented by the RFG, we predict that morphology and classification into broad taxonomic groups will explain RFG independently from environmental information and expert knowledge. To evaluate the predictive ability of morphological traits (e.g. volume) and taxonomic affiliation (e.g. chroococcal Cyanobacteria) as discriminant variables of RFG, we compiled 1,300 species (264 waterbodies) and applied Random Forest (RF) and Classification and Regression Trees (CART). We divided the data to train the models and test their performance. RF successfully classified species into the 28 RFG (only c. 10% test error) with an average individual RFG success rate of 84.6 (range = 33%–100%). This is a relatively high percentage of success from an ecological point of view. It suggests that the selected variables are able to reconstruct the RFG and represent well environmental preferences, without including information about local environmental conditions as classifiers. Our results reinforce the functional basis of the RFG and support both morphological traits and taxonomic classification as good proxies of phytoplankton responses to environmental conditions. A dichotomous key based on the CART was constructed, and an R code to classify species into the RFG is freely available. This work may help users to classify species into the RFG, including those that were not previously listed in the Reynolds classification system. © 2017 John Wiley & Sons Ltd","Classification and Regression Trees; freshwater ecosystems; morphological traits; Random Forest; taxonomic classification",,2-s2.0-85026379891
"Louie A.K., Balon R., Beresin E.V., Coverdale J.H., Brenner A.M., Guerrero A.P.S., Roberts L.W.","Teaching to See Behaviors - Using Machine Learning?",2017,"Academic Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030661515&doi=10.1007%2fs40596-017-0786-1&partnerID=40&md5=ae228a84424db6cffeb94ffafef7faa9",[No abstract available],,,2-s2.0-85030661515
"Firouzi F., Farahani B., Kahng A.B., Rabaey J.M., Balac N.","Guest Editorial: Alternative Computing and Machine Learning for Internet of Things",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030993838&doi=10.1109%2fTVLSI.2017.2742098&partnerID=40&md5=9b5fda0d7b7f2ea41d7a7787e61f3491",[No abstract available],,,2-s2.0-85030993838
"Campobasso F., Fanizzi A., Bello G., Santamaria N., Corriero A.","A ‘machine learning’ technique for discriminating captive-reared from wild Atlantic bluefin tuna, Thunnus thynnus (Osteichthyes: Scombridae), based on differential fin spine bone resorption",2017,"Fisheries Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019606127&doi=10.1016%2fj.fishres.2017.05.008&partnerID=40&md5=8640bd909f2a5db062d16ad5cb80df35","The Atlantic bluefin tuna (ABFT) fishery is regulated by the International Commission for the Conservation of Atlantic Tunas (ICCAT), which establishes the allowable annual yield and the minimum capture size, and allocates capture quotas to the Contracting Parties. Despite fishery monitoring, a considerable amount of captures escapes ICCAT control. In the Mediterranean Sea, the purse seine fishery supports ABFT farming, a capture-based aquaculture activity that involves catching fish from the wild and rearing them in sea cages for a few months. The first spine of the cranial dorsal fin undergoes a continuous bone remodeling process consisting in old bone (primary bone) resorption and new bone (secondary bone) apposition. A marked increase of spine bone resorption was shown in captive-reared ABFT with respect to wild specimens. In this paper, the Random Forest (RF), a Computer Aided Detection system, was applied to distinguish captive-reared from wild ABFT based on fish age, fish fork length, total surface of spine cross section, and surface of remodeled bone tissue in the spine cross section (sum of reabsorbed bone tissue and secondary cancellous bone). The RF system was also compared to the Logistic Regression method (LR). The percentages of properly classified animals, either wild or captive-reared, with respect to the overall number of animals, i.e. accuracy, was 95.3 ± 2.6% and 79.0 ± 5.1% for RF and LR, respectively. The percentages of the properly classified captive-reared specimens, i.e. sensitivity, were 93.5 ± 3.1% and 75.8 ± 5.3% for RF and LR, respectively. The percentages of the properly classified wild specimens was 96.7 ± 2.2% and 81.4 ± 4.9%, for RF and LR, respectively. The proposed technique appears to be a reliable investigation tool anytime the suspicion arises that illegally caught ABFT are sold as aquaculture products. © 2017 Elsevier B.V.","Atlantic bluefin tuna; Random forest; Spine bone resorption; Supervised classification; Tuna farming",,2-s2.0-85019606127
"Baum A., Scarpa J., Bruzelius E., Tamler R., Basu S., Faghmous J.","Targeting weight loss interventions to reduce cardiovascular complications of type 2 diabetes: a machine learning-based post-hoc analysis of heterogeneous treatment effects in the Look AHEAD trial",2017,"The Lancet Diabetes and Endocrinology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023782858&doi=10.1016%2fS2213-8587%2817%2930176-6&partnerID=40&md5=d1eb626786dda4ef3cfccf12e5241c9c","Background The Action for Health in Diabetes (Look AHEAD) trial investigated whether long-term cardiovascular disease morbidity and mortality could be reduced through a weight loss intervention among people with type 2 diabetes. Despite finding no significant reduction in cardiovascular events on average, it is possible that some subpopulations might have derived benefit. In this post-hoc analysis, we test the hypothesis that the overall neutral average treatment effect in the trial masked important heterogeneous treatment effects (HTEs) from intensive weight loss interventions. Methods We used causal forest modelling, which identifies HTEs, using a random half of the trial data (the training set). We applied Cox proportional hazards models to test the potential HTEs on the remaining half of the data (the testing set). The analysis was deemed exempt from review by the Columbia University Institutional Review Board, Protocol ID# AAAO3003. Findings Between Aug 22, 2001, and April 30, 2004, 5145 patients with type 2 diabetes were enrolled in the Look AHEAD randomised controlled trial, of whom 4901 were included in the The National Institute of Diabetes and Digestive and Kidney Diseases Repository and included in our analyses: 2450 for model development and 2451 in the testing dataset. Baseline HbA1c and self-reported general health distinguished participants who differentially benefited from the intervention. Cox models for the primary composite cardiovascular outcome revealed a number needed to treat of 28·9 to prevent 1 event over 9·6 years among participants with HbA1c 6·8% or higher, or both HbA1c less than 6·8% and Short Form Health Survey (SF-36) general health score of 48 or more (2101 [86%] of 2451 participants in the testing dataset; 167 [16%] of 1046 primary outcome events for intervention vs 205 [19%] of 1055 for control, absolute risk reduction of 3·46%, 95% CI 0·21–6·73%, p=0·038) By contrast, participants with HbA1c less than 6·8% and baseline SF-36 general health score of less than 48 (350 [14%] of 2451 participants in the testing data; 27 [16%] of 171 primary outcome events for intervention vs 15 [8%] of 179 primary outcome events for control) had an absolute risk increase of the primary outcome of 7·41% (0·60 to 14·22, p=0·003). Interpretation Look AHEAD participants with moderately or poorly controlled diabetes (HbA1c 6·8% or higher) and subjects with well controlled diabetes (HbA1c less than 6·8%) and good self-reported health (85% of the overall study population) averted cardiovascular events from a behavioural intervention aimed at weight loss. However, 15% of participants with well controlled diabetes and poor self-reported general health experienced negative effects that rendered the overall study outcome neutral. HbA1c and a short questionnaire on general health might identify people with type 2 diabetes likely to derive benefit from an intensive lifestyle intervention aimed at weight loss. Funding None. © 2017 Elsevier Ltd",,"hemoglobin A1c; adult; aged; angina pectoris; Article; cardiovascular disease; cardiovascular mortality; cardiovascular risk; carotid endarterectomy; cerebrovascular accident; clinical effectiveness; controlled study; coronary artery bypass graft; exercise; female; heart failure; heart infarction; hospital admission; human; lifestyle modification; major clinical study; male; non insulin dependent diabetes mellitus; obesity; percutaneous coronary intervention; peripheral vascular disease; physical activity; post hoc analysis; priority journal; proportional hazards model; random forest; risk reduction; social support; weight loss program",2-s2.0-85023782858
"Mortelmans K.","Data-driven screening for type 2 diabetes: Machine Learning on Belgian Health Expenditure Data",2017,"Tijdschrift voor Bedrijfs- en Verzekeringsgeneeskunde",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032201561&doi=10.1007%2fs12498-017-0142-0&partnerID=40&md5=c395180edc9f2b8f2f8a949d72754395",[No abstract available],,,2-s2.0-85032201561
"Wang S., Li C., Zhao K., Chen H.","Learning to context-aware recommend with hierarchical factorization machines",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019256136&doi=10.1016%2fj.ins.2017.05.015&partnerID=40&md5=c8e731eda375b868cad7231b030cd97f","In recent years, a considerable number of context-aware recommendation methods have been proposed. One such technique, the Factorization Machines (FM) model, estimates interactions among features by working with any real valued feature vector. Although the FM model can efficiently handle arbitrary relationships (i.e., dyad, triad, etc.), it still has its limitations. In real world scenarios, contextual features can be considered as being organized in an understandable and intuitive hierarchy. However, existing the FM model performs poorly with regard to exploiting the hierarchical properties of contextual features during prediction. In this study, we consider the problem of exploiting hierarchical structures to improve recommendation quality and propose a novel two-stage recommendation model called Hierarchical Factorization Machines (HFM). In the first stage of HFM, the proposed model estimates the FM model parameters locally for each tree node and returns the initial predictions at all resolutions. Then, it finely tunes these predictions globally through a tree-structured Markov model. In the second stage, model fitting is achieved through an Expectation-Maximization (EM) algorithm, wherein the generalized Kalman filtering algorithm is used in the inner loop. Extensive experiments on real datasets verify that the proposed model is efficient and effective. © 2017 Elsevier Inc.","Collaborative filtering; Context-aware recommendation; Factorization machines; Hierarchical structure; Kalman filtering; Tree-structured Markov model","Collaborative filtering; Factorization; Forecasting; Forestry; Frequency modulation; Markov processes; Maximum principle; Context-aware recommendations; Factorization machines; Hierarchical structures; Kalman-filtering; Markov model; Kalman filters",2-s2.0-85019256136
"Rhee P.K., Erdenee E., Kyun S.D., Ahmed M.U., Jin S.","Active and semi-supervised learning for object detection with imperfect data",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030532834&doi=10.1016%2fj.cogsys.2017.05.006&partnerID=40&md5=621c23f81a1ef55ac9fe7e2c08e70e25","In this paper, we address the combination of the active learning (AL) and semi-supervised (SSL) learnings, called ASSL, to leverage the strong points of the both learning paradigms for improving the performance of object detection. Considering the pros and cons of the AL and SSL learning methods, ASSL where SSL method provides the incremental improvement of semi-supervised detection performance by combining the concept of diversity imported from AL methods. The proposed method demonstrates outstanding performance compared with state-of-art methods on the challenging Caltech pedestrian detection dataset, reducing the miss rate to 12.2%, which is significantly smaller than current state-of-art. In addition, extensive experiments have been carried out using ILSVRC detection dataset and online evaluation for activity recognition. © 2017 Elsevier B.V.","Active Learning (AL); Convolutional Neural Network (CNN); Deep learning; Object detection; Semi-Supervised Learning (SSL)","Aluminum; Artificial intelligence; Deep learning; Neural networks; Object recognition; Supervised learning; Active Learning; Activity recognition; Convolutional neural network; Detection performance; Incremental improvements; Semi- supervised learning; Semi-supervised learning (SSL); State-of-art methods; Object detection; active learning; Article; artificial intelligence; data analysis; evaluation study; human; information processing; machine learning; online system; pattern recognition; pedestrian; priority journal; semi supervised learning; task performance; vision",2-s2.0-85030532834
"Sharma M., Jayadeva, Soman S., Pant H.","Large-scale minimal complexity machines using explicit feature maps",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020478484&doi=10.1109%2fTSMC.2017.2694321&partnerID=40&md5=8fc442ed1409e76d799ce5f66554a425","Minimal complexity machines (MCMs) are a class of hyperplane classifiers that try to minimize a tight bound on the Vapnik-Chervonenkis dimension. MCMs can be used both in the input space and in a higher dimensional feature space via the kernel trick. MCMs tend to produce very sparse solutions in comparison to support vector machines, often using three to ten times fewer support vectors. However, large datasets present significant challenges in terms of storage and operations on the kernel matrix. In this paper, we present a stochastic subgradient descent solver for large-scale machine learning with the MCM. The proposed approach uses an explicit feature map-based approximation of the kernel, to improve the scalability of the algorithm. © 2013 IEEE.","Fastfood; minimal complexity machines (MCMs); random Fourier features; stochastic gradient descent; Vapnik-Chervonenkis (VC) dimension","Approximation algorithms; Stochastic systems; Support vector machines; Higher dimensional features; Kernel matrices; Large datasets; Large-scale machine learning; Sparse solutions; Subgradient descent; Support vector; Vapnik-Chervonenkis dimensions; Learning systems",2-s2.0-85020478484
"Hatipoglu N., Bilgin G.","Cell segmentation in histopathological images with deep learning algorithms by utilizing spatial relationships",2017,"Medical and Biological Engineering and Computing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014027955&doi=10.1007%2fs11517-017-1630-1&partnerID=40&md5=8f940ab9027e1ed0b8e3c3daa50efff9","In many computerized methods for cell detection, segmentation, and classification in digital histopathology that have recently emerged, the task of cell segmentation remains a chief problem for image processing in designing computer-aided diagnosis (CAD) systems. In research and diagnostic studies on cancer, pathologists can use CAD systems as second readers to analyze high-resolution histopathological images. Since cell detection and segmentation are critical for cancer grade assessments, cellular and extracellular structures should primarily be extracted from histopathological images. In response, we sought to identify a useful cell segmentation approach with histopathological images that uses not only prominent deep learning algorithms (i.e., convolutional neural networks, stacked autoencoders, and deep belief networks), but also spatial relationships, information of which is critical for achieving better cell segmentation results. To that end, we collected cellular and extracellular samples from histopathological images by windowing in small patches with various sizes. In experiments, the segmentation accuracies of the methods used improved as the window sizes increased due to the addition of local spatial and contextual information. Once we compared the effects of training sample size and influence of window size, results revealed that the deep learning algorithms, especially convolutional neural networks and partly stacked autoencoders, performed better than conventional methods in cell segmentation. © 2017, International Federation for Medical and Biological Engineering.","Computer-aided diagnosis systems; Deep learning algorithms; Histopathological images; Segmentation; Spatial relationships","Cells; Computer aided diagnosis; Computer aided instruction; Convolution; Cytology; Deep learning; Deep neural networks; Diseases; Image processing; Learning algorithms; Learning systems; Neural networks; Computer aided diagnosis systems; Computer Aided Diagnosis(CAD); Contextual information; Convolutional neural network; Extracellular structure; Histopathological images; Segmentation accuracy; Spatial relationships; Image segmentation; Article; artificial neural network; cancer cell; cancer grading; computer assisted diagnosis; deep belief network; experimental study; histopathology; human; image processing; image segmentation; learning algorithm; machine learning; priority journal; random forest; stacked autoencoder; support vector machine",2-s2.0-85014027955
"Kanamori T., Fujiwara S., Takeda A.","Robustness of learning algorithms using hinge loss with outlier indicators",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026845058&doi=10.1016%2fj.neunet.2017.07.005&partnerID=40&md5=44b5279e50cf370b1404750751badfe0","We propose a unified formulation of robust learning methods for classification and regression problems. In the learning methods, the hinge loss is used with outlier indicators in order to detect outliers in the observed data. To analyze the robustness property, we evaluate the breakdown point of the learning methods in the situation that the outlier ratio is not necessarily small. Although minimization of the hinge loss with outlier indicators is a non-convex optimization problem, we prove that any local optimal solution of our learning algorithms has the robustness property. The theoretical findings are confirmed in numerical experiments. © 2017 Elsevier Ltd","Breakdown point; Local optima; Non-convex optimization; Support vector machines","Convex optimization; Learning systems; Optimization; Statistics; Support vector machines; Breakdown points; Local optima; Local optimal solution; Nonconvex optimization; Numerical experiments; Regression problem; Robustness properties; Unified formulations; Learning algorithms; Article; classification; data processing; learning algorithm; machine learning; priority journal; problem solving; process optimization; regression analysis; statistics; support vector machine",2-s2.0-85026845058
"Tušar T., Gantar K., Koblar V., Ženko B., Filipič B.","A study of overfitting in optimization of a manufacturing quality control procedure",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020021065&doi=10.1016%2fj.asoc.2017.05.027&partnerID=40&md5=1cf34087a953f2d52f47de7edd76a97f","Quality control of the commutator manufacturing process can be automated by means of a machine learning model that can predict the quality of commutators as they are being manufactured. Such a model can be constructed by combining machine vision, machine learning and evolutionary optimization techniques. In this procedure, optimization is used to minimize the model error, which is estimated using single cross-validation. This work exposes the overfitting that emerges in such optimization. Overfitting is shown for three machine learning methods with different sensitivity to it (trees, additionally pruned trees and random forests) and assessed in two ways (repeated cross-validation and validation on a set of unseen instances). Results on two distinct quality control problems show that optimization amplifies overfitting, i.e., the single cross-validation error estimate for the optimized models is overly optimistic. Nevertheless, minimization of the error estimate by single cross-validation in general results in minimization of the other error estimates as well, showing that optimization is indeed beneficial in this context. © 2017 The Author(s)","Machine learning; Machine vision; Optimization; Overfitting; Quality control","Artificial intelligence; Computer vision; Decision trees; Errors; Forestry; Learning systems; Manufacture; Optimization; Quality assurance; Cross validation errors; Evolutionary Optimization Techniques; Machine learning models; Manufacturing process; Manufacturing quality control; Optimized models; Overfitting; Three machine learning methods; Quality control",2-s2.0-85020021065
"Valerio L., Passarella A., Conti M.","A communication efficient distributed learning framework for smart environments",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027870240&doi=10.1016%2fj.pmcj.2017.07.014&partnerID=40&md5=c4f1533d88ab1f8e0c9f285704bec116","Due to the pervasive diffusion of personal mobile and IoT devices, many “smart environments” (e.g., smart cities and smart factories) will be, among others, generators of huge amounts of data. To provide value-add services in these environments, data will have to be analysed to extract knowledge. Currently, this is typically achieved through centralised cloud-based data analytics services. However, according to many studies, this approach may present significant issues from the standpoint of data ownership, and even wireless network capacity. One possibility to cope with these shortcomings is to move data analytics closer to where data is generated. In this paper we tackle this issue by proposing and analysing a distributed learning framework, whereby data analytics are performed at the edge of the network, i.e., on locations very close to where data is generated. Specifically, in our framework, partial data analytics are performed directly on the nodes that generate the data, or on nodes close by (e.g., some of the data generators can take this role on behalf of subsets of other nodes nearby). Then, nodes exchange partial models and refine them accordingly. Our framework is general enough to host different analytics services. In the specific case analysed in the paper we focus on a learning task, considering two distributed learning algorithms. Using an activity recognition and a pattern recognition task, both on reference datasets, we compare the two learning algorithms between each other and with a central cloud solution (i.e., one that has access to the complete datasets). Our results show that using distributed machine learning techniques, it is possible to drastically reduce the network overhead, while obtaining performance comparable to the cloud solution in terms of learning accuracy. The analysis also shows when each distributed learning approach is preferable, based on the specific distribution of the data on the nodes. © 2017 Elsevier B.V.","Big data; Communications efficiency; Distributed learning; Iot; Smart cities","Big data; Learning systems; Network function virtualization; Pattern recognition; Smart city; Activity recognition; Distributed learning; Distributed learning algorithms; Distributed machine learning; Learning accuracy; Smart environment; Specific distribution; Wireless network capacity; Learning algorithms",2-s2.0-85027870240
"Luo C., Wu D., Wu D.","A deep learning approach for credit scoring using credit default swaps",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008224825&doi=10.1016%2fj.engappai.2016.12.002&partnerID=40&md5=d0948500bc87dade47ed45a1d6d88edf","After 2007–2008 crisis, it is clear that corporate credit scoring is becoming a key role in credit risk management. In this paper, we investigate the performances of credit scoring models applied to CDS data sets. The classification performance of deep learning algorithm such as deep belief networks with Restricted Boltzmann Machines are evaluated and compared with some popular credit scoring models such as logistic regression, multi-layer perceptron and support vector machine. The performance is assessed using the classification accuracy and the area under the receiver operating characteristic curve. It is found that DBN yields the best performance. © 2016 Elsevier Ltd","CDS; Credit scoring; Deep learning; Machine learning","Learning systems; Risk assessment; Risk management; Classification accuracy; Classification performance; Credit risk management; Credit scoring; Deep learning; Multi layer perceptron; Receiver operating characteristic curves; Restricted boltzmann machine; Learning algorithms",2-s2.0-85008224825
"Tsai C.-H., Yu W.-J., Wong W.H., Lee C.-Y.","A 41.3/26.7 pJ per Neuron Weight RBM Processor Supporting On-Chip Learning/Inference for IoT Applications",2017,"IEEE Journal of Solid-State Circuits",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023161329&doi=10.1109%2fJSSC.2017.2715171&partnerID=40&md5=14cb2e4f8e4a6d8d6aa3bc7c63653733","An energy-efficient restricted Boltzmann machine (RBM) processor (RBM-P) supporting on-chip learning and inference is proposed for machine learning and Internet of Things (IoT) applications in this paper. To train a neural network (NN) model, the RBM structure is applied to supervised and unsupervised learning, and a multi-layer NN can be constructed and initialized by stacking multiple RBMs. Featuring NN model reduction for external memory bandwidth saving, low power neuron binarizer (LPNB) with dynamic clock gating and area-efficient NN-like activation function calculators for power reduction, user-defined connection map (UDCM) for both computation time and bandwidth saving, and early stopping (ES) mechanism for learning process, the proposed system integrates 32 RBM cores with maximal 4k neurons per layer and 128 candidates per sample for machine learning applications. Implemented in 65nm CMOS technology, the proposed RBM-P chip costs 2.2 M gates and 128 kB SRAM with 8.8 mm2 area. Operated at 1.2 V and 210 MHz, this chip achieves 7.53G neuron weights (NWs) and 11.63G NWs per second with 41.3 and 26.7 pJ per NW for learning and inference, respectively. © 2017 IEEE.","Low-power design; machine learning; memory bandwidth reduction; non-linear functions; restricted Boltzmann machine (RBM)","Artificial intelligence; Bandwidth; Data structures; Electric load management; Energy efficiency; Functions; Internet of things; Learning systems; Low power electronics; Neural networks; Neurons; Static random access storage; Computational model; Load modeling; Low-power design; Memory bandwidths; Nonlinear functions; Reduced order systems; Restricted boltzmann machine; Education",2-s2.0-85023161329
"Mei N., Grossberg M.D., Ng K., Navarro K.T., Ellmore T.M.","Identifying sleep spindles with multichannel EEG and classification optimization",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028703469&doi=10.1016%2fj.compbiomed.2017.08.030&partnerID=40&md5=e47b6af19d85f4c588d47ffb7c65b3fd","Researchers classify critical neural events during sleep called spindles that are related to memory consolidation using the method of scalp electroencephalography (EEG). Manual classification is time consuming and is susceptible to low inter-rater agreement. This could be improved using an automated approach. This study presents an optimized filter based and thresholding (FBT) model to set up a baseline for comparison to evaluate machine learning models using naïve features, such as raw signals, peak frequency, and dominant power. The FBT model allows us to formally define sleep spindles using signal processing but may miss examples most human scorers would agree are spindles. Machine learning methods in theory should be able to approach performance of human raters but they require a large quantity of scored data, proper feature representation, intensive feature engineering, and model selection. We evaluate both the FBT model and machine learning models with naïve features. We show that the machine learning models derived from the FBT model improve classification performance. An automated approach designed for the current data was applied to the DREAMS dataset [1]. With one of the expert's annotation as a gold standard, our pipeline yields an excellent sensitivity that is close to a second expert's scores and with the advantage that it can classify spindles based on multiple channels if more channels are available. More importantly, our pipeline could be modified as a guide to aid manual annotation of sleep spindles based on multiple channels quickly (6–10 s for processing a 40-min EEG recording), making spindle detection faster and more objective. © 2017 Elsevier Ltd","Machine learning; Memory consolidation; Optimization; Sleep spindle; Thresholding","Artificial intelligence; Biomedical signal processing; Electroencephalography; Electrophysiology; Learning systems; Optimization; Pipeline processing systems; Pipelines; Signal processing; Classification performance; Feature representation; Inter-rater agreements; Machine learning methods; Machine learning models; Memory consolidation; Sleep spindles; Thresholding; Sleep research; Article; automation; classification; electroencephalogram; gold standard; machine learning; mathematical analysis; mathematical model; priority journal; signal processing; sleep spindle",2-s2.0-85028703469
"Deng M., Yang W., Liu Q.","Geographically Weighted Extreme Learning Machine: A Method for Space–Time Prediction",2017,"Geographical Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017109932&doi=10.1111%2fgean.12127&partnerID=40&md5=3a2331b5a6970a51e1658440b3b3f342","Spatial heterogeneity has been regarded as an important issue in space–time prediction. Although some statistical methods of space–time predictions have been proposed to address spatial heterogeneity, the linear assumption makes it difficult for these methods to predict geographical processes accurately because geographical processes always involve complicated nonlinear characteristics. An extreme learning machine (ELM) has the advantage of approximating nonlinear relationships with a rapid learning speed and excellent generalization performance. However, determining how to incorporate spatial heterogeneity into an ELM to predict space–time data is an urgent problem. For this purpose, a new method called geographically weighted ELM (GWELM) is proposed to address spatial heterogeneity based on an ELM in this article. GWELM is essentially a locally varying ELM in which the parameters are regarded as functions of spatial locations, and geographically weighted least squares is applied to estimate the parameters in a local model. The proposed method is used to analyze two groups of different data sets, and the results demonstrate that the GWELM method is superior to the comparative method, which is also developed to address spatial heterogeneity. © 2017 The Ohio State University",,"data set; prediction; spatiotemporal analysis; statistical analysis",2-s2.0-85017109932
"Cui Y., Matsubara T., Sugimoto K.","Kernel dynamic policy programming: Applicable reinforcement learning to robot systems with high dimensional states",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024491109&doi=10.1016%2fj.neunet.2017.06.007&partnerID=40&md5=1408a57def1863b0e73e3f6b67b23d50","We propose a new value function approach for model-free reinforcement learning in Markov decision processes involving high dimensional states that addresses the issues of brittleness and intractable computational complexity, therefore rendering the value function approach based reinforcement learning algorithms applicable to high dimensional systems. Our new algorithm, Kernel Dynamic Policy Programming (KDPP) smoothly updates the value function in accordance to the Kullback–Leibler divergence between current and updated policies. Stabilizing the learning in this manner enables the application of the kernel trick to value function approximation, which greatly reduces computational requirements for learning in high dimensional state spaces. The performance of KDPP against other kernel trick based value function approaches is first investigated in a simulated n DOF manipulator reaching task, where only KDPP efficiently learned a viable policy at n=40. As an application to a real world high dimensional robot system, KDPP successfully learned the task of unscrewing a bottle cap via a Pneumatic Artificial Muscle (PAM) driven robotic hand with tactile sensors; a system with a state space of 32 dimensions, while given limited samples and with ordinary computing resources. © 2017 Elsevier Ltd","Kernel methods; Reinforcement learning; Robot learning","Bottles; Education; Fracture mechanics; Learning algorithms; Learning systems; Markov processes; Robot learning; Robot programming; Robots; Computational requirements; Computing resource; High-dimensional systems; Kernel methods; Markov Decision Processes; Pneumatic artificial muscle; Value function approach; Value function approximation; Reinforcement learning; analytic method; Article; artificial pneumatic hand; computer analysis; controlled study; kernel method; learning algorithm; machine learning; priority journal; robotics; sensor; system analysis; task performance",2-s2.0-85024491109
"Yamazaki K.","Effects of additional data on Bayesian clustering",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026266075&doi=10.1016%2fj.neunet.2017.06.015&partnerID=40&md5=749ae4e0a8aaefa86810a6fa28df9f97","Hierarchical probabilistic models, such as mixture models, are used for cluster analysis. These models have two types of variables: observable and latent. In cluster analysis, the latent variable is estimated, and it is expected that additional information will improve the accuracy of the estimation of the latent variable. Many proposed learning methods are able to use additional data; these include semi-supervised learning and transfer learning. However, from a statistical point of view, a complex probabilistic model that encompasses both the initial and additional data might be less accurate due to having a higher-dimensional parameter. The present paper presents a theoretical analysis of the accuracy of such a model and clarifies which factor has the greatest effect on its accuracy, the advantages of obtaining additional data, and the disadvantages of increasing the complexity. © 2017 Elsevier Ltd","Hierarchical parametric models; Latent variable estimation; Semi-supervised learning; Unsupervised learning","Cluster analysis; Supervised learning; Unsupervised learning; Bayesian clustering; Higher-dimensional; Latent variable; Parametric models; Probabilistic modeling; Probabilistic models; Semi- supervised learning; Transfer learning; Education; algorithm; Article; artificial neural network; Bayesian learning; cluster analysis; error; mathematical computing; measurement accuracy; priority journal; probability; statistical analysis; structural equation modeling; supervised machine learning; unsupervised machine learning",2-s2.0-85026266075
"Arin A., Rabadi G.","Integrating estimation of distribution algorithms versus Q-learning into Meta-RaPS for solving the 0-1 multidimensional knapsack problem",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006847834&doi=10.1016%2fj.cie.2016.10.022&partnerID=40&md5=74cd180cf44c412ef0c48f7ea8817e60","Finding near-optimal solutions in an acceptable amount of time is a challenge when developing sophisticated approximate approaches. A powerful answer to this challenge might be reached by incorporating intelligence into metaheuristics. We propose integrating two methods into Meta-RaPS (Metaheuristic for Randomized Priority Search), which is currently classified as a memoryless metaheuristic. The first method is the Estimation of Distribution Algorithms (EDA), and the second is utilizing a machine learning algorithm known as Q-Learning. To evaluate their performance, the proposed algorithms are tested on the 0-1 Multidimensional Knapsack Problem (MKP). Meta-RaPS EDA appears to perform better than Meta-RaPS Q-Learning. However, both showed promising results compared to other approaches presented in the literature for the 0-1 MKP. © 2016 Elsevier Ltd","0-1 multidimensional knapsack problem; Estimation of distribution algorithms; Machine learning; Meta-RaPS; Q-learning","Artificial intelligence; Combinatorial optimization; Learning systems; 0-1 multidimensional Knapsack problems; Estimation of distribution algorithm (EDA); Estimation of distribution algorithms; Meta heuristics; Meta-RaPS; Metaheuristic; Near-optimal solutions; Q-learning; Learning algorithms",2-s2.0-85006847834
"Wu O., Mao X., Hu W.","Iteratively divide-and-conquer learning for nonlinear classification and ranking",2017,"ACM Transactions on Intelligent Systems and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644092&doi=10.1145%2f3122802&partnerID=40&md5=f91fb8020fc7f80e5200c389e66b48e2","Nonlinear classifiers (i.e., kernel support vector machines (SVMs)) are effective for nonlinear data classification. However, nonlinear classifiers are usually prohibitively expensive when dealing with large nonlinear data. Ensembles of linear classifiers have been proposed to address this inefficiency, which is called the ensemble linear classifiers for nonlinear data problem. In this article, a new iterative learning approach is introduced that involves two steps at each iteration: partitioning the data into clusters according to Gaussian mixture models with local consistency and then training basic classifiers (i.e., linear SVMs) for each cluster. The two divide-and-conquer steps are combined into a graphical model. Meanwhile, with training, each classifier is regarded as a task; clustered multitask learning is employed to capture the relatedness among different tasks and avoid overfitting in each task. In addition, two novel extensions are introduced based on the proposed approach. First, the approach is extended for quality-aware web data classification. In this problem, the types of web data vary in terms of information quality. The ignorance of the variations of information quality of web data leads to poor classification models. The proposed approach can effectively integrate quality-aware factors into web data classification. Second, the approach is extended for listwise learning to rank to construct an ensemble of linear ranking models, whereas most existing listwise ranking methods construct a solely linear ranking model. Experimental results on benchmark datasets show that our approach outperforms state-of-the-art algorithms. During prediction for nonlinear classification, it also obtains comparable classification performance to kernel SVMs, with much higher efficiency. © 2017 ACM.","Classification; Clustering; Divide-and-conquer; Listwise learning to rank; Multi-task learning","Information analysis; Iterative methods; Learning algorithms; Learning systems; Support vector machines; Clustering; Divide and conquer; Divide and conquer learning; Iterative learning approaches; Learning to rank; Multitask learning; State-of-the-art algorithms; Support vector machine (SVMs); Classification (of information)",2-s2.0-85032644092
"Jung W.-S., Yim J., Ko Y.-B.","QGeo: Q-Learning-Based Geographic Ad Hoc Routing Protocol for Unmanned Robotic Networks",2017,"IEEE Communications Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032259482&doi=10.1109%2fLCOMM.2017.2656879&partnerID=40&md5=8fc07444b9e83c94b28a0d794e50ad70","This letter proposes a novel protocol that uses Q-learning-based geographic routing (QGeo) to improve the network performance of unmanned robotic networks. A rapid and reliable network is essential for the remote control and monitoring of mobile robotic devices. However, controlling the network overhead required for route selection and repair is still a notable challenge, owing to high mobility of the devices. To alleviate this problem, we propose a machine-learning-based geographic routing scheme to reduce network overhead in high-mobility scenarios. We evaluate the performance of QGeo in comparison with other methods using the NS-3 simulator. We find that QGeo has a higher packet delivery ratio and a lower network overhead than existing methods. © 1997-2012 IEEE.","Geographic routing; machine learning; Q-learning","Artificial intelligence; Internet protocols; Learning algorithms; Learning systems; Remote control; Robotics; Routing protocols; Ad hoc routing protocol; Control and monitoring; Geographic routing; Network overhead; Packet delivery ratio; Q-learning; Reliable Networks; Robotic networks; Network routing",2-s2.0-85032259482
"Kholghi M., Sitbon L., Zuccon G., Nguyen A.","Active learning reduces annotation time for clinical concept extraction",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026899122&doi=10.1016%2fj.ijmedinf.2017.08.001&partnerID=40&md5=5e4aafa2f9b6190a9eb0fd07c12f0e6c","Objective To investigate: (1) the annotation time savings by various active learning query strategies compared to supervised learning and a random sampling baseline, and (2) the benefits of active learning-assisted pre-annotations in accelerating the manual annotation process compared to de novo annotation. Materials and methods There are 73 and 120 discharge summary reports provided by Beth Israel institute in the train and test sets of the concept extraction task in the i2b2/VA 2010 challenge, respectively. The 73 reports were used in user study experiments for manual annotation. First, all sequences within the 73 reports were manually annotated from scratch. Next, active learning models were built to generate pre-annotations for the sequences selected by a query strategy. The annotation/reviewing time per sequence was recorded. The 120 test reports were used to measure the effectiveness of the active learning models. Results When annotating from scratch, active learning reduced the annotation time up to 35% and 28% compared to a fully supervised approach and a random sampling baseline, respectively. Reviewing active learning-assisted pre-annotations resulted in 20% further reduction of the annotation time when compared to de novo annotation. Discussion The number of concepts that require manual annotation is a good indicator of the annotation time for various active learning approaches as demonstrated by high correlation between time rate and concept annotation rate. Conclusion Active learning has a key role in reducing the time required to manually annotate domain concepts from clinical free text, either when annotating from scratch or reviewing active learning-assisted pre-annotations. © 2017 Elsevier B.V.","Active learning; Annotation time; Clinical free text; Concept extraction; Machine-assisted pre-annotation","Extraction; Active Learning; Annotation time; Concept extraction; Discharge summary; Domain concepts; Free texts; Manual annotation; Query strategies; Artificial intelligence; active learning; annotation; Article; coding; concept analysis; human; human experiment; learning; medical informatics; practice guideline; priority journal; simulation; supervised machine learning; task performance",2-s2.0-85026899122
"Wang D., Zhang M., Li Z., Li J., Fu M., Cui Y., Chen X.","Modulation Format Recognition and OSNR Estimation Using CNN-Based Deep Learning",2017,"IEEE Photonics Technology Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028505466&doi=10.1109%2fLPT.2017.2742553&partnerID=40&md5=74f58272bb7d64fefa7b8c22a4771184","An intelligent eye-diagram analyzer is proposed to implement both modulation format recognition (MFR) and optical signal-to-noise rate (OSNR) estimation by using a convolution neural network (CNN)-based deep learning technique. With the ability of feature extraction and self-learning, CNN can process eye diagram in its raw form (pixel values of an image) from the perspective of image processing, without knowing other eye-diagram parameters or original bit information. The eye diagram images of four commonly-used modulation formats over a wide OSNR range (1025 dB) are obtained from an eye-diagram generation module in oscilloscope combined with the simulation system. Compared with four other machine learning algorithms (decision tress, k-nearest neighbors, back-propagation artificial neural network, and support vector machine), CNN obtains the higher accuracies. The accuracies of OSNR estimation and MFR both attain 100%. The proposed technique has the potential to be embedded in the test instrument to perform intelligent signal analysis or applied for optical performance monitoring. © 1989-2012 IEEE.","convolution neural network (CNN); deep learning; eye diagram; Machine learning; modulation format recognition (MFR); optical performance monitoring (OPM); optical signal-to-noise rate (OSNR)","Artificial intelligence; Backpropagation; Backpropagation algorithms; Convolution; Deep learning; Extraction; Feature extraction; Image processing; Learning algorithms; Learning systems; Nearest neighbor search; Neural networks; Optical data processing; Optical signal processing; Signal to noise ratio; Convolution neural network; Eye diagrams; Kernel; Modulation formats; Optical imaging; Optical noise; Optical performance monitoring; Optical signals; Modulation",2-s2.0-85028505466
"Greene J.D.","The rat-a-gorical imperative: Moral intuition and the limits of affective learning",2017,"Cognition",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016422280&doi=10.1016%2fj.cognition.2017.03.004&partnerID=40&md5=2903e1fb3af1df364c4b5fd2abb3cf16","Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I've argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions. © 2017 Elsevier B.V.","Consequentialism; Deontology; Ethics; Machine learning; Model-free learning; Moral judgment; Normative ethics; Reinforcement learning; Utilitarianism","affective learning; Article; ethics; heuristics; human; intuition; learning; morality; nonhuman; optimism; pessimism; philosophy; priority journal; rat; thinking; trust",2-s2.0-85016422280
"Pratama M., Lu J., Lughofer E., Zhang G., Er M.J.","An Incremental Learning of Concept Drifts Using Evolving Type-2 Recurrent Fuzzy Neural Networks",2017,"IEEE Transactions on Fuzzy Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441894&doi=10.1109%2fTFUZZ.2016.2599855&partnerID=40&md5=c2134edab87d683e9ddd75e9b37bab3e","The age of online data stream and dynamic environments results in the increasing demand of advanced machine learning techniques to deal with concept drifts in large data streams. Evolving fuzzy systems (EFS) are one of recent initiatives from the fuzzy system community to resolve the issue. Existing EFSs are not robust against data uncertainty, temporal system dynamics, and the absence of system order, because a vast majority of EFSs are designed in the type-1 feedforward network architecture. This paper aims to solve the issue of data uncertainty, temporal behavior, and the absence of system order by developing a novel evolving recurrent fuzzy neural network, called evolving type-2 recurrent fuzzy neural network (eT2RFNN). eT2RFNN is constructed in a new recurrent network architecture, featuring double recurrent layers. The new recurrent network architecture evolves a generalized interval type-2 fuzzy rule, where the rule premise is built upon the interval type-2 multivariate Gaussian function, whereas the rule consequent is crafted by the nonlinear wavelet function. The eT2RFNN adopts a holistic concept of evolving systems, where the fuzzy rule can be automatically generated, pruned, merged, and recalled in the single-pass learning mode. eT2RFNN is capable of coping with the problem of high dimensionality because it is equipped with online feature selection technology. The efficacy of eT2RFNN was experimentally validated using artificial and real-world data streams and compared with prominent learning algorithms. eT2RFNN produced more reliable predictive accuracy, while retaining lower complexity than its counterparts. © 2017 IEEE.","Evolving fuzzy systems (EFSs); fuzzy neural networks; incremental learning; recurrent fuzzy neural networks; type-2 fuzzy systems","Fuzzy inference; Fuzzy logic; Fuzzy rules; Fuzzy systems; Knowledge engineering; Learning algorithms; Learning systems; Network architecture; Recurrent neural networks; Automatically generated; Evolving Fuzzy Systems; Incremental learning; Machine learning techniques; Multivariate Gaussian functions; Online feature selection; Recurrent fuzzy neural network; Type-2 fuzzy systems; Fuzzy neural networks",2-s2.0-85032441894
"Sun W., Zheng B., Qian W.","Automatic feature learning using multichannel ROI based on deep structured algorithms for computerized lung cancer diagnosis",2017,"Computers in Biology and Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018458426&doi=10.1016%2fj.compbiomed.2017.04.006&partnerID=40&md5=64b01e25e7cf0850fe37814b2435910b","This study aimed to analyze the ability of extracting automatically generated features using deep structured algorithms in lung nodule CT image diagnosis, and compare its performance with traditional computer aided diagnosis (CADx) systems using hand-crafted features. All of the 1018 cases were acquired from Lung Image Database Consortium (LIDC) public lung cancer database. The nodules were segmented according to four radiologists’ markings, and 13,668 samples were generated by rotating every slice of nodule images. Three multichannel ROI based deep structured algorithms were designed and implemented in this study: convolutional neural network (CNN), deep belief network (DBN), and stacked denoising autoencoder (SDAE). For the comparison purpose, we also implemented a CADx system using hand-crafted features including density features, texture features and morphological features. The performance of every scheme was evaluated by using a 10-fold cross-validation method and an assessment index of the area under the receiver operating characteristic curve (AUC). The observed highest area under the curve (AUC) was 0.899±0.018 achieved by CNN, which was significantly higher than traditional CADx with the AUC=0.848±0.026. The results from DBN was also slightly higher than CADx, while SDAE was slightly lower. By visualizing the automatic generated features, we found some meaningful detectors like curvy stroke detectors from deep structured schemes. The study results showed the deep structured algorithms with automatically generated features can achieve desirable performance in lung nodule diagnosis. With well-tuned parameters and large enough dataset, the deep learning algorithms can have better performance than current popular CADx. We believe the deep learning algorithms with similar data preprocessing procedure can be used in other medical image analysis areas as well. © 2017","Big data; Computer aided diagnosis; Deep learning; Lung cancer; Unsupervised feature learning","Big data; Biological organs; Computer aided diagnosis; Computer aided instruction; Computerized tomography; Deep learning; Diagnosis; Diseases; Image analysis; Image processing; Medical computing; Medical imaging; Neural networks; Structured programming; 10-fold cross-validation; Automatically generated; Convolutional neural network; Deep belief network (DBN); Lung Cancer; Morphological features; Receiver operating characteristic curves; Unsupervised feature learning; Learning algorithms; area under the curve; Article; artificial neural network; cancer diagnosis; cancer screening; computer aided design; deep belief network; human; image analysis; learning algorithm; lung cancer; lung nodule; machine learning; major clinical study; priority journal; radiologist; receiver operating characteristic; stacked denoising autoencoder; x-ray computed tomography",2-s2.0-85018458426
"Lin J., He C., Wang Z.J., Li S.","Structure Preserving Transfer Learning for Unsupervised Hyperspectral Image Classification",2017,"IEEE Geoscience and Remote Sensing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028450291&doi=10.1109%2fLGRS.2017.2723763&partnerID=40&md5=c7af3493cce4faaff1658f742d250347","Recent advances on remote sensing techniques allow easier access to imaging spectrometer data. Manually labeling and processing of such collected hyperspectral images (HSIs) with a vast quantities of samples and a large number of bands is labor and time consuming. To relieve these manual processes, machine learning based HSI processing methods have attracted increasing research attention. A major assumption in many machine learning problems is that the training and testing data are in the same feature space and follow the same distribution. However, this assumption doesn't always hold true in many real world problems, especially in certain HSI processing problems with extremely insufficient or even without training samples. In this letter, we present a transfer learning framework to address this unsupervised challenge (i.e., without training samples in the target domain), by making the following three main contributions: 1) to the best of our knowledge, this is the first time for transfer learning framework to be used for the classification of totally unknown target HSI data with no training samples; 2) the characteristics of HSI are learned on dual spaces to exploit its structure knowledge to better label HSI samples; and 3) two specific new scenarios suitable for transfer learning are investigated. Experimental results on several real world HSIs support the superiority of the proposed work. © 2017 IEEE.","Hyperspectral image (HSI); remote sensing; transfer learning","Artificial intelligence; Data structures; Hyperspectral imaging; Image classification; Independent component analysis; Learning systems; Markov processes; Personnel training; Processing; Remote sensing; Sampling; Space optics; Spectroscopy; Testing; Imaging spectrometers; Machine learning problem; Processing problems; Real-world problem; Remote sensing techniques; Structure-preserving; Training and testing; Transfer learning; Image processing",2-s2.0-85028450291
"Li H., Giger M.L., Huynh B.Q., Antropova N.O.","Deep learning in breast cancer risk assessment: Evaluation of convolutional neural networks on a clinical dataset of full-field digital mammograms",2017,"Journal of Medical Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029820268&doi=10.1117%2f1.JMI.4.4.041304&partnerID=40&md5=ac28e4cce8354d78b4b8d63d411368e2","To evaluate deep learning in the assessment of breast cancer risk in which convolutional neural networks (CNNs) with transfer learning are used to extract parenchymal characteristics directly from full-field digital mammographic (FFDM) images instead of using computerized radiographic texture analysis (RTA), 456 clinical FFDM cases were included: a ""high-risk"" BRCA1/2 gene-mutation carriers dataset (53 cases), a ""high-risk"" unilateral cancer patients dataset (75 cases), and a ""low-risk dataset"" (328 cases). Deep learning was compared to the use of features from RTA, as well as to a combination of both in the task of distinguishing between high- and low-risk subjects. Similar classification performances were obtained using CNN [area under the curve (AUC)=0.83; standard error (SE)=0.03] and RTA (AUC=0.82; SE=0.03) in distinguishing BRCA1/2 carriers and low-risk women. However, in distinguishing unilateral cancer patients and low-risk women, performance was significantly greater with CNN (AUC=0.82; SE=0.03) compared to RTA (AUC=0.73; SE=0.03). Fusion classifiers performed significantly better than the RTA-alone classifiers with AUC values of 0.86 and 0.84 in differentiating BRCA1/2 carriers from low-risk women and unilateral cancer patients from low-risk women, respectively. In conclusion, deep learning extracted parenchymal characteristics from FFDMs performed as well as, or better than, conventional texture analysis in the task of distinguishing between cancer risk populations. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","breast cancer risk assessment; convolutional neural network; deep learning; full-field digital mammogram; mammographic parenchymal patterns; radiographic texture analysis; transfer learning","adult; area under the curve; Article; breast cancer; cancer risk; controlled study; convolutional neural network; digital mammography; female; gene mutation; human; learning; low risk population; machine learning; major clinical study; retrospective study; risk assessment; tumor suppressor gene",2-s2.0-85029820268
"Kolb S., Paramonov S., Guns T., De Raedt L.","Learning constraints in spreadsheets and tabular data",2017,"Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020207644&doi=10.1007%2fs10994-017-5640-x&partnerID=40&md5=c9bbe6830d2d265fd79962b5a25177e8","Spreadsheets, comma separated value files and other tabular data representations are in wide use today. However, writing, maintaining and identifying good formulas for tabular data and spreadsheets can be time-consuming and error-prone. We investigate the automatic learning of constraints (formulas and relations) in raw tabular data in an unsupervised way. We represent common spreadsheet formulas and relations through predicates and expressions whose arguments must satisfy the inherent properties of the constraint. The challenge is to automatically infer the set of constraints present in the data, without labeled examples or user feedback. We propose a two-stage generate and test method where the first stage uses constraint solving techniques to efficiently reduce the number of candidates, based on the predicate signatures. Our approach takes inspiration from inductive logic programming, constraint learning and constraint satisfaction. We show that we are able to accurately discover constraints in spreadsheets from various sources. © 2017, The Author(s).","Constraint discovery; Constraint learning; Constraint programming; Excel; Machine learning; Spreadsheets; Tabular constraint learning","Computer programming; Constraint theory; Inductive logic programming (ILP); Learning systems; Network security; Spreadsheets; Testing; Constraint discovery; Constraint learning; Constraint programming; Excel; Tabular constraint learning; Computer programming languages",2-s2.0-85020207644
"Liao X.L., Zhang C.","Toward situation awareness: a survey on adaptive learning for model-free tracking",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992124400&doi=10.1007%2fs11042-016-4001-2&partnerID=40&md5=9853dd1aed1eda1f3cc84b37ede2c1f5","Visual tracking estimates the trajectory of an object of interest in non-stationary image streams that change over time. Recently, approaches for model-free tracking have received increased interest since manually annotating sufficient examples of all objects in the world is prohibitively expensive. By definition, a model-free tracker has only one labeled instance in the form of an identified object in the first frame. In the subsequent frames, it has to learn variations of the tracked object with only unlabeled data available. There exists a dilemma for model-free trackers, i.e., whether the tracker would shift the focus to clutters (i.e., adaptivity) or result in very short tracks (i.e., stability) largely depends on how sensitive the appearance model is. In contrast to recent survey efforts with data-driven approaches focusing on the performance on benchmarks, this article aims to provide an in-depth survey on solutions to the dilemma between adaptivity and stability in model-free tracking focusing on the ability of achieving situation awareness, i.e., learning the object appearance adaptively in a non-stationary environment. The survey results show that, regardless of visual representations and statistical models involved, the way of exploiting unlabeled data in the changing environment and the extent of how rapidly the appearance model need be updated accordingly with selected example(s) of estimated labels are the key to many, if not all, evaluation measures for tracking. Such conceptual consensuses, despite the diversity of approaches in this field, for the first time capture the essence of model-free tracking and facilitate the design of visual tracking systems. © 2016, Springer Science+Business Media New York.","Computer vision; Machine learning; Model-free tracking; Semi-supervised online learning; Video surveillance","Artificial intelligence; Benchmarking; Computer vision; Image segmentation; Learning systems; Security systems; Tracking (position); Changing environment; Data-driven approach; Model free; Non-stationary environment; Online learning; Video surveillance; Visual representations; Visual tracking systems; Surveys",2-s2.0-84992124400
"Fattahi H., Babanouri N.","Applying Optimized Support Vector Regression Models for Prediction of Tunnel Boring Machine Performance",2017,"Geotechnical and Geological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018838765&doi=10.1007%2fs10706-017-0238-4&partnerID=40&md5=47655d0315ea97484736b5d1cc2a561c","One of the main factors in the effective application of a tunnel boring machine (TBM) is the ability to accurately estimate the machine performance in order to determine the project costs and schedule. Predicting the TBM performance is a nonlinear and multivariable complex problem. The aim of this study is to predict the performance of TBM using the hybrid of support vector regression (SVR) and the differential evolution algorithm (DE), artificial bee colony algorithm (ABC), and gravitational search algorithm (GSA). The DE, ABC and GSA are combined with the SVR for determining the optimal value of its user defined parameters. The optimization implementation by the DE, ABC and GSA significantly improves the generalization ability of the SVR. The uniaxial compressive strength (UCS), average distance between planes of weakness (DPW), the angle between tunnel axis and the planes of weakness (α), and intact rock brittleness (BI) were considered as the input parameters, while the rate of penetration was the output parameter. The prediction models were applied to the available data given in the literature, and their performance was assessed based on statistical criteria. The results clearly show the superiority of DE when integrated with SVR for optimizing values of its parameters. In addition, the suggested model was compared with the methods previously presented for predicting the TBM penetration rate. The comparative results revealed that the hybrid of DE and SVR yields a robust model which outperforms other models in terms of the higher correlation coefficient and lower mean squared error. © 2017, Springer International Publishing Switzerland.","Artificial bee colony algorithm; Differential evolution algorithm; Gravitational search algorithm; Penetration rate; Support vector regression; Tunnel boring machine","Boring machines (machine tools); Compressive strength; Construction equipment; Forecasting; Fracture mechanics; Learning algorithms; Mean square error; Optimization; Regression analysis; Tunneling machines; Artificial bee colony algorithms; Differential evolution algorithms; Gravitational search algorithms; Penetration rates; Support vector regression (SVR); Tunnel boring machines; Evolutionary algorithms; compressive strength; error analysis; genetic algorithm; numerical model; penetration test; performance assessment; prediction; regression analysis; support vector machine; TBM; Apoidea",2-s2.0-85018838765
"Cao R., Freitas C., Chan L., Sun M., Jiang H., Chen Z.","ProLanGO: Protein function prediction using neural machine translation based on a recurrent neural network",2017,"Molecules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032702712&doi=10.3390%2fmolecules22101732&partnerID=40&md5=5ca81f0478bf9d7309851b8a917397fd","With the development of next generation sequencing techniques, it is fast and cheap to determine protein sequences but relatively slow and expensive to extract useful information from protein sequences because of limitations of traditional biological experimental techniques. Protein function prediction has been a long standing challenge to fill the gap between the huge amount of protein sequences and the known function. In this paper, we propose a novel method to convert the protein function problem into a language translation problem by the new proposed protein sequence language ""ProLan"" to the protein function language ""GOLan"", and build a neural machine translation model based on recurrent neural networks to translate ""ProLan"" language to ""GOLan"" language. We blindly tested our method by attending the latest third Critical Assessment of Function Annotation (CAFA 3) in 2016, and also evaluate the performance of our methods on selected proteins whose function was released after CAFA competition. The good performance on the training and testing datasets demonstrates that our new proposed method is a promising direction for protein function prediction. In summary, we first time propose a method which converts the protein function prediction problem to a language translation problem and applies a neural machine translation model for protein function prediction. © 2017 by the authors.","Machine learning; Neural machine translation; Protein function prediction",,2-s2.0-85032702712
"Zhang M.-L., Yu F., Tang C.-Z.","Disambiguation-free partial label learning",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021994749&doi=10.1109%2fTKDE.2017.2721942&partnerID=40&md5=cc252b1713f56a1ac92c418a68758cf5","In partial label learning, each training example is associated with a set of candidate labels among which only one is the ground-truth label. The common strategy to induce predictive model is trying to disambiguate the candidate label set, i.e., differentiating the modeling outputs of individual candidate labels. Specifically, disambiguation by differentiation can be conducted either by identifying the ground-truth label iteratively or by treating each candidate label equally. Nonetheless, the disambiguation strategy is prone to be misled by the false positive labels co-occurring with ground-truth label. In this paper, a new partial label learning strategy is studied which refrains from conducting disambiguation. Specifically, by adapting error-correcting output codes (ECOC), a simple yet effective approach named Pl-ecoc is proposed by utilizing candidate label set as an entirety. During training phase, to build binary classifier w.r.t. each column coding, any partially labeled example will be regarded as a positive or negative training example only if its candidate label set entirely falls into the coding dichotomy. During testing phase, class label for the unseen instance is determined via loss-based decoding which considers binary classifiers' empirical performance and predictive margin. Extensive experiments show that Pl-ecoc performs favorably against state-of-the-art partial label learning approaches. © 1989-2012 IEEE.","Disambiguation; Error-correcting output codes; Machine learning; Partial label learning; Weak supervision","Bins; Classification (of information); Codes (symbols); Encoding (symbols); Learning systems; Painting; Personnel training; Silicon; Collaboration; disambiguation; Error correcting output code; Face; partial label learning; Predictive models; weak supervision; Education",2-s2.0-85021994749
"Tani S., Matsuoka T., Hirai Y., Kurata T., Tatsumi K., Asano T., Ueda M., Kamata T.","Behavior-level analysis of a successive stochastic approximation analog-to-digital conversion system for multi-channel biomedical data acquisition",2017,"IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030320908&doi=10.1587%2ftransfun.E100.A.2073&partnerID=40&md5=4d6f84400fb83996e7b641e25e394d72","In the present paper, we propose a novel high-resolution analog-to-digital converter (ADC) for low-power biomedical analog front-ends, which we call the successive stochastic approximation ADC. The proposed ADC uses a stochastic flash ADC (SF-ADC) to realize a digitally controlled variable-threshold comparator in a successive-approximation-register ADC (SAR-ADC), which can correct errors originating from the internal digital-to-analog converter in the SAR-ADC. For the residual error after SAR-ADC operation, which can be smaller than thermal noise, the SF-ADC uses the statistical characteristics of noise to achieve high resolution. The SF-ADC output for the residual signal is combined with the SAR-ADC output to obtain high-precision output data using the supervised machine learning method. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","DAC error calibration; Machine learning; Mismatch; SAR-ADC; Stochastic A/D conversion","Approximation theory; Artificial intelligence; Data acquisition; Digital to analog conversion; Errors; Learning systems; Stochastic systems; Supervised learning; Thermal noise; A/D conversion; Analog to digital converters; Error calibration; Mismatch; SAR ADC; Statistical characteristics; Successive approximation register adc; Supervised machine learning; Analog to digital conversion",2-s2.0-85030320908
"Stadler R., Pasquini R., Fodor V.","Learning from Network Device Statistics",2017,"Journal of Network and Systems Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029795404&doi=10.1007%2fs10922-017-9426-z&partnerID=40&md5=cf4493ae1ff3b4eb6a9b7f8c894084e2","We estimate end-to-end service metrics from network device statistics. Our approach is based upon statistical, supervised learning, whereby the mapping from device-level to service-level metrics is learned from observations, i.e., through monitoring the system. The approach enables end-to-end performance prediction without requiring an explicit model of the system, which is different from traditional engineering techniques that use stochastic modeling and simulation. The fact that end-to-end service metrics can be estimated from local network statistics with good accuracy in the scenarios we consider suggests that service-level properties are “encoded” in network-level statistics. We show that the set of network statistics needed for estimation can be reduced to a set of measurements along the network path between client and service backend, with little loss in estimation accuracy. The reported work is largely experimental and its results have been obtained through testbed measurements from a video streaming service and a KV store over an OpenFlow network. © 2017, The Author(s).","End-to-end performance Prediction; Feature selection; Machine learning; Network analytics; Network management; OpenFlow; Statistical learning","Feature extraction; Learning systems; Network management; Stochastic models; Stochastic systems; Video streaming; End-to-end performance; End-to-end service; Network statistics; Openflow; Statistical learning; Testbed measurements; Traditional engineerings; Video streaming services; Statistics",2-s2.0-85029795404
"Jiang H., Tian Q., Farrell J., Wandell B.A.","Learning the Image Processing Pipeline",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029389860&doi=10.1109%2fTIP.2017.2713942&partnerID=40&md5=8c45969d94827611728c976224631e3c","Many creative ideas are being proposed for image sensor designs, and these may be useful in applications ranging from consumer photography to computer vision. To understand and evaluate each new design, we must create a corresponding image processing pipeline that transforms the sensor data into a form, that is appropriate for the application. The need to design and optimize these pipelines is time-consuming and costly. We explain a method that combines machine learning and image systems simulation that automates the pipeline design. The approach is based on a new way of thinking of the image processing pipeline as a large collection of local linear filters. We illustrate how the method has been used to design pipelines for novel sensor architectures in consumer photography applications. © 1992-2012 IEEE.","camera image processing pipeline; Local linear learned; machine learning","Artificial intelligence; Image sensors; Learning systems; Photography; Pipeline processing systems; Pipelines; Camera image processing; Creative ideas; Image processing pipeline; Image sensor designs; Image systems; Local linear; Pipeline design; Sensor architectures; Image processing",2-s2.0-85029389860
"Ponti M., Riva M.","An incremental linear-time learning algorithm for the Optimum-Path Forest classifier",2017,"Information Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020006256&doi=10.1016%2fj.ipl.2017.05.004&partnerID=40&md5=3bd9a2a54ba58ec21506ab3e9a2d9e70","We present a classification method with incremental capabilities based on the Optimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a fully-connected training graph, arc weights represent distances between two feature vectors. Our algorithm includes new instances in an OPF in linear-time, while keeping similar accuracies when compared with the original quadratic-time model. © 2017 Elsevier B.V.","Computational complexity; Graph algorithms; Machine learning; Minimum spanning tree; OPF","Computational complexity; Forestry; Learning systems; Classification methods; Feature vectors; Graph algorithms; Linear time; Minimum spanning trees; Optimum-path forests; Quadratic time; Learning algorithms",2-s2.0-85020006256
"Rivera-Caicedo J.P., Verrelst J., Muñoz-Marí J., Camps-Valls G., Moreno J.","Hyperspectral dimensionality reduction for biophysical variable statistical retrieval",2017,"ISPRS Journal of Photogrammetry and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029070767&doi=10.1016%2fj.isprsjprs.2017.08.012&partnerID=40&md5=ecbaa7c06f9649c173e3f81a7744b188","Current and upcoming airborne and spaceborne imaging spectrometers lead to vast hyperspectral data streams. This scenario calls for automated and optimized spectral dimensionality reduction techniques to enable fast and efficient hyperspectral data processing, such as inferring vegetation properties. In preparation of next generation biophysical variable retrieval methods applicable to hyperspectral data, we present the evaluation of 11 dimensionality reduction (DR) methods in combination with advanced machine learning regression algorithms (MLRAs) for statistical variable retrieval. Two unique hyperspectral datasets were analyzed on the predictive power of DR + MLRA methods to retrieve leaf area index (LAI): (1) a simulated PROSAIL reflectance data (2101 bands), and (2) a field dataset from airborne HyMap data (125 bands). For the majority of MLRAs, applying first a DR method leads to superior retrieval accuracies and substantial gains in processing speed as opposed to using all bands into the regression algorithm. This was especially noticeable for the PROSAIL dataset: in the most extreme case, using the classical linear regression (LR), validation results RCV 2 (RMSECV) improved from 0.06 (12.23) without a DR method to 0.93 (0.53) when combining it with a best performing DR method (i.e., CCA or OPLS). However, these DR methods no longer excelled when applied to noisy or real sensor data such as HyMap. Then the combination of kernel CCA (KCCA) with LR, or a classical PCA and PLS with a MLRA showed more robust performances (RCV 2 of 0.93). Gaussian processes regression (GPR) uncertainty estimates revealed that LAI maps as trained in combination with a DR method can lead to lower uncertainties, as opposed to using all HyMap bands. The obtained results demonstrated that, in general, biophysical variable retrieval from hyperspectral data can largely benefit from dimensionality reduction in both accuracy and computational efficiency. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","ARTMO; Biophysical parameter retrieval; Hyperspectral; Machine learning regression algorithms; Spectral dimensionality reduction methods; Vegetation properties","Artificial intelligence; Biophysics; Computational efficiency; Data handling; Hyperspectral imaging; Learning algorithms; Learning systems; Regression analysis; Uncertainty analysis; Vegetation; ARTMO; Biophysical parameter retrievals; Dimensionality reduction method; HyperSpectral; Regression algorithms; Vegetation properties; Data reduction; airborne sensing; algorithm; biophysics; data processing; data set; leaf area index; machine learning; model; numerical method; radiative transfer; reflectance; sensor; spectral analysis; vegetation dynamics",2-s2.0-85029070767
"Kužnar D., Piltaver R., Gradišek A., Gams M., Luštrek M.","An intelligent system to monitor refrigeration devices",2017,"Expert Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023621330&doi=10.1111%2fexsy.12199&partnerID=40&md5=383318bec29634f71d8f2d8d8204278d","Refrigeration systems have been a vital component of our lives for more than a century. Apart from storing food, they are used to store sensitive goods such as pharmaceutical products or reactive chemicals. The deterioration of the refrigeration system performance due to aging or malfunction directly affects the quality of stored goods. Therefore, an early detection of deviation in performance is an important task. This paper presents a system that monitors operation of refrigeration devices and alerts the user to possible irregularities in the operation run. The emphasis is on recognition of gradual changes of performance that indicate upcoming hardware problems. The system consists of 2 modules: human-defined expert rules and machine learning. The machine-learning module learns to recognize abnormal behaviour of devices automatically. Furthermore, it can distinguish between different abnormal events and allow the user to classify some of the types as normal, so that they not longer raise an alarm. The machine learning was evaluated by comparing its recognition of abnormal events and classification accuracy of such events to the performance of a human operator. The system can in principle be adapted to any electronic device that periodically applies some system for sustaining a predefined quality (e.g., temperature). Copyright © 2017 John Wiley & Sons, Ltd.","event classification; event detection; intelligent systems; machine learning; refrigeration devices; time series","Artificial intelligence; Chemical contamination; Education; Electron devices; Intelligent systems; Learning systems; Refrigeration; Time series; Abnormal behaviours; Classification accuracy; Event classification; Event detection; Machine learning module; Pharmaceutical products; Refrigeration devices; Refrigeration system; Monitoring",2-s2.0-85023621330
"Eskandarpour R., Khodaei A.","Leveraging Accuracy-Uncertainty Tradeoff in SVM to Achieve Highly Accurate Outage Predictions",2017,"IEEE Transactions on Power Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030785687&doi=10.1109%2fTPWRS.2017.2759061&partnerID=40&md5=b11aa565428fff88c446e583a8640acf","This letter proposes a three-dimensional Support Vector Machine (SVM) for power grid component outage prediction, and furthermore leverages its accuracy-uncertainty tradeoff to achieve highly accurate results. The model is developed based on three distinct features of component deterioration, distance from the extreme event, and the intensity of the extreme event, and is analytically investigated to exhibit its acceptable performance. IEEE","extreme events; Hurricanes; Learning systems; Logistics; machine learning method; Power grids; power system resilience; Support vector machines; Training","Electric power transmission networks; Hurricanes; Learning systems; Logistics; Personnel training; Support vector machines; Acceptable performance; Extreme events; Highly accurate; Machine learning methods; Power grids; System resiliences; Outages",2-s2.0-85030785687
"Zhao J., Lv Y., Zhou Z., Cao F.","A novel deep learning algorithm for incomplete face recognition: Low-rank-recovery network",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026459206&doi=10.1016%2fj.neunet.2017.06.013&partnerID=40&md5=9f5e70d76617495deeab9674f3346cb3","There have been a lot of methods to address the recognition of complete face images. However, in real applications, the images to be recognized are usually incomplete, and it is more difficult to realize such a recognition. In this paper, a novel convolution neural network frame, named a low-rank-recovery network (LRRNet), is proposed to conquer the difficulty effectively inspired by matrix completion and deep learning techniques. The proposed LRRNet first recovers the incomplete face images via an approach of matrix completion with the truncated nuclear norm regularization solution, and then extracts some low-rank parts of the recovered images as the filters. With these filters, some important features are obtained by means of the binaryzation and histogram algorithms. Finally, these features are classified with the classical support vector machines (SVMs). The proposed LRRNet method has high face recognition rate for the heavily corrupted images, especially for the images in the large databases. The proposed LRRNet performs well and efficiently for the images with heavily corrupted, especially in the case of large databases. Extensive experiments on several benchmark databases demonstrate that the proposed LRRNet performs better than some other excellent robust face recognition methods. © 2017 Elsevier Ltd","ADMM; Convolutional neural networks; Deep learning; Face recognition; Recovery of low-rank matrix","Computer system recovery; Convolution; Database systems; Deep learning; Deep neural networks; Education; Learning algorithms; Neural networks; Recovery; Support vector machines; ADMM; Convolution neural network; Convolutional neural network; Face recognition methods; Face recognition rates; Low-rank matrices; Nuclear norm regularizations; Support vector machine (SVMs); Face recognition; algorithm; analytic method; Article; artificial neural network; binaryzation algorithm; controlled study; cost; dense hybrid representation method; facial recognition; histogram algorithm; image analysis; intermethod comparison; learning algorithm; low rank recovery network; matrix completion; measurement accuracy; priority journal; sparse representation based classification method; support vector machine",2-s2.0-85026459206
"Choi Y., Lee H.","Data properties and the performance of sentiment classification for electronic commerce applications",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014735254&doi=10.1007%2fs10796-017-9741-7&partnerID=40&md5=3692113a99c8322af3f1aad03670ce84","Sentiment classification has played an important role in various research area including e-commerce applications and a number of advanced Computational Intelligence techniques including machine learning and computational linguistics have been proposed in the literature for improved sentiment classification results. While such studies focus on improving performance with new techniques or extending existing algorithms based on previously used dataset, few studies provide practitioners with insight on what techniques are better for their datasets that have different properties. This paper applies four different sentiment classification techniques from machine learning (Naïve Bayes, SVM and Decision Tree) and sentiment orientation approaches to datasets obtained from various sources (IMDB, Twitter, Hotel review, and Amazon review datasets) to learn how different data properties including dataset size, length of target documents, and subjectivity of data affect the performance of those techniques. The results of computational experiments confirm the sensitivity of the techniques on data properties including training data size, the document length and subjectivity of training /test data in the improvement of performances of techniques. The theoretical and practical implications of the findings are discussed. © 2017, The Author(s).","Comparative analysis; Data properties; Machine learning approach; Opinion mining; Sentiment classification; Sentiment orientation approach","Artificial intelligence; Commerce; Decision trees; Electronic commerce; Information retrieval systems; Learning systems; Trees (mathematics); Comparative analysis; Data properties; Machine learning approaches; Opinion mining; Sentiment classification; Sentiment orientation approach; Classification (of information)",2-s2.0-85014735254
"Hegelich S.","Deep learning and punctuated equilibrium theory",2017,"Cognitive Systems Research",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019642130&doi=10.1016%2fj.cogsys.2017.02.006&partnerID=40&md5=fdd3835da31901fdd91e96dc5385eed0","Deep learning is associated with the latest success stories in AI. In particular, deep neural networks are applied in increasingly different fields to model complex processes. Interestingly, the underlying algorithm of backpropagation was originally designed for political science models. The theoretical foundations of this approach are very similar to the concept of Punctuated Equilibrium Theory (PET). The article discusses the concept of deep learning and shows parallels to PET. A showcase model demonstrates how deep learning can be used to provide a missing link in the study of the policy process: the connection between attention in the political system (as inputs) and budget shifts (as outputs). © 2017 Elsevier B.V.","Backpropagation; Deep learning; Neural networks; Policy process; Punctuated equilibrium","Backpropagation; Backpropagation algorithms; Budget control; Deep neural networks; Neural networks; Model complexes; Policy process; Political science; Political systems; Punctuated equilibrium; Theoretical foundations; Deep learning; algorithm; area under the curve; Article; artificial neural network; backpropagation algorithm; data processing; deep learning; deep neural network; machine learning; mathematical computing; measurement accuracy; measurement noise; model; noise; policy; politics; prediction; priority journal; punctuated equilibrium theory; receiver operating characteristic; theory",2-s2.0-85019642130
"Rabindra B.","CloudGanga: Cloud computing based SDI model for ganga river basin management in India",2017,"International Journal of Agricultural and Environmental Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028603375&doi=10.4018%2fIJAEIS.2017100104&partnerID=40&md5=632fb6d725900a69f65be2278c71215d","The present research paper proposes and develops a Cloud computing based Spatial Data Infrastructure (SDI) Model named as CloudGanga for sharing, analysis and processing of geospatial data particularly in River Ganga Basin management in India. The main purpose of the CloudGanga is to integrate all the geospatial information such as dam location, well location, irrigation project, hydro power project, canal network and central Water Commission gauge stations locations related to River Ganga. CloudGanga can help the decision maker/ planner or common users to get enough information for their further research and studies. The open source software (Quantum GIS) has been used for the development of geospatial database. QGIS Plugin has been linked with Quantum GIS for invoking cloud computing environment. It has also discussed about the various overlay analysis in CloudGanga environment. In the present research, machine learning approaches are also used in a R tool for well locations which are associated with the basin of River Ganga.","Cloud Computing; Geospatial data; Overlay Analysis; κ-Means","Cloud computing; Data handling; Decision making; Hydroelectric power plants; Irrigation canals; Learning systems; Location; Network function virtualization; Open source software; Open systems; Rivers; Software engineering; Water management; Cloud computing environments; Geo-spatial data; Geo-spatial database; Geo-spatial informations; Machine learning approaches; Overlay analysis; River basin management; Spatial data infrastructure; River basin projects; basin management; data processing; GIS; machine learning; river basin; software; spatial data; Ganges River",2-s2.0-85028603375
"Ichikawa D., Saito T., Oyama H.","Impact of predicting health-guidance candidates using massive health check-up data: A data-driven analysis",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028581759&doi=10.1016%2fj.ijmedinf.2017.08.002&partnerID=40&md5=4e20a2fe2c506ef4f10cdf35d505fb05","Introduction Starting in 2008, specific health checkups and health guidance to prevent non-communicable diseases have been provided in Japan, which has the highest proportion of elderly citizens in the world. The attendance rate for health guidance appointments is 17.7%, which is far from the national goal of the system (45%). To improve the attendance rate, we present a model for predicting whether an examinee is a candidate for health guidance; this model was based on a machine learning method and a restricted but massive amount of health checkup information. Materials and methods Using machine learning methods, we developed the following five prediction models for identifying health-guidance candidates: baseline: this model included sex and age; model 1: this model included variables that can be measured in person + information on whether the examinee was a candidate in the past year; model 2: model 1 + systolic blood pressure + diastolic blood pressure; model 3: model 2 + all health checkup results from the past year; and model 4: model 3 using the training dataset excluding cases with missing data. Results The performance levels of the five prediction models (the AUC values of the models for the test dataset) were as follows: 0.592 [95% CI: 0.586–0.596] for the baseline model, 0.855 [95% CI: 0.851–0.858] for model 1, 0.985 [95% CI: 0.984–0.985] for model 2, 0.993 [95% CI: 0.993–0.993] for model 3, and 0.943 [95% CI: 0.941–0.945] for model 4. Conclusions We studied five models for identifying health-guidance candidates. The model that used all health checkup results from the past year had the highest predictive power. Application of the prediction model developed in the present study to the selection of health-guidance candidates could reduce the cost of guidance. © 2017 Elsevier B.V.","Data-driven; Health checkup; Health guidance; Machine learning; Prediction","Artificial intelligence; Blood pressure; Forecasting; Learning systems; Statistical tests; Data driven; Data-driven analysis; Diastolic blood pressures; Health check-ups; Machine learning methods; Non-communicable disease; Performance level; Systolic blood pressure; Health; adult; Article; body height; body mass; clinical article; diastolic blood pressure; female; human; Japan; machine learning; male; medical education; medical examination; medical history; medical record; non communicable disease; practice guideline; prediction; predictive value; priority journal; systolic blood pressure",2-s2.0-85028581759
"Peer M., Prüss H., Ben-Dayan I., Paul F., Arzy S., Finke C.","Functional connectivity of large-scale brain networks in patients with anti-NMDA receptor encephalitis: an observational study",2017,"The Lancet Psychiatry",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028712386&doi=10.1016%2fS2215-0366%2817%2930330-9&partnerID=40&md5=3608458a11648002a5eb9b281aefb3b9","Background In anti-NMDA receptor (NMDAR) encephalitis, antibody-mediated dysfunction of NMDARs causes severe neuropsychiatric symptoms, including psychosis, memory deficits, and movement disorders. However, it remains elusive how antibody-mediated NMDAR dysfunction leads to these symptoms, and whether the symptoms arise from impairment in specific brain regions and the interactions between impaired regions. Methods In this observational study, we recruited 43 patients with anti-NMDAR encephalitis from a tertiary university hospital and 43 age-matched and sex-matched healthy controls without a history of neurological or psychiatric disorders, who were recruited from the general population of Berlin. We used structural and resting-state functional MRI to investigate alterations in connectivity in all participants. We did functional connectivity analyses, including large-scale network analysis, whole-brain pair-wise connectivity, and machine-learning classification, and compared the results with patients' functional impairment. Findings Although structural MRI was normal in 31 (72%) of the 43 patients, we observed widespread alterations of functional connectivity that correlated with clinical measures. These alterations included impaired hippocampal functional connectivity, decoupling of the medial temporal and the default-mode networks, and an overall impairment of frontotemporal connections. Furthermore, functional connectivity was impaired within distributed large-scale networks, including sensorimotor, frontoparietal, lateral-temporal, and visual networks. Memory impairment correlated with hippocampal and medial-temporal-lobe network connectivity, whereas schizophrenia-like symptoms were associated with functional connectivity changes in frontoparietal networks. Machine-learning analyses corroborated these findings and identified frontoparietal and frontotemporal connections as reliably discriminating features between patients and controls, yielding an overall accuracy of 81%. Interpretation This study reveals a characteristic pattern of whole-brain functional connectivity alterations in anti-NMDAR encephalitis that is well suited to explain the major clinical symptoms of the disorder. These observations advance the pathophysiological understanding of NMDAR dysfunction in the human brain and could be similarly relevant for other neuropsychiatric disorders, such as schizophrenia. Funding Deutsche Forschungsgemeinschaft, Israeli National Science Foundation, Ministry of Science and Technology of Israel, Orion Foundation, and the Agnes Ginges Center for Neurologenetics. © 2017 Elsevier Ltd",,"immunoglobulin G; adult; anhedonia; anti n methyl d aspartate receptor encephalitis; Article; artificial neural network; blunted affect; BOLD signal; brain region; catatonia; clinical article; cohort analysis; control group; controlled study; default mode network; delusion; disease severity; episodic memory; female; frontal lobe; frontoparietal cortex; functional connectivity; functional magnetic resonance imaging; functional neuroimaging; Germany; hallucination; hippocampus; human; machine learning; male; medial prefrontal cortex; medial temporal lobe; mental disease; middle frontal gyrus; mutism; negative syndrome; nerve cell network; neurologic disease; nuclear magnetic resonance scanner; observational study; pathophysiology; population; positive syndrome; priority journal; Rankin scale; Rey auditory verbal learning test; schizophrenia; sensorimotor function; supervised machine learning; thought disorder; time series analysis; university hospital; verbal episodic memory",2-s2.0-85028712386
"Tomàs J.C., Faria F.A., Esquerdo J.C.D.M., Coutinho A.C., Medeiros C.B.","SiRCub: A novel approach to recognize agricultural crops using supervised classification",2017,"International Journal of Agricultural and Environmental Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028621877&doi=10.4018%2fIJAEIS.2017100102&partnerID=40&md5=2b7299b903e1935321bf3dfb3f880ee1","This paper presents a new approach to deal with agricultural crop recognition using SVM (Support Vector Machine), applied to time series of NDVI images. The presented method can be divided into two steps. First, the Timesat software package is used to extract a set of crop features from the NDVI time series. These features serve as descriptors that characterize each NDVI vegetation curve, i.e., the period comprised between sowing and harvesting dates. Then, it is used an SVM to learn the patterns that define each type of crop, and create a crop model that allows classifying new series. The authors present a set of experiments that show the effectiveness of this technique. They evaluated their algorithm with a collection of more than 3000 time series from the Brazilian State of Mato Grosso spanning 4 years (2009-2013). Such time series were annotated in the field by specialists from Embrapa (Brazilian Agricultural Research Corporation). This methodology is generic, and can be adapted to distinct regions and crop profiles. Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","Crop Classification; LULC; Machine Learning; NDVI; Remote Sensing; SVM; Time; Time Series","Agricultural machinery; Agriculture; Learning systems; Remote sensing; Supervised learning; Support vector machines; Time series; Agricultural crops; Agricultural research; Crop classification; LULC; NDVI; Supervised classification; SVM(support vector machine); Time; Crops; algorithm; crop plant; land cover; land use change; machine learning; NDVI; remote sensing; support vector machine; time series; Brazil; Mato Grosso",2-s2.0-85028621877
"Sun Y., Wang B.","Indoor corner recognition from crowdsourced trajectories using smartphone sensors",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017567134&doi=10.1016%2fj.eswa.2017.04.024&partnerID=40&md5=5384fbde3f6a69136d5ccc35c11ec23c","Recently, fingerprint crowdsourcing from pedestrian movement trajectories has been promoted to alleviate the site survey burden for radio map construction in fingerprinting-based indoor localization. Indoor corners, as one of the most common indoor landmarks, play an important role in movement trajectory analysis. This paper studies the problem of indoor corner recognition in crowdsourced movement trajectories. In a movement trajectory, smartphone internal sensor measurements experience some signal changes when passing by a corner. However, the state-of-the-art solutions based on signal change detection cannot well deal with the fake corner problem and pose diversity problem in most practical movement trajectories. In this paper, we study the corner recognition problem from an expert system viewpoint by applying machine learning techniques. In particular, we extract recognition features from both the time and frequency domain and propose a hierarchical corner recognition scheme consisting of three classifiers. The first pose classifier is to classify various poses into only two groups according to whether or not a smartphone is kept in a fixed position relative to a user upper body when collecting sensor measurements. Feature selection is then applied to train two corner classifiers each for one pose group. Field experiments are conducted to compare our proposed scheme with three state-of-the-art algorithms. In all cases, our scheme outperforms the best of these algorithms in terms of much higher F1-measure and precision for corner recognition. The results also provide insights on the potentials of using more advanced techniques from expert systems in indoor localization. © 2017 Elsevier Ltd","Fake corner problem; Indoor corner recognition; Indoor positioning system; Machine learning; Pose diversity problem","Artificial intelligence; Expert systems; Frequency domain analysis; Indoor positioning systems; Learning systems; Signal encoding; Smartphones; Trajectories; Corner recognition; Fake corner problem; Machine learning techniques; Movement trajectories; Pose diversity problem; Recognition features; State-of-the-art algorithms; Time and frequency domains; Edge detection",2-s2.0-85017567134
"Hein D., Hentschel A., Runkler T., Udluft S.","Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029956200&doi=10.1016%2fj.engappai.2017.07.005&partnerID=40&md5=7a9b25a20969f7d9b14c0b4c6f023d6b","Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem's dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies. © 2017 Elsevier Ltd","Fuzzy controller; Fuzzy policy; Interpretable; Particle swarm optimization; Reinforcement learning","Controllers; Cost functions; E-learning; Learning systems; Optimal control systems; Particle swarm optimization (PSO); Poles; Problem solving; Swarm intelligence; System theory; Autonomous machines; Fuzzy controllers; Fuzzy particle swarm; Fuzzy reinforcement learning; Interpretable; Optimal control strategy; Self-organizing fuzzy controllers; Training parameters; Reinforcement learning",2-s2.0-85029956200
"Chen X., Peng X., Duan R., Li J.","Deep kernel learning method for SAR image target recognition",2017,"Review of Scientific Instruments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032729397&doi=10.1063%2f1.4993064&partnerID=40&md5=b098278681f053b2c8c63c7758be1146","With the development of deep learning, research on image target recognition has made great progress in recent years. Remote sensing detection urgently requires target recognition for military, geographic, and other scientific research. This paper aims to solve the synthetic aperture radar image target recognition problem by combining deep and kernel learning. The model, which has a multilayer multiple kernel structure, is optimized layer by layer with the parameters of Support Vector Machine and a gradient descent algorithm. This new deep kernel learning method improves accuracy and achieves competitive recognition results compared with other learning methods. © 2017 Author(s).",,"Learning systems; Radar imaging; Remote sensing; Synthetic aperture radar; Gradient descent algorithms; Image target recognition; Kernel learning methods; Learning methods; Multiple kernels; Scientific researches; Synthetic aperture radar (SAR) images; Target recognition; Radar target recognition; learning; support vector machine; telecommunication",2-s2.0-85032729397
"Masino J., Thumm J., Frey M., Gauterin F.","Learning from the crowd: Road infrastructure monitoring system",2017,"Journal of Traffic and Transportation Engineering (English Edition)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030178220&doi=10.1016%2fj.jtte.2017.06.003&partnerID=40&md5=02f43e67f81d4a459b0f131cb2640a58","The condition of the road infrastructure has severe impacts on the road safety, driving comfort, and on the rolling resistance. Therefore, the road infrastructure must be monitored comprehensively and in regular intervals to identify damaged road segments and road hazards. Methods have been developed to comprehensively and automatically digitize the road infrastructure and estimate the road quality, which are based on vehicle sensors and a supervised machine learning classification. Since different types of vehicles have various suspension systems with different response functions, one classifier cannot be taken over to other vehicles. Usually, a high amount of time is needed to acquire training data for each individual vehicle and classifier. To address this problem, the methods to collect training data automatically for new vehicles based on the comparison of trajectories of untrained and trained vehicles have been developed. The results show that the method based on a k-dimensional tree and Euclidean distance performs best and is robust in transferring the information of the road surface from one vehicle to another. Furthermore, this method offers the possibility to merge the output and road infrastructure information from multiple vehicles to enable a more robust and precise prediction of the ground truth. © 2017 The Authors","Classification; Euclidean distance; Machine learning; Monitoring; Road infrastructure condition; Tree graphs",,2-s2.0-85030178220
"Ao Z., Su Y., Li W., Guo Q., Zhang J.","One-class classification of airborne LiDAR data in urban areas using a presence and background learning algorithm",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860126&doi=10.3390%2frs9101001&partnerID=40&md5=f506f84e6e58896964bb4f895be51518","Automatic classification of light detection and ranging (LiDAR) data in urban areas is of great importance for many applications such as generating three-dimensional (3D) building models and monitoring power lines. Traditional supervised classification methods require training samples of all classes to construct a reliable classifier. However, complete training samples are normally hard and costly to collect, and a common circumstance is that only training samples for a class of interest are available, in which traditional supervised classification methods may be inappropriate. In this study, we investigated the possibility of using a novel one-class classification algorithm, i.e., the presence and background learning (PBL) algorithm, to classify LiDAR data in an urban scenario. The results demonstrated that the PBL algorithm implemented by back propagation (BP) neural network (PBL-BP) could effectively classify a single class (e.g., building, tree, terrain, power line, and others) from airborne LiDAR point cloud with very high accuracy. The mean F-score for all of the classes from the PBL-BP classification results was 0.94, which was higher than those from one-class support vector machine (SVM), biased SVM, and maximum entropy methods (0.68, 0.82 and 0.93, respectively). Moreover, the PBL-BP algorithm yielded a comparable overall accuracy to the multi-class SVM method. Therefore, this method is very promising in the classification of the LiDAR point cloud. © 2017 by the authors.","LiDAR; One-class classification; Presence and background learning algorithm; Remote sensing","Backpropagation; Backpropagation algorithms; Classification (of information); Maximum entropy methods; Optical radar; Remote sensing; Sampling; Supervised learning; Support vector machines; Three dimensional computer graphics; Trees (mathematics); Automatic classification; Back propagation neural networks; Light detection and ranging; One-class Classification; One-class classification algorithm; One-class support vector machine; Supervised classification; Three-dimensional (3D) buildings; Learning algorithms",2-s2.0-85032860126
"Esposito G., Martin M.","Bellman residuals minimization using online support vector machines",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017506464&doi=10.1007%2fs10489-017-0910-7&partnerID=40&md5=c21cc0c085b1317d5b5a3111889e8c53","In this paper we present and theoretically study an Approximate Policy Iteration (API) method called API − BRMϵ using a very effective implementation of incremental Support Vector Regression (SVR) to approximate the value function able to generalize Reinforcement Learning (RL) problems with continuous (or large) state space. API − BRMϵ is presented as a non-parametric regularization method based on an outcome of the Bellman Residual Minimization (BRM) able to minimize the variance of the problem. The proposed method can be cast as incremental and may be applied to the on-line agent interaction framework of RL. Being also based on SVR which are based on convex optimization, is able to find the global solution of the problem. API − BRMϵ using SVR can be seen as a regularization problem using insensitive loss. Compared to standard squared loss also used in regularization, this allows to naturally build a sparse solution for the approximation function. We extensively analyze the statistical properties of API − BRMϵ founding a bound which controls the performance loss of the algorithm under some assumptions on the kernel and assuming that the collected samples are not-i.i.d. following a β−mixing process. Some experimental evidence and performance for well known RL benchmarks are also presented. © 2017, Springer Science+Business Media New York.","Approximate policy iteration; Regression; Regularization; Reinforcement learning; Support vector machine","Benchmarking; Convex optimization; Iterative methods; Reinforcement learning; Vector spaces; Approximate policy iteration (API); Approximation function; Incremental support vector regressions; Online support vector machines; Policy iteration; Regression; Regularization; Statistical properties; Support vector machines",2-s2.0-85017506464
"Nemoto M., Hayashi N., Hanaoka S., Nomura Y., Miki S., Yoshikawa T.","Feasibility Study of a Generalized Framework for Developing Computer-Aided Detection Systems—a New Paradigm",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017440061&doi=10.1007%2fs10278-017-9968-3&partnerID=40&md5=7e560dc3e97371268117baf48a868950","We propose a generalized framework for developing computer-aided detection (CADe) systems whose characteristics depend only on those of the training dataset. The purpose of this study is to show the feasibility of the framework. Two different CADe systems were experimentally developed by a prototype of the framework, but with different training datasets. The CADe systems include four components; preprocessing, candidate area extraction, candidate detection, and candidate classification. Four pretrained algorithms with dedicated optimization/setting methods corresponding to the respective components were prepared in advance. The pretrained algorithms were sequentially trained in the order of processing of the components. In this study, two different datasets, brain MRA with cerebral aneurysms and chest CT with lung nodules, were collected to develop two different types of CADe systems in the framework. The performances of the developed CADe systems were evaluated by threefold cross-validation. The CADe systems for detecting cerebral aneurysms in brain MRAs and for detecting lung nodules in chest CTs were successfully developed using the respective datasets. The framework was shown to be feasible by the successful development of the two different types of CADe systems. The feasibility of this framework shows promise for a new paradigm in the development of CADe systems: development of CADe systems without any lesion specific algorithm designing. © 2017, Society for Imaging Informatics in Medicine.","Automatic optimization; CADe training dataset; Computer-aided detection (CADe) system; Generalized CADe framework; Machine learning method","Biological organs; Brain; Computer aided instruction; Learning systems; Optimization; Automatic optimization; Computer aided detection; Generalized CADe framework; Machine learning methods; Training dataset; Network function virtualization",2-s2.0-85017440061
"Almeida J.G., Preto A.J., Koukos P.I., Bonvin A.M.J.J., Moreira I.S.","Membrane proteins structures: A review on computational modeling tools",2017,"Biochimica et Biophysica Acta - Biomembranes",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025089521&doi=10.1016%2fj.bbamem.2017.07.008&partnerID=40&md5=0be525de81c3621fb832cd53f2b0c604","Background Membrane proteins (MPs) play diverse and important functions in living organisms. They constitute 20% to 30% of the known bacterial, archaean and eukaryotic organisms' genomes. In humans, their importance is emphasized as they represent 50% of all known drug targets. Nevertheless, experimental determination of their three-dimensional (3D) structure has proven to be both time consuming and rather expensive, which has led to the development of computational algorithms to complement the available experimental methods and provide valuable insights. Scope of review This review highlights the importance of membrane proteins and how computational methods are capable of overcoming challenges associated with their experimental characterization. It covers various MP structural aspects, such as lipid interactions, allostery, and structure prediction, based on methods such as Molecular Dynamics (MD) and Machine-Learning (ML). Major conclusions Recent developments in algorithms, tools and hybrid approaches, together with the increase in both computational resources and the amount of available data have resulted in increasingly powerful and trustworthy approaches to model MPs. General significance Even though MPs are elementary and important in nature, the determination of their 3D structure has proven to be a challenging endeavor. Computational methods provide a reliable alternative to experimental methods. In this review, we focus on computational techniques to determine the 3D structure of MP and characterize their binding interfaces. We also summarize the most relevant databases and software programs available for the study of MPs. © 2017 Elsevier B.V.","Computational modeling; GPCRs; Machine-learning; Membrane proteins; Transporters","carrier protein; G protein coupled receptor; ion channel; membrane protein; multiprotein complex; algorithm; allosterism; human; machine learning; mathematical model; molecular docking; molecular dynamics; oligomerization; priority journal; protein database; protein lipid interaction; protein localization; protein structure; Review; structural bioinformatics; structure analysis",2-s2.0-85025089521
"Barbosa J.M., Asner G.P.","Prioritizing landscapes for restoration based on spatial patterns of ecosystem controls and plant–plant interactions",2017,"Journal of Applied Ecology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029372282&doi=10.1111%2f1365-2664.12857&partnerID=40&md5=a7f3cdbc0c9dc23fa7c546a914fdd5d1","The widespread degradation of natural ecosystems requires cost-efficient restoration techniques that minimize risk and consider context-specific restoration conditions. However, meeting these demands can be difficult because information on ecosystem-level factors controlling vegetation and continuous spatial data on species interactions are often lacking. Using airborne light detection and ranging (LiDAR) data from a Hawaiian dry forest, we delineated crowns and assessed the 3D structure of more than 700 000 shrubs and trees. We used Random Forest machine learning to assess the relative importance of resource availability, environmental conditions, and disturbance regimes on canopy density. We then modelled and scaled up plant–plant interactions (i.e. potential nursery effects) to landscape units using a LiDAR-derived Canopy Coalescence (CC) index. We used the relative importance of ecosystem factors, canopy cover, and CC index to prioritize landscapes for restoration. Here, we demonstrate a methodological framework that prioritizes landscapes in need of restoration (i.e. planting woody species) using two ecological perspectives: (i) ecosystem-level controls on remnant woody vegetation, and (ii) potential interactions between established canopies and seedlings (e.g. nursery effect, competition). Our results highlight the heterogeneous nature of ecosystem-level drivers affecting forest structure along elevation gradients. Consequently, the degree of potential nursery interactions between established canopies and seedlings at the landscape-scale was context-specific. Synthesis and applications. Our study provides a methodological approach that prioritizes landscapes for restoration by identifying the main controls on tree spatial distribution and by inferring the favourable conditions for seedlings. This approach can guide land managers to define cost-efficient restoration strategies for large ecological areas. © 2016 The Authors. Journal of Applied Ecology © 2016 British Ecological Society","Carnegie Airborne Observatory; facilitation; forest regrowth; landscape ecology; LiDAR; plant–plant interactions; Random Forest machine learning; remote sensing; restoration ecology","canopy; disturbance; facilitation; landscape ecology; lidar; machine learning; prioritization; regrowth; resource availability; restoration ecology; spatial analysis; spatial data; spatial distribution; vegetation type",2-s2.0-85029372282
"Tomeny T.S., Vargo C.J., El-Toukhy S.","Geographic and demographic correlates of autism-related anti-vaccine beliefs on Twitter, 2009-15",2017,"Social Science and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029469486&doi=10.1016%2fj.socscimed.2017.08.041&partnerID=40&md5=dc92c9670358eabf0066dc7ce590298c","This study examines temporal trends, geographic distribution, and demographic correlates of anti-vaccine beliefs on Twitter, 2009–2015. A total of 549,972 tweets were downloaded and coded for the presence of anti-vaccine beliefs through a machine learning algorithm. Tweets with self-disclosed geographic information were resolved and United States Census data were collected for corresponding areas at the micropolitan/metropolitan level. Trends in number of anti-vaccine tweets were examined at the national and state levels over time. A least absolute shrinkage and selection operator regression model was used to determine census variables that were correlated with anti-vaccination tweet volume. Fifty percent of our sample of 549,972 tweets collected between 2009 and 2015 contained anti-vaccine beliefs. Anti-vaccine tweet volume increased after vaccine-related news coverage. California, Connecticut, Massachusetts, New York, and Pennsylvania had anti-vaccination tweet volume that deviated from the national average. Demographic characteristics explained 67% of variance in geographic clustering of anti-vaccine tweets, which were associated with a larger population and higher concentrations of women who recently gave birth, households with high income levels, men aged 40 to 44, and men with minimal college education. Monitoring anti-vaccination beliefs on Twitter can uncover vaccine-related concerns and misconceptions, serve as an indicator of shifts in public opinion, and equip pediatricians to refute anti-vaccine arguments. Real-time interventions are needed to counter anti-vaccination beliefs online. Identifying clusters of anti-vaccination beliefs can help public health professionals disseminate targeted/tailored interventions to geographic locations and demographic sectors of the population. © 2017 Elsevier Ltd","Autism spectrum disorder; Beliefs; Big data; Machine learning algorithms; Social media; Twitter; Vaccines","algorithm; demography; geographical distribution; Internet; machine learning; mental health; social media; temporal variation; vaccine; adolescent; adult; age; Article; autism; birth; California; Connecticut; controlled study; correlational study; demography; educational status; female; gender; geographic distribution; health belief; highest income group; household; human; information dissemination; machine learning; male; Massachusetts; New York; online system; Pennsylvania; population research; public opinion; social media; therapeutic misconception; trend study; vaccination; United States",2-s2.0-85029469486
"Hernandez-Meza G., Izzetoglu M., Sacan A., Green M., Izzetoglu K.","Investigation of data-driven optical neuromonitoring approach during general anesthesia with sevoflurane",2017,"Neurophotonics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028524994&doi=10.1117%2f1.NPh.4.4.041408&partnerID=40&md5=bec0bf8a49246da81033134de0d6b628","Anesthesia monitoring currently needs a reliable method to evaluate the effects of the anesthetics on its primary target, the brain. This study focuses on investigating the clinical usability of a functional near-infrared spectroscopy (fNIRS)-derived machine learning classifier to perform automated and real-time classification of maintenance and emergence states during sevoflurane anesthesia. For 19 surgical procedures, we examine the entire continuum of the maintenance-transition-emergence phases and evaluate the predictive capability of a support vector machine (SVM) classifier during these phases. We demonstrate the robustness of the predictions made by the SVM classifier and compare its performance with that of minimum alveolar concentration (MAC) and bispectral (BIS) index-based predictions. The fNIRS-SVM investigated in this study provides evidence to the usability of the fNIRS signal for anesthesia monitoring. The method presented enables classification of the signal as maintenance or emergence automatically as well as in real-time with high accuracy, sensitivity, and specificity. The features local mean HbTotal, std HbO2, local min Hb and HbO2, and range Hb and HbO2 were found to be robust biomarkers of this binary classification task. Furthermore, fNIRS-SVM was capable of identifying emergence before movement in a larger number of patients than BIS and MAC. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","anesthesia monitoring; cerebral hemodynamics; depth of anesthesia; functional near-infrared spectroscopy; machine learning","Artificial intelligence; Infrared devices; Learning systems; Maintenance; Near infrared spectroscopy; Spectroscopy; Support vector machines; Anesthesia monitoring; Binary classification; Cerebral hemodynamics; Depth of anesthesia; Functional near infrared spectroscopy; Functional near-infrared spectroscopy (fnirs); General anesthesias; Predictive capabilities; Anesthesiology; desflurane; ephedrine; fentanyl; lidocaine; midazolam; morphine; nitrous oxide; propofol; rocuronium; sevoflurane; adult; anesthesia induction; anesthesia level; Article; bispectral index; brain blood flow; clinical article; clinical evaluation; cohort analysis; concentration (parameters); controlled study; data analysis; female; functional near-infrared spectroscopy; general anesthesia; human; male; measurement accuracy; minimum alveolar concentration; neuroimaging; neuromonitoring; observational study; predictive value; sensitivity and specificity; signal processing; support vector machine",2-s2.0-85028524994
"Hsiao S.-W., Chen S.-K., Lee C.-H.","Methodology for stage lighting control based on music emotions",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019625713&doi=10.1016%2fj.ins.2017.05.026&partnerID=40&md5=273e15e75f4401f136d5826a8b70821e","Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music. © 2017 Elsevier Inc.","Automatic music segment detection; Automatic stage-lighting regulation; Lighting color regulation based on music emotions and genre; Music emotion recognition; Support vector regression (SVR)","Electronic musical instruments; Learning systems; Speech recognition; Supervised learning; Support vector machines; Cross-validation tests; Music emotions; Music segments; Musical instrument digital interfaces; Neural network algorithm; Stage lighting; Supervised machine learning; Support vector regression (SVR); Lighting",2-s2.0-85019625713
"Safavi A., Zadeh M.H.","Teaching the user by learning from the user: Personalizing movement control in physical human-robot interaction",2017,"IEEE/CAA Journal of Automatica Sinica",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029807578&doi=10.1109%2fJAS.2017.7510634&partnerID=40&md5=00340d750728c8ffb9da03d9cefa2de5","This paper proposes a novel approach for physical human-robot interactions (pHRI), where a robot provides guidance forces to a user based on the user performance. This framework tunes the forces in regards to behavior of each user in coping with different tasks, where lower performance results in higher intervention from the robot. This personalized physical human-robot interaction (p2HRI) method incorporates adaptive modeling of the interaction between the human and the robot as well as learning from demonstration (LfD) techniques to adapt to the users'performance. This approach is based on model predictive control where the system optimizes the rendered forces by predicting the performance of the user. Moreover, continuous learning of the user behavior is added so that the models and personalized considerations are updated based on the change of user performance over time. Applying this framework to a field such as haptic guidance for skill improvement, allows a more personalized learning experience where the interaction between the robot as the intelligent tutor and the student as the user, is better adjusted based on the skill level of the individual and their gradual improvement. The results suggest that the precision of the model of the interaction is improved using this proposed method, and the addition of the considered personalized factors to a more adaptive strategy for rendering of guidance forces. © 2014 Chinese Association of Automation.","Haptic guidance; learning from demonstration (LfD); personalized physical human-robot interaction (p2HRI); user performance","Air navigation; Behavioral research; Human computer interaction; Intelligent robots; Man machine systems; Model predictive control; Robots; Students; Teaching; Continuous learning; Haptic guidance; Intelligent tutors; Learning from demonstration; Personalized learning; Physical human-robot interactions; Physical humanrobot interaction (phri); User performance; Human robot interaction",2-s2.0-85029807578
"Shrivastava V.K., Londhe N.D., Sonawane R.S., Suri J.S.","A novel and robust Bayesian approach for segmentation of psoriasis lesions and its risk stratification",2017,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026869880&doi=10.1016%2fj.cmpb.2017.07.011&partnerID=40&md5=8c896a0a11feae03c3a3cb36c0c544bb","Background and Objective The need for characterization of psoriasis lesion severity is clinically valuable and vital for dermatologists since it provides a reliable and precise decision on risk assessment. The automated delineation of lesion is a prerequisite prior to characterization, which is challenging itself. Thus, this paper has two major objectives: (a) design of a segmentation system which can model by learning the lesion characteristics and this is posed as a Bayesian model; (b) develop a psoriasis risk assessment system (pRAS) by crisscrossing the blocks which drives the fundamental machine learning paradigm. Methods The segmentation system uses the knowledge derived by the experts along with the features reflected by the lesions to build a Bayesian framework that helps to classify each pixel of the image into lesion vs. background. Since this lesion has several stages and grades, hence the system undergoes the risk assessment to classify into five levels of severity: healthy, mild, moderate, severe and very severe. We build nine kinds of pRAS utilizing different combinations of the key blocks. These nine pRAS systems use three classifiers (Support Vector Machine (SVM), Decision Tree (DT) and Neural Network (NN)) and three feature selection techniques (Principal Component Analysis (PCA), Fisher Discriminant Ratio (FDR) and Mutual Information (MI)). The two major experiments conducted using these nine systems were: (i) selection of best system combination based on classification accuracy and (ii) understanding the reliability of the system. This leads us to computation of key system performance parameters such as: feature retaining power, aggregated feature effect and reliability index besides conventional attributes like accuracy, sensitivity, specificity. Results Using the database used in this study consisted of 670 psoriasis images, the combination of SVM and FDR was revealed as the optimal pRAS system and yielded a classification accuracy of 99.84% using cross-validation protocol. Further, SVM-FDR system provides the reliability of 99.99% using cross-validation protocol. Conclusions The study demonstrates a fully novel model of segmentation embedded with risk assessment. Among all nine systems, SVM-FDR produced best results. Further, we validated our pRAS system with automatic segmented lesions against manually segmented lesions showing comparable performance. © 2017 Elsevier B.V.","Bayesian segmentation; Color features; Machine learning; Performance evaluation; Psoriasis; Texture features","Artificial intelligence; Bayesian networks; Classification (of information); Decision trees; Dermatology; Image segmentation; Learning systems; Principal component analysis; Reliability; Support vector machines; Bayesian segmentation; Color features; Performance evaluation; Psoriasis; Texture features; Risk assessment; Article; Bayes theorem; benchmarking; classifier; decision tree; diagnostic accuracy; disease severity assessment; Fsher Discriminant Ratio; image segmentation; machine learning; Mutual Information; nerve cell network; principal component analysis; psoriasis; risk assessment; support vector machine",2-s2.0-85026869880
"Qi Z., Wang B., Meng F., Niu L.","Learning with Label Proportions via NPSVM",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029216712&doi=10.1109%2fTCYB.2016.2598749&partnerID=40&md5=1655e1c144874c51c5821fbf6b91bd01","Recently, learning from label proportions (LLPs), which seeks generalized instance-level predictors merely based on bag-level label proportions, has attracted widespread interest. However, due to its weak label scenario, LLP usually falls into a transductive learning framework accounting for an intractable combinatorial optimization issue. In this paper, we propose a brand new algorithm, called LLPs via nonparallel support vector machine (LLP-NPSVM), to facilitate this dilemma. To harness satisfactory data adaption, instead of transductive learning fashion, our scheme determined instance labels according to two nonparallel hyper-planes under the supervision of label proportion information. In a geometrical view, our approach can be interpreted as an alternative competitive method benefiting from large margin clustering. In practice, LLP-NPSVM can be efficiently addressed by applying two fast sequential minimal optimization paths iteratively. To rationally support the effectiveness of our method, finite termination and monotonic decrease of the proposed LLP-NPSVM procedure were essentially analyzed. Various experiments demonstrated our algorithm enjoys rapid convergence and robust numerical stability, along with best accuracies among several recently developed methods in most cases. © 2013 IEEE.","k-plane clustering; learning with label proportions (LLPs); nonparallel support vector machine (NPSVM)","Combinatorial optimization; Convergence of numerical methods; Numerical methods; Optimization; Support vector machines; Finite termination; K-plane clustering; learning with label proportions (LLPs); Monotonic decrease; nonparallel support vector machine (NPSVM); Rapid convergence; Sequential minimal optimization; Transductive learning; Iterative methods",2-s2.0-85029216712
"Nomura Y.","Pervasive artificial intelligence",2017,"NTT Technical Review",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031676845&partnerID=40&md5=15490b8abb68e55783e74e45de607dca","Widely accessible advanced machine learning will result in the expanded use of artificial intelligence (AI). AI will increase convenience, resolve intellectual labor shortages, and drastically advance science. Consequently, mastering AI will become a critical component of competitiveness.","Artificial intelligence; Deep learning; Machine learning","Deep learning; Learning systems; Critical component; Labor shortages; Artificial intelligence",2-s2.0-85031676845
"Gao S., Van 'T Klooster R., Kitslaar P.H., Coolen B.F., Van Den Berg A.M., Smits L.P., Shahzad R., Shamonin D.P., De Koning P.J.H., Nederveen A.J., Van Der Geest R.J.","Learning-based automated segmentation of the carotid artery vessel wall in dual-sequence MRI using subdivision surface fitting",2017,"Medical Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031299200&doi=10.1002%2fmp.12476&partnerID=40&md5=6526d23ef6ea3fcf2a597b66617c13b5","Purpose: The quantification of vessel wall morphology and plaque burden requires vessel segmentation, which is generally performed by manual delineations. The purpose of our work is to develop and evaluate a new 3D model-based approach for carotid artery wall segmentation from dual-sequence MRI. Methods: The proposed method segments the lumen and outer wall surfaces including the bifurcation region by fitting a subdivision surface constructed hierarchical-tree model to the image data. In particular, a hybrid segmentation which combines deformable model fitting with boundary classification was applied to extract the lumen surface. The 3D model ensures the correct shape and topology of the carotid artery, while the boundary classification uses combined image information of 3D TOF-MRA and 3D BB-MRI to promote accurate delineation of the lumen boundaries. The proposed algorithm was validated on 25 subjects (48 arteries) including both healthy volunteers and atherosclerotic patients with 30% to 70% carotid stenosis. Results: For both lumen and outer wall border detection, our result shows good agreement between manually and automatically determined contours, with contour-to-contour distance less than 1 pixel as well as Dice overlap greater than 0.87 at all different carotid artery sections. Conclusions: The presented 3D segmentation technique has demonstrated the capability of providing vessel wall delineation for 3D carotid MRI data with high accuracy and limited user interaction. This brings benefits to large-scale patient studies for assessing the effect of pharmacological treatment of atherosclerosis by reducing image analysis time and bias between human observers. © 2017 American Association of Physicists in Medicine.","3D segmentation; 3D vessel wall MRI; carotid bifurcation; hierarchical-tree model; subdivision surface","aged; algorithm; arterial wall thickness; Article; automation; blood vessel wall; cardiac imaging; carotid artery; carotid artery bifurcation; carotid artery obstruction; carotid atherosclerosis; clinical article; comparative study; controlled study; dual sequence nuclear magnetic resonance imaging; female; hierarchical tree model; human; image analysis; image processing; image reconstruction; image segmentation; machine learning; male; measurement accuracy; measurement error; model; nuclear magnetic resonance imaging; subdivision surface fitting; three dimensional imaging; validation process",2-s2.0-85031299200
"Furlow B.","Deep learning poised to revolutionise diagnostic imaging",2017,"The Lancet Respiratory Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025462961&doi=10.1016%2fS2213-2600%2817%2930292-8&partnerID=40&md5=9dd8cd34fbeb240af36f34429546a8a7",[No abstract available],,"artificial neural network; automation; chronic obstructive lung disease; colon polyp; computed tomographic colonography; computer assisted diagnosis; computer assisted tomography; diagnostic accuracy; diagnostic imaging; diagnostic test accuracy study; false positive result; human; human computer interaction; image analysis; image processing; imaging software; interstitial lung disease; lung angiography; lung disease; lung embolism; machine learning; natural language processing; Note; priority journal; radiologist; sensitivity and specificity; spine metastasis",2-s2.0-85025462961
"Daee P., Peltola T., Soare M., Kaski S.","Knowledge elicitation via sequential probabilistic inference for high-dimensional prediction",2017,"Machine Learning",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023185870&doi=10.1007%2fs10994-017-5651-7&partnerID=40&md5=07641bb3601aeb242e2176ef0abf0a9d","Prediction in a small-sized sample with a large number of covariates, the “small n, large p” problem, is challenging. This setting is encountered in multiple applications, such as in precision medicine, where obtaining additional data can be extremely costly or even impossible, and extensive research effort has recently been dedicated to finding principled solutions for accurate prediction. However, a valuable source of additional information, domain experts, has not yet been efficiently exploited. We formulate knowledge elicitation generally as a probabilistic inference process, where expert knowledge is sequentially queried to improve predictions. In the specific case of sparse linear regression, where we assume the expert has knowledge about the relevance of the covariates, or of values of the regression coefficients, we propose an algorithm and computational approximation for fast and efficient interaction, which sequentially identifies the most informative features on which to query expert knowledge. Evaluations of the proposed method in experiments with simulated and real users show improved prediction accuracy already with a small effort from the expert. © 2017, The Author(s).","Bayesian methods; Experimental design; Human-to-machine transfer learning; Interactive machine learning; Statistics in high dimensions","Approximation algorithms; Bayesian networks; Computational efficiency; Design of experiments; Education; Forecasting; Bayesian methods; Efficient interaction; High dimensions; Interactive machine learning; Probabilistic inference; Regression coefficient; Sparse linear regressions; Transfer learning; Knowledge management",2-s2.0-85023185870
"Cao Y., Xu L., Clausi D.","Exploring the potential of active learning for automatic identification of marine oil spills using 10-year (2004-2013) RADARSAT data",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032864747&doi=10.3390%2frs9101041&partnerID=40&md5=cc1392dae0a3fc0698126575f6114c2e","This paper intends to find a more cost-effective way for training oil spill classification systems by introducing active learning (AL) and exploring its potential, so that satisfying classifiers could be learned with reduced number of labeled samples. The dataset used has 143 oil spills and 124 look-alikes from 198 RADARSAT images covering the east and west coasts of Canada from 2004 to 2013. Six uncertainty-based active sample selecting (ACS) methods are designed to choose the most informative samples. A method for reducing information redundancy amongst the selected samples and a method with varying sample preference are considered. Four classifiers (k-nearest neighbor (KNN), support vector machine (SVM), linear discriminant analysis (LDA) and decision tree (DT)) are coupled with ACS methods to explore the interaction and possible preference between classifiers and ACS methods. Three kinds of measures are adopted to highlight different aspect of classification performance of these AL-boosted classifiers. Overall, AL proves its strong potential with 4% to 78% reduction on training samples in different settings. The SVM classifier shows to be the best one for using in the AL frame, with perfect performance evolving curves in different kinds of measures. The exploration and exploitation criterion can further improve the performance of the AL-boosted SVM classifier but not of the other classifiers. © 2017 by the authors.","Active learning; Active sample selecting; Decision tree; K-nearest neighbor; Linear discriminant analysis; Oil spill detection; SAR; Support vector machine","Aluminum; Artificial intelligence; Automation; Cost effectiveness; Decision trees; Discriminant analysis; Image retrieval; Marine pollution; Motion compensation; Nearest neighbor search; Oil spills; Support vector machines; Synthetic aperture radar; Active Learning; Active sample selecting; K-nearest neighbors; Linear discriminant analysis; Oil spill detection; Learning algorithms",2-s2.0-85032864747
"Sakizadeh M., Rahmatinia H.","Statistical learning methods for classification and prediction of groundwater quality using a small data record",2017,"International Journal of Agricultural and Environmental Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028605461&doi=10.4018%2fIJAEIS.2017100103&partnerID=40&md5=419cd59e47a4ee39883ace98279ad43d","The objective of this study was to consider the efficiency of support vector machine (SVM) and artificial neural network (ANN) for the classification and prediction of groundwater quality using a small data record in Malayer, Iran. For this purpose, 14 groundwater quality variables that had been collected from 27 groundwater sampling wells were used. Cluster analysis discriminated the total sampling stations into two groups. The classification was implemented by SVM using polynomial and RBF kernel methods. The respective sensitivity and specificity of this model were 0.89 and 0.80 while that of positive predictive value and negative predictive value were 0.89 and 0.86, respectively. The prediction of water quality index (WQI) was implemented using ANN. Despite the high correlation coefficient between the predicted and observed values of WQI(r = 0.90), the generalization ability of this model was low(r = 0.60) indicating the over-fitting of the model to the training data set.","Artificial Neural Networks; Cluster Analysis; Groundwater Quality; Support Vector Machines","Cluster analysis; Forecasting; Groundwater; Neural networks; Support vector machines; Water quality; Correlation coefficient; Generalization ability; Groundwater sampling; Negative predictive value; Positive predictive values; Sensitivity and specificity; Statistical learning methods; Water quality indexes; Quality control; artificial neural network; cluster analysis; data set; groundwater; learning; statistical analysis; support vector machine; Hamadan; Iran; Malayer",2-s2.0-85028605461
"Puri V., Chauhan Y.K., Singh N.","A comparative design study and analysis of inner and outer rotor permanent magnet synchronous machine for power generation in vertical axis wind turbine using GSA and GSA-PSO",2017,"Sustainable Energy Technologies and Assessments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216356&doi=10.1016%2fj.seta.2017.09.008&partnerID=40&md5=e7f36ca3c17895f549a5e6095242ac3b","In wind energy conversion system, an electric generator is coupled with a wind turbine. The wind turbine is either vertical axis or horizontal axis wind turbine. To increase the efficiency the gear driven generator has been replaced by directly driven generator. This paper recounts the design optimization of inner and outer rotor permanent magnet synchronous machine of rating 500 KVA, 3.3 kV, 3-phase, and 600 rpm. The weight of the generator has been taken as an objective function. The gravitational search algorithm and its hybridization with particle swarm optimization have been used as tools for optimization. The temperature rise, efficiency, regulation and maximum flux density in stator teeth are taken as constraints for design problem. The results of both types of generator configurations are obtained and compared. It is found that the efficiency of outer rotor permanent magnet synchronous generator is improved by 2.19% and regulation by 2.83% as compared to inner rotor permanent magnet synchronous generator. Lower temperature rise as 22.61 °C is observed for outer rotor permanent magnet synchronous generator whereas it is 31.18 °C in case of outer rotor permanent magnet synchronous machine. Further, the sensitivity analysis has been performed over design parameters using local sensitivity method for both generators. © 2017 Elsevier Ltd","Gravitational search algorithm; Particle swarm optimization and sensitivity analysis; Permanent magnet synchronous machine; Vertical axis wind turbine","Energy conversion; Learning algorithms; Magnets; Optimization; Particle swarm optimization (PSO); Permanent magnets; Sensitivity analysis; Synchronous generators; Synchronous machinery; Wind power; Wind turbines; Gravitational search algorithms; Horizontal axis wind turbines; Inner and outer rotors; Permanent magnet synchronous generator; Permanent magnet synchronous machines; Sensitivity methods; Vertical axis wind turbines; Wind energy conversion system; Electric generators",2-s2.0-85032216356
"Sen B., Mandal U.K., Mondal S.P.","Advancement of an intelligent system based on ANFIS for predicting machining performance parameters of Inconel 690 – A perspective of metaheuristic approach",2017,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019596859&doi=10.1016%2fj.measurement.2017.05.050&partnerID=40&md5=fde2c43a35d7a34e94b25965f511d235","The convincing potential of sophisticated milling tools exploited for machining of Inconel alloys in CNC milling machine offers minimal surface roughness-cutting force-cutting temperature trade-off footprint as compared to the conventional machining operation, which afforded a strong motivation to accomplish an in-depth discovery of the parametric design of CNC milling operation. In this study, the synergistic potential of contemporary Cubic boron nitride coated tool is used for machining of Inconel 690 in CNC milling machine, which has been scrutinized in order to build up a correlation among the objective function and the control variables by a metamodel called Adaptive Neuro-fuzzy inference system (ANFIS). The developed ANFIS model was capable of predicting the performance parameters with commendable accuracy as observed from correlation coefficients within the range of 0.946542–0.988996, Mean absolute percentage error (MAPE) in the range of 3.879652–7.456275% along with noticeably low root mean square errors (RMSE). Moreover, the ANFIS acquired results were compared with an Artificial Neural Network (ANN) model, developed on the identical parametric ranges. The comparison of the obtained results indicated that the ANFIS overtakes the ANN model in predicting the preferred response variables, which suggests the modesty of the ANFIS model. © 2017 Elsevier Ltd","Adaptive Neuro-fuzzy inference system (ANFIS); Cubic boron nitride (CBN) coated tool; Dry milling; Inconel 690; Machine learning; Response surface methodology (RSM); Statistical evolution","Adaptive control systems; Cubic boron nitride; Cutting; Cutting tools; Economic and social effects; Forecasting; Fuzzy neural networks; Fuzzy systems; Intelligent systems; Learning algorithms; Learning systems; Mean square error; Milling (machining); Milling machines; Neural networks; Nitrides; Surface roughness; Tracking (position); Adaptive neuro-fuzzy inference system; Coated tools; Dry milling; Inconel 690; Response surface methodology; Statistical evolution; Fuzzy inference",2-s2.0-85019596859
"Kuntzer T., Courbin F.","Detecting unresolved binary stars in Euclid VIS images",2017,"Astronomy and Astrophysics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032290889&doi=10.1051%2f0004-6361%2f201730792&partnerID=40&md5=22fd83219ff8ce31208c086eb5718d97","Measuring a weak gravitational lensing signal to the level required by the next generation of space-based surveys demands exquisite reconstruction of the point-spread function (PSF). However, unresolved binary stars can significantly distort the PSF shape. In an effort to mitigate this bias, we aim at detecting unresolved binaries in realistic Euclid stellar populations. We tested methods in numerical experiments where (i) the PSF shape is known to Euclid requirements across the field of view; and (ii) the PSF shape is unknown. We drew simulated catalogues of PSF shapes for this proof-of-concept paper. Following the Euclid survey plan, the objects were observed four times. We propose three methods to detect unresolved binary stars. The detection is based on the systematic and correlated biases between exposures of the same object. One method is a simple correlation analysis, while the two others use supervised machine-learning algorithms (random forest and artificial neural network). In both experiments, we demonstrate the ability of our methods to detect unresolved binary stars in simulated catalogues. The performance depends on the level of prior knowledge of the PSF shape and the shape measurement errors. Good detection performances are observed in both experiments. Full complexity, in terms of the images and the survey design, is not included, but key aspects of a more mature pipeline are discussed. Finding unresolved binaries in objects used for PSF reconstruction increases the quality of the PSF determination at arbitrary positions. We show, using different approaches, that we are able to detect at least binary stars that are most damaging for the PSF reconstruction process. © ESO, 2017.","Binaries: close; Methods: data analysis; Methods: statistical","Decision trees; Learning algorithms; Learning systems; Neural networks; Numerical methods; Optical transfer function; Supervised learning; Surveys; Binaries: close; Detection performance; Methods:data analysis; Methods:statistical; Numerical experiments; Reconstruction process; Supervised machine learning; Weak gravitational lensing; Stars",2-s2.0-85032290889
"Li Z., Wang Y., Zhi T., Chen T.","A survey of neural network accelerators",2017,"Frontiers of Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019578579&doi=10.1007%2fs11704-016-6159-1&partnerID=40&md5=d6791b76ee9378dcdb9c35124004de23","Machine-learning techniques have recently been proved to be successful in various domains, especially in emerging commercial applications. As a set of machine-learning techniques, artificial neural networks (ANNs), requiring considerable amount of computation and memory, are one of the most popular algorithms and have been applied in a broad range of applications such as speech recognition, face identification, natural language processing, ect. Conventionally, as a straightforward way, conventional CPUs and GPUs are energy-inefficient due to their excessive effort for flexibility. According to the aforementioned situation, in recent years, many researchers have proposed a number of neural network accelerators to achieve high performance and low power consumption. Thus, the main purpose of this literature is to briefly review recent related works, as well as the DianNao-family accelerators. In summary, this review can serve as a reference for hardware researchers in the area of neural networks. © 2017, Higher Education Press and Springer-Verlag Berlin Heidelberg.","accelerators; ASICs; DianNao series; FPGAs; neural networks","Application specific integrated circuits; Artificial intelligence; Face recognition; Field programmable gate arrays (FPGA); Learning algorithms; Learning systems; Natural language processing systems; Neural networks; Particle accelerators; Program processors; Speech recognition; Commercial applications; DianNao series; Face identification; Low-power consumption; Machine learning techniques; NAtural language processing; Related works; Low power electronics",2-s2.0-85019578579
"Miao Y., Jiang H., Liu H., Yao Y.-D.","An Alzheimers disease related genes identification method based on multiple classifier integration",2017,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027867396&doi=10.1016%2fj.cmpb.2017.08.006&partnerID=40&md5=f4cc4c1b9f2dd7abf02b3f6fa27db5f0","Background and Objective: Alzheimers disease (AD) is a fatal neurodegenerative disease and the onset of AD is insidious. Full understanding of the AD-related genes (ADGs) has not been completed. The National Center for Biotechnology Information (NCBI) provides an AD dataset of 22,283 genes. Among these genes, 71 genes have been identified as ADGs. But there may still be underlying ADGs that have not yet been identified in the remaining 22,212 genes. This paper aims to identify additional ADGs using machine learning techniques. Methods: To improve the accuracy of ADG identification, we propose a gene identification method through multiple classifier integration. First, a feature selection algorithm is applied to select the most relevant attributes. Second, a two-stage cascading classifier is developed to identify ADGs. The first stage classification task is based on the relevance vector machine and, in the second stage, the results of three classifiers, support vector machine, random forest and extreme learning machine, are combined through voting. Results: According to our results, feature selection improves accuracy and reduces training time. Voting based classifier reduces the classification errors. The proposed ADG identification system provides accuracy, sensitivity and specificity at levels of 78.77%, 83.10% and 74.67%, respectively. Based on the proposed ADG identification method, potentially additional ADGs are identified and top 13 genes (predicted ADGs) are presented. Conclusions: In this paper, an ADG identification method for identifying ADGs is presented. The proposed method which combines feature selection, cascading classifier and majority voting leads to higher specificity and significantly increases the accuracy and sensitivity of ADG identification. Potentially new ADGs are identified. © 2017 Elsevier B.V.","Alzheimers disease; Cascading classifier; Feature selection; Gene identification; Majority voting","Classification (of information); Decision trees; Feature extraction; Genes; Learning systems; Alzheimers disease; Feature selection algorithm; Gene identification; Machine learning techniques; Majority voting; National center for biotechnology informations; Relevance Vector Machine; Sensitivity and specificity; Neurodegenerative diseases",2-s2.0-85027867396
"Wu J., Ye C., Sheng V.S., Zhang J., Zhao P., Cui Z.","Active learning with label correlation exploration for multi-label image classification",2017,"IET Computer Vision",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572773&doi=10.1049%2fiet-cvi.2016.0243&partnerID=40&md5=f29da67811350a1f3ccb074e9b76e64e","Multi-label image classification has attracted considerable attention in machine learning recently. Active learning is widely used in multi-label learning because it can effectively reduce the human annotation workload required to construct high-performance classifiers. However, annotation by experts is costly, especially when the number of labels in a dataset is large. Inspired by the idea of semi-supervised learning, in this study, the authors propose a novel, semi-supervised multi-label active learning (SSMAL) method that combines automated annotation with human annotation to reduce the annotation workload associated with the active learning process. In SSMAL, they capture three aspects of potentially useful information - classification prediction information, label correlation information, and example spatial information - and they use this information to develop an effective strategy for automated annotation of selected unlabelled example-label pairs. The experimental results obtained in this study demonstrate the effectiveness of the authors' proposed approach. © 2017, The Institution of Engineering and Technology.",,"Artificial intelligence; Classifiers; Image classification; Learning algorithms; Learning systems; Supervised learning; Active-learning process; Human annotations; Information classification; Label correlations; Multi-label learning; Semi- supervised learning; Semi-supervised; Spatial informations; Classification (of information)",2-s2.0-85029572773
"Jouault C., Seta K., Hayashi Y.","SOLS: An LOD based semantically enhanced open learning space supporting self-directed learning of history",2017,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030236836&doi=10.1587%2ftransinf.2016EDP7417&partnerID=40&md5=703f5869a8b65140747f6b9e183e36a9","The purpose of this research is to support learners in selfdirected learning on the Internet using automatically generated support using the current state of the semantic web. The main issue of creating meaningful content-dependent questions automatically is that it requires the machine to understand the concepts in the learning domain. The originality of this work is that it uses Linked Open Data (LOD) to enable meaningful content-dependent support in open learning space. Learners are supported by a learning environment, the Semantic Open Learning Space (SOLS). Learners use the system to build a concept map representing their knowledge. SOLS supports learners following the principle of inquiry-based learning. Learners that request help are provided with automatically generated questions that give them learning objectives. To verify whether the current system can support learners with fully automatically generated support, we evaluated the system with three objectives: judge whether the LOD based support was feasible and useful, whether the question support improved the development of historical considerations in the learners' mind and whether the engagement of learners was improved by the question support. The results showed that LOD based support was feasible. Learners felt that the support provided was useful and helped them learn. The question support succeeded in improving the development of learners' deep historical considerations. In addition, the engagement and interest in history of learners was improved by the questions. The results are meaningful because they show that LOD based question support can be a viable tool to support self-directed learning in open learning space. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","History learning; Linked open data; Question generation; Semantic open learning space","Computer aided instruction; Sols; Automatically generated; Inquiry-based learning; Learning environments; Linked open data (LOD); Linked open datum; Open learning; Question generation; Self-directed learning; Learning systems",2-s2.0-85030236836
"Sandino J., Wooler A., Gonzalez F.","Towards the automatic detection of pre-existing termite mounds through UAS and hyperspectral imagery",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030114703&doi=10.3390%2fs17102196&partnerID=40&md5=51efc117926996ce1d917b206be8ff5d","The increased technological developments in Unmanned Aerial Vehicles (UAVs) combined with artificial intelligence and Machine Learning (ML) approaches have opened the possibility of remote sensing of extensive areas of arid lands. In this paper, a novel approach towards the detection of termite mounds with the use of a UAV, hyperspectral imagery, ML and digital image processing is intended. A new pipeline process is proposed to detect termite mounds automatically and to reduce, consequently, detection times. For the classification stage, several ML classification algorithms’ outcomes were studied, selecting support vector machines as the best approach for their role in image classification of pre-existing termite mounds. Various test conditions were applied to the proposed algorithm, obtaining an overall accuracy of 68%. Images with satisfactory mound detection proved that the method is “resolution-dependent”. These mounds were detected regardless of their rotation and position in the aerial image. However, image distortion reduced the number of detected mounds due to the inclusion of a shape analysis method in the object detection phase, and image resolution is still determinant to obtain accurate results. Hyperspectral imagery demonstrated better capabilities to classify a huge set of materials than implementing traditional segmentation methods on RGB images only. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Hyperspectral camera; Image segmentation; Machine learning; Pre-existing termite mounds; Support vector machines; UAV","Aircraft detection; Artificial intelligence; Image processing; Image resolution; Learning systems; Object detection; Remote sensing; Spectroscopy; Support vector machines; Unmanned aerial vehicles (UAV); Automatic Detection; Classification algorithm; Hyper-spectral cameras; Hyper-spectral imageries; Pre-existing termite mounds; Segmentation methods; Shape analysis method; Technological development; Image segmentation",2-s2.0-85030114703
"Xin X., Di K., Wang Y., Wan W., Yue Z.","Automated detection of new impact sites on Martian surface from HiRISE images",2017,"Advances in Space Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026315371&doi=10.1016%2fj.asr.2017.06.044&partnerID=40&md5=cb1d4cad7e49ecab01a3fc01b5eb9ae9","In this study, an automated method for Martian new impact site detection from single images is presented. It first extracts dark areas in full high resolution image, then detects new impact craters within dark areas using a cascade classifier which combines local binary pattern features and Haar-like features trained by an AdaBoost machine learning algorithm. Experimental results using 100 HiRISE images show that the overall detection rate of proposed method is 84.5%, with a true positive rate of 86.9%. The detection rate and true positive rate in the flat regions are 93.0% and 91.5%, respectively. © 2017","AdaBoost; Automatic detection; HiRISE; Machine learning; Martian surface; New impact sites","Adaptive boosting; Artificial intelligence; Classification (of information); Education; Learning systems; Automated detection; Automatic Detection; High resolution image; HiRISE; Impact site; Local binary patterns; Martian surface; True positive rates; Learning algorithms",2-s2.0-85026315371
"Ding J., Wang H., Li C., Chai T., Wang J.","An online learning neural network ensembles with random weights for regression of sequential data stream",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979263695&doi=10.1007%2fs00500-016-2269-9&partnerID=40&md5=4cfcbe50a02ed8889e4a8cb102b4123d","An ensemble of neural networks has been proved to be an effective machine learning framework. However, very limited studies in the current literature examined the neural network ensemble for online regression; furthermore, these methods were combination of online individual models and did not consider the ensemble diversity. In this paper, a novel online sequential learning algorithm for neural network ensembles for online regression is proposed. The algorithm is built upon the decorrelated neural network ensembles (DNNE) and thus referred to as Online-DNNE; so it uses single-hidden layer feed-forward neural networks with random hidden nodes’ parameters as ensemble components and introduces negative correlation learning to train base models simultaneously in a cooperative manner which can effectively maintain the ensemble diversity. The Online-DNNE only learns the newly arrived data, and the computation complexity is thus reduced. The results of the experiments with benchmarks show the effectiveness and significant advantages of the proposed approach. © 2016, Springer-Verlag Berlin Heidelberg.","Decorrelated neural network; Negative correlation learning; Neural network ensembles; Online sequential learning algorithm","Artificial intelligence; Complex networks; E-learning; Learning systems; Regression analysis; Computation complexity; Individual models; Negative correlation learning; Neural network ensembles; Random hidden nodes; Sequential data; Sequential learning algorithm; Single-hidden layer feed-forward neural network; Learning algorithms",2-s2.0-84979263695
"Chaudhry T., Moinuddin K.","Method of identifying burning material from its smoke using attenuation of light",2017,"Fire Safety Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029383518&doi=10.1016%2fj.firesaf.2017.08.001&partnerID=40&md5=0ab801402a00c189c3ba63f93eb18c3e","In this study, it is verified that several materials can be accurately distinguished from their aerosols or from the smoke they emit when they are burnt individually. This is done by comparisons of transmitted and scattered light at various wavelengths using a Machine Learning Algorithm. Smoke was introduced in the paths of light of different wavelengths, simultaneously. The wavelengths were chosen from widest spectrum of radiation, for which LEDs and photodiodes were available commercially. These include UVC 275 nm, UVA 365 nm, Blue 405 nm, Red 620 nm and IR 960 nm. At least one photodiode was used to sense transmitted and at least one photodiode to sense scattered light from each wavelength of light. Each smoke or aerosol, from a single material, was tested many times to create large datasets. After a selection process, a Machine Learning Algorithm, namely Random Forest, was trained with the data from all materials burnt. It was found that a number of materials that are commonly involved in building fires can be identified with high accuracy using this model. The materials were identified with an accuracy of 99.6%–59%, which are N-Heptane, polyester carpet, Can smoke, PVC insulated wire, polyurethane foam, cotton fabric, cardboard, cigarette and polystyrene foam. The proposed method provides a model, whose accuracy is quantifiable, with easily trainable algorithm for new materials and can be tailored for certain materials of interest. © 2017 Elsevier Ltd","Fuel identification; Light attenuation; Machine learning algorithm; Quantifiable accuracy; Random forest; Smoke detection; Wavelength","Aerosols; Artificial intelligence; Decision trees; Heptane; Learning systems; Light scattering; Photodiodes; Smoke; Wavelength; Fuel identification; Light attenuation; Quantifiable accuracy; Random forests; Smoke detection; Learning algorithms; Aerosols; Algorithms; Forests; Light; Smoke",2-s2.0-85029383518
"Liu K., Zeng X., Bruniaux P., Wang J., Kamalha E., Tao X.","Fit evaluation of virtual garment try-on by learning from digital pressure data",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022033035&doi=10.1016%2fj.knosys.2017.07.007&partnerID=40&md5=e9f5f018cef82acc4d5c9d58a832f1c8","Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies. © 2017","Active learning; Digital clothing pressure; Ease allowance; Naive Bayes; Real try-on; Support vector machines","Artificial intelligence; Classifiers; Computer aided design; Education; Electronic commerce; Support vector machines; Active Learning; Clothing pressure; Ease allowance; Naive bayes; Real try-on; E-learning",2-s2.0-85022033035
"Lauer C.J., Montgomery C.A., Dietterich T.G.","Spatial interactions and optimal forest management on a fire-threatened landscape",2017,"Forest Policy and Economics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024384894&doi=10.1016%2fj.forpol.2017.07.006&partnerID=40&md5=df00446a17930564613a9dd4edae7d21","Forest management in the face of fire risk is a challenging problem because fire spreads across a landscape and because its occurrence is unpredictable. Accounting for the existence of stochastic events that generate spatial interactions in the context of a dynamic decision process is crucial for determining optimal management. This paper demonstrates a method for incorporating spatial information and interactions into management decisions made over time. A machine learning technique called approximate dynamic programming is applied to determine the optimal timing and location of fuel treatments and timber harvests for a fire-threatened landscape. Larger net present values can be achieved using policies that explicitly consider evolving spatial interactions created by fire spread, compared to policies that ignore the spatial dimension of the inter-temporal optimization problem. © 2017 Elsevier B.V.","Approximate dynamic programming; Ecological disturbance; Forestry; Reinforcement learning; Risk; Spatial; Wildland fire","Decision making; Ecology; Education; Fires; Forestry; Learning systems; Optimization; Reinforcement learning; Risks; Stochastic systems; Timber; Approximate dynamic programming; Ecological disturbance; Machine learning techniques; Optimal forest managements; Optimization problems; Spatial; Spatial informations; Wildland fire; Dynamic programming",2-s2.0-85024384894
"da Silva J.F., Cavaco S., Lopes G.P.","Automatic Classification of Impact Sounds with Rejection of Unknown Samples",2017,"New Generation Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026829472&doi=10.1007%2fs00354-017-0025-z&partnerID=40&md5=fb330925ed7569eb999fbd83e8fc15dc","The discrimination of very similar sounds is a hard task both for artificial systems and humans. For the former, the main problem lies on finding appropriate features to discriminate each class of sounds, which is an especially hard task when the sounds are very similar, such as impacts on rods of different metals. This paper presents a method to automatically select the features to be used in the classification. Given an initial large set of features, the method measures their discriminative power and builds a reduced set of new features which discriminates the sound classes very accurately. This feature selection method is part of the learning phase of a supervised classification approach also proposed here. In addition, this approach contains a module that rejects unknown sounds also very accurately. This is also an important innovation since most audio classifiers assume all test sounds belong to one of the known classes. © 2017, Ohmsha, Ltd. and Springer Japan KK.","Feature selection; Impact sounds classification; Machine learning","Acoustic waves; Audio acoustics; Feature extraction; Learning systems; Supervised learning; Artificial systems; Automatic classification; Discriminative power; Feature selection methods; Hard task; Impact sound; Learning phase; Supervised classification; Classification (of information)",2-s2.0-85026829472
"Lopes U.K., Valiati J.F.","Pre-trained convolutional neural networks as feature extractors for tuberculosis detection",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026881283&doi=10.1016%2fj.compbiomed.2017.08.001&partnerID=40&md5=396e9bae77f2fa9c6ef8e078f46bb385","It is estimated that in 2015, approximately 1.8 million people infected by tuberculosis died, most of them in developing countries. Many of those deaths could have been prevented if the disease had been detected at an earlier stage, but the most advanced diagnosis methods are still cost prohibitive for mass adoption. One of the most popular tuberculosis diagnosis methods is the analysis of frontal thoracic radiographs; however, the impact of this method is diminished by the need for individual analysis of each radiography by properly trained radiologists. Significant research can be found on automating diagnosis by applying computational techniques to medical images, thereby eliminating the need for individual image analysis and greatly diminishing overall costs. In addition, recent improvements on deep learning accomplished excellent results classifying images on diverse domains, but its application for tuberculosis diagnosis remains limited. Thus, the focus of this work is to produce an investigation that will advance the research in the area, presenting three proposals to the application of pre-trained convolutional neural networks as feature extractors to detect the disease. The proposals presented in this work are implemented and compared to the current literature. The obtained results are competitive with published works demonstrating the potential of pre-trained convolutional networks as medical image feature extractors. © 2017 Elsevier Ltd","Computer assisted diagnosis; Convolutional neural networks; Deep learning; Ensemble learning; Multiple instance learning; Tuberculosis","Computer aided diagnosis; Computer aided instruction; Convolution; Cost benefit analysis; Deep learning; Developing countries; Feature extraction; Medical imaging; Neural networks; Tubes (components); Computer assisted diagnosis; Convolutional neural network; Ensemble learning; Multiple instance learning; Tuberculosis; Diagnosis; accuracy; Article; convolutional neural network; human; image analysis; machine learning; priority journal; radiography; tuberculosis",2-s2.0-85026881283
"Lu X., Wang K., Qiao L., Zhou W., Wang Y., Chi N.","Nonlinear Compensation of Multi-CAP VLC System Employing Clustering Algorithm Based Perception Decision",2017,"IEEE Photonics Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029160116&doi=10.1109%2fJPHOT.2017.2748153&partnerID=40&md5=71e7961c14c1f447233496e02c9bda91","Nonlinearities induced by the electrical amplifiers and the optoelectronic devices can be detrimental effects in visible light communication (VLC) systems. In this paper, clustering algorithm based perception decision (CAPD) is proposed to mitigate the nonlinear distortion in a VLC system. Aided by CAPD nonlinear compensation, we experimentally demonstrate a multiband CAP modulated VLC system consisting of a red light-emitting diode as a transmitter and a p-i-n photodiode based differential receiver. The system performances including the Q factor, bit error rate (BER), and computational complexity are thoroughly investigated when using a pure linear blind equalization scheme (modified cascaded multimodulus algorithm, M-CMMA) and when using hybrid linear and nonlinear equalizers (M-CMMA + Volterra series based nonlinear equalizer). The experiment results show that compared to pure linear equalizer case, the measured BER can be enhanced up to 1e-6, correspondingly the Q factor of each subband can be improved for around 1.6-2.5 dB by employing CAPD. The CAPD method can outperform the Volterra series based nonlinear equalizer with a lower BER value (at least 10% reduction) and relatively lower complexity. To the best of our knowledge, this is the first time that the clustering algorithm in machine learning is successfully applied to VLC systems. © 2009-2012 IEEE.","clustering algorithm-based perception decision.; machine learning; nonlinear distortion; Visible light communication","Artificial intelligence; Blind equalization; Distortion (waves); Equalizers; Learning algorithms; Learning systems; Light; Light emitting diodes; Nonlinear distortion; Optical communication; Optoelectronic devices; Q factor measurement; Quadrature amplitude modulation; Visible light communication; Differential receivers; Linear equalizer; Lower complexity; Multimodulus algorithm; Non-linear compensations; Nonlinear equalizer; Perception decision; Visible light communications (VLC); Clustering algorithms",2-s2.0-85029160116
"Greff K., Srivastava R.K., Koutnik J., Steunebrink B.R., Schmidhuber J.","LSTM: A Search Space Odyssey",2017,"IEEE Transactions on Neural Networks and Learning Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979010616&doi=10.1109%2fTNNLS.2016.2582924&partnerID=40&md5=09bc7ad0127895c9ff6491b721c0f1d8","Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: Speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ≈15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment. © 2012 IEEE.","Functional ANalysis Of VAriance (fANOVA); long short-term memory (LSTM); random search; recurrent neural networks; sequence learning","Artificial intelligence; Character recognition; Computer music; Learning systems; Network architecture; Speech recognition; Activation functions; Computational components; Critical component; Handwriting recognition; Large-scale analysis; Long short term memory; Machine learning problem; Polyphonic music; Recurrent neural networks",2-s2.0-84979010616
"Wang C., Li Z., Mo X., Yang H., Zhao Y.","An android malware dynamic detection method based on service call co-occurrence matrices",2017,"Annales des Telecommunications/Annals of Telecommunications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019218181&doi=10.1007%2fs12243-017-0580-9&partnerID=40&md5=f49d3b41b39cd778a8d960c345f26c18","With the market share of Android mobile devices increasing, Android has come to dominate the smartphone operating system market. It also draws the attention of malware authors and researchers. The number of Android malicious applications is constantly increasing. However, due to the limitations of static detection in code obfuscation and dynamic loading, the current research of Android malicious code detection needs to be deeply studied in dynamic detection. In this paper, a new Android malware identification method is proposed. This method extracts the feature of Android system service call sequences by using a co-occurrence matrix and uses machine-learning algorithm to classify the feature sequence and to verify whether this feature sequence can expose Android malware behaviors or not. By using 750 malware samples and 1000 benign samples, this paper has designed an experiment to evaluate this method. The results show that this method has a high detection precision rate (97.1%) in the best case and a low false-positive rate (2.1%) in the worst case based on the system service call co-occurrence matrix. © 2017, Institut Mines-Télécom and Springer-Verlag France.","Android; Android malware identify; Co-occurrence matrix; Machine-learning; System service call","Artificial intelligence; Classification (of information); Commerce; Competition; Computer crime; Dynamic loads; Learning algorithms; Learning systems; Malware; Android; Android malware; Co-occurrence-matrix; Detection precision; False positive rates; Identification method; Malicious code detection; System services; Android (operating system)",2-s2.0-85019218181
"Ramírez-Gallego S., Krawczyk B., García S., Wózniak M., Benítez J.M., Herrera F.","Nearest neighbor classification for high-speed big data streams using spark",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028953120&doi=10.1109%2fTSMC.2017.2700889&partnerID=40&md5=79802f894d9b8f386aee95a23348df00","Mining massive and high-speed data streams among the main contemporary challenges in machine learning. This calls for methods displaying a high computational efficacy, with ability to continuously update their structure and handle ever-arriving big number of instances. In this paper, we present a new incremental and distributed classifier based on the popular nearest neighbor algorithm, adapted to such a demanding scenario. This method, implemented in Apache Spark, includes a distributed metric-space ordering to perform faster searches. Additionally, we propose an efficient incremental instance selection method for massive data streams that continuously update and remove outdated examples from the case-base. This alleviates the high computational requirements of the original classifier, thus making it suitable for the considered problem. Experimental study conducted on a set of real-life massive data streams proves the usefulness of the proposed solution and shows that we are able to provide the first efficient nearest neighbor solution for high-speed big and streaming data. © 2017 IEEE.","Apache Spark; Big data; Data streams; Distributed computing; Instance reduction; Machine learning; Nearest neighbor","Artificial intelligence; Computer hardware description languages; Computer science; Data communication systems; Data mining; Distributed computer systems; Electric sparks; Learning algorithms; Learning systems; Metadata; Nearest neighbor search; Personnel training; Computational requirements; Data stream; Instance selection; Massive data streams; Memory management; Nearest neighbor algorithm; Nearest neighbor classification; Nearest neighbors; Big data",2-s2.0-85028953120
"Sobolewski P., Woźniak M.","SCR: simulated concept recurrence – a non-supervised tool for dealing with shifting concept",2017,"Expert Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887972184&doi=10.1111%2fexsy.12059&partnerID=40&md5=9635b9be0d602fcc3d43cae674395570","Most of the approaches coping with concept drift described in the machine learning literature are focused solely on detecting the concept changes or adapting the classification system, and hardly any works exist, which try to also describe the changes in concept. Nowadays, we desire methods, which are able to detect the concept drift in the absence of information about class labels. In this article, we present a semi-supervised method and analyze the possibilities of using the simulated concept recurrence against concept drift and also expanding the previously presented functionality of the algorithm from the sole concept characterization to both concept drift detection and concept characterization. The supervision is limited to the system setup phase, and during the evaluation of the algorithm, we assume no support from the experts. Further comparing this work to our previous publication, the scope of experiments has been extended from the single concept drift problems to the multi-concept scenarios, and also, the new method is evaluated on three different levels of the prior knowledge presented to the system beforehand. © 2013 Wiley Publishing Ltd","classification; concept drift; simulated concept recurrence; supervision","Classification (of information); Supervised learning; Class labels; Classification system; Concept drifts; Machine learning literature; Prior knowledge; Semi-supervised method; simulated concept recurrence; supervision; Learning systems",2-s2.0-84887972184
"Zhang Y., Song S., You K., Zhang X., Wu C.","Relevance vector machines using weighted expected squared distance for ore grade estimation with incomplete data",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028021312&doi=10.1007%2fs13042-016-0535-x&partnerID=40&md5=693383049ee952bd37ec867208a3c497","Accurate ore grade estimation is crucial to mineral resources evaluation and exploration. In this paper, we consider the borehole data collected from the Solwara 1 deposit, where the hydrothermal sulfide ore body is quite complicated with incomplete ore grade values. To solve this estimation problem, the relevance vector machine (RVM) and the expected squared distance (ESD) algorithm are incorporated into one regression model. Moreover, we improve the ESD algorithm by weighting the attributes of the data set and propose the weighted expected squared distance (WESD). In this paper, we uncover the symbiosis characteristics among different elements of the deposits by statistical analysis, which leads to estimating certain metal based on the data of other elements instead of on geographical position. The proposed WESD-RVM features high sparsity and accuracy, as well as the capability of handling incomplete data. Effectiveness of the proposed model is demonstrated by comparing with other estimating algorithms, such as inverse distance weighted method and Kriging algorithm which utilize only geographical spatial coordinates for inputs; extreme learning machine, which is unable to deal with incomplete data; and ordinary ESD based RVM regression model without entropy weighted distance. The experimental results show that the proposed WESD-RVM outperforms other methods with considerable predictive and generalizing ability. © 2016, Springer-Verlag Berlin Heidelberg.","Entropy weight; Grade estimation; Incomplete data; Relevance vector machine; Weighted expected squared distance","Data handling; Deposits; Electrostatic devices; Electrostatic discharge; Entropy; Inverse problems; Learning systems; Mineral resources; Ore deposits; Regression analysis; Entropy weights; Grade estimations; Incomplete data; Relevance Vector Machine; Squared distances; Ores",2-s2.0-85028021312
"Li L., Solana C., Canters F., Kervyn M.","Testing random forest classification for identifying lava flows and mapping age groups on a single Landsat 8 image",2017,"Journal of Volcanology and Geothermal Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028012193&doi=10.1016%2fj.jvolgeores.2017.07.014&partnerID=40&md5=6cccd1f84ce44558ede52c9c9b78016f","Mapping lava flows using satellite images is an important application of remote sensing in volcanology. Several volcanoes have been mapped through remote sensing using a wide range of data, from optical to thermal infrared and radar images, using techniques such as manual mapping, supervised/unsupervised classification, and elevation subtraction. So far, spectral-based mapping applications mainly focus on the use of traditional pixel-based classifiers, without much investigation into the added value of object-based approaches and into advantages of using machine learning algorithms. In this study, Nyamuragira, characterized by a series of > 20 overlapping lava flows erupted over the last century, was used as a case study. The random forest classifier was tested to map lava flows based on pixels and objects. Image classification was conducted for the 20 individual flows and for 8 groups of flows of similar age using a Landsat 8 image and a DEM of the volcano, both at 30-meter spatial resolution. Results show that object-based classification produces maps with continuous and homogeneous lava surfaces, in agreement with the physical characteristics of lava flows, while lava flows mapped through the pixel-based classification are heterogeneous and fragmented including much “salt and pepper noise”. In terms of accuracy, both pixel-based and object-based classification performs well but the former results in higher accuracies than the latter except for mapping lava flow age groups without using topographic features. It is concluded that despite spectral similarity, lava flows of contrasting age can be well discriminated and mapped by means of image classification. The classification approach demonstrated in this study only requires easily accessible image data and can be applied to other volcanoes as well if there is sufficient information to calibrate the mapping. © 2017 Elsevier B.V.","Lava flows; Nyamuragira; Object-based classification; Pixel-based classification; Random forest","Classification (of information); Decision trees; Image classification; Learning algorithms; Learning systems; Photomapping; Pixels; Remote sensing; Volcanoes; Lava flows; Nyamuragira; Object-based classifications; Pixel based classifications; Random forests; Mapping; forest cover; image classification; Landsat; lava flow; machine learning; pixel; satellite imagery; spatial resolution; testing method; topographic mapping; Democratic Republic Congo; Nord Kivu; Nyamuragira",2-s2.0-85028012193
"Lin W.-C., Tsai C.-F., Hu Y.-H., Jhang J.-S.","Clustering-based undersampling in class-imbalanced data",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019061365&doi=10.1016%2fj.ins.2017.05.008&partnerID=40&md5=74e5609d2c8e685200dbd3cecc433ed3","Class imbalance is often a problem in various real-world data sets, where one class (i.e. the minority class) contains a small number of data points and the other (i.e. the majority class) contains a large number of data points. It is notably difficult to develop an effective model using current data mining and machine learning algorithms without considering data preprocessing to balance the imbalanced data sets. Random undersampling and oversampling have been used in numerous studies to ensure that the different classes contain the same number of data points. A classifier ensemble (i.e. a structure containing several classifiers) can be trained on several different balanced data sets for later classification purposes. In this paper, we introduce two undersampling strategies in which a clustering technique is used during the data preprocessing step. Specifically, the number of clusters in the majority class is set to be equal to the number of data points in the minority class. The first strategy uses the cluster centers to represent the majority class, whereas the second strategy uses the nearest neighbors of the cluster centers. A further study was conducted to examine the effect on performance of the addition or deletion of 5 to 10 cluster centers in the majority class. The experimental results obtained using 44 small-scale and 2 large-scale data sets revealed that the clustering-based undersampling approach with the second strategy outperformed five state-of-the-art approaches. Specifically, this approach combined with a single multilayer perceptron classifier and C4.5 decision tree classifier ensembles delivered optimal performance over both small- and large-scale data sets. © 2017 Elsevier Inc.","Class imbalance; Classifier ensembles; Clustering; Imbalanced data; Machine learning","Artificial intelligence; Data mining; Decision trees; Learning algorithms; Learning systems; Trees (mathematics); Virtual reality; C4.5 Decision tree classifier; Class imbalance; Classifier ensembles; Clustering; Imbalanced data; Large scale data sets; Multi-layer perceptron classifiers; Random under samplings; Classification (of information)",2-s2.0-85019061365
"Kovalerchuk B.","Visual cognitive algorithms for high-dimensional data and super-intelligence challenges",2017,"Cognitive Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020879793&doi=10.1016%2fj.cogsys.2017.05.007&partnerID=40&md5=a7bf27429c5959f16700751faa760cf5","In the long run the cognitive algorithms intend to make super-intelligent machines and super-intelligent humans. This paper presents a technical process to reach specific aspects of super-intelligence that are out of the current human cognitive abilities. These aspects are inabilities to discover patterns in large numeric multidimensional data with a naked eye. This is a long-standing problem in Data Science and Modeling in general. The major obstacle is in human inability to see n-D data by a naked eye and our needs in visualization means to represent n-D data in 2-D losslessly. While these means exist their number and abilities are limited. This paper expands the class of such lossless visual methods, by further developing a new concept of Generalized Shifted Paired Coordinates. It shows the advantages of proposed reversible lossless technique by representing real data and by proving mathematical properties. © 2017 Elsevier B.V.","Cognitive algorithms; High-dimensional data; Machine learning, generalized coordinates, super-intelligence; Visualization","Clustering algorithms; Flow visualization; Learning systems; Visualization; Generalized coordinates; High dimensional data; Human cognitive abilities; Intelligent machine; Mathematical properties; Multidimensional data; Standing problems; Technical process; Data visualization; accuracy; algorithm; Article; artificial intelligence; classification algorithm; computer prediction; data analysis; executive function; mathematical analysis; mathematical model; priority journal; process development; visual cognitive algorithm; visual information",2-s2.0-85020879793
"Teixeira M., Cook D.A., Heale B.S.E., Del Fiol G.","Optimization of infobutton design and Implementation: A systematic review",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028710485&doi=10.1016%2fj.jbi.2017.08.010&partnerID=40&md5=f1b8fe2cc49d7e26e5009fa8e754928a","Objective Infobuttons are clinical decision tools embedded in the electronic health record that attempt to link clinical data with context sensitive knowledge resources. We systematically reviewed technical approaches that contribute to improved infobutton design, implementation and functionality. Methods We searched databases including MEDLINE, EMBASE, and the Cochrane Library database from inception to March 1, 2016 for studies describing the use of infobuttons. We selected full review comparative studies, usability studies, and qualitative studies examining infobutton design and implementation. We abstracted usability measures such as user satisfaction, impact, and efficiency, as well as prediction accuracy of infobutton content retrieval algorithms and infobutton adoption/interoperability. Results We found 82 original research studies on infobuttons. Twelve studies met criteria for detailed abstraction. These studies investigated infobutton interoperability (1 study); tools to help tailor infobutton functionality (1 study); interventions to improve user experience (7 studies); and interventions to improve content retrieval by improving prediction of relevant knowledge resources and information needs (3 studies). In-depth interviews with implementers showed the Health Level Seven (HL7) Infobutton standard to be simple and easy to implement. A usability study demonstrated the feasibility of a tool to help medical librarians tailor infobutton functionality. User experience studies showed that access to resources with which users are familiar increased user satisfaction ratings; and that links to specific subsections of drug monographs increased information seeking efficiency. However, none of the user experience improvements led to increased usage uptake. Recommender systems based on machine learning algorithms outperformed hand-crafted rules in the prediction of relevant resources and clinicians’ information needs in a laboratory setting, but no studies were found using these techniques in clinical settings. Improved content indexing in one study led to improved content retrieval across three health care organizations. Conclusion Best practice technical approaches to ensure optimal infobutton functionality, design and implementation remain understudied. The HL7 Infobutton standard has supported wide adoption of infobutton functionality among clinical information systems and knowledge resources. Limited evidence supports infobutton enhancements such as links to specific subtopics, configuration of optimal resources for specific tasks and users, and improved indexing and content coverage. Further research is needed to investigate user experience improvements to increase infobutton use and effectiveness. © 2017 Elsevier Inc.","Decision support systems; Health information technology; Infobutton; Information needs; Machine learning; Medical informatics applications","Abstracting; Artificial intelligence; Decision support systems; Efficiency; Forecasting; Human computer interaction; Indexing (of information); Information retrieval; Learning algorithms; Learning systems; Medical computing; Optimization; Clinical information system; Design and implementations; Electronic health record; Health information technologies; Healthcare organizations; Infobutton; Information needs; Medical informatics applications; Medical information systems; access to information; clinical decision support system; computer interface; electronic health record; health care organization; human; infobutton; information retrieval; machine learning; medical informatics; medical information; priority journal; Review; systematic review",2-s2.0-85028710485
"Lang H., Wu S.","Ship Classification in Moderate-Resolution SAR Image by Naive Geometric Features-Combined Multiple Kernel Learning",2017,"IEEE Geoscience and Remote Sensing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029171790&doi=10.1109%2fLGRS.2017.2734889&partnerID=40&md5=fd6de01cd1610695aec1e408d27ae446","Compared with the high-resolution synthetic aperture radar (SAR) image, a moderate-resolution SAR image can offer wider swath, which is more suitable for maritime ship surveillance. Taking into account the amount of information in a moderate-resolution SAR image and the stability of feature extraction, we propose naive geometric features (NGFs) for ship classification. In contrast to the strictly defined geometric features (SGFs), the extraction of NGFs is very simpler and efficient. And more importantly, the NGFs are enough to reveal the essential difference between different types of ships for classification. To fuse various NGFs with different physical properties and discriminability, the multiple kernel learning (MKL) is utilized to learn the combination weights, rather than assigning the same weight to all features as usually applied by the traditional support vector machines (SVMs). The comprehensive experiments validate that: 1) the performance of the proposed NGF-combined MKL outperforms that of NGF-combined SVM by 3.4% and is very close to that obtained by SGF-combined MKL and 2) in terms of classifying ships in a moderate-resolution SAR image, NGFs are more feasible than scattering features. © 2004-2012 IEEE.","Multiple kernel learning (MKL); naive geometric features (NGFs); remote sensing; ship classification; synthetic aperture radar (SAR)","Classification (of information); Extraction; Feature extraction; Geometry; Image classification; Image processing; Image resolution; Marine radar; Radar; Remote sensing; Scattering; Ships; Support vector machines; Synthetic aperture radar; Geometric feature; Kernel; Marine vehicles; Multiple Kernel Learning; Ship classification; Radar imaging",2-s2.0-85029171790
"Raghavendra U., Acharya U.R., Gudigar A., Shetty R., Krishnananda N., Pai U., Samanth J., Nayak C.","Automated screening of congestive heart failure using variational mode decomposition and texture features extracted from ultrasound images",2017,"Neural Computing and Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009232027&doi=10.1007%2fs00521-017-2839-5&partnerID=40&md5=79521984a5da0be780bcb440a1f6a4ad","Heart is an important and hardest working muscular organ of the human body. Inability of the heart to restore normal perfusion to the entire body refers to cardiac failure, which then with symptoms results in manifestation of congestive heart failure (CHF). Impairment in systolic function associated with chronic dilation of left ventricle is referred as dilated cardiomyopathy (DCM). The clinical examination, surface electrocardiogram (ECG), chest X-ray, blood markers and echocardiography play major role in the diagnosis of CHF. Though the ECG manifests chamber enlargement changes, it does not possess sensitive marker for the diagnosis of DCM, whereas echocardiographic assessment can effectively reveal the presence of asymptomatic DCM. This work proposes an automated screening method for classifying normal and CHF echocardiographic images affected due to DCM using variational mode decomposition technique. The texture features are extracted from variational mode decomposed image. These features are selected using particle swarm optimization and classified using support vector machine classifier with different kernel functions. We have validated our experiment using 300 four-chamber echocardiography images (150: normal, 150: CHF) obtained from 50 normal and 50 CHF patients. Our proposed approach yielded maximum average accuracy, sensitivity and specificity of 99.33%, 98.66% and 100%, respectively, using ten features. Thus, the developed diagnosis system can effectively detect CHF in its early stage using ultrasound images and aid the clinicians in their diagnosis. © 2017, The Natural Computing Applications Forum.","Congestive heart failure; Dilated cardiomyopathy; Machine learning; Texture features; VMD","Automation; Cardiology; Diagnosis; Echocardiography; Electrocardiography; Heart; Image processing; Learning systems; Particle swarm optimization (PSO); Ultrasonic applications; Congestive heart failures; Dilated cardiomyopathy; Echocardiographic images; Echocardiography Images; Mode decomposition techniques; Sensitivity and specificity; Support vector machine classifiers; Texture features; Image texture",2-s2.0-85009232027
"Yu H.S., Scalera J., Khalid M., Touret A.-S., Bloch N., Li B., Qureshi M.M., Soto J.A., Anderson S.W.","Texture analysis as a radiomic marker for differentiating renal tumors",2017,"Abdominal Radiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017583655&doi=10.1007%2fs00261-017-1144-1&partnerID=40&md5=e3ae4e1aeae4017edce658508fd0ed70","Purpose: To evaluate the utility of texture analysis for the differentiation of renal tumors, including the various renal cell carcinoma subtypes and oncocytoma. Materials and methods: Following IRB approval, a retrospective analysis was performed, including all patients with pathology-proven renal tumors and an abdominal computed tomography (CT) examination. CT images of the tumors were manually segmented, and texture analysis of the segmented tumors was performed. A support vector machine (SVM) method was also applied to classify tumor types. Texture analysis results were compared to the various tumors and areas under the curve (AUC) were calculated. Similar calculations were performed with the SVM data. Results: One hundred nineteen patients were included. Excellent discriminators of tumors were identified among the histogram-based features noting features skewness and kurtosis, which demonstrated AUCs of 0.91 and 0.93 (p < 0.0001), respectively, for differentiating clear cell subtype from oncocytoma. Histogram feature median demonstrated an AUC of 0.99 (p < 0.0001) for differentiating papillary subtype from oncocytoma and an AUC of 0.92 for differentiating oncocytoma from other tumors. Machine learning further improved the results achieving very good to excellent discrimination of tumor subtypes. The ability of machine learning to distinguish clear cell subtype from other tumors and papillary subtype from other tumors was excellent with AUCs of 0.91 and 0.92, respectively. Conclusion: Texture analysis is a promising non-invasive tool for distinguishing renal tumors on CT images. These results were further improved upon application of machine learning, and support the further development of texture analysis as a quantitative biomarker for distinguishing various renal tumors. © 2017, Springer Science+Business Media New York.","Machine learning; Oncocytoma; Radiomic marker; Renal cell carcinoma; Texture analysis","tumor marker; analytical parameters; Article; computed tomography scanner; computer assisted tomography; contrast enhancement; histogram; human; image analysis; image segmentation; kidney tumor; major clinical study; medical record; oncocytoma; priority journal; renal cell carcinoma; retrospective study; support vector machine; texture analysis; tumor differentiation",2-s2.0-85017583655
"Poggio T., Mhaskar H., Rosasco L., Miranda B., Liao Q.","Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review",2017,"International Journal of Automation and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015184566&doi=10.1007%2fs11633-017-1054-2&partnerID=40&md5=6676c49f883587a945a9db14f44c3f0f","The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures. © 2017, The Author(s).","convolutional neural networks; deep and shallow networks; deep learning; function approximation; Machine learning; neural networks","Convolution; Deep learning; Learning systems; Neural networks; Convolutional networks; Convolutional neural network; Curse of dimensionality; Function approximation; New results; Deep neural networks",2-s2.0-85015184566
"Zhou T., Chung F.-L., Wang S.","Deep TSK Fuzzy Classifier With Stacked Generalization and Triplely Concise Interpretability Guarantee for Large Data",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032294633&doi=10.1109%2fTFUZZ.2016.2604003&partnerID=40&md5=44410bed4461f23e5642cac15e57a734","Although Takagi-Sugeno-Kang (TSK) fuzzy classifier has been applied to a wide range of practical scenarios, how to enhance its classification accuracy and interpretability simultaneously is still a challenging task. In this paper, based on the powerful stacked generalization principle, a deep TSK fuzzy classifier (D-TSK-FC) is proposed to achieve the enhanced classification accuracy and triplely concise interpretability for fuzzy rules. D-TSK-FC consists of base-building units. Just like the existing popular deep learning, D-TSK-FC can be built in a layer-by-layer way. In terms of the stacked generalization principle, the training set plus random shifts obtained from random projections of prediction results of current base-building unit are presented as the input of the next base-building unit. The hidden layer in each base-building unit of D-TSK-FC is represented by triplely concise interpretable fuzzy rules in the sense of randomly selected features with the fixed five fuzzy partitions, random rule combinations, and the same input space kept in every base-building unit of D-TSK-FC. The output layer of each base-building unit can be learnt quickly by least learning machine (LLM). Besides, benefiting from LLM, D-TSK-FC's deep learning can be well scaled up for large datasets. Our extensive experimental results witness the power of the proposed deep TSK fuzzy classifier. © 2016 IEEE.","Deep learning Takagi-Sugeno-Kang (TSK); fuzzy classifier; interpretability; large datastacked generalization; least learning machine (LLM)","Buildings; Deep learning; Fuzzy inference; Fuzzy rules; Fuzzy systems; Learning systems; Fuzzy classifiers; Interpretability; large datastacked generalization; Learning machines; Takagi-sugeno; Fuzzy sets",2-s2.0-85032294633
"Ma W.-J., Gupta V., Topcu U.","Distributed Charging Control of Electric Vehicles Using Online Learning",2017,"IEEE Transactions on Automatic Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031033108&doi=10.1109%2fTAC.2016.2636740&partnerID=40&md5=a62aeeb5c25f0e9abf447e8b2787a2c3","We propose an algorithm for distributed charging control of electric vehicles (EVs) using online learning and online convex optimization. Many distributed charging control algorithms in the literature implicitly assume fast two-way communication between the distribution company and EV customers. This assumption is impractical at present and also raises security and privacy concerns. Our algorithm does not use this assumption; however, at the expense of slower convergence to the optimal solution and by relaxing the sense of optimality. The proposed algorithm requires one-way communication, which is implemented through the distribution company publishing the pricing profiles for the previous days. We provide convergence results for the algorithm and illustrate the results through numerical examples. © 1963-2012 IEEE.","Charging control; demand response; online learning; regret minimization","Charging (batteries); Convex optimization; Electric machine control; Electric vehicles; Optimization; Charging control; Demand response; Distribution companies; Electric Vehicles (EVs); Online convex optimizations; Online learning; Regret minimization; Two way communications; E-learning",2-s2.0-85031033108
"Khalighi S., Ribeiro B., Nunes U.J.","Importance Weighted Import Vector Machine for Unsupervised Domain Adaptation",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994248966&doi=10.1109%2fTCYB.2016.2616119&partnerID=40&md5=1e76b7ed2e859e4743ff73053251806e","In real-world applications, the assumption of independent and identical distribution is no longer consistent. To alleviate the significant mismatch between source and target domains, importance weighting import vector machine, which is an adaptive classifier, is proposed. This adaptive probabilistic classification method, which is sparse and computationally efficient, can be used for unsupervised domain adaptation (DA). The effectiveness of the proposed approach is demonstrated via a toy problem, and a real-world cross-domain object recognition task. Even though the sparseness, the proposed method outperforms the state-of-the-art in both unsupervised and semisupervised DA scenarios. We also introduce a reliable importance weighted cross validation (RIWCV), which is an improvement of importance weighted cross validation, for parameter and model selection. The RIWCV avoid falling down in local minimum, by selecting a more reliable combination of the parameters instead of the best parameters. © 2013 IEEE.","Domain adaptation (DA); instance weighting; transfer learning","Adaptive classifiers; Adaptive classifiers; Computationally efficient; Computationally efficient; Cross validation; Cross validation; Domain adaptation; Domain adaptation; Import vector machines; Import vector machines; Importance weighting; Importance weighting; nocv2; nocv2; Probabilistic classification method; Probabilistic classification method; State of the art; State of the art; Object recognition; Object recognition",2-s2.0-84994248966
"Costa J., Silva C., Antunes M., Ribeiro B.","Adaptive learning for dynamic environments: A comparative approach",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029575032&doi=10.1016%2fj.engappai.2017.08.004&partnerID=40&md5=0bf7e19f3393194cecbe76fe849a674d","Nowadays most learning problems demand adaptive solutions. Current challenges include temporal data streams, drift and non-stationary scenarios, often with text data, whether in social networks or in business systems. Various efforts have been pursued in machine learning settings to learn in such environments, specially because of their non-trivial nature, since changes occur between the distribution data used to define the model and the current environment. In this work we present the Drift Adaptive Retain Knowledge (DARK) framework to tackle adaptive learning in dynamic environments based on recent and retained knowledge. DARK handles an ensemble of multiple Support Vector Machine (SVM) models that are dynamically weighted and have distinct training window sizes. A comparative study with benchmark solutions in the field, namely the Learn++.NSE algorithm, is also presented. Experimental results revealed that DARK outperforms Learn++.NSE with two different base classifiers, an SVM and a Classification and Regression Tree (CART). © 2017","Dynamic environments; Ensembles; Learn++.NSE; Twitter","Learning systems; Benchmark solutions; Classification and regression tree; Comparative approach; Comparative studies; Dynamic environments; Ensembles; Learn+; Twitter; Support vector machines",2-s2.0-85029575032
"Danglade F., Pernot J.-P., Véron P., Fine L.","A priori evaluation of simulation models preparation processes using artificial intelligence techniques",2017,"Computers in Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020798119&doi=10.1016%2fj.compind.2017.06.001&partnerID=40&md5=923cc61c46df7cc97195658a3d7eb331","Controlling the well-known triptych costs, quality and time during the different phases of the Product Development Process (PDP) is an everlasting challenge for the industry. Among the numerous issues that are to be addressed, the development of new methods and tools to adapt to the various needs the models used all along the PDP is certainly one of the most challenging and promising improvement area. This is particularly true for the adaptation of Computer-Aided Design (CAD) models to Computer-Aided Engineering (CAE) applications, and notably during the CAD models simplification steps. Today, even if methods and tools exist, such a preparation phase still requires a deep knowledge and a huge amount of time when considering Digital Mock-Up (DMU) composed of several hundreds of thousands of parts. Thus, being able to estimate a priori the impact of DMU adaptation scenarios on the simulation results would help identifying the best scenario right from the beginning. This paper addresses such a difficult problem and uses artificial intelligence (AI) techniques to learn and accurately predict behaviours from carefully selected examples. The main idea is to identify rules from these examples used as inputs of learning algorithms. Once those rules obtained, they can be used on a new case to a priori estimate the impact of a preparation process without having to perform it. To reach this objective, a method to build a representative database of examples has been developed, the right input (explanatory) and output (preparation process quality criteria) variables have been identified, then the learning model and its associated control parameters have been tuned. One challenge was to identify explanatory variables from geometrical key characteristics and data characterizing the preparation processes. A second challenge was to build a effective learning model despite a limited number of examples. The rules linking the output variables to the input ones are obtained using AI techniques such as well-known neural networks and decision trees. The proposed approach is illustrated and validated on industrial examples in the context of computational fluid dynamics simulations. © 2017 Elsevier B.V.","Artificial intelligence; Digital Mock-Up preparation; Knowledge formalization; Machine learning; Process evaluation","Artificial intelligence; Computational fluid dynamics; Computer aided engineering; Decision trees; Learning algorithms; Learning systems; Mockups; Quality control; Artificial intelligence techniques; Computational fluid dynamics simulations; Computer aided design models; Digital Mock-up; Explanatory variables; Knowledge formalizations; Process Evaluation; Product development process; Computer aided design",2-s2.0-85020798119
"Nikolić D.","Why deep neural nets cannot ever match biological intelligence and what to do about it?",2017,"International Journal of Automation and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021828132&doi=10.1007%2fs11633-017-1093-8&partnerID=40&md5=6c4a519125a04877d687bdf98cfe97a7","The recently introduced theory of practopoiesis offers an account on how adaptive intelligent systems are organized. According to that theory, biological agents adapt at three levels of organization and this structure applies also to our brains. This is referred to as tri-traversal theory of the organization of mind or for short, a T3-structure. To implement a similar T3-organization in an artificially intelligent agent, it is necessary to have multiple policies, as usually used as a concept in the theory of reinforcement learning. These policies have to form a hierarchy. We define adaptive practopoietic systems in terms of hierarchy of policies and calculate whether the total variety of behavior required by real-life conditions of an adult human can be satisfactorily accounted for by a traditional approach to artificial intelligence based on T2-agents, or whether a T3-agent is needed instead. We conclude that the complexity of real life can be dealt with appropriately only by a T3-agent. This means that the current approaches to artificial intelligence, such as deep architectures of neural networks, will not suffice with fixed network architectures. Rather, they will need to be equipped with intelligent mechanisms that rapidly alter the architectures of those networks. © 2017, The Author(s).","Artificial intelligence; machine learning; neural networks; practopoiesis; strong artificial intelligence","Artificial intelligence; Complex networks; Deep neural networks; Education; Intelligent systems; Learning systems; Neural networks; Reinforcement learning; Biological agents; Deep architectures; Deep neural nets; Intelligent mechanisms; practopoiesis; Traditional approaches; Network architecture",2-s2.0-85021828132
"Dou Q., Yu L., Chen H., Jin Y., Yang X., Qin J., Heng P.-A.","3D deeply supervised network for automated segmentation of volumetric medical images",2017,"Medical Image Analysis",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019483926&doi=10.1016%2fj.media.2017.05.001&partnerID=40&md5=bf5f0cba83debfa77d77cc780e7d674c","While deep convolutional neural networks (CNNs) have achieved remarkable success in 2D medical image segmentation, it is still a difficult task for CNNs to segment important organs or structures from 3D medical images owing to several mutually affected challenges, including the complicated anatomical environments in volumetric images, optimization difficulties of 3D networks and inadequacy of training samples. In this paper, we present a novel and efficient 3D fully convolutional network equipped with a 3D deep supervision mechanism to comprehensively address these challenges; we call it 3D DSN. Our proposed 3D DSN is capable of conducting volume-to-volume learning and inference, which can eliminate redundant computations and alleviate the risk of over-fitting on limited training data. More importantly, the 3D deep supervision mechanism can effectively cope with the optimization problem of gradients vanishing or exploding when training a 3D deep model, accelerating the convergence speed and simultaneously improving the discrimination capability. Such a mechanism is developed by deriving an objective function that directly guides the training of both lower and upper layers in the network, so that the adverse effects of unstable gradient changes can be counteracted during the training procedure. We also employ a fully connected conditional random field model as a post-processing step to refine the segmentation results. We have extensively validated the proposed 3D DSN on two typical yet challenging volumetric medical image segmentation tasks: (i) liver segmentation from 3D CT scans and (ii) whole heart and great vessels segmentation from 3D MR images, by participating two grand challenges held in conjunction with MICCAI. We have achieved competitive segmentation results to state-of-the-art approaches in both challenges with a much faster speed, corroborating the effectiveness of our proposed 3D DSN. © 2017 Elsevier B.V.","3D deeply supervised networks; 3D fully convolutional networks; Deep learning; Volumetric medical image segmentation","Computerized tomography; Convolution; Deep learning; Deep neural networks; Image segmentation; Magnetic resonance imaging; Medical imaging; Network layers; Neural networks; Optimization; Automated segmentation; Conditional random field; Convolutional networks; Convolutional neural network; Optimization problems; State-of-the-art approach; Supervised network; Supervision mechanisms; Medical image processing; Article; automation; cone beam computed tomography; controlled study; density gradient; discrimination learning; great blood vessel; heart; human; image segmentation; liver; priority journal; process optimization; supervised machine learning; three dimensional imaging; velocity; volume; x-ray computed tomography",2-s2.0-85019483926
"Diao Y., Shwartz L.","Building Automated Data Driven Systems for IT Service Management",2017,"Journal of Network and Systems Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328032&doi=10.1007%2fs10922-017-9430-3&partnerID=40&md5=fd73691482392a5ca196ed216d107bbc","Enterprises and service providers are increasingly challenged with improving the quality of service delivery while containing the cost. However, it is often difficult to effectively manage the complex relationships among dynamic customer workloads, strict service level requirements, and efficient service management processes. In this paper, we present our progress on building autonomic systems for IT service management through a collection of automated data driven methodologies. This includes the design of feedback controllers for workload management, the use of simulation-optimization methodology for workforce management, and the development of machine learning models for event management. We demonstrate the applicability of the presented approaches using examples and data from a large IT service delivery environment. © 2017, Springer Science+Business Media, LLC.","Autonomic computing; Feedback control; IT service management; Recommender system; Simulation-based optimization","Feedback; Feedback control; Learning systems; Optimization; Quality of service; Recommender systems; Autonomic Computing; Complex relationships; Feedback controller; IT service management; Machine learning models; Simulation optimization methodology; Simulation-based optimizations; Workforce management; Information management",2-s2.0-85030328032
"Baka N., Leenstra S., Van Walsum T.","Ultrasound Aided Vertebral Level Localization for Lumbar Surgery",2017,"IEEE Transactions on Medical Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028955915&doi=10.1109%2fTMI.2017.2738612&partnerID=40&md5=e1a0e88df39781b9dfa1f6b8f5fbb8af","Localization of the correct vertebral level for surgical entry during lumbar hernia surgery is not straightforward. In this paper, we develop and evaluate a solution using free-hand 2-D ultrasound (US) imaging in the operation room (OR). Our system exploits the difference in spinous process shapes of the vertebrae. The spinous processes are pre-operatively outlined and labeled in a lateral lumbar X-ray of the patient. Then, in the OR the spinous processes are imaged with 2-D sagittal US, and are automatically segmented and registered with the X-ray shapes. After a small number of scanned vertebrae, the system robustly matches the shapes, and propagates the X-ray label to the US images. The main contributions of our work are: we propose a deep convolutional neural network-based bone segmentation algorithm from US imaging that outperforms state of the art methods in both performance and speed. We present a matching strategy that determines the levels of the spinal processes being imaged. And lastly, we evaluate the complete procedure on 19 clinical data sets from two hospitals, and two observers. The final labeling was correct in 92% of the cases, demonstrating the feasibility of US-based surgical entry point detection for spinal surgeries. © 1982-2012 IEEE.","Bone segmentation; computer aided surgery; deep learning; lumbar X-ray; machine learning; spine; surgical guidance","Bone; Computer aided instruction; Deep learning; Deep neural networks; Learning systems; Musculoskeletal system; Neural networks; Radiology; Surgery; Transplantation (surgical); Ultrasonic applications; Ultrasonic imaging; Bone segmentation; Computer aided surgery; Shape; spine; Surgical guidance; Two-dimensional displays; Xray imaging; Image segmentation",2-s2.0-85028955915
"Inkpen D., Liu J., Farzindar A., Kazemi F., Ghazi D.","Location detection and disambiguation from twitter messages",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016613252&doi=10.1007%2fs10844-017-0458-3&partnerID=40&md5=7f4a3eeab7902863439485904cf79df1","A remarkable amount of Twitter messages are generated every second. Detecting the location entities mentioned in these messages is useful in text mining applications. Therefore, techniques for extracting the location entities from the Twitter textual content are needed. In this work, we approach this task in a similar manner to the Named Entity Recognition (NER) task, but we focus only on locations, while NER systems detect names of persons, organizations, locations, and sometimes more (e.g., dates, times). But, unlike NER systems, we address a deeper task: classifying the detected locations into names of cities, provinces/states, and countries in order to map them into physical locations. We approach the task in a novel way, consisting in two stages. In the first stage, we train Conditional Random Fields (CRF) models that are able to detect the locations mentioned in the messages. We train three classifiers: one for cities, one for provinces/states, and one for countries, with various sets of features. Since a dataset annotated with this kind of information was not available, we collected and annotated our own dataset to use for training and testing. In the second stage, we resolve the remaining ambiguities, namely, cases when there exists more than one place with the same name. We proposed a set of heuristics able to choose the correct physical location in these cases. Our two-stage model will allow a social media monitoring system to visualize the places mentioned in Twitter messages on a map of the world or to compute statistics about locations. This kind of information can be of interest to business or marketing applications. © 2017, Springer Science+Business Media New York.","Artificial intelligence; Information extraction; Machine learning; Natural language processing; Social media","Artificial intelligence; Data mining; Information retrieval; Learning algorithms; Learning systems; Natural language processing systems; Random processes; Social networking (online); Statistical tests; Text processing; Conditional random field; Location detection; Marketing application; Named entity recognition; NAtural language processing; Social media; Social media monitoring; Training and testing; Location",2-s2.0-85016613252
"Zhu S., Zhang C.","A fast algorithm of intra prediction modes pruning for HEVC based on decision trees and a new three-step search",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994411648&doi=10.1007%2fs11042-016-4056-0&partnerID=40&md5=fc134e7b14c4aa3e3bf198f5e6aea307","The High Efficiency Video Coding (HEVC) standard is a new generation video coding scheme, succeeding to H.264/AVC. HEVC requires only 50 % bitrate of H.264/AVC at the same perceptual quality by adopting new coding tools and more flexible block structures. HEVC specifies 35 different intra prediction directions that can be associated to different block sizes. Each possible combination needs to be tested within the Rate Distortion (RD) process to enable selecting the optimal intra mode and block splitting depth. This leads to a significant processing weight and therefore any improvement that might be achieved will bring significative increase in the computational efficiency of the algorithm. This paper proposes a novel intra prediction modes pruning method based on decision trees and a new three-step search algorithm, aiming at achieving higher encoding efficiency compared to the standard—HEVC. This fast algorithm is composed of two algorithms. The first algorithm is a modes pruning algorithm depending on decision trees. We first calculate variances of the above side, the left side and all the reference samples of all the PUs (Prediction Units), which are used to divide the PUs into three groups of different candidate intra prediction modes. The first group only includes Planar mode and DC mode, the optimal mode will be selected from the two modes. The second and third groups include 19 and 35 intra modes, respectively. Then the decision trees are trained using the information obtained previously by the software WEKA. The classification process has an accuracy of 85.29 %. The second algorithm is a three-step search algorithm which is defined to be suitable for prediction units classified into class two and class three after the execution of decision trees. The detailed implementations of three-step search algorithms for prediction units belong to those two classes are subtly different. Experimental results verify that, compared with the reference software HM15.0, on average, the proposed algorithm reduces the encoding time by 37.87 % with a slightly decreasing of BD-PSNR (0.058 dB) and increasing of BD-Rate (1.19 %). © 2016, Springer Science+Business Media New York.","Decision trees; High efficiency video coding (HEVC); Intra mode decision; Machine learning","Artificial intelligence; Codes (symbols); Decision trees; Efficiency; Electric distortion; Encoding (symbols); Forecasting; Forestry; Image coding; Learning algorithms; Learning systems; Motion Picture Experts Group standards; Signal distortion; Trees (mathematics); Video signal processing; Classification process; Encoding efficiency; High-efficiency video coding; Intra mode decision; Intra prediction modes; New three-step searches; Pruning algorithms; Video coding schemes; Computational efficiency",2-s2.0-84994411648
"Do B.H., Langlotz C., Beaulieu C.F.","Bone Tumor Diagnosis Using a Naïve Bayesian Model of Demographic and Radiographic Features",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026921111&doi=10.1007%2fs10278-017-0001-7&partnerID=40&md5=c5973153919de1753b46d5adfddf62cf","Because many bone tumors have a variety of appearances and are uncommon, few radiologists develop sufficient expertise to guide optimal management. Bayesian inference can guide decision-making by computing probabilities of multiple diagnoses to generate a differential. We built and validated a naïve Bayes machine (NBM) that processes 18 demographic and radiographic features. We reviewed over 1664 analog radiographic cases of bone tumors and selected 811 cases (66 diagnoses) for annotation using a quantitative imaging platform. Leave-one-out cross validation was performed. Primary accuracy was defined as the correct pathological diagnosis as the top machine prediction. Differential accuracy was defined as whether the correct pathological diagnosis was within the top three predictions. For the 29 most common diagnoses (710 cases), primary accuracy was 44%, and differential accuracy was 60%. For the top 10 most common diagnoses (478 cases), primary accuracy was 62%, and differential accuracy was 80%. The machine returned relevant diagnoses for the majority of unknown test cases and may be a feasible alternative to machine learning approaches such as deep neural networks or support vector machines that typically require larger training data (our model required a minimum of five samples per diagnosis) and are “black boxes” (our model can provide details of probability calculations to identify features that most significantly contribute to truth diagnoses). Finally, our Bayes model was designed to scale and “learn” from external data, enabling incorporation of outside knowledge such as Dahlin’s Bone Tumors, a reference of anatomic and demographic statistics of more than 10,000 tumors. © 2017, Society for Imaging Informatics in Medicine.","Bone tumor diagnosis; Naïve Bayes model","Bayesian networks; Bone; Decision making; Deep neural networks; Inference engines; Learning systems; Population statistics; Probability; Sodium; Statistical methods; Tumors; Bayes models; Bayesian inference; Bone tumor; Feasible alternatives; Leave-one-out cross validations; Machine learning approaches; Probability calculations; Quantitative imaging; Diagnosis",2-s2.0-85026921111
"Sun B., Zhang Z., Liu X., Hu B., Zhu T.","Self-esteem recognition based on gait pattern using Kinect",2017,"Gait and Posture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029091705&doi=10.1016%2fj.gaitpost.2017.09.001&partnerID=40&md5=2d1c9385ec80d174c1de16af2eecb1ff","Background Self-esteem is an important aspect of individual's mental health. When subjects are not able to complete self-report questionnaire, behavioral assessment will be a good supplement. In this paper, we propose to use gait data collected by Kinect as an indicator to recognize self-esteem. Methods 178 graduate students without disabilities participate in our study. Firstly, all participants complete the 10-item Rosenberg Self-Esteem Scale (RSS) to acquire self-esteem score. After completing the RRS, each participant walks for two minutes naturally on a rectangular red carpet, and the gait data are recorded using Kinect sensor. After data preprocessing, we extract a few behavioral features to train predicting model by machine learning. Based on these features, we build predicting models to recognize self-esteem. Results For self-esteem prediction, the best correlation coefficient between predicted score and self-report score is 0.45 (p < 0.001). We divide the participants according to gender, and for males, the correlation coefficient is 0.43 (p < 0.001), for females, it is 0.59 (p < 0.001). Conclusion Using gait data captured by Kinect sensor, we find that the gait pattern could be used to recognize self-esteem with a fairly good criterion validity. The gait predicting model can be taken as a good supplementary method to measure self-esteem. © 2017 Elsevier B.V.","Behavioral assessment; Gait pattern; Kinect; Machine learning; Self-esteem","adult; ankle; Article; behavior; correlation coefficient; criterion related validity; elbow; epsilon support vector regression; female; foot; gait; graduate student; hand; head; human; human experiment; intermethod comparison; kernel method; knee; linear regression analysis; machine learning; male; motion analysis system; neck; normal human; nu support vector regression; observational study; pattern recognition; prediction; priority journal; Rosenberg Self-Esteem Scale; self esteem; self report; shoulder; simple linear regression; spine; support vector machine; thumb; walking; wrist; young adult",2-s2.0-85029091705
"Zhang X., Zhang X.","Adaptive multiclass support vector machine for multimodal data analysis",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019265197&doi=10.1016%2fj.patcog.2017.05.006&partnerID=40&md5=fd4929485c69b6e858f19f820bb87e3a","Multimodal data commonly exists in human lives. Early analysis usually concentrates on mining information based on single modality. Recent studies show that learning tasks could be greatly enhanced by analyzing data from the aspect of multimodality. This paper deals with classifying multimodal data comprised of visual and acoustic contents. Different data features are fused under a hierarchical structure to achieve a good semantic understanding. Then, to accomplish accurate classification, an adaptive support vector machine method (ASVM) is proposed. The method is support vector machine with hyperparameters controlled by a novel and efficient artificial bee colony algorithm. First, a micro colony is set as the number of hyperparameters is usually less than 5. Second, one position inheritance based on roulette wheel selection is used. Third, discarded solutions are mutated by position shift operation instead of random reinitialization. The ASVM method is first verified on classical data sets demonstrating the goodness of the proposed method. Then the proposed method is applied on a multimodal data set. Each sample includes both image and audio data features. Experimental results show that the ASVM method is more effective and robust than the compared methods. © 2017 Elsevier Ltd","Artificial bee colony; Feature selection; Hyperparameter optimization; Multiclass classification; Support vector machine","Evolutionary algorithms; Feature extraction; Modal analysis; Optimization; Semantics; Support vector machines; Vectors; Artificial bee colonies; Artificial bee colony algorithms; Hierarchical structures; Hyper-parameter optimizations; Multi-class classification; Multi-class support vector machines; Multimodal data analysis; Roulette wheel selection; Classification (of information)",2-s2.0-85019265197
"Chen C., Li K., Ouyang A., Li K.","A parallel approximate SS-ELM algorithm based on MapReduce for large-scale datasets",2017,"Journal of Parallel and Distributed Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011277363&doi=10.1016%2fj.jpdc.2017.01.007&partnerID=40&md5=e6233f91a2c56966f17117543e53892d","Extreme Learning Machine (ELM) algorithm not only has gained much attention of many scholars and researchers, but also has been widely applied in recent years especially when dealing with big data because of its better generalization performance and learning speed. The proposal of SS-ELM (semi-supervised Extreme Learning Machine) extends ELM algorithm to the area of semi-supervised learning which is an important issue of machine learning on big data. However, the original SS-ELM algorithm needs to store the data in the memory before processing it, so that it could not handle large and web-scale data sets which are of frequent appearance in the era of big data. To solve this problem, this paper firstly proposes an efficient parallel SS-ELM (PSS-ELM) algorithm on MapReduce model, adopting a series of optimizations to improve its performance. Then, a parallel approximate SS-ELM Algorithm based on MapReduce (PASS-ELM) is proposed. PASS-ELM is based on the approximate adjacent similarity matrix (AASM) algorithm, which leverages the Locality-Sensitive Hashing (LSH) scheme to calculate the approximate adjacent similarity matrix, thus greatly reducing the complexity and occupied memory. The proposed AASM algorithm is general, because the calculation of the adjacent similarity matrix is the key operation in many other machine learning algorithms. The experimental results have demonstrated that the proposed PASS-ELM algorithm can efficiently process very large-scale data sets with a good performance, without significantly impacting the accuracy of the results. © 2017 Elsevier Inc.","Approximate algorithm; Big data; LSH; MapReduce; Parallel; PASS-ELM","Artificial intelligence; Big data; Knowledge acquisition; Learning systems; Matrix algebra; Optimization; Supervised learning; Approximate algorithms; Extreme learning machine; Generalization performance; Locality sensitive hashing; Map-reduce; Parallel; PASS-ELM; Semi- supervised learning; Learning algorithms",2-s2.0-85011277363
"Gao N., Dredze M., Oard D.W.","Person entity linking in email with NIL detection",2017,"Journal of the Association for Information Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021794472&doi=10.1002%2fasi.23888&partnerID=40&md5=e74054e713d92e8dcefc40a3ad587d2a","For each specific mention of an entity found in a text, the goal of entity linking is to determine whether the referenced entity is present in an existing knowledge base, and if so to determine which KB entity is the correct referent. Entity linking has been well explored for dissemination-oriented sources such as news stories, blogs, and microblog posts, but the limited work to date on “conversational” sources such as email or text chat has not yet attempted to determine when the referent entity is not in the knowledge base (a task known as “NIL detection”). This article presents a supervised machine learning system for linking named mentions of people in email messages to a collection-specific knowledge base, and that is also capable of NIL detection. This system learns from manually annotated training examples to leverage a rich set of features. The entity linking accuracy for entities present in the knowledge base is substantially and significantly better than the best previously reported results on the Enron email collection, comparable accuracy is reported for the challenging NIL detection task, and these results are for the first time replicated on a second email collection from a different source with comparable results. © 2017 ASIS&T",,"Knowledge based systems; Learning systems; Detection tasks; Email messages; Knowledge base; Micro-blog; Specific knowledge; Supervised machine learning; Text chat; Training example; Electronic mail; e-mail; human; knowledge base; supervised machine learning",2-s2.0-85021794472
"Nilashi M., Dalvi-Esfahani M., Ibrahim O., Bagherifard K., Mardani A., Zakuan N.","A soft computing method for the prediction of energy performance of residential buildings",2017,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020455137&doi=10.1016%2fj.measurement.2017.05.048&partnerID=40&md5=9e48b8f923ff559ba701b1c33fb86b2f","Buildings are a crucial factor of energy concerns and one of the most significant energy consumers. Accurate estimation of energy efficiency of residential buildings based on the computation of Heating Load (HL) and the Cooling Load (CL) is an important task. Developing computational tools and methods for prediction of energy performance will help the policy makers in efficient design of building. The aim of this study is therefore to develop an efficient method for the prediction of energy performance of residential buildings using machine learning techniques. Our method is developed through clustering, noise removal and prediction techniques. Accordingly, we use Expectation Maximization (EM), Principal Component Analysis (PCA) and Adaptive Neuro-Fuzzy Inference System (ANFIS) methods for clustering, noise removal and prediction tasks, respectively. Experimental results on real-world dataset show that proposed method remarkably improves the accuracy of prediction in relation to the existing state-of-the-art techniques and is efficient in estimating the energy efficiency of residential buildings. The Mean Absolute Error (MAE) of the predictions for HL and CL are respectively 0.16 and 0.52 which show the effectiveness of our method in predicting HL and CL. © 2017 Elsevier Ltd","Adaptive-Network-based Fuzzy Inference System; Cooling load; Estimation of energy efficiency; Heating load; PCA; Residential buildings","Air conditioning; Buildings; Forecasting; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Heating; Housing; Intelligent agents; Learning systems; Maximum principle; Principal component analysis; Soft computing; Adaptive network based fuzzy inference system; Adaptive neuro-fuzzy inference system; Cooling load; Expectation Maximization; Heating load; Machine learning techniques; Residential building; State-of-the-art techniques; Energy efficiency",2-s2.0-85020455137
"Gorban A.N., Tyukin I.Y.","Stochastic separation theorems",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029412956&doi=10.1016%2fj.neunet.2017.07.014&partnerID=40&md5=033b39ae45977a93c0b7804281fce621","The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M&lt;aexp(bn) random M-element sets in Rn are linearly separable with probability p, p&gt;1−ϑ, where 1&gt;ϑ&gt;0 is a given small constant. Exact values of a,b&gt;0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant ϑ. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples. © 2017 Elsevier Ltd","Extreme point; Fisher's discriminant; Linear separability; Machine learning; Measure concentration; Random set","Artificial intelligence; Iterative methods; Learning systems; Probability; Probability distributions; Stochastic systems; Extreme points; Fisher's discriminant; Linear separability; Measure concentration; Random set; Separation; Article; artificial intelligence; data analysis; linear system; machine learning; mathematical computing; priority journal; probability; probability sample; sampling error; separation technique; stochastic model",2-s2.0-85029412956
"Tan A., Wu W., Tao Y.","A set-cover-based approach for the test-cost-sensitive attribute reduction problem",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969812448&doi=10.1007%2fs00500-016-2173-3&partnerID=40&md5=cf8b78caae617660d6550c2e82fff885","In data mining application, the test-cost-sensitive attribute reduction is an important task which aims to decrease the test cost of data. In operational research, the set cover problem is a typical optimization problem and has a long investigation history compared to the attribute reduction problem. In this paper, we employ the methods of set cover problem to deal with the test-cost-sensitive attribute reduction. First, we equivalently transform the test-cost-sensitive reduction problem into the set cover problem by using a constructive approach. It is shown that computing a reduct of a decision system with minimal test cost is equal to computing an optimal solution of the set cover problem. Then, a set-cover-based heuristic algorithm is introduced to solve the test-cost-sensitive reduction problem. In the end, we conduct several numerical experiments on data sets from UCI machine learning repository. Experimental results indicate that the set-cover-based algorithm has superior performances in most cases, and the algorithm is efficient on data sets with many attributes. © 2016, Springer-Verlag Berlin Heidelberg.","Attribute reduction; Decision table; Rough set; Set cover problem; Test cost","Artificial intelligence; Costs; Data mining; Data reduction; Decision tables; Heuristic algorithms; Learning systems; Optimization; Rough set theory; Testing; Attribute reduction; Constructive approach; Data mining applications; Numerical experiments; Optimization problems; Set cover problem; Test cost; UCI machine learning repository; Cost reduction",2-s2.0-84969812448
"Rezaie-balf M., Naganna S.R., Ghaemi A., Deka P.C.","Wavelet coupled MARS and M5 Model Tree approaches for groundwater level forecasting",2017,"Journal of Hydrology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027551989&doi=10.1016%2fj.jhydrol.2017.08.006&partnerID=40&md5=11ce5a21183abe8b6b08aca27901e219","In this study, two different machine learning models, Multivariate Adaptive Regression Splines (MARS) and M5 Model Trees (MT) have been applied to simulate the groundwater level (GWL) fluctuations of three shallow open wells within diverse unconfined aquifers. The Wavelet coupled MARS and MT hybrid models were developed in an attempt to further increase the GWL forecast accuracy. The Discrete Wavelet Transform (DWT) which is particularly effective in dealing with non-stationary time-series data was employed to decompose the input time series into various sub-series components. Historical data of 10 years (August-1996 to July-2006) comprising monthly groundwater level, rainfall, and temperature were used to calibrate and validate the models. The models were calibrated and tested for one, three and six months ahead forecast horizons. The wavelet coupled MARS and MT models were compared with their simple counterpart using standard statistical performance evaluation measures such as Root Mean Square Error (RMSE), Normalized Nash-Sutcliffe Efficiency (NNSE) and Coefficient of Determination (R2). The wavelet coupled MARS and MT models developed using multi-scale input data performed better compared to their simple counterpart and the forecast accuracy of W-MARS models were superior to that of W-MT models. Specifically, the DWT offered a better discrimination of non-linear and non-stationary trends that were present at various scales in the time series of the input variables thus crafting the W-MARS models to provide more accurate GWL forecasts. © 2017 Elsevier B.V.","Discrete wavelet transform; Forecasting; Groundwater level; M5 model trees; Multivariate adaptive regression splines","Aquifers; Discrete wavelet transforms; Forecasting; Forestry; Groundwater; Groundwater resources; Hydrogeology; Learning systems; Mean square error; Splines; Time series; Coefficient of determination; Groundwater level forecasting; M5 model tree; Machine learning models; Multivariate adaptive regression splines; Non-stationary time series; Root mean square errors; Statistical performance; Wavelet transforms",2-s2.0-85027551989
"Hosseini M., Azar F.T.","A new eigenvector selection strategy applied to develop spectral clustering",2017,"Multidimensional Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960086277&doi=10.1007%2fs11045-016-0391-6&partnerID=40&md5=96ca49792922faf1bf01669b3b9b2d46","Spectral methods are strong tools that can be used for extraction of the data’s structure based on eigenvectors of constructed affinity matrices. In this paper, we aim to propose some new measurement functions to evaluate the ability of each eigenvector of affinity matrix in data clustering. In the proposed strategy, each eigenvector’s elements are clustered by traditional fuzzy c-means algorithm and then informative eigenvectors selection is performed by optimization of an objective function which defined based on three criterions. These criterions are the compactness of clusters, distance between clusters and stability of clustering to evaluate each eigenvector based on considering the structure of clusters which placed on. Finally, Lagrange multipliers method is used to minimize the proposed objective function and extract the most informative eigenvectors. To indicate the merits of our algorithm, we consider UCI Machine Learning Repository databases, COIL20, YALE-B and PicasaWeb as benchmark data sets. Our simulation’s results confirm the superior performance of the proposed strategy in developing spectral clustering compared to conventional clustering methods and recent eigenvector selection based algorithms. © 2016, Springer Science+Business Media New York.","Curse of dimensionality; High-dimensional data; Pattern recognition; Spectral clustering","Artificial intelligence; Cluster analysis; Copying; Data mining; Eigenvalues and eigenfunctions; Function evaluation; Fuzzy clustering; Lagrange multipliers; Learning systems; Matrix algebra; Optimization; Pattern recognition; Structure (composition); Conventional clustering; Curse of dimensionality; Fuzzy C-means algorithms; High dimensional data; Lagrange multipliers method; Measurement function; Spectral clustering; UCI machine learning repository; Clustering algorithms",2-s2.0-84960086277
"Shirazi M., Dhavala S.S., Lord D., Geedipally S.R.","A methodology to design heuristics for model selection based on the characteristics of data: Application to investigate when the Negative Binomial Lindley (NB-L) is preferred over the Negative Binomial (NB)",2017,"Accident Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028733685&doi=10.1016%2fj.aap.2017.07.002&partnerID=40&md5=ed2c301771c00ba89b4e9aaca60898e2","Safety analysts usually use post-modeling methods, such as the Goodness-of-Fit statistics or the Likelihood Ratio Test, to decide between two or more competitive distributions or models. Such metrics require all competitive distributions to be fitted to the data before any comparisons can be accomplished. Given the continuous growth in introducing new statistical distributions, choosing the best one using such post-modeling methods is not a trivial task, in addition to all theoretical or numerical issues the analyst may face during the analysis. Furthermore, and most importantly, these measures or tests do not provide any intuitions into why a specific distribution (or model) is preferred over another (Goodness-of-Logic). This paper ponders into these issues by proposing a methodology to design heuristics for Model Selection based on the characteristics of data, in terms of descriptive summary statistics, before fitting the models. The proposed methodology employs two analytic tools: (1) Monte-Carlo Simulations and (2) Machine Learning Classifiers, to design easy heuristics to predict the label of the ‘most-likely-true’ distribution for analyzing data. The proposed methodology was applied to investigate when the recently introduced Negative Binomial Lindley (NB-L) distribution is preferred over the Negative Binomial (NB) distribution. Heuristics were designed to select the ‘most-likely-true’ distribution between these two distributions, given a set of prescribed summary statistics of data. The proposed heuristics were successfully compared against classical tests for several real or observed datasets. Not only they are easy to use and do not need any post-modeling inputs, but also, using these heuristics, the analyst can attain useful information about why the NB-L is preferred over the NB - or vice versa- when modeling data. © 2017 Elsevier Ltd","Characteristics of Data; Heuristics; Machine Learning; Model Selection; Negative Binomial; Negative Binomial Lindley","Artificial intelligence; Design; Intelligent systems; Learning systems; Monte Carlo methods; Numerical methods; Statistical tests; Characteristics of Data; Heuristics; Model Selection; Negative binomial; Negative binomial Lindley; Heuristic methods",2-s2.0-85028733685
"Hammami Z., Mouelhi W., Ben Said L.","On-line self-adaptive framework for tailoring a neural-agent learning model addressing dynamic real-time scheduling problems",2017,"Journal of Manufacturing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030762400&doi=10.1016%2fj.jmsy.2017.08.003&partnerID=40&md5=396651b5e6f8068b2fff30c8ee112286","The dynamic nature and time-varying behavior of actual environments provide serious challenges for learning models. Thus, changes may deteriorate the constructed control policy over time, which requires permanent adaptation strategies. Changes usually appear as an evolution in the relationship between instance variables composing stream data, known in machine learning under the term concept drift. Several adaptation strategies have been performed to tackle concept drifting data streams, always assuming that arrived instances are labeled, either completely or partially. However, this assumption is violated in many application areas, especially in the manufacturing field. We propose, in this paper, a new framework called Labeling Extraction from the current Model (LEM). LEM is adapted to retrieve learning labels, relying uniquely on unlabeled received instances and without any external supervision, which has never been previously addressed. Hence, to the best of our knowledge, there has been no effort addressing scheduling manufacturing problems for adaptation to data streams with concept drifts. Experiments are conducted to show the effectiveness of LEM. The obtained results demonstrate the ability of LEM to maintain the stability and efficiency of the control policy approximated by the learning model, by significantly improving its prediction performance, compared to its use without adaptation. © 2017 The Society of Manufacturing Engineers","Concept drift; Data stream; Dynamic scheduling; Neural-agent learning model; Real time adaptation","E-learning; Learning systems; Manufacture; Agent learning; Concept drifts; Data stream; Dynamic scheduling; Real-time adaptation; Scheduling",2-s2.0-85030762400
"Sovizi J., Mathieu K.B., Thrower S.L., Stefan W., Hazle J.D., Fuentes D.","Gaussian process classification of superparamagnetic relaxometry data: Phantom study",2017,"Artificial Intelligence in Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028975826&doi=10.1016%2fj.artmed.2017.07.001&partnerID=40&md5=1c87aaade33ebc3bc3a5fa39aae9cb22","Motivation Superparamagnetic relaxometry (SPMR) is an emerging technology that holds potential for use in early cancer detection. Measurement of the magnetic field after the excitation of cancer-bound superparamagnetic iron oxide nanoparticles (SPIONs) enables the reconstruction of SPIONs spatial distribution and hence tumor detection. However, image reconstruction often requires solving an ill-posed inverse problem that is computationally challenging and sensitive to measurement uncertainty. Moreover, an additional image processing module is required to automatically detect and localize the tumor in the reconstructed image. Objective Our goal is to examine the use of data-driven machine learning technique to detect a weak signal induced by a small cluster of SPIONs (surrogate tumor) in presence of background signal and measurement uncertainty. We aim to investigate the performance of both data-driven and image reconstruction models to characterize situations that one can replace the computationally-challenging reconstruction technique by the data-driven model. Methods We utilize Gaussian process (GP) classification model and a physics-based image reconstruction method, tailored to SPMR datasets that are obtained from (i) in silico simulations designed based on mouse cancer models and (ii) phantom experiments using MagSense system (Imagion Biosystems, Inc.). We investigate the performance of the GP classifier against the reconstruction technique, for different levels of measurement noise, different scenarios of SPIONs distribution, and different concentrations of SPIONs at the surrogate tumor. Results In our in silico source detection analysis, we were able to achieve high sensitivity results using GP model that outperformed the image reconstruction model for various choices of SPIONs concentration at the surrogate tumor and measurement noise levels. Moreover, in our phantom studies we were able to detect the surrogate tumor phantoms with 5% and 7.3% of the total used SPIONs, surrounded by 9 low-concentration phantoms with accuracies of 87.5% and 96.4%, respectively. Conclusions The GP framework provides acceptable classification accuracies when dealing with in silico and phantom SPMR datasets and can outperform an image reconstruction method for binary classification of SPMR data. © 2017 Elsevier B.V.","Gaussian process; Superparamagnetic relaxometry; Weak source detection","Biomedical signal processing; Classification (of information); Diseases; Gaussian distribution; Gaussian noise (electronic); Image reconstruction; Inverse problems; Learning systems; Nanomagnetics; Spurious signal noise; Superparamagnetism; Tumors; Uncertainty analysis; Gaussian process classifications; Gaussian Processes; Image reconstruction methods; Machine learning techniques; Reconstruction techniques; Relaxometry; Source detection; Superparamagnetic iron oxide nanoparticles; Image processing; superparamagnetic iron oxide nanoparticle; animal model; Article; cancer model; classification algorithm; computer model; concentration (parameters); gaussian process; image reconstruction; mathematical model; measurement; measurement accuracy; mouse; nanoimaging; noise measurement; nonhuman; priority journal; quantitative study; signal detection; simulation; superparamagnetic relaxometry; validation process",2-s2.0-85028975826
"Rodriguez N., Cabrera G., Lagos C., Cabrera E.","Stationary wavelet singular entropy and kernel extreme learning for bearing multi-fault diagnosis",2017,"Entropy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031917269&doi=10.3390%2fe19100541&partnerID=40&md5=10c96b7d7a1a2565ca12792676e556e2","The behavioural diagnostics of bearings play an essential role in the management of several rotation machine systems. However, current diagnostic methods do not deliver satisfactory results with respect to failures in variable speed rotational phenomena. In this paper, we consider the Shannon entropy as an important fault signature pattern. To compute the entropy, we propose combining stationary wavelet transform and singular value decomposition. The resulting feature extraction method, that we call stationary wavelet singular entropy (SWSE), aims to improve the accuracy of the diagnostics of bearing failure by finding a small number of high-quality fault signature patterns. The features extracted by the SWSE are then passed on to a kernel extreme learning machine (KELM) classifier. The proposed SWSE-KELM algorithm is evaluated using two bearing vibration signal databases obtained from Case Western Reserve University. We compare our SWSE feature extraction method to other well-known methods in the literature such as stationary wavelet packet singular entropy (SWPSE) and decimated wavelet packet singular entropy (DWPSE). The experimental results show that the SWSE-KELM consistently outperforms both the SWPSE-KELM and DWPSE-KELM methods. Further, our SWSE method requires fewer features than the other two evaluated methods, which makes our SWSE-KELM algorithm simpler and faster. © 2017 by the authors.","Kernel extreme learning machine; Singular value decomposition; Stationary wavelet singular entropy",,2-s2.0-85031917269
"Livingstone M., Folkman L., Yang Y., Zhang P., Mort M., Cooper D.N., Liu Y., Stantic B., Zhou Y.","Investigating DNA-, RNA-, and protein-based features as a means to discriminate pathogenic synonymous variants",2017,"Human Mutation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022338217&doi=10.1002%2fhumu.23283&partnerID=40&md5=5ead83ffec5bd0bf14c87674be4ae215","Synonymous single-nucleotide variants (SNVs), although they do not alter the encoded protein sequences, have been implicated in many genetic diseases. Experimental studies indicate that synonymous SNVs can lead to changes in the secondary and tertiary structures of DNA and RNA, thereby affecting translational efficiency, cotranslational protein folding as well as the binding of DNA-/RNA-binding proteins. However, the importance of these various features in disease phenotypes is not clearly understood. Here, we have built a support vector machine (SVM) model (termed DDIG-SN) as a means to discriminate disease-causing synonymous variants. The model was trained and evaluated on nearly 900 disease-causing variants. The method achieves robust performance with the area under the receiver operating characteristic curve of 0.84 and 0.85 for protein-stratified 10-fold cross-validation and independent testing, respectively. We were able to show that the disease-causing effects in the immediate proximity to exon–intron junctions (1–3 bp) are driven by the loss of splicing motif strength, whereas the gain of splicing motif strength is the primary cause in regions further away from the splice site (4–69 bp). The method is available as a part of the DDIG server at http://sparks-lab.org/ddig. © 2017 Wiley Periodicals, Inc.","bioinformatics; machine learning; same-sense variant; silent mutation; synonymous SNV","DNA; messenger RNA; messenger RNA precursor; amino acid sequence; Article; DNA sequence; exon; gene frequency; genetic conservation; genetic variation; human; human genome; intron; machine learning; priority journal; protein folding; protein structure; RNA splicing; silent mutation; single nucleotide polymorphism; support vector machine",2-s2.0-85022338217
"Chou J.-S., Ngo N.-T., Chong W.K.","The use of artificial intelligence combiners for modeling steel pitting risk and corrosion rate",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008656047&doi=10.1016%2fj.engappai.2016.09.008&partnerID=40&md5=98fdaab74aeee1ee4d2e42735cced426","Corrosion is a common deterioration that reduces the service life of concrete structures and steels. Particularly, corrosion behavior is a highly nonlinear problem influenced by complex characteristics. This study used advanced artificial intelligence (AI) techniques to predict pitting corrosion risk of steel reinforced concrete and marine corrosion rate of carbon steel. The AI-based models used for prediction included single and ensemble models constructed from four well-known machine learners including artificial neural networks (ANNs), support vector regression/machines (SVR/SVMs), classification and regression tree (CART), and linear regression (LR). Notably, a hybrid metaheuristic regression model was implemented by integrating a smart nature-inspired metaheuristic optimization algorithm (i.e., smart firefly algorithm) with a least squares SVR. Prediction accuracy was evaluated using two real-world datasets. According to the comparison results, the hybrid metaheuristic regression model was better than the single and ensemble models in predicting the pitting corrosion risk (mean absolute percentage error=5.6%) and the marine corrosion rate (mean absolute percentage error = 1.26%). The hybrid metaheuristic regression model is a promising and practical methodology for real-time tracking of corrosion in steel rebar. Civil engineers can use the hybrid model to schedule maintenance process that leads to risk reduction of structure failure and maintenance cost. © 2016 Elsevier Ltd","Artificial intelligence; Corrosion rate; Engineering application; Machine learning; Meta ensemble; Metaheuristic regression; Pitting risk","Artificial intelligence; Carbon; Carbon steel; Complex networks; Composite structures; Concretes; Corrosion rate; Electrochemical corrosion; Forecasting; Learning systems; Marine engineering; Neural networks; Optimization; Pitting; Regression analysis; Reinforced concrete; Seawater corrosion; Classification and regression tree; Engineering applications; Mean absolute percentage error; Meta ensemble; Meta-heuristic optimizations; Metaheuristic; Steel reinforced concrete; Support vector regression (SVR); Corrosion",2-s2.0-85008656047
"Zhang Z., Jia L., Zhang M., Li B., Zhang L., Li F.","Discriminative clustering on manifold for adaptive transductive classification",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027419074&doi=10.1016%2fj.neunet.2017.07.013&partnerID=40&md5=cb168a65a93d6da64fa06ff93bb14144","In this paper, we mainly propose a novel adaptive transductive label propagation approach by joint discriminative clustering on manifolds for representing and classifying high-dimensional data. Our framework seamlessly combines the unsupervised manifold learning, discriminative clustering and adaptive classification into a unified model. Also, our method incorporates the adaptive graph weight construction with label propagation. Specifically, our method is capable of propagating label information using adaptive weights over low-dimensional manifold features, which is different from most existing studies that usually predict the labels and construct the weights in the original Euclidean space. For transductive classification by our formulation, we first perform the joint discriminative K-means clustering and manifold learning to capture the low-dimensional nonlinear manifolds. Then, we construct the adaptive weights over the learnt manifold features, where the adaptive weights are calculated through performing the joint minimization of the reconstruction errors over features and soft labels so that the graph weights can be joint-optimal for data representation and classification. Using the adaptive weights, we can easily estimate the unknown labels of samples. After that, our method returns the updated weights for further updating the manifold features. Extensive simulations on image classification and segmentation show that our proposed algorithm can deliver the state-of-the-art performance on several public datasets. © 2017 Elsevier Ltd","Adaptive transductive classification; Discriminative clustering; Label propagation; Manifold learning","Clustering algorithms; Data mining; Image segmentation; Adaptive classification; Discriminative clustering; Extensive simulations; High dimensional data; Label propagation; Low-dimensional manifolds; Manifold learning; State-of-the-art performance; Classification (of information); analytical error; Article; classification algorithm; cluster analysis; discriminant analysis; image reconstruction; image segmentation; nonlinear system; priority journal; simulation; unsupervised machine learning",2-s2.0-85027419074
"Kafashan M., Ching S.","Recurrent networks with soft-thresholding nonlinearities for lightweight coding",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029411852&doi=10.1016%2fj.neunet.2017.07.008&partnerID=40&md5=f5ec987ec9cc9166c537dbc22e8888b6","A long-standing and influential hypothesis in neural information processing is that early sensory networks adapt themselves to produce efficient codes of afferent inputs. Here, we show how a nonlinear recurrent network provides an optimal solution for the efficient coding of an afferent input and its history. We specifically consider the problem of producing lightweight codes, ones that minimize both ℓ1 and ℓ2 constraints on sparsity and energy, respectively. When embedded in a linear coding paradigm, this problem results in a non-smooth convex optimization problem. We employ a proximal gradient descent technique to develop the solution, showing that the optimal code is realized through a recurrent network endowed with a nonlinear soft thresholding operator. The training of the network connection weights is readily achieved through gradient-based local learning. If such learning is assumed to occur on a slower time-scale than the (faster) recurrent dynamics, then the network as a whole converges to an optimal set of codes and weights via what is, in effect, an alternative minimization procedure. Our results show how the addition of thresholding nonlinearities to a recurrent network may enable the production of lightweight, history-sensitive encoding schemes. © 2017 Elsevier Ltd","Efficient sparse coding; Neural networks; Proximal gradient descent; Short-term memory; Unsupervised learning","Convex optimization; Network coding; Neural networks; Optimal systems; Optimization; Scales (weighing instruments); Unsupervised learning; Alternative minimizations; Gradient descent; Gradient descent techniques; Neural information processing; Non-smooth convex optimizations; Short term memory; Soft-thresholding operators; Sparse coding; Codes (symbols); Article; artificial neural network; coding; coding algorithm; energy; linear system; nonlinear system; priority journal; process optimization; short term memory; unsupervised machine learning; weight",2-s2.0-85029411852
"Li H., Zhang Z., Liu Z.","Application of artificial neural networks for catalysis: A review",2017,"Catalysts",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032577034&doi=10.3390%2fcatal7100306&partnerID=40&md5=3acfbce0baa183865f8fa76df73fc82d","Machine learning has proven to be a powerful technique during the past decades. Artificial neural network (ANN), as one of the most popular machine learning algorithms, has been widely applied to various areas. However, their applications for catalysis were not well-studied until recent decades. In this review, we aim to summarize the applications of ANNs for catalysis research reported in the literature. We show how this powerful technique helps people address the highly complicated problems and accelerate the progress of the catalysis community. From the perspectives of both experiment and theory, this review shows how ANNs can be effectively applied for catalysis prediction, the design of new catalysts, and the understanding of catalytic structures. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial neural network (ANN); Catalysis; Catalyst; Experiment; Machine learning; Theory",,2-s2.0-85032577034
"Sánchez-Rodríguez A., Pérez-Castillo Y., Schürer S.C., Nicolotti O., Mangiatordi G.F., Borges F., Cordeiro M.N.D.S., Tejera E., Medina-Franco J.L., Cruz-Monteagudo M.","From flamingo dance to (desirable) drug discovery: a nature-inspired approach",2017,"Drug Discovery Today",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021071373&doi=10.1016%2fj.drudis.2017.05.008&partnerID=40&md5=7f720611625feeaaa9742a12986d71d3","The therapeutic effects of drugs are well known to result from their interaction with multiple intracellular targets. Accordingly, the pharma industry is currently moving from a reductionist approach based on a ‘one-target fixation’ to a holistic multitarget approach. However, many drug discovery practices are still procedural abstractions resulting from the attempt to understand and address the action of biologically active compounds while preventing adverse effects. Here, we discuss how drug discovery can benefit from the principles of evolutionary biology and report two real-life case studies. We do so by focusing on the desirability principle, and its many features and applications, such as machine learning-based multicriteria virtual screening. Here, we describe a multicriteria virtual screening approach based on desirability functions and tailored ensemble machine-learning classifiers. © 2017 Elsevier Ltd",,"drug research; drug screening; machine learning; quantitative structure activity relation; Review",2-s2.0-85021071373
"Gomariz-Castillo F., Alonso-Sarría F., Cánovas-García F.","Improving classification accuracy of multi-temporal landsat images by assessing the use of different algorithms, textural and ancillary information for a Mediterranean semiarid area from 2000 to 2015",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032876914&doi=10.3390%2frs9101058&partnerID=40&md5=3f3ff923e58cd30e8c7c664cff9e8bd6","The aim of this study was to evaluate three different strategies to improve classification accuracy in a highly fragmented semiarid area using, (i) different classification algorithms with parameter optimization in some cases; (ii) different feature sets including spectral, textural and terrain features; and (iii) different seasonal combinations of images. A three-way ANOVA was used to discern which of these approaches and their interactions significantly increases accuracy. Tukey-Kramer contrast using a heteroscedasticity-consistent estimation of the kappa covariances matrix was used to check for significant differences in accuracy. The experiment was carried out with Landsat TM, ETM and OLI images corresponding to the period 2000-2015. A combination of four images using random forest and the three feature sets was the best way to improve accuracy. Maximum likelihood, random forest and support vector machines do not significantly increase accuracy when textural information was added, but do so when terrain features were taken into account. On the other hand, sequential maximum a posteriori increased accuracy when textural features were used, but reduced accuracy substantially when terrain features were included. Random forest using the three feature subsets and sequential maximum a posteriori with spectral and textural features had the largest kappa values, around 0.9. © 2017 by the authors.","Contextual information; Land use classification; Machine learning; Textural information","Decision trees; Image classification; Image enhancement; Land use; Landforms; Learning systems; Maximum likelihood; Maximum likelihood estimation; Optimization; Classification accuracy; Classification algorithm; Consistent estimation; Contextual information; Covariances matrices; Landuse classifications; Parameter optimization; Textural information; Classification (of information)",2-s2.0-85032876914
"Wang D., He H., Liu D.","Improving the Critic Learning for Event-Based Nonlinear H∞ Control Design",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011706654&doi=10.1109%2fTCYB.2017.2653800&partnerID=40&md5=56e536f414878ad9cda89952fec31759","In this paper, we aim at improving the critic learning criterion to cope with the event-based nonlinear H∞ state feedback control design. First of all, the H∞ control problem is regarded as a two-player zero-sum game and the adaptive critic mechanism is used to achieve the minimax optimization under event-based environment. Then, based on an improved updating rule, the event-based optimal control law and the time-based worst-case disturbance law are obtained approximately by training a single critic neural network. The initial stabilizing control is no longer required during the implementation process of the new algorithm. Next, the closed-loop system is formulated as an impulsive model and its stability issue is handled by incorporating the improved learning criterion. The infamous Zeno behavior of the present event-based design is also avoided through theoretical analysis on the lower bound of the minimal intersample time. Finally, the applications to an aircraft dynamics and a robot arm plant are carried out to verify the efficient performance of the present novel design method. © 2013 IEEE.","adaptive systems; adaptive/approximate dynamic programming; critic network; event-based design; H∞ control; learning criterion; neural control","Closed loop systems; Machine design; Optimization; State feedback; Training aircraft; Aircraft dynamics; Implementation process; Impulsive models; Learning criterion; Minimax optimization; Optimal control law; Stability issues; Stabilizing control; Design",2-s2.0-85011706654
"Chen H.-Y., Chen C.-C., Hwang W.-J.","An efficient hardware circuit for spike sorting based on competitive learning networks",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030540839&doi=10.3390%2fs17102232&partnerID=40&md5=9b1bc311e969769f6946eaddaa6fc8ef","This study aims to present an effective VLSI circuit for multi-channel spike sorting. The circuit supports the spike detection, feature extraction and classification operations. The detection circuit is implemented in accordance with the nonlinear energy operator algorithm. Both the peak detection and area computation operations are adopted for the realization of the hardware architecture for feature extraction. The resulting feature vectors are classified by a circuit for competitive learning (CL) neural networks. The CL circuit supports both online training and classification. In the proposed architecture, all the channels share the same detection, feature extraction, learning and classification circuits for a low area cost hardware implementation. The clock-gating technique is also employed for reducing the power dissipation. To evaluate the performance of the architecture, an application-specific integrated circuit (ASIC) implementation is presented. Experimental results demonstrate that the proposed circuit exhibits the advantages of a low chip area, a low power dissipation and a high classification success rate for spike sorting. © 2017 by the authors.","Brain machine interface; Competitive learning; Spike sorting; VLSI","Brain computer interface; Classification (of information); Electric losses; Extraction; Feature extraction; Hardware; Network architecture; Timing circuits; VLSI circuits; Brain machine interface; Clock gating techniques; Competitive learning; Feature extraction and classification; Hardware implementations; Proposed architectures; Spike-sorting; VLSI; Low power electronics",2-s2.0-85030540839
"Cushman F., Kumar V., Railton P.","Moral learning: Psychological and philosophical perspectives",2017,"Cognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020828418&doi=10.1016%2fj.cognition.2017.06.008&partnerID=40&md5=443f3960e177478132939cdf99032dd8","The past 15 years occasioned an extraordinary blossoming of research into the cognitive and affective mechanisms that support moral judgment and behavior. This growth in our understanding of moral mechanisms overshadowed a crucial and complementary question, however: How are they learned? As this special issue of the journal Cognition attests, a new crop of research into moral learning has now firmly taken root. This new literature draws on recent advances in formal methods developed in other domains, such as Bayesian inference, reinforcement learning and other machine learning techniques. Meanwhile, it also demonstrates how learning and deciding in a social domain—and especially in the moral domain—sometimes involves specialized cognitive systems. We review the contributions to this special issue and situate them within the broader contemporary literature. Our review focuses on how we learn moral values and moral rules, how we learn about personal moral character and relationships, and the philosophical implications of these emerging models. © 2017 Elsevier B.V.",,"Bayesian learning; character; cognition; Editorial; empathy; habit; heuristics; human; imagination; learning; morality; philosophy; priority journal; reward; self concept; social learning; social norm; social psychology",2-s2.0-85020828418
"Isik Z., Ercan M.E.","Integration of RNA-Seq and RPPA data for survival time prediction in cancer patients",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028692279&doi=10.1016%2fj.compbiomed.2017.08.028&partnerID=40&md5=2f8f1e109595dd567f95da6e178a4f59","Integration of several types of patient data in a computational framework can accelerate the identification of more reliable biomarkers, especially for prognostic purposes. This study aims to identify biomarkers that can successfully predict the potential survival time of a cancer patient by integrating the transcriptomic (RNA-Seq), proteomic (RPPA), and protein-protein interaction (PPI) data. The proposed method —RPBioNet— employs a random walk-based algorithm that works on a PPI network to identify a limited number of protein biomarkers. Later, the method uses gene expression measurements of the selected biomarkers to train a classifier for the survival time prediction of patients. RPBioNet was applied to classify kidney renal clear cell carcinoma (KIRC), glioblastoma multiforme (GBM), and lung squamous cell carcinoma (LUSC) patients based on their survival time classes (long- or short-term). The RPBioNet method correctly identified the survival time classes of patients with between 66% and 78% average accuracy for three data sets. RPBioNet operates with only 20 to 50 biomarkers and can achieve on average 6% higher accuracy compared to the closest alternative method, which uses only RNA-Seq data in the biomarker selection. Further analysis of the most predictive biomarkers highlighted genes that are common for both cancer types, as they may be driver proteins responsible for cancer progression. The novelty of this study is the integration of a PPI network with mRNA and protein expression data to identify more accurate prognostic biomarkers that can be used for clinical purposes in the future. © 2017 Elsevier Ltd","Biomarker; Interaction network; RNA-Seq; RPPA; Survival time","Data integration; Diseases; Forecasting; Gene expression; Genes; Hospital data processing; Integration; Nucleic acids; Proteins; RNA; Computational framework; Expression measurements; Glioblastoma multiforme; Interaction networks; Protein-protein interactions; RPPA; Squamous cell carcinoma; Survival time; Biomarkers; messenger RNA; Notch1 receptor; Notch2 receptor; Article; cancer diagnosis; cancer growth; cancer patient; controlled study; gene expression; gene ontology; glioblastoma; human; machine learning; major clinical study; priority journal; protein expression; protein protein interaction; proteomics; renal cell carcinoma; RNA sequence; squamous cell lung carcinoma; support vector machine; survival time; transcriptomics",2-s2.0-85028692279
"Simpson D., Leipzig R.M., Sauvigné K., Donald W. Reynolds Geriatrics Education Collaborative","The 2025 Big “G” Geriatrician: Defining Job Roles to Guide Fellowship Training",2017,"Journal of the American Geriatrics Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023208582&doi=10.1111%2fjgs.14995&partnerID=40&md5=1d56b42709e6d9a1afc1ff3018ee2217","Changes in health care that are already in progress, including value- and population-based care, use of new technologies for care, big data and machine learning, and the patient as consumer and decision maker, will determine the job description for geriatricians practicing in 2025. Informed by these future certainties, 115 geriatrics educators attending the 2016 Donald W. Reynolds Foundation Annual meeting identified five 2025 geriatrician job roles: complexivist; consultant; health system leader and innovator; functional preventionist; and educator for big “G” and little “g” providers. By identifying these job roles, geriatrics fellowship training can be preemptively redesigned. © 2017, Copyright the Authors Journal compilation © 2017, The American Geriatrics Society","education and training; fellowship; future; workforce","consultation; consumer; doctor patient relation; education; geriatrician; geriatrics; human; leadership; machine learning; work; forecasting; geriatrician; medical education; organization; physician attitude; psychology; trends; Congresses as Topic; Fellowships and Scholarships; Forecasting; Geriatricians; Geriatrics; Humans; Physician's Role",2-s2.0-85023208582
"Kooi T., Karssemeijer N.","Classifying symmetrical differences and temporal change for the detection of malignant masses in mammography using deep neural networks",2017,"Journal of Medical Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032749172&doi=10.1117%2f1.JMI.4.4.044501&partnerID=40&md5=c1267b4a16dadbaede19ffb2f6cd98c8","We investigate the addition of symmetry and temporal context information to a deep convolutional neural network (CNN) with the purpose of detecting malignant soft tissue lesions in mammography. We employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contralateral or prior mammogram, and regions of interest (ROIs) are extracted around each location. Two different architectures are subsequently explored: (1) a fusion model employing two datastreams where both ROIs are fed to the network during training and testing and (2) a stagewise approach where a single ROI CNN is trained on the primary image and subsequently used as a feature extractor for both primary and contralateral or prior ROIs. A shallow gradient boosted tree classifier is then trained on the concatenation of these features and used to classify the joint representation. The baseline yielded an AUC of 0.87 with confidence interval [0.853, 0.893]. For the analysis of symmetrical differences, the first architecture where both primary and contralateral patches are presented during training obtained an AUC of 0.895 with confidence interval [0.877, 0.913], and the second architecture where a new classifier is retrained on the concatenation an AUC of 0.88 with confidence interval [0.859, 0.9]. We found a significant difference between the first architecture and the baseline at high specificity with p 0.02. When using the same architectures to analyze temporal change, we yielded an AUC of 0.884 with confidence interval [0.865, 0.902] for the first architecture and an AUC of 0.879 with confidence interval [0.858, 0.898] in the second setting. Although improvements for temporal analysis were consistent, they were not found to be significant. The results show our proposed method is promising and we suspect performance can greatly be improved when more temporal data become available. ©2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","breast cancer.; computer-aided diagnosis; convolutional neural networks; deep learning; machine learning",,2-s2.0-85032749172
"Rister B., Rubin D.L.","Piecewise convexity of artificial neural networks",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024495755&doi=10.1016%2fj.neunet.2017.06.009&partnerID=40&md5=fdf161bdf1b2114da78c5b4bd18884da","Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. First, that the network is piecewise convex as a function of the input data. Second, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Third, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize the objective on certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space. © 2017 Elsevier Ltd","Convergence; Convex analysis; Gradient descent; Machine learning; Neural networks; Optimization","Functions; Global optimization; Learning systems; Neural networks; Speech recognition; Three term control systems; Convergence; Convergence conditions; Convex analysis; Global optimization problems; Gradient descent; Gradient Descent method; Optimization algorithms; Theoretical guarantees; Optimization; algorithm; Article; artificial neural network; brain function; mathematical analysis; mathematical parameters; priority journal; process optimization; speech discrimination; theoretical model",2-s2.0-85024495755
"Marchal S., Armano G., Grondahl T., Saari K., Singh N., Asokan N.","Off-the-hook: An efficient and usable client-side phishing prevention application",2017,"IEEE Transactions on Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029649779&doi=10.1109%2fTC.2017.2703808&partnerID=40&md5=1e900dff24eff13c7da13dc8b66446fb","Phishing is a major problem on the Web. Despite the significant attention it has received over the years, there has been no definitive solution. While the state-of-the-art solutions have reasonably good performance, they suffer from several drawbacks including potential to compromise user privacy, difficulty of detecting phishing websites whose content change dynamically, and reliance on features that are too dependent on the training data. To address these limitations we present a new approach for detecting phishing webpages in real-time as they are visited by a browser. It relies on modeling inherent phisher limitations stemming from the constraints they face while building a webpage. Consequently, the implementation of our approach, Off-the-Hook, exhibits several notable properties including high accuracy, brand-independence and good language-independence, speed of decision, resilience to dynamic phish and resilience to evolution in phishing techniques. Off-the-Hook is implemented as a fully-client-side browser add-on, which preserves user privacy. In addition, Off-the-Hook identifies the target website that a phishing webpage is attempting to mimic and includes this target in its warning. We evaluated Off-the-Hook in two different user studies. Our results show that users prefer Off-the-Hook warnings to Firefox warnings. © 1968-2012 IEEE.","browser add-on; machine learning; phishing prevention; phishing target identification; Phishing webpage detection; web security","Learning systems; Web browsers; Websites; Browser add-ons; Language independence; New approaches; Phishing; Phishing targets; Phishing websites; State of the art; WEB security; Computer crime",2-s2.0-85029649779
"Carnelossi Furlaneto D., Oliveira L.S., Menotti D., Cavalcanti G.D.C.","Bias effect on predicting market trends with EMD",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017161780&doi=10.1016%2fj.eswa.2017.03.053&partnerID=40&md5=1242ad2666bb4eeb0e2d8d685f0d2513","Financial time series are notoriously difficult to analyze and predict, given their non-stationary, highly oscillatory nature. In this study, we evaluate the effectiveness of the Ensemble Empirical Mode Decomposition (EEMD), the ensemble version of Empirical Mode Decomposition (EMD), at generating a representation for market indexes that improves trend prediction. Our results suggest that the promising results reported using EEMD on financial time series were obtained by inadvertently adding look-ahead bias to the testing protocol via pre-processing the entire series with EMD, which affects predictive results. In contrast to conclusions found in the literature, our results indicate that the application of EMD and EEMD with the objective of generating a better representation for financial time series is not sufficient to improve the accuracy or cumulative return obtained by the models used in this study. © 2017 Elsevier Ltd","EEMD; Finance; Machine learning; Time series; Trend prediction","Commerce; Finance; Financial data processing; Forecasting; Learning systems; Time series; EEMD; Empirical Mode Decomposition; Ensemble empirical mode decompositions (EEMD); Financial time series; Market trends; Pre-processing; Testing protocols; Trend prediction; Signal processing",2-s2.0-85017161780
"Duessel P., Gehl C., Flegel U., Dietrich S., Meier M.","Detecting zero-day attacks using context-aware anomaly detection at the application-layer",2017,"International Journal of Information Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979211525&doi=10.1007%2fs10207-016-0344-y&partnerID=40&md5=c56efa7dd9a987515e67d119f041ba01","Anomaly detection allows for the identification of unknown and novel attacks in network traffic. However, current approaches for anomaly detection of network packet payloads are limited to the analysis of plain byte sequences. Experiments have shown that application-layer attacks become difficult to detect in the presence of attack obfuscation using payload customization. The ability to incorporate syntactic context into anomaly detection provides valuable information and increases detection accuracy. In this contribution, we address the issue of incorporating protocol context into payload-based anomaly detection. We present a new data representation, called cn-grams, that allows to integrate syntactic and sequential features of payloads in an unified feature space and provides the basis for context-aware detection of network intrusions. We conduct experiments on both text-based and binary application-layer protocols which demonstrate superior accuracy on the detection of various types of attacks over regular anomaly detection methods. Furthermore, we show how cn-grams can be used to interpret detected anomalies and thus, provide explainable decisions in practice. © 2016, Springer-Verlag Berlin Heidelberg.","Anomaly detection; Deep packet inspection; Intrusion detection; Machine learning; Protocol analysis","Artificial intelligence; Learning systems; Signal detection; Syntactics; Anomaly detection; Anomaly detection methods; Application layer protocols; Attacks in networks; Data representations; Deep packet inspection; Identification of unknowns; Protocol analysis; Intrusion detection",2-s2.0-84979211525
"Gutierrez-Becker B., Mateus D., Peter L., Navab N.","Guiding multimodal registration with learned optimization updates",2017,"Medical Image Analysis",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019143539&doi=10.1016%2fj.media.2017.05.002&partnerID=40&md5=6f61dd81bc5d0a5270a3c834377ee14b","In this paper, we address the multimodal registration problem from a novel perspective, aiming to predict the transformation aligning images directly from their visual appearance. We formulate the prediction as a supervised regression task, with joint image descriptors as input and the output are the parameters of the transformation that guide the moving image towards alignment. We model the joint local appearance with context aware descriptors that capture both local and global cues simultaneously in the two modalities, while the regression function is based on the gradient boosted trees method capable of handling the very large contextual feature space. The good properties of our predictions allow us to couple them with a simple gradient-based optimization for the final registration. Our approach can be applied to any transformation parametrization as well as a broad range of modality pairs. Our method learns the relationship between the intensity distributions of a pair of modalities by using prior knowledge in the form of a small training set of aligned image pairs (in the order of 1–5 in our experiments). We demonstrate the flexibility and generality of our method by evaluating its performance on a variety of multimodal imaging pairs obtained from two publicly available datasets, RIRE (brain MR, CT and PET) and IXI (brain MR). We also show results for the very challenging deformable registration of Intravascular Ultrasound and Histology images. In these experiments, our approach has a larger capture range when compared to other state-of-the-art methods, while improving registration accuracy in complex cases. © 2017 Elsevier B.V.","Image registration; Machine learning; Motion estimation; Multimodal registration","Forecasting; Image processing; Image registration; Image segmentation; Learning systems; Motion estimation; Deformable registration; Gradient-based optimization; Intensity distribution; Intravascular ultrasound; Multimodal registration; Registration accuracy; Regression function; State-of-the-art methods; Computerized tomography; accuracy; Article; human; image analysis; intravascular ultrasound; multimodal imaging; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; priority journal; x-ray computed tomography",2-s2.0-85019143539
"Zhu J., Guan Q., Zhao X., Cao Y., Chen G.","A steganalytic scheme based on classifier selection using joint image characteristics",2017,"International Journal of Digital Crime and Forensics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028076488&doi=10.4018%2fIJDCF.2017100101&partnerID=40&md5=93797a5670942bd1ded2fbd73c46784e","Steganalysis relies on steganalytic features and classification techniques. Because of the complexity and different characteristics of cover images, to make steganalysis more applicable toward detecting stego images in real applications, we need to train different classifiers so as to match different images according to their characteristics. Selection of classifiers according to characteristics of images is the key point to improve accuracy of steganalysis. In our work, we study the methods of classifier selection based on characteristics of images including image size, quantization factor, or matrix. Besides, we also discuss other characteristics, such as texture, cover source, which makes an appreciable difference to steganalysis. Copyright © 2017, IGI Global.","Blind Steganalytic; Classifier Selection; Cover Source Mismatch; Information Security; Machine Learning; Steganography","Image classification; Learning systems; Security of data; Steganography; Blind Steganalytic; Classification technique; Classifier selection; Image characteristics; Quantization factor; Real applications; Source mismatch; Steganalysis; Classification (of information)",2-s2.0-85028076488
"Adler J., Veneris A.","Leveraging Software Configuration Management in Automated RTL Design Debug",2017,"IEEE Design and Test",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029807935&doi=10.1109%2fMDAT.2017.2713391&partnerID=40&md5=40fc4cdb709ef9568f2159643a811135","Editor's note: This article presents an enhancement to the existing automated debugging software by leveraging statistics from the revision control history. - Li-C Wang, University of California at Santa Barbara. © 2013 IEEE.","Debugging; Machine Learning; Verification","Computer debugging; Learning systems; Verification; Automated debugging; Revision control; RTL designs; Santa Barbara; Software configuration management; University of California; Program debugging",2-s2.0-85029807935
"Pang Y., Xue X., Namin A.S.","Fault Localizations Through Feature Selections",2017,"International Journal of Software Engineering and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032024732&doi=10.1142%2fS0218194017500474&partnerID=40&md5=00476d1d8075f65470650ef997f77ce3","We introduce a novel application of feature ranking methods to the fault localization problem. We envision the problem of localizing causes of failures as instances of ranking program's elements where elements are conceptualized as features. In this paper, we define features as program's statements. However, in its fine-grained definition, the idea of program's features can refer to any traits of programs. This paper proposes feature ranking-based algorithms. The algorithms analyze execution traces of both passing and failing test cases, and extract the bug signatures from the failing test cases. The proposed procedure extracts possible combinations of program's elements when executed together from bug signatures. The feature ranking-based algorithms then order statements according to the suspiciousness of the combinations. When viewed as sequences, the combination of program's elements produced and traced in bug signatures can be utilized to reason about the common longest subsequence. The common longest subsequence of bug signatures represents the common statements executed by all failing test cases and thus provides a means for identifying statements that contain possible faults. Our evaluation indicates that the proposed feature-based fault localization outperforms existing fault localization ranking schemes. © 2017 World Scientific Publishing Company.","debugging; Fault localization; feature ranking methods; machine learning","Artificial intelligence; Computer debugging; Software engineering; Execution trace; Fault localization; Feature ranking; Feature-based; Fine grained; Novel applications; Test case; Learning systems",2-s2.0-85032024732
"Foremski P., Callegari C., Pagano M.","Waterfall Traffic Classification: A Quick Approach to Optimizing Cascade Classifiers",2017,"Wireless Personal Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990852375&doi=10.1007%2fs11277-016-3751-5&partnerID=40&md5=ac8ff36250dc48ae31107459fd6f576a","Heterogeneous wireless communication networks, like 4G LTE, transport diverse kinds of IP traffic: voice, video, Internet data, and more. In order to effectively manage such networks, administrators need adequate tools, of which traffic classification is the basis for visualizing, shaping, and filtering the broad streams of IP packets observed nowadays. In this paper, we describe a modular, cascading traffic classification system—the Waterfall architecture—and we extensively describe a novel technique for its optimization—in terms of CPU time, number of errors, and percentage of unrecognized flows. We show how to significantly accelerate the process of exhaustive search for the best performing cascade. We employ five datasets of real Internet transmissions and seven traffic analysis methods to demonstrate that our proposal yields valid results and outperforms a greedy optimizer. © 2016, The Author(s).","Convergent networks; Machine learning; Network management; Traffic classification","Artificial intelligence; Classification (of information); Internet; Internet protocols; Learning systems; Network management; Telecommunication traffic; Voice/data communication systems; Cascade classifiers; Internet data; Internet transmission; Novel techniques; Quick approach; Traffic analysis; Traffic classification; Wireless communication network; Wireless telecommunication systems",2-s2.0-84990852375
"Lin W.-C., Ke S.-W., Tsai C.-F.","When should we ignore examples with missing values?",2017,"International Journal of Data Warehousing and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028038261&doi=10.4018%2fIJDWM.2017100104&partnerID=40&md5=6d77ba5c7c601f4dd374af7e4cf6b2cf","In practice, the dataset collected from data mining usually contains some missing values. It is common practice to perform case deletion by ignoring those data with missing values if the missing rate is certainly small. The aim of this paper is to answer the following question: When should one directly ignore sampled data with missing values? By using different types of datasets having various numbers of attributes, data samples, and classes, it is found that there are some specific patterns that can be considered for case deletion over different datasets without significant performance degradation. In particular, these patterns are extracted to act as the decision rules by a decision tree model. In addition, a comparison is made between cases with deletion and imputation over different datasets with the allowed missing rates and the decision rules. The results show that the classification performance results obtained by case deletion and imputation are similar, which demonstrates the reliability of the extracted decision rules. © 2017, IGI Global.","Case Deletion; Categorical Data; Classification; Data Mining; Imputation; Machine Learning; Missing Values; Numerical Data","Data mining; Decision trees; Learning systems; Categorical data; Classification performance; Decision rules; Decision tree modeling; Imputation; Missing values; Numerical data; Performance degradation; Classification (of information)",2-s2.0-85028038261
"Kalantarian H., Sarrafzadeh M.","Probabilistic time-series segmentation",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017372697&doi=10.1016%2fj.pmcj.2017.03.005&partnerID=40&md5=e352e2b3640701226e245209b50fcd77","Among the major challenges in the realization of practical health monitoring systems is the identification of short-duration events from larger signals. Time-series segmentation refers to the challenge of subdividing a continuous stream of data into discrete windows, which are individually processed using statistical classifiers to recognize various activities or events. In this paper, we propose a probabilistic algorithm for segmenting time-series signals, in which window boundaries are dynamically adjusted when the probability of correct classification is low. Our proposed scheme is benchmarked using an audio-based nutrition-monitoring case-study. Our evaluation shows that the algorithm improves the number of correctly classified instances from a baseline of 75%–94% using the RandomForest classifier. © 2017 Elsevier B.V.","Machine learning; Pervasive computing; Signal processing; Time-series segmentation","Classification (of information); Learning systems; Signal processing; Ubiquitous computing; Audio-based; Health monitoring system; Probabilistic algorithm; Probability of correct classifications; Short duration events; Statistical classifier; Time series signals; Time-series segmentation; Time series",2-s2.0-85017372697
"Baka N., Leenstra S., van Walsum T.","Random Forest-Based Bone Segmentation in Ultrasound",2017,"Ultrasound in Medicine and Biology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025445758&doi=10.1016%2fj.ultrasmedbio.2017.04.022&partnerID=40&md5=893c27b03f3d888444a2da40bd2658ae","Ultrasound (US) imaging is a safe alternative to radiography for guidance during minimally invasive orthopedic procedures. However, ultrasound is challenging to interpret because of the relatively low signal-to-noise ratio and its inherent speckle pattern that decreases image quality. Here we describe a method for automatic bone segmentation in 2-D ultrasound images using a patch-based random forest classifier and several ultrasound specific features, such as shadowing. We illustrate that existing shadow features are not robust to changes in US acquisition parameters, and propose a novel robust shadow feature. We evaluate the method on several US data sets and report that it favorably compares with existing techniques. We achieve a recall of 0.86 at a precision of 0.82 on a test set of 143 spinal US images. © 2017 World Federation for Ultrasound in Medicine & Biology","Intra-operative; Machine learning; Orthopedic procedure; Spine; Ultrasound; Ultrasound guidance; Vertebra","Bone; Decision trees; Learning systems; Signal to noise ratio; Speckle; Ultrasonics; Acquisition parameters; Intra-operative; Low signal-to-noise ratio; Random forest classifier; Spine; Ultrasound guidance; Ultrasound imaging; Vertebra; Image segmentation; Article; classification; classifier; human; image analysis; image intensification; image processing; image quality; image segmentation; measurement precision; priority journal; random forest; sample size; two dimensional echocardiography",2-s2.0-85025445758
"Kafaf D.A., Kim D.-K.","A web service-based approach for developing self-adaptive systems",2017,"Computers and Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021809660&doi=10.1016%2fj.compeleceng.2017.06.030&partnerID=40&md5=5a28695aff3b8904d4a43cafbe4661c3","The current development on self-adaptive systems mainly focuses on autonomy and self-containment where decision-making solely depends on the local knowledge base within the system. This limits the evolution of the knowledge base for making more precise decisions. There have been some recent works using the cloud for the knowledge base. However, they suffer from the overhead caused by the communication with the cloud. In this work, we propose a hybrid approach for developing self-adaptive systems using both the local knowledge base in the vehicle and the global knowledge base provided via a web service. The global knowledge base is shared and evolves by multiple vehicles through the web service. We validate the approach using Gazebo, a 3D simulation environment for robotic systems. The results show 96% precision in identifying objects with a viable overhead introduced by the web service and 40% improvement in precision over the traditional approach. © 2017 Elsevier Ltd","k-means; kNN; Machine learning; Robots; Self-adaptation; Web services","Adaptive systems; Knowledge based systems; Learning systems; Robots; Websites; Global knowledge; Hybrid approach; K-means; Local knowledge; Robotic systems; Self adaptation; Self-adaptive system; Traditional approaches; Web services",2-s2.0-85021809660
"Zhao W., Du S., Wang Q., Emery W.J.","Contextually guided very-high-resolution imagery classification with semantic segments",2017,"ISPRS Journal of Photogrammetry and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028705162&doi=10.1016%2fj.isprsjprs.2017.08.011&partnerID=40&md5=5a709832851fea41a1d3f6187b896199","Contextual information, revealing relationships and dependencies between image objects, is one of the most important information for the successful interpretation of very-high-resolution (VHR) remote sensing imagery. Over the last decade, geographic object-based image analysis (GEOBIA) technique has been widely used to first divide images into homogeneous parts, and then to assign semantic labels according to the properties of image segments. However, due to the complexity and heterogeneity of VHR images, segments without semantic labels (i.e., semantic-free segments) generated with low-level features often fail to represent geographic entities (such as building roofs usually be partitioned into chimney/antenna/shadow parts). As a result, it is hard to capture contextual information across geographic entities when using semantic-free segments. In contrast to low-level features, “deep” features can be used to build robust segments with accurate labels (i.e., semantic segments) in order to represent geographic entities at higher levels. Based on these semantic segments, semantic graphs can be constructed to capture contextual information in VHR images. In this paper, semantic segments were first explored with convolutional neural networks (CNN) and a conditional random field (CRF) model was then applied to model the contextual information between semantic segments. Experimental results on two challenging VHR datasets (i.e., the Vaihingen and Beijing scenes) indicate that the proposed method is an improvement over existing image classification techniques in classification performance (overall accuracy ranges from 82% to 96%). © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Contextual information; CRF model; Deep learning; Semantic segmentation; VHR images","Classification (of information); Deep learning; Image segmentation; Neural networks; Random processes; Remote sensing; Semantics; Classification performance; Classification technique; Contextual information; Convolutional neural network; Crf models; Geographic object-based image analysis; Semantic segmentation; VHR images; Image classification; artificial neural network; complexity; image analysis; image classification; image resolution; machine learning; numerical model; remote sensing; segmentation; Beijing [China]; China",2-s2.0-85028705162
"Woodman S., Hiden H., Watson P.","Applications of provenance in performance prediction and data storage optimisation",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011256374&doi=10.1016%2fj.future.2017.01.003&partnerID=40&md5=20fabe69fccfb24a3233e992d7d963ae","Accurate and comprehensive storage of provenance information is a basic requirement for modern scientific computing. A significant effort in recent years has developed robust theories and standards for the representation of these traces across a variety of execution platforms. Whilst these are necessary to enable repeatability they do not exploit the captured information to its full potential. This data is increasingly being captured from applications hosted on Cloud Computing platforms, which offer large scale computing resources without significant up front costs. Medical applications, which generate large datasets are also suited to cloud computing as the practicalities of storing and processing such data locally are becoming increasingly challenging. This paper shows how provenance can be captured from medical applications, stored using a graph database and then used to answer audit questions and enable repeatability. This static provenance will then be combined with performance data to predict future workloads, inform decision makers and reduce latency. Finally, cost models which are based on real world cloud computing costs will be used to determine optimum strategies for data retention over potentially extended periods of time. © 2017 Elsevier B.V.","E-science; Machine learning; Provenance; Workflow","Cloud computing; Computation theory; Costs; Data handling; Decision making; Learning systems; Medical applications; Cloud computing costs; Cloud computing platforms; E-sciences; Execution platforms; Large-scale computing; Performance prediction; Provenance; Workflow; Digital storage",2-s2.0-85011256374
"Jimenez J., Martin A., Uc V., Espinosa A.","Mexican Sign Language Alphanumerical Gestures Recognition using 3D Haar-like Features",2017,"IEEE Latin America Transactions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032640819&doi=10.1109%2fTLA.2017.8071247&partnerID=40&md5=2326d16bce4f2939de924de64aa4efa6","The Mexican Sign Language (LSM) is a language of the deaf Mexican community, which consists of a series of gestural signs articulated by hands and accompanied with facial expressions. The lack of automated systems to translate signs from LSM makes integration of hearing-impaired people to society more difficult. This work presents a new method for LSM alphanumerical signs recognition based on 3D Haar-like features extracted from depth images captured by the Microsoft Kinect sensor. Features are processed with a boosting algorithm. To evaluate performance of our method, we recognized a set of signs from letters and numbers, and compared the results with the use of traditional 2D Haar-like features. Our system is able to recognize static LSM signs with a higher accuracy rate than the one obtained with widely used 2D features. © 2003-2012 IEEE.","3D Haar-like features; Boosting; Gesture recognition; Machine learning; Sign language","Adaptive boosting; Audition; Automation; Learning systems; Automated systems; Boosting; Boosting algorithm; Facial Expressions; Gestures recognition; Haar-like features; Microsoft Kinect sensors; Sign language; Gesture recognition",2-s2.0-85032640819
"Tang Y., Cui H., Wang Q.","Prediction model of the power system frequency using a cross-entropy ensemble algorithm",2017,"Entropy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031903576&doi=10.3390%2fe19100552&partnerID=40&md5=442fc2cc5f32714ccfbde22939cf3cbf","Frequency prediction after a disturbance has received increasing research attention given its substantial value in providing a decision-making foundation in power system emergency control. With the advancing development of machine learning, analysis power systems with machine-learning methods has become completely different from traditional approaches. In this paper, an ensemble algorithm using cross-entropy as a combination strategy is presented to address the trade-off between prediction accuracy and calculation speed. The prediction difficulty caused by inadequate numbers of severe disturbance samples is also overcome by the ensemble model. In the proposed ensemble algorithm, base learners are selected following the principle of diversity, which guarantees the ensemble algorithm-s accuracy. Cross-entropy is applied to evaluate the fitting performance of the base learners and to set the weight coefficient in the ensemble algorithm. Subsequently, an online prediction model based on the algorithm is established that integrates training, prediction and updating. In theWestern System Coordinating Council 9-bus (WSCC 9) system and the Institute of Electrical and Electronics Engineers 39-bus (IEEE 39) system, the algorithm is shown to significantly improve the prediction accuracy in both sample-rich and sample-poor situations, verifying the effectiveness and superiority of the proposed ensemble algorithm. © 2017 by the authors.","Cross-entropy; Ensemble algorithm; Frequency prediction; Machine learning",,2-s2.0-85031903576
"Ahmed A., Shah M.A., Wahid A., Islam S.U., Abbasi M.K., Asghar M.N.","Big data analytics using neural networks for earlier cancer detection",2017,"Journal of Medical Imaging and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030631043&doi=10.1166%2fjmihi.2017.2189&partnerID=40&md5=30c0070c3caa0e0a326eaf0d477b9c27","Big data represents massive amount of datasets having complex structure. These datasets contain the data in a multidimensional form which is difficult in storing, analyzing and visualizing for futuristic results. In these circumstances, artificial intelligence is used in such a way that a machine can learn the data which can be used to make predictions, classifications and reorganizations. There are many techniques for the classification of data but neural network method is most common. This paper discusses how the datasets are selected, trained and classified, using latest technologies, the trained data is used for enterprise, business and healthcare. We determine the relationship among the given inputs and outputs to find out the pattern among datasets using different neural networks. We consider a case study to review a system that is capable for predicting and analyzing the cancer in early stages. We use different machine learning algorithms and evaluate each of them for prediction and accuracy. © 2017 American Scientific Publishers All rights reserved.","AI; Analytics; Big Data; Cancer; Machine Learning; Medical Information; Neural Networks","cancer diagnosis; cancer staging; machine learning; medical information; nervous system; prediction",2-s2.0-85030631043
"Wang W., Xi J., Chong A., Li L.","Driving Style Classification Using a Semisupervised Support Vector Machine",2017,"IEEE Transactions on Human-Machine Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028505671&doi=10.1109%2fTHMS.2017.2736948&partnerID=40&md5=65dc181507707303be07b5b97de6447d","Supervised learning approaches are widely used for driving style classification; however, they often require a large amount of labeled training data, which is usually scarce in a real-world setting. Moreover, it is time-consuming to manually label huge amounts of driving data due to uncertainties of driver behavior and variances among the data analysts. To address this problem, a semisupervised approach, a semisupervised support vector machine (S3VM), is employed to classify drivers into aggressive and normal styles based on a few labeled data points. First, a few data clusters are selected and manually labeled using a k-means clustering method. Then, a specific differentiable surrogate of a loss function is developed, which makes it feasible to use standard optimization tools to solve the nonconvex optimization problem. One of the most popular quasi-Newton algorithms is then used to assign the optimal label to all of the training data. Finally, we compare the S3VM method with a support vector machine method for classifying driving styles from different amounts of labeled data. Experiments show that the S3VM method can improve the classification accuracy by about 10% and reduce the labeling effort by using only a few labeled data clusters among huge amounts of unlabeled data. © 2013 IEEE.","Driving style classification; longitudinal driving behavior; nonconvex optimization; quasi-Newton (QN) methods; semisupervised support vector machine (S3VM)","Behavioral research; Cluster analysis; Consumer behavior; Optimization; Support vector machines; Vectors; Driving styles; Longitudinal driving; Nonconvex optimization; Quasi-Newton methods; Semi-supervised; Classification (of information)",2-s2.0-85028505671
"Kumar A., Kumar R.","Time-frequency analysis and support vector machine in automatic detection of defect from vibration signal of centrifugal pump",2017,"Measurement: Journal of the International Measurement Confederation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019855964&doi=10.1016%2fj.measurement.2017.04.041&partnerID=40&md5=00139bded793ac4da5a4e7a5c0980daf","Centrifugal pumps operate at moderate to high speed. Contamination in the fluid in terms of solid particles and chemically reactive substances causes damage to the impeller, casing, and seals. A defect in bearing due to improper lubrication, adverse loading and manufacturing defect may also affect the performance of the pump. Hence there is a need to develop a reliable procedure for defect identification in the centrifugal pump. A robust automated signal processing algorithm is proposed for the purpose. Features sensitive to defective conditions are extracted from raw signal and scale marginal integration graph. The genetic algorithm (GA) is used to find the optimal parameters of support vector machine (SVM). Using the optimal parameters, training of SVM is carried out for the learning of defective conditions of the pump. After training, features are applied to SVM for the identification of the defective condition of the pump. The performance evaluation of the proposed method is made using receiver operating characteristics graph and is found to be reliable. The overall recognition rate of the proposed method in identifying the specific conditions of the pump is 96.66%. In this work, an attempt is also been made to reduce the training time of GA-SVM model. © 2017 Elsevier Ltd","Centrifugal pump and bearing; Genetic algorithm (GA); Support vector machine (SVM); Wavelet transform (WT)","Centrifugal pumps; Defects; Genetic algorithms; Pumps; Signal processing; Vibration analysis; Wavelet transforms; Automatic Detection; Defect identification; Manufacturing defects; Marginal integration; Optimal parameter; Receiver operating characteristics graphs; Signal processing algorithms; Time frequency analysis; Support vector machines",2-s2.0-85019855964
"Li Q., Li G., Niu W., Cao Y., Chang L., Tan J., Guo L.","Boosting imbalanced data learning with Wiener process oversampling",2017,"Frontiers of Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995481288&doi=10.1007%2fs11704-016-5250-y&partnerID=40&md5=9c322e961049862ed9bff8fa8e463560","Learning from imbalanced data is a challenging task in a wide range of applications, which attracts significant research efforts from machine learning and data mining community. As a natural approach to this issue, oversampling balances the training samples through replicating existing samples or synthesizing new samples. In general, synthesization outperforms replication by supplying additional information on the minority class. However, the additional information needs to follow the same normal distribution of the training set, which further constrains the new samples within the predefined range of training set. In this paper, we present the Wiener process oversampling (WPO) technique that brings the physics phenomena into sample synthesization. WPO constructs a robust decision region by expanding the attribute ranges in training set while keeping the same normal distribution. The satisfactory performance of WPO can be achieved with much lower computing complexity. In addition, by integrating WPO with ensemble learning, the WPOBoost algorithm outperformsmany prevalent imbalance learning solutions. © 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg.","AdaBoost; ensemble learning; imbalanced-data learning; oversampling; Wiener process","Adaptive boosting; Artificial intelligence; Data mining; Learning systems; Normal distribution; Computing complexity; Data mining community; Ensemble learning; Imbalanced data; Natural approaches; Over sampling; Research efforts; Wiener process; Random processes",2-s2.0-84995481288
"Yang L., Li Y., Li Z.","Improved-ELM method for detecting false data attack in smart grid",2017,"International Journal of Electrical Power and Energy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016604148&doi=10.1016%2fj.ijepes.2017.03.011&partnerID=40&md5=a41e6de9e79a36dc3d9ffb00da809c22","Power grid is a complex system which closely links the power generation and power consumer through transmission and distribution networks. With the development of smart grid, smart grid is more open to external communication systems, it also has exposed some problems in the network attacks. A new false data injection attack (called the unobservable attack) that can bypass the traditional BDD and inject random errors into state estimation. We propose an improved extreme learning machine (ELM) for attack detection. The artificial bee colony (ABC) incorporates the thought of differential evolution algorithm (DE) to optimize ELM for improving detection precision. In this paper, Autoencoder is used to reduce the dimensionality of the measurement data, which makes the low-dimensional data information basically and fully represent high-dimensional data. We verify the performance of the proposed method on IEEE bus systems, and prove that the proposed method can effectively detect such unobservable attack. © 2017","Dimension reduction; Extreme learning machine (ELM); False data injection attack; Smart grid","Clustering algorithms; Evolutionary algorithms; Knowledge acquisition; Learning systems; Optimization; Random errors; Smart power grids; Artificial bee colonies (ABC); Differential evolution algorithms; Dimension reduction; External communications; Extreme learning machine; False data injection attacks; High dimensional data; Smart grid; Electric power transmission networks",2-s2.0-85016604148
"Wu S.-J., Pham V.-H., Nguyen T.-N.","Two-phase optimization for support vectors and parameter selection of support vector machines: Two-class classification",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020757502&doi=10.1016%2fj.asoc.2017.05.021&partnerID=40&md5=9833924cb4487a6c91c1306169bb39b0","Support vector machines (SVMs) are one of the most popular classification tools and show the most potential to address under-sampled noisy data (a large number of features and a relatively small number of samples). However, the computational cost is too expensive, even for modern-scale samples, and the performance largely depends on the proper setting of parameters. As the data scale increases, the improvement in speed becomes increasingly challenging. As the dimension (feature number) largely increases while the sample size remains small, the avoidance of overfitting becomes a significant challenge. In this study, we propose a two-phase sequential minimal optimization (TSMO) to largely reduce the training cost for large-scale data (tested with 3186–70,000-sample datasets) and a two-phased-in differential-learning particle swarm optimization (tDPSO) to ensure the accuracy for under-sampled data (tested with 2000–24481-feature datasets). Because the purpose of training SVMs is to identify support vectors that denote a hyperplane, TSMO is developed to quickly select support vector candidates from the entire dataset and then identify support vectors from those candidates. In this manner, the computational burden is largely reduced (a 29.4%–65.3% reduction rate). The proposed tDPSO uses topology variation and differential learning to solve PSO's premature convergence issue. Population diversity is ensured through dynamic topology until a ring connection is achieved (topology-variation phases). Further, particles initiate chemo-type simulated-annealing operations, and the global-best particle takes a two-turn diversion in response to stagnation (event-induced phases). The proposed tDPSO-embedded SVMs were tested with several under-sampled noisy cancer datasets and showed superior performance over various methods, even those methods with feature selection for the preprocessing of data. © 2017 Elsevier B.V.","Classification; Mimetic computation; Systems biology; Working set selection","Classification (of information); Optimization; Particle swarm optimization (PSO); Simulated annealing; Topology; Vectors; Pre-mature convergences; Pre-processing of data; Sequential minimal optimization; Setting of parameters; Support vector machine (SVMs); Systems biology; Two-phase optimizations; Working set selection; Support vector machines",2-s2.0-85020757502
"Alitto A., Gatta R., Vanneste B., Vallati M., Meldolesi E., Damiani A., Lanzotti V., Mattiucci G., Frascino V., Masciocchi C., Catucci F., Dekker A., Lambin P., Valentini V., Mantini G.","PRODIGE: PRediction models in prOstate cancer for personalized meDIcine challenGE",2017,"Future Oncology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032625902&doi=10.2217%2ffon-2017-0142&partnerID=40&md5=04448e26dd9cfbb03d486505b054f24d","Aim: Identifying the best care for a patient can be extremely challenging. To support the creation of multifactorial Decision Support Systems (DSSs), we propose an Umbrella Protocol, focusing on prostate cancer. Materials & methods: The PRODIGE project consisted of a workflow for standardizing data, and procedures, to create a consistent dataset useful to elaborate DSSs. Techniques from classical statistics and machine learning will be adopted. The general protocol accepted by our Ethical Committee can be downloaded from cancerdata.org. Results: A standardized knowledge sharing process has been implemented by using a semi-formal ontology for the representation of relevant clinical variables. Conclusion: The development of DSSs, based on standardized knowledge, could be a tool to achieve a personalized decision-making. © 2017 2017 Future Medicine Ltd.","Decision Support System; individualized medicine; large database; machine learning; ontology; predictive model",,2-s2.0-85032625902
"Bernard E., Jiao Y., Scornet E., Stoven V., Walter T., Vert J.-P.","Kernel Multitask Regression for Toxicogenetics",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031110340&doi=10.1002%2fminf.201700053&partnerID=40&md5=8b55b4ebaa04d6cd2c673d6f8c7d8b9f","The development of high-throughput in vitro assays to study quantitatively the toxicity of chemical compounds on genetically characterized human-derived cell lines paves the way to predictive toxicogenetics, where one would be able to predict the toxicity of any particular compound on any particular individual. In this paper we present a machine learning-based approach for that purpose, kernel multitask regression (KMR), which combines chemical characterizations of molecular compounds with genetic and transcriptomic characterizations of cell lines to predict the toxicity of a given compound on a given cell line. We demonstrate the relevance of the method on the recent DREAM8 Toxicogenetics challenge, where it ranked among the best state-of-the-art models, and discuss the importance of choosing good descriptors for cell lines and chemicals. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","kernel methods; machine learning; multitask regression; Toxicogenetics","Article; cytotoxicity; genetic analysis; human; kernel method; machine learning; priority journal; regression analysis; toxicogenetics; transcriptomics",2-s2.0-85031110340
"Liu Z., Wu M., Cao W., Chen L., Xu J., Zhang R., Zhou M., Mao J.","A facial expression emotion recognition based human-robot interaction system",2017,"IEEE/CAA Journal of Automatica Sinica",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029781916&doi=10.1109%2fJAS.2017.7510622&partnerID=40&md5=0f28d1f25108d503095a8019cca33adb","A facial expression emotion recognition based human-robot interaction (FEER-HRI) system is proposed, for which a four-layer system framework is designed. The FEER-HRI system enables the robots not only to recognize human emotions, but also to generate facial expression for adapting to human emotions. A facial emotion recognition method based on 2D-Gabor, uniform local binary pattern (LBP) operator, and multiclass extreme learning machine (ELM) classifier is presented, which is applied to real-time facial expression recognition for robots. Facial expressions of robots are represented by simple cartoon symbols and displayed by a LED screen equipped in the robots, which can be easily understood by human. Four scenarios, i.e., guiding, entertainment, home service and scene simulation are performed in the human-robot interaction experiment, in which smooth communication is realized by facial expression recognition of humans and facial expression generation of robots within 2 seconds. As a few prospective applications, the FEER-HRI system can be applied in home service, smart home, safe driving, and so on. © 2014 Chinese Association of Automation.","Emotion generation; facial expression emotion recognition (FEER); human-robot interaction (HRI); system design","Automation; Face recognition; Human computer interaction; Intelligent buildings; Learning systems; Light emitting diodes; Machine design; Man machine systems; Robots; Speech recognition; Systems analysis; Emotion generation; Emotion recognition; Extreme learning machine; Facial expression generation; Facial expression recognition; Human robot Interaction (HRI); Prospective applications; Uniform local binary patterns; Human robot interaction",2-s2.0-85029781916
"Zhang Y., Xu Y., Dong Z.Y., Xu Z., Wong K.P.","Intelligent Early Warning of Power System Dynamic Insecurity Risk: Toward Optimal Accuracy-Earliness Tradeoff",2017,"IEEE Transactions on Industrial Informatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031666157&doi=10.1109%2fTII.2017.2676879&partnerID=40&md5=7deda9f2dc36ce9d60a99c6456a09219","Dynamic insecurity risk of a power system has been increasingly concerned due to the integration of stochastic renewable power sources (such as wind and solar power) and complicated demand response. In this paper, an intelligent early-warning system to achieve reliable online detection of risky operating conditions is proposed. The proposed intelligent system (IS) consists of an ensemble learning model based on extreme learning machine (ELM) and a decision-making process under a multiobjective programming framework. Taking an ensemble form, the randomness existing in individual ELM training is generalized and reliable classification results can be obtained. The decision making is designed for ELM ensemble whose parameters are optimized to search for the optimal tradeoff between the warning accuracy and the warning earliness of the proposed IS. The compromise solution turns out to significantly speed up the overall computation with an acceptable sacrifice in the accuracy (e.g., from 100% to 99.9%). More importantly, the proposed IS can provide multiple and switchable performances to the operators in order to satisfy different local dynamic security assessment requirements. © 2005-2012 IEEE.","Dynamic insecurity risk; early warning; extreme learning machine (ELM); intelligent system (IS); multiobjective programming (MOP)","Computer programming; Decision making; Intelligent systems; Knowledge acquisition; Multiobjective optimization; Solar energy; Stochastic systems; Classification results; Decision making process; Early warning; Early Warning System; Extreme learning machine; Multiobjective programming; Power system dynamics; Wind and solar power; Learning systems",2-s2.0-85031666157
"Zhang A., Wang K.C.P., Li B., Yang E., Dai X., Peng Y., Fei Y., Liu Y., Li J.Q., Chen C.","Automated Pixel-Level Pavement Crack Detection on 3D Asphalt Surfaces Using a Deep-Learning Network",2017,"Computer-Aided Civil and Infrastructure Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029008687&doi=10.1111%2fmice.12297&partnerID=40&md5=e8547a71c68aa6e32bd689bc4c505e0e","The CrackNet, an efficient architecture based on the Convolutional Neural Network (CNN), is proposed in this article for automated pavement crack detection on 3D asphalt surfaces with explicit objective of pixel-perfect accuracy. Unlike the commonly used CNN, CrackNet does not have any pooling layers which downsize the outputs of previous layers. CrackNet fundamentally ensures pixel-perfect accuracy using the newly developed technique of invariant image width and height through all layers. CrackNet consists of five layers and includes more than one million parameters that are trained in the learning process. The input data of the CrackNet are feature maps generated by the feature extractor using the proposed line filters with various orientations, widths, and lengths. The output of CrackNet is the set of predicted class scores for all pixels. The hidden layers of CrackNet are convolutional layers and fully connected layers. CrackNet is trained with 1,800 3D pavement images and is then demonstrated to be successful in detecting cracks under various conditions using another set of 200 3D pavement images. The experiment using the 200 testing 3D images showed that CrackNet can achieve high Precision (90.13%), Recall (87.63%) and F-measure (88.86%) simultaneously. Compared with recently developed crack detection methods based on traditional machine learning and imaging algorithms, the CrackNet significantly outperforms the traditional approaches in terms of F-measure. Using parallel computing techniques, CrackNet is programmed to be efficiently used in conjunction with the data collection software. © 2017 Computer-Aided Civil and Infrastructure Engineering",,"Asphalt; Convolution; Cracks; Deep learning; Learning systems; Neural networks; Pavements; Pixels; Convolutional neural network; Data collection software; Detection methods; Efficient architecture; Imaging algorithm; Parallel computing techniques; Pavement crack detection; Traditional approaches; Crack detection; asphalt; crack; detection method; imaging method; Internet; learning; parameterization; pavement; pixel",2-s2.0-85029008687
"Bexiga F., Rodrigues D., Guerra R., Brázio A., Balegas T., Cavaco A.M., Antunes M.D., Valente de Oliveira J.","A TSS classification study of ‘Rocha’ pear (Pyrus communis L.) based on non-invasive visible/near infra-red reflectance spectra",2017,"Postharvest Biology and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020234235&doi=10.1016%2fj.postharvbio.2017.05.014&partnerID=40&md5=7311b5ad891c4e6d092f478b9e3329ef","The study focuses on the application of machine learning techniques for classifying the internal quality of ‘Rocha’ Pear (Pyrus communis L.), i.e., the total soluble solids (TSS), using the non-invasive technique of visible/near infra-red reflectance spectroscopy. Six representative classifiers were evaluated under realistic experimental conditions. The classifiers include representatives of classic parametric (logistic and multiple linear regression), non-parametric distance based methods (K-nearest neighbors), correlation-based (partial least squares), ensemble methods (random forests) and maximum margin classifiers (support vector machines). The classifiers were assessed against metrics such as accuracy, Cohen's Kappa, F-Measure, and the area under the precision-recall curve (AUC) in a 10 × 10-fold cross-validation plan. For result analysis non-parametric statistical test of hypotheses were employed. A total of 4880 fruit samples from different origins, maturation states, and harvest years were considered. The main conclusion is that the maximum margin classifier outperforms all the others studied ones, including the commonly used partial least squares. The conclusion holds for both a reflectance spectrum with 1024 features and for a 128 subsample of these. An estimate of the out-of-sample performance for the best classifier is also provided. © 2017 Elsevier B.V.","Classification; Data analysis; Diffuse reflectance spectroscopy; Machine learning; Non-invasive sensor data; TSS",,2-s2.0-85020234235
"Rose S., Bergquist S.L., Layton T.J.","Computational health economics for identification of unprofitable health care enrollees",2017,"Biostatistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032451850&doi=10.1093%2fbiostatistics%2fkxx012&partnerID=40&md5=1218e17d8a4d10495bfdce05f2dcdd30","Health insurers may attempt to design their health plans to attract profitable enrollees while deterring unprofitable ones. Such insurerswould not be delivering socially efficient levels of care by providing health plans that maximize societal benefit, but rather intentionally distorting plan benefits to avoid high-cost enrollees, potentially to the detriment of health and efficiency. In this work, we focus on a specific component of health plan design at risk for health insurer distortion in the Health Insurance Marketplaces: the prescription drug formulary.We introduce an ensembled machine learning function to determine whether drug utilization variables are predictive of a new measure of enrollee unprofitability we derive, and thus vulnerable to distortions by insurers. Our implementation also contains a unique application-specific variable selection tool. This study demonstrates that super learning is effective in extracting the relevant signal for this prediction problem, and that a small number of drug variables can be used to identify unprofitable enrollees. The results are both encouraging and concerning. While risk adjustment appears to have been reasonably successful at weakening the relationship between therapeutic-class-specific drug utilization and unprofitability, some classes remain predictive of insurer losses. The vulnerable enrollees whose prescription drug regimens include drugs in these classes may need special protection from regulators in health insurance market design. © The Author 2017. Published by Oxford University Press. All rights reserved.","Classification and prediction; Ensembles; Machine learning; Statistical methods in health economics; Variable selection",,2-s2.0-85032451850
"Jahedi A., Nasamran C.A., Faires B., Fan J., Müller R.-A.","Distributed Intrinsic Functional Connectivity Patterns Predict Diagnostic Status in Large Autism Cohort",2017,"Brain Connectivity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031719959&doi=10.1089%2fbrain.2017.0496&partnerID=40&md5=ff5a8cfed5c8372dd3e6974d74af97eb","Diagnosis of autism spectrum disorder (ASD) currently relies on behavioral observations because brain markers are unknown. Machine learning approaches can identify patterns in imaging data that predict diagnostic status, but most studies using functional connectivity MRI (fcMRI) data achieved only modest accuracies of 60-80%. We used conditional random forest (CRF), an ensemble learning technique protected against bias from feature correlation (which exists in fcMRI matrices). We selected 252 low-motion resting-state functional MRI scans from the Autism Brain Imaging Data Exchange, including 126 typically developing (TD) and 126 ASD participants, matched for age, nonverbal IQ, and head motion. A matrix of functional connectivities between 220 functionally defined regions of interest was used for diagnostic classification. In several runs, we achieved accuracies of 92-99% for classifiers with >300 features (most informative connections). Features, including pericentral somatosensory and motor regions, were disproportionately informative. Findings differed partially from a previous study in the same sample that used feature selection with random forest (which is biased by feature correlations). External validation in a smaller in-house data set, however, achieved only 67-71% accuracy. The large number of features in optimal models can be attributed to etiological heterogeneity under the clinical ASD umbrella. Lower accuracy in external validation is expected due to differences in unknown composition of ASD variants across samples. High accuracy in the main data set is unlikely due to noise overfitting, but rather indicates optimized characterization of a given cohort. © Copyright 2017, Mary Ann Liebert, Inc..","autism spectrum disorder; conditional random forest; diagnostic prediction; intrinsic functional connectivity; machine learning; resting-state fMRI","adolescent; adult; Article; autism; child; conditional random forest; controlled study; default mode network; diagnostic accuracy; diagnostic test accuracy study; female; functional connectivity; functional magnetic resonance imaging; functional neuroimaging; human; intelligence quotient; major clinical study; male; priority journal; random forest; salience network; sensitivity and specificity",2-s2.0-85031719959
"Dehouche N.","On evaluating the quality of rule-based classification systems",2017,"ICIC Express Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030100427&partnerID=40&md5=a43d618a8acb38cd5ec34a085c0fced3","Two indicators are classically used to evaluate the quality of rule-based classification systems: predictive accuracy, i.e., the system’s ability to successfully reproduce learning data and coverage, i.e., the proportion of possible cases for which the logical rules constituting the system apply. In this work, we claim that these two indicators may be insufficient, and additional measures of quality may need to be developed. We theoretically show that classification systems presenting “good” predictive accuracy and coverage can, nonetheless, be trivially improved and illustrate this proposition with examples. To conceptualize our main claim, we characterize a property of reducibility. A classification system is said to be reducible, if and only if, its constituent rules can be replaced by a subset of their elementary conditions, while preserving the quality of the system. We derive a time-efficient constructive algorithm to test this property and to improve a system’s predictive accuracy and coverage in case of a positive response. Furthermore, we provide a set of sufficient conditions that can be used to detect non-reducibility and thus validate rule-based classification systems. We use the proposed approach to evaluate a previously published work applied to a public dataset pertaining to the business bankruptcy prediction, using three popular machine learning approaches (namely genetic algorithms, inductive learning and neural networks). The results of this application support our main claim. We conclude this paper by suggesting that a classification system’s ability to clarify trade-offs between attributes should be measured, and used as an additional performance indicator. A possible further development of this work consists in developing such an indicator. © 2017.","Bankruptcy prediction; Classification; Expert systems; Machine learning",,2-s2.0-85030100427
"Pernek I., Ferscha A.","A survey of context recognition in surgery",2017,"Medical and Biological Engineering and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022003988&doi=10.1007%2fs11517-017-1670-6&partnerID=40&md5=c5efebf613c0904d4b2680df2e76d62a","With the introduction of operating rooms of the future context awareness has gained importance in the surgical environment. This paper organizes and reviews different approaches for recognition of context in surgery. Major electronic research databases were queried to obtain relevant publications submitted between the years 2010 and 2015. Three different types of context were identified: (i) the surgical workflow context, (ii) surgeon’s cognitive and (iii) technical state context. A total of 52 relevant studies were identified and grouped based on the type of context detected and sensors used. Different approaches were summarized to provide recommendations for future research. There is still room for improvement in terms of methods used and evaluations performed. Machine learning should be used more extensively to uncover hidden relationships between different properties of the surgeon’s state, particularly when performing cognitive context recognition. Furthermore, validation protocols should be improved by performing more evaluations in situ and with a higher number of unique participants. The paper also provides a structured outline of recent context recognition methods to facilitate development of new generation context-aware surgical support systems. © 2017, International Federation for Medical and Biological Engineering.","Cognitive state; Context recognition; Surgery; Surgical skill; Surgical workflow","Transplantation (surgical); Cognitive state; Context recognition; Context- awareness; Research database; Surgical environment; Surgical skill; Surgical workflow; Validation protocols; Surgery; Article; cognition; evaluation study; futurology; galvanic skin response sensor; heart rate; human; hybrid; kinematics; machine learning; priority journal; publication; recognition; sensor; surgeon; surgery; total quality management; validation process; videorecording; workflow",2-s2.0-85022003988
"Jing L., Shen C., Yang L., Yu J., Ng M.K.","Multi-Label Classification by Semi-Supervised Singular Value Decomposition",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023764628&doi=10.1109%2fTIP.2017.2719939&partnerID=40&md5=db50d1dd2ed24668bc3930fb106b9632","Multi-label problems arise in various domains, including automatic multimedia data categorization, and have generated significant interest in computer vision and machine learning community. However, existing methods do not adequately address two key challenges: exploiting correlations between labels and making up for the lack of labelled data or even missing labelled data. In this paper, we proposed to use a semi-supervised singular value decomposition (SVD) to handle these two challenges. The proposed model takes advantage of the nuclear norm regularization on the SVD to effectively capture the label correlations. Meanwhile, it introduces manifold regularization on mapping to capture the intrinsic structure among data, which provides a good way to reduce the required labelled data with improving the classification performance. Furthermore, we designed an efficient algorithm to solve the proposed model based on the alternating direction method of multipliers, and thus, it can efficiently deal with large-scale data sets. Experimental results for synthetic and real-world multimedia data sets demonstrate that the proposed method can exploit the label correlations and obtain promising and better label prediction results than the state-of-the-art methods. © 1992-2012 IEEE.","Image classification; manifold regularization; multi-label; nuclear norm regularization; singular value decomposition","Correlation methods; Fasteners; Logistics; Multimedia systems; Optimization; Alternating direction method of multipliers; Classification performance; Machine learning communities; Multi label classification; Multi-media communications; Nuclear norm regularizations; State-of-the-art methods; Training data; Singular value decomposition",2-s2.0-85023764628
"Aichele S., Rabbitt P., Ghisletta P.","Illness and intelligence are comparatively strong predictors of individual differences in depressive symptoms following middle age",2017,"Aging and Mental Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032462866&doi=10.1080%2f13607863.2017.1394440&partnerID=40&md5=4bd84d9c6da5b8ef6b2a4f812b5a5bde","Objective: We compared the importance of socio-demographic, lifestyle, health, and multiple cognitive measures for predicting individual differences in depressive symptoms in later adulthood. Method: Data came from 6203 community-dwelling older adults (age 41–93 years at study entry) from the United Kingdom. Predictors (36 in total) were assessed up to four times across a period of approximately 12 years. Depressive symptoms were measured with the Geriatric Depression Scale. Statistical methods included multiple imputation (for missing data), random forest analysis (a machine learning approach), and multivariate regression. Results: On average, depressive symptoms increased gradually following middle age and appeared to accelerate in later life. Individual differences in depressive symptoms were most strongly associated with differences in combined symptoms of physical illness (positive relation) and fluid intelligence (negative relation). The strength of association between depressive symptoms and fluid intelligence was unaffected by differences in health status within a subsample of chronically depressed individuals. Conclusion: Joint consideration of general health status and fluid intelligence may facilitate prediction of depressive symptoms severity during later life and may also serve to identify sub-populations of community-dwelling elders at risk for chronic depression. © 2017 Informa UK Limited, trading as Taylor & Francis Group","aging; cognition; Depression; fluid intelligence; machine learning",,2-s2.0-85032462866
"Creton B.","Chemoinformatics at IFP Energies Nouvelles: Applications in the Fields of Energy, Transport, and Environment",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018577078&doi=10.1002%2fminf.201700028&partnerID=40&md5=8ce542ee0146ddd414bcd89aac6c993c","The objective of the present paper is to summarize chemoinformatics based research, and more precisely, the development of quantitative structure property relationships performed at IFP Energies nouvelles (IFPEN) during the last decade. A special focus is proposed on research activities performed in the “Thermodynamics and Molecular Simulation” department, i. e. the use of multiscale molecular simulation methods in responses to projects. Molecular simulation techniques can be envisaged to supplement dataset when experimental information lacks, thus the review includes a section dedicated to molecular simulation codes, development of intermolecular potentials, and some of their possible applications. Know-how and feedback from our experiences in terms of machine learning application for thermophysical property predictions are included in a section dealing with methodological aspects. The generic character of chemoinformatics is emphasized through applications in the fields of energy, transport, and environment, with illustrations for three IFPEN business units: “Transports”, “Energy Resources”, and “Processes”. More precisely, the review focus on different challenges such as the prediction of properties for alternative fuels, the prediction of fuel compatibility with polymeric materials, the prediction of properties for surfactants usable in chemical enhanced oil recovery, and the prediction of guest-host interactions between gases and nanoporous materials in the frame of carbon dioxide capture or gas separation activities. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","Data mining; Fluids; Machine learning; Nanoporous materials; Property","biofuel; carbon dioxide; polymer; surfactant; carbon footprint; chemoinformatics; energy resource; environment; feedback system; information processing; machine learning; mathematical model; molecular dynamics; oil field; physical chemistry; priority journal; quantitative structure property relation; Review; thermodynamics",2-s2.0-85018577078
"Hoshino E., Hayashi K., Suzuki M., Obatake M., Urayama K.Y., Nakano S., Taura Y., Nio M., Takahashi O.","An iPhone application using a novel stool color detection algorithm for biliary atresia screening",2017,"Pediatric Surgery International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027718424&doi=10.1007%2fs00383-017-4146-8&partnerID=40&md5=749fb954ead0661099c53b185b1a6caa","Background: The stool color card has been the primary tool for identifying acholic stools in infants with biliary atresia (BA), in several countries. However, BA stools are not always acholic, as obliteration of the bile duct occurs gradually. This study aims to introduce Baby Poop (Baby unchi in Japanese), a free iPhone application, employing a detection algorithm to capture subtle differences in colors, even with non-acholic BA stools. Methods: The application is designed for use by caregivers of infants aged approximately 2 weeks–1 month. Baseline analysis to determine optimal color parameters predicting BA stools was performed using logistic regression (n = 50). Pattern recognition and machine learning processes were performed using 30 BA and 34 non-BA images. Additional 5 BA and 35 non-BA pictures were used to test accuracy. Results: Hue, saturation, and value (HSV) were the preferred parameter for BA stool identification. A sensitivity and specificity were 100% (95% confidence interval 0.48–1.00 and 0.90–1.00, respectively) even among a collection of visually non-acholic, i.e., pigmented BA stools and relatively pale-colored non-BA stools. Conclusions: Results suggest that an iPhone mobile application integrated with a detection algorithm is an effective and convenient modality for early detection of BA, and potentially for other related diseases. © 2017, Springer-Verlag GmbH Germany.","Biliary atresia; Detection algorithm; iPhone application; Screening; Stool color","algorithm; Article; bile duct atresia; caregiver; feces analysis; feces color; human; infant; machine learning; mobile application; pattern recognition; priority journal; screening",2-s2.0-85027718424
"Li P., Schloss B., Follmer D.J.","Speaking two “Languages” in America: A semantic space analysis of how presidential candidates and their supporters represent abstract political concepts differently",2017,"Behavior Research Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024484869&doi=10.3758%2fs13428-017-0931-5&partnerID=40&md5=9211784c727633a70cffb6764b845663","In this article we report a computational semantic analysis of the presidential candidates’ speeches in the two major political parties in the USA. In Study One, we modeled the political semantic spaces as a function of party, candidate, and time of election, and findings revealed patterns of differences in the semantic representation of key political concepts and the changing landscapes in which the presidential candidates align or misalign with their parties in terms of the representation and organization of politically central concepts. Our models further showed that the 2016 US presidential nominees had distinct conceptual representations from those of previous election years, and these patterns did not necessarily align with their respective political parties’ average representation of the key political concepts. In Study Two, structural equation modeling demonstrated that reported political engagement among voters differentially predicted reported likelihoods of voting for Clinton versus Trump in the 2016 presidential election. Study Three indicated that Republicans and Democrats showed distinct, systematic word association patterns for the same concepts/terms, which could be reliably distinguished using machine learning methods. These studies suggest that given an individual’s political beliefs, we can make reliable predictions about how they understand words, and given how an individual understands those same words, we can also predict an individual’s political beliefs. Our study provides a bridge between semantic space models and abstract representations of political concepts on the one hand, and the representations of political concepts and citizens’ voting behavior on the other. © 2017, Psychonomic Society, Inc.","Computational modeling; Dynamic change; Predictive modeling; Representation of political concepts; Semantic space; Word association","election; landscape; machine learning; prediction; speech; structural equation modeling",2-s2.0-85024484869
"Yu J., Hou B., Lelyakin A., Xu Z., Jordan T.","Gas detonation cell width prediction model based on support vector regression",2017,"Nuclear Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028315063&doi=10.1016%2fj.net.2017.06.014&partnerID=40&md5=66098d79112c337d14cd41dea42d0f71","Detonation cell width is an important parameter in hydrogen explosion assessments. The experimental data on gas detonation are statistically analyzed to establish a universal method to numerically predict detonation cell widths. It is commonly understood that detonation cell width, λ is highly correlated with the characteristic reaction zone width, δ. Classical parametric regression methods were widely applied in earlier research to build an explicit semiempirical correlation for the ratio of λ/δ. The obtained correlations formulate the dependency of the ratio λ/δ on a dimensionless effective chemical activation energy and a dimensionless temperature of the gas mixture. In this paper, support vector regression (SVR), which is based on nonparametric machine learning, is applied to achieve functions with better fitness to experimental data and more accurate predictions. Furthermore, a third parameter, dimensionless pressure, is considered as an additional independent variable. It is found that three-parameter SVR can significantly improve the performance of the fitting function. Meanwhile, SVR also provides better adaptability and the model functions can be easily renewed when experimental database is updated or new regression parameters are considered. © 2017","Detonation Cell Width; Hydrogen Safety; Machine Learning; Support Vector Regression",,2-s2.0-85028315063
"Escobar G.J., Baker J.M., Kipnis P., Greene J.D., Mast T.C., Gupta S.B., Cossrow N., Mehta V., Liu V., Dubberke E.R.","Prediction of recurrent clostridium difficile infection using comprehensive electronic medical records in an integrated healthcare delivery system",2017,"Infection Control and Hospital Epidemiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029857162&doi=10.1017%2fice.2017.176&partnerID=40&md5=7a0a90b0091c449dcb735b0b5282254f","BACKGROUND Predicting recurrent Clostridium difficile infection (rCDI) remains difficult. METHODS. We employed a retrospective cohort design. Granular electronic medical record (EMR) data had been collected from patients hospitalized at 21 Kaiser Permanente Northern California hospitals. The derivation dataset (2007-2013) included data from 9,386 patients who experienced incident CDI (iCDI) and 1,311 who experienced their first CDI recurrences (rCDI). The validation dataset (2014) included data from 1,865 patients who experienced incident CDI and 144 who experienced rCDI. Using multiple techniques, including machine learning, we evaluated more than 150 potential predictors. Our final analyses evaluated 3 models with varying degrees of complexity and 1 previously published model. RESULTS Despite having a large multicenter cohort and access to granular EMR data (eg, vital signs, and laboratory test results), none of the models discriminated well (c statistics, 0.591-0.605), had good calibration, or had good explanatory power. CONCLUSIONS Our ability to predict rCDI remains limited. Given currently available EMR technology, improvements in prediction will require incorporating new variables because currently available data elements lack adequate explanatory power. Infect Control Hosp Epidemiol 2017;38:1196-1203. © 2017 by The Society for Healthcare Epidemiology of America. All rights reserved.",,"antibiotic agent; bilirubin; creatinine; lactic acid; quinoline derived antiinfective agent; vancomycin; access to information; age; aged; arterial pH; Article; body temperature; calibration; California; clinical evaluation; Clostridium difficile infection; cohort analysis; COPS2 score; critically ill patient; dependent variable; diagnostic test accuracy study; disease severity assessment; electronic medical record; explanatory variable; female; follow up; gastrointestinal surgery; high risk patient; hospital admission; hospital patient; human; immune deficiency; incidental finding; infection risk; integrated health care system; intensive care unit; Laboratory based Acute Physiology Score version 2; laboratory test; length of stay; leukocyte count; machine learning; major clinical study; male; nursing home; personal experience; predictive value; predictor variable; recurrence risk; recurrent clostridium difficile infection; recurrent infection; retrospective study; sensitivity and specificity; urea nitrogen blood level; validation study; vital sign",2-s2.0-85029857162
"Redman J.S., Natarajan Y., Hou J.K., Wang J., Hanif M., Feng H., Kramer J.R., Desiderio R., Xu H., El-Serag H.B., Kanwal F.","Accurate Identification of Fatty Liver Disease in Data Warehouse Utilizing Natural Language Processing",2017,"Digestive Diseases and Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028776195&doi=10.1007%2fs10620-017-4721-9&partnerID=40&md5=cd1a17ab8124d0ade04e79c689cb2c52","Introduction: Natural language processing is a powerful technique of machine learning capable of maximizing data extraction from complex electronic medical records. Methods: We utilized this technique to develop algorithms capable of “reading” full-text radiology reports to accurately identify the presence of fatty liver disease. Abdominal ultrasound, computerized tomography, and magnetic resonance imaging reports were retrieved from the Veterans Affairs Corporate Data Warehouse from a random national sample of 652 patients. Radiographic fatty liver disease was determined by manual review by two physicians and verified with an expert radiologist. A split validation method was utilized for algorithm development. Results: For all three imaging modalities, the algorithms could identify fatty liver disease with >90% recall and precision, with F-measures >90%. Discussion: These algorithms could be used to rapidly screen patient records to establish a large cohort to facilitate epidemiological and clinical studies and examine the clinic course and outcomes of patients with radiographic hepatic steatosis. © 2017, Springer Science+Business Media, LLC (Outside the USA).","Electronic health records; Epidemiology; Fatty liver; Natural language processing; Nonalcoholic fatty liver disease; Triglycerides","algorithm; Article; computer assisted tomography; data base; disease classification; echography; electronic medical record; fatty liver; human; machine learning; major clinical study; natural language processing; nonalcoholic fatty liver; nuclear magnetic resonance imaging; priority journal; radiologist; data mining; diagnostic imaging; echography; electronic health record; factual database; fatty liver; government; nuclear magnetic resonance imaging; predictive value; procedures; prognosis; United States; veterans health; x-ray computed tomography; Algorithms; Data Mining; Databases, Factual; Electronic Health Records; Fatty Liver; Humans; Magnetic Resonance Imaging; Natural Language Processing; Predictive Value of Tests; Prognosis; Tomography, X-Ray Computed; Ultrasonography; United States; United States Department of Veterans Affairs; Veterans Health",2-s2.0-85028776195
"Hajek P., Henriques R.","Modelling innovation performance of European regions using multi-output neural networks",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030244751&doi=10.1371%2fjournal.pone.0185755&partnerID=40&md5=d7ddf72060afd9a8b49872700416568b","Regional innovation performance is an important indicator for decision-making regarding the implementation of policies intended to support innovation. However, patterns in regional innovation structures are becoming increasingly diverse, complex and nonlinear. To address these issues, this study aims to develop a model based on a multi-output neural network. Both intra- and inter-regional determinants of innovation performance are empirically investigated using data from the 4th and 5th Community Innovation Surveys of NUTS 2 (Nomenclature of Territorial Units for Statistics) regions. The results suggest that specific innovation strategies must be developed based on the current state of input attributes in the region. Thus, it is possible to develop appropriate strategies and targeted interventions to improve regional innovation performance. We demonstrate that support of entrepreneurship is an effective instrument of innovation policy. We also provide empirical support that both business and government R&amp;D activity have a sigmoidal effect, implying that the most effective R&amp;D support should be directed to regions with below-average and average R&amp;D activity. We further show that the multi-output neural network outperforms traditional statistical and machine learning regression models. In general, therefore, it seems that the proposed model can effectively reflect both the multiple-output nature of innovation performance and the interdependency of the output attributes. © 2017 Hajek, Henriques. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"clinical article; entrepreneurship; government; human; machine learning; nervous system; nomenclature; statistics; decision making; Europe; organization; social class; Decision Making; Europe; Organizational Innovation; Social Class",2-s2.0-85030244751
"Dente C.J., Bradley M., Schobel S., Gaucher B., Buchman T., Kirk A.D., Elster E.","Towards precision medicine: Accurate predictive modeling of infectious complications in combat casualties",2017,"Journal of Trauma and Acute Care Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019546274&doi=10.1097%2fTA.0000000000001596&partnerID=40&md5=90582d1570d14ed4708abbec8c54ed9a","BACKGROUND The biomarker profile of trauma patients may allow for the creation of models to assist bedside decision making and prediction of complications. We sought to determine the utility of modeling in the prediction of bacteremia and pneumonia in combat casualties. METHODS This is a prospective, observational trial of patients with complex wounds treated at Walter Reed National Military Medical Center (2007-2012). Tissue, serum, and wound effluent samples were collected during operative interventions until wound closure. Clinical, biomarker, and outcome data were used in machine learning algorithms to develop models predicting bacteremia or pneumonia. Modeling was performed on the first operative washout to maximize predictive benefit. Variable selection of dataset variables was performed and the best-fitting Bayesian belief network (BBN), using Bayesian information criterion (BIC), was selected for predictive modeling. Random forest was performed using variables from BBN step. Model performance was evaluated using area under the receiver operating characteristic curve (AUC) analysis. RESULTS Seventy-three patients (mean age 23, mean Injury Severity Score 25) were enrolled. Patients required a median of 3 (2-13) operations. The incidence of bacteremia and pneumonia was 22% and 12%, respectively. Best-fitting variable selected BBNs were maximum-minimum parents and children (MMPC) for both bacteremia (BIC-24948) and pneumonia (BIC-17886). Full variable and MMPC random forest models AUC were 0.721 and 0.834, respectively, for bacteremia and 0.809 and 0.856, respectively, for pneumonia. CONCLUSIONS We identified a profile predictive of bacteremia and pneumonia in combat casualties. This has important clinical implications and should be validated in the civilian trauma population. This and similar tools will allow for increasing precision in the management of critically ill and injured patients. LEVEL OF EVIDENCE Prognostic, level III. Copyright © 2017 Wolters Kluwer Health, Inc. All rights reserved.","bacteremia; clinical decision support; pneumonia; Prediction","antibiotic agent; biological marker; interleukin 2 receptor; biological marker; antibiotic therapy; bacteremia; battle injury; child; Conference Paper; Glasgow coma scale; human; infectious complication; injury scale; injury severity; length of stay; machine learning; major clinical study; observational study; personalized medicine; pleura effusion; pneumonia; priority journal; prospective study; random forest; traumatic brain injury; algorithm; APACHE; bacteremia; Bayes theorem; complication; decision making; decision support system; military medicine; pneumonia; polymerase chain reaction; Postoperative Complications; predictive value; risk assessment; statistics and numerical data; Wounds and Injuries; young adult; Algorithms; APACHE; Bacteremia; Bayes Theorem; Biomarkers; Decision Making; Decision Support Techniques; Glasgow Coma Scale; Humans; Injury Severity Score; Length of Stay; Machine Learning; Military Medicine; Pneumonia; Polymerase Chain Reaction; Postoperative Complications; Precision Medicine; Predictive Value of Tests; Prospective Studies; Risk Assessment; Wounds and Injuries; Young Adult",2-s2.0-85019546274
"Trezzi J.-P., Galozzi S., Jaeger C., Barkovits K., Brockmann K., Maetzler W., Berg D., Marcus K., Betsou F., Hiller K., Mollenhauer B.","Distinct metabolomic signature in cerebrospinal fluid in early parkinson's disease",2017,"Movement Disorders",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028329332&doi=10.1002%2fmds.27132&partnerID=40&md5=a4bcafad6085b1db44668e20a72a47e9","Objective: The purpose of this study was to profile cerebrospinal fluid (CSF) from early-stage PD patients for disease-related metabolic changes and to determine a robust biomarker signature for early-stage PD diagnosis. Methods: By applying a non-targeted and mass spectrometry-driven approach, we investigated the CSF metabolome of 44 early-stage sporadic PD patients yet without treatment (DeNoPa cohort). We compared all detected metabolite levels with those measured in CSF of 43 age- and gender-matched healthy controls. After this analysis, we validated the results in an independent PD study cohort (Tübingen cohort). Results: We identified that dehydroascorbic acid levels were significantly lower and fructose, mannose, and threonic acid levels were significantly higher (P <.05) in PD patients when compared with healthy controls. These changes reflect pathological oxidative stress responses, as well as protein glycation/glycosylation reactions in PD. Using a machine learning approach based on logistic regression, we successfully predicted the origin (PD patients vs healthy controls) in a second (n = 18) as well as in a third and completely independent validation set (n = 36). The biomarker signature is composed of the three markers—mannose, threonic acid, and fructose—and allows for sample classification with a sensitivity of 0.790 and a specificity of 0.800. Conclusion: We identified PD-specific metabolic changes in CSF that were associated with antioxidative stress response, glycation, and inflammation. Our results disentangle the complexity of the CSF metabolome to unravel metabolome changes related to early-stage PD. The detected biomarkers help understanding PD pathogenesis and can be applied as biomarkers to increase clinical diagnosis accuracy and patient care in early-stage PD. © 2017 International Parkinson and Movement Disorder Society. © 2017 International Parkinson and Movement Disorder Society","biomarker; CSF; logistic regression; metabolomics; Parkinson's disease","alpha hydroxybutyric acid; ascorbic acid; biological marker; carbonic acid; dehydroascorbic acid; endoplasmic reticulum golgi intermediate compartment protein 53; fructose; glucose; glyceric acid; hemoglobin; immunoglobulin G; lauric acid; mannose; mannose binding lectin; marker; threonic acid; unclassified drug; urea; adult; aged; Article; cerebrospinal fluid; clinical article; cohort analysis; comparative study; controlled study; early diagnosis; female; human; machine learning; male; mass spectrometry; metabolomics; oxidative stress; Parkinson disease; priority journal; protein cerebrospinal fluid level; protein expression; protein glycosylation; sensitivity and specificity",2-s2.0-85028329332
"Febres G., Jaffe K.","Music viewed by its entropy content: A novel window for comparative analysis",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031781217&doi=10.1371%2fjournal.pone.0185757&partnerID=40&md5=aa32906be9aeccace38156fb8fcccd7a","Polyphonic music files were analyzed using the set of symbols that produced the Minimal Entropy Description, which we call the Fundamental Scale. This allowed us to create a novel space to represent music pieces by developing: (a) a method to adjust a textual description from its original scale of observation to an arbitrarily selected scale, (b) a method to model the structure of any textual description based on the shape of the symbol frequency profiles, and (c) the concept of higher order entropy as the entropy associated with the deviations of a frequency-ranked symbol profile from a perfect Zipfian profile. We call this diversity index the ‘2nd Order Entropy’. Applying these methods to a variety of musical pieces showed how the space of ‘symbolic specific diversity-entropy’ and that of ‘2nd order entropy’ captures characteristics that are unique to each music type, style, composer and genre. Some clustering of these properties around each musical category is shown. These methods allow us to visualize a historic trajectory of academic music across this space, from medieval to contemporary academic music. We show that the description of musical structures using entropy, symbol frequency profiles and specific symbolic diversity allows us to characterize traditional and popular expressions of music. These classification techniques promise to be useful in other disciplines for pattern recognition and machine learning. © 2017 Febres, Jaffe. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"classification; entropy; human; human experiment; machine learning; Middle Ages; music; pattern recognition; auditory stimulation; automated pattern recognition; comparative study; entropy; information retrieval; Markov chain; music; natural language processing; physiology; psychology; Acoustic Stimulation; Entropy; Humans; Information Storage and Retrieval; Markov Chains; Music; Natural Language Processing; Pattern Recognition, Automated; Pattern Recognition, Physiological",2-s2.0-85031781217
"Mendoza J.A., Haaland W., Jacobs M., Abbey-Lambertz M., Miller J., Salls D., Todd W., Madding R., Ellis K., Kerr J.","Bicycle Trains, Cycling, and Physical Activity: A Pilot Cluster RCT",2017,"American Journal of Preventive Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021262798&doi=10.1016%2fj.amepre.2017.05.001&partnerID=40&md5=5288a90a8c18ae2c7cd76c4fd5fa34ac","Introduction Increasing children's cycling to school and physical activity are national health goals. The objective was to conduct an RCT of a bicycle train program to assess impact on students’ school travel mode and moderate-to-vigorous physical activity (MVPA). Study design Pilot cluster RCT with randomization at the school level and N=54 participants. Setting/participants Fourth–fifth graders from four public schools serving low-income families in Seattle, WA in 2014 with analyses in 2015–2016. All participants were provided and fitted with bicycles, safety equipment (helmets, locks, and lights), and a 2- to 3-hour bicycle safety course. Intervention The intervention was a bicycle train offered daily (i.e., students volunteered to cycle with study staff to and from school). Main outcome measures Time 1 assessments occurred prior to randomization. Time 2 assessments occurred after 3–5 weeks of the intervention (i.e., during Weeks 4–6 of the intervention period). The primary outcome was the percentage of daily commutes to school by cycling measured by validated survey. MVPA, measured by accelerometry and GPS units and processed by machine learning algorithms, was a secondary outcome. Results For two separate adjusted repeated measures linear mixed effects models in which students (N=54) were nested within schools (N=4), intervention participants had: (1) an absolute increase in mean percentage of daily commutes by cycling of 44.9%, (95% CI=26.8, 63.0) and (2) an increase in mean MVPA of 21.6 minutes/day, (95% CI=8.7, 34.6) from Time 1 to Time 2 compared with controls. Conclusions A pilot bicycle train intervention increased cycling to school and daily MVPA in the short term among diverse, inner-city elementary school students. The bicycle train intervention appears promising and warrants further experimental trials among large, diverse samples with longer follow-up. Trial registration This study is registered at www.clinicaltrials.gov NCT02006186. © 2017 American Journal of Preventive Medicine",,"accelerometry; algorithm; Article; child; controlled study; cycling; elementary student; female; follow up; health program; helmet; human; human experiment; lowest income group; machine learning; male; normal human; physical activity; pilot study; protective equipment; randomization; randomized controlled trial; school; travel",2-s2.0-85021262798
"Kellner-Weldon F., Stippich C., Wiest R., Lehmann V., Meier R., Beck J., Schucht P., Raabe A., Reyes M., Bink A.","Comparison of perioperative automated versus manual two-dimensional tumor analysis in glioblastoma patients",2017,"European Journal of Radiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026869147&doi=10.1016%2fj.ejrad.2017.07.028&partnerID=40&md5=46f06ea620c18485f83f6b0f1c622f82","Objectives Current recommendations for the measurement of tumor size in glioblastoma continue to employ manually measured 2D product diameters of enhancing tumor. To overcome the rater dependent variability, this study aimed to evaluate the potential of automated 2D tumor analysis (ATA) compared to highly experienced rater teams in the workup of pre- and postoperative image interpretation in a routine clinical setting. Materials and methods From 92 patients with newly diagnosed GB and performed surgery, manual rating of the sum product diameter (SPD) of enhancing tumor on magnetic resonance imaging (MRI) contrast enhanced T1w was compared to automated machine learning-based tumor analysis using FLAIR, T1w, T2w and contrast enhanced T1w. Results Preoperative correlation of SPD between two rater teams (1 and 2) was r = 0.921 (p < 0.0001). Difference among the rater teams and ATA (p = 0.567) was not statistically significant. Correlation between team 1 vs. automated tumor analysis and team 2 vs. automated tumor analysis was r = 0.922 and r = 0.897, respectively (p < 0.0001 for both). For postoperative evaluation interrater agreement between team 1 and 2 was moderate (Kappa 0.53). Manual consensus classified 46 patients as completely resected enhancing tumor. Automated tumor analysis agreed in 13/46 (28%) due to overestimation caused by hemorrhage and choroid plexus enhancement. Conclusions Automated 2D measurements can be promisingly translated into clinical trials in the preoperative evaluation. Immediate postoperative SPD evaluation for extent of resection is mainly influenced by postoperative blood depositions and poses challenges for human raters and ATA alike. © 2017 Elsevier B.V.","automated data analysis; Computer assisted reading; glioblastoma; machine learning; MRI","adult; aged; Article; automation; bleeding; cancer patient; cancer surgery; comparative study; contrast enhancement; correlation analysis; female; glioblastoma; human; image analysis; machine learning; major clinical study; male; medical examination; nuclear magnetic resonance imaging; postoperative period; preoperative period; priority journal; retrospective study; two dimensional tumor analysis",2-s2.0-85026869147
"Shinmoto Torres R.L., Visvanathan R., Abbott D., Hill K.D., Ranasinghe D.C.","A battery-less and wireless wearable sensor system for identifying bed and chair exits in a pilot trial in hospitalized older people",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031014290&doi=10.1371%2fjournal.pone.0185670&partnerID=40&md5=deaab5b873afe8649d28ca0aebf29943","Falls in hospitals are common, therefore strategies to minimize the impact of these events in older patients and needs to be examined. In this pilot study, we investigate a movement monitoring sensor system for identifying bed and chair exits using a wireless wearable sensor worn by hospitalized older patients. We developed a movement monitoring sensor system that recognizes bed and chair exits. The system consists of a machine learning based activity classifier and a bed and chair exit recognition process based on an activity score function. Twenty-six patients, aged 71 to 93 years old, hospitalized in the Geriatric Evaluation and Management Unit participated in the supervised trials. They wore over their attire a battery-less, lightweight and wireless sensor and performed scripted activities such as getting off the bed and chair. We investigated the system performance in recognizing bed and chair exits in hospital rooms where RFID antennas and readers were in place. The system’s acceptability was measured using two surveys with 0–10 likert scales. The first survey measured the change in user perception of the system before and after a trial; the second survey, conducted only at the end of each trial, measured user acceptance of the system based on a multifactor sensor acceptance model. The performance of the system indicated an overall recall of 81.4%, precision of 66.8% and F-score of 72.4% for joint bed and chair exit recognition. Patients demonstrated improved perception of the system after use with overall score change from 7.8 to 9.0 and high acceptance of the system with score 6.7 for all acceptance factors. The present pilot study suggests the use of wireless wearable sensors is feasible for detecting bed and chair exits in a hospital environment. © 2017 Shinmoto Torres et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"aged; Article; clinical article; cohort analysis; controlled study; falling; female; hospital patient; human; machine learning; male; pilot study; prospective study; geriatric assessment; hospital; physiologic monitoring; physiology; questionnaire; very elderly; walking; wireless communication; Aged; Aged, 80 and over; Female; Geriatric Assessment; Hospitals; Humans; Male; Monitoring, Physiologic; Pilot Projects; Surveys and Questionnaires; Walking; Wireless Technology",2-s2.0-85031014290
"Lee J., Byun J., Kim B., Yoo D.-G.","Delineation of gas hydrate reservoirs in the Ulleung Basin using unsupervised multi-attribute clustering without well log data",2017,"Journal of Natural Gas Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028083314&doi=10.1016%2fj.jngse.2017.08.007&partnerID=40&md5=9c762536a132f4b02a61e4ee575331dd","To target a well location for gas hydrate reservoirs, many previous studies have used only some discontinuous indicators such as the bottom simulating reflector (BSR) and dimming or acoustic impedance (AI). However, when drilling the first well, there are no well log and core analysis data for AI inversion and geological analysis. Moreover, these methods may have some risk of finding gas hydrate zones and overestimating the reservoir distribution. First of all, we inverted AI using seismic data and root-mean-square (RMS) velocity for AI inversion excepting well logs. In this inversion, the low-frequency impedance variations were estimated from RMS velocity by the product of the interval velocity calculated by the Dix equation and the bulk density from this interval velocity. Then, this low frequency information was integrated with the seismic frequency information obtained from a reflectivity series by sparse-spike impedance inversion. To prevent overestimation due to the use of AI alone, we focused on another rock property, shear impedance (SI), which indicates the ‘rigidity’ of rocks, as gas hydrate consolidate sediments and increases the value of SI noticeably compared to that in surrounding non-reservoirs. To estimate this property, we used this inverted AI and partial stack seismic data using the two-term Fatti's equation. This amplitude variation with an offset (AVO) equation excludes the density term, which is too sensitive to noise, and has only the two terms of AI and SI. As a result, we applied K-mean clustering, which is the method of unsupervised machine learning, to delineate a more accurate and quantitative distribution of hydrated reservoirs from areas with higher impedances and higher values of two additional attributes (RMS amplitude and instantaneous frequency) compared to surrounding formations. In conclusion, we verified that this workflow is useful for identifying the distribution of potential reservoirs and aiding in well-site location determination. © 2017 Elsevier B.V.","Gas hydrate; Machine learning; Multi-attribute analysis; Quantitative seismic interpretation; Ulleung Basin; Unsupervised clustering",,2-s2.0-85028083314
"Scicluna B.P., van Vught L.A., Zwinderman A.H., Wiewel M.A., Davenport E.E., Burnham K.L., Nürnberg P., Schultz M.J., Horn J., Cremer O.L., Bonten M.J., Hinds C.J., Wong H.R., Knight J.C., van der Poll T., de Beer F.M., Bos L.D.J., Frencken J.F., Koster-Brouwer M.E., van de Groep K., Verboom D.M., Glas G.J., van Hooijdonk R.T.M., Hoogendijk A.J., Huson M.A., Klouwenberg P.M.K., Ong D.S.Y., Schouten L.R.A., Straat M., Witteveen E., Wieske L.","Classification of patients with sepsis according to blood genomic endotype: a prospective cohort study",2017,"The Lancet Respiratory Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028540198&doi=10.1016%2fS2213-2600%2817%2930294-1&partnerID=40&md5=7283e711c516975fb34cf77437e0cfeb","Background Host responses during sepsis are highly heterogeneous, which hampers the identification of patients at high risk of mortality and their selection for targeted therapies. In this study, we aimed to identify biologically relevant molecular endotypes in patients with sepsis. Methods This was a prospective observational cohort study that included consecutive patients admitted for sepsis to two intensive care units (ICUs) in the Netherlands between Jan 1, 2011, and July 20, 2012 (discovery and first validation cohorts) and patients admitted with sepsis due to community-acquired pneumonia to 29 ICUs in the UK (second validation cohort). We generated genome-wide blood gene expression profiles from admission samples and analysed them by unsupervised consensus clustering and machine learning. The primary objective of this study was to establish endotypes for patients with sepsis, and assess the association of these endotypes with clinical traits and survival outcomes. We also established candidate biomarkers for the endotypes to allow identification of patient endotypes in clinical practice. Findings The discovery cohort had 306 patients, the first validation cohort had 216, and the second validation cohort had 265 patients. Four molecular endotypes for sepsis, designated Mars1–4, were identified in the discovery cohort, and were associated with 28-day mortality (log-rank p=0·022). In the discovery cohort, the worst outcome was found for patients classified as having a Mars1 endotype, and at 28 days, 35 (39%) of 90 people with a Mars1 endotype had died (hazard ratio [HR] vs all other endotypes 1·86 [95% CI 1·21–2·86]; p=0·0045), compared with 23 (22%) of 105 people with a Mars2 endotype (HR 0·64 [0·40–1·04]; p=0·061), 16 (23%) of 71 people with a Mars3 endotype (HR 0·71 [0·41–1·22]; p=0·19), and 13 (33%) of 40 patients with a Mars4 endotype (HR 1·13 [0·63–2·04]; p=0·69). Analysis of the net reclassification improvement using a combined clinical and endotype model significantly improved risk prediction to 0·33 (0·09–0·58; p=0·008). A 140-gene expression signature reliably stratified patients with sepsis to the four endotypes in both the first and second validation cohorts. Only Mars1 was consistently significantly associated with 28-day mortality across the cohorts. To facilitate possible clinical use, a biomarker was derived for each endotype; BPGM and TAP2 reliably identified patients with a Mars1 endotype. Interpretation This study provides a method for the molecular classification of patients with sepsis to four different endotypes upon ICU admission. Detection of sepsis endotypes might assist in providing personalised patient management and in selection for trials. Funding Center for Translational Molecular Medicine, Netherlands. © 2017 Elsevier Ltd",,"bisphosphoglycerate mutase; transporter associated with antigen processing 1; adult; APACHE; Article; BPGM gene; cardiovascular disease; Charlson Comorbidity Index; chronic obstructive lung disease; clinical practice; cohort analysis; community acquired pneumonia; comorbidity; consensus development; controlled study; diabetes mellitus; female; gene; gene expression; gene expression profiling; genetic analysis; hazard ratio; hospital admission; human; hypertension; intensive care unit; kidney failure; machine learning; major clinical study; male; malignant neoplasm; mortality; Netherlands; observational study; patient coding; patient selection; personalized medicine; prediction; priority journal; prospective study; reliability; respiratory failure; risk assessment; sepsis; Sequential Organ Failure Assessment Score; survival rate; TAP2 gene; United Kingdom",2-s2.0-85028540198
"Omura T., Ishigami G.","Wheel slip classification method for mobile robot in sandy terrain using in-wheel sensor",2017,"Journal of Robotics and Mechatronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031911402&doi=10.20965%2fjrm.2017.p0902&partnerID=40&md5=09756235781af580c106f173ae10561e","This paper proposes a method that can estimate and classify the magnitude of wheel slippage for a mobile robot in sandy terrains. The proposed method exploits a sensor suite, called an in-wheel sensor, which measures the normal force and contact angle at the wheel-sand interaction boundary. An experimental test using the in-wheel sensor reveals that the maximum normal force and exit angle of the wheel explicitly vary with the magnitude of the wheel slippage. These characteristics are then fed into a machine learning algorithm, which classifies the wheel slippage into three categories: non-stuck wheel, quasi-stuck wheel, and stuck wheel. The usefulness of the proposed method for slip classification is experimentally evaluated using a four-wheel-drive test bed rover. © 2017, Fuji Technology Press. All rights reserved.","In-wheel sensor; Support vector machine; Wheel slip classification; Wheel-soil interaction","Learning algorithms; Learning systems; Mobile robots; Support vector machines; Classification methods; Experimental test; Four-wheel drives; In-wheel; Three categories; Wheel slippage; Wheel slips; Wheel-soil interactions; Wheels",2-s2.0-85031911402
"Nibali A., He Z., Wollersheim D.","Pulmonary nodule classification with deep residual networks",2017,"International Journal of Computer Assisted Radiology and Surgery",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019232353&doi=10.1007%2fs11548-017-1605-6&partnerID=40&md5=c9cdb492716410952687fbde78c04122","Purpose : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules. Methods: We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification. Results: Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy. Conclusions: The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains. © 2017, CARS.","Convolutional neural network; CT images; Lung nodule","Article; artificial neural network; cancer classification; computer assisted tomography; convolutional neural network; curriculum; diagnostic accuracy; diagnostic test accuracy study; false negative result; false positive result; follow up; histogram; human; image processing; lung cancer; lung nodule; machine learning; major clinical study; perceptron; priority journal; sensitivity and specificity",2-s2.0-85019232353
"Alghabban W.G., Salama R.M., Altalhi A.H.","Mobile cloud computing: An effective multimodal interface tool for students with dyslexia",2017,"Computers in Human Behavior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019182097&doi=10.1016%2fj.chb.2017.05.014&partnerID=40&md5=5f981d68cf9eaae671a1e1423848a593","The explosive growth of mobile technology and developments in cloud computing have provided new and powerful possibilities for mobile learning (m-learning). M-learning, which is an influential trend in the educational process, promotes learning accessibility and flexibility for a substantial and valuable segment of society: students with dyslexia. Although there is a substantial amount of research that discusses the use of m-learning by students with dyslexia, there are many gaps that must still be addressed. One substantial issue pertains to the lack of an effective, multimodal, human–computer user interface m-learning tool that combines different input and output modes based on each student's learning style. In this paper, the authors develop a novel, interactive, multimodal interfaced, cloud-based m-learning tool with which students can naturally interact based on their preferred learning styles. This multimodal interface tool enhances the learning capabilities of students with dyslexia by almost 30% by customizing their multimodal functionality to meet their learning needs. © 2017 Elsevier Ltd","Dyslexia; M-learning; Mobile application; Mobile cloud computing (MCC); Multimodal","Cloud computing; E-learning; Education; Interactive computer systems; Machine tools; Students; User interfaces; Dyslexia; Educational process; Input and outputs; Learning capabilities; M-Learning; Mobile applications; Multi-modal; Multi-modal interfaces; Mobile cloud computing",2-s2.0-85019182097
"Zaniewicz Ł., Jaroszewicz S.","Lp -Support vector machines for uplift modeling",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016098782&doi=10.1007%2fs10115-017-1040-6&partnerID=40&md5=e3384bae8f6175c247fb32c1f6f67393","Uplift modeling is a branch of machine learning which aims to predict not the class itself, but the difference between the class variable behavior in two groups: treatment and control. Objects in the treatment group have been subjected to some action, while objects in the control group have not. By including the control group, it is possible to build a model which predicts the causal effect of the action for a given individual. In this paper, we present a variant of support vector machines designed specifically for uplift modeling. The SVM optimization task has been reformulated to explicitly model the difference in class behavior between two datasets. The model predicts whether a given object will have a positive, neutral or negative response to a given action, and by tuning a parameter of the model the analyst is able to influence the relative proportion of neutral predictions and thus the conservativeness of the model. Further, we extend Lp-SVMs to the case of uplift modeling and demonstrate that they allow for a more stable selection of the size of negative, neutral and positive groups. Finally, we present quadratic and convex optimization methods for efficiently solving the two proposed optimization tasks. © 2017, The Author(s).","Control group; Support vector machine; Uplift modeling",,2-s2.0-85016098782
"Tang Y., Xiao Y.","Learning fuzzy semantic cell by principles of maximum coverage, maximum specificity, and maximum fuzzy entropy of vague concept",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021854248&doi=10.1016%2fj.knosys.2017.05.014&partnerID=40&md5=20d8dc256301d6a83bd6fb9b83160515","Concept modeling and learning have important significance in data mining, machine learning and knowledge discovery. In this paper a fuzzy semantic cell which is composed of a prototype P, a distance function d and a probability density function δ of granularity is considered as the smallest unit of vague concepts and the building brick of concept representation. For each fuzzy semantic cell we introduce three fundamental numeric characteristics, prototype P, expectation granularity R and fuzzy entropy H, to characterize the underlying concept. Then a novel learning strategy for the fuzzy semantic cell is proposed by using the principles of maximum coverage, maximum specificity, and maximum fuzzy entropy. Furthermore a granularity control factor λ is introduced into the learning strategy in order to make these principles coordinate with each other. The ultimate goal is to obtain a fuzzy semantic cell from a given data set which is the most appropriate to describe the data set. Finally the fuzzy semantic cell learning algorithm as well as the crisp semantic cell learning algorithm is formulated. We test the proposed methods on synthetic data and real-world data to demonstrate their feasibility and validity. © 2017","Concept modeling; Expectation granularity; Fuzzy entropy; Fuzzy semantic cell; Justifiable granularity; Prototype theory","Cells; Cytology; Entropy; Learning algorithms; Learning systems; Probability density function; Semantics; Concept model; Expectation granularity; Fuzzy entropy; Fuzzy semantics; Justifiable granularity; Prototype theory; Education",2-s2.0-85021854248
"Gu H., Wang X., Chen X., Deng S., Shi J.","Manifold learning by curved cosine mapping",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028831872&doi=10.1109%2fTKDE.2017.2728790&partnerID=40&md5=81f47c19abde70dc65c982b41bf947ca","In the field of pattern recognition, data analysis, and machine learning, data points are usually modeled as high-dimensional vectors. Due to the curse-of-dimensionality, it is non-trivial to efficiently process the orginal data directly. Given the unique properties of nonlinear dimensionality reduction techniques, nonlinear learning methods are widely adopted to reduce the dimension of data. However, existing nonlinear learning methods fail in many real applications because of the too-strict requirements (for real data) or the difficulty in parameters tuning. Therefore, in this paper, we investigate the manifold learning methods which belong to the family of nonlinear dimensionality reduction methods. Specifically, we proposed a new manifold learning principle for dimensionality reduction named Curved Cosine Mapping (CCM). Based on the law of cosines in Euclidean space, CCM applies a brand new mapping pattern to manifold learning. In CCM, the nonlinear geometric relationships are obtained by utlizing the law of cosines, and then quantified as the dimensionality-reduced features. Compared with the existing approaches, the model has weaker theoretical assumptions over the input data. Moreover, to further reduce the computation cost, an optimized version of CCM is developed. Finally, we conduct extensive experiments over both artificial and real-world datasets to demonstrate the performance of proposed techniques. © 1989-2012 IEEE.","Dimensionality reduction; Law of cosines; Manifold learning; Nearest neighbour graph; Pattern recognition","Analytical models; Automobile engine manifolds; Bridges; Data structures; Learning systems; Mapping; Nearest neighbor search; Nonlinear analysis; Pattern recognition; Pattern recognition systems; Computational model; Dimensionality reduction; Law of cosines; Manifold learning; Nearest neighbour; Data reduction",2-s2.0-85028831872
"Trambaiolli L.R., Spolaôr N., Lorena A.C., Anghinah R., Sato J.R.","Feature selection before EEG classification supports the diagnosis of Alzheimer's disease",2017,"Clinical Neurophysiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028599323&doi=10.1016%2fj.clinph.2017.06.251&partnerID=40&md5=907830c77029e1accbd76388401486cb","Objective In many decision support systems, some input features can be marginal or irrelevant to the diagnosis, while others can be redundant among each other. Thus, feature selection (FS) algorithms are often considered to find relevant/non-redundant features. This study aimed to evaluate the relevance of FS approaches applied to Alzheimer's Disease (AD) EEG-based diagnosis and compare the selected features with previous clinical findings. Methods Eight different FS algorithms were applied to EEG spectral measures from 22 AD patients and 12 healthy age-matched controls. The FS contribution was evaluated by considering the leave-one-subject-out accuracy of Support Vector Machine classifiers built in the datasets described by the selected features. Results The Filtered Subset Evaluator technique achieved the best performance improvement both on a per-patient basis (91.18% of accuracy) and on a per-epoch basis (85.29 ± 21.62%), after removing 88.76 ± 1.12% of the original features. All algorithms found out that alpha and beta bands are relevant features, which is in agreement with previous findings from the literature. Conclusion Biologically plausible EEG datasets could achieve improved accuracies with pre-processing FS steps. Significance The results suggest that the FS and classification techniques are an attractive complementary tool in order to reveal potential biomarkers aiding the AD clinical diagnosis. © 2017","Alzheimer's disease; Dementia; Electroencephalography; Feature selection; Pattern recognition","biological marker; aged; algorithm; Alzheimer disease; Article; classification; clinical article; clinical feature; controlled study; diagnostic accuracy; diagnostic test accuracy study; electroencephalography; female; human; male; patient coding; pattern recognition; predictive value; priority journal; sensitivity and specificity; algorithm; Alzheimer disease; electroencephalography; machine learning; middle aged; procedures; very elderly; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Electroencephalography; Female; Humans; Machine Learning; Male; Middle Aged",2-s2.0-85028599323
"Zhang C., Liu C., Zhang X., Almpanidis G.","An up-to-date comparison of state-of-the-art classification algorithms",2017,"Expert Systems with Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017304883&doi=10.1016%2fj.eswa.2017.04.003&partnerID=40&md5=790acc02569231ed9a0e1d0c22b003a0","Current benchmark reports of classification algorithms generally concern common classifiers and their variants but do not include many algorithms that have been introduced in recent years. Moreover, important properties such as the dependency on number of classes and features and CPU running time are typically not examined. In this paper, we carry out a comparative empirical study on both established classifiers and more recently proposed ones on 71 data sets originating from different domains, publicly available at UCI and KEEL repositories. The list of 11 algorithms studied includes Extreme Learning Machine (ELM), Sparse Representation based Classification (SRC), and Deep Learning (DL), which have not been thoroughly investigated in existing comparative studies. It is found that Stochastic Gradient Boosting Trees (GBDT) matches or exceeds the prediction performance of Support Vector Machines (SVM) and Random Forests (RF), while being the fastest algorithm in terms of prediction efficiency. ELM also yields good accuracy results, ranking in the top-5, alongside GBDT, RF, SVM, and C4.5 but this performance varies widely across all data sets. Unsurprisingly, top accuracy performers have average or slow training time efficiency. DL is the worst performer in terms of accuracy but second fastest in prediction efficiency. SRC shows good accuracy performance but it is the slowest classifier in both training and testing. © 2017 Elsevier Ltd","Classification benchmarking; Classifier comparison; Classifier evaluation","Decision trees; Efficiency; Forecasting; Learning systems; Stochastic systems; Support vector machines; Classification algorithm; Classifier evaluation; Comparative studies; Extreme learning machine; Prediction performance; Sparse representation based classifications; Stochastic gradient boosting; Training and testing; Classification (of information)",2-s2.0-85017304883
"Lundström C.F., Gilmore H.L., Ros P.R.","Integrated diagnostics: The computational revolution catalyzing cross-disciplinary practices in radiology, pathology, and genomics",2017,"Radiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029713684&doi=10.1148%2fradiol.2017170062&partnerID=40&md5=3ffdb7ad0d9edfd3c4ddd2d348d0ea9e",[No abstract available],,"decision making; diagnostic test; Europe; genomics; human; information technology; integrated diagnostics; limit of quantitation; machine learning; Note; patient care; personalized medicine; practice guideline; priority journal; radiologist; radiology information system; United States",2-s2.0-85029713684
"Li T., Li M., Gao Q., Xie D.","F-norm distance metric based robust 2DPCA and face recognition",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029434231&doi=10.1016%2fj.neunet.2017.07.011&partnerID=40&md5=0b42c41fbc776dd451bc7507f8c514b6","Two-dimensional principal component analysis (2DPCA) employs squared F-norm as the distance metric for dimensionality reduction. It is commonly known that squared F-norm is sensitive to the presence of outliers. To address this problem, we use F-norm instead of squared F-norm as the distance metric in the objective function and develop a non-greedy algorithm, which has a closed-form solution in each iteration and can maximize the criterion function, to solve the optimal solution. Our approach not only is robust to outliers but also well characterizes the geometric structure of data. Experimental results on several face databases illustrate that our method is more effective and robust than the other robust 2DPCA algorithms. © 2017 Elsevier Ltd","2DPCA; Dimensionality reduction; F-norm; Geometric structure","Geometry; Iterative methods; Principal component analysis; Statistics; 2DPCA; Closed form solutions; Criterion functions; Dimensionality reduction; F norms; Geometric structure; Objective functions; Two dimensional principal component analysis (2DPCA); Face recognition; algorithm; Article; automated facial recognition; controlled study; data base; F norm; intermethod comparison; machine learning; measurement accuracy; measurement error; principal component analysis; priority journal; statistical parameters; two dimensional principal component analysis",2-s2.0-85029434231
"Tesche C., De Cecco C.N., Albrecht M.H., Duguay T.M., Bayer R.R., II, Litwin S.E., Steinberg D.H., Schoepf U.J.","Coronary CT angiography-derived fractional flow reserve",2017,"Radiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029704787&doi=10.1148%2fradiol.2017162641&partnerID=40&md5=cccbe19f30f730360ba2a8cb15b49936",[No abstract available],,"methoxy isobutyl isonitrile technetium tc 99m; algorithm; area under the curve; Article; cardiovascular magnetic resonance; computational fluid dynamics; computed tomographic angiography; computed tomographic myocardial perfusion imaging; computer assisted tomography; coronary angiography; coronary artery calcification; coronary artery disease; coronary artery obstruction; coronary computed tomographic angiography; cost effectiveness analysis; death; diagnostic accuracy; diagnostic test accuracy study; fractional flow reserve; heart infarction; heart muscle ischemia; human; intermethod comparison; invasive coronary angiography; machine learning; myocardial perfusion imaging; nuclear magnetic resonance imaging; patient care; positron emission tomography; predictive value; priority journal; quality of life; receiver operating characteristic; revascularization; sensitivity and specificity; single energy stress perfusion CT myocardial blood pool imaging; single photon emission computed tomography; stress perfusion magnetic resonance imaging; treatment planning; aged; cohort analysis; computed tomographic angiography; coronary angiography; coronary artery obstruction; diagnostic imaging; female; fractional flow reserve; male; middle aged; physiology; procedures; reproducibility; standards; Aged; Cohort Studies; Computed Tomography Angiography; Coronary Angiography; Coronary Stenosis; Female; Fractional Flow Reserve, Myocardial; Humans; Machine Learning; Male; Middle Aged; Reproducibility of Results",2-s2.0-85029704787
"Moskowitz A., Chen K.P., Cooper A.Z., Chahin A., Ghassemi M.M., Celi L.A.","Management of atrial fibrillation with rapid ventricular response in the intensive care unit: A secondary analysis of electronic health record data",2017,"Shock",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015861195&doi=10.1097%2fSHK.0000000000000869&partnerID=40&md5=17fba2c33026ff99c34dce29ab7aa3a0","Purpose: Atrial fibrillation with rapid ventricular response (RVR) is common during critical illness. In this study, we explore the comparative effectiveness of three commonly used drugs (metoprolol, diltiazem, and amiodarone) in the management of atrial fibrillation with RVR in the intensive care unit (ICU). Methods: Data pertaining to the first ICU admission were extracted from the Medical Information Mart for Intensive Care III database. Patients who received one of the above pharmacologic agents while their heart rate was>110 bpm and had atrial fibrillation documented in the clinical chart were included. Propensity score weighting using a generalized boosted model was used to compare medication failure rates (second agent prior to termination of RVR). Secondary outcomes included time to control, control within 4 h, and mortality. Results: One thousand six hundred forty-six patients were included: 736 received metoprolol, 292 received diltiazem, and 618 received amiodarone. Compared with those who received metoprolol, failure rates were higher amongst those who received amiodarone (OR 1.39, 95% CI 1.03-1.87, P0.03) and there was a trend towards increased failure rates in patients who received diltiazem (OR 1.35, CI 0.89-2.07, P0.16). Amongst patients who received a single agent, patients who received diltiazem were less likely to be controlled at 4-h than those who received metoprolol (OR 0.64, CI 0.43-097, P0.03). Initial agent was not associated with in-hospital mortality. Conclusions: In this study, metoprolol was the most commonly used agent for atrial fibrillation with RVR. Metoprolol had a lower failure rate than amiodarone and was superior to diltiazem in achieving rate control at 4 h. Copyright © 2017 by the Shock Society.","Amiodarone; Beta blocker; Big data; Calcium channel blocker; Critical care; Machine learning","amiodarone; digoxin; diltiazem; esmolol; ibutilide; metoprolol; procainamide; propafenone; aged; Article; atrial fibrillation; comparative effectiveness; controlled study; critically ill patient; disease severity; electronic health record; female; heart rate; hospital mortality; human; intensive care unit; major clinical study; male; medical information; secondary analysis; treatment outcome; treatment response",2-s2.0-85015861195
"Miran S., Purdon P.L., Brown E.N., Babadi B.","Robust Estimation of Sparse Narrowband Spectra from Neuronal Spiking Data",2017,"IEEE Transactions on Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029919790&doi=10.1109%2fTBME.2016.2642783&partnerID=40&md5=03e4aded8222deebd3f5ea76c3210841","Objective: Characterizing the spectral properties of neuronal responses is an important problem in computational neuroscience, as it provides insight into the spectral organization of the underlying functional neural processes. Although spectral analysis techniques are widely used in the analysis of noninvasive neural recordings such as EEG, their application to spiking data is limited due to the binary and nonlinear nature of neuronal spiking. In this paper, we address the problem of estimating the power spectral density of the neural covariate driving the spiking statistics of a neuronal population from binary observations. Methods: We consider a neuronal ensemble spiking according to Bernoulli statistics, for which the conditional intensity function is given by the logistic map of a harmonic second-order stationary process with sparse narrowband spectra. By employing sparsity-promoting priors, we compute the maximum a posteriori estimate of the power spectral density of the process from the binary spiking observations. Furthermore, we construct confidence intervals for these estimates by an efficient posterior sampling procedure. Results: We provide simulation studies which reveal that our method outperforms the existing methods for extracting the frequency content of spiking data. Application of our method to clinically recorded spiking data from a patient under general anesthesia reveals a striking resemblance between our estimated power spectral density and that of the local field potential signal. This result corroborates existing findings regarding the salient role of the local field potential as a major neural covariate of rhythmic cortical spiking activity under anesthesia. Conclusion: Our technique allows us to analyze the harmonic structure of spiking activity in a robust fashion, independently of the local field potentials, and without any prior assumption of the spectral spread and content of the underlying neural processes. Significance: Other than its usage in the spectral analysis of neuronal spiking data, our technique can be applied to a wide variety of binary data, such as heart beat data, in order to obtain a robust spectral representation. © 1964-2012 IEEE.","Neural signal processing; point process models; power spectral density (PSD); spectral estimation","Anesthesiology; Electrophysiology; Neurons; Power spectral density; Signal processing; Spectral density; Spectrum analysis; Computational neuroscience; Conditional intensity function; Maximum a posteriori estimates; Neural signal processing; Point process; Power spectral densities (PSD); Spectral analysis techniques; Spectral Estimation; Population statistics; adult; brain cortex; female; general anesthesia; heart beat; human; local field potential; male; process model; sampling; signal processing; spectrometry; spectroscopy; statistics; structure activity relation; action potential; algorithm; automated pattern recognition; brain; electrocorticography; machine learning; nerve cell; physiology; procedures; reproducibility; sensitivity and specificity; Action Potentials; Algorithms; Brain; Electrocorticography; Humans; Machine Learning; Neurons; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",2-s2.0-85029919790
"Kadukova M., Grudinin S.","Convex-PL: a novel knowledge-based potential for protein-ligand interactions deduced from structural databases using convex optimization",2017,"Journal of Computer-Aided Molecular Design",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029577882&doi=10.1007%2fs10822-017-0068-8&partnerID=40&md5=7f474e173c57ea5fd1622ded83ec0da9","We present a novel optimization approach to train a free-shape distance-dependent protein-ligand scoring function called Convex-PL. We do not impose any functional form of the scoring function. Instead, we decompose it into a polynomial basis and deduce the expansion coefficients from the structural knowledge base using a convex formulation of the optimization problem. Also, for the training set we do not generate false poses with molecular docking packages, but use constant RMSD rigid-body deformations of the ligands inside the binding pockets. This allows the obtained scoring function to be generally applicable to scoring of structural ensembles generated with different docking methods. We assess the Convex-PL scoring function using data from D3R Grand Challenge 2 submissions and the docking test of the CASF 2013 study. We demonstrate that our results outperform the other 20 methods previously assessed in CASF 2013. The method is available at http://team.inria.fr/nano-d/software/Convex-PL/. © 2017, Springer International Publishing AG.","Knowledge-based potential; Machine learning; Molecular docking; Protein-ligand interactions; Scoring function","halogen; nitrogen; oxygen; phosphorus; sulfur; algorithm; Article; atom; controlled study; crystal structure; decomposition; hydrogen bond; machine learning; molecular docking; priority journal; protein conformation; protein database; protein ligand interaction; protein protein interaction; protein structure",2-s2.0-85029577882
"Wang L., Qian C., Kats P., Kontokosta C., Sobolevsky S.","Structure of 311 service requests as a signature of urban location",2017,"PLoS ONE",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031904048&doi=10.1371%2fjournal.pone.0186314&partnerID=40&md5=96d2b8b1c04305d5a80aa8b18e85e1cc","While urban systems demonstrate high spatial heterogeneity, many urban planning, economic and political decisions heavily rely on a deep understanding of local neighborhood contexts. We show that the structure of 311 Service Requests enables one possible way of building a unique signature of the local urban context, thus being able to serve as a low-cost decision support tool for urban stakeholders. Considering examples of New York City, Boston and Chicago, we demonstrate how 311 Service Requests recorded and categorized by type in each neighborhood can be utilized to generate a meaningful classification of locations across the city, based on distinctive socioeconomic profiles. Moreover, the 311-based classification of urban neighborhoods can present sufficient information to model various socioeconomic features. Finally, we show that these characteristics are capable of predicting future trends in comparative local real estate prices. We demonstrate 311 Service Requests data can be used to monitor and predict socioeconomic performance of urban neighborhoods, allowing urban stakeholders to quantify the impacts of their interventions. © 2017 Wang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"classification; decision support system; human; Illinois; Massachusetts; neighborhood; New York; real estate; city planning; demography; ecology; forecasting; housing; machine learning; population research; socioeconomics; spatial analysis; urban population; Boston; Censuses; Chicago; City Planning; Ecology; Forecasting; Housing; Humans; Machine Learning; New York City; Residence Characteristics; Socioeconomic Factors; Spatial Analysis; Urban Population",2-s2.0-85031904048
"Phillips F.","A perspective on 'Big Data'",2017,"Science and Public Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032017702&doi=10.1093%2fscipol%2fscx012&partnerID=40&md5=c341e07e9a4671768d6f4f7e4708a601","Many of the lessons learned with what passed for big data in the 1980s still apply today. The lessons have to do with deciding whether something is true or merely useful, the role of human creativity in posing questions, the treatment of hypotheses and the role of theory in data mining, skill development, and organizational dynamics. This essay details what has changed in the present era of 'big data', what has remained the same, what we may learn, and what promise the future holds. Important highlights include the role of executives in building a data-based decision culture, and the potential of big data for analyzing diversity rather than regression to means. © The Author 2017. Published by Oxford University Press. All rights reserved.","Analytics; Big data; Business intelligence; Machine learning","analytical framework; data mining; data set; machine learning",2-s2.0-85032017702
"Voets N.","Can functional connectivity diagnose autoantibody-associated encephalitides?",2017,"The Lancet Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028725717&doi=10.1016%2fS2215-0366%2817%2930366-8&partnerID=40&md5=bdf364a1426f43f3ac2026f28e2b0f1b",[No abstract available],,"allergic encephalitis; anti n methyl d aspartate receptor encephalitis; differential diagnosis; functional connectivity; functional magnetic resonance imaging; functional neuroimaging; hippocampus; human; medial temporal lobe; nerve cell network; Note; prefrontal cortex; priority journal; resting state network; sensorimotor cortex; supervised machine learning; ventral attention network; visual cortex",2-s2.0-85028725717
"Wang Y., Spincemaille P., Liu Z., Dimov A., Deh K., Li J., Zhang Y., Yao Y., Gillen K.M., Wilman A.H., Gupta A., Tsiouris A.J., Kovanlikaya I., Chiang G.C.-Y., Weinsaft J.W., Tanenbaum L., Chen W., Zhu W., Chang S., Lou M., Kopell B.H., Kaplitt M.G., Devos D., Hirai T., Huang X., Korogi Y., Shtilbans A., Jahng G.-H., Pelletier D., Gauthier S.A., Pitt D., Bush A.I., Brittenham G.M., Prince M.R.","Clinical quantitative susceptibility mapping (QSM): Biometal imaging and its emerging roles in patient care",2017,"Journal of Magnetic Resonance Imaging",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015188941&doi=10.1002%2fjmri.25693&partnerID=40&md5=a472b3f9225ee21e5d187b578b1459cf","Quantitative susceptibility mapping (QSM) has enabled magnetic resonance imaging (MRI) of tissue magnetic susceptibility to advance from simple qualitative detection of hypointense blooming artifacts to precise quantitative measurement of spatial biodistributions. QSM technology may be regarded to be sufficiently developed and validated to warrant wide dissemination for clinical applications of imaging isotropic susceptibility, which is dominated by metals in tissue, including iron and calcium. These biometals are highly regulated as vital participants in normal cellular biochemistry, and their dysregulations are manifested in a variety of pathologic processes. Therefore, QSM can be used to assess important tissue functions and disease. To facilitate QSM clinical translation, this review aims to organize pertinent information for implementing a robust automated QSM technique in routine MRI practice and to summarize available knowledge on diseases for which QSM can be used to improve patient care. In brief, QSM can be generated with postprocessing whenever gradient echo MRI is performed. QSM can be useful for diseases that involve neurodegeneration, inflammation, hemorrhage, abnormal oxygen consumption, substantial alterations in highly paramagnetic cellular iron, bone mineralization, or pathologic calcification; and for all disorders in which MRI diagnosis or surveillance requires contrast agent injection. Clinicians may consider integrating QSM into their routine imaging practices by including gradient echo sequences in all relevant MRI protocols. Level of Evidence: 1. Technical Efficacy: Stage 5. J. Magn. Reson. Imaging 2017;46:951–971. © 2017 International Society for Magnetic Resonance in Medicine","biometals; quantitative susceptibility mapping","biochemical marker; calcium; contrast medium; ferritin; gadolinium; heme; iron; artifact reduction; autoanalysis; biochemical analysis; bleeding; bone mineralization; calcification; calcium homeostasis; cell composition; contrast enhancement; echo planar imaging; erythrocyte; human; image processing; inflammation; iron deficiency; iron homeostasis; iron overload; Kupffer cell; liver cell; machine learning; magnetism; measurement accuracy; microglia; nerve degeneration; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; outcome assessment; oxygen consumption; Parkinson disease; patient care; patient monitoring; priority journal; quantitative diagnosis; quantitative susceptibility mapping biometal imaging; Review; susceptibility weighted imaging",2-s2.0-85015188941
"Barak O.","Recurrent neural networks as versatile tools of neuroscience research",2017,"Current Opinion in Neurobiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021395359&doi=10.1016%2fj.conb.2017.06.003&partnerID=40&md5=b7d11dceb480692840b4ba062386bc2d","Recurrent neural networks (RNNs) are a class of computational models that are often used as a tool to explain neurobiological phenomena, considering anatomical, electrophysiological and computational constraints. RNNs can either be designed to implement a certain dynamical principle, or they can be trained by input–output examples. Recently, there has been large progress in utilizing trained RNNs both for computational tasks, and as explanations of neural phenomena. I will review how combining trained RNNs with reverse engineering can provide an alternative framework for modeling in neuroscience, potentially serving as a powerful hypothesis generation tool. Despite the recent progress and potential benefits, there are many fundamental gaps towards a theory of these networks. I will discuss these challenges and possible methods to attack them. © 2017 Elsevier Ltd",,"algorithm; artificial neural network; machine learning; nervous system electrophysiology; neuroanatomy; neurobiology; neuroscience; priority journal; recurrent neural network; reverse engineering; Review; statistical analysis",2-s2.0-85021395359
"Bruse J.L., Zuluaga M.A., Khushnood A., McLeod K., Ntsinjana H.N., Hsia T.-Y., Sermesant M., Pennec X., Taylor A.M., Schievano S.","Detecting Clinically Meaningful Shape Clusters in Medical Image Data: Metrics Analysis for Hierarchical Clustering Applied to Healthy and Pathological Aortic Arches",2017,"IEEE Transactions on Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029911740&doi=10.1109%2fTBME.2017.2655364&partnerID=40&md5=fb9158479d245bd8a467f8d748b2b94a","Objective: Today's growing medical image databases call for novel processing tools to structure the bulk of data and extract clinically relevant information. Unsupervised hierarchical clustering may reveal clusters within anatomical shape data of patient populations as required for modern precision medicine strategies. Few studies have applied hierarchical clustering techniques to three-dimensional patient shape data and results depend heavily on the chosen clustering distance metrics and linkage functions. In this study, we sought to assess clustering classification performance of various distance/linkage combinations and of different types of input data to obtain clinically meaningful shape clusters. Methods: We present a processing pipeline combining automatic segmentation, statistical shape modeling, and agglomerative hierarchical clustering to automatically subdivide a set of 60 aortic arch anatomical models into healthy controls, two groups affected by congenital heart disease, and their respective subgroups as defined by clinical diagnosis. Results were compared with traditional morphometrics and principal component analysis of shape features. Results: Our pipeline achieved automatic division of input shape data according to primary clinical diagnosis with high F-score (0.902 ± 0.042) and Matthews correlation coefficient (0.851 ± 0.064) using the correlation/weighted distance/linkage combination. Meaningful subgroups within the three patient groups were obtained and benchmark scores for automatic segmentation and classification performance are reported. Conclusion: Clustering results vary depending on the distance/linkage combination used to divide the data. Yet, clinically relevant shape clusters and subgroups could be found with high specificity and low misclassification rates. Significance: Detecting disease-specific clusters within medical image data could improve image-based risk assessment, treatment planning, and medical device development in complex disease. © 1964-2012 IEEE.","Aortic arch; automatic segmentation; cardiovascular magnetic resonance imaging; clinical decision support; congenital heart disease; hierarchical clustering; statistical shape analysis","Arches; Benchmarking; Biomedical equipment; Blood vessels; Cardiology; Clustering algorithms; Decision support systems; Diagnosis; Diseases; Magnetic resonance imaging; Medical imaging; Pipeline processing systems; Pipelines; Population statistics; Principal component analysis; Risk assessment; Aortic arch; Automatic segmentations; Cardiovascular magnetic resonance imaging; Clinical decision support; Congenital heart disease; Hier-archical clustering; Statistical shape analysis; Cluster analysis; adolescent; adult; anatomic model; aortic arch; aortic disease; Article; automation; cardiac imaging; cardiovascular magnetic resonance; classification algorithm; clinical decision support system; cluster analysis; cohort analysis; congenital heart disease; controlled study; correlation coefficient; diagnostic accuracy; diagnostic imaging; female; hierarchical cluster analysis; human; image analysis; image processing; image segmentation; major clinical study; male; measurement precision; morphometry; nuclear magnetic resonance scanner; principal component analysis; scoring system; statistical analysis; statistical model; statistical shape analysis; three dimensional imaging; young adult; abnormalities; algorithm; aorta; automated pattern recognition; child; cine magnetic resonance imaging; computer assisted diagnosis; congenital heart malformation; evaluation study; machine learning; pathology; procedures; reproducibility; sensitivity and specificity; Adolescent; Algorithms; Aorta; Child; Female; Heart Defects, Congenital; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Magnetic Resonance Imaging, Cine; Male; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",2-s2.0-85029911740
"Gatys L.A., Ecker A.S., Bethge M.","Texture and art with deep neural networks",2017,"Current Opinion in Neurobiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029542741&doi=10.1016%2fj.conb.2017.08.019&partnerID=40&md5=3361bb193bdb44aeed25839b64c4739f","Although the study of biological vision and computer vision attempt to understand powerful visual information processing from different angles, they have a long history of informing each other. Recent advances in texture synthesis that were motivated by visual neuroscience have led to a substantial advance in image synthesis and manipulation in computer vision using convolutional neural networks (CNNs). Here, we review these recent advances and discuss how they can in turn inspire new research in visual perception and computational neuroscience. © 2017",,"art; artificial neural network; convolutional neural network; human; image processing; image quality; machine learning; priority journal; Review; vision; visual system",2-s2.0-85029542741
"Jones P., Davies M., Khunti K., Seidu S., Chatterjee S.","Artificial neural networks in diabetes health care professional education: Effective Diabetes Education Now (EDEN)",2017,"Practical Diabetes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031103794&doi=10.1002%2fpdi.2134&partnerID=40&md5=4c91f77ee50188fd67279efe40c8e67e",[No abstract available],,"Article; artificial intelligence; artificial neural network; diabetes education; health care access; health care cost; health care need; health care personnel; health care utilization; health program; human; information processing; machine learning; medical decision making; medical education; patient care",2-s2.0-85031103794
"Birks J., Bankhead C., Holt T.A., Fuller A., Patnick J.","Evaluation of a prediction model for colorectal cancer: retrospective analysis of 2.5 million patient records",2017,"Cancer Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030235445&doi=10.1002%2fcam4.1183&partnerID=40&md5=547fc0d93b0c48510db832af0edc1234","Earlier detection of colorectal cancer greatly improves prognosis, largely through surgical excision of neoplastic polyps. These include benign adenomas which can transform over time to malignant adenocarcinomas. This progression may be associated with changes in full blood count indices. An existing risk algorithm derived in Israel stratifies individuals according to colorectal cancer risk using full blood count data, but has not been validated in the UK. We undertook a retrospective analysis using the Clinical Practice Research Datalink. Patients aged over 40 with full blood count data were risk-stratified and followed up for a diagnosis of colorectal cancer over a range of time intervals. The primary outcome was the area under the receiver operating characteristic curve for the 18–24-month interval. We also undertook a case–control analysis (matching for age, sex, and year of risk score), and a cohort study of patients undergoing full blood count testing during 2012, to estimate predictive values. We included 2,550,119 patients. The area under the curve for the 18–24-month interval was 0.776 [95% confidence interval (CI): 0.771, 0.781]. Performance improves as the time interval reduces. The area under the curve for the age-matched case–control analysis was 0.583 [0.574, 0.591]. For the population risk-scored in 2012, the positive predictive value at 99.5% specificity was 8.8% with negative predictive value 99.6%. The algorithm offers an additional means of identifying risk of colorectal cancer, and could support other approaches to early detection, including screening and active case finding. © 2017 The Authors. Cancer Medicine published by John Wiley & Sons Ltd.","Blood cell count; colorectal neoplasms; early detection of cancer; electronic health records; machine learning; risk assessment","hemoglobin; adult; aged; Article; blood cell count; cancer risk; cancer screening; case control study; cohort analysis; colorectal cancer; controlled study; early cancer diagnosis; female; hematocrit; hemoglobin blood level; human; major clinical study; male; predictive value; priority journal; receiver operating characteristic; red blood cell distribution width; retrospective study; risk assessment; sensitivity and specificity; thrombocyte volume",2-s2.0-85030235445
"Riaz F., Niazi M.A.","Towards social autonomous vehicles: Efficient collision avoidance scheme using Richardson’s arms race model",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732691&doi=10.1371%2fjournal.pone.0186103&partnerID=40&md5=4ec32e5e68228b92191dfb98501a3f53","This paper presents the concept of a social autonomous agent to conceptualize such Autonomous Vehicles (AVs), which interacts with other AVs using social manners similar to human behavior. The presented AVs also have the capability of predicting intentions, i.e. mentalizing and copying the actions of each other, i.e. mirroring. Exploratory Agent Based Modeling (EABM) level of the Cognitive Agent Based Computing (CABC) framework has been utilized to design the proposed social agent. Furthermore, to emulate the functionality of mentalizing and mirroring modules of proposed social agent, a tailored mathematical model of the Richardson’s arms race model has also been presented. The performance of the proposed social agent has been validated at two levels–firstly it has been simulated using NetLogo, a standard agent-based modeling tool and also, at a practical level using a prototype AV. The simulation results have confirmed that the proposed social agent-based collision avoidance strategy is 78.52% more efficient than Random walk based collision avoidance strategy in congested flock-like topologies. Whereas practical results have confirmed that the proposed scheme can avoid rear end and lateral collisions with the efficiency of 99.876% as compared with the IEEE 802.11n-based existing state of the art mirroring neuron-based collision avoidance scheme. © 2017 Riaz, Niazi. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"avoidance behavior; exploratory research; human; human experiment; nerve cell; race; simulation; behavior; car; car driving; computer simulation; devices; imitation; machine learning; prevention and control; procedures; psychological model; psychology; robotics; system analysis; traffic accident; Accidents, Traffic; Automobile Driving; Automobiles; Computer Simulation; Humans; Imitative Behavior; Intention; Machine Learning; Models, Psychological; Robotics; Systems Analysis",2-s2.0-85031732691
"Goodswen S.J., Kennedy P.J., Ellis J.T.","On the application of reverse vaccinology to parasitic diseases: a perspective on feature selection and ranking of vaccine candidates",2017,"International Journal for Parasitology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029527552&doi=10.1016%2fj.ijpara.2017.08.004&partnerID=40&md5=4dfccf6566fa86b815e34d44b3f2920a","Reverse vaccinology has the potential to rapidly advance vaccine development against parasites, but it is unclear which features studied in silico will advance vaccine development. Here we consider Neospora caninum which is a globally distributed protozoan parasite causing significant economic and reproductive loss to cattle industries worldwide. The aim of this study was to use a reverse vaccinology approach to compile a worthy vaccine candidate list for N. caninum, including proteins containing pathogen-associated molecular patterns to act as vaccine carriers. The in silico approach essentially involved collecting a wide range of gene and protein features from public databases or computationally predicting those for every known Neospora protein. This data collection was then analysed using an automated high-throughput process to identify candidates. The final vaccine list compiled was judged to be the optimum within the constraints of available data, current knowledge, and existing bioinformatics programs. We consider and provide some suggestions and experience on how ranking of vaccine candidate lists can be performed. This study is therefore important in that it provides a valuable resource for establishing new directions in vaccine research against neosporosis and other parasitic diseases of economic and medical importance. © 2017 Australian Society for Parasitology","In silico vaccine discovery; Machine learning; Neospora caninum; Pathogen-associated molecular patterns; Reverse vaccinology","pathogen associated molecular pattern; protozoal protein; protozoal vaccine; machine learning; parasite; parasitic disease; pathogen; pathogenicity; vaccination; vaccine; amino acid sequence; Article; bioinformatics; controlled study; expressed sequence tag; immune response; molecular genetic phenomena and functions; Neospora caninum; neosporosis; nonhuman; parasitosis; prediction; protein analysis; reverse vaccinology; RNA sequence; Bos; Neospora; Neospora caninum; Protozoa",2-s2.0-85029527552
"Ardakani A., Leduc-Primeau F., Onizawa N., Hanyu T., Gross W.J.","VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011660684&doi=10.1109%2fTVLSI.2017.2654298&partnerID=40&md5=5dc7ac7a06b8125436eba82c6e365c35","The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing (SC) has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral SC. The proposed architecture has been implemented on a Virtex7 field-programmable gate array, resulting in 45% and 62% average reductions in area and latency compared with the best reported architecture in the literature. We also synthesize the circuits in a 65-nm CMOS technology, and we show that the proposed integral stochastic architecture results in up to 21% reduction in energy consumption compared with the binary radix implementation at the same misclassification rate. Due to fault-tolerant nature of stochastic architectures, we also consider a quasi-synchronous implementation that yields 33% reduction in energy consumption with respect to the binary radix implementation without any compromise on performance. © 1993-2012 IEEE.","Deep neural network (DNN); hardware implementation; integral stochastic computation; machine learning; pattern recognition; VLSI","Bins; Complex networks; Computer hardware description languages; Deep neural networks; Energy utilization; Field programmable gate arrays (FPGA); Hardware; Network architecture; VLSI circuits; Efficient implementation; Hardware implementations; High-speed operation; Misclassification rates; Proposed architectures; Stochastic algorithms; Stochastic computations; Stochastic computing; Stochastic systems",2-s2.0-85011660684
"Geeleher P., Zhang Z., Wang F., Gruener R.F., Nath A., Morrison G., Bhutra S., Grossman R.L., Huang R.S.","Discovering novel pharmacogenomic biomarkers by imputing drug response in cancer patients from large genomics studies",2017,"Genome Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030633782&doi=10.1101%2fgr.221077.117&partnerID=40&md5=2cccbb8cc5293e6432b0a96ba4889309","Obtaining accurate drug response data in large cohorts of cancer patients is very challenging; thus, most cancer pharmacogenomics discovery is conducted in preclinical studies, typically using cell lines and mouse models. However, these platforms suffer from serious limitations, including small sample sizes. Here, we have developed a novel computational method that allows us to impute drug response in very large clinical cancer genomics data sets, such as The Cancer Genome Atlas (TCGA). The approach works by creating statistical models relating gene expression to drug response in large panels of cancer cell lines and applying these models to tumor gene expression data in the clinical data sets (e.g., TCGA). This yields an imputed drug response for every drug in each patient. These imputed drug response data are then associated with somatic genetic variants measured in the clinical cohort, such as copy number changes or mutations in protein coding genes. These analyses recapitulated drug associations for known clinically actionable somatic genetic alterations and identified new predictive biomarkers for existing drugs. © 2017 Geeleher et al.",,"2 [4 (2 dimethylaminoethoxy)phenyl] 4 (1 hydroxyimino 5 indanyl) 5 (4 pyridinyl) 1h imidazole; antineoplastic agent; B Raf kinase; cyclin dependent kinase 4; epidermal growth factor receptor 2; erlotinib; gefitinib; imatinib; K ras protein; lapatinib; microRNA 21; n (2,3 dihydroxypropoxy) 3,4 difluoro 2 (2 fluoro 4 iodoanilino)benzamide; n [3 (5 chloro 1h pyrrolo[2,3 b]pyridine 3 carbonyl) 2,4 difluorophenyl]propanesulfonamide; nilotinib; nutlin 3; palbociclib; protein tyrosine kinase inhibitor; sorafenib; sunitinib; trastuzumab; tumor marker; vinorelbine tartrate; Article; breast cancer; CAMA-1 cell line; cancer cell line; cancer patient; clinical study; coding; cohort analysis; controlled study; copy number variation; drug response; gene expression; gene mutation; genetic variability; genomics; human; human cell; machine learning; malignant neoplasm; pharmacogenomics; priority journal; tumor gene",2-s2.0-85030633782
"Yoshino Y., Miyajima T., Lu H., Tan J., Kim H., Murakami S., Aoki T., Tachibana R., Hirano Y., Kido S.","Automatic classification of lung nodules on MDCT images with the temporal subtraction technique",2017,"International Journal of Computer Assisted Radiology and Surgery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019139416&doi=10.1007%2fs11548-017-1598-1&partnerID=40&md5=e55649d5d04b605d5b0416ecefb8d2a5","Purpose: A temporal subtraction (TS) image is obtained by subtracting a previous image, which is warped to match the structures of the previous image and the related current image. The TS technique removes normal structures and enhances interval changes such as new lesions and substitutes in existing abnormalities from a medical image. However, many artifacts remaining on the TS image can be detected as false positives. Method: This paper presents a novel automatic segmentation of lung nodules using the Watershed method, multiscale gradient vector flow snakes and a detection method using the extracted features and classifiers for small lung nodules (20 mm or less). Result: Using the proposed method, we conduct an experiment on 30 thoracic multiple-detector computed tomography cases including 31 small lung nodules. Conclusion: The experimental results indicate the efficiency of our segmentation method. © 2017, CARS.","CAD; Lung nodule; Machine learning; MDCT; Temporal subtraction","accuracy; Article; artificial neural network; class featuring information compression; classifier; clinical article; computed tomography scanner; evaluation study; false positive result; feature extraction; Fisher linear discriminant classifier; human; image processing; image segmentation; image subtraction; lung nodule; Mahalanobis distance classifier; multidetector computed tomography; multiscale gradient vector flow snake method; priority journal; receiver operating characteristic; tumor classification; Watershed method",2-s2.0-85019139416
"Chen F., Wang Z., Wang C., Xu Q., Liang J., Xu X., Yang J., Wang C., Jiang T., Yu R.","Application of reverse docking for target prediction of marine compounds with anti-tumor activity",2017,"Journal of Molecular Graphics and Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029674065&doi=10.1016%2fj.jmgm.2017.09.015&partnerID=40&md5=bbaf3718a90bf1aa4204bd29bdbba738","A large number of structures of anti-cancer drug targets have been solved and deposited to the protein data bank already. Identification of the targets for marine compounds with anti-tumor activity presents a challenge for marine natural products scientists. In this study, fast and efficient computational reverse docking was applied to predict the probable targeting proteins of the marine compounds with anti-tumor activity. Crystal structures of the proteins involved in tumor genesis, growth and metastasis were collected from PDB to construct the anti-tumor protein database (APD) for reverse docking. Two non-commercial docking programs, AutoDock Vina and LeDock, were used to perform the docking. Our results suggest that reverse docking is efficient for target fishing of compounds with known anti-tumor activities. In addition, the results show that performance of reverse docking using LeDock is superior to that using AutoDock Vina. Overall, reverse docking is a fast and efficient computational method to identify the probable target of the compounds with anti-tumor activities, and it can be complementary to the biological testing methods. © 2017 Elsevier Inc.","Anti-tumor compounds; Anti-tumor protein database; AutoDock vina; LeDock; Reverse docking; Target fishing; Target prediction","Computational efficiency; Fisheries; Forecasting; Proteins; Testing; Anti-tumors; Autodock vinas; LeDock; Protein database; Target prediction; Tumors; natural product; protein; antineoplastic activity; Article; AutoDock Vina; carcinogenesis; computational fluid dynamics; crystal structure; drug protein binding; LeDock; machine learning; metastasis; molecular biology; molecular docking; prediction; priority journal; protein database; protein targeting; reverse docking; tumor growth",2-s2.0-85029674065
"Guépié B.K., Sciolla B., Millioz F., Almar M., Delachartre P.","Discrimination between emboli and artifacts for outpatient transcranial Doppler ultrasound data",2017,"Medical and Biological Engineering and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012941594&doi=10.1007%2fs11517-017-1624-z&partnerID=40&md5=3f523424608eda465cfd37e8be245b3f","This paper addresses the detection of emboli in transcranial Doppler ultrasound data acquired from an original portable device. The challenge is the removal of several artifacts (motion and voice) intrinsically related to long-duration (up to 1 h 40 mn per patient) outpatient signals monitoring from this device, as well as high intensities due to the stochastic nature of blood flow. This paper proposes an adapted removal procedure. This firstly consists of reducing the background noise and detecting the blood flow in the time–frequency domain using a likelihood method for contour detection. Then, a hierarchical extraction of features from magnitude and bounding boxes is achieved for the discrimination of emboli and artifacts. After processing of the long-duration outpatient signals, the number of artifacts predicted as emboli is considerably reduced (by 92% for some parameter values) between the first and the last step of our algorithm. © 2017, International Federation for Medical and Biological Engineering.","Artifacts rejection; Emboli detection; Likelihood; Spectral kurtosis; Time–frequency approach; Transcranial Doppler; Ultrasound","Blood; Frequency domain analysis; Hemodynamics; Non Newtonian flow; Stochastic systems; Ultrasonics; Artifacts rejection; Frequency approach; Likelihood; Spectral Kurtosis; Transcranial Doppler; Ultrasonic applications; algorithm; Article; blood flow; comparative study; controlled study; embolism; Fourier transformation; heart cycle; kernel method; machine learning; magnitude estimation method; outpatient; priority journal; probability; surface property; transcranial doppler; transcranial Doppler ultrasonography; velocity",2-s2.0-85012941594
"Xia J., Bombrun L., Berthoumieu Y., Germain C., Du P.","Spectral-Spatial Rotation Forest for Hyperspectral Image Classification",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023631935&doi=10.1109%2fJSTARS.2017.2720259&partnerID=40&md5=15e5cbad1c09ae2e965cc5b331b471df","Rotation Forest (RoF) is a recent powerful decision tree (DT) ensemble classifier of hyperspectral images. RoF exploits random feature selection and data transformation techniques to improve both the diversity and accuracy of DT classifiers. Conventional RoF only considers data transformation on spectral information. To overcome this limitation, we propose a spectral and spatial RoF (SSRoF), to further improve the performance. In SSRoF, pixels are first smoothed by the multiscale (MS) spatial weight mean filtering. Then, spectral-spatial data transformation, which is based on a joint spectral and spatial rotation matrix, is introduced into the RoF. Finally, classification results obtained from each scale are integrated by a majority voting rule. Experimental results on two datasets indicate the competitive performance of the proposed method when compared to other state-of-The-Art methods. © 2008-2012 IEEE.","Classification ensemble; hyperspectral images (HSIs); rotation forest (RoF); spectral-spatial transformation","Decision trees; Forestry; Hyperspectral imaging; Independent component analysis; Linear transformations; Matrix algebra; Metadata; Personnel training; Rotation; Spectroscopy; Spectrum analysis; Classification ensembles; Matrix decomposition; Radio frequencies; Rotation forests; Spatial transformation; Image classification; data set; image classification; machine learning; matrix; multispectral image; pixel; spatial resolution; spectral analysis",2-s2.0-85023631935
"Nascimento J.C., Carneiro G.","Deep Learning on Sparse Manifolds for Faster Object Segmentation",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023604068&doi=10.1109%2fTIP.2017.2725582&partnerID=40&md5=7a482760e46fb442420fe1c4b9de5a1a","We propose a new combination of deep belief networks and sparse manifold learning strategies for the 2D segmentation of non-rigid visual objects. With this novel combination, we aim to reduce the training and inference complexities while maintaining the accuracy of machine learning-based non-rigid segmentation methodologies. Typical non-rigid object segmentation methodologies divide the problem into a rigid detection followed by a non-rigid segmentation, where the low dimensionality of the rigid detection allows for a robust training (i.e., a training that does not require a vast amount of annotated images to estimate robust appearance and shape models) and a fast search process during inference. Therefore, it is desirable that the dimensionality of this rigid transformation space is as small as possible in order to enhance the advantages brought by the aforementioned division of the problem. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection space. Furthermore, we propose the use of deep belief networks to allow for a training process that can produce robust appearance models without the need of large annotated training sets. We test our approach in the segmentation of the left ventricle of the heart from ultrasound images and lips from frontal face images. Our experiments show that the use of sparse manifolds and deep belief networks for the rigid detection stage leads to segmentation results that are as accurate as the current state of the art, but with lower search complexity and training processes that require a small amount of annotated training data. © 1992-2012 IEEE.","Deep belief netwolks; defonnable objects; non-rigid segmentation; sparse manifold","Automobile engine manifolds; Complex networks; Deep learning; Education; Flow visualization; Learning systems; Personnel training; Robustness (control systems); Ultrasonic applications; Annotated training data; Deep belief networks; Low dimensionality; Object segmentation; Rigid transformations; Search problem; Segmentation results; Shape; Image segmentation",2-s2.0-85023604068
"Alimjan G., Sun T., Jumahun H., Guan Y., Zhou W., Sun H.","A Hybrid Classification Approach Based on Support Vector Machine and K-Nearest Neighbor for Remote Sensing Data",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020169245&doi=10.1142%2fS0218001417500343&partnerID=40&md5=fb792847d11ac82523a9223f0fab689a","Analysis and classification for remote sensing landscape based on remote sensing imagery is a popular research topic. In this paper, we propose a new remote sensing data classifier by incorporating the support vector machine (SVM) learning information into the K-nearest neighbor (KNN) classifier. The SVM is well known for its extraordinary generalization capability even with limited learning samples, and it is very useful for remote sensing applications as data samples are usually limited. The KNN has been widely used in data classification due to its simplicity and effectiveness. However, the KNN is instance-based and needs to keep all the training samples for classification, which could cause not only high computation complexity but also overfitting problems. Meanwhile, the performance of the KNN classifier is sensitive to the neighborhood size K and how to select the value of the parameter K relies heavily on practice and experience. Based on the observations that the SVM can contribute to the KNN on the problems of smaller training samples size as well as the selection of the parameter K, we propose a support vector nearest neighbor (abbreviated as SV-NN) hybrid classification approach which can simplify the parameter selection while maintaining classification accuracy. The proposed approach is consist of two stages. In the first stage, the SVM is performed on the training samples to obtain the reduced support vectors (SVs) for each of the sample categories. In the second stage, a nearest neighbor classifier (NNC) is used to classify a testing sample, i.e. the average Euclidean distance between the testing data point to each set of SVs from different categories is calculated and the NNC identifies the category with minimum distance. To evaluate the effectiveness of the proposed approach, firstly experiments of classification for samples from remote sensing data are evaluated, and then experiments of identifying different land covers regions in the remote sensing images are evaluated. Experimental results show that the SV-NN approach maintains good classification accuracy while reduces the training samples compared with the conventional SVM and KNN classification model. © 2017 World Scientific Publishing Company.","K-nearest neighbor; land cover classification; Support vector machines; SV-NN classification","Classifiers; Image reconstruction; Motion compensation; Nearest neighbor search; Remote sensing; Sampling; Support vector machines; Vectors; Classification accuracy; Generalization capability; K-nearest neighbor classifiers (KNN); K-nearest neighbors; Land cover classification; Nearest neighbor classifiers; Practice and experience; Remote sensing applications; Classification (of information)",2-s2.0-85020169245
"Sui Y., He Y., Yu W., Li Y.","A hybrid strategy to control uncertain nonlinear chaotic system",2017,"Chinese Physics B",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031023469&doi=10.1088%2f1674-1056%2f26%2f10%2f100503&partnerID=40&md5=bd01140a7bedc6a61dcb65792b9f48ee","In this paper, a new method, based on firefly algorithm (FA) and extreme learning machine (ELM), is proposed to control chaos in nonlinear system. ELM is an efficient predicted and classified tool, and can match and fit nonlinear systems efficiently. Hence, mathematical model of uncertain nonlinear system is obtained indirectly. For higher fitting accuracy, a novel swarm intelligence algorithm FA is drawn in our proposed way. The main advantage is that our proposed method can remove the limitation that mathematical model must be known clearly and can be applied to unknown nonlinear chaotic system. © 2017 Chinese Physical Society and IOP Publishing Ltd.","chaos; extreme learning machine; firefly algorithm","Bioluminescence; Chaos theory; Chaotic systems; Knowledge acquisition; Learning systems; Nonlinear systems; Optimization; Extreme learning machine; Firefly algorithms; Fitting accuracy; Hybrid strategies; Nonlinear chaotic systems; Swarm intelligence algorithms; Uncertain nonlinear systems; Learning algorithms",2-s2.0-85031023469
"Rudek R.","The single machine total weighted completion time scheduling problem with the sum-of-processing time based models: Strongly NP-hard",2017,"Applied Mathematical Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028006023&doi=10.1016%2fj.apm.2017.05.034&partnerID=40&md5=ee9f74d6ff49f4acd01d5b531531cf24","Although the single machine scheduling problem to minimize the total weighted completion times with the sum-of-processing time based learning or aging effects have been known for a decade, it is still an open question whether these problems are strongly NP-hard. We resolve this issue and prove them to be strongly NP-hard with the learning effect as well as with the aging effect. Furthermore, we construct an exact parallel branch and bound algorithm for the problem with general sum-of-processing time based models, which can solve optimally moderate problem instances in reasonable time. © 2017 Elsevier Inc.","Aging effect; Computational complexity; Learning effect; Parallel branch and bound; Scheduling; Strongly NP-hard",,2-s2.0-85028006023
"Lee C.-S., Wang M.-H., Yang S.-C., Hung P.-H., Lin S.-W., Shuo N., Kubota N., Chou C.-H., Chou P.-C., Kao C.-H.","FML-based Dynamic Assessment Agent for Human-Machine Cooperative System on Game of Go",2017,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028932070&doi=10.1142%2fS0218488517500295&partnerID=40&md5=c2903237f5eacf29e607f9df266dc19e","In this paper, we demonstrate the application of Fuzzy Markup Language (FML) to construct an FML-based Dynamic Assessment Agent (FDAA), and we present an FML-based Human-Machine Cooperative System (FHMCS) for the game of Go. The proposed FDAA comprises an intelligent decision-making and learning mechanism, an intelligent game bot, a proximal development agent, and an intelligent agent. The intelligent game bot is based on the open-source code of Facebook's Darkforest, and it features a representational state transfer application programming interface mechanism. The proximal development agent contains a dynamic assessment mechanism, a GoSocket mechanism, and an FML engine with a fuzzy knowledge base and rule base. The intelligent agent contains a GoSocket engine and a summarization agent that is based on the estimated win rate, real-time simulation number, and matching degree of predicted moves. Additionally, the FML for player performance evaluation and linguistic descriptions for game results commentary are presented. We experimentally verify and validate the performance of the FDAA and variants of the FHMCS by testing five games in 2016 and 60 games of Google's Master Go, a new version of the AlphaGo program, in January 2017. The experimental results demonstrate that the proposed FDAA can work effectively for Go applications. © 2017 World Scientific Publishing Company.","decision support engine; FAIR darkforest Go engine; Fuzzy markup language; prediction agent; robot engine","Application programming interfaces (API); Decision making; Decision support systems; Engines; Intelligent agents; Interface states; Knowledge based systems; Markup languages; Open source software; Open systems; Software testing; Decision supports; Fuzzy markup languages; Fuzzy markup languages (FML); Human-machine cooperative system; Intelligent decision making; Linguistic descriptions; Prediction agent; Representational state transfer; Dynamics",2-s2.0-85028932070
"Liu D., Liu C., Fu Q., Li T., Imran K.M., Cui S., Abrar F.M.","ELM evaluation model of regional groundwater quality based on the crow search algorithm",2017,"Ecological Indicators",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020618090&doi=10.1016%2fj.ecolind.2017.06.009&partnerID=40&md5=db78aab20bf3db92bab4c5a6c0ef27f8","According to the multi-parameter evaluation of groundwater quality, an evaluation model of groundwater quality based on the improved Extreme Learning Machine (ELM) was proposed to resolve fuzziness of the water quality evaluation and incompatibility of water parameters. A training sample set and testing sample set were randomly generated according to the classification standards of groundwater quality, then Crow Search Algorithm (CSA) was used to optimize the input weights and thresholds of hidden-layer neurons of the ELM; thus, the CSA-ELM evaluation model of groundwater quality was constructed based on optimization of the ELM by the CSA. Base on the training sample set and testing sample set, the CSA-ELM model was tested. The test results indicate that the evaluating precision and generalization ability of the CSA-ELM model reach a high level and can be used for comprehensive evaluations of groundwater quality. The Jiansanjiang Administration in Heilongjiang Province, China, was used as an example; the groundwater quality of 15 farms in this region was evaluated based on the CSA-ELM model. The groundwater quality in this region was generally good, and the groundwater quality appeared to have spatial distribution characteristics. Compared with the Nemerow Index Method (NIM), the CSA-ELM evaluation model of groundwater quality is more reasonable and can be used for the comprehensive evaluation of groundwater quality. The stability of the NIM, ELM model, back propagation (BP) model and CSA-ELM model was analyzed using the theory of serial number summation and Spearman's correlation coefficient. The stability of the NIM and BP model in groundwater quality evaluation was poor, while the stability of the ELM model and CSA-ELM model was relatively superior. The ranked results of stability are CSA-ELM model > ELM model > NIM > BP model. The reliability of the NIM, ELM model, BP model and CSA-ELM model was analyzed using the theory of distinction degree. The reliability of the NIM was not good, although its distinction degree was large; the distinction degrees of the ELM model, BP model and CSA-ELM model were close to each other. The ranked results of reliability are CSA-ELM model > ELM model > BP model. The CSA-ELM model can provide a stable and reliable evaluation method for the evaluation of related fields and thus has important practical applicability. © 2017 Elsevier Ltd","Crow search algorithm; Extreme learning machine; Groundwater quality; Reliability; Stability","Backpropagation; Convergence of numerical methods; Groundwater; Knowledge acquisition; Learning algorithms; Learning systems; Optimization; Radial basis function networks; Reliability; Reliability theory; Sampling; Stability; Standards; Water quality; Classification standard; Comprehensive evaluation; Distribution characteristics; Extreme learning machine; Groundwater quality evaluations; Reliable evaluation method; Search Algorithms; Water quality evaluation; Quality control; algorithm; back propagation; groundwater; model test; numerical model; optimization; reliability analysis; spatial distribution; water quality; China; Heilongjiang; Jiansanjiang",2-s2.0-85020618090
"Ghahramani A., Karvigh S.A., Becerik-Gerber B.","HVAC system energy optimization using an adaptive hybrid metaheuristic",2017,"Energy and Buildings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025164293&doi=10.1016%2fj.enbuild.2017.07.053&partnerID=40&md5=c48d8ec972285faa193435ab67da0311","Previous research efforts, for optimizing energy usage of HVAC systems, require either mathematical models of HVAC systems to be built or they require substantial historical operational data for learning optimal operational settings. We introduce a model-free control policy that begins learning optimal settings with no prior historical data and optimizes HVAC operations. The control policy is an adaptive hybrid metaheuristic that uses real-time data, stored in building automation systems (e.g., gas/electricity consumption, weather, and occupancy). It finds optimal setpoints at the building level and controls setpoints accordingly. The algorithm consists of metaheuristic (k-nearest neighbor stochastic hill climbing), machine learning (regression decision tree), and self-tuning (recursive brute-force search) components. The control policy uses smart selection of daily setpoints as its control basis, making the control schema complementary to legacy building management systems. To evaluate our approach, we used the DOE reference small office building in all U.S. climate zones and simulated different control policies using EnergyPlus. The proposed algorithm resulted in 31.17% energy savings compared to the baseline operations (22.5 °C and 3 K). The algorithm has a superior performance in all climate zones for the goodness of measure (i.e., normalized root mean square error) with a value of 0.047. © 2017 Elsevier B.V.","Adaptive learning; Energy efficiency; HVAC system; Online learning; Optimal control; Setpoint optimization","Adaptive control systems; Automation; Climate control; Data mining; Decision trees; Education; Energy efficiency; Energy utilization; Intelligent buildings; Legacy systems; Machine components; Mean square error; Nearest neighbor search; Office buildings; Optimal control systems; Real time systems; Stochastic systems; Trees (mathematics); Adaptive learning; HVAC system; Online learning; Optimal controls; Set-point optimization; Optimization",2-s2.0-85025164293
"Dong G., Chen J., Zhao F.","Incipient Bearing Fault Feature Extraction Based on Minimum Entropy Deconvolution and K-Singular Value Decomposition",2017,"Journal of Manufacturing Science and Engineering, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028551586&doi=10.1115%2f1.4037419&partnerID=40&md5=a0b2c8822669fa9811463ba83d9106e8","Machinery condition monitoring and fault diagnosis are essential for early detection of equipment malfunctions or failures, which insure productivity, quality, and safety in the manufacturing process. This paper aims at extracting fault features of rolling element bearings at the incipient fault stage. K-singular value decomposition (K-SVD), one technique for sparse representation of signals, is used for study. In K-SVD, its dictionary is trained from data by machine learning techniques, which allows more flexibility to adapt to variation of real signals than the predefined dictionaries. Analysis on simulated bearing signals and real signals shows that K-SVD can give better bearing fault features than the predefined dictionaries such as wavelet dictionaries. However, during our simulation study, K-SVD was found to have large representation error under heavy noise. To reduce the noise effect, minimum entropy deconvolution (MED) is used as a prefilter. The combination of MED and K-SVD is proposed for incipient bearing fault detection. The method is verified by simulation and experimental study. It is shown that the proposed method can effectively extract the impulsive fault feature of the tested bearing at its incipient fault stage. © 2017 by ASME.","dictionary learning; K-SVD; machinery condition monitoring; minimum entropy deconvolution; rolling element bearing; sparse representation","Bearings (machine parts); Condition monitoring; Entropy; Learning systems; Machinery; Roller bearings; Singular value decomposition; Dictionary learning; Machinery condition monitoring; Minimum entropy deconvolution; Rolling Element Bearing; Sparse representation; Fault detection",2-s2.0-85028551586
"Golay J., Leuenberger M., Kanevski M.","Feature selection for regression problems based on the Morisita estimator of intrinsic dimension",2017,"Pattern Recognition",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020014509&doi=10.1016%2fj.patcog.2017.05.008&partnerID=40&md5=3e991ca954a78af3b34e8280f2f30436","Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results, and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. © 2017 Elsevier Ltd","Data mining; Feature selection; Intrinsic dimension; Measure of relevance; Morisita index","Data acquisition; Data mining; Digital storage; Filtration; Information management; Learning systems; Extreme learning machine; Feature selection methods; Graphical representations; Intrinsic dimensions; Measure of relevance; Morisita index; Numerical experiments; Redundant features; Feature extraction",2-s2.0-85020014509
"Gónzalez S., García S., Lázaro M., Figueiras-Vidal A.R., Herrera F.","Class Switching according to Nearest Enemy Distance for learning from highly imbalanced data-sets",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020010517&doi=10.1016%2fj.patcog.2017.04.028&partnerID=40&md5=b37e44a37b4772f84349d261deb4acb1","The imbalanced data classification has been deeply studied by the machine learning practitioners over the years and it is one of the most challenging problems in the field. In many real-life situations, the under representation of a class in contrary to the rest commonly produces the tendency to ignore the minority class, this being normally the target of the problem. Consequently, many different techniques have been proposed. Among those, the ensemble approaches have resulted to be very reliable. New ways of generating ensembles have also been studied for standard classification. In particular, Class Switching, as a mechanism to produce training perturbed sets, has been proved to perform well in slightly imbalanced scenarios. In this paper, we analyze its potential to deal with highly imbalanced problems, fighting against its major limitations. We introduce a novel ensemble approach based on Switching with a new technique to select the switched examples based on Nearest Enemy Distance. We compare the resulting SwitchingNED with five distinctive ensemble-based approaches, with different combinations of sampling techniques. With a better performance, SwitchingNED is settled as one of best approaches on the field. © 2017 Elsevier Ltd","Class Switching; Ensembles; Imbalanced classification; Preprocessing","Learning systems; Class-switching; Ensemble approaches; Ensembles; Imbalanced classification; Imbalanced data; Imbalanced Data-sets; Preprocessing; Sampling technique; Switching",2-s2.0-85020010517
"Han M., Zhang R., Xu M.","Multivariate Chaotic Time Series Prediction Based on ELM–PLSR and Hybrid Variable Selection Algorithm",2017,"Neural Processing Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016927544&doi=10.1007%2fs11063-017-9616-4&partnerID=40&md5=b1be1abf4e3026df023a749a2bd58cd6","In this paper, a novel method (Hybrid–ELM–PLSR) is proposed based on hybrid variable selection algorithm and improved extreme learning machine (ELM) for multivariate chaotic time series prediction. The hybrid variable selection algorithm combines the advantages of filter and wrapper, effectively balancing the calculation speed and prediction accuracy. Moreover, for ELM, multicollinearity, which can result in ill-condition, is always existent among the hidden layer output matrix. And the optimal number of hidden nodes is also difficult to be determined. Therefore,in order to overcome these problems, an improved ELM (ELM–PLSR) is proposed based on partial least square regression (PLSR). It can effectively enhance the stability performance and prediction performance of ELM. Hybrid–ELM–PLSR can be divided into three stages. At first, filter is used to rearrange the input variables through the correlations with desired variables. Then wrapper is used to select the optimal variable subset through evaluating the prediction performance of different subsets. Finally, ELM–PLSR is used to build the prediction model. The simulation experiment results based on San Francisco river runoff dataset demonstrate that the proposed method is effective for multivariate chaotic time series. And the prediction accuracy and reliability are higher than other methods. © 2017, Springer Science+Business Media New York.","Extreme learning machine; Multivariate chaotic time series; Partial least square regression; Variable selection","Knowledge acquisition; Learning systems; Regression analysis; Time series; Extreme learning machine; Multivariate chaotic time-series; Optimal variables; Partial least square regression; Prediction accuracy; Prediction performance; Stability performance; Variable selection; Forecasting",2-s2.0-85016927544
"Ali A., Yangyu F.","κ-Sparse Autoencoder-Based Automatic Modulation Classification with Low Complexity",2017,"IEEE Communications Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021805638&doi=10.1109%2fLCOMM.2017.2717821&partnerID=40&md5=f39d1de2a4f1cd32675e7283e3385d5e","How to reduce complexity of the practical automatic modulation classification systems is a very active research area. Moreover, Keeping the classification accuracy to a near optimal level is an added challenge. Recently, three new classifiers have been proposed with reduced complexity, mainly: linear support vector machine classifier, approximate maximum likelihood classifier, and backpropogation neural networks classifier. However, these methods include the sorting process of the features z to form an ordered vector z employing Klog(K) comparison operations. Here, we propose a κ-sparse autoencoder-based classifer, with unsorted input data features and called it unsorted deep neural network(UDNN). Thus, we strive to omit the Klog(K) comparison operations. The results obtained using the UDNN classifier show improved performance when compared with the above three methods. Moreover, using Khighest hidden units to reconstruct input data further reduces the overall complexity of the AMC system. © 1997-2012 IEEE.","Automatic modulation classification; deep neural network; κ-sparse autoencoders","Complex networks; Input output programs; Learning systems; Maximum likelihood; Modulation; Support vector machines; Autoencoders; Automatic modulation classification; Classification accuracy; Linear Support Vector Machines; Maximum likelihood classifiers; Neural networks classifiers; Reduced complexity; Sorting process; Deep neural networks",2-s2.0-85021805638
"Leturiondo U., Salgado O., Ciani L., Galar D., Catelani M.","Architecture for hybrid modelling and its application to diagnosis and prognosis with missing data",2017,"Measurement: Journal of the International Measurement Confederation",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012901785&doi=10.1016%2fj.measurement.2017.02.003&partnerID=40&md5=6f9b3b6b3b1193a3b855c62d05e25c8a","The advances in technology involving internet of things, cloud computing and big data mean a new perspective in the calculation of reliability, maintainability, availability and safety by combining physics-based modelling with data-driven modelling. This paper proposes an architecture to implement hybrid modelling based on the fusion of real data and synthetic data obtained in simulations using a physics-based model. This architecture has two levels of analysis: an online process carried out locally and virtual commissioning performed in the cloud. The former results in failure detection analysis to avoid upcoming failures whereas the latter leads to both diagnosis and prognosis. The proposed hybrid modelling architecture is validated in the field of rotating machinery using time-domain and frequency-domain analysis. A multi-body model and a semi-supervised learning algorithm are used to perform the hybrid modelling. The state of a rolling element bearing is analysed and accurate results for fault detection, localisation and quantification are obtained. The contextual information increases the accuracy of the results; the results obtained by the model can help improve maintenance decision making and production scheduling. Future work includes a prescriptive analysis approach. © 2017 Elsevier Ltd","Condition based maintenance; Diagnosis; Hybrid modelling; Prognosis; Rolling element bearing; Synthetic data","Bearings (machine parts); Decision making; Diagnosis; Fault detection; Frequency domain analysis; Learning algorithms; Machinery; Production control; Roller bearings; Supervised learning; Time domain analysis; Virtual reality; Condition based maintenance; Hybrid modelling; Prognosis; Rolling Element Bearing; Synthetic data; Big data",2-s2.0-85012901785
"Uddin M.Z., Hassan M.M., Almogren A., Zuair M., Fortino G., Torresen J.","A facial expression recognition system using robust face features from depth videos and deep learning",2017,"Computers and Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018336757&doi=10.1016%2fj.compeleceng.2017.04.019&partnerID=40&md5=3851a8937900e2e339d93ad74882357f","This work proposes a depth camera-based robust facial expression recognition (FER) system that can be adopted for better human machine interaction. Although video-based facial expression analysis has been focused on by many researchers, there are still various problems to be solved in this regard such as noise due to illumination variations over time. Depth video data in the helps to make an FER system person-independent as pixel values in depth images are distributed based on distances from a depth camera. Besides, depth images should resolve some privacy issues as real identity of a user can be hidden. The accuracy of an FER system is much dependent on the extraction of robust features. Here, we propose a novel method to extract salient features from depth faces that are further combined with deep learning for efficient training and recognition. Eight directional strengths are obtained for each pixel in a depth image where signs of some top strengths are arranged to represent unique as well as robust face features, which can be denoted as Modified Local Directional Patterns (MLDP). The MLDP features are further processed by Generalized Discriminant Analysis (GDA) for better face feature extraction. GDA is an efficient tool that helps distinguishing MLDP features of different facial expressions by clustering the features from the same expression as close as possible and separating the features from different expressions as much as possible in a non-linear space. Then, MLDP-GDA features are applied with Deep Belief Network (DBN) for training different facial expressions. Finally, the trained DBN is used to recognize facial expressions in a depth video for testing. The proposed approach was compared with other traditional approaches in a standalone system where the proposed one showed its superiority by achieving mean recognition rate of 96.25% where the other approaches could make 91.67% at the best. The deep learning-based training and recognition of the facial expression features can also be undertaken with cloud computing to support many users and make the system faster than a standalone system. © 2017","Deep belief network; Depth image; Facial expression recognition; Generalized discriminant analysis; Modified local directional patterns","Cameras; Deep learning; Discriminant analysis; Distributed computer systems; Extraction; Feature extraction; Human computer interaction; Pixels; Deep belief networks; Depth image; Facial expression recognition; Generalized discriminant analysis; Local directional patterns; Face recognition",2-s2.0-85018336757
"Yang C., Wang X., Cheng L., Ma H.","Neural-Learning-Based Telerobot Control with Guaranteed Performance",2017,"IEEE Transactions on Cybernetics",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028713194&doi=10.1109%2fTCYB.2016.2573837&partnerID=40&md5=45003ff755d8c13cab774ca8801e131f","In this paper, a neural networks (NNs) enhanced telerobot control system is designed and tested on a Baxter robot. Guaranteed performance of the telerobot control system is achieved at both kinematic and dynamic levels. At kinematic level, automatic collision avoidance is achieved by the control design at the kinematic level exploiting the joint space redundancy, thus the human operator would be able to only concentrate on motion of robot's end-effector without concern on possible collision. A posture restoration scheme is also integrated based on a simulated parallel system to enable the manipulator restore back to the natural posture in the absence of obstacles. At dynamic level, adaptive control using radial basis function NNs is developed to compensate for the effect caused by the internal and external uncertainties, e.g., unknown payload. Both the steady state and the transient performance are guaranteed to satisfy a prescribed performance requirement. Comparative experiments have been performed to test the effectiveness and to demonstrate the guaranteed performance of the proposed methods. © 2013 IEEE.","Collision avoidance; guaranteed performance; neural networks (NNs); telerobot control","Collision avoidance; Control systems; Kinematics; Machine design; Radial basis function networks; Restoration; Comparative experiments; External uncertainties; Guaranteed performance; Neural networks (NNS); Prescribed performance; Radial basis functions; Robot's end effectors; Telerobot control; End effectors",2-s2.0-85028713194
"Zhang L., Zhang D., Sun M.-M., Chen F.-M.","Facial beauty analysis based on geometric feature: Toward attractiveness assessment application",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017596758&doi=10.1016%2fj.eswa.2017.04.021&partnerID=40&md5=271411330c6caf2c430abb2033233989","Facial beauty analysis has been an emerging subject of multimedia and biometrics. This paper aims at exploring the essence of facial beauty from the viewpoint of geometric characteristic toward an interactive attractiveness assessment (IAA) application. As a result, a geometric facial beauty analysis method is proposed from the perspective of machine learning. Due to the troublesome and subjective beauty labeling, the accurately labeled data scarcity is caused, and result in very few labeled data. Additionally, facial beauty is related to several typical features such as texture, color, etc., which, however, can be easily deformed by make-up. For addressing these issues, a semi-supervised facial beauty analysis framework that is characterized by feeding geometric feature into the intelligent attractiveness assessment system is proposed. For experimental study, we have established a geometric facial beauty (GFB) dataset including Asian male and female faces. Moreover, an existing multi-modal beauty (M2B) database including western and eastern female faces is also tested. Experiments demonstrate the effectiveness of the proposed method. Some new perspectives on the essence of beauty and the topic of facial aesthetic are revealed. The impact of this work lies in that it will attract more researchers in related areas for beauty exploration by using intelligent algorithms. Also, the significance lies in that it should well promote the diversity of expert and intelligent systems in addressing such challenging facial aesthetic perception and rating issue. © 2017 Elsevier Ltd","Attractiveness assessment; Facial aesthetic perceptron; Facial beauty analysis; Geometric feature; Semi-supervised learning","Intelligent systems; Learning algorithms; Learning systems; Supervised learning; Aesthetic perception; Analysis frameworks; Attractiveness assessment; Facial beauty analysis; Geometric characteristics; Geometric feature; Intelligent Algorithms; Semi- supervised learning; Geometry",2-s2.0-85017596758
"Vapnik V., Izmailov R.","Knowledge transfer in SVM and neural networks",2017,"Annals of Mathematics and Artificial Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013168333&doi=10.1007%2fs10472-017-9538-x&partnerID=40&md5=a24e54d637e32b1b6d9c7f4869f74f72","The paper considers general machine learning models, where knowledge transfer is positioned as the main method to improve their convergence properties. Previous research was focused on mechanisms of knowledge transfer in the context of SVM framework; the paper shows that this mechanism is applicable to neural network framework as well. The paper describes several general approaches for knowledge transfer in both SVM and ANN frameworks and illustrates algorithmic implementations and performance of one of these approaches for several synthetic examples. © 2017, Springer International Publishing Switzerland.","Classification; Frames; Intelligent teacher; Knowledge representation; Knowledge transfer; Learning theory; Neural network; Privileged information; Regression; Similarity control; Support vector machine",,2-s2.0-85013168333
"Gao H., Jian S., Peng Y., Liu X.","A subspace ensemble framework for classification with high dimensional missing data",2017,"Multidimensional Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962132687&doi=10.1007%2fs11045-016-0393-4&partnerID=40&md5=23a8fe8cbdac237e9fdd8faaab0dc17f","Real world classification tasks may involve high dimensional missing data. The traditional approach to handling the missing data is to impute the data first, and then apply the traditional classification algorithms on the imputed data. This method first assumes that there exist a distribution or feature relations among the data, and then estimates missing items with existing observed values. A reasonable assumption is a necessary guarantee for accurate imputation. The distribution or feature relations of data, however, is often complex or even impossible to be captured in high dimensional data sets, leading to inaccurate imputation. In this paper, we propose a complete-case projection subspace ensemble framework, where two alternative partition strategies, namely bootstrap subspace partition and missing pattern-sensitive subspace partition, are developed for incomplete datasets with even missing patterns and uneven missing patterns, respectively. Multiple component classifiers are then separately trained in these subspaces. After that, a final ensemble classifier is constructed by a weighted majority vote of component classifiers. In the experiments, we demonstrate the effectiveness of the proposed framework over eight high dimensional UCI datasets. Meanwhile, we apply the two proposed partition strategies over data sets with different missing patterns. As indicated, the proposed algorithm significantly outperforms existing imputation methods in most cases. © 2016, Springer Science+Business Media New York.","Extreme learning machine; High dimensional data; Missing data; Subspace ensemble","Clustering algorithms; Data handling; Data mining; Learning systems; Classification algorithm; Classification tasks; Component classifiers; Extreme learning machine; High dimensional data; Missing data; Subspace ensemble; Traditional approaches; Classification (of information)",2-s2.0-84962132687
"Shen W., Wu Y., Jia Y.","Compact discriminative object representation via weakly supervised learning for real-time visual tracking",2017,"IET Computer Vision",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029583985&doi=10.1049%2fiet-cvi.2016.0287&partnerID=40&md5=bb4d9c86e17290b5530dece9c9fc01fb","Object representations are of great importance for robust visual tracking. Although the high-dimensional representation can effectively encode the input data with more information, exploiting it in a real-time tracking system would be intractable and infeasible due to the high computational cost and memory requirements. In this study, the authors propose a compact discriminative object representation to achieve both good tracking accuracy and efficiency. An ensemble of weak training sets is generated based on the self-representative ability of tracking samples, which is applied to learn discriminative functions. Each candidate is represented by the concatenation of project values on all the weak training sets. Tracking is then carried out within a Bayesian inference framework where the classification score of the support vector machine is used to construct the observation model. The evaluations on TB50 benchmark dataset demonstrate that the proposed algorithm is much more computationally efficient than the state-of-the-art methods with comparable accuracy. © 2017, The Institution of Engineering and Technology.",,"Inference engines; Computational costs; Computationally efficient; Discriminative functions; Memory requirements; Object representations; Real-time tracking systems; State-of-the-art methods; Weakly supervised learning; Bayesian networks",2-s2.0-85029583985
"Nguyen D.T., Yoon H.S., Pham T.D., Park K.R.","Spoof detection for finger-vein recognition system using NIR camera",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030709290&doi=10.3390%2fs17102261&partnerID=40&md5=871ba31e32c59c56a71bada76f217b02","Finger-vein recognition, a new and advanced biometrics recognition method, is attracting the attention of researchers because of its advantages such as high recognition performance and lesser likelihood of theft and inaccuracies occurring on account of skin condition defects. However, as reported by previous researchers, it is possible to attack a finger-vein recognition system by using presentation attack (fake) finger-vein images. As a result, spoof detection, named as presentation attack detection (PAD), is necessary in such recognition systems. Previous attempts to establish PAD methods primarily focused on designing feature extractors by hand (handcrafted feature extractor) based on the observations of the researchers about the difference between real (live) and presentation attack finger-vein images. Therefore, the detection performance was limited. Recently, the deep learning framework has been successfully applied in computer vision and delivered superior results compared to traditional handcrafted methods on various computer vision applications such as image-based face recognition, gender recognition and image classification. In this paper, we propose a PAD method for near-infrared (NIR) camera-based finger-vein recognition system using convolutional neural network (CNN) to enhance the detection ability of previous handcrafted methods. Using the CNN method, we can derive a more suitable feature extractor for PAD than the other handcrafted methods using a training procedure. We further process the extracted image features to enhance the presentation attack finger-vein image detection ability of the CNN method using principal component analysis method (PCA) for dimensionality reduction of feature space and support vector machine (SVM) for classification. Through extensive experimental results, we confirm that our proposed method is adequate for presentation attack finger-vein image detection and it can deliver superior detection results compared to CNN-based methods and other previous handcrafted methods. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; NIR camera-based finger-vein recognition; Presentation attack detection; Spoof detection; Transfer learning","Cameras; Computer vision; Convolution; Face recognition; Image enhancement; Image processing; Infrared devices; Neural networks; Optical character recognition; Principal component analysis; Support vector machines; Vector spaces; Attack detection; Convolutional neural network; Finger-vein recognition; Spoof detection; Transfer learning; Palmprint recognition",2-s2.0-85030709290
"Feng Y., Lu B., Zhang D.","Multiscale morphological manifold for rolling bearing fault diagnosis",2017,"Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029456014&doi=10.1177%2f0954406216646803&partnerID=40&md5=57331e1cdbdd4fd5fc0000e9942c0708","The vibration signals of fault rolling bearing are high-dimensional information with complex components. In order to identify different classes of bearing fault, a new multiscale morphological manifold method based on multiscale morphology and manifold learning is proposed. The multiscale morphological manifold method consists of three main steps. Firstly, multiscale difference filter based on multiscale morphological transformation is applied to obtain multiscale observation results of each signal sample. Secondly, the nonlinear feature vectors of each signal sample are constructed according to the observation approach. Finally, manifold learning is introduced to extract the low-dimensional multiscale morphological manifold features through reducing the dimension of nonlinear features. The low-dimensional multiscale morphological manifold features can reveal the differences of signal classes, which are applicable for fault diagnosis. The performance of proposed method is tested by experimental data from bearings with different types of defects. Experimental verifications confirm that the proposed method is applicable and effective for rolling bearing fault diagnosis. © IMechE 2016.","fault diagnosis; manifold learning; multiscale morphology; Rolling bearing","Bearings (machine parts); Failure analysis; Roller bearings; Complex components; Difference filter; Experimental verification; Manifold learning; Morphological transformations; Multi scale morphology; Nonlinear features; Rolling bearings; Fault detection",2-s2.0-85029456014
"Wang H., Shi Y., Niu L., Tian Y.","Nonparallel Support Vector Ordinal Regression",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017153057&doi=10.1109%2fTCYB.2017.2682852&partnerID=40&md5=8c54fe0c7e2722790b53fb5383ea487a","Ordinal regression is a supervised learning problem where training samples are labeled by an ordinal scale. The ordering relation and nonmetric property of the label set distinguish it from the multiclass classification and metric regression. To better exploit the inherent structure in the label and benefit from the hidden information in data distribution, we propose a novel ordinal regression model, which is named as nonparallel support vector ordinal regression (NPSVOR) to emphasis the utilization of nonparallel proximal hyperplanes. The new model constructs a hyperplane for each rank such that the patterns of this rank lie in the close proximity while maintaining clear separation with the other ranks. Since the learning of hyperplanes can be carried out independently, NPSVOR can be trained in parallel. Furthermore, we design an efficient solver at the same time for training the hyperplanes in NPSVOR based on the alternating direction method of multipliers. Extensive experimentation demonstrates that NPSVOR yields a large and statistically significant improvement in terms of generalization performance and training speed against nine baselines. © 2013 IEEE.","Alternating direction method of multiplier (ADMM); nonparallel SVM; ordinal regression; proximal hyperplane; support vector machine (SVM)","Geometry; Alternating direction method of multipliers; Efficient solvers; Generalization performance; Hidden information; Modeling construct; Multi-class classification; Ordinal regression; Supervised learning problems; Regression analysis",2-s2.0-85017153057
"Lal B.K., Dux M.C., Sikdar S., Goldstein C., Khan A.A., Yokemick J., Zhao L.","Asymptomatic carotid stenosis is associated with cognitive impairment",2017,"Journal of Vascular Surgery",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023769830&doi=10.1016%2fj.jvs.2017.04.038&partnerID=40&md5=34e6b52d0d91448f304d700579c21bd1","Background Cerebrovascular risk factors (eg, hypertension, coronary artery disease) and stroke can lead to vascular cognitive impairment. The Asymptomatic Carotid Stenosis and Cognitive Function study evaluated the isolated impact of asymptomatic carotid stenosis (no prior ipsilateral or contralateral stroke or transient ischemic attack) on cognitive function. Cerebrovascular hemodynamic and carotid plaque characteristics were analyzed to elucidate potential mechanisms affecting cognition. Methods There were 82 patients with ≥50% asymptomatic carotid stenosis and 62 controls without stenosis but matched for vascular comorbidities who underwent neurologic, National Institutes of Health Stroke Scale, and comprehensive neuropsychological examination. Overall cognitive function and five domain-specific scores were computed. Duplex ultrasound with Doppler waveform and B-mode imaging defined the degree of stenosis, least luminal diameter, plaque area, and plaque gray-scale median. Breath-holding index (BHI) and microembolization were measured using transcranial Doppler. We assessed cognitive differences between stenosis patients and control patients and of stenosis patients with low vs high BHI and correlated cognitive function with microembolic counts and plaque characteristics. Results Stenosis and control patients did not differ in vascular risk factors, education, estimated intelligence, or depressive symptoms. Stenosis patients had worse composite cognitive scores (P =.02; Cohen's d = 0.43) and domain-specific scores for learning/memory (P =.02; d = 0.42) and motor/processing speed (P =.01; d = 0.65), whereas scores for executive function were numerically lower (P =.08). Approximately 49.4% of all stenosis patients were impaired in at least two cognitive domains. Precisely 50% of stenosis patients demonstrated a reduced BHI. Stenosis patients with reduced BHI performed worse on the overall composite cognitive score (t = −2.1; P =.02; d = 0.53) and tests for learning/memory (t = −2.7; P =.01; d = 0.66). Cognitive function did not correlate with measures of plaque burden (degree of stenosis, least luminal diameter, and plaque area) or with plaque gray-scale median. Conclusions Asymptomatic carotid stenosis is associated with cognitive impairment independent of known vascular risk factors for vascular cognitive impairment. Approximately 49.4% of these patients demonstrate impairment in at least two neuropsychological domains. The deficit is driven primarily by reduced motor/processing speed and learning/memory and is mild to moderate in severity. The mechanism for impairment is likely to be hemodynamic as evidenced by reduced cerebrovascular reserve and the likely result of hypoperfusion from a pressure drop across the stenosis in the presence of inadequate collateralization. © 2017 Society for Vascular Surgery",,"aged; artery embolism; asymptomatic disease; atherosclerotic plaque; breath holding index; carotid artery; carotid artery obstruction; cognition; cognitive defect; cognitive function test; comorbidity; Conference Paper; controlled study; depression; disease association; echography; education; executive function; female; hemodynamics; human; intelligence; learning; major clinical study; male; medical parameters; memory; motor performance; National Institutes of Health Stroke Scale; priority journal; prospective study; risk factor; transcranial doppler; transcranial Doppler ultrasonography; ultrasound transducer; asymptomatic disease; attention; brain circulation; carotid artery; carotid artery obstruction; case control study; cognition; Cognition Disorders; complication; diagnostic imaging; duplex Doppler ultrasonography; Intracranial Embolism; middle aged; motor activity; neurologic examination; neuropsychological test; pathophysiology; psychology; severity of illness index; Aged; Asymptomatic Diseases; Attention; Carotid Arteries; Carotid Stenosis; Case-Control Studies; Cerebrovascular Circulation; Cognition; Cognition Disorders; Executive Function; Female; Humans; Intracranial Embolism; Male; Memory; Middle Aged; Motor Activity; Neurologic Examination; Neuropsychological Tests; Plaque, Atherosclerotic; Prospective Studies; Risk Factors; Severity of Illness Index; Ultrasonography, Doppler, Duplex; Ultrasonography, Doppler, Transcranial",2-s2.0-85023769830
"Lin F.-J., Chen S.-G., Sun I.-F.","Intelligent Sliding-Mode Position Control Using Recurrent Wavelet Fuzzy Neural Network for Electrical Power Steering System",2017,"International Journal of Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031802547&doi=10.1007%2fs40815-017-0342-x&partnerID=40&md5=9f6b1101f4777fee17501189dac9e408","A digital signal processor (DSP)-based intelligent sliding-mode control (SMC) is proposed for the position control of a six-phase permanent magnet synchronous motor (PMSM) drive system installed in an electric power steering (EPS) system in this study. First, the dynamic mathematical model of the EPS system is derived by the Lagrangian dynamics. Since the EPS system is a nonlinear and time-varying system, the control accuracy is very sensitive to the parameter variations and external disturbances. Therefore, a SMC is developed for the position control of the EPS system. However, the upper bound of the uncertainties is difficult to obtain in advance and the choice of switching control gain in SMC is vital but time-consuming and may cause undesired chattering phenomenon. Hence, an intelligent SMC with a novel recurrent wavelet fuzzy neural network (ISMC-RWFNN) is proposed, in which a recurrent wavelet fuzzy neural network (RWFNN) is adopted as an uncertainty estimator to overcome the aforementioned disadvantage of SMC. Moreover, a robust compensator is employed to reduce the estimation error. In addition, the adaptive learning algorithms for the online training of the RWFNN are derived using the Lyapunov theorem and Taylor series. Finally, the proposed ISMC-RWFNN to control the position of a six-phase PMSM drive system for the EPS system is implemented in a 32-bit floating-point DSP, and some experimental results are provided to verify its effectiveness. © 2017, Taiwan Fuzzy Systems Association and Springer-Verlag GmbH Germany.","Electric power steering system; Recurrent wavelet fuzzy neural network; Six-phase permanent synchronous motor; Sliding-mode control; Taylor series expansion","Digital arithmetic; Digital signal processing; Digital signal processors; Electric machine control; Estimation; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Permanent magnets; Position control; Signal processing; Sliding mode control; Synchronous motors; Taylor series; Time varying systems; Adaptive learning algorithm; Digital signal processors (DSP); Electric power steering system; Intelligent sliding mode controls; Permanent magnet synchronous motor drives; Six-phase; Taylor series expansions; Wavelet fuzzy neural network; Electric power system control",2-s2.0-85031802547
"Lu D., Chen X.","Interpreting and extracting open knowledge for human-robot interaction",2017,"IEEE/CAA Journal of Automatica Sinica",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802740&doi=10.1109%2fJAS.2017.7510628&partnerID=40&md5=2b0ddc961362ae6a8623c7549cdcf264","A more natural way for non-expert users to express their tasks in an open-ended set is to use natural language. In this case, a human-centered intelligent agent/robot is required to be able to understand and generate plans for these naturally expressed tasks. For this purpose, it is a good way to enhance intelligent robot's abilities by utilizing open knowledge extracted from the web, instead of hand-coded knowledge. A key challenge of utilizing open knowledge lies in the semantic interpretation of the open knowledge organized in multiple modes, which can be unstructured or semi-structured, before one can use it. Previous approaches used a limited lexicon to employ combinatory categorial grammar (CCG) as the underlying formalism for semantic parsing over sentences. Here, we propose a more effective learning method to interpret semi-structured user instructions. Moreover, we present a new heuristic method to recover missing semantic information from the context of an instruction. Experiments showed that the proposed approach renders significant performance improvement compared to the baseline methods and the recovering method is promising. © 2014 Chinese Association of Automation.","Human-robot interaction; intelligent robot; natural language processing; open knowledge; semantic role labeling","Computational grammars; Formal languages; Heuristic methods; Intelligent robots; Man machine systems; Natural language processing systems; Robots; Semantics; Syntactics; Combinatory categorial grammar (CCG); Effective learning; Natural languages; open knowledge; Semantic information; Semantic interpretation; Semantic parsing; Semantic role labeling; Human robot interaction",2-s2.0-85029802740
"Huang Z., Lin C., Kanai-Pak M., Maeda J., Kitajima Y., Nakamura M., Kuwahara N., Ogata T., Ota J.","Robot Patient Design to Simulate Various Patients for Transfer Training",2017,"IEEE/ASME Transactions on Mechatronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028909331&doi=10.1109%2fTMECH.2017.2730848&partnerID=40&md5=a67e97994b7ad2625621077dba20f8d1","To improve the patient transfer skill of nursing education students, we developed a robot patient that can simulate three categories of patients: 1) patients whose movements are affected by paralysis; 2) patients whose movements are sensitive to pain with painful expression; and 3) patients whose movements are constrained by medical devices. By practicing with the robot patient, nursing students can learn the skills required for interacting with various patients. To simulate trunk movements of these different patients, novel waist and hip joints with hardware-inherent compliance and force-sensing capability were proposed. In addition, control methods were developed and the parameters were tuned based on actual patient videos. To evaluate the developed robot, nursing teachers performed trials of transferring the robot patient as they would transfer an actual patient. The nursing teachers scored the robot patients based on a checklist. Moreover, subjective evaluations of a questionnaire were performed by the nursing teachers. The results showed that the nursing teachers performed most of the required skills of the checklist and agreed regarding the learning effectiveness of the robot. They recommended training nursing students using the robot patient in the questionnaire. Finally, hugging speed comparison showed that the nurses slow down the speed when dealing with a robot patient with painful expression. © 1996-2012 IEEE.","Compliant joint; patient transfer; robot patient; various patients","Biomedical equipment; Education; Machine design; Nursing; Personnel training; Robots; Surveys; Teaching; Compliant joints; Learning effectiveness; Nursing education; Nursing students; Patient transfer; Subjective evaluations; Three categories; various patients; Students",2-s2.0-85028909331
"Sriraam N., Raghu S.","Classification of Focal and Non Focal Epileptic Seizures Using Multi-Features and SVM Classifier",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028762295&doi=10.1007%2fs10916-017-0800-x&partnerID=40&md5=08771b93185db5739bb4895ca24d17ba","Identifying epileptogenic zones prior to surgery is an essential and crucial step in treating patients having pharmacoresistant focal epilepsy. Electroencephalogram (EEG) is a significant measurement benchmark to assess patients suffering from epilepsy. This paper investigates the application of multi-features derived from different domains to recognize the focal and non focal epileptic seizures obtained from pharmacoresistant focal epilepsy patients from Bern Barcelona database. From the dataset, five different classification tasks were formed. Total 26 features were extracted from focal and non focal EEG. Significant features were selected using Wilcoxon rank sum test by setting p-value (p < 0.05) and z-score (−1.96 > z > 1.96) at 95% significance interval. Hypothesis was made that the effect of removing outliers improves the classification accuracy. Turkey’s range test was adopted for pruning outliers from feature set. Finally, 21 features were classified using optimized support vector machine (SVM) classifier with 10-fold cross validation. Bayesian optimization technique was adopted to minimize the cross-validation loss. From the simulation results, it was inferred that the highest sensitivity, specificity, and classification accuracy of 94.56%, 89.74%, and 92.15% achieved respectively and found to be better than the state-of-the-art approaches. Further, it was observed that the classification accuracy improved from 80.2% with outliers to 92.15% without outliers. The classifier performance metrics ensures the suitability of the proposed multi-features with optimized SVM classifier. It can be concluded that the proposed approach can be applied for recognition of focal EEG signals to localize epileptogenic zones. © 2017, Springer Science+Business Media, LLC.","Classifier; EEG; Epileptic seizures; Focal and non focal; Multi-feature; SVM","Article; Bayesian learning; clinical assessment; clinical feature; comparative study; computer simulation; controlled study; diagnostic accuracy; electroencephalogram; focal epilepsy; human; non focal epileptic seizure; rank sum test; seizure; sensitivity and specificity; support vector machine",2-s2.0-85028762295
"Zhao F., Liu Y., Zhang Y., Ma W., Zhang C.","A hybrid harmony search algorithm with efficient job sequence scheme and variable neighborhood search for the permutation flow shop scheduling problems",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028019151&doi=10.1016%2fj.engappai.2017.07.023&partnerID=40&md5=ab183cf3804a202c79c30075b4847a78","The permutation flow shop scheduling problem (PFSSP), one of the most widely studied production scheduling problems, is a typical NP-hard combinatorial optimization problem. In this paper, a hybrid harmony search algorithm with efficient job sequence mapping scheme and variable neighborhood search (VNS), named HHS, is proposed to solve the PFFSP with the objective to minimize the makespan. First of all, to extend the HHS algorithm to solve the PFSSP effectively, an efficient smallest order value (SOV) rule based on random key is introduced to convert continuous harmony vector into a discrete job permutation after fully investigating the effect of different job sequence mapping schemes. Secondly, an effective initialization scheme, which is based on NEH heuristic mechanism combining with chaotic sequence, is employed with the aim of improving the solution's quality of the initial harmony memory (HM). Thirdly, an opposition-based learning technique in the selection process and the best harmony (best individual) in the pitch adjustment process are made full use of to accelerate convergence performances and improve solution accuracy. Meanwhile, the parameter sensitivity is studied to investigate the properties of HHS, and the recommended values of parameters adopted in HHS are presented. Finally, by making use of a novel variable neighborhood search, the efficient insert and swap structures are incorporated into the HHS to adequately emphasize local exploitation ability. Experimental simulations and comparisons on both continuous and combinatorial benchmark problems demonstrate that the HHS algorithm outperforms the standard HS algorithm and other recently proposed efficient algorithms in terms of solution quality and stability. © 2017 Elsevier Ltd","Evolution computation; Harmony search; Operation research; Optimization; Parameter sensitivity; Permutation flow shop scheduling","Benchmarking; Combinatorial optimization; Hydraulic structures; Job shop scheduling; Learning algorithms; Machine shop practice; Mapping; Production control; Scheduling; Evolution computation; Harmony search; Operation research; Parameter sensitivities; Permutation flow-shop scheduling; Optimization",2-s2.0-85028019151
"Chen X., Zhang H., Zhang L., Shen C., Lee S.-W., Shen D.","Extraction of dynamic functional connectivity from brain grey matter and white matter for MCI classification",2017,"Human Brain Mapping",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021759413&doi=10.1002%2fhbm.23711&partnerID=40&md5=f30873078a876af71923cb64bd066e47","Brain functional connectivity (FC) extracted from resting-state fMRI (RS-fMRI) has become a popular approach for diagnosing various neurodegenerative diseases, including Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI). Current studies mainly construct the FC networks between grey matter (GM) regions of the brain based on temporal co-variations of the blood oxygenation level-dependent (BOLD) signals, which reflects the synchronized neural activities. However, it was rarely investigated whether the FC detected within the white matter (WM) could provide useful information for diagnosis. Motivated by the recently proposed functional correlation tensors (FCT) computed from RS-fMRI and used to characterize the structured pattern of local FC in the WM, we propose in this article a novel MCI classification method based on the information conveyed by both the FC between the GM regions and that within the WM regions. Specifically, in the WM, the tensor-based metrics (e.g., fractional anisotropy [FA], similar to the metric calculated based on diffusion tensor imaging [DTI]) are first calculated based on the FCT and then summarized along each of the major WM fiber tracts connecting each pair of the brain GM regions. This could capture the functional information in the WM, in a similar network structure as the FC network constructed for the GM, based only on the same RS-fMRI data. Moreover, a sliding window approach is further used to partition the voxel-wise BOLD signal into multiple short overlapping segments. Then, both the FC and FCT between each pair of the brain regions can be calculated based on the BOLD signal segments in the GM and WM, respectively. In such a way, our method can generate dynamic FC and dynamic FCT to better capture functional information in both GM and WM and further integrate them together by using our developed feature extraction, selection, and ensemble learning algorithms. The experimental results verify that the dynamic FCT can provide valuable functional information in the WM; by combining it with the dynamic FC in the GM, the diagnosis accuracy for MCI subjects can be significantly improved even using RS-fMRI data alone. Hum Brain Mapp 38:5019–5034, 2017. © 2017 Wiley Periodicals, Inc. © 2017 Wiley Periodicals, Inc.","Alzheimer's disease; functional connectivity; functional correlation tensor; mild cognitive impairment; resting-state fMRI","Article; BOLD signal; brain region; classifier; controlled study; diagnostic accuracy; diagnostic test accuracy study; diffusion tensor imaging; disease classification; female; fractional anisotropy; functional connectivity; functional magnetic resonance imaging; functional neuroimaging; gray matter; human; learning algorithm; major clinical study; male; mild cognitive impairment; nuclear magnetic resonance scanner; posterior cingulate; priority journal; receiver operating characteristic; sensitivity and specificity; support vector machine; white matter",2-s2.0-85021759413
"Yu X., Yu H., Tian X.-Y., Yu G., Li X.-M., Zhang X., Wang J.-Y.","Recognition of college students from Weibo with deep neural networks",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027999713&doi=10.1007%2fs13042-016-0515-1&partnerID=40&md5=6d0b5eb7f472748459193cd376e618eb","Classification of college students is a key to conduct further research on students. In this paper, we collect a set of samples and build deep neural network classifiers to recognize them. We also analyze the experiences and behaviors of the college students on Weibo. Firstly, we manually label 1502 student users and 1498 non-college students. Then, the data about their posts are crawled from Weibo to be transformed into input vectors by feature engineering techniques. Finally, classifiers are built based on two deep learning algorithms, including stacked autoencoders and deep belief network. Experimental results show that deep neural networks performs better than other machine learning algorithms and the classification of the college students can achieve a very high accuracy. © 2016, Springer-Verlag Berlin Heidelberg.","College students; Deep neural network; Student classification; Weibo","Deep learning; Deep neural networks; Education; Learning algorithms; Learning systems; Social networking (online); Autoencoders; College students; Deep belief networks; Feature engineerings; High-accuracy; Input vector; Neural network classifier; Weibo; Students",2-s2.0-85027999713
"Lin J., Wang Z.-J., Li X.","A backtracking search hyper-heuristic for the distributed assembly flow-shop scheduling problem",2017,"Swarm and Evolutionary Computation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018385184&doi=10.1016%2fj.swevo.2017.04.007&partnerID=40&md5=5988e980db2eb9e52f63fc24ab678676","Distributed assembly permutation flow-shop scheduling problem (DAPFSP) is recognized as an important class of problems in modern supply chains and manufacturing systems. In this paper, a backtracking search hyper-heuristic (BS-HH) algorithm is proposed to solve the DAPFSP. In the BS-HH scheme, ten simple and effective heuristic rules are designed to construct a set of low-level heuristics (LLHs), and the backtracking search algorithm is employed as the high-level strategy to manipulate the LLHs to operate on the solution space. Additionally, an efficient solution encoding and decoding scheme is proposed to generate a feasible schedule. The effectiveness of the BS-HH is evaluated on two typical benchmark sets and the computational results indicate the superiority of the proposed BS-HH scheme over the state-of-the-art algorithms. © 2017 Elsevier B.V.","Backtracking search algorithm; Distributed assembly; Flow-shop scheduling; Hyper-heuristic","Learning algorithms; Machine shop practice; Manufacture; Scheduling; Supply chains; Backtracking search algorithms; Computational results; Encoding and decoding; Flow shop scheduling problem; Flow-shop scheduling; Hyperheuristic; Permutation flow-shop scheduling; State-of-the-art algorithms; Heuristic methods",2-s2.0-85018385184
"Gomes H.M., Bifet A., Read J., Barddal J.P., Enembreck F., Pfharinger B., Holmes G., Abdessalem T.","Adaptive random forests for evolving data stream classification",2017,"Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020727910&doi=10.1007%2fs10994-017-5642-8&partnerID=40&md5=428bbaef33bd2202d8965e9723654220","Random forests is currently one of the most used machine learning algorithms in the non-streaming (batch) setting. This preference is attributable to its high learning performance and low demands with respect to input preparation and hyper-parameter tuning. However, in the challenging context of evolving data streams, there is no random forests algorithm that can be considered state-of-the-art in comparison to bagging and boosting based algorithms. In this work, we present the adaptive random forest (ARF) algorithm for classification of evolving data streams. In contrast to previous attempts of replicating random forests for data stream learning, ARF includes an effective resampling method and adaptive operators that can cope with different types of concept drifts without complex optimizations for different data sets. We present experiments with a parallel implementation of ARF which has no degradation in terms of classification performance in comparison to a serial implementation, since trees and adaptive operators are independent from one another. Finally, we compare ARF with state-of-the-art algorithms in a traditional test-then-train evaluation and a novel delayed labelling evaluation, and show that ARF is accurate and uses a feasible amount of resources. © 2017, The Author(s).","Concept drift; Data stream mining; Ensemble learning; Random forests","Classification (of information); Data communication systems; Decision trees; Learning algorithms; Learning systems; Optimization; Classification performance; Complex optimization; Concept drifts; Data stream mining; Ensemble learning; Parallel implementations; Random forests; State-of-the-art algorithms; Data mining",2-s2.0-85020727910
"Bouguelia M.-R., Gonzalez R., Iagnemma K., Byttner S.","Unsupervised classification of slip events for planetary exploration rovers",2017,"Journal of Terramechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029811187&doi=10.1016%2fj.jterra.2017.09.001&partnerID=40&md5=af6975e9d645e00aedd8d4891537ab25","This paper introduces an unsupervised method for the classification of discrete rovers’ slip events based on proprioceptive signals. In particular, the method is able to automatically discover and track various degrees of slip (i.e. low slip, moderate slip, high slip). The proposed method is based on aggregating the data over time, since high level concepts, such as high and low slip, are concepts that are dependent on longer time perspectives. Different features and subsets of the data have been identified leading to a proper clustering, interpreting those clusters as initial models of the prospective concepts. Bayesian tracking has been used in order to continuously improve the parameters of these models, based on the new data. Two real datasets are used to validate the proposed approach in comparison to other known unsupervised and supervised machine learning methods. The first dataset is collected by a single-wheel testbed available at MIT. The second dataset was collected by means of a planetary exploration rover in real off-road conditions. Experiments prove that the proposed method is more accurate (up to 86% of accuracy vs. 80% for K-means) in discovering various levels of slip while being fully unsupervised (no need for hand-labeled data for training). © 2017 ISTVS","Clustering; Data-driven modeling; LATUV rover; MSL rover; Slip; Unsupervised learning","Learning systems; Supervised learning; Unsupervised learning; Clustering; Data-driven model; LATUV rover; MSL rover; Slip; Interplanetary spacecraft",2-s2.0-85029811187
"Chalouli M., Berrached N.-E., Denai M.","Intelligent Health Monitoring of Machine Bearings Based on Feature Extraction",2017,"Journal of Failure Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028767770&doi=10.1007%2fs11668-017-0343-y&partnerID=40&md5=841de84555ba105ae2ed661284cb3c8d","Finding reliable condition monitoring solutions for large-scale complex systems is currently a major challenge in industrial research. Since fault diagnosis is directly related to the features of a system, there have been many research studies aimed to develop methods for the selection of the relevant features. Moreover, there are no universal features for a particular application domain such as machine diagnosis. For example, in machine bearing fault diagnosis, these features are often selected by an expert or based on previous experience. Thus, for each bearing machine type, the relevant features must be selected. This paper attempts to solve the problem of relevant features identification by building an automatic fault diagnosis process based on relevant feature selection using a data-driven approach. The proposed approach starts with the extraction of the time-domain features from the input signals. Then, a feature reduction algorithm based on cross-correlation filter is applied to reduce the time and cost of the processing. Unsupervised learning mechanism using K-means++ selects the relevant fault features based on the squared Euclidian distance between different health states. Finally, the selected features are used as inputs to a self-organizing map producing our health indicator. The proposed method is tested on roller bearing benchmark datasets. © 2017, ASM International.","Bearing faults; Condition-based maintenance; Failure diagnosis; Fault feature extraction; Health indicators; Relevant features; Time-domain features","Condition monitoring; Conformal mapping; Electric fault currents; Extraction; Failure analysis; Fault detection; Health; Industrial research; Large scale systems; Roller bearings; Self organizing maps; Time domain analysis; Bearing fault; Condition based maintenance; Failure Diagnosis; Fault feature extractions; Health indicators; Relevant features; Time domain features; Feature extraction",2-s2.0-85028767770
"Wang F.-Y., Zheng N.-N., Cao D., Martinez C.M., Li L., Liu T.","Parallel driving in CPSS: A unified approach for transport automation and vehicle intelligence",2017,"IEEE/CAA Journal of Automatica Sinica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029795861&doi=10.1109%2fJAS.2017.7510598&partnerID=40&md5=c13fcbe2bdb7cf99903ab26467fe573c","The emerging development of connected and automated vehicles imposes a significant challenge on current vehicle control and transportation systems. This paper proposes a novel unified approach, Parallel Driving, a cloud-based cyberphysical-social systems (CPSS) framework aiming at synergizing connected automated driving. This study first introduces the CPSS and ACP-based intelligent machine systems. Then the parallel driving is proposed in the cyber-physical-social space, considering interactions among vehicles, human drivers, and information. Within the framework, parallel testing, parallel learning and parallel reinforcement learning are developed and concisely reviewed. Development on intelligent horizon (iHorizon (and its applications are also presented towards parallel horizon. The proposed parallel driving offers an ample solution for achieving a smooth, safe and efficient cooperation among connected automated vehicles with different levels of automation in future road transportation systems. © 2014 Chinese Association of Automation.","ACP theory; connected automated driving; cyber-physical-social systems (CPSS); iHorizon; parallel driving; parallel horizon; parallel learning; parallel reinforcement learning; parallel testing","Automation; Control system synthesis; Cyber Physical System; Transportation; Vehicles; Acp theories; Automated driving; iHorizon; parallel driving; parallel horizon; Parallel learning; Parallel reinforcement learning; Parallel testing; Social systems; Reinforcement learning",2-s2.0-85029795861
"Pou J.-M.","The Metrologist's place is by the machines!",2017,"IEEE Instrumentation and Measurement Magazine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029811793&doi=10.1109%2fMIM.2017.8036689&partnerID=40&md5=e3e538621cb66711b76d09e6199f20c7","Industry 4.0, smart objects, new versions of ISO 9001 and 17025 standards, and so on... massive changes are taking place in companies that, as a result, are learning to be increasingly agile and responsive. It is time for metrology to remodel itself to keep pace. Smart Metrology needs to go beyond the role of ensuring conformity to regulatory standards and traceability to national measurement standards to gain the approval of auditors and focus its energy upon the true essential consideration: the quality of the measurements made within a company! © 1998-2012 IEEE.",,"Units of measurement; Essential considerations; ISO 9001; National measurements; Regulatory standards; Smart objects; Standards",2-s2.0-85029811793
"Ho J., Kang D.-K.","Mini-batch bagging and attribute ranking for accurate user authentication in keystroke dynamics",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020009965&doi=10.1016%2fj.patcog.2017.05.002&partnerID=40&md5=442722c19b1fa6694f5d2a405922cee8","We consider the problem of differentiating users’ typing behavior patterns using machine learning algorithms with keystroke dynamics features. We have proposed mini-batch bagging (MINIBAG) method and attribute ranking of one-class naïve Bayes (AR-ONENB) algorithm. MINIBAG is motivated from bagging because MINIBAG chunks each attribute of the dataset into multiple sub-datasets during the preprocessing phase. Meanwhile, AR-ONENB sorts the attributes based on the time length during the preprocessing phase for effective classification. Both proposed algorithms have shown promising experimental results from various keystroke dynamics based user authentication benchmark tests. From the experimental results, it can be seen that MINIBAG facilitates machine learning algorithms to have an ensemble of multiple models from mini-batches. AR-ONENB, on the other hands, calculates log-likelihood value from keystroke index order for anomaly estimation, which exploits the observation that the user's typing speed is unique. © 2017 Elsevier Ltd","Attribute ranking; Bagging; Keystroke dynamics; Mini-batch; One-class naïve Bayes; User authentication","Artificial intelligence; Authentication; Benchmarking; Dynamics; Learning systems; Attribute ranking; Bagging; Keystroke dynamics; Mini-batch; User authentication; Learning algorithms",2-s2.0-85020009965
"Dahmouni A., Aharrane N., El Moutaouakil K., Satori K.","Multi-classifiers face recognition system using LBPP face representation",2017,"International Journal of Innovative Computing, Information and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030211992&partnerID=40&md5=c3dc4b1ce2deb2e98fb3b2c94ae0fa0f","Local Binary Probabilistic Pattern (LBPP) is a local descriptor able to improve the recognition capabilities of a typical pattern recognition system. It is a new alternative of the famous Local Binary Pattern (LBP) descriptor based on confidence interval concept. To achieve an enhanced representation for face’s principal components, LBPP evaluates each current pixel using a probabilistic confidence interval related to its neighborhood. In this paper, to improve face recognition performance we propose a new methodology based on the combinative use of LBPP descriptor, Two Dimensional Discrete Cosine Transform (2DDCT) frequency subspace, and some machine learning algorithms. The main idea behind this methodology is to elevate the weak points of each one of them, while making use of their major advantages. Hence, after the LBPP processing phase, 2DDCT method decomposes obtained image into set of local features vectors. Each local vector is formed by the k-first zigzag coefficients for each sub-image. Then, we carefully concatenate all local vectors into a single features vector. In addition, obtained features dataset will be classified using relevant machine learning classifiers. To access our solution, we applied it on ORL, Yale and AR face databases. Obtained results clearly show the effectiveness of the proposed approach compared to the existing state of the art techniques. Indeed, the LBPP capacity to discriminate face components, the small size of 2DDCT features vector, and the efficiency of used classifiers, allow justifying the proposed approach’s good performance. © 2017 ICIC International.","2DDCT; Confidence interval; KNN; LBPP; LIBSVM; MLP; SMO; Vote rules","Artificial intelligence; Bins; Classification (of information); Discrete cosine transforms; Learning algorithms; Learning systems; Nearest neighbor search; Pattern recognition; Pattern recognition systems; Vectors; 2DDCT; Confidence interval; LBPP; LIBSVM; Vote rules; Face recognition",2-s2.0-85030211992
"Lukander K., Toivanen M., Puolamäki K.","Inferring intent and action from gaze in naturalistic behavior: A review",2017,"International Journal of Mobile Human Computer Interaction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028048357&doi=10.4018%2fIJMHCI.2017100104&partnerID=40&md5=970c846e5a84c384bd08d98237a118a2","We constantly move our gaze to gather acute visual information from our environment. Conversely, as originally shown by Yarbus in his seminal work, the elicited gaze patterns hold information over our changing attentional focus while performing a task. Recently, the proliferation of machine learning algorithms has allowed the research community to test the idea of inferring, or even predicting action and intent from gaze behaviour. The on-going miniaturization of gaze tracking technologies toward pervasive wearable solutions allows studying inference also in everyday activities outside research laboratories. This paper scopes the emerging field and reviews studies focusing on the inference of intent and action in naturalistic behaviour. While the task-specific nature of gaze behavior, and the variability in naturalistic setups present challenges, gaze-based inference holds a clear promise for machine-based understanding of human intent and future interactive solutions. Copyright © 2017, IGI Global.","Eye Movements; Gaze Tracking; Inference; Intent Modeling; Scoping Study; Task Modeling","Learning algorithms; Learning systems; Research laboratories; Tracking (position); Wearable technology; Gaze tracking; Inference; Intent models; Scoping; Task modeling; Eye movements",2-s2.0-85028048357
"Friston K.J., Lin M., Frith C.D., Pezzulo G., Hobson J.A., Ondobaka S.","Active inference, curiosity and insight",2017,"Neural Computation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029283962&doi=10.1162%2fNECO_a_00999&partnerID=40&md5=76fb1ee4073674d0d0d7a1a9df95033e","This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generativemodels of the world, thereby reducing uncertainty and satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of ""aha"" moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., con: ""together""; scire: ""to know""). © 2017 Massachusetts Institute of Technology.",,"Bayesian networks; Free energy; Inference engines; Learning systems; Active sampling; Approximate Bayesian inference; Explanatory gaps; Generative model; Model Selection; Statistical structures; Structure-learning; Variational free energy; Abstracting",2-s2.0-85029283962
"Rey N., Volpi M., Joost S., Tuia D.","Detecting animals in African Savanna with UAVs and the crowds",2017,"Remote Sensing of Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028081773&doi=10.1016%2fj.rse.2017.08.026&partnerID=40&md5=a73281dc43d95808321f74c2d84daf5f","Unmanned aerial vehicles (UAVs) offer new opportunities for wildlife monitoring, with several advantages over traditional field-based methods. They have readily been used to count birds, marine mammals and large herbivores in different environments, tasks which are routinely performed through manual counting in large collections of images. In this paper, we propose a semi-automatic system able to detect large mammals in semi-arid Savanna. It relies on an animal-detection system based on machine learning, trained with crowd-sourced annotations provided by volunteers who manually interpreted sub-decimeter resolution color images. The system achieves a high recall rate and a human operator can then eliminate false detections with limited effort. Our system provides good perspectives for the development of data-driven management practices in wildlife conservation. It shows that the detection of large mammals in semi-arid Savanna can be approached by processing data provided by standard RGB cameras mounted on affordable fixed wings UAVs. © 2017 Elsevier Inc.","Active learning; Animal conservation; Crowd-sourcing data; Object detection; Unmanned aerial vehicles; Very high resolution; Wildlife monitoring","Animals; Artificial intelligence; Conservation; Data handling; Fixed wings; Information management; Learning systems; Mammals; Object detection; Unmanned aerial vehicles (UAV); Active Learning; Detection system; False detections; Management practices; Semi-automatic systems; Very high resolution; Wildlife conservation; Wildlife monitoring; Aircraft detection; Animalia; Aves; Mammalia",2-s2.0-85028081773
"Karunaratne P., Karunasekera S., Harwood A.","Distributed stream clustering using micro-clusters on Apache Storm",2017,"Journal of Parallel and Distributed Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003946086&doi=10.1016%2fj.jpdc.2016.06.004&partnerID=40&md5=54c5de5d15f8d2a38a8ac24813f9e29c","The recent need to extract real-time insights from data has driven the need for machine learning algorithms that can operate on data streams. Given the current extreme rates of data generation (around 5000 messages per second), these algorithms need to be able to handle data streams of very high velocity. Many current algorithms do not reach this requirement, in some cases processing only tens of messages per second. In this work we address the problem of limited achievable throughput of stream clustering by developing scalable distributed algorithms based on the micro-clustering paradigm that run on cloud platforms. We present two distributed architectures to execute the algorithms in parallel and implement these architectures on the Apache Storm stream processing platform. We demonstrate that we are able to gain close to an order of magnitude of improvement of performance in our experiments. © 2016 Elsevier Inc.","Distributed data mining; Stream clustering; Stream data mining","Artificial intelligence; Clustering algorithms; Data communication systems; Learning algorithms; Learning systems; Storms; Achievable throughputs; Cloud platforms; Data generation; Distributed architecture; Distributed data mining; Stream clustering; Stream data mining; Stream processing; Data mining",2-s2.0-85003946086
"Liu K., Detwiler D., Tovar A.","Optimal design of nonlinear multimaterial structures for crashworthiness using cluster analysis",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028599752&doi=10.1115%2f1.4037620&partnerID=40&md5=742e005d287c3e2f2d1e7dd952e93f08","This study presents an efficient multimaterial design optimization algorithm that is suitable for nonlinear structures. The proposed algorithm consists of three steps: conceptual design generation, clustering, and metamodel-based global optimization. The conceptual design is generated using a structural optimization algorithm for linear models or a heuristic design algorithm for nonlinear models. Then, the conceptual design is clustered into a predefined number of clusters (materials) using a machine learning algorithm. Finally, the global optimization problem aims to find the optimal material parameters of the clustered design using metamodels. The metamodels are built using sampling and cross-validation and sequentially updated using an expected improvement function until convergence. The proposed methodology is demonstrated using examples from multiple physics and compared with traditional multimaterial topology optimization (MTOP) method. The proposed approach is applied to a nonlinear, multi-objective design problems for crashworthiness. Copyright © 2017 by ASME.",,"Cluster analysis; Clustering algorithms; Conceptual design; Crashworthiness; Global optimization; Learning algorithms; Learning systems; Optimization; Structural optimization; Expected improvements; Global optimization problems; Heuristic designs; Multi-material designs; Multi-material structure; Multi-objective design; Nonlinear structure; Number of clusters; Structural design",2-s2.0-85028599752
"Sun X., Liu R., Chen Y.-J., Chiu H.-Y., Chen W.-H., Chang M.-F., Yu S.","Low-VDD Operation of SRAM Synaptic Array for Implementing Ternary Neural Network",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028936782&doi=10.1109%2fTVLSI.2017.2727528&partnerID=40&md5=1ad60ac70a0e52d33ef45679717eef17","For Internet of Things (IoT) edge devices, it is very attractive to have the local sensemaking capability instead of sending all the data back to the cloud for information processing. For image pattern recognition, neuro-inspired machine learning algorithms have demonstrated enormous powerfulness. To effectively implement learning algorithms on-chip for IoT edge devices, on-chip synaptic memory architectures have been proposed to implement the key operations such as weighted-sum or matrix-vector multiplication. In this paper, we proposed a low-power design of static random access memory (SRAM) synaptic array for implementing a low-precision ternary neural network. We experimentally demonstrated that the supply voltage (VDD) of the SRAM array could be aggressively reduced to a level, where the SRAM cell is susceptible to bit failures. The testing results from 65-nm SRAM chips indicate that VDD could be reduced from the nominal 1-0.55 V (or 0.5 V) with a bit error rate ∼0.23% (or ∼1.56%), which only introduced ∼0.08% (or ∼1.68%) degradation in the classification accuracy. As a result, the power consumption could be reduced by more than 8× (or 10×). © 2017 IEEE.","Binary synapses; classification; low power; neural network; static random access memory (SRAM)","Bit error rate; Classification (of information); Electric power supplies to apparatus; Image coding; Internet of things; Learning algorithms; Learning systems; Low power electronics; Memory architecture; Neural networks; Neurons; Pattern recognition; Personnel training; Random access storage; Arrays; Binary synapsis; Low Power; SRAM Cell; Static random access memory; Static random access storage",2-s2.0-85028936782
"Uçar E., Uzun E., Tüfekci P.","A novel algorithm for extracting the user reviews from web pages",2017,"Journal of Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029475331&doi=10.1177%2f0165551516666446&partnerID=40&md5=090c234a6b42df58729e55a6a4289452","Extracting the user reviews in websites such as forums, blogs, newspapers, commerce, trips, etc. is crucial for text processing applications (e.g. sentiment analysis, trend detection/monitoring and recommendation systems) which are needed to deal with structured data. Traditional algorithms have three processes consisting of Document Object Model (DOM) tree creation, extraction of features obtained from this tree and machine learning. However, these algorithms increase time complexity of extraction process. This study proposes a novel algorithm that involves two complementary stages. The first stage determines which HTML tags correspond to review layout for a web domain by using the DOM tree as well as its features and decision tree learning. The second stage extracts review layout for web pages in a web domain using the found tags obtained from the first stage. This stage is more time-efficient, being approximately 21 times faster compared to the first stage. Moreover, it achieves a relatively high accuracy of 96.67% in our experiments of review block extraction. © Chartered Institute of Library and Information Professionals.","Efficient extraction; web data extraction; web user reviews","Computational complexity; Decision trees; Extraction; Learning systems; Text processing; Websites; XML; Block extraction; Decision tree learning; Document object model; Extraction process; Sentiment analysis; Time complexity; Web data extraction; Web users; Data mining",2-s2.0-85029475331
"Ganesan J., Inbarani H.H., Azar A.T., Polat K.","Tolerance rough set firefly-based quick reduct",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982883828&doi=10.1007%2fs00521-016-2514-2&partnerID=40&md5=c31b4b81bd8ec48e163f3323b7e9af90","In medical information system, there are a lot of features and the relationship among elements is solid. In this way, feature selection of medical datasets gets awesome worry as of late. In this article, tolerance rough set firefly-based quick reduct, is developed and connected to issue of differential finding of diseases. The hybrid intelligent framework intends to exploit the advantages of the fundamental models and, in the meantime, direct their restrictions. Feature selection is procedure for distinguishing ideal feature subset of the original features. A definitive point of feature selection is to build the precision, computational proficiency and adaptability of expectation strategy in machine learning, design acknowledgment and information mining applications. Along these lines, the learning framework gets a brief structure without lessening the prescient precision by utilizing just the chose remarkable features. In this research, a hybridization of two procedures, tolerance rough set and as of late created meta-heuristic enhancement calculation, the firefly algorithm is utilized to choose the conspicuous features of medicinal information to have the capacity to characterize and analyze real sicknesses. The exploratory results exhibited that the proficiency of the proposed system outflanks the current supervised feature selection techniques. © 2016, The Natural Computing Applications Forum.","Firefly algorithm; Rough set theory; Soft computing techniques; Supervised feature selection; Swarm intelligent; Tolerance rough set","Artificial intelligence; Bioluminescence; Computation theory; Feature extraction; Learning systems; Medical information systems; Optimization; Soft computing; Firefly algorithms; Fundamental models; Information mining; Learning frameworks; Selection techniques; Softcomputing techniques; Swarm intelligent; Tolerance rough sets; Rough set theory",2-s2.0-84982883828
"Li Q., Dietrich F., Bollt E.M., Kevrekidis I.G.","Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the koopman operator",2017,"Chaos",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031693065&doi=10.1063%2f1.4993854&partnerID=40&md5=a5cddcdf9586a8e1611c823e767b7736","Numerical approximation methods for the Koopman operator have advanced considerably in the last few years. In particular, data-driven approaches such as dynamic mode decomposition (DMD)51 and its generalization, the extended-DMD (EDMD), are becoming increasingly popular in practical applications. The EDMD improves upon the classical DMD by the inclusion of a flexible choice of dictionary of observables which spans a finite dimensional subspace on which the Koopman operator can be approximated. This enhances the accuracy of the solution reconstruction and broadens the applicability of the Koopman formalism. Although the convergence of the EDMD has been established, applying the method in practice requires a careful choice of the observables to improve convergence with just a finite number of terms. This is especially difficult for high dimensional and highly nonlinear systems. In this paper, we employ ideas from machine learning to improve upon the EDMD method. We develop an iterative approximation algorithm which couples the EDMD with a trainable dictionary represented by an artificial neural network. Using the Duffing oscillator and the Kuramoto Sivashinsky partical differential equation as examples, we show that our algorithm can effectively and efficiently adapt the trainable dictionary to the problem at hand to achieve good reconstruction accuracy without the need to choose a fixed dictionary a priori. Furthermore, to obtain a given accuracy, we require fewer dictionary terms than EDMD with fixed dictionaries. This alleviates an important shortcoming of the EDMD algorithm and enhances the applicability of the Koopman framework to practical problems.",,,2-s2.0-85031693065
"Joan S.P.F., Valli S.","An enhanced text detection technique for the visually impaired to read text",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988632647&doi=10.1007%2fs10796-016-9699-x&partnerID=40&md5=97060f5b40c2fb479c5b323b148bc2b1","An enhanced text detection technique (ETDT) is proposed, which is expected to aid the visually impaired to overcome their reading challenges. This work enhances the edge-preserving maximally stable extremal regions (eMSER) algorithm using the pyramid histogram of oriented gradients (PHOG). Histogram of oriented gradients (HOG) derived from different pyramid levels is important while detecting maximally stable extremal regions (MSER) in the ETDT approach because it gives more spatial information when compared to HOG information from a single level. To group text, a four-line, text-grouping method is newly designed for this work. Also, a new text feature, Shapeness Score is proposed, which significantly identifies text regions when combined with the other features based on morphology and stroke widths. Using the feature vector of dimension 10, the J48 decision tree and AdaBoost machine learning algorithms identify the text regions in the images. The algorithm yields better results than the existing benchmark algorithms for the ICDAR 2011 born-digital dataset and must be improved with respect to the scene text dataset. © 2016, Springer Science+Business Media New York.","MSER; PHOG; Shapeness score; Stroke width; Text detection","Adaptive boosting; Artificial intelligence; Data mining; Decision trees; Graphic methods; Learning algorithms; Learning systems; MSER; PHOG; Shapeness score; Stroke widths; Text detection; Character recognition",2-s2.0-84988632647
"Luo C., He F., Ghezzi C.","Inferring software behavioral models with MapReduce",2017,"Science of Computer Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019099784&doi=10.1016%2fj.scico.2017.04.004&partnerID=40&md5=013a9ed5e3411afeb966755c2ce1a9b0","In the real world practice, software systems are often built without developing any explicit upfront model. This can cause serious problems that may hinder the almost inevitable future evolution, since at best the only documentation about the software is in the form of source code comments. To address this problem, research has been focusing on automatic inference of models by applying machine learning algorithms to execution logs. However, the logs generated by a real software system may be very large and the inference algorithm can exceed the processing capacity of a single computer. This paper proposes a scalable, general approach to the inference of behavior models that can handle large execution logs via parallel and distributed algorithms implemented using the MapReduce programming model and executed on a cluster of interconnected execution nodes. The approach consists of two distributed phases that perform trace slicing and model synthesis. For each phase, a distributed algorithm using MapReduce is developed. With the parallel data processing capacity of MapReduce, the problem of inferring behavior models from large logs can be efficiently solved. The technique is implemented on top of Hadoop. Experiments on Amazon clusters show efficiency and scalability of our approach. © 2017 Elsevier B.V.","Log analysis; MapReduce; Model inference; Parametric trace","Computer software; Data handling; Distributed computer systems; Inference engines; Learning algorithms; Learning systems; Log analysis; Map-reduce; Map-reduce programming; Model inference; Parallel and distributed algorithms; Parallel data processing; Parametric trace; Processing capacities; Clustering algorithms",2-s2.0-85019099784
"Chi M., Sun Z., Qin Y., Shen J., Benediktsson J.A.","A Novel Methodology to Label Urban Remote Sensing Images Based on Location-Based Social Media Photos",2017,"Proceedings of the IEEE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029173213&doi=10.1109%2fJPROC.2017.2730585&partnerID=40&md5=1b88a3a6cbae343445f602e4077a9bc4","With the rapid development of the internet and popularization of intelligent mobile devices, social media is evolving fast and contains rich spatial information, such as geolocated posts, tweets, photos, video, and audio. Those location-based social media data have offered new opportunities for hazards and disaster identification or tracking, recommendations for locations, friends or tags, pay-per-click advertising, etc. Meanwhile, a massive amount of remote sensing (RS) data can be easily acquired in both high temporal and spatial resolution with a multiple satellite system, if RS maps can be provided, to possibly enable the monitoring of our location-based living environments with some devices like charge-coupled device (CCD) cameras but on a much larger scale. To generate the classification maps, usually, labeled RS image pixels should be provided by RS experts to train a classification system. Traditionally, labeled samples are obtained according to ground surveys, image photo interpretation or a combination of the aforementioned strategies. All the strategies should be taken care of by domain experts, in a means which is costly, time consuming, and sometimes of a low quality due to reasons such as photo interpretation based on RS images only. These practices and constraints make it more challenging to classify land-cover RS images using big RS data. In this paper, a new methodology is proposed to classify urban RS images by exploiting the semantics of location-based social media photos (SMPs). To validate the effectiveness of this methodology, an automatic classification system is developed based on RS images as well as SMPs via big data analysis techniques including active learning, crowdsourcing, shallow machine learning, and deep learning. As the labels of RS training data are given by ordinary people with a crowdsourcing technique, the developed system is named Crowd4RS. The quantitative and qualitative experiments confirm the effectiveness of the proposed Crowd4RS system as well as the proposed methodology for automatically generating RS image maps in terms of classification results based on big RS data made up of multispectral RS images in a high spatial resolution and a large amount of photos from social media sites, such as Flickr and Panoramio. © 1963-2012 IEEE.","Big data; crowdsourcing; deep learning; remote sensing; social media","Artificial intelligence; Big data; Charge coupled devices; Crowdsourcing; Deep learning; Image reconstruction; Image resolution; Learning systems; Location; Photointerpretation; Remote sensing; Satellites; Semantics; Shape memory effect; Social networking (online); Automatic classification systems; Data analysis techniques; Multiple satellite systems; Social media; Social network services; Spatial resolution; Urban areas; Urban remote sensing images; Image classification",2-s2.0-85029173213
"Nasr-Azadani F., Khan R., Rahimikollu J., Unnikrishnan A., Akanda A., Alam M., Huq A., Jutla A., Colwell R.","Hydroclimatic sustainability assessment of changing climate on cholera in the Ganges-Brahmaputra basin",2017,"Advances in Water Resources",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009460388&doi=10.1016%2fj.advwatres.2016.11.018&partnerID=40&md5=e89a683287f108371e357272257320bf","The association of cholera and climate has been extensively documented. However, determining the effects of changing climate on the occurrence of disease remains a challenge. Bimodal peaks of cholera in Bengal Delta are hypothesized to be linked to asymmetric flow of the Ganges and Brahmaputra rivers. Spring cholera is related to intrusion of bacteria-laden coastal seawater during low flow seasons, while autumn cholera results from cross-contamination of water resources when high flows in the rivers cause massive inundation. Coarse resolution of General Circulation Model (GCM) output (usually at 100 – 300 km)cannot be used to evaluate variability at the local scale(10–20 km),hence the goal of this study was to develop a framework that could be used to understand impacts of climate change on occurrence of cholera. Instead of a traditional approach of downscaling precipitation, streamflow of the two rivers was directly linked to GCM outputs, achieving reasonable accuracy (R2 = 0.89 for the Ganges and R2 = 0.91 for the Brahmaputra)using machine learning algorithms (Support Vector Regression-Particle Swarm Optimization). Copula methods were used to determine probabilistic risks of cholera under several discharge conditions. Key results, using model outputs from ECHAM5, GFDL, andHadCM3for A1B and A2 scenarios, suggest that the combined low flow of the two rivers may increase in the future, with high flows increasing for first half of this century, decreasing thereafter. Spring and autumn cholera, assuming societal conditions remain constant e.g., at the current rate, may decrease. However significant shifts were noted in the magnitude of river discharge suggesting that cholera dynamics of the delta may well demonstrate an uncertain predictable pattern of occurrence over the next century. © 2016 Elsevier Ltd","Cholera; Climate change; Downscaling streamflow; GBM basin; Infectious diseases","Climate models; Contamination; Diseases; Learning algorithms; Learning systems; Particle swarm optimization (PSO); Rivers; Stream flow; Water resources; Cholera; Down-scaling; Ganges-brahmaputra basins; GBM basin; General circulation model; Infectious disease; Support vector regression (SVR); Sustainability assessment; Climate change; algorithm; bacterium; cholera; climate change; climate effect; risk assessment; river basin; river pollution; streamflow; sustainability; Brahmaputra River; Ganges Delta; Ganges River",2-s2.0-85009460388
"Nanni L., Paci M., Brahnam S., Ghidoni S.","An ensemble of visual features for Gaussians of local descriptors and non-binary coding for texture descriptors",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017166137&doi=10.1016%2fj.eswa.2017.03.065&partnerID=40&md5=932ed2707d5c1d3ffa95fe0e8bea112f","This paper presents an improved version of a recent state-of-the-art texture descriptor called Gaussians of Local Descriptors (GOLD), which is based on a multivariate Gaussian that models the local feature distribution that describes the original image. The full rank covariance matrix, which lies on a Riemannian manifold, is projected on the tangent Euclidean space and concatenated to the mean vector for representing a given image. In this paper, we test the following features for describing the original image: scale-invariant feature transform (SIFT), histogram of gradients (HOG), and weber's law descriptor (WLD). To improve the baseline version of GOLD, we describe the covariance matrix using a set of visual features that are fed into a set of Support Vector Machines (SVMs). The SVMs are combined by sum rule. The scores obtained by an SVM trained using the original GOLD approach and the SVMs trained with visual features are then combined by sum rule. Experiments show that our proposed variant outperforms the original GOLD approach. The superior performance of the proposed system is validated across a large set of datasets. Particularly interesting is the performance obtained in two widely used person re-identification datasets, CAVIAR4REID and IAS, where the proposed GOLD variant is coupled with a state-of-the-art ensemble to obtain an improvement of performance on these two datasets. Moreover, we performed further tests that combine GOLD with non-binary features (local ternary/quinary patterns) and deep transfer learning. The fusion among SVMs trained with deep features and the SVMs trained using the ternary/quinary coding ensemble is demonstrated to obtain a very high performance across datasets. The MATLAB code for the ensemble of classifiers and for the extraction of the features will be publicly available1 to other researchers for future comparisons. © 2017 Elsevier Ltd","Ensemble of descriptors; Image classification; Image processing; Person re-identification; Texture","Codes (symbols); Covariance matrix; Gold; Image classification; Image processing; MATLAB; Support vector machines; Textures; Vector spaces; Descriptors; Ensemble of classifiers; Histogram of gradients (HOG); Person re identifications; Riemannian manifold; Scale invariant feature transforms; Support vector machine (SVMs); Texture descriptors; Image texture",2-s2.0-85017166137
"Zhou H., Zhu X., Wang S., Zhou K., Ma Z., Li J., Hou K.-M., De Vaulx C.","A Novel Cardiac Arrhythmias Detection Approach for Real-Time Ambulatory ECG Diagnosis",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017617146&doi=10.1142%2fS0218001417580046&partnerID=40&md5=940e606f79ccbc2fc86757423674ce3a","In view of requirements of low-resource consumption and high-efficiency in real-time Ambulatory Electrocardiograph Diagnosis (AED) applications, a novel Cardiac Arrhythmias Detection (CAD) algorithm is proposed. This algorithm consists of three core modules: an automatic-learning machine that models diagnostic criteria and grades the emergency events of cardiac arrhythmias by studying morphological characteristics of ECG signals and experiential knowledge of cardiologists; a rhythm classifier that recognizes and classifies heart rhythms basing on statistical features comparison and linear discriminant with confidence interval estimation; and an arrhythmias interpreter that assesses emergency events of cardia arrhythmias basing on a two rule-relative interpretation mechanisms. The experiential results on off-line MIT-BIH cardiac arrhythmia database as well as online clinical testing explore that this algorithm has 92.8% sensitivity and 97.5% specificity in average, so that it is suitable for real-time cardiac arrhythmias monitoring. © 2017 World Scientific Publishing Company.","Cardiac arrhythmias detection; linear discriminant with confidence interval estimation; morphological characteristics; statistical features","Diseases; Electrocardiography; Learning systems; Automatic-learning; Cardiac arrhythmia; Confidence interval estimation; Experiential knowledge; Linear discriminants; Morphological characteristic; Resource consumption; Statistical features; Heart",2-s2.0-85017617146
"Drury B., Valverde-Rebaza J., Moura M.-F., de Andrade Lopes A.","A survey of the applications of Bayesian networks in agriculture",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029952872&doi=10.1016%2fj.engappai.2017.07.003&partnerID=40&md5=7b4f21fd6d394c977c122d28670ed5cf","The application of machine learning to agriculture is currently experiencing a “surge of interest” from the academic community as well as practitioners from industry. This increased attention has produced a number of differing approaches that use varying machine learning frameworks. It is arguable that Bayesian Networks are particularly suited to agricultural research due to their ability to reason with incomplete information and incorporate new information. Bayesian Networks are currently underrepresented in the machine learning applied to agriculture research literature, and to date there are no survey papers that currently centralize the state of the art. The aim of this paper is rectify the lack of a survey paper in this area by providing a self-contained resource that will: centralize the current state of the art, document the historical progression of Bayesian Networks in agriculture and indicate possible future lines of research as well as providing an introduction to Bayesian Networks for researchers who are new to the area. © 2017 Elsevier Ltd","Agriculture; Bayesian inference; Bayesian networks; Computational agriculture","Agricultural machinery; Agriculture; Artificial intelligence; Education; Inference engines; Learning systems; Surveys; Academic community; Agricultural research; Bayesian inference; Incomplete information; Possible futures; State of the art; Bayesian networks",2-s2.0-85029952872
"Tiwari S., Singh B., Kaur M.","An approach for feature selection using local searching and global optimization techniques",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016453447&doi=10.1007%2fs00521-017-2959-y&partnerID=40&md5=6acef4fb5a0358a932048828b8627856","Classification problems such as gene expression array analysis, text processing of Internet document, combinatorial chemistry, software defect prediction and image retrieval involve tens or hundreds of thousands of features in the dataset. However, many of these features may be irrelevant and redundant, which only worsen the performance of the learning algorithms, and this may lead to the problem of overfitting. These superfluous features only degrade the accuracy and the computation time of a classification algorithm. So, the selection of relevant and nonredundant features is an important preprocessing step of any classification problem. Most of the global optimization techniques have the ability to converge to a solution quickly, but these begin with initializing a population randomly and the choice of initial population is an important step. In this paper, local searching algorithms have been used for generating a subset of relevant and nonredundant features; thereafter, a global optimization algorithm has been used so as to remove the limitations of global optimization algorithms, like lack of consistency in classification results and very high time complexity, to some extent. The computation time and classification accuracy are improved by using a feature set obtained from sequential backward selection and mutual information maximization algorithm which is fed to a global optimization technique (genetic algorithm, differential evolution or particle swarm optimization). In this proposed work, the computation time of these global optimization techniques has been reduced by using variance as stopping criteria. The proposed approach has been tested on publicly available Sonar, Wdbc and German datasets. © 2017, The Natural Computing Applications Forum.","Mutual information maximization; Optimization algorithms; Sequential backward selection; Support vector machine","Bioassay; Biochips; Chemical analysis; Classification (of information); Evolutionary algorithms; Feature extraction; Gene expression; Genetic algorithms; Global optimization; Image retrieval; Information retrieval systems; Microarrays; Particle swarm optimization (PSO); Support vector machines; Text processing; Classification algorithm; Global optimization algorithm; Global optimization techniques; Local searching algorithm; Mutual information maximization; Optimization algorithms; Sequential backward selection; Software defect prediction; Optimization",2-s2.0-85016453447
"Alexandrov Y.I., Krylov A.K., Arutyunova K.R.","Activity during learning and the nonlinear differentiation of experience",2017,"Nonlinear Dynamics, Psychology, and Life Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030561318&partnerID=40&md5=ca374696e98cd75e25f50c483381f5c3","Walter Freeman's work emphasises the role of individual activity and intentionality as opposed to the traditional stimulus-reaction view and the machine metaphor. The results of our computer modeling studies suggest the nonlinear dynamics of experience emerging from perception-action cycles. We consider the perception-action cycle as a behavioral continuum of anticipated outcomes of actions. Neuroscientific research shows that each behavioral act is based on the activity of behaviorally specialized neurons distributed across the brain. Active learning during individual development leads to an increasing differentiation of the structure of individual experience through the formation of such groups of behaviorally specialized neurons. We consider the differentiation of individual experience as a nonlinear process which is implemented at different levels, and argue that consciousness and emotion can be described as dynamic characteristics prominent at the most and least differentiated systemic levels, correspondingly. © 2017 Society for Chaos Theory in Psychology & Life Sciences.","Behavior; Brain; Individual development; Intentionality",,2-s2.0-85030561318
"Lu W., Li Z., Chu J.","Adaptive Ensemble Undersampling-Boost: A novel learning framework for imbalanced data",2017,"Journal of Systems and Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026228060&doi=10.1016%2fj.jss.2017.07.006&partnerID=40&md5=34b30efac69820763ce3fc4ddf262ade","As one of the most challenging and attractive problems in the pattern recognition and machine intelligence field, imbalanced classification has received a large amount of research attention for many years. In binary classification tasks, one class usually tends to be underrepresented when it consists of far fewer patterns than the other class, which results in undesirable classification results, especially for the minority class. Several techniques, including resampling, boosting and cost-sensitive methods have been proposed to alleviate this problem. Recently, some ensemble methods that focus on combining individual techniques to obtain better performance have been observed to present better classification performance on the minority class. In this paper, we propose a novel ensemble framework called Adaptive Ensemble Undersampling-Boost for imbalanced learning. Our proposal combines the Ensemble of Undersampling (EUS) technique, Real Adaboost, cost-sensitive weight modification, and adaptive boundary decision strategy to build a hybrid algorithm. The superiority of our method over other state-of-the-art ensemble methods is demonstrated by experiments on 18 real world data sets with various data distributions and different imbalance ratios. Given the experimental results and further analysis, our proposal is proven to be a promising alternative that can be applied to various imbalanced classification domains. © 2017 Elsevier Inc.","Adaptive decision boundary; Classification; Ensemble Undersampling; Imbalanced data sets; Real Adaboost; Voting algorithm","Adaptive boosting; Education; Pattern recognition; Binary classification; Classification performance; Classification results; Decision boundary; Imbalanced classification; Imbalanced Data-sets; Under-sampling; Voting algorithm; Classification (of information)",2-s2.0-85026228060
"Li Y., Chen H.","Reverberation Robust Feature Extraction for Sound Source Localization Using a Small-Sized Microphone Array",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461136&doi=10.1109%2fJSEN.2017.2739144&partnerID=40&md5=b95f0e0f64a07234d4903fcc97a1a8b8","Conventional methods for sound source localization using microphone arrays are usually addressed from the signal processing viewpoint, where the sound source location is treated as a continuous parameter to be estimated over some spatial space. Actually, in some practical scenarios, such as in conference rooms and cars, sound source locations are only confined to some predefined areas. Therefore, it is more reasonable to deal with the problem from a machine learning point of view. By incorporating the prior information available about sound environments, machine learning-based methods have the potential to better deal with sound source localization in the presence of room reverberation. The key to machine learning-based sound source localization methods is how to extract effective source location features. The existing feature extraction schemes, such as the popular time-difference-of-arrival features, however, are not suitable for small-sized sensor arrays, due to the fact that sound source localization in reverberant environments become much challenging for small-sized arrays. To combat the problem, in this paper, we propose a reverberation robust feature extraction method for sound source localization based on sound intensity (SI) estimation using a small-sized microphone array. In particular, three robust feature extraction procedures have been employed in the proposed features, including normalization, phase transform weighting, and fully incorporating the redundancies in SI estimation. Simulation and real-world experimental results both show that the proposed sound source location features are more effective for small-sized arrays in reverberant environments when compared with the existing features. © 2001-2012 IEEE.","feature extraction; microphone array; room reverberation; sound intensity; Sound source localization","Acoustic intensity; Architectural acoustics; Artificial intelligence; Extraction; Feature extraction; Learning systems; Location; Microphones; Position measurement; Reverberation; Sensor arrays; Signal processing; Silicon; Continuous parameters; Conventional methods; Microphone arrays; Reverberant environment; Robust feature extractions; Room reverberations; Sound source localization; Sound source location; Acoustic generators",2-s2.0-85028461136
"Gnecco G.","Symmetry and antisymmetry properties of optimal solutions to regression problems",2017,"Optimization Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006942904&doi=10.1007%2fs11590-016-1101-x&partnerID=40&md5=a2ce9f4fe921a8862fc14eb5c8e0f3be","Besides requiring a good fit of the learned model to the empirical data, machine learning problems usually require such a model to satisfy additional constraints. Their satisfaction can be either imposed a-priori, or checked a-posteriori, once the optimal solution to the learning problem has been determined. In this framework, it is proved in the paper that the optimal solutions to several batch and online regression problems (specifically, the Ordinary Least Squares, Tikhonov regularization, and Kalman filtering problems) satisfy, under certain conditions, either symmetry or antisymmetry constraints, where the symmetry/antisymmetry is defined with respect to a suitable transformation of the data. Computational issues related to the obtained theoretical results (i.e., reduction of the dimensions of the matrices involved in the computations of the optimal solutions) are also described. The results, which are validated numerically, have potential application in machine-learning problems such as pairwise binary classification, learning of preference relations, and learning the weights associated with the directed arcs of a graph under symmetry/antisymmetry constraints. © 2016, Springer-Verlag Berlin Heidelberg.","Kalman filtering; Optimal solutions; Ordinary Least Squares; Symmetry and antisymmetry; Tikhonov regularization","Artificial intelligence; Directed graphs; Kalman filters; Learning systems; Metadata; Optimal systems; Antisymmetries; Kalman-filtering; Optimal solutions; Ordinary least squares; Tikhonov regularization; Filtration",2-s2.0-85006942904
"Kumar R.","Hand image biometric based personal authentication system",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992052954&doi=10.1007%2f978-3-319-44790-2_10&partnerID=40&md5=a7c78e5f2ceeb78d75fc0a5e23d8ef97","Hand geometry is widely accepted biometric modality for identification of human beings. This is considered as safest biometric indicator due to its strong resistance against the unauthorized access and easy to use modality from the user point of view. This chapter presents an approach for the personal authentication using geometrical structure of hand images. The proposed approach consists of many phases like acquisition of hand images of the user to the system, normalization of images, normalized contour and palm region extraction etc. The contour of the hand region from Region of Interest (ROI) is computed and is used to extract structural information, which describe the shape of the hand. The features of the test and the trainee images are matched using machine learning based classifier at the verification stage. © Springer International Publishing Switzerland 2017.","Feature extraction; Finger width; Hand geometry; Support vector machine",,2-s2.0-84992052954
"Qu K., Han K., Wu S., Wang G., Wei L.","Identification of DNA-binding proteins using mixed feature representation methods",2017,"Molecules",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764658&doi=10.3390%2fmolecules22101602&partnerID=40&md5=85e42dffab22a874b3353226f0028225","DNA-binding proteins play vital roles in cellular processes, such as DNA packaging, replication, transcription, regulation, and other DNA-associated activities. The current main prediction method is based on machine learning, and its accuracy mainly depends on the features extraction method. Therefore, using an efficient feature representation method is important to enhance the classification accuracy. However, existing feature representation methods cannot efficiently distinguish DNA-binding proteins from non-DNA-binding proteins. In this paper, a multi-feature representation method, which combines three feature representation methods, namely, K-Skip-N-Grams, Information theory, and Sequential and structural features (SSF), is used to represent the protein sequences and improve feature representation ability. In addition, the classifier is a support vector machine. The mixed-feature representation method is evaluated using 10-fold cross-validation and a test set. Feature vectors, which are obtained from a combination of three feature extractions, show the best performance in 10-fold cross-validation both under non-dimensional reduction and dimensional reduction by max-relevance-max-distance. Moreover, the reduced mixed feature method performs better than the non-reduced mixed feature technique. The feature vectors, which are a combination of SSF and K-Skip-N-Grams, show the best performance in the test set. Among these methods, mixed features exhibit superiority over the single features. 2017 by the authors.","DNA-binding protein; Mixed feature representation methods; Support vector machine",,2-s2.0-85030764658
"Lahoti M., Narang P., Tan K.H., Yang E.-H.","Mix design factors and strength prediction of metakaolin-based geopolymer",2017,"Ceramics International",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020218909&doi=10.1016%2fj.ceramint.2017.06.006&partnerID=40&md5=2c812217db8e23e99b5a21ba4f65a2d1","Geopolymer is a promising alternative binder to Portland cement. However, the importance of mix design parameters affecting the mechanical properties of geopolymer has yet to be quantitatively assessed. This work evaluates the significance of the four common mix design parameters, namely Si/Al (molar ratio), water/solids (mass ratio), Al/Na (molar ratio) and H2O/Na2O (molar ratio), in determining compressive strength of metakaolin-based geopolymers through experiments and statistical analyses. In addition, machine learning-based classifiers were engaged for strength predictions. Results showed that Si/Al ratio is the most significant parameter followed by Al/Na ratio. Unlike ordinary Portland cement system, water/solids ratio is not the chief factor governing strength of metakaolin-based geopolymers. Machine learning-based classifiers were able to predict the compressive strength with high precision. The strength predictions can potentially guide preliminary mix proportioning of metakaolin-based geopolymers to achieve required strength grade without going through tedious (trial and error) mix formulation. © 2017 Elsevier Ltd and Techna Group S.r.l.","Attribute evaluation; Electron microscopy; Porosity; Strength","Aluminum; Artificial intelligence; Cements; Electron microscopy; Forecasting; Geopolymers; Inorganic polymers; Learning systems; Porosity; Portland cement; Attribute evaluation; High-precision; Mix formulation; Mix proportioning; Ordinary Portland cement; Strength; Strength prediction; Trial and error; Compressive strength",2-s2.0-85020218909
"Kanduri A., Haghbayan M.-H., Rahmani A.M., Liljeberg P., Jantsch A., Tenhunen H., Dutt N.","Accuracy-Aware Power Management for Many-Core Systems Running Error-Resilient Applications",2017,"IEEE Transactions on Very Large Scale Integration (VLSI) Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018969687&doi=10.1109%2fTVLSI.2017.2694388&partnerID=40&md5=1b496e7a5baa74af62d4e90c36c32ee2","Power capping techniques based on dynamic voltage and frequency scaling (DVFS) and power gating (PG) are oriented toward power actuation, compromising on performance and energy. Inherent error resilience of emerging application domains, such as Internet-of-Things (IoT) and machine learning, provides opportunities for energy and performance gains. Leveraging accuracy-performance tradeoffs in such applications, we propose approximation (APPX) as another knob for close-looped power management, to complement power knobs with performance and energy gains. We design a power management framework, APPEND+, that can switch between accurate and approximate modes of execution subject to system throughput requirements. APPEND+ considers the sensitivity of the application to error to make disciplined alteration between levels of APPX such that performance is maximized while error is minimized. We implement a power management scheme that uses APPX, DVFS, and PG knobs hierarchically. We evaluated our proposed approach over machine learning and signal processing applications along with two case studies on IoT - early warning score system and fall detection. APPEND+ yields $1.9\times $ higher throughput, improved latency up to five times, better performance per energy, and dark silicon mitigation compared with the state-of-the-art power management techniques over a set of applications ranging from high to no error resilience. © 1993-2012 IEEE.","Approximate computing; dark silicon; Internet-of-Things (IoT); power management; runtime mapping","Artificial intelligence; Dynamic frequency scaling; Energy management; Errors; Internet of things; Knobs; Learning systems; Signal processing; Voltage scaling; Early warning score; Emerging applications; Internet of Things (IOT); Management frameworks; Performance trade-off; Power management scheme; Power management techniques; Signal processing applications; Power management",2-s2.0-85018969687
"Sethi R.J., Gil Y.","Scientific workflows in data analysis: Bridging expertise across multiple domains",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010888019&doi=10.1016%2fj.future.2017.01.001&partnerID=40&md5=86aedb1fa3c657ccf279af42a01c198b","In this paper, we demonstrate the use of scientific workflows in bridging expertise across multiple domains by re-purposing workflow fragments in the areas of text analysis, image analysis, and analysis of activity in video. We highlight how the reuse of workflows allows scientists to link across disciplines and avail themselves of the benefits of inter-disciplinary research beyond their normal area of expertise. In addition, we present in-depth studies of various tasks, including tasks for text analysis, multimedia analysis involving both images and text, video activity analysis, and analysis of artistic style using deep learning. These tasks show how the re-use of workflow fragments can turn a pre-existing, rudimentary approach into an expert-grade analysis. We also examine how workflow fragments save time and effort while amalgamating expertise in multiple areas such as machine learning and computer vision. © 2017 Elsevier B.V.","Data analysis; Workflow fragments; Workflows","Computer vision; Data handling; Data reduction; Information analysis; Learning systems; Inter-disciplinary researches; Multi-media analysis; Multiple areas; Multiple domains; Scientific workflows; Video activity; Work-flows; Workflow fragments; Image analysis",2-s2.0-85010888019
"Tian S., Yan Y., Yu L., Qian J., Ye F.","Prediction of anti-HIV activity on the basis of stacked auto-encoder",2017,"Journal of Chemometrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025086041&doi=10.1002%2fcem.2916&partnerID=40&md5=01ca7e417284822b28160d15847946df","The prediction of biologically active compounds plays a very important role for high-throughput screening approaches in drug discovery. Most computational models, in this area, concentrate on measuring structural similarities between chemical elements. There are various methods to predict anti-HIV activity, such as artificial neural network and support vector machine, but generally using shallow machine learning with low accuracies and less samples. In this work, one of deep learning methods, stacked auto-encoder (SAE), is proposed to predict anti-HIV activity of a broad group of compounds for the first time. Through contrasting experiments of artificial neural network, support vector machine, and SAE under the same condition, the accuracy after descriptors screening is higher than using raw descriptors, and SAE performs better than the other two methods to achieve the perfect forecast of anti-HIV activity. It has a great significance on promoting anti-HIV drug design, which therefore can reduce research and development costs and improve the efficiency of anti-HIV drug discovery. Copyright © 2017 John Wiley & Sons, Ltd.","anti-HIV activity prediction; classification modeling; deep learning; stacked auto-encoder",,2-s2.0-85025086041
"Pizzolato S., Tagliapietra L., Cognolato M., Reggiani M., Müller H., Atzori M.","Comparison of six electromyography acquisition setups on hand movement classification tasks",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031088739&doi=10.1371%2fjournal.pone.0186132&partnerID=40&md5=7cdfdb5b9985360e82dc3bbe9a6816d8","Hand prostheses controlled by surface electromyography are promising due to the noninvasive approach and the control capabilities offered by machine learning. Nevertheless, dexterous prostheses are still scarcely spread due to control difficulties, low robustness and often prohibitive costs. Several sEMG acquisition setups are now available, ranging in terms of costs between a few hundred and several thousand dollars. The objective of this paper is the relative comparison of six acquisition setups on an identical hand movement classification task, in order to help the researchers to choose the proper acquisition setup for their requirements. The acquisition setups are based on four different sEMG electrodes (including Otto Bock, Delsys Trigno, Cometa Wave + Dormo ECG and two Thalmic Myo armbands) and they were used to record more than 50 hand movements from intact subjects with a standardized acquisition protocol. The relative performance of the six sEMG acquisition setups is compared on 41 identical hand movements with a standardized feature extraction and data analysis pipeline aimed at performing hand movement classification. Comparable classification results are obtained with three acquisition setups including the Delsys Trigno, the Cometa Wave and the affordable setup composed of two Myo armbands. The results suggest that practical sEMG tests can be performed even when costs are relevant (e.g. in small laboratories, developing countries or use by children). All the presented datasets can be used for offline tests and their quality can easily be compared as the data sets are publicly available. © 2017 Pizzolato et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; clinical laboratory; clinical protocol; controlled study; data analysis; data extraction; developing country; electromyography; female; hand movement; health care cost; human; image acquisition; imaging and display; intermethod comparison; male; standardization; task performance; accelerometry; amputee; comparative study; devices; electrode; electromyography; hand; limb prosthesis; movement (physiology); physiology; procedures; rehabilitation; support vector machine; young adult; Accelerometry; Adult; Amputees; Artificial Limbs; Electrodes; Electromyography; Female; Hand; Humans; Male; Movement; Support Vector Machine; Young Adult",2-s2.0-85031088739
"Nasim M., Rextin A., Hayat S., Khan N., Malik M.M.","Data analysis and call prediction on dyadic data from an understudied population",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028710103&doi=10.1016%2fj.pmcj.2017.08.002&partnerID=40&md5=39d19c7727ea56aec83c9eecc0d5d71a","In this paper we predict outgoing mobile phone calls using machine learning and time clusters based approaches. We analyze to which extent the calling activity of mobile phone users is predictable. The premise is that mobile phone users exhibit temporal regularity in their interactions with majority of their contacts. In the sociological context, most social interactions have fairly reliable temporal regularity. If we quantify the extension of this behavior to interactions on mobile phones we expect that pairwise interaction is not merely a result of randomness, rather it exhibits a temporal pattern. To this end, we not only tested our approach on an original mobile phone usage dataset from a developing country, Pakistan, but we also analyzed the famous Reality Mining Dataset and the Nokia Dataset (from a European country), where we found an equitable basis for comparison with our data. Our original data consists of 783 users and more than 12,000 active dyads. Our results show that temporal information about pairwise user interactions can predict future calls with reasonable accuracy. © 2017 Elsevier B.V.","Call prediction; Call-logs; Smartphone; Temporal regularity","Cellular telephones; Developing countries; Forecasting; Learning systems; Mobile phones; Telephone sets; Call-logs; European Countries; Mobile phone usages; Pairwise interaction; Reasonable accuracy; Social interactions; Temporal information; Temporal regularity; Population statistics",2-s2.0-85028710103
"Huang J., Keung J.W., Sarro F., Li Y.-F., Yu Y.T., Chan W.K., Sun H.","Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study",2017,"Journal of Systems and Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024833152&doi=10.1016%2fj.jss.2017.07.012&partnerID=40&md5=69d3f010b51fccd7e2b9e5719f182806","Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation. © 2017 Elsevier Inc.","Cross-validation; Empirical software engineering estimation; Imputation; KNN; Missing data","Autocorrelation; Classification (of information); Computer software selection and evaluation; Data handling; Data mining; Learning algorithms; Motion compensation; Software engineering; Classification accuracy; Cross validation; Empirical Software Engineering; Experimental assessment; Imputation; K nearest neighbor (KNN); Missing data; Missing data imputations; Nearest neighbor search",2-s2.0-85024833152
"Che J., Yang Y., Li L., Bai X., Zhang S., Deng C.","Maximum relevance minimum common redundancy feature selection for nonlinear data",2017,"Information Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019245357&doi=10.1016%2fj.ins.2017.05.013&partnerID=40&md5=fca33751d597401bc87c6bc514a5fa30","In recent years, feature selection based on relevance redundancy trade-off criteria has become a very promising and popular approach in the field of machine learning. However, the existing algorithmic frameworks of mutual information feature selection have certain limitations for the common feature selection problems in practice. To overcome these limitations, the idea of a new framework is developed by introducing a novel maximum relevance and minimum common redundancy criterion and a minimax nonlinear optimization approach. In particular, a novel mutual information feature selection method based on the normalization of the maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented, which produces a normalized value in the range [0, 1] and results in a regression problem. We perform extensive experimental comparisons over numerous state-of-art algorithms using different forecasts (Bayesian Additive Regression tree, treed Gaussian process, k-NN, and SVM) and different data sets (two simulated and five real datasets). The results show that the proposed algorithm outperforms the others in terms of feature selection and forecasting accuracy. © 2017 Elsevier Inc.","Feature selection; Maximal relevance; Minimal common redundancy; Mutual information; Normalization","Economic and social effects; Forestry; Learning systems; Nearest neighbor search; Nonlinear programming; Optimization; Redundancy; Trees (mathematics); Algorithmic framework; Bayesian additive regression trees; Experimental comparison; Maximal relevance; Mutual information feature selections; Mutual informations; Non-linear optimization; Normalization; Feature extraction",2-s2.0-85019245357
"Valiant G., Valiant P.","Estimating the unseen: Improved estimators for entropy and other properties",2017,"Journal of the ACM",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032826826&doi=10.1145%2f3125643&partnerID=40&md5=0e8fba8a782aa6a9d75a670a411c6905","We show that a class of statistical properties of distributions, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Specifically, given a sample consisting of independent draws from any distribution over at most k distinct elements, these properties can be estimated accurately using a sample of size O(k/log k). For these estimation tasks, this performance is optimal, to constant factors. Complementing these theoretical results, we also demonstrate that our estimators perform exceptionally well, in practice, for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. The key step in our approach is to first use the sample to characterize the ""unseen"" portion of the distribution-effectively reconstructing this portion of the distribution as accurately as if one had a logarithmic factor larger sample. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: We seek to estimate the shape of the unobserved portion of the distribution. This work can be seen as introducing a robust, general, and theoretically principled framework that, for many practical applications, essentially amplifies the sample size by a logarithmic factor; we expect that it may be fruitfully used as a component within larger machine learning and statistical analysis systems.","Distinct elements; Entropy estimation; Statistical property estimation; Unseen species","Entropy; Frequency estimation; Learning systems; Constant factors; Distinct elements; Entropy estimation; Natural distribution; Statistical analysis systems; Statistical properties; Total probabilities; Unseen species; Probability distributions",2-s2.0-85032826826
"Zhang P., Zhou X., Pelliccione P., Leung H.","RBF-MLMR: A Multi-Label Metamorphic Relation Prediction Approach Using RBF Neural Network",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030757764&doi=10.1109%2fACCESS.2017.2758790&partnerID=40&md5=5bcfb6bdbe4f23a6c01baf666ffc5ef7","Metamorphic testing has been successfully used in many different fields to solve the test oracle problem. However, how to find a set of appropriate metamorphic relations for metamorphic testing remains a complicated and tedious task. Recently some machine learning approaches have been proposed to predict metamorphic relations. These approaches predicting single label metamorphic relation can alleviate this problem to some extent. However, many applications involve multi-group metamorphic relations, and these approaches are clearly inefficient. To address this problem, in this paper we propose a Multi-Label Metamorphic Relations prediction approach based on an improved RBF (Radial Basis Function) neural network named RBF-MLMR. First, RBF-MLMR uses state-of-the-art soot analysis tool to generate control flow graph and corresponds labels from the source codes of programs. Second, the extracted nodes and the path properties constitute multi-label data sets for the control flow graph. Finally, a multi-label RBF neural network prediction model is established to predict whether the program satisfies multiple metamorphic relations. In order to improve the prediction results, AP (Affinity Propagation) and k-means clustering algorithms are used to optimize the RBF neural network structure of RBF-MLMR. A set of dedicated experiments based on public programs is conducted to validate RBF-MLMR. The experimental results show that RBF-MLMR can achieve accuracy of around 80&#x0025; for predicting two and three metamorphic relations. OAPA","Algorithm design and analysis; Label count vector; Mathematical model; Metamorphic relation; Metamorphic testing; Multi-label; Neural networks; Prediction algorithms; Predictive models; RBF neural network; Software; Testing","Clustering algorithms; Codes (symbols); Computer software; Data flow analysis; Flow graphs; Forecasting; Graphic methods; Learning systems; Mathematical models; Neural networks; Software testing; Testing; Algorithm design and analysis; Metamorphic relations; Metamorphic testing; Multi-label; Prediction algorithms; Predictive models; RBF Neural Network; Radial basis function networks",2-s2.0-85030757764
"Jia H., Ding S., Du M.","A Nyström spectral clustering algorithm based on probability incremental sampling",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965005897&doi=10.1007%2fs00500-016-2160-8&partnerID=40&md5=8e52a62b183efe8292dc6b1ca9b3d95c","Spectral clustering will map the data points of the original space into a low-dimensional eigen-space to make them linearly separable, so it is able to process the data with complex structures. However, spectral clustering needs to store the entire similarity matrix and requires eigen-decomposition. Both procedures will consume a lot of time and space resources, limiting the application of spectral clustering algorithm in large-scale data environment. To reduce the complexity of spectral clustering algorithm, we may use the Nyström extension technique to calculate the approximate eigenvectors by sampling a few of data points. This method sacrifices the clustering accuracy in exchange for the improvement of the algorithm efficiency. To select more representative sample points to reflect the distribution of data sets much better, this paper designs a dynamic incremental sampling method used for the Nyström spectral clustering, in which the data points are sampled according to different probability distributions and we theoretically prove that the increase of sampling times can effectively decrease the sampling error. The feasibility and effectiveness of the proposed algorithm are analyzed by the experiments on UCI machine learning data sets. © 2016, Springer-Verlag Berlin Heidelberg.","Eigen-decomposition; Incremental sampling; Nyström method; Spectral clustering","Algorithms; Artificial intelligence; Learning systems; Probability distributions; Sampling; Algorithm efficiency; Approximate eigenvectors; Eigen decomposition; Incremental samplings; M method; Representative sample; Spectral clustering; Spectral clustering algorithms; Clustering algorithms",2-s2.0-84965005897
"Prasad S., Peddoju S.K., Ghosh D.","An adaptive plant leaf mobile informatics using RSSC",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992383120&doi=10.1007%2fs11042-016-4040-8&partnerID=40&md5=6ae7301cedb0677166ac8aa0a009d953","An automated plant biometric system is now an important step in preserving nature’s biodiversity. This paper presents a novel Relative Sub-image Sparse Coefficient (RSSC) algorithm for mobile devices (MDs) representing plant leaves into a mathematically compact vector for its classification. The RSSC feature vector includes local Statistical Entropy Texture (SET) information inter-related to all the sub-images within a leaf. RSSC space is merged with Gray Level Co-occurrence Matrix (GLCM) feature to refine the outputs using best-Nearest Neighbor (best-NN), designed for MDs. The experiments were performed on three different types of leaf datasets: (i) Flavia, (ii) ICL and (iii) Diseased leaf datasets. The results proves our method more accurate and better compared to other existing plant identification systems. The proposed approach is also tolerant under shape distortion caused while capturing. The mobile machine learning system for leaf image informatics is deployed on Android devices which helps botanists, agriculturists and medical biologists to recognize ubiquitously the herbs and plant species anywhere-anytime. © 2016, Springer Science+Business Media New York.","Best-NN; Human mobile interaction (HMI); Leaf image informatics; Mobile vision (MV); Relative sub-image sparse coefficients (RSSC); Shape descriptor","Artificial intelligence; Biodiversity; Biometrics; Information science; Learning systems; Medical imaging; Mobile devices; Best-NN; Leaf images; Mobile interaction; Mobile vision; Shape descriptors; Subimages; Plants (botany)",2-s2.0-84992383120
"Manjarres D., Mera A., Perea E., Lejarazu A., Gil-Lopez S.","An energy-efficient predictive control for HVAC systems applied to tertiary buildings based on regression techniques",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026436776&doi=10.1016%2fj.enbuild.2017.07.056&partnerID=40&md5=aedcfd978b24bc854db511c978c2822e","Heating ventilation and air conditioning (HVAC) systems represent an important amount of the total energy use in office buildings, accounting for near 30%. Moreover, in countries affected by extreme climates HVAC systems’ contribution to energy demand increases up to 50%. Therefore, the automation of energy efficient strategies that act on the Building Energy Management System (BEMS) in order to improve building energy use becomes increasingly relevant. This paper delves into the devising of a novel HVAC optimization framework, coined as Next24h-Energy, which consists on a two-way communication system, an enhanced database management system and a set of machine learning algorithms based on random forest (RF) regression techniques mainly focused on providing an energy-efficient predictive control of the HVAC system. Therefore, the proposed framework achieves optimal HVAC ON/OFF and mechanical ventilation (MV) schedule operation that minimizes the energy consumption while keeps the building between a predefined indoor temperature margins. Simulation results assess the performance of the proposed Next 24 h-Energy framework at a real office building named Mikeletegi 1 (M1) in Donostia-San Sebastian (Spain) yielding to excellent results and significant energy savings by virtue of its capability of adapting the parameters that control the HVAC schedule in a daily basis without affecting user comfort conditions. Specifically, the energy reduction for the test period is estimated in 48% for the heating and 39% for the cooling consumption. © 2017 Elsevier B.V.","Building automation; Building energy management system (BEMS); Energy efficiency; Heating ventilation and air conditioning (HVAC); Optimisation algorithms; Supply air temperature optimal control; Thermal comfort","Air conditioning; Buildings; Climate control; Decision trees; Energy conservation; Energy efficiency; Energy management; Energy utilization; Heat pump systems; Intelligent buildings; Learning algorithms; Office buildings; Optimization; Thermal comfort; Ventilation; Building automation; Building energy management systems; Heating ventilation and air conditioning; Optimal controls; Optimisations; Energy management systems",2-s2.0-85026436776
"Zhou Z., So A.M.-C.","A unified approach to error bounds for structured convex optimization problems",2017,"Mathematical Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010824780&doi=10.1007%2fs10107-016-1100-9&partnerID=40&md5=42eb3f75488b1d20ab26f1d0fcb1e10d","Error bounds, which refer to inequalities that bound the distance of vectors in a test set to a given set by a residual function, have proven to be extremely useful in analyzing the convergence rates of a host of iterative methods for solving optimization problems. In this paper, we present a new framework for establishing error bounds for a class of structured convex optimization problems, in which the objective function is the sum of a smooth convex function and a general closed proper convex function. Such a class encapsulates not only fairly general constrained minimization problems but also various regularized loss minimization formulations in machine learning, signal processing, and statistics. Using our framework, we show that a number of existing error bound results can be recovered in a unified and transparent manner. To further demonstrate the power of our framework, we apply it to a class of nuclear-norm regularized loss minimization problems and establish a new error bound for this class under a strict complementarity-type regularity condition. We then complement this result by constructing an example to show that the said error bound could fail to hold without the regularity condition. We believe that our approach will find further applications in the study of error bounds for structured convex optimization problems. © 2017, Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society.","65K10; 90C25; 90C52","Constrained optimization; Convex optimization; Error analysis; Errors; Functions; Iterative methods; Learning systems; Optimization; Signal processing; Constrained minimization problem; Convergence rates; Objective functions; Optimization problems; Regularity condition; Residual functions; Strict complementarity; Structured convex optimizations; Problem solving",2-s2.0-85010824780
"Lefevre S., Tuia D., Wegner J.D., Produit T., Nassaar A.S.","Toward Seamless Multiview Scene Analysis from Satellite to Street Level",2017,"Proceedings of the IEEE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018876359&doi=10.1109%2fJPROC.2017.2684300&partnerID=40&md5=ba00b310131ffa81030a093bb797c032","In this paper, we discuss and review how combined multiview imagery from satellite to street level can benefit scene analysis. Numerous works exist that merge information from remote sensing and images acquired from the ground for tasks such as object detection, robots guidance, or scene understanding. What makes the combination of overhead and street-level images challenging are the strongly varying viewpoints, the different scales of the images, their illuminations and sensor modality, and time of acquisition. Direct (dense) matching of images on a per-pixel basis is thus often impossible, and one has to resort to alternative strategies that will be discussed in this paper. For such purpose, we review recent works that attempt to combine images taken from the ground and overhead views for purposes like scene registration, reconstruction, or classification. After the theoretical review, we present three recent methods to showcase the interest and potential impact of such fusion on real applications (change detection, image orientation, and tree cataloging), whose logic can then be reused to extend the use of ground-based images in remote sensing and vice versa. Through this review, we advocate that cross fertilization between remote sensing, computer vision, and machine learning is very valuable to make the best of geographic data available from Earth observation sensors and ground imagery. Despite its challenges, we believe that integrating these complementary data sources will lead to major breakthroughs in Big GeoData. It will open new perspectives for this exciting and emerging field. © 1963-2012 IEEE.","classification; Computer vision; data fusion; ground based imagery; localization; object detection; remote sensing","Computer vision; Learning systems; Object detection; Satellite imagery; Complementary data; Cross fertilization; Earth observation sensors; Image orientation; Potential impacts; Real applications; Scene registration; Scene understanding; Remote sensing",2-s2.0-85018876359
"Gao Q., Dey P., Ahammad P.","Perceived performance of top retail webpages in the wild",2017,"Computer Communication Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032728183&doi=10.1145%2f3155055.3155062&partnerID=40&md5=dbdd63cf782e93cbda6e425ebcdebdee","Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly inuence human end-user's perception of the Speed of a page. In this paper we present SpeedPerception, a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process. In Phase-1 of our SpeedPerception study using Internet Retailer Top 500 (IR 500) websites, we found that commonly used navigation metrics such as onLoad and Time To First Byte (TTFB) fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with 87 2% accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its visualComplete eventGrant:-We thank Estelle Weyl, Patrick Meenan (WebPagetest), and Ilya Grigorik for evangelizing SpeedPerception and gen-erating significant user participation. We thank Paul Irish, Pierre-Marie Dartus, Shubhie Panicker, and Addy Osmani for independently evaluating and porting PSI to Google Chrome Lighthouse project.","Above-the-fold; Crowdsourcing; OnLoad; Perceived speed; Perceptual SpeedIndex; Quality of experience; SpeedIndex; TTFB; Web performance","Crowdsourcing; Learning systems; Open systems; Websites; Above-the-fold; OnLoad; Perceptual SpeedIndex; Quality of experience (QoE); SpeedIndex; TTFB; Web performance; Quality of service",2-s2.0-85032728183
"Campos Y., Sossa H., Pajares G.","Comparative analysis of texture descriptors in maize fields with plants, soil and object discrimination",2017,"Precision Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007496841&doi=10.1007%2fs11119-016-9483-4&partnerID=40&md5=ae3253b5e1bbac6741c13b3e2e7ef0e8","Precision Agriculture aims to apply selective treatments and tasks at localized areas concerning crop fields. Robotized and autonomous tractors, equipped with perception, decision-making and actuation systems, can apply specific treatments as may be required. Correct plant identification through the perception system, including crops and weeds, is an important issue. Additionally, it is well known that, in autonomous vehicles, safety is a major challenge, where unexpected obstacles in the working area must be conveniently addressed in order to guarantee the security and the continuity of the process. The objective of this study was to design a tri-class Support Vector Machine classifier for identifying plants (crops and weeds), soil and objects in maize fields based on unsupervised learning. For this, a strategy for automatic sample selection was designed to obtain elements of the three involved classes for the training process. In this context, the identification of obstacles for safe navigation makes an important contribution. A comparative analysis of different texture descriptors and local patterns was carried out with the aim of determining the best for characterizing the classes under study; results have shown that the Speeded-Up Robust Features descriptor is the most appropriate to discriminate between plants, soil and objects. The development of an object detection algorithm for agricultural images proved the effectiveness of the tri-class classifier with an accuracy of 94.3%. © 2016, Springer Science+Business Media New York.","Automatic sample selection; Autonomous vehicles; Natural images; Object detection; Texture analysis","agricultural land; agricultural soil; algorithm; comparative study; maize; navigation; precision agriculture; sampling; soil texture; support vector machine; vehicle component; Zea mays",2-s2.0-85007496841
"Chen C., Liu M., Liu H., Zhang B., Han J., Kehtarnavaz N.","Multi-Temporal Depth Motion Maps-Based Local Binary Patterns for 3D Human Action Recognition",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030749110&doi=10.1109%2fACCESS.2017.2759058&partnerID=40&md5=d887f8c0013395984421665cd0dc7d4d","This paper presents a local spatio-temporal descriptor for action recognition from depth video sequences which is capable of distinguishing similar actions as well as coping with different speeds of actions. This descriptor is based on three processing stages. In the first stage, the shape and motion cues are captured from a weighted depth sequence by temporally overlapped depth segments, leading to three improved depth motion maps (DMMs) compared to previously introduced DMMs. In the second stage, the improved DMMs are partitioned into dense patches, from which the local binary patterns histogram features are extracted to characterize local rotation invariant texture information. In the final stage, a Fisher kernel is used for generating a compact feature representation, which is then combined with a kernel-based extreme learning machine (ELM) classifier. The developed solution is applied to five public domain datasets and is extensively evaluated. The results obtained demonstrate the effectiveness of this solution as compared to the existing approaches. OAPA","Action Recognition; Cameras; Color; Depth Motion Maps; ELM Classifier; Feature extraction; Fisher Kernel; Histograms; Local Binary Patterns; Robustness; Skeleton; Three-dimensional displays","Cameras; Color; Feature extraction; Learning systems; Robustness (control systems); Action recognition; Fisher kernels; Histograms; Local binary patterns; Skeleton; Three-dimensional display; Graphic methods",2-s2.0-85030749110
"Treesatayapun C.","Discrete-time adaptive controller based on non-switch reaching condition and compact system dynamic estimator",2017,"Journal of the Franklin Institute",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028927226&doi=10.1016%2fj.jfranklin.2017.08.030&partnerID=40&md5=93d46db65c60b79a69ce556aff1daafe","An adaptive controller is proposed for a class of compact discrete-time systems when the controlled plants have unknown dynamics. The special structure of artificial neural networks (ANN) with membership activation functions is established to determine a pseudo-partial derivative (PPD) of the unknown plant. The on-line learning algorithm is only required to tune all adjustable parameters with the convergence analysis. The result of PPD estimator can be utilized for both positive and negative control directions. The control law is established by the estimated PPD and the sliding-reaching condition without any switching term. The convergence of tracking error is theoretically analyzed for the closed-loop system. A numerical example of negative control direction is investigated to verify the effectiveness of PPD estimator and control scheme. The experimental system with the prototype of DC-motor current control is constructed to demonstrate the practicability and performance for the case of positive control direction. © 2017 The Franklin Institute",,"Closed loop systems; Controllers; DC motors; Digital control systems; Discrete time control systems; Electric machine control; Membership functions; Neural networks; Activation functions; Adaptive controllers; Adjustable parameters; Control directions; Convergence analysis; Discrete - time systems; Experimental system; Partial derivatives; Adaptive control systems",2-s2.0-85028927226
"Yi F., Moon I., Javidi B.","Automated red blood cells extraction from holographic images using fully convolutional neural networks",2017,"Biomedical Optics Express",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030975386&doi=10.1364%2fBOE.8.004466&partnerID=40&md5=78a54129accb6d6779aa4be3f66b6121","In this paper, we present two models for automatically extracting red blood cells (RBCs) from RBCs holographic images based on a deep learning fully convolutional neural network (FCN) algorithm. The first model, called FCN-1, only uses the FCN algorithm to carry out RBCs prediction, whereas the second model, called FCN-2, combines the FCN approach with the marker-controlled watershed transform segmentation scheme to achieve RBCs extraction. Both models achieve good segmentation accuracy. In addition, the second model has much better performance in terms of cell separation than traditional segmentation methods. In the proposed methods, the RBCs phase images are first numerically reconstructed from RBCs holograms recorded with off-axis digital holographic microscopy. Then, some RBCs phase images are manually segmented and used as training data to fine-tune the FCN. Finally, each pixel in new input RBCs phase images is predicted into either foreground or background using the trained FCN models. The RBCs prediction result from the first model is the final segmentation result, whereas the result from the second model is used as the internal markers of the marker-controlled transform algorithm for further segmentation. Experimental results show that the given schemes can automatically extract RBCs from RBCs phase images and much better RBCs separation results are obtained when the FCN technique is combined with the marker-controlled watershed segmentation algorithm. © 2017 Optical Society of America.","(090.1995) digital holography; (100.6890) three-dimensional image processing; (150.0150) machine vision; (150.1135) algorithms; (170.3880) medical and biological imaging","Bioinformatics; Blood; Cells; Convolution; Extraction; Holograms; Holography; Image processing; Medical imaging; Microscopic examination; Neural networks; Convolutional neural network; Digital holographic microscopy; Digital holography; Marker-controlled watershed segmentation; Marker-controlled watersheds; Medical and biological imaging; Segmentation accuracy; Three-dimensional image processing; Image segmentation",2-s2.0-85030975386
"Karal O.","Maximum likelihood optimal and robust Support Vector Regression with lncosh loss function",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024501613&doi=10.1016%2fj.neunet.2017.06.008&partnerID=40&md5=96cd5a69b9d73cf622f732769b2a86e7","In this paper, a novel and continuously differentiable convex loss function based on natural logarithm of hyperbolic cosine function, namely lncosh loss, is introduced to obtain Support Vector Regression (SVR) models which are optimal in the maximum likelihood sense for the hyper-secant error distributions. Most of the current regression models assume that the distribution of error is Gaussian, which corresponds to the squared loss function and has helpful analytical properties such as easy computation and analysis. However, in many real world applications, most observations are subject to unknown noise distributions, so the Gaussian distribution may not be a useful choice. The developed SVR model with the parameterized lncosh loss provides a possibility of learning a loss function leading to a regression model which is maximum likelihood optimal for a specific input–output data. The SVR models obtained with different parameter choices of lncosh loss with ε-insensitiveness feature, possess most of the desirable characteristics of well-known loss functions such as Vapnik's loss, the Squared loss, and Huber's loss function as special cases. In other words, it is observed in the extensive simulations that the mentioned lncosh loss function is entirely controlled by a single adjustable λ parameter and as a result, it allows switching between different losses depending on the choice of λ. The effectiveness and feasibility of lncosh loss function are validated through a number of synthetic and real world benchmark data sets for various types of additive noise distributions. © 2017 Elsevier Ltd","Loss functions; Noise models; Outliers; Robustness; Support Vector Regression","Cosine transforms; Gaussian noise (electronic); Maximum likelihood; Regression analysis; Robustness (control systems); Analytical properties; Continuously differentiable; Error distributions; Extensive simulations; Loss functions; Noise models; Outliers; Support vector regression (SVR); Hyperbolic functions; Article; comparative effectiveness; computer model; computer simulation; data processing; feasibility study; mathematical computing; maximum likelihood method; noise; normal distribution; priority journal; regression analysis; support vector machine",2-s2.0-85024501613
"Wieczorek W.","Introduction",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992535773&doi=10.1007%2f978-3-319-46801-3_1&partnerID=40&md5=e03c65880c03466d523cab660fb5c516","The introductory chapter contains the different formulations of grammatical inference and indicates such formulations that will be considered in this book. Basically, the problem of grammatical inference within the present book is to be studied from machine learning and combinatorial optimization perspectives. In addition to this, the different representations of languages are assumed: deterministic and non-deterministic finite-state automata, regular expressions, and context-free grammars. As regards the machine learning approach, the design and analysis of learning experiments for comparing GI algorithms to each other or with machine learning methods will be discussed. Typical applications of the GI field are presented in this chapter as well. © Springer International Publishing AG 2017.",,,2-s2.0-84992535773
"Jayakanthan N., Ramani A.V.","Graph based classifier to detect malicious URL",2017,"International Journal of Mechanical and Production Engineering Research and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029672423&partnerID=40&md5=7c72fdf5287b21785d62785a11c68cbb","Malicious attack is a major issue in cyberspace. The criminal obtains vital information like username, password, and Credit/Debit card numbers, from the victims through deception. Various detection solutions are proposed in recent years. These techniques include blacklist, heuristics, machine learning, similarity and pattern matching methods. But, most of them are heavy weight methodologies in terms of time complexity and requires dedicated server for their execution. A Graph based Classifier to Detect Malicious URL (GCDMU), is proposed in this paper, which is a feature based classifier. It is a light weight, reliable approach and also effective, in detecting malicious URL. © TJPRC Pvt. Ltd.","Graph Based Classifier & Detection; Malicious URL",,2-s2.0-85029672423
"Eve M.P.","The great automatic grammatizator: Writing, labour, computers",2017,"Critical Quarterly",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032439344&doi=10.1111%2fcriq.12359&partnerID=40&md5=658f561e2c0dc002d01c162e3cbed4de","What does it actually mean when we say that computers can ‘write’, and how are recent developments in neural networks and machine learning changing this capacity? This article examines the long-standing literary fear of authorship being replaced by machines while also interrogating the labour and credit implications that sit behind widely used structures of authorship in a technological age. The argument makes reference to one work of computer-generated writing – Johannes Heldén and Håkan Jonson’s Evolution (2014) – and to one software paradigm (character-based recurrent neural networks for language acquisition trained on the corpus of the journal Textual Practice). I here argue that unless we conceive more broadly of the criteria for ‘authorship’ as a labour function, and unless we take seriously the need to see textual production as social production, hybridised (but predominantly) machine identities will come to dominate a literary landscape. © 2017, Blackwell Publishing Ltd. All rights reserved.",,,2-s2.0-85032439344
"Gálvez J.A., Jalali A., Ahumada L., Simpao A.F., Rehman M.A.","Neural Network Classifier for Automatic Detection of Invasive Versus Noninvasive Airway Management Technique Based on Respiratory Monitoring Parameters in a Pediatric Anesthesia",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028064381&doi=10.1007%2fs10916-017-0787-3&partnerID=40&md5=2d94c6dc03e52344fba8b3379c8be8cc","Children undergoing general anesthesia require airway monitoring by an anesthesia provider. The airway may be supported with noninvasive devices such as face mask or invasive devices such as a laryngeal mask airway or an endotracheal tube. The physiologic data stored provides an opportunity to apply machine learning algorithms distinguish between these modes based on pattern recognition. We retrieved three data sets from patients receiving general anesthesia in 2015 with either mask, laryngeal mask airway or endotracheal tube. Patients underwent myringotomy, tonsillectomy, adenoidectomy or inguinal hernia repair procedures. We retrieved measurements for end-tidal carbon dioxide, tidal volume, and peak inspiratory pressure and calculated statistical features for each data element per patient. We applied machine learning algorithms (decision tree, support vector machine, and neural network) to classify patients into noninvasive or invasive airway device support. We identified 300 patients per group (mask, laryngeal mask airway, and endotracheal tube) for a total of 900 patients. The neural network classifier performed better than the boosted trees and support vector machine classifiers based on the test data sets. The sensitivity, specificity, and accuracy for neural network classification are 97.5%, 96.3%, and 95.8%. In contrast, the sensitivity, specificity, and accuracy of support vector machine are 89.1%, 92.3%, and 88.3% and with the boosted tree classifier they are 93.8%, 92.1%, and 91.4%. We describe a method to automatically distinguish between noninvasive and invasive airway device support in a pediatric surgical setting based on respiratory monitoring parameters. The results show that the neural network classifier algorithm can accurately classify noninvasive and invasive airway device support. © 2017, Springer Science+Business Media, LLC.","Algorithms; Intubation, intratracheal, masks; Laryngeal masks; Neural networks (computer)",,2-s2.0-85028064381
"Jonasson J.","Slow mixing for Latent Dirichlet Allocation",2017,"Statistics and Probability Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020426936&doi=10.1016%2fj.spl.2017.05.011&partnerID=40&md5=60dfe6002b4bdee06f2565c1c2639f60","Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability theory in general and in machine learning in particular. A Markov chain is devised so that its stationary distribution is some probability distribution of interest. Then one samples from the given distribution by running the Markov chain for a “long time” until it appears to be stationary and then collects the sample. However these chains are often very complex and there are no theoretical guarantees that stationarity is actually reached. In this paper we study the Gibbs sampler of the posterior distribution of a very simple case of Latent Dirichlet Allocation, an attractive Bayesian unsupervised learning model for text generation and text classification. It turns out that in some situations, the mixing time of the Gibbs sampler is exponential in the length of documents and so it is practically impossible to properly sample from the posterior when documents are sufficiently long. © 2017","Gibbs sampler; MCMC; Mixing time; Topic model",,2-s2.0-85020426936
"Keshavan M.S., Sudarshan M.","Deep dreaming, aberrant salience and psychosis: Connecting the dots by artificial neural networks",2017,"Schizophrenia Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010280527&doi=10.1016%2fj.schres.2017.01.020&partnerID=40&md5=b768b3cdf3b69073cc6e12cac9c4b0a7","Why some individuals, when presented with unstructured sensory inputs, develop altered perceptions not based in reality, is not well understood. Machine learning approaches can potentially help us understand how the brain normally interprets sensory inputs. Artificial neural networks (ANN) progressively extract higher and higher-level features of sensory input and identify the nature of an object based on a priori information. However, some ANNs which use algorithms such as the “deep-dreaming” developed by Google, allow the network to over-emphasize some objects it “thinks” it recognizes in those areas, and iteratively enhance such outputs leading to representations that appear farther and farther from “reality”. We suggest that such “deep dreaming” ANNs may model aberrant salience, a mechanism suggested for pathogenesis of psychosis. Such models can generate testable predictions for psychosis. © 2017 Elsevier B.V.","Artificial neural networks; Deep dreaming; Psychosis; Schizophrenia",,2-s2.0-85010280527
"Yue F., Wang G.","COSDF: A novel method for software defect detection based on co-training and SMOTE with density based noise filtering strategy",2017,"ICIC Express Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030110235&partnerID=40&md5=ed6648b31da468bc74ce6207e7de4110","In recent years many machine learning and data mining methods have been proposed for software defect detection. However, the datasets of software defect detection are often imbalanced and only a small portion of instances are labeled in reality. In this paper, considering these practical issues simultaneously, a novel software defect detection method, COSDF, was proposed based on co-training and SMOTE. At the same time, in order to avoid introducing noise from the synthetic instances in SMOTE or new labeled instances in co-training, density based noise filtering strategy is used in the research. Experimental results on six public real-world datasets show that among the compared methods, COSDF gets the best result and COSDF is a potential solution for software defect detection. © 2017.","Co-training; COSDF; Density; Noise filtering; SMOTE; Software defect detection",,2-s2.0-85030110235
"Aartsen M.G., Ackermann M., Adams J., Aguilar J.A., Ahlers M., Ahrens M., Al Samarai I., Altmann D., Andeen K., Anderson T., Ansseau I., Anton G., Archinger M., Argüelles C., Auffenberg J., Axani S., Bagherpour H., Bai X., Barwick S.W., Baum V., Bay R., Beatty J.J., Becker Tjus J., Becker K.-H., BenZvi S., Berley D., Bernardini E., Besson D.Z., Binder G., Bindig D., Blaufuss E., Blot S., Bohm C., Börner M., Bos F., Bose D., Böser S., Botner O., Bradascio F., Braun J., Brayeur L., Bretz H.-P., Bron S., Burgman A., Carver T., Casier M., Cheung E., Chirkin D., Christov A., Clark K., Classen L., Coenders S., Collin G.H., Conrad J.M., Cowen D.F., Cross R., Day M., de André J.P.A.M., De Clercq C., del Pino Rosendo E., Dembinski H., De Ridder S., Desiati P., de Vries K.D., de Wasseige G., de With M., DeYoung T., Díaz-Vélez J.C., di Lorenzo V., Dujmovic H., Dumm J.P., Dunkman M., Eberhardt B., Ehrhardt T., Eichmann B., Eller P., Euler S., Evenson P.A., Fahey S., Fazely A.R., Feintzeig J., Felde J., Filimonov K., Finley C., Flis S., Fösig C.-C., Franckowiak A., Friedman E., Fuchs T., Gaisser T.K., Gallagher J., Gerhardt L., Ghorbani K., Giang W., Gladstone L., Glauch T., Glüsenkamp T., Goldschmidt A., Gonzalez J.G., Grant D., Griffith Z., Haack C., Hallgren A., Halzen F., Hansen E., Hansmann T., Hanson K., Hebecker D., Heereman D., Helbing K., Hellauer R., Hickford S., Hignight J., Hill G.C., Hoffman K.D., Hoffmann R., Hoshina K., Huang F., Huber M., Hultqvist K., In S., Ishihara A., Jacobi E., Japaridze G.S., Jeong M., Jero K., Jones B.J.P., Kang W., Kappes A., Karg T., Karle A., Katz U., Kauer M., Keivani A., Kelley J.L., Kheirandish A., Kim J., Kim M., Kintscher T., Kiryluk J., Kittler T., Klein S.R., Kohnen G., Koirala R., Kolanoski H., Konietz R., Köpke L., Kopper C., Kopper S., Koskinen D.J., Kowalski M., Krings K., Kroll M., Krückl G., Krüger C., Kunnen J., Kunwar S., Kurahashi N., Kuwabara T., Kyriacou A., Labare M., Lanfranchi J.L., Larson M.J., Lauber F., Lennarz D., Lesiak-Bzdak M., Leuermann M., Lu L., Lünemann J., Madsen J., Maggi G., Mahn K.B.M., Mancina S., Maruyama R., Mase K., Maunu R., McNally F., Meagher K., Medici M., Meier M., Menne T., Merino G., Meures T., Miarecki S., Micallef J., Momenté G., Montaruli T., Moulai M., Nahnhauer R., Naumann U., Neer G., Niederhausen H., Nowicki S.C., Nygren D.R., Obertacke Pollmann A., Olivas A., O’Murchadha A., Palczewski T., Pandya H., Pankova D.V., Peiffer P., Penek Ö., Pepper J.A., Pérez de los Heros C., Pieloth D., Pinat E., Price P.B., Przybylski G.T., Quinnan M., Raab C., Rädel L., Rameez M., Rawlins K., Reimann R., Relethford B., Relich M., Resconi E., Rhode W., Richman M., Riedel B., Robertson S., Rongen M., Rott C., Ruhe T., Ryckbosch D., Rysewyk D., Sabbatini L., Sanchez Herrera S.E., Sandrock A., Sandroos J., Sarkar S., Satalecka K., Schlunder P., Schmidt T., Schoenen S., Schöneberg S., Schumacher L., Seckel D., Seunarine S., Soldin D., Song M., Spiczak G.M., Spiering C., Stachurska J., Stanev T., Stasik A., Stettner J., Steuer A., Stezelberger T., Stokstad R.G., Stößl A., Ström R., Strotjohann N.L., Sullivan G.W., Sutherland M., Taavola H., Taboada I., Tatar J., Tenholt F., Ter-Antonyan S., Terliuk A., Tešić G., Tilav S., Toale P.A., Tobin M.N., Toscano S., Tosi D., Tselengidou M., Tung C.F., Turcati A., Unger E., Usner M., Vandenbroucke J., van Eijndhoven N., Vanheule S., van Rossem M., van Santen J., Vehring M., Voge M., Vogel E., Vraeghe M., Walck C., Wallace A., Wallraff M., Wandkowsky N., Waza A., Weaver C., Weiss M.J., Wendt C., Westerhoff S., Whelan B.J., Wickmann S., Wiebe K., Wiebusch C.H., Wille L., Williams D.R., Wills L., Wolf M., Wood T.R., Woolsey E., Woschnagg K., Xu D.L., Xu X.W., Xu Y., Yanez J.P., Yodh G., Yoshida S., Zoll M.","Measurement of the νμ energy spectrum with IceCube-79: IceCube Collaboration",2017,"European Physical Journal C",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032267568&doi=10.1140%2fepjc%2fs10052-017-5261-3&partnerID=40&md5=5ce5d973d30eefd7a1d12cce9eaa3aaa","IceCube is a neutrino observatory deployed in the glacial ice at the geographic South Pole. The νμ energy unfolding described in this paper is based on data taken with IceCube in its 79-string configuration. A sample of muon neutrino charged-current interactions with a purity of 99.5% was selected by means of a multivariate classification process based on machine learning. The subsequent unfolding was performed using the software Truee. The resulting spectrum covers an Eν-range of more than four orders of magnitude from 125 GeV to 3.2 PeV. Compared to the Honda atmospheric neutrino flux model, the energy spectrum shows an excess of more than 1.9σ in four adjacent bins for neutrino energies Eν≥177.8TeV. The obtained spectrum is fully compatible with previous measurements of the atmospheric neutrino flux and recent IceCube measurements of a flux of high-energy astrophysical neutrinos. © 2017, The Author(s).",,,2-s2.0-85032267568
"Constable D.J.C.","The practice of chemistry still needs to change",2017,"Current Opinion in Green and Sustainable Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029407352&doi=10.1016%2fj.cogsc.2017.08.002&partnerID=40&md5=c34b6508b834801767ecdc445ef97885","There is now over a 20-year history of green and sustainable chemistry efforts in the US, but for a majority of chemicals that have been synthesized, chemists and chemical engineers lack key information about what it takes to commercialize them, their toxicity to humans or the environment, their degradability (biological or otherwise), their ability to be recycled or reused, or their ability to be source renewably. While the depth, breadth, and variety of innovations in chemistry gives one hope that chemists and chemical engineers will make many significant advances in the next 20 years, there is still a need to incorporate systems and life cycle thinking into chemistry. This is especially true as one considers limitations in the supply of key elements chemists rely on very heavily. Recent advances in computational chemistry and machine learning show great promise for moving chemistry toward a more sustainable practice of chemistry. © 2017",,,2-s2.0-85029407352
"Döpke J., Fritsche U., Pierdzioch C.","Predicting recessions with boosted regression trees",2017,"International Journal of Forecasting",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020740280&doi=10.1016%2fj.ijforecast.2017.02.003&partnerID=40&md5=4421ebb59c57dc46cd42c7f2406d7ca3","We use a machine-learning approach known as boosted regression trees (BRT) to reexamine the usefulness of selected leading indicators for predicting recessions. We estimate the BRT approach on German data and study the relative importance of the indicators and their marginal effects on the probability of a recession. Our results show that measures of the short-term interest rate and the term spread are important leading indicators. The recession probability is a nonlinear function of these leading indicators. The BRT approach also helps to uncover the way in which the recession probability depends on the interactions between the leading indicators. While the predictive power of the short-term interest rates has declined over time, the term spread and the stock market have gained in importance. The BRT approach shows a better out-of-sample performance than popular probit approaches. © 2017 International Institute of Forecasters","Boosting; Recession forecasting; Regression trees",,2-s2.0-85020740280
"Aizenberg I.","Multiple-valued logic and complex-valued neural networks",2017,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992315954&doi=10.1007%2f978-3-319-48317-7_10&partnerID=40&md5=d056a3304a12a1e3edd38f70455c6f86","In classical multiple-valued logic its values are encoded by integers. This complicates the use of multiple-valued logic as a basic model, which can be utilized in an artificial neuron, because the values of k-valued logic encoded by integers 0, 1, 2, …, k are not normalized. To overcome this obstacle, it was suggested to encode the values of k-valued logic by complex numbers located on the unit circle, namely by the kth roots of unity. It is described in the paper how this model of multiple-valued logic over the field of complex numbers was suggested and how it was used to develop a multi-valued neuron (MVN). Then it is considered how a feedforward neural network based on MVN-a multilayer neural network with multi-valued neurons (MLMVN) was designed and its derivative-free learning algorithm based on the error-correction learning rule was presented. Different applications of MLMVN, which outperforms many other machine learning tools in terms of learning speed and generalization capability are also observed. © Springer International Publishing AG 2017.",,,2-s2.0-84992315954
"Allende H., Valle C.","Ensemble methods for time series forecasting",2017,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992317807&doi=10.1007%2f978-3-319-48317-7_13&partnerID=40&md5=781ae8fb78f7d5583e36d5b9549d1ce6","Improvement of time series forecasting accuracy is an active research area that has significant importance in many practical domains. Ensemble methods have gained considerable attention from machine learning and soft computing communities in recent years. There are several practical and theoretical reasons, mainly statistical reasons, why an ensemble may be preferred. Ensembles are recognized as one of the most successful approaches to prediction tasks. Previous theoretical studies of ensembles have shown that one of the key reasons for this performance is diversity among ensemble members. Several methods exist to generate diversity. Extensive works in literature suggest that substantial improvements in accuracy can be achieved by combining forecasts from different models. The focus of this chapter will be on ensemble for time series prediction. We describe the use of ensemble methods to compare different models for time series prediction and extensions to the classical ensemble methods for neural networks for classification and regression prediction by using different model architectures. Design, implementation and application will be the main topics of the chapter, and more specifically: conditions under which ensemble based systems may be more beneficial than their single machine; algorithms for generating individual components of ensemble systems; and various procedures through which they can be combined. Various ensemble based algorithms will be analyzed: Bagging, Adaboost and Negative Correlation; as well as combination rules and decision templates. Finally, future directions will be time series forecasting, machine fusion and others areas in which ensemble of machines have shown great promise. © Springer International Publishing AG 2017.",,,2-s2.0-84992317807
"Finnegan A., Song J.S.","Maximum entropy methods for extracting the learned features of deep neural networks",2017,"PLoS Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731576&doi=10.1371%2fjournal.pcbi.1005836&partnerID=40&md5=9943256b5dbb910eaa0e642313da80b2","New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for interpreting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possible sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemical cleavage nucleosome maps. Imposing a further constraint on the maximum entropy distribution also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed-forward neural networks. © 2017 Finnegan, Song.",,,2-s2.0-85032731576
"Zhang Y., Chen H., Lu J., Zhang G.","Detecting and predicting the topic change of Knowledge-based Systems: A topic-based bibliometric analysis from 1991 to 2016",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024905068&doi=10.1016%2fj.knosys.2017.07.011&partnerID=40&md5=6e65953a136b6d6238a6cf07417fde69","The journal Knowledge-based Systems (KnoSys) has been published for over 25 years, during which time its main foci have been extended to a broad range of studies in computer science and artificial intelligence. Answering the questions: “What is the KnoSys community interested in?” and “How does such interest change over time?” are important to both the editorial board and audience of KnoSys. This paper conducts a topic-based bibliometric study to detect and predict the topic changes of KnoSys from 1991 to 2016. A Latent Dirichlet Allocation model is used to profile the hotspots of KnoSys and predict possible future trends from a probabilistic perspective. A model of scientific evolutionary pathways applies a learning-based process to detect the topic changes of KnoSys in sequential time slices. Six main research areas of KnoSys are identified, i.e., expert systems, machine learning, data mining, decision making, optimization, and fuzzy, and the results also indicate that the interest of KnoSys communities in the area of computational intelligence is raised, and the ability to construct practical systems through knowledge use and accurate prediction models is highly emphasized. Such empirical insights can be used as a guide for KnoSys submissions. © 2017","Bibliometrics; Knowledge-based Systems; Text mining; Topic analysis; Topic detection and tracking","Artificial intelligence; Education; Expert systems; Forecasting; Knowledge based systems; Statistics; Accurate prediction; Bibliometric analysis; Bibliometrics; Evolutionary pathway; Latent Dirichlet allocation; Text mining; Topic analysis; Topic detection and tracking; Data mining",2-s2.0-85024905068
"Goldstein S.P., Evans B.C., Flack D., Juarascio A., Manasse S., Zhang F., Forman E.M.","Return of the JITAI: Applying a Just-in-Time Adaptive Intervention Framework to the Development of m-Health Solutions for Addictive Behaviors",2017,"International Journal of Behavioral Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009241615&doi=10.1007%2fs12529-016-9627-y&partnerID=40&md5=594e3e9a206cc40f321b910d0cc36493","Purpose: Lapses are strong indicators of later relapse among individuals with addictive disorders, and thus are an important intervention target. However, lapse behavior has proven resistant to change due to the complex interplay of lapse triggers that are present in everyday life. It could be possible to prevent lapses before they occur by using m-Health solutions to deliver interventions in real-time. Method: Just-in-time adaptive intervention (JITAI) is an intervention design framework that could be delivered via mobile app to facilitate in-the-moment monitoring of triggers for lapsing, and deliver personalized coping strategies to the user to prevent lapses from occurring. An organized framework is key for successful development of a JITAI. Results: Nahum-Shani and colleagues (2014) set forth six core elements of a JITAI and guidelines for designing each: distal outcomes, proximal outcomes, tailoring variables, decision points, decision rules, and intervention options. The primary aim of this paper is to illustrate the use of this framework as it pertains to developing a JITAI that targets lapse behavior among individuals following a weight control diet. Conclusion: We will detail our approach to various decision points during the development phases, report on preliminary findings where applicable, identify problems that arose during development, and provide recommendations for researchers who are currently undertaking their own JITAI development efforts. Issues such as missing data, the rarity of lapses, advantages/disadvantages of machine learning, and user engagement are discussed. © 2017, International Society of Behavioral Medicine.","Addictions; Just-in-time adaptive interventions; Lapses; m-Health",,2-s2.0-85009241615
"Varshney D., Kumar S., Gupta V.","Predicting information diffusion probabilities in social networks: A Bayesian networks based approach",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021789096&doi=10.1016%2fj.knosys.2017.07.003&partnerID=40&md5=43d495f7756c61f0eaa14eb25a94a6c8","In past few years, social networking has significantly contributed to online presence of users. These social networks are hosts to a number of viral phenomena. This has fetched a lot of attention from various researchers and marketers all over the world. Major portion of the studies done in the field of information diffusion through social networks has focused on the problem of influence maximization. These methods demand the diffusion probabilities associated with the links in the social networks to be provided as inputs. However, the problem of computing these diffusion probabilities has not been as widely explored as the problem of influence maximization. In this paper, we tackle the problem of predicting the probabilities of diffusion of a message through the links of a social network. This paper presents a Bayesian network based approach for solving the aforesaid problem. In addition to the features related to the social network, this machine learning based Bayesian framework utilizes user interests and content similarity modeled using the latent topic information. We evaluate the proposed method using the data obtained from the well-known social network platform - Twitter. © 2017 Elsevier B.V.","Bayesian network modeling; Diffusion network; Diffusion probability; Information diffusion; Social network analysis","Knowledge based systems; Probability; Problem solving; Social networking (online); Bayesian frameworks; Bayesian network models; Content similarity; Diffusion networks; Influence maximizations; Information diffusion; Network platforms; Network-based approach; Bayesian networks",2-s2.0-85021789096
"Erasmus N., Mommert M., Trilling D.E., Sickafoose A.A., Van Gend C., Hora J.L.","Characterization of Near-Earth Asteroids Using KMTNET-SAAO",2017,"Astronomical Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031116853&doi=10.3847%2f1538-3881%2faa88be&partnerID=40&md5=2a83ccb197a2ba38f992c036b64b4ca6","We present here VRI spectrophotometry of 39 near-Earth asteroids (NEAs) observed with the Sutherland, South Africa, node of the Korea Microlensing Telescope Network (KMTNet). Of the 39 NEAs, 19 were targeted, but because of KMTNet's large 2° ×2° field of view, 20 serendipitous NEAs were also captured in the observing fields. Targeted observations were performed within 44 days (median: 16 days, min: 4 days) of each NEA's discovery date. Our broadband spectrophotometry is reliable enough to distinguish among four asteroid taxonomies and we were able to confidently categorize 31 of the 39 observed targets as either an S-, C-, X-, or D-type asteroid by means of a Machine Learning algorithm approach. Our data suggest that the ratio between ""stony"" S-type NEAs and ""not-stony"" (C+X+D)-type NEAs, with H magnitudes between 15 and 25, is roughly 1:1. Additionally, we report ∼1 hr light curve data for each NEA, and of the 39 targets, we were able to resolve the complete rotation period and amplitude for six targets and report lower limits for the remaining targets. © 2017. The American Astronomical Society. All rights reserved.","minor planets, asteroids: individual (near-Earth objects); surveys; techniques: photometric",,2-s2.0-85031116853
"Cabitza F., Alderighi C., Rasoini R., Gensini G.F.","Potenziali conseguenze inattese dell'uso di sistemi di intelligenza artificiale oracolari in medicina",2017,"Recenti Progressi in Medicina",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032572067&partnerID=40&md5=c1fa4938af3277df1377a2419db55b03","Decisional support systems based on machine learning (ML) in medicine are gaining a growing interest as some recent articles have highlighted the high diagnostic accuracy exhibited by these systems in specific medical contexts. However, it is implausible that any potential advantage can be obtained without some potential drawbacks. In light of the current gaps in medical research about the side effects of the application of these new AI systems in medical practice, in this article we summarize the main unexpected consequences that may result from the widespread application of ""oracular"" systems, that is highly accurate systems that cannot give reasonable explanations of their advice as those endowed with predictive models developed with ML techniques usually are. These consequences range from the intrinsic uncertainty in the data that are used to train and feed these systems, to the inadequate explainability of their output; through the risk of overreliance, deskilling and context desensitization of their end-users. Although some of these issues may be currently hard to evaluate due to the still scarce adoption of these decisional systems in medical practice, we advocate the study of these potential consequences also for a more informed policy of approval beyond hype and disenchantment.",,,2-s2.0-85032572067
"Makhdoumi A., Ozdaglar A.","Convergence Rate of Distributed ADMM over Networks",2017,"IEEE Transactions on Automatic Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031042101&doi=10.1109%2fTAC.2017.2677879&partnerID=40&md5=56a1b019bd9da3ea5caa3a225d5bbf4d","We propose a new distributed algorithm based on alternating direction method of multipliers (ADMM) to minimize sum of locally known convex functions using communication over a network. This optimization problem emerges in many applications in distributed machine learning and statistical estimation. Our algorithm allows for a general choice of the communication weight matrix, which is used to combine the iterates at different nodes. We show that when functions are convex, both the objective function values and the feasibility violation converge with rate O(1/T), where $T$ is the number of iterations. We then show that when functions are strongly convex and have Lipschitz continuous gradients, the sequence generated by our algorithm converges linearly to the optimal solution. In particular, an psilon-optimal solution can be computed with O(κ (1)) iterations, where κ is the condition number of the problem. Our analysis highlights the effect of network and communication weights on the convergence rate through degrees of the nodes, the smallest nonzero eigenvalue, and operator norm of the communication matrix. © 1963-2012 IEEE.","Alternating direction method of multipliers (ADMM); condition number and network effects; convergence rate; distributed; optimization",,2-s2.0-85031042101
"Cinar A.","Multivariable Adaptive Artificial Pancreas System in Type 1 Diabetes",2017,"Current Diabetes Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027891958&doi=10.1007%2fs11892-017-0920-1&partnerID=40&md5=0c856d7c1a3218027ba096fc0caced9d","Purpose of Review: The review summarizes the current state of the artificial pancreas (AP) systems and introduces various new modules that should be included in future AP systems. Recent Findings: A fully automated AP must be able to detect and mitigate the effects of meals, exercise, stress and sleep on blood glucose concentrations. This can only be achieved by using a multivariable approach that leverages information from wearable devices that provide real-time streaming data about various physiological variables that indicate imminent changes in blood glucose concentrations caused by meals, exercise, stress and sleep. Summary: The development of a fully automated AP will necessitate the design of multivariable and adaptive systems that use information from wearable devices in addition to glucose sensors and modify the models used in their model-predictive alarm and control systems to adapt to the changes in the metabolic state of the user. These AP systems will also integrate modules for controller performance assessment, fault detection and diagnosis, machine learning and classification to interpret various signals and achieve fault-tolerant control. Advances in wearable devices, computational power, and safe and secure communications are enabling the development of fully automated multivariable AP systems. © 2017, Springer Science+Business Media, LLC.","Adaptive control; Artificial pancreas; Multivariable data interpretation; Type 1 diabetes",,2-s2.0-85027891958
"Wan C., Lees J.G., Minneci F., Orengo C.A., Jones D.T.","Analysis of temporal transcription expression profiles reveal links between protein function and developmental stages of Drosophila melanogaster",2017,"PLoS Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032632596&doi=10.1371%2fjournal.pcbi.1005791&partnerID=40&md5=9ce76708ada5ec21490857d3d68922ec","Accurate gene or protein function prediction is a key challenge in the post-genome era. Most current methods perform well on molecular function prediction, but struggle to provide useful annotations relating to biological process functions due to the limited power of sequence-based features in that functional domain. In this work, we systematically evaluate the predictive power of temporal transcription expression profiles for protein function prediction in Drosophila melanogaster. Our results show significantly better performance on predicting protein function when transcription expression profile-based features are integrated with sequence-derived features, compared with the sequence-derived features alone. We also observe that the combination of expression-based and sequence-based features leads to further improvement of accuracy on predicting all three domains of gene function. Based on the optimal feature combinations, we then propose a novel multi-classifier-based function prediction method for Drosophila melanogaster proteins, FFPred-fly+. Interpreting our machine learning models also allows us to identify some of the underlying links between biological processes and developmental stages of Drosophila melanogaster. © 2017 Wan et al.",,"Drosophila protein; transcriptome; animal; biology; cluster analysis; computer simulation; Drosophila melanogaster; gene expression profiling; genetics; growth, development and aging; metabolism; phenotype; physiology; procedures; statistical model; Animals; Cluster Analysis; Computational Biology; Computer Simulation; Drosophila melanogaster; Drosophila Proteins; Gene Expression Profiling; Models, Statistical; Phenotype; Transcriptome",2-s2.0-85032632596
"Solovev A., Solov’ev V.","3D molecular fragment descriptors for structure–property modeling: predicting the free energies for the complexation between antipodal guests and β-cyclodextrins",2017,"Journal of Inclusion Phenomena and Macrocyclic Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029624366&doi=10.1007%2fs10847-017-0739-z&partnerID=40&md5=2ada401881b0458899e4e72f8464d731","We report new 3D fragment descriptors to model parameters and properties of stereoisomeric molecules and conformers. New 3D fragment descriptors have been applied to discriminate between stereoisomers in predictive QSPR modeling of the standard free energy (∆G°) for the 1:1 inclusion complexation of 76 chiral guests with β-cyclodextrin (β-CD) and 40 chiral guests with 6-amino-6-deoxy-β-cyclodextrin (am-β-CD) in water at 298 K. The in-house software, mfSpace (Molecular Fragments Space), was used for QSPR modeling, generation and coding of the 3D fragment descriptors. The program implements the Singular Value Decomposition for Multiple Linear Regression analysis as machine learning method. We used ensemble modeling techniques which include the generation of many individual models, the selection of the most relevant ones and followed by their joint application to test compounds, i.e., applying a consensus model for average predictions. The models based on 2D and 3D fragment descriptors provide the best predictions in external fivefold cross-validation: root mean squared error RMSE = 1.1 kJ/mol and determination coefficient Rdet2 = 0.918 (β-CD), RMSE = 0.89 kJ/mol and Rdet2 = 0.910 (am-β-CD). © 2017, Springer Science+Business Media B.V.","3D fragment descriptors; Chiral recognition; Cyclodextrins; Inclusion complexes; Prediction of free energy; QSPR consensus modeling",,2-s2.0-85029624366
"Dharaniya R., Uma G.V.","Hybrid genre recognition based on movie script features",2017,"Journal of Computational and Theoretical Nanoscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028474373&doi=10.1166%2fjctn.2017.6933&partnerID=40&md5=1ad30228e8096e99d09f979c6783f2ba","Identifying a template and categorizing the movies with a Hybrid genre is an illustrious issue with many tries and approaches to resolve it. A Hybrid genre is a genre that blends themes and elements from two or more different genres. From the parallel analysis of various movie scripts with Multiple subgenres, 9 recurrent features namely the focus of concentration of the main character in the movie, changes of locations, proportions of verbal and non-verbal communication in the movie, number of scene changes, number of night/day scenes and number of Interior/exterior scenes are identified and variations in the proportions of these features will give the clarity to classify and identify the movies with hybrid genre. This work is a step towards automatic identification of movie script with hybrid genres given a random movie script. The methodology for automatic subgenre identification of movies uses 9 script based features and the correctness of the proposed classification is verified with Naive Bayes machine learning techniques. Thus an accuracy of 85% is achieved in identifying the 5 most popular hybrid genres namely Action comedy, Horror comedy, Romantic comedy, Sci-fic comedy and Thriller comedy and also the experimental results shows that the proposed work performs better than the existing techniques. © Copyright 2017 American Scientific Publishers All rights reserved.","Feature extraction; Hybrid genre identification; Movie genre",,2-s2.0-85028474373
"Merkel R., Dittmann J., Vielhauer C.","A First Public Research Collection of High-Resolution Latent Fingerprint Time Series for Short- and Long-Term Print Age Estimation",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021632789&doi=10.1109%2fTIFS.2017.2705622&partnerID=40&md5=f83bfd8e5ce6e5f22d0f5721e86cb01a","The creation of publicly available image databases for the signal processing community is a very time-consuming, yet immensely valuable task, enabling scientific progress by providing the opportunity of an objective comparison and reproduction of results. This paper presents for the first time a public research collection of high-resolution latent fingerprint time series for age estimation, captured from a pool of 116 different test subjects. It comprises ten different sets with a total of 2,618 time series (117,384 scans), varying between capturing devices (CWL and CLSM), data types (intensity versus topography), aging periods (short-term aging: 24 h, long-term aging: 0.5 - 3 years) and resolutions (1,270 - 180,142 ppi). Most series are annotated with donor information (age and gender) and capturing conditions (scan parameters, ambient temperature, and humidity). The data are anonymized (using partial prints only) and an organizational revocation mechanism is included to assure non-identifiability of donors in the future. Baseline results for age estimation on all ten sets are provided in the form of correlation coefficients and machine-learning based age estimation (kappa), using 19 features from prior feature spaces as well as new ones (Tamura contrast, Benford's law, and improved dust feature). Classification results exhibit kappa values between 0.51 and 0.85, highlighting the progress made in this very challenging area in recent years and also emphasizing the need of future studies on the issue. © 2005-2012 IEEE.","age estimation; baseline performance; computer forensics; digitized forensics; fingerprint processing pipeline; Latent fingerprints; public research collection","Computer forensics; Signal processing; Time series; Age estimation; Base-line performance; digitized forensics; Latent fingerprint; Public research; Pipeline processing systems",2-s2.0-85021632789
"Zinchuk A.V., Gentry M.J., Concato J., Yaggi H.K.","Phenotypes in obstructive sleep apnea: A definition, examples and evolution of approaches",2017,"Sleep Medicine Reviews",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005865096&doi=10.1016%2fj.smrv.2016.10.002&partnerID=40&md5=5a6ddc2d0770cca3d02da4e2f6c9655e","Obstructive sleep apnea (OSA) is a complex and heterogeneous disorder and the apnea hypopnea index alone can not capture the diverse spectrum of the condition. Enhanced phenotyping can improve prognostication, patient selection for clinical trials, understanding of mechanisms, and personalized treatments. In OSA, multiple condition characteristics have been termed “phenotypes.” To help classify patients into relevant prognostic and therapeutic categories, an OSA phenotype can be operationally defined as: “A category of patients with OSA distinguished from others by a single or combination of disease features, in relation to clinically meaningful attributes (symptoms, response to therapy, health outcomes, quality of life).” We review approaches to clinical phenotyping in OSA, citing examples of increasing analytic complexity. Although clinical feature based OSA phenotypes with significant prognostic and treatment implications have been identified (e.g., excessive daytime sleepiness OSA), many current categorizations lack association with meaningful outcomes. Recent work focused on pathophysiologic risk factors for OSA (e.g., arousal threshold, craniofacial morphology, chemoreflex sensitivity) appears to capture heterogeneity in OSA, but requires clinical validation. Lastly, we discuss the use of machine learning as a promising phenotyping strategy that can integrate multiple types of data (genomic, molecular, cellular, clinical) to identify unique, meaningful OSA phenotypes. © 2016 Elsevier Ltd","Cluster analysis; Obstructive sleep apnea; Personalized medicine; Phenotype; Positional; Rapid eye movement (REM) related","apnea hypopnea index; body mass; chronic obstructive lung disease; cohort analysis; cross-sectional study; disease association; disease severity; glucose metabolism; human; hypertension; longitudinal study; nonREM sleep; oxygen desaturation; pathogenesis; phenotype; polysomnography; population research; positive end expiratory pressure; quality of life; REM sleep; Review; risk factor; sleep disordered breathing; Th2 cell",2-s2.0-85005865096
"Vaiciukynas E., Verikas A., Gelzinis A., Bacauskiene M.","Detecting Parkinson’s disease from sustained phonation and speech signals",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030766664&doi=10.1371%2fjournal.pone.0185613&partnerID=40&md5=529e909279ed245d951d6cae71dbe066","This study investigates signals from sustained phonation and text-dependent speech modalities for Parkinson’s disease screening. Phonation corresponds to the vowel /a/ voicing task and speech to the pronunciation of a short sentence in Lithuanian language. Signals were recorded through two channels simultaneously, namely, acoustic cardioid (AC) and smart phone (SP) microphones. Additional modalities were obtained by splitting speech recording into voiced and unvoiced parts. Information in each modality is summarized by 18 well-known audio feature sets. Random forest (RF) is used as a machine learning algorithm, both for individual feature sets and for decision-level fusion. Detection performance is measured by the out-of-bag equal error rate (EER) and the cost of log-likelihood-ratio. Essentia audio feature set was the best using the AC speech modality and YAAFE audio feature set was the best using the SP unvoiced modality, achieving EER of 20.30% and 25.57%, respectively. Fusion of all feature sets and modalities resulted in EER of 19.27% for the AC and 23.00% for the SP channel. Non-linear projection of a RF-based proximity matrix into the 2D space enriched medical decision support by visualization. © 2017 Vaiciukynas et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"human; Parkinson disease; pathophysiology; phonation; speech; Humans; Parkinson Disease; Phonation; Speech",2-s2.0-85030766664
"Wai T.T., Aung S.S.","Enhanced frequent itemsets based on topic modeling in information filtering",2017,"International Journal of Software Innovation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028085486&doi=10.4018%2fIJSI.2017100103&partnerID=40&md5=c448775e554587dfb7ae7b7e29180c75","In order to generate user's information needs from a collection of documents, many term-based and pattern-based approaches have been used in Information Filtering. In these approaches, the documents in the collection are all about one topic. However, user's interests can be diverse and the documents in the collection often involve multiple topics. Topic modeling is useful for the area of machine learning and text mining. It generates models to discover the hidden multiple topics in a collection of documents and each of these topics are presented by distribution of words. But its effectiveness in information filtering has not been so well explored. Patterns are always thought to be more discriminative than single terms for describing documents. The major challenge found in frequent pattern mining is a large number of result patterns. As the minimum threshold becomes lower, an exponentially large number of patterns are generated. To deal with the above mentioned limitations and problems, in this paper, a novel information filtering model, EFITM (Enhanced Frequent Itemsets based on Topic Model) model is proposed. Experimental results using the CRANFIELD dataset for the task of information filtering show that the proposed model outperforms over state-of-the-art models. Copyright © 2017, IGI Global.","Information Filtering; Pattern Mining; Text Documents; Topic Modeling; User Interest Modeling",,2-s2.0-85028085486
"Valletta J.J., Recker M.","Identification of immune signatures predictive of clinical protection from malaria",2017,"PLoS Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032721183&doi=10.1371%2fjournal.pcbi.1005812&partnerID=40&md5=acf0538a444bb97fd04a6fce455961bb","Antibodies are thought to play an essential role in naturally acquired immunity to malaria. Prospective cohort studies have frequently shown how continuous exposure to the malaria parasite Plasmodium falciparum cause an accumulation of specific responses against various antigens that correlate with a decreased risk of clinical malaria episodes. However, small effect sizes and the often polymorphic nature of immunogenic parasite proteins make the robust identification of the true targets of protective immunity ambiguous. Furthermore, the degree of individual-level protection conferred by elevated responses to these antigens has not yet been explored. Here we applied a machine learning approach to identify immune signatures predictive of individual-level protection against clinical disease. We find that commonly assumed immune correlates are poor predictors of clinical protection in children. On the other hand, antibody profiles predictive of an individual’s malaria protective status can be found in data comprising responses to a large set of diverse parasite proteins. We show that this pattern emerges only after years of continuous exposure to the malaria parasite, whereas susceptibility to clinical episodes in young hosts (< 10 years) cannot be ascertained by measured antibody responses alone. © 2017 Valletta, Recker.",,,2-s2.0-85032721183
"Truong X.-T., Ngo T.D.","Toward Socially Aware Robot Navigation in Dynamic and Crowded Environments: A Proactive Social Motion Model",2017,"IEEE Transactions on Automation Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028458074&doi=10.1109%2fTASE.2017.2731371&partnerID=40&md5=e86e54f4fefe9c9188bd30c966d79287","Safe and social navigation is the key to deploying a mobile service robot in a human-centered environment. Widespread acceptability of mobile service robots in daily life is hindered by robot's inability to navigate in crowded and dynamic human environments in a socially acceptable way that would guarantee human safety and comfort. In this paper, we propose an effective proactive social motion model (PSMM) that enables a mobile service robot to navigate safely and socially in crowded and dynamic environments. The proposed method considers not only human states (position, orientation, motion, field of view, and hand poses) relative to the robot but also social interactive information about human-object and human group interactions. This allows development of the PSMM that consists of elements of an extended social force model and a hybrid reciprocal velocity obstacle technique. The PSMM is then combined with a path planning technique to generate a motion planning system that drives a mobile robot in a socially acceptable manner and produces respectful and polite behaviors akin to human movements. Note to Practitioners - In this paper, we validated the effectiveness and feasibility of the proposed proactive social motion model (PSMM) through both simulation and real-world experiments under the newly proposed human comfortable safety indices. To do that, we first implemented the entire navigation system using the open-source robot operating system. We then installed it in a simulated robot model and conducted experiments in a simulated shopping mall-like environment to verify its effectiveness. We also installed the proposed algorithm on our mobile robot platform and conducted experiments in our office-like laboratory environment. Our results show that the developed socially aware navigation framework allows a mobile robot to navigate safely, socially, and proactively while guaranteeing human safety and comfort in crowded and dynamic environments. In this paper, we examined the proposed PSMM with a set of predefined parameters selected based on our empirical experiences about the robot mechanism and selected social environment. However, in fact a mobile robot might need to adapt to various contextual and cultural situations in different social environments. Thus, it should be equipped with an online adaptive interactive learning mechanism allowing the robot to learn to auto-adjust their parameters according to such embedded environments. Using machine learning techniques, e.g., inverse reinforcement learning [1] to optimize the parameter set for the PSMM could be a promising research direction to improve adaptability of mobile service robots in different social environments. In the future, we will evaluate the proposed framework based on a wider variety of scenarios, particularly those with different social interaction situations and dynamic environments. Furthermore, various kinds of social cues and signals introduced in [2] and [3] will be applied to extend the proposed framework in more complicated social situations and contexts. Last but not least, we will investigate different machine learning techniques and incorporate them in the PSMM in order to allow the robot to automatically adapt to diverse social environments. © 2017 IEEE.","Human comfortable safety; mobile service robots; proactive social motion model (PSMM); social robots; socially aware robot navigation","Behavioral research; Human robot interaction; Intelligent robots; Mobile robots; Mobile telecommunication systems; Motion planning; Navigation; Robot programming; Robots; Dynamic environments; Interactive informations; Mobile service robots; Motion modeling; Path planning techniques; Reciprocal velocity obstacles; Robot navigation; Social robots; Economic and social effects",2-s2.0-85028458074
"Silva S., Ait Aissa D., Cocquet P., Hoarau L., Ruiz J., Ferre F., Rousset D., Mora M., Mari A., Fourcade O., Riu B., Jaber S., Bataille B.","Combined Thoracic Ultrasound Assessment during a Successful Weaning Trial Predicts Postextubation Distress",2017,"Anesthesiology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021287691&doi=10.1097%2fALN.0000000000001773&partnerID=40&md5=7183dc1e992462d6dae30b23c58de652","Background: Recent studies suggest that isolated sonographic assessment of the respiratory, cardiac, or neuromuscular functions in mechanically ventilated patients may assist in identifying patients at risk of postextubation distress. The aim of the present study was to prospectively investigate the value of an integrated thoracic ultrasound evaluation, encompassing bedside respiratory, cardiac, and diaphragm sonographic data in predicting postextubation distress. Methods: Longitudinal ultrasound data from 136 patients who were extubated after passing a trial of pressure support ventilation were measured immediately after the start and at the end of this trial. In case of postextubation distress (31 of 136 patients), an additional combined ultrasound assessment was performed while the patient was still in acute respiratory failure. We applied machine-learning methods to improve the accuracy of the related predictive assessments. Results: Overall, integrated thoracic ultrasound models accurately predict postextubation distress when applied to thoracic ultrasound data immediately recorded before the start and at the end of the trial of pressure support ventilation (learning sample area under the curve: start, 0.921; end, 0.951; test sample area under the curve: start, 0.972; end, 0.920). Among integrated thoracic ultrasound data, the recognition of lung interstitial edema and the increased telediastolic left ventricular pressure were the most relevant predictive factors. In addition, the use of thoracic ultrasound appeared to be highly accurate in identifying the causes of postextubation distress. Conclusions: The decision to attempt extubation could be significantly assisted by an integrative, dynamic, and fully bedside ultrasonographic assessment of cardiac, lung, and diaphragm functions. © Copyright 2017, the American Society of Anesthesiologists, Inc. Wolters Kluwer Health, Inc. All Rights Reserved.",,"diagnostic imaging; diaphragm; echography; extubation; female; heart; human; longitudinal study; male; middle aged; predictive value; prospective study; reproducibility; Respiratory Insufficiency; respiratory system; ventilator weaning; Airway Extubation; Diaphragm; Female; Heart; Humans; Longitudinal Studies; Male; Middle Aged; Predictive Value of Tests; Prospective Studies; Reproducibility of Results; Respiratory Insufficiency; Respiratory System; Ultrasonography; Ventilator Weaning",2-s2.0-85021287691
"Giannini R., Ugolini C., Poma A.M., Urpì M., Niccoli C., Elisei R., Chiarugi M., Vitti P., Miccoli P., Basolo F.","Identification of Two Distinct Molecular Subtypes of Non-Invasive Follicular Neoplasm with Papillary-Like Nuclear Features by Digital RNA Counting",2017,"Thyroid",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030463773&doi=10.1089%2fthy.2016.0605&partnerID=40&md5=ec7d2669a65a053c33d30b5840ede207","Background: The follicular variant (FV) of papillary thyroid cancer (PTC) is one of the most common variants of PTC. Clinically, non-infiltrative FVPTC is considered a low-risk variant of PTC, and the non-invasive encapsulated forms of FVPTC represent a group of thyroid tumors with a particularly good prognosis. Consequently, these neoplasms have been very recently reclassified as non-invasive follicular neoplasms with papillary-like nuclear features (NIFTP). From a molecular standpoint, NIFTP appears to be similar to follicular neoplasms. However, only limited data are currently available regarding their gene expression profile. Methods: The aim of this study was to identify specific molecular signatures of 26 NIFTPs compared to those of 19 follicular adenomas (FAs) and 18 infiltrative FVPTCs (IFVPTCs). A nanoString custom assay was used to perform mRNA expression analysis. All cases were also genotyped for BRAF, N-, H-, and K-RAS mutations. Samples were grouped on the basis of gene expression profiles by Pearson's correlation and non-negative matrix factorization clustering analysis. Finally, the uncorrelated shrunken centroid machine-learning algorithm was used to classify the samples. Results: The results revealed distinct expression profiles of FAs and IFVPTCs. NIFTP samples can exhibit different expression profiles, more similar to FAs (FA-like) or to IFVPTCs (IFVPTC-like), and these different expression profiles largely depend on the presence of different mutations (RAS or BRAF). Conclusion: In conclusion, although further validation of the model is required by using a larger group of prospective cases, these data reinforce the hypothesis that IFVPTC-like NIFTPs might represent precursors of IFVPTC. © Copyright 2017, Mary Ann Liebert, Inc. 2017.","expression profile; FVPTC; nanoString; NIFTP; thyroid neoplasm","B Raf kinase; K ras protein; messenger RNA; Article; classification algorithm; clinical feature; controlled study; female; follicular adenoma; gene expression profiling; gene identification; genotype; human; human tissue; major clinical study; mutation; non invasive follicular neoplasms with papillary like nuclear feature; oncogene H ras; oncogene K ras; oncogene N ras; priority journal; retrospective study; thyroid adenoma; thyroid papillary carcinoma; thyroidectomy",2-s2.0-85030463773
"Recker M., Laabei M., Toleman M.S., Reuter S., Saunderson R.B., Blane B., Török M.E., Ouadi K., Stevens E., Yokoyama M., Steventon J., Thompson L., Milne G., Bayliss S., Bacon L., Peacock S.J., Massey R.C.","Clonal differences in Staphylococcus aureus bacteraemia-associated mortality",2017,"Nature Microbiology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029436742&doi=10.1038%2fs41564-017-0001-x&partnerID=40&md5=53cbae52dbe41d4b62409ce7d3e709ea","The bacterium Staphylococcus aureus is a major human pathogen for which the emergence of antibiotic resistance is a global public health concern. Infection severity, and in particular bacteraemia-associated mortality, has been attributed to several host-related factors, such as age and the presence of comorbidities. The role of the bacterium in infection severity is less well understood, as it is complicated by the multifaceted nature of bacterial virulence, which has so far prevented a robust mapping between genotype, phenotype and infection outcome. To investigate the role of bacterial factors in contributing to bacteraemia-associated mortality, we phenotyped a collection of sequenced clinical S. aureus isolates from patients with bloodstream infections, representing two globally important clonal types, CC22 and CC30. By adopting a genome-wide association study approach we identified and functionally verified several genetic loci that affect the expression of cytolytic toxicity and biofilm formation. By analysing the pooled data comprising bacterial genotype and phenotype together with clinical metadata within a machine-learning framework, we found significant clonal differences in the determinants most predictive of poor infection outcome. Whereas elevated cytolytic toxicity in combination with low levels of biofilm formation was predictive of an increased risk of mortality in infections by strains of a CC22 background, these virulence-specific factors had little influence on mortality rates associated with CC30 infections. Our results therefore suggest that different clones may have adopted different strategies to overcome host responses and cause severe pathology. Our study further demonstrates the use of a combined genomics and data analytic approach to enhance our understanding of bacterial pathogenesis at the individual level, which will be an important step towards personalized medicine and infectious disease management. © 2017 The Author(s).",,,2-s2.0-85029436742
"Clark M.L.","Comparison of simulated hyperspectral HyspIRI and multispectral Landsat 8 and Sentinel-2 imagery for multi-seasonal, regional land-cover mapping",2017,"Remote Sensing of Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028023993&doi=10.1016%2fj.rse.2017.08.028&partnerID=40&md5=6cfa5e30be6f96e5c6c82e6346429250","This study used simulated hyperspectral (HyspIRI) and multispectral (Landsat 8 OLI, Sentinel-2 MSI) satellite imagery to compare regional land-cover mapping capabilities (San Francisco Bay Area, California) within an analytical framework that included consistent reference data and classification rules. Imagery had the pixel resolution (30 m) and extent (30,000 km2) of a Landsat scene, with multi-seasonal (spring, summer, fall) acquisitions from year 2013. Primary study objectives were to assess differences in map accuracy related to Multiple Endmember Spectral Mixture Analysis (MESMA) and Random Forests (RF) classifiers, spectral resolution (hyperspectral vs. multispectral), and temporal resolution (multi-seasonal vs. summer). The RF classifier generally outperformed MESMA by 1.1 to 9.0% overall accuracy, with the exception of summer HyspIRI reflectance data. There were no clear patterns in accuracy when comparing HyspIRI and simulated multispectral reflectance bands with RF and MESMA classifiers. With summer data, HyspIRI had significantly higher accuracy for MESMA (+ 5.5 to + 8.7%) and significantly lower accuracy for RF (− 9.7 to − 16.4%). There were no significant differences in accuracy when using multi-seasonal HyspIRI and multispectral data with RF or MESMA (&lt; 1.2% differences). There were highly significant improvements in overall accuracy (1.7 to 20.9%) with multi-season over summer-only images for all sensors, sample scales and classifiers. This result indicates that repeat image acquisitions from satellite sensors are important for land-cover classification, irrespective of sensor spectral resolution. A companion study (Clark &amp; Kilham, 2016) that used RF with hyperspectral metrics derived from HyspIRI reflectance bands (from indices, derivatives, and absorption fitting) showed significant improvements in overall accuracy relative to map classifications in this study, which all used independent reflectance bands. These findings point to the need of additional land-cover mapping research with machine learning and hyperspectral data that span the spatial and temporal scales afforded by a satellite. © 2017 Elsevier Inc.","Hyperspectral satellite; HyspIRI; Imaging spectroscopy; Land cover and land use; Landsat 8; MESMA; Multi-seasonal; Multispectral; Random Forests; Sentinel-2",,2-s2.0-85028023993
"Basu S., Sussman J.B., Berkowitz S.A., Hayward R.A., Yudkin J.S.","Development and validation of Risk Equations for Complications Of type 2 Diabetes (RECODe) using individual participant data from randomised trials",2017,"The Lancet Diabetes and Endocrinology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028310249&doi=10.1016%2fS2213-8587%2817%2930221-8&partnerID=40&md5=c292187d3f5f9b0de6923242474267eb","Background In view of substantial mis-estimation of risks of diabetes complications using existing equations, we sought to develop updated Risk Equations for Complications Of type 2 Diabetes (RECODe). Methods To develop and validate these risk equations, we used data from the Action to Control Cardiovascular Risk in Diabetes study (ACCORD, n=9635; 2001–09) and validated the equations for microvascular events using data from the Diabetes Prevention Program Outcomes Study (DPPOS, n=1018; 1996–2001), and for cardiovascular events using data from the Action for Health in Diabetes (Look AHEAD, n=4760; 2001–12). Microvascular outcomes were nephropathy, retinopathy, and neuropathy. Cardiovascular outcomes were myocardial infarction, stroke, congestive heart failure, and cardiovascular mortality. We also included all-cause mortality as an outcome. We used a cross-validating machine learning method to select predictor variables from demographic characteristics, clinical variables, comorbidities, medications, and biomarkers into Cox proportional hazards models for each outcome. The new equations were compared to older risk equations by assessing model discrimination, calibration, and the net reclassification index. Findings All equations had moderate internal and external discrimination (C-statistics 0·55–0·84 internally, 0·57–0·79 externally) and high internal and external calibration (slopes 0·71–1·31 between observed and estimated risk). Our equations had better discrimination and calibration than the UK Prospective Diabetes Study Outcomes Model 2 (for microvascular and cardiovascular outcomes, C-statistics 0·54–0·62, slopes 0·06–1·12) and the American College of Cardiology/American Heart Association Pooled Cohort Equations (for fatal or non-fatal myocardial infarction or stroke, C-statistics 0·61–0·66, slopes 0·30–0·39). Interpretation RECODe might improve estimation of risk of complications for patients with type 2 diabetes. Funding National Institute for Diabetes and Digestive and Kidney Disease, National Heart, Lung and Blood Institute, and National Institute on Minority Health and Health Disparities, National Institutes of Health, and US Department of Veterans Affairs. © 2017 Elsevier Ltd",,"aged; Article; cardiovascular mortality; cerebrovascular accident; cohort analysis; comorbidity; congestive heart failure; controlled study; diabetic nephropathy; diabetic neuropathy; diabetic retinopathy; disease association; female; heart infarction; human; major clinical study; male; non insulin dependent diabetes mellitus; predictor variable; priority journal; prospective study; risk assessment; validation study",2-s2.0-85028310249
"Stommel M., Deng Z., Xu W.L.","Probabilistic Automata Model of a Soft Robot for the Planning of Manipulation Tasks",2017,"IEEE Transactions on Automation Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023175413&doi=10.1109%2fTASE.2017.2714167&partnerID=40&md5=281b1de79f39b73cf91c012ec1dca28b","Soft robots must be able to structure an automation problem into a sequence of actions that lead to a desired state, before they can fulfill a meaningful role in automation applications. This, however, can only be successful if the robot can predict the outcome of an action. The theory of rigid industrial robots is not applicable without major changes, because kinematic chains do not adequately describe the continuous deformation of the complex, often biologically inspired shapes of soft robots. Analytic solutions have not been found yet. Numerical solutions based on finite elements are slow, technically challenging, and only suitable for one specific robot. It is, however, possible to observe the outcome of an action, and use these observations to plan a sequence of actions that let the robot accomplish an automation task. In this paper, we analyze a probabilistic automaton that computes the optimal sequence of actions to bring the robot into a desired state. An earlier article explained the functioning of the method in a toy example. In this paper, we analyze if it is feasible to apply the method to a planning problem inspired by a real soft robot. We show the results and document the planning process. We identify the analog of an impulse response, although it is not closed form due to the nonparametric nature of the method.Note to Practitioners - A soft robotic sorting table has a computer-controlled soft surface that can move delicate objects without damaging them. There are currently no closed-loop control systems for such robots, because it is unclear how to relate the control signals to the behavior of the table, or which actions to choose in order to solve a manipulation task. In this paper, we propose a probabilistic automaton to plan the best action sequence on average. The sequence brings the workpieces on top of a soft table into a desired condition. It is a machine learning solution that is based on observations of the input signals and their effect, rather than a detailed analytical or numerical modeling of the robot. We show that it is feasible to model an existing soft robotic table. We demonstrate that the planning is successful by solving complex maze tasks. Our results are based on experiments and simulations. © 2004-2012 IEEE.","Control; planning; soft robotics","Automata theory; Automation; Control; Impulse response; Industrial robots; Kinematics; Planning; Probabilistic logics; Robot programming; Robotics; Automata; Automation applications; Biologically inspired; Continuous deformations; Probabilistic automata; Robot sensing system; Sequence of actions; Soft robotics; Robots",2-s2.0-85023175413
"Babahajiani P., Fan L., Kämäräinen J.-K., Gabbouj M.","Urban 3D segmentation and modelling from street view images and LiDAR point clouds",2017,"Machine Vision and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019692066&doi=10.1007%2fs00138-017-0845-3&partnerID=40&md5=41fd95e1ea6ebec0e1a699c06c5c002f","3D urban maps with semantic labels and metric information are not only essential for the next generation robots such autonomous vehicles and city drones, but also help to visualize and augment local environment in mobile user applications. The machine vision challenge is to generate accurate urban maps from existing data with minimal manual annotation. In this work, we propose a novel methodology that takes GPS registered LiDAR (Light Detection And Ranging) point clouds and street view images as inputs and creates semantic labels for the 3D points clouds using a hybrid of rule-based parsing and learning-based labelling that combine point cloud and photometric features. The rule-based parsing boosts segmentation of simple and large structures such as street surfaces and building facades that span almost 75% of the point cloud data. For more complex structures, such as cars, trees and pedestrians, we adopt boosted decision trees that exploit both structure (LiDAR) and photometric (street view) features. We provide qualitative examples of our methodology in 3D visualization where we construct parametric graphical models from labelled data and in 2D image segmentation where 3D labels are back projected to the street view images. In quantitative evaluation we report classification accuracy and computing times and compare results to competing methods with three popular databases: NAVTEQ True, Paris-Rue-Madame and TLS (terrestrial laser scanned) Velodyne. © 2017, The Author(s).","LiDAR; Point cloud; Robotics; Semantic segmentation; Street view; Urban 3D","Classification (of information); Data visualization; Decision trees; Forestry; Image segmentation; Knowledge based systems; Photometry; Robotics; Semantics; Surveying instruments; Three dimensional computer graphics; Boosted decision trees; Classification accuracy; LIDAR (light detection and ranging); Mobile user applications; Next generation robots; Point cloud; Quantitative evaluation; Semantic segmentation; Optical radar",2-s2.0-85019692066
"Silva J.C.F., Carvalho T.F.M., Fontes E.P.B., Cerqueira F.R.","Fangorn forest (F2): A machine learning approach to classify genes and genera in the family Geminiviridae",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030257772&doi=10.1186%2fs12859-017-1839-x&partnerID=40&md5=a3a3eb011945e41bb72cdb7d7385528b","Background: Geminiviruses infect a broad range of cultivated and non-cultivated plants, causing significant economic losses worldwide. The studies of the diversity of species, taxonomy, mechanisms of evolution, geographic distribution, and mechanisms of interaction of these pathogens with the host have greatly increased in recent years. Furthermore, the use of rolling circle amplification (RCA) and advanced metagenomics approaches have enabled the elucidation of viromes and the identification of many viral agents in a large number of plant species. As a result, determining the nomenclature and taxonomically classifying geminiviruses turned into complex tasks. In addition, the gene responsible for viral replication (particularly, the viruses belonging to the genus Mastrevirus) may be spliced due to the use of the transcriptional/splicing machinery in the host cells. However, the current tools have limitations concerning the identification of introns. Results: This study proposes a new method, designated Fangorn Forest (F2), based on machine learning approaches to classify genera using an ab initio approach, i.e., using only the genomic sequence, as well as to predict and classify genes in the family Geminiviridae. In this investigation, nine genera of the family Geminiviridae and their related satellite DNAs were selected. We obtained two training sets, one for genus classification, containing attributes extracted from the complete genome of geminiviruses, while the other was made up to classify geminivirus genes, containing attributes extracted from ORFs taken from the complete genomes cited above. Three ML algorithms were applied on those datasets to build the predictive models: support vector machines, using the sequential minimal optimization training approach, random forest (RF), and multilayer perceptron. RF demonstrated a very high predictive power, achieving 0.966, 0.964, and 0.995 of precision, recall, and area under the curve (AUC), respectively, for genus classification. For gene classification, RF could reach 0.983, 0.983, and 0.998 of precision, recall, and AUC, respectively. Conclusions: Therefore, Fangorn Forest is proven to be an efficient method for classifying genera of the family Geminiviridae with high precision and effective gene prediction and classification. The method is freely accessible at www.geminivirus.org:8080/geminivirusdw/discoveryGeminivirus.jsp. © 2017 The Author(s).","Geminivirus; machine learning; Gene classification; Genus classification; Multilayer perceptron; Random Forest; Support vector machines","Artificial intelligence; Decision trees; Forecasting; Genes; Geographical distribution; Learning systems; Losses; Machinery; Multilayer neural networks; Multilayers; Optimization; Support vector machines; Viruses; Ab initio approach; Area under the curves; Gene classification; Machine learning approaches; Random forests; Rolling circle amplifications; Sequential minimal optimization; Viral replication; Classification (of information); ab initio calculation; area under the curve; genome; genus; host cell; human; intron; machine learning; Mastrevirus; nomenclature; nonhuman; open reading frame; perceptron; prediction; random forest; recall; RNA splicing; support vector machine; virus replication",2-s2.0-85030257772
"Sun J., Shao J., He C.","Abnormal event detection for video surveillance using deep one-class learning",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030155458&doi=10.1007%2fs11042-017-5244-2&partnerID=40&md5=b8c6f036a87c88967db6dc263c4ce6e0","Abnormal event detection and localization is a challenging research problem in intelligent video surveillance. It is designed to automatically identify abnormal events from monitoring videos. The main difficulty of this task lies in that there is only one class called “normal event” in training video sequences. In recent years, many advanced algorithms have been proposed on the basis of hand-crafted features. Only a few algorithms are based on high-level features, but almost all these methods use two-stage learning. In this paper, we propose a novel end-to-end model which integrates the one-class Support Vector Machine (SVM) into Convolutional Neural Network (CNN), named Deep One-Class (DOC) model. Specifically, the robust loss function derived from the one-class SVM is proposed to optimize the parameters of this model. Compared with the hierarchical models, our model not only simplifies the complexity of the process, but also obtains the global optimal solution of the whole process. In the experiments, we validate our DOC model with a publicly available dataset and compare it with some state-of-art methods. The comparison results demonstrate that our model has great performance and it is effective for abnormal events detection from surveillance videos. © 2017 Springer Science+Business Media, LLC","Abnormal event detection; Deep learning; One-class SVM; Video surveillance","Deep learning; Hierarchical systems; Image retrieval; Image segmentation; Monitoring; Neural networks; Support vector machines; Abnormal event detections; Convolutional neural network; Global optimal solutions; Intelligent video surveillance; One class-SVM; One-class support vector machine; State-of-art methods; Video surveillance; Security systems",2-s2.0-85030155458
"Lu G., Li B., Yang W., Yin J.","Unsupervised feature selection with graph learning via low-rank constraint",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030178091&doi=10.1007%2fs11042-017-5207-7&partnerID=40&md5=05c0a1dd9bde5f489b944eb265b1a171","Feature selection is one of the most important machine learning procedure, and it has been successfully applied to make a preprocessing before using classification and clustering methods. High-dimensional features often appear in big data, and it’s characters block data processing. So spectral feature selection algorithms have been increasing attention by researchers. However, most feature selection methods, they consider these tasks as two steps, learn similarity matrix from original feature space (may be include redundancy for all features), and then conduct data clustering. Due to these limitations, they do not get good performance on classification and clustering tasks in big data processing applications. To address this problem, we propose an Unsupervised Feature Selection method with graph learning framework, which can reduce the redundancy features influence and utilize a low-rank constraint on the weight matrix simultaneously. More importantly, we design a new objective function to handle this problem. We evaluate our approach by six benchmark datasets. And all empirical classification results show that our new approach outperforms state-of-the-art feature selection approaches. © 2017 Springer Science+Business Media, LLC","Feature selection; Graph learning; Spectral clustering","Big data; Cluster analysis; Clustering algorithms; Data handling; Feature extraction; Learning systems; Matrix algebra; Redundancy; Classification and clustering; Classification results; Data processing applications; Feature selection methods; Graph learning; High dimensional feature; Spectral clustering; Unsupervised feature selection; Classification (of information)",2-s2.0-85030178091
"Park S., Kim T.","Forecasting audience of motion pictures considering competitive environment",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030319906&partnerID=40&md5=9201c8755e18c6495bcfb2d283a51928","Film industry is one of the most prospective cultural business sectors and attracts much of public attention. However, investment on individual film titles is notorious for its high risk level, which has raised the necessity of research on the influential factors for box office revenue. Despite of abundant extant studies, those attempts have been mostly limited to a certain kind of factors and the competitive environment is even hardly explored. In this regard, we investigate the significance and relative contribution of a wide variety of possible influential factors including distribution power and competition. In this study, we devised various new variables to reflect competition environment and categorized independent variables into several groups in order to compare their influences. Results showed that all various information reflected in four variable groups contribute to box office revenue in both estimation and forecasting. Between mathematical models, SVM models exhibit better result than linear models as expected. Regarding competition variables, the number of competitors is found to be more effective compared to their distribution power. © 2005 - Ongoing JATIT & LLS.","Box office; Competition; Forecasting; Linear regression; Machine learning; Movie",,2-s2.0-85030319906
"Sun J., Herazo-Maya J.D., Kaminski N., Zhao H., Warren J.L.","A Dirichlet process mixture model for clustering longitudinal gene expression data",2017,"Statistics in Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021305386&doi=10.1002%2fsim.7374&partnerID=40&md5=36a81bd8385c0c7ef50f3b8825394fa9","Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel clustering method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time, while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to identify interesting subgroups. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd.","Bayesian factor analysis; Bayesian nonparametrics; clustering; longitudinal gene expression study","Article; Bayes theorem; blood sampling; blunt trauma; burn; cluster analysis; computer simulation; conceptual framework; correlation analysis; correlation coefficient; disease severity; factor analysis; gene expression; human; linear regression analysis; machine learning; major clinical study; statistical model",2-s2.0-85021305386
"Fang Y., Li Y., Lei C., Li Y., Deng X.","Hypergraph expressing low-rank feature selection algorithm",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030150904&doi=10.1007%2fs11042-017-5235-3&partnerID=40&md5=d456a872f65cb7a0db717a3c0cde937b","Dimensionality reduction has been attracted extensive attention in machine learning. It usually includes two types: feature selection and subspace learning. Previously, many researchers have demonstrated that the dimensionality reduction is meaningful for real applications. Unfortunately, a large mass of these works utilize the feature selection and subspace learning independently. This paper explores a novel supervised feature selection algorithm by considering the subspace learning. Specifically, this paper employs an ℓ2,1−norm and an ℓ2,p−norm regularizers, respectively, to conduct sample denoising and feature selection via exploring the correlation structure of data. Then this paper uses two constraints (i.e. hypergraph and low-rank) to consider the local structure and the global structure among the data, respectively. Finally, this paper uses the optimizing framework to iteratively optimize each parameter while fixing the other parameter until the algorithm converges. A lot of experiments show that our new supervised feature selection method can get great results on the eighteen public data sets. © 2017 Springer Science+Business Media, LLC","Feature selection; Hypergraph; LowRank","Iterative methods; Learning systems; Correlation structure; Dimensionality reduction; Feature selection algorithm; Feature selection methods; Hypergraph; LowRank; Real applications; Subspace learning; Feature extraction",2-s2.0-85030150904
"Keshavarzi A., Haghighat A.T., Bohlouli M.","Adaptive resource management and provisioning in the cloud computing: A survey of definitions, standards and research roadmaps",2017,"KSII Transactions on Internet and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030842868&doi=10.3837%2ftiis.2017.09.006&partnerID=40&md5=880a7ba1feb868a5a9f476edcd9c1da2","The fact that cloud computing services have been proposed in recent years, organizations and individuals face with various challenges and problems such as how to migrate applications and software platforms into cloud or how to ensure security of migrated applications. This study reviews the current challenges and open issues in cloud computing, with the focus on autonomic resource management especially in federated clouds. In addition, this study provides recommendations and research roadmaps for scientific activities, as well as potential improvements in federated cloud computing. This survey study covers results achieved through 190 literatures including books, journal and conference papers, industrial reports, forums, and project reports. A solution is proposed for autonomic resource management in the federated clouds, using machine learning and statistical analysis in order to provide better and efficient resource management. © 2017 KSII.","Cloud computing; Federated clouds resource provisioning; Research challenges in the cloud; Service level agreement","Application programs; Learning systems; Natural resources management; Network function virtualization; Resource allocation; Surveys; Adaptive Resource Management; Cloud computing services; Federated Cloud computing; Federated clouds; Research challenges; Resource management; Scientific activity; Service Level Agreements; Cloud computing",2-s2.0-85030842868
"Mayor J.J.V., Costa R.M., Frizera Neto A., Bastos T.F.","Dexterous hand gestures recognition based on low-density sEMG signals for upper-limb forearm amputees",2017,"Research on Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030089076&doi=10.1590%2f2446-4740.08516&partnerID=40&md5=755465d388a36081f5c5f8d0288a164f","Introduction: Intuitive prosthesis control is one of the most important challenges in order to reduce the user effort in learning how to use an artificial hand. This work presents the development of a novel method for pattern recognition of sEMG signals able to discriminate, in a very accurate way, dexterous hand and fingers movements using a reduced number of electrodes, which implies more confidence and usability for amputees. Methods: The system was evaluated for ten forearm amputees and the results were compared with the performance of able-bodied subjects. Multiple sEMG features based on fractal analysis (detrended fluctuation analysis and Higuchi’s fractal dimension) combined with traditional magnitude-based features were analyzed. Genetic algorithms and sequential forward selection were used to select the best set of features. Support vector machine (SVM), K-nearest neighbors (KNN) and linear discriminant analysis (LDA) were analyzed to classify individual finger flexion, hand gestures and different grasps using four electrodes, performing contractions in a natural way to accomplish these tasks. Statistical significance was computed for all the methods using different set of features, for both groups of subjects (able-bodied and amputees). Results: The results showed average accuracy up to 99.2% for able-bodied subjects and 98.94% for amputees using SVM, followed very closely by KNN. However, KNN also produces a good performance, as it has a lower computational complexity, which implies an advantage for real-time applications. Conclusion: The results show that the method proposed is promising for accurately controlling dexterous prosthetic hands, providing more functionality and better acceptance for amputees. © 2017, Brazilian Society of Biomedical Engineering. All rights reserved.","Dexterous hand gestures; Electromyography; Low-density surface electromyography; Pattern recognition; Upper-limb prosthesis","Artificial limbs; Biomedical signal processing; Discriminant analysis; Electrodes; Electromyography; Fractal dimension; Fractals; Genetic algorithms; Gesture recognition; Image retrieval; Muscle; Nearest neighbor search; Palmprint recognition; Pattern recognition; Robotic arms; Support vector machines; Detrended fluctuation analysis; Dexterous hands; K nearest neighbor (KNN); Linear discriminant analysis; Low density; Sequential forward selection; Statistical significance; Upper limb prosthesis; Prosthetics",2-s2.0-85030089076
"Lee E.-M., Joo M.-K.","A relative evaluation of aesthetic value for contemporary abstract art created by computer creativity",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030329813&partnerID=40&md5=d38d00c38a0b2726e4dddef0e3a21778","Research of Computational Creativity is one of the most challenging fields in artificial intelligence of computer systems. Contemporary artists have been created various kinds of visual art based on artificial intelligence such as neural networks, machine learning and so on. However, there is a controversy about whether the artwork created by Computational Creativity has significant aesthetic value against artworks created by humans. In this paper, we evaluate aesthetic value of contemporary abstract artworks created by computer program that Versteeg developed based on neural networks. We relatively analyze the aesthetic value of Computational Creativity against the aesthetic value of human creativity. We conduct experiments with people that 10 pairs of paintings are presented that one is created by Versteeg’s program and the other is painted by an artist of middle standing. The experimental results show that the relative aesthetic value of artworks created by the computer program is slightly lower than those of artists of middle standing. Also, similar to human artists, we found that Computer Creativity can create masterpiece or failure. As the results created by a human artist fluctuates, Computer Creativity also may create various level of artworks in the view of aesthetic value. Since Computational Creativity research in visual art is in the very early stage, creating artwork by Computational Creativity has potentials to be improved and to surpass human’s creativity. © 2005 - Ongoing JATIT & LLS.","Abstract art; Aesthetic value; Artificial intelligence; Computer creativity; Neural networks",,2-s2.0-85030329813
"Ajaeiya G., Elhajj I.H., Chehab A., Kayssi A., Kneppers M.","Mobile Apps identification based on network flows",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030149170&doi=10.1007%2fs10115-017-1111-8&partnerID=40&md5=f0fe85e5f21f0d7d7e8ec9e4081bdfa8","Network operators and mobile carriers are facing serious security challenges caused by an increasing number of services provided by smartphone Apps. For example, Android OS has more than 1 million Apps in stores. Hence, network administrators tend to adopt strict policies to secure their infrastructure. The aim of this study is to propose an efficient framework that has a classification component based on traffic analysis of Android Apps. The framework differs from other proposed studies by focusing on identifying Apps traffic from a network perspective without introducing any overhead on subscribers smartphones. Additionally, it involves a technique for pre-processing network flows generated by Apps to acquire a set of features that are used to build an identification model using machine learning algorithms. The classification model is built using classification ensembles. A group of chosen users contribute in training the classification model, which learns the normal behavior of selected Apps. Eventually, the model should be able to detect abnormal behavior of similar Apps across the network. A 93.78% classification accuracy is achieved with a low false positive rate under 0.5%. In addition, the framework is able to detect abnormal flows of unknown classes by implementing an outlier detection mechanism and reported a 94% accuracy. © 2017 Springer-Verlag London Ltd.","Android security; App profiling; Flow-based classification; Traffic analysis",,2-s2.0-85030149170
"Manaf S.A., Mustapha N., Sulaiman N., Husin N.A., Shah Zainuddin M.N., Mohd Shafri H.Z.","Majority voting of ensemble classifiers to improve shoreline extraction of medium resolution satellite images",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030312554&partnerID=40&md5=bf2dfd44ff8ab75beeb1104762b5834f","Coastal zones are constantly exposed to changes caused by natural processes, anthropogenic activities or both, which can precariously alter the coastal landscapes of many countries. Thus, monitoring of coastal zones is needed to provide important information about current conditions of a country’s coastal areas by examining changes that are taking place. In this respect, such monitoring can be carried out by traditional ground survey, airborne aerial photo, or remote sensing. However, the former is more effective and efficient as it can extract vital boundary information from satellite images using appropriate image analysis. Nonetheless, shoreline extraction has a number of challenges, and many methods have been proposed to improve such extraction, such as the use of machine learning methods. Thus, this study was carried out to determine the most effective ensemble voting classifier based on two different types of classifiers, comprising 11 single classifiers and 4 ensemble classifiers. Performance criteria of the classifiers were based on the overall accuracy, training time, and testing time. The analysis of the experimental data revealed several interesting results. First, for the combination of single and ensemble classifiers, ensemble classifiers with majority voting of Random Forest and Support Vector Machine RBF kernel were the most effective classifiers, attaining high overall accuracy. Second, for the combination of two single classifiers, Multilayer Perceptron and k-Nearest Neighbor attained high overall accuracy, rendering them as the most effective classifiers in this category of classifiers. Third, there were trade-offs between performance measures, as increased overall accuracy was accompanied by longer training and testing time. in the performance of such classifiers as both of voting-based ensemble classifiers increased significantly. © 2005 - Ongoing JATIT & LLS.","Ensemble classifier; Image classification; Majority voting; Satellite images; Shoreline extraction",,2-s2.0-85030312554
"Aledo J.C., Cantón F.R., Veredas F.J.","A machine learning approach for predicting methionine oxidation sites",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030226248&doi=10.1186%2fs12859-017-1848-9&partnerID=40&md5=516c2fd607a09c0664fad4253f76a514","Background: The oxidation of protein-bound methionine to form methionine sulfoxide, has traditionally been regarded as an oxidative damage. However, recent evidences support the view of this reversible reaction as a regulatory post-translational modification. The perception that methionine sulfoxidation may provide a mechanism to the redox regulation of a wide range of cellular processes, has stimulated some proteomic studies. However, these experimental approaches are expensive and time-consuming. Therefore, computational methods designed to predict methionine oxidation sites are an attractive alternative. As a first approach to this matter, we have developed models based on random forests, support vector machines and neural networks, aimed at accurate prediction of sites of methionine oxidation. Results: Starting from published proteomic data regarding oxidized methionines, we created a hand-curated dataset formed by 113 unique polypeptides of known structure, containing 975 methionyl residues, 122 of which were oxidation-prone (positive dataset) and 853 were oxidation-resistant (negative dataset). We use a machine learning approach to generate predictive models from these datasets. Among the multiple features used in the classification task, some of them contributed substantially to the performance of the predictive models. Thus, (i) the solvent accessible area of the methionine residue, (ii) the number of residues between the analyzed methionine and the next methionine found towards the N-terminus and (iii) the spatial distance between the atom of sulfur from the analyzed methionine and the closest aromatic residue, were among the most relevant features. Compared to the other classifiers we also evaluated, random forests provided the best performance, with accuracy, sensitivity and specificity of 0.7468±0.0567, 0.6817±0.0982 and 0.7557±0.0721, respectively (mean ± standard deviation). Conclusions: We present the first predictive models aimed to computationally detect methionine sites that may become oxidized in vivo in response to oxidative signals. These models provide insights into the structural context in which a methionine residue become either oxidation-resistant or oxidation-prone. Furthermore, these models should be useful in prioritizing methinonyl residues for further studies to determine their potential as regulatory post-translational modification sites. © 2017 The Author(s).","Machine learning; Methionine sufoxide; Oxidation prediction; Post-translation modification","Artificial intelligence; Classification (of information); Decision trees; Forecasting; Learning systems; Oxidation; Oxidation resistance; Classification tasks; Experimental approaches; Machine learning approaches; Methionine oxidation; Methionine sufoxide; Post-translation modification; Post-translational modifications; Sensitivity and specificity; Amino acids",2-s2.0-85030226248
"Li K., Su L., Wu J., Wang H., Chen P.","A rolling bearing fault diagnosis method based on variational mode decomposition and an improved kernel extreme learning machine",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030651664&doi=10.3390%2fapp7101004&partnerID=40&md5=a9acdcce41f620a8f7204d0d60103b77","Rolling bearings are key components of rotary machines. To ensure early effective fault diagnosis for bearings, a new rolling bearing fault diagnosis method based on variational mode decomposition (VMD) and an improved kernel extreme learning machine (KELM) is proposed in this paper. A fault signal is decomposed via VMD to obtain the intrinsic mode function (IMF) components, and the approximate entropy (ApEn) of the IMF component containing the main fault information is calculated. An eigenvector is created from the approximate entropy of each component. A bearing diagnosis model is created via a KELM; the KELM parameters are optimized using the particle swarm optimization (PSO) algorithm to obtain a KELM diagnosis model with optimal parameters. Finally, the effectiveness of the diagnosis method proposed in this paper is verified via a fan bearing fault diagnosis test. Under identical conditions, the result is compared with the results obtained using a back propagation (BP) neural network, a conventional extreme learning machine (ELM), and a support vector machine (SVM). The test result shows that the method proposed in this paper is superior to the other three methods in terms of diagnostic accuracy. © 2017 by the authors.","Approximate entropy; Fault diagnosis; kernel extreme learning machine; Rolling bearing; Variational mode decomposition",,2-s2.0-85030651664
"Go H., Kang M.J., Kim P.-J., Lee J.-L., Park J.Y., Park J.-M., Ro J.Y., Cho Y.M.","Development of Response Classifier for Vascular Endothelial Growth Factor Receptor (VEGFR)-Tyrosine Kinase Inhibitor (TKI) in Metastatic Renal Cell Carcinoma",2017,"Pathology and Oncology Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030170399&doi=10.1007%2fs12253-017-0323-2&partnerID=40&md5=086fcf7bf544ebc37d6fe025f87d48fd","Vascular endothelial growth factor receptor (VEGFR)-targeted therapy improved the outcome of metastatic renal cell carcinoma (mRCC) patients. However, a prediction of the response to VEGFR-tyrosine kinase inhibitor (TKI) remains to be elucidated. We aimed to develop a classifier for VEGFR-TKI responsiveness in mRCC patients. Among 101 mRCC patients, ones with complete response, partial response, or ≥24 weeks stable disease in response to VEGFR-TKI treatment were defined as clinical benefit group, whereas patients with <24 weeks stable disease or progressive disease were classified as clinical non-benefit group. Clinicolaboratory-histopathological data, 41 gene mutations, 20 protein expression levels and 1733 miRNA expression levels were compared between clinical benefit and non-benefit groups. The classifier was built using support vector machine (SVM). Seventy-three patients were clinical benefit group, and 28 patients were clinical non-benefit group. Significantly different features between the groups were as follows: age, time from diagnosis to TKI initiation, thrombocytosis, tumor size, pT stage, ISUP grade, sarcomatoid change, necrosis, lymph node metastasis and expression of pAKT, PD-L1, PD-L2, FGFR2, pS6, PDGFRβ, HIF-1α, IL-8, CA9 and miR-421 (all, P < 0.05). A classifier including necrosis, sarcomatoid component and HIF-1α was built with 0.87 accuracy using SVM. When the classifier was checked against all patients, the apparent accuracy was 0.875 (95% CI, 0.782–0.938). The classifier can be presented as a simple decision tree for clinical use. We developed a VEGFR-TKI response classifier based on comprehensive inclusion of clinicolaboratory-histopathological, immunohistochemical, mutation and miRNA features that may help to guide appropriate treatment in mRCC patients. © 2017 Arányi Lajos Foundation","Machine learning; Metastatic renal cell carcinoma; Response classifier; Tyrosine kinase inhibitors; Vascular endothelial growth factor signaling",,2-s2.0-85030170399
"Yoto","Preparing skilled labor in industry through production-based curriculum approach in vocational high school",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031313834&doi=10.1063%2f1.5003485&partnerID=40&md5=b134454a06cd8bba9b06927d55fbb7bb","Vocational high school (Sekolah Menengah Kejuruan / SMK) aims to prepare mid-level skilled labors to work in the industry and are able to create self-employment opportunities. For those reasons, the curriculum in SMK should be based on meeting the needs of the industries and is able to prepare learners to master the competence in accordance with the skills program of their choice. Production based curriculum is the curriculum which the learning process is designed together with the production process or using production process as a learning medium. This approach with the primary intention to introduce students with the real working environment and not merely simulations. In the production-based curriculum implementation model, students are directly involved in the industry through the implementation of industrial working practices, do work on production units in school, and do practical work in school by doing the job as done in the industry by using industry standards machines. © 2017 Author(s).",,,2-s2.0-85031313834
"Rouet-Leduc B., Hulbert C., Lubbers N., Barros K., Humphreys C.J., Johnson P.A.","Machine Learning Predicts Laboratory Earthquakes",2017,"Geophysical Research Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030155977&doi=10.1002%2f2017GL074677&partnerID=40&md5=a07c0ce12a0a130e1359e452ce007c9a","We apply machine learning to data sets from shear laboratory experiments, with the goal of identifying hidden signals that precede earthquakes. Here we show that by listening to the acoustic signal emitted by a laboratory fault, machine learning can predict the time remaining before it fails with great accuracy. These predictions are based solely on the instantaneous physical characteristics of the acoustical signal and do not make use of its history. Surprisingly, machine learning identifies a signal emitted from the fault zone previously thought to be low-amplitude noise that enables failure forecasting throughout the laboratory quake cycle. We infer that this signal originates from continuous grain motions of the fault gouge as the fault blocks displace. We posit that applying this approach to continuous seismic data may lead to significant advances in identifying currently unknown signals, in providing new insights into fault physics, and in placing bounds on fault failure times. ©2017. The Authors.","acoustic signal identification; earthquake precursors; earthquake prediction; laboratory earthquakes; machine learning","Acoustic waves; Artificial intelligence; Earthquakes; Faulting; Geophysics; Laboratories; Learning systems; Seismology; Acoustic signals; Acoustical signals; Earthquake precursors; Earthquake prediction; Laboratory experiments; Low-amplitude; Physical characteristics; Seismic datas; Forecasting; acoustics; earthquake; earthquake precursor; earthquake prediction; fault gouge; fault zone; identification method; laboratory method; machine learning; seismic data",2-s2.0-85030155977
"Diamant R., Campagnaro F., De Filippo De Grazia M., Casari P., Testolin A., Sanjuan Calzado V., Zorzi M.","On the Relationship between the Underwater Acoustic and Optical Channels",2017,"IEEE Transactions on Wireless Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782819&doi=10.1109%2fTWC.2017.2756055&partnerID=40&md5=169dafd3eafca7c98a06db73c80db116","Wireless transmissions in water are mostly carried out via long-range (but low-rate) underwater acoustic communications, or short-range (but high-rate) underwater optical communications. In this paper we are interested in finding whether a statistical relationship exists between underwater acoustics and optics. Besides the theoretical interest of such relationship, predicting the quality of the optical link through acoustics is also relevant in the context of a multimodal system with both acoustics and optics. Our study is based on a large dataset acquired during the NATO ALOMEX&#x2019;2015 expedition. During this experiment, we simultaneously measured several characteristics of the acoustic and optical links at multiple locations, reflecting a diversity of sea environments. Our results, show a strong correlation between the properties of the acoustic link and the reliability of optical communications. This correlation makes it possible to predict the state of the underwater optical link at a certain depth and range. Due to the complexity of the acoustic and optical channels, we could not find the source of this correlation. This work is therefore aimed to stimulate a theoretical study of the mutual properties of underwater acoustic and optical communication links. For reproducibility, we share the processed data from the experiment. IEEE","Classification; Machine learning; Multimodal systems; Optical fiber communication; Optical receivers; Optical transmitters; Prediction; Sea Experiment; Signal to noise ratio; Support vector machine; Underwater acoustic communications; Underwater acoustics; Underwater optical communications","Classification (of information); Forecasting; Learning systems; Optical communication; Optical correlation; Optical fiber communication; Optical fibers; Optical links; Optical receivers; Optical transmitters; Signal receivers; Signal to noise ratio; Support vector machines; Multimodal system; Reproducibilities; Statistical relationship; Strong correlation; Theoretical study; Underwater acoustic communications; Underwater optical communications; Wireless transmissions; Underwater acoustics",2-s2.0-85030782819
"Liu Q., Wang J., Du P., Hu L., Zheng X., Chen G.","Improving the Performance of Long-Range-Corrected Exchange-Correlation Functional with an Embedded Neural Network",2017,"Journal of Physical Chemistry A",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030532911&doi=10.1021%2facs.jpca.7b07045&partnerID=40&md5=c223b810b123d984ece4efc4a6fcefd3","A machine-learning-based exchange-correlation functional is proposed for general-purpose density functional theory calculations. It is built upon the long-range-corrected Becke-Lee-Yang-Parr (LC-BLYP) functional, along with an embedded neural network which determines the value of the range-separation parameter μ for every individual system. The structure and the weights of the neural network are optimized with a reference data set containing 368 highly accurate thermochemical and kinetic energies. The newly developed functional (LC-BLYP-NN) achieves a balanced performance for a variety of energetic properties investigated. It largely improves the accuracy of atomization energies and heats of formation on which the original LC-BLYP with a fixed μ performs rather poorly. Meanwhile, it yields a similar or slightly compromised accuracy for ionization potentials, electron affinities, and reaction barriers, for which the original LC-BLYP works reasonably well. This work clearly highlights the potential usefulness of machine-learning techniques for improving density functional calculations. (Figure Presented). © 2017 American Chemical Society.",,"Artificial intelligence; Ionization potential; Kinetic energy; Learning systems; Atomization energies; Embedded neural networks; Energetic properties; Exchange-correlation functionals; Heats of formation; Individual systems; Machine learning techniques; Separation parameters; Density functional theory",2-s2.0-85030532911
"Heuser A., Picek S., Guilley S., Mentens N.","Lightweight Ciphers and their Side-channel Resilience",2017,"IEEE Transactions on Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030767165&doi=10.1109%2fTC.2017.2757921&partnerID=40&md5=7e36eaeba2eadae125e256613fc077f5","Side-channel attacks represent a powerful category of attacks against cryptographic devices. Still, side-channel analysis for lightweight ciphers is much less investigated than for instance for AES. Although intuition may lead to the conclusion that lightweight ciphers are weaker in terms of side-channel resistance, that remains to be confirmed and quantified. In this paper, we consider various side-channel analysis metrics which should provide an insight on the resistance of lightweight ciphers against side-channel attacks. In particular, for the non-profiled scenario we use the theoretical confusion coefficient and empirical optimal distinguisher. Our study considers side-channel attacks on the first, the last, or both rounds simultaneously. Furthermore, we conduct a profiled side-channel analysis using various machine learning attacks to recover 4-bit and 8-bit intermediate states of the cipher. Our results show that the difference between AES and lightweight ciphers is smaller than one would expect, and even find scenarios in which lightweight ciphers may be more resistant. Interestingly, we observe that the studied 4-bit S-boxes have a different side-channel resilience, while the difference in the 8-bit ones is only theoretically present. IEEE","Ciphers; confusion coefficient; Encryption; Hardware; Immune system; Light emitting diodes; lightweight ciphers; machine learning attacks; optimal distinguisher; Side-channel analysis; Side-channel attacks; success rate","Artificial intelligence; Computer hardware; Cryptography; Immune system; Learning systems; Light emitting diodes; Standards; Ciphers; confusion coefficient; Distinguishers; Lightweight ciphers; Side-channel analysis; Side channel attack",2-s2.0-85030767165
"Obermeyer Z., Lee T.H.","Lost in thought — The limits of the human mind and the future of medicine",2017,"New England Journal of Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030466573&doi=10.1056%2fNEJMp1705348&partnerID=40&md5=1c5cd36735171295c6a1953ba4b5aa8e",[No abstract available],,"algorithm; clinical decision making; clinical practice; computer; data processing; electronic medical record; health care system; human; machine learning; medical education; medical research; medicine; priority journal; Short Survey; systems biology; thinking; artificial intelligence; decision making; forecasting; information processing; machine learning; medicine; trends; Algorithms; Artificial Intelligence; Datasets as Topic; Decision Making; Forecasting; Humans; Machine Learning; Medicine; Thinking",2-s2.0-85030466573
"Arulmurugan R., Sabarmathi K.R., Anandakumar H.","Classification of sentence level sentiment analysis using cloud machine learning techniques",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030089501&doi=10.1007%2fs10586-017-1200-1&partnerID=40&md5=7fd8eac69874528dcdb35e2188199839","Cloud machine learning (CML) techniques offer contemporary machine learning services, with pre-trained models and a service to generate own personalized models. This paper presents a completely unique emotional modeling methodology for incorporating human feeling into intelligent systems. The projected approach includes a technique to elicit emotion factors from users, a replacement illustration of emotions and a framework for predicting and pursuit user’s emotional mechanical phenomenon over time. The neural network based CML service has better training concert and enlarged exactness compare to other large scale deep learning systems. Opinions are important to almost all human activities and cloud based sentiment analysis is concerned with the automatic extraction of sentiment related information from text. With the rising popularity and availability of opinion rich resources such as personal blogs and online appraisal sites, new opportunities and issues arise as people now, actively use information technologies to explore and capture others opinions. In the existing system, a segmentation ranking model is designed to score the usefulness of a segmentation candidate for sentiment classification. A classification model is used for predicting the sentiment polarity of segmentation. The joint framework is trained directly using the sentences annotated with only sentiment polarity, without the use of any syntactic or sentiment annotations in segmentation level. However the existing system still has issue with classification accuracy results. To improve the classification performance, in the proposed system, cloud integrate the support vector machine, naive bayes and neural network algorithms along with joint segmentation approaches has been proposed to classify the very positive, positive, neutral, negative and very negative features more effectively using important feature selection. Also to handle the outliers we apply modified k-means clustering method on the given dataset. It is used to cloud cluster the outliers and hence the label as well as unlabeled features is handled efficiently. From the experimental result, we conclude that the proposed system yields better performance than the existing system. © 2017 Springer Science+Business Media, LLC","Classification; Cloud clustering; Cloud machine learning; Segmentation; Sentiment analysis","Artificial intelligence; Classification (of information); Cluster analysis; Clustering algorithms; Data mining; Image segmentation; Intelligent systems; Learning algorithms; Statistics; Classification accuracy; Classification models; Classification performance; Machine learning techniques; Modified k-means clustering; Neural network algorithm; Sentiment analysis; Sentiment classification; Learning systems",2-s2.0-85030089501
"Yu J., Sang J., Gao X.","Machine learning and signal processing for big multimedia analysis",2017,"Neurocomputing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013641466&doi=10.1016%2fj.neucom.2017.01.091&partnerID=40&md5=9ddbea537845345c3c69917b951eab8f",[No abstract available],,"accuracy; algorithm; artificial neural network; back propagation; calculation; classification; data analysis; Editorial; image reconstruction; image retrieval; machine learning; multimedia; priority journal; semantics; signal processing",2-s2.0-85013641466
"Kwon D., Kim H., Kim J., Suh S.C., Kim I., Kim K.J.","A survey of deep learning-based network anomaly detection",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030103322&doi=10.1007%2fs10586-017-1117-8&partnerID=40&md5=974fb3183a7d39543f0ced4c4f467b0d","A great deal of attention has been given to deep learning over the past several years, and new deep learning techniques are emerging with improved functionality. Many computer and network applications actively utilize such deep learning algorithms and report enhanced performance through them. In this study, we present an overview of deep learning methodologies, including restricted Bolzmann machine-based deep belief network, deep neural network, and recurrent neural network, as well as the machine learning techniques relevant to network anomaly detection. In addition, this article introduces the latest work that employed deep learning techniques with the focus on network anomaly detection through the extensive literature survey. We also discuss our local experiments showing the feasibility of the deep learning approach to network traffic analysis. © 2017 Springer Science+Business Media, LLC","Deep learning; Intrusion detection; Network anomaly detection; Network security; Network traffic analysis","Computer networks; Deep neural networks; Intrusion detection; Learning algorithms; Learning systems; Network security; Recurrent neural networks; Surveys; Computer and networks; Deep belief networks; Learning approach; Learning techniques; Literature survey; Machine learning techniques; Network anomaly detection; Network traffic analysis; Deep learning",2-s2.0-85030103322
"Ye H., Li G.Y., Juang B.","Power of Deep Learning for Channel Estimation and Signal Detection in OFDM Systems",2017,"IEEE Wireless Communications Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030787323&doi=10.1109%2fLWC.2017.2757490&partnerID=40&md5=8a3b7cdd7606ae2afd04c00b0ce94260","This article presents our initial results in deep learning for channel estimation and signal detection in orthogonal frequency-division multiplexing (OFDM) systems. In this article, we exploit deep learning to handle wireless OFDM channels in an end-to-end manner. Different from existing OFDM receivers that first estimate channel state information (CSI) explicitly and then detect/recover the transmitted symbols using the estimated CSI, the proposed deep learning based approach estimates CSI implicitly and recovers the transmitted symbols directly. To address channel distortion, a deep learning model is first trained offline using the data generated from simulation based on channel statistics and then used for recovering the online transmitted data directly. From our simulation results, the deep learning based approach can address channel distortion and detect the transmitted symbols with performance comparable to the minimum mean-square error (MMSE) estimator. Furthermore, the deep learning based approach is more robust than conventional methods when fewer training pilots are used, the cyclic prefix (CP) is omitted, and nonlinear clipping noise exists. In summary, deep learning is a promising tool for channel estimation and signal detection in wireless communications with complicated channel distortion and interference. IEEE","Channel estimation; Data models; Machine learning; Nonlinear distortion; OFDM; Training; Wireless communication","Channel capacity; Channel state information; Communication channels (information theory); Data structures; Deep learning; Frequency estimation; Learning algorithms; Learning systems; Mean square error; Mobile telecommunication systems; Nonlinear distortion; Orthogonal frequency division multiplexing; Personnel training; Signal detection; Wireless telecommunication systems; Channel distortions; Channel statistics; Conventional methods; Learning-based approach; Minimum mean-square error estimators; Orthogonal frequency division multiplexing systems; Wireless communications; Wireless OFDM channels; Channel estimation",2-s2.0-85030787323
"Zhang J., Li K., Liang Y., Li N.","Learning 3D faces from 2D images via Stacked Contractive Autoencoder",2017,"Neurocomputing",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012870255&doi=10.1016%2fj.neucom.2016.11.062&partnerID=40&md5=c221f99bc314fc7405d5a697c59e57ea","3D face reconstruction from a 2D face image has been found important to various applications such as face detection and recognition because a 3D face provides more semantic information than 2D image. This paper proposes a deep learning framework for 3D face reconstruction. The framework is designed to compute subspace feature of arbitrary face image, then map the feature to its counterpart in another subspace learned with 3D faces, and reconstruct the 3D face using the counterpart feature. During the course of training, we learn 2D and 3D subspaces through Stacked Contractive Autoencoders (SCAE), use a one-layer fully connected neural network to learn the mapping, and use the pre-trained parameters of the SCAEs and the one-layer network to initialize a deep feedforward neural network whose input are face images and output are 3D faces. The network is optimized by gradient descent algorithm with back-propagation. Extensive experimental results on various data sets indicate the effectiveness of the proposed SCAE-based 3D face reconstruction method. © 2017","3D face reconstruction; Deep learning; Neural network; Stacked Contractive Autoencoder; Subspace","Backpropagation; Backpropagation algorithms; Deep learning; Deep neural networks; Feedforward neural networks; Image processing; Image reconstruction; Learning systems; Network layers; Neural networks; Semantics; 3D face reconstruction; Auto encoders; Face detection and recognition; Fully connected neural network; Gradient descent algorithms; Learning frameworks; Semantic information; Subspace; Face recognition; Article; artificial neural network; automated face detection; automated face recognition; image analysis; image processing; image reconstruction; imaging and display; intermethod comparison; machine learning; measurement error; priority journal; three dimensional imaging; two dimensional imaging",2-s2.0-85012870255
"Wang S., Li Z., Yu Y., Xu J.","Folding Membrane Proteins by Deep Transfer Learning",2017,"Cell Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031094104&doi=10.1016%2fj.cels.2017.09.001&partnerID=40&md5=6958eb978e1b701e83bda448cc4f5343","Computational elucidation of membrane protein (MP) structures is challenging partially due to lack of sufficient solved structures for homology modeling. Here, we describe a high-throughput deep transfer learning method that first predicts MP contacts by learning from non-MPs and then predicts 3D structure models using the predicted contacts as distance restraints. Tested on 510 non-redundant MPs, our method has contact prediction accuracy at least 0.18 better than existing methods, predicts correct folds for 218 MPs, and generates 3D models with root-mean-square deviation (RMSD) less than 4 and 5 Å for 57 and 108 MPs, respectively. A rigorous blind test in the continuous automated model evaluation project shows that our method predicted high-resolution 3D models for two recent test MPs of 210 residues with RMSD ∼2 Å. We estimated that our method could predict correct folds for 1,345–1,871 reviewed human multi-pass MPs including a few hundred new folds, which shall facilitate the discovery of drugs targeting at MPs. A deep transfer learning method is presented to predict membrane protein contact map by learning sequence-structure relationships from non-membrane proteins, which overcomes the challenge that there are not many solved membrane protein structures for deep learning model training. The predicted contacts are pretty accurate and can help predict correct folds and accurate 3D models for ∼40% and ∼20% of 510 non-redundant membrane proteins, respectively. © 2017 Elsevier Inc.","co-evolution analysis; deep learning; deep transfer learning; homology modeling; membrane protein contact prediction; membrane protein folding; multiple sequence alignment","membrane protein; accuracy; Article; deep transfer learning; machine learning; priority journal; protein folding; protein structure; sequence homology; structural model",2-s2.0-85031094104
"Bao J., Chen Y., Yu L., Chen C.","A multi-scale kernel learning method and its application in image classification",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012307065&doi=10.1016%2fj.neucom.2016.11.069&partnerID=40&md5=90ae7c22202c783f044ce2e39e2c7339","The success of support vector machine depends on the kernel function, which directly affects the performance of SVM. Therefore, to improve the generalization of SVM, we will study the selection of kernel function. The multi-scale kernel method is one particular type of multiple kernel method which combines multi-scale kernels through a multi-kernel learning framework. It has the capability of generalizing not only the scattered region of a training set very well but also generalizing the dense region of data sets very well. Inspired by the advantages of the multi-scale kernel learning method, we applied kernel centered polarization to construct an optimization problem which was used to learn the multi scale kernel function and select the optimal parameters. A thorough analysis and proofs are provided. Experimental results show that the proposed kernel learning method and algorithm are reasonable and effective and have very good generalization performance. © 2017","Image classification; Kernel polarization; Multi-scale kernel; Multiple kernel learning","Learning systems; Optimization; Polarization; Support vector machines; Generalization performance; Kernel learning methods; Multi-kernel learning; Multi-scale kernel; Multiple Kernel Learning; Multiple kernels; Optimal parameter; Optimization problems; Image classification; Article; automated pattern recognition; image analysis; image processing; intermethod comparison; kernel method; priority journal; support vector machine",2-s2.0-85012307065
"Yasini S., Pelckmans K.","Worst-case Prediction Performance Analysis of the Kalman Filter",2017,"IEEE Transactions on Automatic Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764624&doi=10.1109%2fTAC.2017.2757908&partnerID=40&md5=7fb095ea98cc3256bfd62a87983ea711","In this paper, we study the prediction performance of the Kalman filter (KF) in a worst-case, minimax setting as studied in online machine learning, information - and game theory. The aim is to predict the sequence of observations almost as well as the best reference predictor (comparator) sequence in a comparison class. We prove worst-case bounds on the cumulative squared prediction errors using a priori knowledge about the complexity of reference predictor sequence. In fact, the performance of the KF is derived as a function of the performance of the best reference predictor and the total amount of drift that occurs in the schedule of the best comparator. IEEE","H1 estimation; Kalman filter; Online machine learning; Tracking worst-case bounds","Artificial intelligence; Bandpass filters; Comparator circuits; Comparators (optical); E-learning; Forecasting; Game theory; Learning systems; Potassium compounds; Information and game theory; Minimax; Online machines; Prediction performance; Priori knowledge; Squared prediction errors; Kalman filters",2-s2.0-85030764624
"Pampouchidou A., Simos P., Marias K., Meriaudeau F., Yang F., Pediaditis M., Tsiknakis M.","Automatic Assessment of Depression Based on Visual Cues: A Systematic Review",2017,"IEEE Transactions on Affective Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030791568&doi=10.1109%2fTAFFC.2017.2724035&partnerID=40&md5=e69274be395fb98eb45cc74a912f6bbe","Automatic depression assessment based on visual cues is a rapidly growing research domain. The present exhaustive review of existing approaches as reported in over sixty publications during the last ten years focuses on image processing and machine learning algorithms. Visual manifestations of depression, various procedures used for data collection, and existing datasets are summarized. The review outlines methods and algorithms for visual feature extraction, dimensionality reduction, decision methods for classification and regression approaches, as well as different fusion strategies. A quantitative meta-analysis of reported results, relying on performance metrics robust to chance, is included, identifying general trends and key unresolved issues to be considered in future studies of automatic depression assessment utilizing visual cues alone or in combination with vocal or verbal cues. OAPA","Affective computing; Affective Computing; Depression Assessment; Europe; Facial Expression; Facial Image Analysis; Machine Learning; Monitoring; Mood; Reliability; Tools; Visualization","Artificial intelligence; Flow visualization; Image processing; Learning systems; Monitoring; Reliability; Reliability analysis; Tools; Affective Computing; Depression Assessment; Europe; Facial Expressions; Facial images; Mood; Learning algorithms",2-s2.0-85030791568
"Yu S., Wu Y., Li W., Song Z., Zeng W.","A model for fine-grained vehicle classification based on deep learning",2017,"Neurocomputing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012926200&doi=10.1016%2fj.neucom.2016.09.116&partnerID=40&md5=fdfacf537d2c8797a6980f3e86044d51","A model for fine-grained vehicle classification based on deep learning is proposed to handle complicated transportation scene. This model comprises of two parts, vehicle detection model and vehicle fine-grained detection and classification model. Faster R-CNN method is adopted in vehicle detection model to extract single vehicle images from an image with clutter background which may contains serval vehicles. This step provides data for the next classification model. In vehicle fine-grained classification model, an image contains only one vehicle is fed into a CNN model to produce a feature, then a joint bayesian network is used to implement the fine-grained classification process. Experiments show that vehicle's make and model can be recognized from transportation images effectively by using our method. Furthermore,in order to build a large scale database easier, this paper comes up with a novel network collaborative annotation mechanism. © 2017","Deep learning; Fine-graind classification; Network collabrative annotation; Vehicle detection","Bayesian networks; Image processing; Object detection; Vehicles; Classification models; Classification process; Clutter background; Fine grained; Large-scale database; Vehicle classification; Vehicle detection; Vehicle images; Deep learning; accuracy; Article; artificial neural network; Bayesian learning; classification algorithm; data analysis; image analysis; mathematical computing; mathematical model; motor vehicle; normal distribution; priority journal; support vector machine",2-s2.0-85012926200
"Ye R., Li X.","Latent semantic concept regularized model for blind image deconvolution",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012928114&doi=10.1016%2fj.neucom.2016.11.064&partnerID=40&md5=aa016c1caeae332f56dab96af6936787","Blind image deconvolution refers to the recovery of a sharp image when the degradation processing is unknown. Many existing methods have the problem that they are designed to exploit low level image descriptors (e.g. image pixels or image gradient) only, rather than high-level latent semantic concepts, thus there is no guarantee of human visual perception. To address this problem, in this paper, a latent semantic concept regularized (LSCR) method is proposed to reduce the blind deconvolution problem at a semantic level. The proposed method explores the relationship between different image descriptors and exploits sparse measure to favor sharp images over blurry images. And matrix factorization is introduced to learn the latent concepts from the image descriptors. Then, the image prior can be described and constrained by the learned latent semantic concepts of image descriptors using a much more effective convolution matrix. In this case, the blind deconvolution problem can be regularized and the sharp version of the blurry image can be recovered at a new latent semantic level. Furthermore, an iterative algorithm is exploited to derive optimal solution. The proposed model is evaluated on two different datasets, including simulation dataset and real dataset, and state-of-the-art performance is achieved compared with other methods. © 2017 Elsevier B.V.","Blind deconvolution; Latent semantic learning; Machine learning; Manifold regularized; Matrix factorization","Convolution; Factorization; Learning systems; Matrix algebra; Blind deconvolution; Blind image deconvolution; Degradation processing; Human visual perception; Latent semantics; Manifold regularized; Matrix factorizations; State-of-the-art performance; Iterative methods; algorithm; Article; blind image deconvolution; data analysis; image analysis; image processing; image quality; image reconstruction; kernel method; mathematical computing; priority journal; semantics; simulation",2-s2.0-85012928114
"Khalili M.M., Naghizadeh P., Liu M.","Designing cyber insurance policies in the presence of security interdependence",2017,"Proceedings of NetEcon 2017 the 12th Workshop on the Economics of Networks, Systems and Computation - In Conjunction with ACM EC 2017 the 18th ACM Conference on Economics and Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032476930&doi=10.1145%2f3106723.3106730&partnerID=40&md5=7014c58c0d4307eef5ecd78deb267c48","Cyber insurance is a method for risk transfer but may or may not improve the state of network security. In this work, we consider a profit-maximizing insurer with voluntarily participating insureds. We are particularly interested in two features of cybersecurity and their impact on the contract design problem. The first is the interdependent nature of cybersecurity, whereby one entity's state of security depends on its own effort and others' effort. The second is our ability to perform accurate quantitative assessment of security posture at a firm level by combining recent advances in Internet measurement and machine learning techniques. We observe that security interdependency leads to a ""profit opportunity"" for the insurer, created by the inefficient effort levels exerted by agents who do not account for risk externalities when insurance is not available; this is in addition to risk transfer that an insurer profits from. Security pre-screening allows the insurer to take advantage of this opportunity by designing appropriate contracts which incentivize agents to increase their effort levels, allowing the insurer to effectively ""sell commitment"" to interdependent agents, in addition to risk transfer. We identify conditions under which this type of contracts lead to an improved state of network security. © 2017 Association for Computing Machinery.",,"Economics; Insurance; Learning systems; Online systems; Profitability; Contract design; Cyber security; Insurance policies; Internet measurement; Machine learning techniques; Quantitative assessments; Risk transfer; Network security",2-s2.0-85032476930
"Zhang P., Zhuo T., Huang W., Chen K., Kankanhalli M.","Online object tracking based on CNN with spatial-temporal saliency guided sampling",2017,"Neurocomputing",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012922935&doi=10.1016%2fj.neucom.2016.10.073&partnerID=40&md5=1bef7829ea0a9c3b13ea64d78f36c29f","Arbitrary tracking is hard due to nonstop intrinsic and extrinsic variations in realistic scenarios. Even for the popular tracking-by-learning strategies, effective appearance modeling of the non-rigid objects is still challenging because of the targets’ articulatory deformations on-the-fly, which may heavily degrade the discriminative capability of the online generated visual features. With widely emerged deep learning showing its success for feature extraction in different recognition tasks, more and more deep models such as CNN have been demonstrated contributive to improving the performance of online tracking. However, only depending on the outputs from last layer of CNN is not an optimum representation since the coarse spatial resolution cannot guarantee an accurate localization for a qualified sampling process, especially when objects have severe deformations, sampling from the region with a pre-defined scale would inevitably guide a poor online learning. To overcome such a limitation of CNN based tracking, in this work, we incorporated spatial-temporal saliency detection to guide a more accurate target localization for qualified sampling within an inter-frame motion flow map. With an optional strategy for the output combination of intra-frame appearance correlations and inter-frame motion saliency based on a compositional energy optimization, the proposed tracking has shown a superior performance in comparison to the other state-of-art trackers on both challenging non-rigid and generic tracking benchmark datasets. © 2017 Elsevier B.V.","CNN; Saliency; Sampling; Spatial-temporal; Tracking","Benchmarking; Deep learning; Deformation; Feature extraction; Image recognition; Object recognition; Sampling; Surface discharges; Appearance modeling; Energy optimization; Online object tracking; Realistic scenario; Saliency; Spatial resolution; Spatial temporals; Target localization; Target tracking; accuracy; algorithm; Article; artificial neural network; benchmarking; circulant matrix based kernelized correlation; compositional energy optimization; controlled study; convolutional neuron network; factual database; human; imaging and display; information processing; interframe motion flow map; kernelized correlation filter; machine learning; online object tracking; online system; priority journal; process optimization; qualitative analysis; salient motion detection; sampling; spatial region localization; spatial temporal saliency guided sampling; spatial temporal target region localization; statistical analysis; statistical concepts; statistical model; video saliency detection; video saliency segmentation",2-s2.0-85012922935
"Wu S., Oerlemans A., Bakker E.M., Lew M.S.","Deep binary codes for large scale image retrieval",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012906614&doi=10.1016%2fj.neucom.2016.12.070&partnerID=40&md5=209b4e5f9df3a088b213bc8cab45888d","Recent studies have shown that image representations built upon deep convolutional layers in Convolutional Neural Networks (CNNs) have strong discriminative characteristics. In this paper, we present a novel and effective method to create compact binary codes (deep binary codes) based on deep convolutional features for image retrieval. Deep binary codes are generated by comparing the response from each feature map and the average response across all the feature maps on the deep convolutional layers. Additionally, a spatial cross-summing strategy is proposed to directly generate bit-scalable binary codes. As the deep binary codes on different deep layers can be obtained by passing the image through the CNN and each of them makes a different contribution to the search accuracy, we then present a dynamic, on-the-fly late fusion approach where the top N high quality search scores from deep binary codes are automatically determined online and fused to further enhance the retrieval precision. Two strengths of the proposed methods are that the generation of deep binary codes is based on a generic model, which does not require additional training for new image domains, and that the dynamic late fusion scheme is query adaptive. Extensive experimental results on well known benchmarks show that the performance of deep binary codes are competitive with state-of-the-art approaches for large scale image retrieval. Moreover, it is shown that the dynamic late fusion scheme significantly enhances the search accuracy. © 2017 Elsevier B.V.","Convolutional neural network; Deep binary codes; Large scale image search; Late fusion","Benchmarking; Binary codes; Bins; Codes (symbols); Convolution; Deep neural networks; Image fusion; Neural networks; Convolutional neural network; Generic modeling; Image representations; Image search; Late fusion; On the flies; Search accuracy; State-of-the-art approach; Image retrieval; accuracy; Article; artificial neural network; benchmarking; classification; comparative study; controlled study; convolutional neural network; deep binary code; dynamic late fusion; error; hashing learning; image analysis; image classification; image representation; image retrieval; imaging and display; large scale image retrieval; machine learning; priority journal; quality control; quantization error; retrieval precision; spatial cross summing; statistical analysis; statistical concepts",2-s2.0-85012906614
"Li J., Xu C., Yang W., Sun C.","SPA: Spatially Pooled Attributes for image retrieval",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014847727&doi=10.1016%2fj.neucom.2016.10.074&partnerID=40&md5=0ce2977a4a48eece43af0f6c79428ea4","Semantic gap, which refers to the limitation that low-level hand-crafted visual features insufficiently encode high-level semantic concepts contained in the images, has been a challenging issue in image retrieval and significantly impairs the performance of real-world retrieval systems. Despite massive efforts that have been devoted to developing effective image signatures, e.g., Bag-of-Visual-Words (BOVW), the Fisher Vector (FV) and the Vector of Locally Aggregated Descriptors (VLAD), these mid-level image features still fail to handle the problem of semantic gap and thus lead to suboptimal results. Towards this end, a large body of work focuses on introducing attribute learning into a variety of vision applications. As inherent nature that describes the intrinsic properties of objects, such as color, shape and rigidity, learned attributes serve as intermediate representations that bridge the semantic gap. However, conventional attribute embedding methods are generally developed for image global representation while ignoring local spatial cues, which prevents them from achieving desirable performance. In this paper, we attempt to encode weak spatial information into attribute embedding for effective image retrieval. Specifically, we partition the image into regular grids and extract Classemes attribute vector from each patch, which results in a large pool of Classemes descriptors followed by VLAD aggregation for generating holistic representation. In order to produce a compact and discriminative code, we employ a piecewise Fisher Discriminant Analysis (FDA) for dimension reduction and concatenate all the compressed Classemes into a single vector coined Spatially Pooled Attributes (SPA). Thorough experimental evaluation and comparative study on three public benchmarks demonstrate the superiority of the proposed approach. © 2017","Attribute embedding; Classemes attribute vector; Local spatial cues; Semantic gap; Spatially Pooled Attributes (SPA)","Discriminant analysis; Encoding (symbols); Fisher information matrix; Search engines; Semantics; Vectors; Attribute embedding; Attribute vectors; Semantic gap; Spatial cues; Spatially Pooled Attributes (SPA); Image retrieval; Article; benchmarking; Classemes attribute vector; discriminant analysis; image processing; image retrieval; intermethod comparison; machine learning; priority journal",2-s2.0-85014847727
"Zhang C., Li R., Huang Q., Tian Q.","Hierarchical deep semantic representation for visual categorization",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012888589&doi=10.1016%2fj.neucom.2016.11.065&partnerID=40&md5=7dd8ad99c5c39af56db445d376ab9c60","Visual features are unsatisfactory to effectively describe the visual semantics. However, single layer based semantic modeling may be not able to cope with complicated semantic contents. In this paper, we propose Hierarchical Deep Semantic Representation (H-DSR), a hierarchical framework which combines semantic context modeling with visual features. First, the input image is sampled with spatially fixed grids. Deep features are then extracted for each sample in particular location. Second, using pre-learned classifiers, a detection response map is constructed for each patch. Semantic representation is then extracted from the map, which have a sense of latent semantic context. We combine the semantic and visual representations for joint representation. Third, a hierarchical deep semantic representation is built with recurrent reconstructions using three layers. The concatenated visual and semantic representations are used as the inputs of subsequent layers for semantic representation extraction. Finally, we verify the effectiveness of H-DSR for visual categorization on two publicly available datasets: Oxford Flowers 17 and UIUC-Sports. Improved performances are obtained over many baseline methods. © 2017 Elsevier B.V.","Image representation; Semantic representation; Visual categorization","Semantic Web; Baseline methods; Image representations; Semantic content; Semantic context; Semantic representation; Visual categorization; Visual representations; Visual semantics; Semantics; Article; artificial neural network; calculation; classifier; controlled study; hierarchical deep semantic representation; image analysis; image reconstruction; information processing; machine learning; mathematical analysis; mathematical computing; priority journal; semantic representation; semantics; visual categorization; visual feature",2-s2.0-85012888589
"Ku E., Scherzer R., Odden M.C., Shlipak M., White C.L., Field T.S., Benavente O., Pergola P.E., Peralta C.A.","Patterns of blood pressure response during intensive BP lowering and clinical events: results from the secondary prevention of small subcortical strokes trial",2017,"Blood Pressure",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030178840&doi=10.1080%2f08037051.2017.1382310&partnerID=40&md5=04c7d517a6be402c918cbec3fbc54ab6","Purpose: We applied cluster analysis to identify discrete patterns of concomitant responses of systolic (SBP), diastolic (DBP) and pulse pressure (PP) during intensive BP lowering; and to evaluate their clinical relevance and association with risk of mortality, major vascular events (MVEs), and stroke. Material and methods: We used an unsupervised cluster procedure to identify distinct patterns of BP change during the first 9 months of anti-hypertensive therapy intensification among 1,331 participants in the Secondary Prevention of Small Subcortical Strokes Trial who were previously randomized to lower BP target (SBP < 130 mm Hg) after lacunar stroke. Results: The cluster procedure partitioned participants into three groups in the lower SBP target arm, persons with: 1) mildly elevated baseline SBP and minimal visit-to-visit BP variability (mild reducers); 2) moderately elevated baseline SBP and moderate visit-to-visit BP variability (moderate reducers); and 3) very elevated baseline SBP with very large visit-to-visit BP variability during intensification (large reducers). In the lower SBP target group, moderate reducers had a higher risk of death (adjusted HR 1.6 [95% CI 1.0–2.7]), MVE (adjusted HR 2.1 [95% CI 1.4–3.2]), and stroke (adjusted HR 2.6[95% CI 1.7–4.1]) compared to mild reducers. Large reducers had the highest risk of death (adjusted HR 2.3 [95% CI 1.2–4.4]), but risk of MVE (HR = 1.7 [95%CI 0.9–3.1]) and stroke (HR = 1.6 [95%CI: 0.8–3.5]) were not statistically significantly different compared to mild reducers. Conclusions: Among persons with prior lacunar stroke, baseline BP levels, and BP variability in the setting of intensive BP lowering can identify discrete groups of persons at higher risk of adverse outcomes. © 2017 Informa UK Ltd, trading as Taylor & Francis Group","hypertension; machine learning; Mortality",,2-s2.0-85030178840
"Chan T.E., Stumpf M.P.H., Babtie A.C.","Gene Regulatory Network Inference from Single-Cell Data Using Multivariate Information Measures",2017,"Cell Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031092991&doi=10.1016%2fj.cels.2017.08.014&partnerID=40&md5=998727020415edfcd18ea5add40f223b","While single-cell gene expression experiments present new challenges for data processing, the cell-to-cell variability observed also reveals statistical relationships that can be used by information theory. Here, we use multivariate information theory to explore the statistical dependencies between triplets of genes in single-cell gene expression datasets. We develop PIDC, a fast, efficient algorithm that uses partial information decomposition (PID) to identify regulatory relationships between genes. We thoroughly evaluate the performance of our algorithm and demonstrate that the higher-order information captured by PIDC allows it to outperform pairwise mutual information-based algorithms when recovering true relationships present in simulated data. We also infer gene regulatory networks from three experimental single-cell datasets and illustrate how network context, choices made during analysis, and sources of variability affect network inference. PIDC tutorials and open-source software for estimating PID are available. PIDC should facilitate the identification of putative functional relationships and mechanistic hypotheses from single-cell transcriptomic data. Chan et al. develop PIDC, a fast, efficient algorithm that makes use of multivariate information theory, to reliably infer gene-gene interactions in heterogeneous, single-cell gene expression data and build gene regulatory networks. © 2017","gene regulation; mutual information; network reconstruction; single-cell PCR; single-cell RNA-seq","messenger RNA; algorithm; Article; computer language; computer model; data analysis software; data synthesis; gene expression; gene interaction; gene regulatory network; information science; machine learning; maximum likelihood method; multivariate analysis; nonhuman; partial information decomposition; priority journal; probability; receiver operating characteristic; simulation; single cell analysis; statistical concepts; systems biology; transcriptomics",2-s2.0-85031092991
"Gornitz N., Lima L.A., Muller K., Kloft M., Nakajima S.","Support Vector Data Descriptions and k-Means Clustering: One Class?",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030751821&doi=10.1109%2fTNNLS.2017.2737941&partnerID=40&md5=3e7d9a1e279b832bc79620f2b6212169","We present ClusterSVDD, a methodology that unifies support vector data descriptions (SVDDs) and k-means clustering into a single formulation. This allows both methods to benefit from one another, i.e., by adding flexibility using multiple spheres for SVDDs and increasing anomaly resistance and flexibility through kernels to k-means. In particular, our approach leads to a new interpretation of k-means as a regularized mode seeking algorithm. The unifying formulation further allows for deriving new algorithms by transferring knowledge from one-class learning settings to clustering settings and vice versa. As a showcase, we derive a clustering method for structured data based on a one-class learning scenario. Additionally, our formulation can be solved via a particularly simple optimization scheme. We evaluate our approach empirically to highlight some of the proposed benefits on artificially generated data, as well as on real-world problems, and provide a Python software package comprising various implementations of primal and dual SVDD as well as our proposed ClusterSVDD. IEEE","Anomaly detection; Anomaly detection; clustering; Clustering algorithms; k-means; Kernel; Learning systems; Level set; one-class classification; Optimization; support vector data description (SVDD); Support vector machines","Cluster analysis; Computer software; Data description; Learning algorithms; Learning systems; Optimization; Support vector machines; Vectors; Anomaly detection; clustering; K-means; Kernel; Level Set; One-class Classification; Support vector data description; Clustering algorithms",2-s2.0-85030751821
"Chen L., Casperson D., Gao L.","Ghost Numbers",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030762187&doi=10.1109%2fTPAMI.2017.2757489&partnerID=40&md5=ee8645a9a2ce2eac5495354ecabb59fb","We comment on a paper describing an algorithm for image set classification. Following the general practice in computer vision research, the performance of the algorithm was evaluated on benchmarks in order to support the claim of its advantage over other algorithms in the literature. We have examined the reported data of experiences on two datasets, and found that many numbers are not a possible answer regardless how the random partitions were selected and regardless how the algorithms performed in each partition. Our finding suggests that the experimental results in the paper (&#x201C;Deep Reconstruction Models for Image Set Classification&#x201D;, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 37, no. 4, pp. 713-727, April 2015) has serious flaws to the extent that all the experimental results should be re-examined. IEEE","Algorithm design and analysis; deep learning; experiment; ghost number; Image reconstruction; image set classification; Machine intelligence; Partitioning algorithms; Pattern recognition; Training; Videos","Artificial intelligence; Benchmarking; Deep learning; Experiments; Image classification; Image processing; Pattern recognition; Personnel training; Algorithm design and analysis; ghost number; Image sets; Machine intelligence; Partitioning algorithms; Videos; Image reconstruction",2-s2.0-85030762187
"Zhao R., Tan V.Y.F.","A Unified Convergence Analysis of the Multiplicative Update Algorithm for Regularized Nonnegative Matrix Factorization",2017,"IEEE Transactions on Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030757687&doi=10.1109%2fTSP.2017.2757914&partnerID=40&md5=7cb741ad477ae6043475311c2612f3ba","The multiplicative update (MU) algorithm has been extensively used to estimate the basis and coefficient matrices in nonnegative matrix factorization (NMF) problems under a wide range of divergences and regularizers. However, theoretical convergence guarantees have only been derived for a few special divergences without regularization. In this work, we provide a conceptually simple, self-contained, and unified proof for the convergence of the MU algorithm applied on NMF with a wide range of divergences and regularizers. Our main result shows the sequence of iterates (i.e., pairs of basis and coefficient matrices) produced by the MU algorithm converges to the set of stationary points of the non-convex NMF optimization problem. Our proof strategy has the potential to open up new avenues for analyzing similar problems in machine learning and signal processing. IEEE","Convergence Analysis; Multiplicative Update Algorithm; Nonconvex Optimization; Nonnegative Matrix Factorization; Stationary Points","Factorization; Learning systems; Optimization; Signal processing; Convergence analysis; Multiplicative updates; Nonconvex optimization; Nonnegative matrix factorization; Stationary points; Matrix algebra",2-s2.0-85030757687
"Yeh C.-W., Tu C.-H., Hung S.-H.","Rapid Hybrid Simulation Methods for Exploring the Design Space of Signal Processors with Dynamic and Scalable Timing Models",2017,"Journal of Signal Processing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029901716&doi=10.1007%2fs11265-017-1285-z&partnerID=40&md5=63d3ebfed2d36c0ca4a413056a975ee9","As today’s state-of-the-art signal processing systems often require heterogeneous computing and special-purpose accelerators to offer highly efficient performance for mixed application workloads, including not only traditional signal processing algorithms, but also the demands to enable smart applications with data analytics, machine learning, as well as the capability interacting with both physical and cyber worlds via sensors and networks. Thus, the complexity of such systems has been increasing, and the focus of designing has been shifting to exploring the design space with a mixture of processing cores/accelerators and the interconnection networks between the components to optimize the performance and efficiency at the system level. Traditional simulation tools may offer accurate performance estimation at micro architectural level, but it is highly complicated to combine the simulators for various components to perform complex applications, and they fall in short in terms of their capabilities to profiling application workload. Furthermore, the speed of such complex simulation would be unacceptably slow with traditional system-level simulation framework such as SystemC. To solve the problem, we develop a rapid hybrid emulation/simulation framework that allows the user to execute full-blown system and application software and plug in emulators, simulators, and timing models for various components in the prototype system, switching the timing models dynamically with our just-in-time model selection mechanism, and connect the emulated/simulated components with scalable communication channels, so that the framework can be accelerated effectively by a multicore host. Our just-in-time model selection mechanism is capable of detecting and skipping regular program patterns to save the simulation time dramatically. In addition, our framework is capable of estimating the performance of different system configurations with concurrent multiple timing models, which further saves the time needed for traversing the design space. Our experimental results have shown that our dynamic model selection and multi-model approach collectively can speed up the design space exploration by 13.4 times on a quad-core host for cache simulation. © 2017 Springer Science+Business Media, LLC","Acceleration; Approximate timing model; Design space exploration; Efficient data transfer; Embedded system; Simulation","Acceleration; Application programs; Complex networks; Computer aided software engineering; Data transfer; Dynamics; Embedded systems; Integrated circuit design; Interconnection networks (circuit switching); Just in time production; Learning systems; Network function virtualization; Software prototyping; Timing circuits; Design space exploration; Dynamic model selections; Heterogeneous computing; Scalable communication; Signal processing algorithms; Signal processing systems; Simulation; Timing modeling; Signal processing",2-s2.0-85029901716
"Meng M., Chua Y.J., Wouterson E., Ong C.P.K.","Ultrasonic signal classification and imaging system for composite materials via deep convolutional neural networks",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016608671&doi=10.1016%2fj.neucom.2016.11.066&partnerID=40&md5=7fb1b717aeb1bd607a0968855f7a9f42","Automated ultrasonic signal classification systems are finding increasing use in many applications for the recognition of large volumes of inspection signals. Wavelet transform is a well-known signal processing technique in fault signal diagnosis system. Most of the proposed approaches have mainly used low-level handcraft features based on wavelet transform to encode the information for different defect classes. In this paper, we proposed a deep learning based framework to classify ultrasonic signals from carbon fiber reinforced polymer (CFRP) specimens with void and delamination. In our proposed algorithm, deep Convolutional Neural Networks (CNNs) are used to learn a compact and effective representation for each signal from wavelet coefficients. To yield superior results, we proposed to use a linear SVM top layer in the training process of signal classification task. The experimental results demonstrated the excellent performance of our proposed algorithm against the classical classifier with manually generated attributes. In addition, a post processing scheme is developed to interpret the classifier outputs with a C-scan imaging process and visualize the locations of defects using a 3D model representation. © 2017","Deep convolutional neural networks; Feature extraction; Ultrasonic signal classification; Wavelet transform","Carbon; Carbon fiber reinforced plastics; Convolution; Defects; Feature extraction; Fiber reinforced plastics; Neural networks; Signal processing; Ultrasonic applications; Wavelet transforms; Carbon fiber reinforced polymer; Convolutional neural network; Diagnosis systems; Post-processing scheme; Signal classification; Signal processing technique; Ultrasonic signals; Wavelet coefficients; Acoustic signal processing; carbon fiber; polymer; Article; artificial neural network; calculation; classification algorithm; classifier; composite material; deep convolutional neural network; entropy; image processing; imaging system; prediction; priority journal; probability; signal processing; support vector machine; wavelet analysis",2-s2.0-85016608671
"Bilal M., Shaikh F.K., Arif M., Wyne M.F.","A revised framework of machine learning application for optimal activity recognition",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029897370&doi=10.1007%2fs10586-017-1212-x&partnerID=40&md5=a5890f2402561ed1501c34e78adbcf28","Data science augments manual data understanding with machine learning for potential performance increase. In this paper, data science methodology is examined to enhance machine learning application in smartphone based automatic human activity recognition (HAR). Eventually, a modified feature engineering and a novel post-learning data engineering are proposed in the machine learning framework as the alternate of data understanding for an effective HAR. The proposed framework is examined on two different HAR data sets demonstrating a possibility of data-driven machine learning for near an optimal classification of activities. The proposed framework exhibited effectiveness and efficiency when compared with the existing methods. The modified feature engineering resulted in 42% fewer features required by support vector machine to yield 97.3% correct recognition of human physical activities. However, the addition of post-learning data engineering further improved the model to perform 99% accurate classification, which is an almost optimal performance. © 2017 Springer Science+Business Media, LLC","Activity recognition; Composite feature set; Data-driven machine learning framework; Post-learning data engineering; Smartphone sensors","Artificial intelligence; Classification (of information); Computer aided instruction; Learning systems; Network function virtualization; Pattern recognition; Smartphones; Activity recognition; Composite features; Effectiveness and efficiencies; Human activity recognition; Learning data; Machine learning applications; Optimal classification; Science methodologies; Engineering education",2-s2.0-85029897370
"Wang Y., Chen Z., Zheng L., Hao L.","The application of machine learning in RCS calculation for antenna-radome system",2017,"2017 International Applied Computational Electromagnetics Society Symposium in China, ACES-China 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032786154&partnerID=40&md5=b48619d280feba5de9a3056388fdc674","In this paper, a method based on machine learning and modified physical optics method is proposed for calculating the radar cross section (RCS) of the antenna-radome system. The scattering mechanism of the antenna-random system is estimated by the Multilayer Perceptron (MLP) model and the measured data in the past is analyzed. The back-propagation algorithm is chosen to train the machine learning model. In the modified physical optics method, the FSS layer is equated as an anisotropic medium, the relationship of the frequency response of the FSS layer with the incident angle, polarization and frequency is estimated by the function fitting method to improve current calculation accuracy on the radome surface. Finally the results calculated by the method proposed in this paper are validated by a real antenna-radome system. © 2017 Applied Computational Electromagnetics Society.","antenna-radome system; machine learning; RCS","Anisotropic media; Antennas; Artificial intelligence; Backpropagation; Backpropagation algorithms; Computational electromagnetics; Frequency response; Physical optics; Radar antennas; Radar cross section; Radomes; Anisotropic medium; Antenna radomes; Calculation accuracy; Function fitting; Machine learning models; Multi layer perceptron; Physical optics method; Scattering mechanisms; Learning systems",2-s2.0-85032786154
"Park S.K., Zhao Z., Mukherjee B.","Construction of environmental risk score beyond standard linear models using machine learning methods: Application to metal mixtures, oxidative stress and cardiovascular disease in NHANES",2017,"Environmental Health: A Global Access Science Source",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029826001&doi=10.1186%2fs12940-017-0310-9&partnerID=40&md5=3473e63b326048a19e19a301557ba473","Background: There is growing concern of health effects of exposure to pollutant mixtures. We initially proposed an Environmental Risk Score (ERS) as a summary measure to examine the risk of exposure to multi-pollutants in epidemiologic research considering only pollutant main effects. We expand the ERS by consideration of pollutant-pollutant interactions using modern machine learning methods. We illustrate the multi-pollutant approaches to predicting a marker of oxidative stress (gamma-glutamyl transferase (GGT)), a common disease pathway linking environmental exposure and numerous health endpoints. Methods: We examined 20 metal biomarkers measured in urine or whole blood from 6 cycles of the National Health and Nutrition Examination Survey (NHANES 2003-2004 to 2013-2014, n = 9664). We randomly split the data evenly into training and testing sets and constructed ERS's of metal mixtures for GGT using adaptive elastic-net with main effects and pairwise interactions (AENET-I), Bayesian additive regression tree (BART), Bayesian kernel machine regression (BKMR), and Super Learner in the training set and evaluated their performances in the testing set. We also evaluated the associations between GGT-ERS and cardiovascular endpoints. Results: ERS based on AENET-I performed better than other approaches in terms of prediction errors in the testing set. Important metals identified in relation to GGT include cadmium (urine), dimethylarsonic acid, monomethylarsonic acid, cobalt, and barium. All ERS's showed significant associations with systolic and diastolic blood pressure and hypertension. For hypertension, one SD increase in each ERS from AENET-I, BART and SuperLearner were associated with odds ratios of 1.26 (95% CI, 1.15, 1.38), 1.17 (1.09, 1.25), and 1.30 (1.20, 1.40), respectively. ERS's showed non-significant positive associations with mortality outcomes. Conclusions: ERS is a useful tool for characterizing cumulative risk from pollutant mixtures, with accounting for statistical challenges such as high degrees of correlations and pollutant-pollutant interactions. ERS constructed for an intermediate marker like GGT is predictive of related disease endpoints. © 2017 The Author(s).","Bayesian additive regression tree (BART); Bayesian kernel machine regression (BKMR); Cardiovascular disease; Elastic-net; Environmental risk score (ERS); Machine learning; Metals; Mixtures; Multipollutants; Super Learner","barium; cadmium; cobalt; dimethanearsonic acid; gamma glutamyltransferase; herbicide; metal complex; methanearsonic acid; unclassified drug; blood system disorder; cardiovascular disease; environmental risk; enzyme activity; epidemiology; health impact; hypertension; machine learning; mortality; oxidative stress; adult; Article; Bayesian additive regression tree; Bayesian kernel machine regression; Bayesian learning; blood; cardiovascular disease; controlled study; diastolic blood pressure; elastic tissue; environmental exposure; environmental impact assessment; environmental risk score; female; human; hypertension; kernel method; machine learning; major clinical study; male; middle aged; mortality; outcome assessment; oxidative stress; priority journal; statistical model; systolic blood pressure; urine",2-s2.0-85029826001
"Evans J.D., Coudert F.-X.","Predicting the Mechanical Properties of Zeolite Frameworks by Machine Learning",2017,"Chemistry of Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029935733&doi=10.1021%2facs.chemmater.7b02532&partnerID=40&md5=d4679949e126ded69dc789ac5878d6a4","We show here that machine learning is a powerful new tool for predicting the elastic response of zeolites. We built our machine learning approach relying on geometric features only, which are related to local geometry, structure, and porosity of a zeolite, to predict bulk and shear moduli of zeolites with an accuracy exceeding that of force field approaches. The development of this model has illustrated clear correlations between characteristic features of a zeolite and elastic moduli, providing exceptional insight into the mechanics of zeolitic frameworks. Finally, we employ this methodology to predict the elastic response of 590 448 hypothetical zeolites, and the results of this massive database provide clear evidence of stability trends in porous materials. © 2017 American Chemical Society.",,"Artificial intelligence; Elastic moduli; Forecasting; Porous materials; Zeolites; Elastic response; Force-field approaches; Geometric feature; Local geometry; Machine learning approaches; Learning systems",2-s2.0-85029935733
"Khedri E., Hasanlou M., Tabatabaeenejad A.","Estimating soil moisture using PolSAR data: A machine learning approach",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032378627&doi=10.5194%2fisprs-archives-XLII-4-W4-133-2017&partnerID=40&md5=ae3dec55c6a4dbdc73569d986549a656","Soil moisture is an important parameter that affects several environmental processes. This parameter has many important functions in numerous sciences including agriculture, hydrology, aerology, flood prediction, and drought occurrence. However, field procedures for moisture calculations are not feasible in a vast agricultural region territory. This is due to the difficulty in calculating soil moisture in vast territories and high-cost nature as well as spatial and local variability of soil moisture. Polarimetric synthetic aperture radar (PolSAR) imaging is a powerful tool for estimating soil moisture. These images provide a wide field of view and high spatial resolution. For estimating soil moisture, in this study, a model of support vector regression (SVR) is proposed based on obtained data from AIRSAR in 2003 in C, L, and P channels. In this endeavor, sequential forward selection (SFS) and sequential backward selection (SBS) are evaluated to select suitable features of polarized image dataset for high efficient modeling. We compare the obtained data with in-situ data. Output results show that the SBS-SVR method results in higher modeling accuracy compared to SFS-SVR model. Statistical parameters obtained from this method show an R2 of 97% and an RMSE of lower than 0.00041 (m3/m3) for P, L, and C channels, which has provided better accuracy compared to other feature selection algorithms.","Sequential backward selection; Sequential forward selection; Soil moisture; Support vector regression","Agricultural machinery; Agriculture; Feature extraction; Learning systems; Moisture; Remote sensing; Soil surveys; Soils; Synthetic aperture radar; Feature selection algorithm; High spatial resolution; Machine learning approaches; Polarimetric synthetic aperture radars; Sequential backward selection; Sequential forward selection; Statistical parameters; Support vector regression (SVR); Soil moisture",2-s2.0-85032378627
"Elhatri C., Tahifa M., Boumhidi J.","Extreme Learning Machine-Based Traffic Incidents Detection with Domain Adaptation Transfer Learning",2017,"Journal of Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031110910&doi=10.1515%2fjisys-2016-0028&partnerID=40&md5=d44c3577c1da0a591f27103770684045","Traffic incidents in big cities are increasing alongside economic growth, causing traffic delays and deteriorating road safety conditions. Thus, developing a universal freeway automatic incident detection (AID) algorithm is a task that took the interest of researchers. This paper presents a novel automatic traffic incident detection method based on the extreme learning machine (ELM) algorithm. Furthermore, transfer learning has recently gained popularity as it can successfully generalise information across multiple tasks. This paper aimed to develop a new approach for the traffic domain-based domain adaptation. The ELM was used as a classifier for detection, and target domain adaptation transfer ELM (TELM-TDA) was used as a tool to transfer knowledge between environments to benefit from past experiences. The detection performance was evaluated by common criteria including detection rate, false alarm rate, and others. To prove the efficiency of the proposed method, a comparison was first made between back-propagation neural network and ELM; then, another comparison was made between ELM and TELM-TDA. © 2017 Walter de Gruyter GmbH, Berlin/Boston.","automatic incident detection (AID); Extreme learning machine (ELM); simulator of urban mobility (SUMO); target domain adaptation transfer ELM (TELM-TDA); traffic control interface (Traci); transfer learning","Backpropagation; Economics; Knowledge acquisition; Motor transportation; Neural networks; Traffic control; Automatic incident detection; Control interfaces; Extreme learning machine; Target domain; Transfer learning; Urban mobility; Learning systems",2-s2.0-85031110910
"Dufourq E., Bassett B.A.","Automated problem identification: Regression vs classification via evolutionary deep networks",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032618664&doi=10.1145%2f3129416.3129429&partnerID=40&md5=35dbf90849dba66bf822b1a26dfe7507","Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine learning algorithms. For example, when creating deep neural networks, the number of parameters must be selected in advance and furthermore, a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function. Humans are able to study a dataset and decide whether it represents a classification or a regression problem, and consequently make decisions which will be applied to the execution of the neural network. We propose the Automated Problem Identification (API) algorithm, which uses an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. We test API on 16 different classification, regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values. API achieves an average accuracy of 96.3% in identifying the problem type without hard-coding any insights about the general characteristics of regression or classification problems. For example, API successfully identifies classification problems even with 1000 target values. Furthermore, the algorithm recommends which loss function to use and also recommends a neural network architecture. Our work is therefore a step towards fully automated machine learning. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.","Classification; Evolutionary algorithm; Machine learning; Neural networks; Regression","Application programming interfaces (API); Artificial intelligence; Automation; Classification (of information); Deep learning; Deep neural networks; Engineers; Learning algorithms; Learning systems; Network architecture; Neural networks; Regression analysis; Deep architectures; Fully automated; Loss functions; Problem identification; Regression; Regression problem; Sentiment analysis; Supervised learning problems; Evolutionary algorithms",2-s2.0-85032618664
"Kunene D., Vadapalli H.","Better feature acquisition through the use of infrared imaging for human detection systems",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032634327&doi=10.1145%2f3129416.3129437&partnerID=40&md5=460051cb5496092f58677b5b67488e19","Human detection on static images remains a challenging research problem. This work evaluates the significance of using infrared imaging (IIR) over several human detection systems. Larger complexities arise when detecting people in colour images due to the possibility of random colour patterns on the image backgrounds and clothes of pedestrians. In most cases, the colour clutter contributes negatively to image representation methods that solely rely on edge information. The basis of our supposition is that the choice of information has a large impact on the robustness of statistical learning systems. To test this supposition, we created and published a new infrared-based pedestrian dataset called “SIGNI"" [9]. Several datasets of the same size were prepared and tested on three different classifiers. The classifiers are first trained with popular colour datasets to determine the optimal parameters that obtain high classification rates on unseen samples. Once satisfactory results are obtained, the same parameters are used for training the classifiers with infrared samples. The conventional use of support vector machines (SVM) on HOG features is tested against extreme learning machines (ELM) and convolutional neural networks (CNN). The results obtained show that the reduction of noise clutter improves the quality of acquired HOG features. As slight performance gains were observed during the classification of infrared samples over the use of visual samples. © 2017 Association for Computing Machinery.","Convolutional neural networks; Extreme learning machines; Feature extraction; Human detection; Infrared imaging; Noise-reduction; Support vec tor machines","Clutter (information theory); Color; Convolution; Engineers; Feature extraction; IIR filters; Infrared imaging; Knowledge acquisition; Learning systems; Neural networks; Noise abatement; Statistical tests; Support vector machines; Thermography (imaging); Classification rates; Convolutional neural network; Extreme learning machine; Feature acquisition; Human detection; Image representations; Research problems; Statistical learning; Classification (of information)",2-s2.0-85032634327
"Nusair K.N., Alomoush M.I.","Optimal reactive power dispatch using teaching learning based optimization algorithm with consideration of FACTS device ""STATCOM""",2017,"2017 10th Jordan International Electrical and Electronics Engineering Conference, JIEEEC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032837688&doi=10.1109%2fJIEEEC.2017.8051398&partnerID=40&md5=230be4c23b5ab6915d5dcb2cddcf6d15","This paper shows solution of optimal reactive power dispatch (ORPD) problem using a Teaching Learning Based Optimization Algorithm (TLBO) with consideration of flexible alternating current transmission systems (FACTS) device 'STATCOM'. The target is to minimize the transmission losses, enhance the voltage profile, determine the optimal value of control variables such as generator voltage magnitudes, tap setting of the transformer and number of compensation devices and also maintain a reasonable system performance in terms of limits on generator real power and reactive power outputs, bus voltages and power flow of transmission lines. In order to reduce the total active power loss, improve power system voltage, enhance reliability and increase power transfer limits. We propose also the optimization of the placement of FACTS devices in the power system (STATCOM). The proposed method is examined on IEEE 14-bus and modified IEEE 30-bus power systems. The results of this technique is compared with previous results obtained by particle swarm optimization, Differential evolution (DE), Modified Hybrid PSO (MHPSO), Mutated PSO (MPSO), Self adaptive real coded genetic algorithm (SARGA), Genetic Search (GS), Comprehensive learning PSO, Control schemes of the strategy parameters (CSSPs), Evolutionary programming (EP), Sequential quadratic programming (SQP), Particle swarm optimization-Cauchy mutation (PSO-CM), Particle swarm optimization-Adaptive mutation (PSO-AM), Hybrid algorithm of differential evolutionary programming (DEEP). © 2017 IEEE.","Active Power Loss; Flexible Alternating Current Transmission Systems (FACTS); Optimal Reactive Power Dispatch (ORPD); Static Synchronous Compensator (STATCOM); Teaching Learning Based Optimization Algorithm (TLBo)","Computer programming; Electric current regulators; Electric impedance measurement; Electric load flow; Electric machine control; Electric power transmission; Energy transfer; Evolutionary algorithms; Flexible AC transmission systems; Genetic algorithms; Learning algorithms; Optimization; Particle swarm optimization (PSO); Quadratic programming; Reactive power; Static synchronous compensators; Active power loss; Flexible alternating current transmission systems; Optimal reactive power dispatch; Static synchronous compensator STATCOM; Teaching-learning-based optimizations; Electric load dispatching",2-s2.0-85032837688
"Dufourq E., Bassett B.A.","Automated classification of text sentiment",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032643076&doi=10.1145%2f3129416.3129420&partnerID=40&md5=1e8b21cd205c85b5085c31ea750d949c","The ability to identify sentiment in text, referred to as sentiment analysis, is one which is natural to adult humans. This task is, however, not one which a computer can perform by default. Identifying sentiments in an automated, algorithmic manner will be a useful capability for business and research in their search to understand what consumers think about their products or services and to understand human sociology. Here we propose two new Genetic Algorithms (GAs) for the task of automated text sentiment analysis. The GAs learn whether words occurring in a text corpus are either sentiment or amplifier words, and their corresponding magnitude. Sentiment words, such as’horrible’, add linearly to the final sentiment. Amplifier words in contrast, which are typically adjectives/adverbs like’very’, multiply the sentiment of the following word. This increases, decreases or negates the sentiment of the following word. The sentiment of the full text is then the sum of these terms. This approach grows both a sentiment and amplifier dictionary which can be reused for other purposes and fed into other machine learning algorithms. We report the results of multiple experiments conducted on large Amazon data sets. The results reveal that our proposed approach was able to outperform several public and/or commercial sentiment analysis algorithms. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.","Genetic algorithm; Machine learning; Sentiment analysis","Artificial intelligence; Automation; Data mining; Engineers; Genetic algorithms; Learning systems; Natural language processing systems; Text processing; Automated classification; New genetic algorithms; Sentiment analysis; Text corpora; Learning algorithms",2-s2.0-85032643076
"Gu J., Hu H., Li H.","Local robust sparse representation for face recognition with single sample per person",2017,"IEEE/CAA Journal of Automatica Sinica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030771894&doi=10.1109%2fJAS.2017.7510658&partnerID=40&md5=0e64478649a6e6f9d8e8abfccd8f5d2d","The purpose of this paper is to solve the problem of robust face recognition U+0028 FR U+0029 with single sample per person U+0028 SSPP U+0029. In the scenario of FR with SSPP, we present a novel model local robust sparse representation U+0028 LRSR U+0029 to tackle the problem of query images with various intra-class variations, e.g., expressions, illuminations, and occlusion. FR with SSPP is a very difficult challenge due to lacking of information to predict the possible intra-class variation of the query images. The key idea of the proposed method is to combine a local sparse representation model and a patch-based generic variation dictionary learning model to predict the possible facial intraclass variation of the query images. The experimental results on the AR database, Extended Yale B database, CMU-PIE database and LFW database show that the proposed method is robust to intra-class variations in FR with SSPP, and outperforms the state-of-art approaches. IEEE","Dictionaries; Face; Face recognition; Feature extraction; Machine learning; Robustness; Training","Database systems; Feature extraction; Glossaries; Learning systems; Personnel training; Query processing; Robustness (control systems); Dictionary learning; Face; Intra-class variation; Patch based; Query images; Single sample; Sparse representation; Yale B database; Face recognition",2-s2.0-85030771894
"Guan H., Liu T., Jiang J., Tao D., Zhang J., Niu H., Zhu W., Wang Y., Cheng J., Kochan N.A., Brodaty H., Sachdev P., Wen W.","Classifying MCI subtypes in community-dwelling elderly using cross-sectional and longitudinal MRI-based biomarkers",2017,"Frontiers in Aging Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030159426&doi=10.3389%2ffnagi.2017.00309&partnerID=40&md5=944c25b5d6e4011019a9aa9617270356","Amnestic MCI (aMCI) and non-amnestic MCI (naMCI) are considered to differ in etiology and outcome. Accurately classifying MCI into meaningful subtypes would enable early intervention with targeted treatment. In this study, we employed structural magnetic resonance imaging (MRI) for MCI subtype classification. This was carried out in a sample of 184 community-dwelling individuals (aged 73-85 years). Cortical surface based measurements were computed from longitudinal and cross-sectional scans. By introducing a feature selection algorithm, we identified a set of discriminative features, and further investigated the temporal patterns of these features. A voting classifier was trained and evaluated via 10 iterations of cross-validation. The best classification accuracies achieved were: 77% (naMCI vs. aMCI), 81% (aMCI vs. cognitively normal (CN)) and 70% (naMCI vs. CN). The best results for differentiating aMCI from naMCI were achieved with baseline features. Hippocampus, amygdala and frontal pole were found to be most discriminative for classifying MCI subtypes. Additionally, we observed the dynamics of classification of several MRI biomarkers. Learning the dynamics of atrophy may aid in the development of better biomarkers, as it may track the progression of cognitive impairment. © 2017 Guan, Liu, Jiang, Tao, Zhang, Niu, Zhu, Wang, Cheng, Kochan, Brodaty, Sachdev and Wen.","Biomarker; Early diagnosis; Feature selection; Longitudinal data; Machine learning; Mild cognitive impairment; MRI","biological marker; aged; amygdala; Article; atrophy; classification algorithm; clinical feature; community living; controlled study; cross-sectional study; diagnostic accuracy; discrimination learning; disease classification; dynamics; female; frontal lobe; hippocampus; human; longitudinal study; machine learning; major clinical study; male; mild cognitive impairment; non amnestic mild cognitive impairment; nuclear magnetic resonance imaging; pattern recognition; validation process; very elderly",2-s2.0-85030159426
"Imbach P., Fung E., Hannah L., Navarro-Racines C.E., Roubik D.W., Ricketts T.H., Harvey C.A., Donatti C.I., Läderach P., Locatelli B., Roehrdanz P.R.","Coupling of pollination services and coffee suitability under climate change",2017,"Proceedings of the National Academy of Sciences of the United States of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029933908&doi=10.1073%2fpnas.1617940114&partnerID=40&md5=69199fb7d84be98d04e5d25bf04b4262","Climate change will cause geographic range shifts for pollinators and major crops, with global implications for food security and rural livelihoods. However, little is known about the potential for coupled impacts of climate change on pollinators and crops. Coffee production exemplifies this issue, because large losses in areas suitable for coffee production have been projected due to climate change and because coffee production is dependent on bee pollination. We modeled the potential distributions of coffee and coffee pollinators under current and future climates in Latin America to understand whether future coffee-suitable areas will also be suitable for pollinators. Our results suggest that coffee-suitable areas will be reduced 73–88% by 2050 across warming scenarios, a decline 46–76% greater than estimated by global assessments. Mean bee richness will decline 8–18% within future coffee-suitable areas, but all are predicted to contain at least 5 bee species, and 46–59% of future coffee-suitable areas will contain 10 or more species. In our models, coffee suitability and bee richness each increase (i.e., positive coupling) in 10–22% of future coffee-suitable areas. Diminished coffee suitability and bee richness (i.e., negative coupling), however, occur in 34–51% of other areas. Finally, in 31–33% of the future coffee distribution areas, bee richness decreases and coffee suitability increases. Assessing coupled effects of climate change on crop suitability and pollination can help target appropriate management practices, including forest conservation, shade adjustment, crop rotation, or status quo, in different regions. © 2017, National Academy of Sciences. All rights reserved.","Adaptation strategies; Coffee; Pollination; Smallholder farms; Suitability modeling","Article; climate change; coffee; crop; crop rotation; forest; machine learning; nonhuman; pollination; predictive value; priority journal; South and Central America; species diversity; species richness; warming",2-s2.0-85029933908
"Goshkoderov A.A., Khlebnikov N.A., Obabkov I.N., Serkov K.V., Gajniyarov I.M., Aliev A.A.","Software development for teleroentgenogram analysis",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031113369&doi=10.1063%2f1.5002999&partnerID=40&md5=40f2dc3aed21caec6edcc30efa234f0b","A framework for the analysis and calculation of teleroentgenograms was developed. Software development was carried out in the Department of Children's Dentistry and Orthodontics in Ural State Medical University. The software calculates the teleroentgenogram by the original method which was developed in this medical department. Program allows designing its own methods for calculating the teleroentgenograms by new methods. It is planned to use the technology of machine learning (Neural networks) in the software. This will help to make the process of calculating the teleroentgenograms easier because methodological points will be placed automatically. © 2017 Author(s).",,,2-s2.0-85031113369
"Dzulkifli S.A., Salleh M.N.M., Leman A.M.","Customer and performance rating in QFD using SVM classification",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030657392&doi=10.1063%2f1.5002396&partnerID=40&md5=f87477a6f4574a12274928448ae6e6a9","In a classification problem, where each input is associated to one output. Training data is used to create a model which predicts values to the true function. SVM is a popular method for binary classification due to their theoretical foundation and good generalization performance. However, when trained with noisy data, the decision hyperplane might deviate from optimal position because of the sum of misclassification errors in the objective function. In this paper, we introduce fuzzy in weighted learning approach for improving the accuracy of Support Vector Machine (SVM) classification. The main aim of this work is to determine appropriate weighted for SVM to adjust the parameters of learning method from a given set of noisy input to output data. The performance and customer rating in Quality Function Deployment (QFD) is used as our case study to determine implementing fuzzy SVM is highly scalable for very large data sets and generating high classification accuracy. © 2017 Author(s).",,,2-s2.0-85030657392
"Boulogne L.H., Wolf B.J., Wiering M.A., Van Netten S.M.","Performance of neural networks for localizing moving objects with an artificial lateral line",2017,"Bioinspiration and Biomimetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031693920&doi=10.1088%2f1748-3190%2faa7fcb&partnerID=40&md5=36059b629d2962b96e3e54fb9e3faa3a","Fish are able to sense water flow velocities relative to their body with their mechanoreceptive lateral line organ. This organ consists of an array of flow detectors distributed along the fish body. Using the excitation of these individual detectors, fish can determine the location of nearby moving objects. Inspired by this sensory modality, it is shown here how neural networks can be used to extract an object's location from simulated excitation patterns, as can be measured along arrays of stationary artificial flow velocity sensors. The applicability, performance and robustness with respect to input noise of different neural network architectures are compared. When trained and tested under high signal to noise conditions (46 dB), the Extreme Learning Machine architecture performs best with a mean Euclidean error of 0.4% of the maximum depth of the field D, which is taken half the length of the sensor array. Under lower signal to noise conditions Echo State Networks, having recurrent connections, enhance the performance while the Multilayer Perceptron is shown to be the most noise robust architecture. Neural network performance decreased when the source moves close to the sensor array or to the sides of the array. For all considered architectures, increasing the number of detectors per array increased localization performance and robustness. © 2017 IOP Publishing Ltd.","flow sensing; hydrodynamic imaging; lateral line; neural network; source localization","Fish; Flow of water; Flow velocity; Learning systems; Neural networks; Signal to noise ratio; Echo state networks; Excitation pattern; Extreme learning machine; Flow sensing; lateral line; Localization performance; Sensory modality; Source localization; Network architecture; biomimetic material; animal; artificial neural network; fish; lateral line system; mechanoreceptor; physiology; standards; Animals; Biomimetic Materials; Fishes; Lateral Line System; Mechanoreceptors; Neural Networks (Computer)",2-s2.0-85031693920
"Ge Z., Song Z., Ding S.X., Huang B.","Data Mining and Analytics in the Process Industry: the Role of Machine Learning",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030772750&doi=10.1109%2fACCESS.2017.2756872&partnerID=40&md5=bccaef13df9476d05a03e9cc8ec8cf15","Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computa-tional engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry. OAPA","Analytical models; Data analytics; Data mining; Data mining; Data models; Industries; Machine learning; Machine learning algorithms; Manufacturing; Predictive models; Process industry","Analytical models; Artificial intelligence; Data structures; Decision making; Industry; Learning algorithms; Learning systems; Manufacture; Pattern recognition; Predictive analytics; Supervised learning; Application status; Data analytics; Predictive models; Process industries; State of the art; Data mining",2-s2.0-85030772750
"Wen Y., Lao Y.","Enhancing PUF reliability by machine learning",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032692682&doi=10.1109%2fISCAS.2017.8050672&partnerID=40&md5=e4558bb3372afa5a4bea16b6a2dd05d2","Physical Unclonable Functions (PUFs) are promising security primitives for device authentication and key generation. This paper proposes a two-step methodology to improve the reliability of PUF under noisy conditions. The first step involves acquiring the parameters of PUF models by using machine learning algorithms. The second step then utilizes these obtained parameters to improve the reliability of PUFs by selectively choosing challenge-response pairs (CRPs) for authentication. Two distinct algorithms for improving the reliability of multiplexer (MUX) PUF, i.e., total delay difference thresholding and sensitive hits grouping, are presented. It is important to note that the methodology can be easily applied to other types of PUFs as well. Our experimental results show that the reliability of PUF-based authentication can be significantly improved by the proposed approaches. For example, in one experimental setting, the reliability of an MUX PUF is improved from 89.75% to 94.07% usmg total delay difference thresholding, while 89.30% of generated challenges are stored. As opposed to total delay difference thresholding, sensitive bits grouping possesses higher efficiency, as it can produce reliable CRPs directly. Our experimental results show that the reliability can be improved to 96.91% under the same setting, when we group 12 bits in the challenge vector of a 128-stage MUX PUF. © 2017 IEEE.",,"Artificial intelligence; Authentication; Learning algorithms; Learning systems; Reliability; Challenge-response pair; Device authentications; Higher efficiency; Key generation; Noisy conditions; Security primitives; Thresholding; Cryptography",2-s2.0-85032692682
"Narayanan P., Sanches L.L., Fumarola A., Shelby R.M., Ambrogio S., Jang J., Hwang H., Leblebici Y., Burr G.W.","Reducing circuit design complexity for neuromorphic machine learning systems based on Non-Volatile Memory arrays",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029492446&doi=10.1109%2fISCAS.2017.8050988&partnerID=40&md5=b541e421025a12ee10169d2bd3af51e3","Machine Learning (ML) is an attractive application of Non-Volatile Memory (NVM) arrays [1,2]. However, achieving speedup over GPUs will require minimal neuron circuit sharing and thus highly area-efficient peripheral circuitry, so that ML reads and writes are massively parallel and time-multiplexing is minimized [2]. This means that neuron hardware offering full 'software-equivalent' functionality is impractical. We analyze neuron circuit needs for implementing back-propagation in NVM arrays and introduce approximations to reduce design complexity and area. We discuss the interplay between circuits and NVM devices, such as the need for an occasional RESET step, the number of programming pulses to use, and the stochastic nature of NVM conductance change. In all cases we show that by leveraging the resilience of the algorithm to error, we can use practical circuit approaches yet maintain competitive test accuracies on ML benchmarks. © 2017 IEEE.",,"Artificial intelligence; Backpropagation; Data storage equipment; Digital storage; Learning systems; Neurons; Nonvolatile storage; Program processors; Stochastic systems; Timing circuits; Circuit designs; Design complexity; Massively parallels; Non-volatile memory; Peripheral circuitry; Programming pulse; Stochastic nature; Time multiplexing; Integrated circuit manufacture",2-s2.0-85029492446
"Neftci E., Augustine C., Paul S., Detorakis G.","Event-driven random backpropagation: Enabling neuromorphic deep learning machines",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032696431&doi=10.1109%2fISCAS.2017.8050529&partnerID=40&md5=589d948734bdbf2e578a4fcb84d56dcf","An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. The gradient descent back-propagation rule is a powerful algorithm that is ubiquitous in deep learning, but it relies on the immediate availability of network-wide information stored with high-precision memory. However, recent work shows that exact backpropagated weights are not essential for learning deep representations. Here, we demonstrate an event-driven random backpropagation (eRBP) rule that uses an error-modulated synaptic plasticity rule for learning deep representations in neuromorphic computing hardware. The rule is very suitable for implementation in neuromorphic hardware using a two-compartment leaky integrate & fire neuron and a membrane-voltage modulated, spike-driven plasticity rule. Our results show that using eRBP, deep representations are rapidly learned without using backpropagated gradients, achieving nearly identical classification accuracies compared to artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning. © 2017 IEEE.",,"Backpropagation algorithms; Deep learning; Hardware; Learning systems; Neural networks; Program processors; Artificial neural network simulation; Classification accuracy; Computationally efficient; Learning machines; Neuromorphic computing; Neuromorphic hardwares; Synaptic plasticity rules; Temporal constraints; Backpropagation",2-s2.0-85032696431
"Pedroni B.U., Sheik S., Cauwenberghs G.","Pipelined parallel contrastive divergence for continuous generative model learning",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032670080&doi=10.1109%2fISCAS.2017.8050273&partnerID=40&md5=4b1efd197860148bf69badec8cb872e6","In this paper we propose a method for continuously processing and learning from data in Restricted Boltzmann Machines (RBMs). Traditionally, RBMs are trained using Contrastive Divergence (CD), which is an algorithm consisting of two phases, of which only one is driven by data. This not only prohibits training of RBMs in conjugation with continuous-time data streams, especially in event-based real-time systems, but also hinders training speed of RBMs in large-scale machine learning systems. The model we propose trades space for time and, by pipelining information propagation in the network, is capable of processing both phases of the CD learning algorithm simultaneously. Simulation results of our model on generative and discriminative tasks show convergence to the original CD algorithm. We finalize with a discussion of applying our method to other deep neural networks, resulting in continuous learning and training time reduction. © 2017 IEEE.",,"Deep learning; Deep neural networks; Information dissemination; Interactive computer systems; Learning algorithms; Learning systems; Real time systems; Continuous learning; Continuous-time; Contrastive divergence; Generative model; Information propagation; Large-scale machine learning; Restricted boltzmann machine; Training speed; Continuous time systems",2-s2.0-85032670080
"Saldana-Perez A.M.M., Moreno-Ibarra M., Tores-Ruiz M.","Classification of traffic related short texts to analyse road problems in urban areas",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032626265&doi=10.5194%2fisprs-archives-XLII-4-W3-91-2017&partnerID=40&md5=987feefac131d9e1ba8e5bd202ef3636","The Volunteer Geographic Information (VGI) can be used to understand the urban dynamics. In the classification of traffic related short texts to analyze road problems in urban areas, a VGI data analysis is done over a social media’s publications, in order to classify traffic events at big cities that modify the movement of vehicles and people through the roads, such as car accidents, traffic and closures. The classification of traffic events described in short texts is done by applying a supervised machine learning algorithm. In the approach users are considered as sensors which describe their surroundings and provide their geographic position at the social network. The posts are treated by a text mining process and classified into five groups. Finally, the classified events are grouped in a data corpus and geo-visualized in the study area, to detect the places with more vehicular problems. © Authors 2017. CC BY 4.0 License.","Classification; Data Analysis; Human sensors; Machine Learning; Traffic; Volunteered Geographic Information","Accidents; Artificial intelligence; Classification (of information); Data handling; Data reduction; Information analysis; Learning algorithms; Learning systems; Roads and streets; Smart city; Supervised learning; Telecommunication traffic; Transportation; Car accidents; Geographic information; Human sensors; Supervised machine learning; Traffic event; Traffic-related; Urban dynamics; Volunteered geographic information; Data mining",2-s2.0-85032626265
"Yong B., Li F., Lv Q., Shen J., Zhou Q.","Derivative-based acceleration of general vector machine",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029811744&doi=10.1007%2fs00500-017-2808-z&partnerID=40&md5=a461255d02d3dde9dccbbf3d258b8977","General vector machine (GVM) is one of supervised learning machine, which is based on three-layer neural network. It is capable of constructing a learning model with limited amount of data. Generally, it employs Monte Carlo algorithm (MC) to adjust weights of the underlying network. However, GVM is time-consuming at training and is not efficient when compared with other learning algorithm based on gradient descent learning. In this paper, we present a derivative-based Monte Carlo algorithm (DMC) to accelerate the training of GVM. Our experimental results indicate that DMC algorithm is faster than the original MC method. Specifically, the training time of our DMC algorithm in GVM for function fitting is also less than some gradient descent-based methods, in which we compare DMC with back-propagation neural network. Experimental results indicate that our algorithm is promising for training GVM. © 2017 Springer-Verlag GmbH Germany","Back-propagation; Derivative; General vector machine; Gradient descent; Neural network","Backpropagation; Derivatives; Learning algorithms; Learning systems; Monte Carlo methods; Network layers; Neural networks; Back propagation neural networks; Function fitting; Gradient descent; Learning models; Monte carlo algorithms; Three-layer neural networks; Underlying networks; Vector machines; Backpropagation algorithms",2-s2.0-85029811744
"Petraglia F.R., Campos R., Gomes J.G.R.C., Petraglia M.R.","Pipeline tracking and event classification for an automatic inspection vision system",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032671721&doi=10.1109%2fISCAS.2017.8050761&partnerID=40&md5=d948af49c33fe0c8b89eccf6b50c5832","Automatic inspection of underwater pipelines has been a task of growing importance for the detection of a variety of events, such as inner coating exposure and twists, which might indicate risks for future leakages. Most often performed by autonomous underwater vehicles, the video inspections might benefit from image processing, estimation and machine learning techniques in order to accurately detect and classify such occurrences. This article describes algorithms designed to segment the pipelines and to classify some important events. In order to obtain a robust technique for pipeline border detection in a diversity of scenarios, several image processing techniques and tracking strategies are employed. A deep convolutional neural network algorithm and a wavelet-based multilayer perceptron are developed for the classification of four types of events. The convolutional neural network technique outperforms the perceptron algorithm, for different event classes and without requiring manual feature extraction, when a large amount of training samples is used. © 2017 IEEE.","deep convolutional neural network; edge detection; machine learning; multilayer perceptron; Underwater pipeline inspection","Artificial intelligence; Autonomous underwater vehicles; Convolution; Deep neural networks; Edge detection; Image processing; Inspection; Learning systems; Multilayer neural networks; Multilayers; Neural networks; Pipelines; Submarine pipelines; Automatic inspection; Convolutional neural network; Event classification; Image processing technique; Machine learning techniques; Perceptron algorithms; Robust technique; Tracking strategies; Pipeline processing systems",2-s2.0-85032671721
"Bytyn A., Springer J., Leupers R., Ascheid G.","VLSI implementation of LS-SVM training and classification using entropy based subset-selection",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032664117&doi=10.1109%2fISCAS.2017.8050590&partnerID=40&md5=4e5544f777498295443725c5bc69f7c6","Machine Learning techniques such as Support Vector Machines (SVM) have found applications in many fields, e.g. in Wireless Sensor Networks (WSN) and sensor data processing in general. Especially in the case of WSN energy is very limited as agents solely operate based on battery power after they have been deployed, therefore energy efficiency is of great importance. Furthermore, agents are supposed to adapt to their environment by being capable of re-training themselves based on feedback they get from their surroundings, which increases the computational demands on the digital hardware involved. To meet these demands, dedicated hardware in form of a very-large-scale integrated (VLSI) circuit is a reasonable approach and is investigated here. In this paper a specific variant of the SVM - the Least-Squares SVM - is implemented as VLSI circuit. Additionally during the training phase a subset-selection technique based on the quadratic Renyi entropy is implemented in order to reduce the computational and hardware demands. The resulting design consumes 21.35 mW and requires an area of 81.2 kGE without memories. © 2017 IEEE.",,"Data handling; Energy efficiency; Hardware; Learning systems; VLSI circuits; Wireless sensor networks; Computational demands; Dedicated hardware; Digital hardware; Least squares SVM; Machine learning techniques; Sensor data processing; Very large scale integrated; VLSI implementation; Support vector machines",2-s2.0-85032664117
"Zhao W., Guan Z., Chen L., He X., Cai D., Wang B., Wang Q.","Weakly-supervised Deep Embedding for Product Review Sentiment Analysis",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773560&doi=10.1109%2fTKDE.2017.2756658&partnerID=40&md5=ac5e58bb55fbd7f1acfa04f6576edee4","Product reviews are valuable for upcoming buyers in helping them make decisions. To this end, different opinion techniques have been proposed, where judging a review sentence's orientation (e.g. positive or negative) is one of their key challenges. Recently, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. We propose a novel deep learning framework for product review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learning a high level representation (an embedding space) which captures the general sentiment distribution of sentences through rating information; (2) adding a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. We explore two kinds of low level network structure for modeling review sentences, namely, convolutional feature extractors and long short-term memory. To evaluate the proposed framework, we construct a dataset containing 1.1M weakly labeled review sentences and 11,754 labeled review sentences from Amazon. Experimental results show the efficacy of the proposed framework and its superiority over baselines. IEEE","Deep learning; Feature extraction; Machine learning; Neural networks; opinion mining; Sentiment analysis; sentiment classification; Syntactics; Training; weak-supervision","Data mining; Deep learning; Feature extraction; Learning systems; Long short-term memory; Network function virtualization; Neural networks; Personnel training; Syntactics; Feature extractor; Learning frameworks; Network structures; Opinion mining; Rating information; Sentiment analysis; Sentiment classification; weak-supervision; Classification (of information)",2-s2.0-85030773560
"Ilie I., Dittrich P., Carvalhais N., Jung M., Heinemeyer A., Migliavacca M., Morison J.I.L., Sippel S., Subke J.-A., Wilkinson M., Mahecha D.M.","Reverse engineering model structures for soil and ecosystem respiration: The potential of gene expression programming",2017,"Geoscientific Model Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029939270&doi=10.5194%2fgmd-10-3519-2017&partnerID=40&md5=28493477b4ee8012bfb9a8cc73e7a8da","Accurate model representation of land- atmosphere carbon fluxes is essential for climate projections. However, the exact responses of carbon cycle processes to climatic drivers often remain uncertain. Presently, knowledge derived from experiments, complemented by a steadily evolving body of mechanistic theory, provides the main basis for developing such models. The strongly increasing availability of measurements may facilitate new ways of identifying suitable model structures using machine learning. Here, we explore the potential of gene expression programming (GEP) to derive relevant model formulations based solely on the signals present in data by automatically applying various mathematical transformations to potential predictors and repeatedly evolving the resulting model structures. In contrast to most other machine learning regression techniques, the GEP approach generates ""readable"" models that allow for prediction and possibly for interpretation. Our study is based on two cases: artificially generated data and real observations. Simulations based on artificial data show that GEP is successful in identifying prescribed functions, with the prediction capacity of the models comparable to four state-of-the-art machine learning methods (random forests, support vector machines, artificial neural networks, and kernel ridge regressions). Based on real observations we explore the responses of the different components of terrestrial respiration at an oak forest in south-eastern England. We find that the GEP-retrieved models are often better in prediction than some established respiration models. Based on their structures, we find previously unconsidered exponential dependencies of respiration on seasonal ecosystem carbon assimilation and water dynamics. We noticed that the GEP models are only partly portable across respiration components, the identification of a ""general"" terrestrial respiration model possibly prevented by equifinality issues. Overall, GEP is a promising tool for uncovering new model structures for terrestrial ecology in the data-rich era, complementing more traditional modelling approaches. © Author(s) 2017.",,"accuracy assessment; algorithm; automation; carbon cycle; carbon flux; deciduous forest; linear programing; machine learning; soil ecosystem; support vector machine; England; United Kingdom",2-s2.0-85029939270
"Hasegawa K., Yanagisawa M., Togawa N.","Trojan-feature extraction at gate-level netlists and its application to hardware-Trojan detection using random forest classifier",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032655111&doi=10.1109%2fISCAS.2017.8050827&partnerID=40&md5=b8a26a7fa0697719d38cc351c3b2959b","Recently, due to the increase of outsourcing in IC design, it has been reported that malicious third-party vendors often insert hardware Trojans into their ICs. How to detect them is a strong concern in IC design process. The features of hardware-Trojan infected nets (or Trojan nets) in ICs often differ from those of normal nets. To classify all the nets in netlists designed by third-party vendors into Trojan ones and normal ones, we have to extract effective Trojan features from Trojan nets. In this paper, we first propose 51 Trojan features which describe Trojan nets from netlists. Based on the importance values obtained from the random forest classifier, we extract the best set of 11 Trojan features out of the 51 features which can effectively detect Trojan nets, maximizing the F-measures. By using the 11 Trojan features extracted, the machine-learning based hardware Trojan classifier has achieved at most 100% true positive rate as well as 100% true negative rate in several TrustHUB benchmarks and obtained the average F-measure of 74.6%, which realizes the best values among existing machine-learning-based hardware-Trojan detection methods. © 2017 IEEE.","F-measure; gate-level netlist; hardware Trojan; machine learning; random forest","Artificial intelligence; Cesium compounds; Decision trees; Feature extraction; Hardware; Integrated circuit design; Integrated circuits; Learning systems; Malware; Outsourcing; F measure; Hardware Trojan detection; Netlist; Random forest classifier; Random forests; Third party vendors; True negative rates; True positive rates; Hardware security",2-s2.0-85032655111
"Yue C., Jin R., Suh K., Qin Y., Wang B., Wei W.","LinkForecast: Cellular Link Bandwidth Prediction in LTE Networks",2017,"IEEE Transactions on Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030776801&doi=10.1109%2fTMC.2017.2756937&partnerID=40&md5=0634dc0a76e4aca5e079f1c98c5e98b0","Accurate cellular link bandwidth prediction can benefit upper-layer protocols significantly. In this paper, we investigate how to predict cellular link bandwidth in LTE networks. We first conduct an extensive measurement study in two major commercial LTE networks in the US, and identify five types of lower-level information that is correlated with cellular link bandwidth. We then develop a machine learning based prediction framework, LinkForecast, that identifies most important features (from both upper and lower layers) and uses these features to predict link bandwidth in real time. Our evaluation shows that LinkForecast is lightweight and the prediction is highly accurate: at the time granularity of one second, the average prediction error is in the range of 3.9% to 17.0% for all the scenarios we explore. We further investigate the prediction performance when using lower-layer features obtained through standard APIs provided by the operating system, instead of specialized tools. Our results show that, while features thus obtained have lower fidelity, they lead to similar prediction accuracy as that using higher fidelity features, indicating that our approach can be easily used over commercial off-the-shelf mobile devices. IEEE","Bandwidth; cellular link bandwidth prediction; Cellular networks; cellular networks; Correlation; Long Term Evolution; machine learning; network measurement; Protocols; Real-time systems; Throughput","Artificial intelligence; Bandwidth; Correlation methods; Forecasting; Interactive computer systems; Learning systems; Long Term Evolution (LTE); Mobile telecommunication systems; Network protocols; Real time systems; Throughput; Wireless networks; Average prediction error; Cellular links; Cellular network; Commercial off the shelves; Network measurement; Prediction accuracy; Prediction performance; Upper-layer protocols; Wireless telecommunication systems",2-s2.0-85030776801
"Wiatowski T., Grohs P., Bolcskei H.","Energy Propagation in Deep Convolutional Neural Networks",2017,"IEEE Transactions on Information Theory",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773944&doi=10.1109%2fTIT.2017.2756880&partnerID=40&md5=58625c0bd1d8b87635d81c8f61853038","Many practical machine learning tasks employ very deep convolutional neural networks. Such large depths pose formidable computational challenges in training and operating the network. It is therefore important to understand how fast the energy contained in the propagated signals (a.k.a. feature maps) decays across layers. In addition, it is desirable that the feature extractor generated by the network be informative in the sense of the only signal mapping to the all-zeros feature vector being the zero input signal. This &#x201C;trivial null-space&#x201D; property can be accomplished by asking for &#x201C;energy conservation&#x201D; in the sense of the energy in the feature vector being proportional to that of the corresponding input signal. This paper establishes conditions for energy conservation (and thus for a trivial null-space) for a wide class of deep convolutional neural network-based feature extractors and characterizes corresponding feature map energy decay rates. Specifically, we consider general scattering networks employing the modulus non-linearity and we find that under mild analyticity and high-pass conditions on the filters (which encompass, inter alia, various constructions of Weyl-Heisenberg filters, wavelets, ridgelets, (&#x03B1;)-curvelets, and shearlets) the feature map energy decays at least polynomially fast. For broad families of wavelets and Weyl-Heisenberg filters, the guaranteed decay rate is shown to be exponential. Moreover, we provide handy estimates of the number of layers needed to have at least ((1-&#x03B5;)&#x00B7;100)&#x0025; of the input signal energy be contained in the feature vector. IEEE","Convolution; deep convolutional neural networks; Energy conservation; energy decay and conservation; Feature extraction; frame theory; Indexes; Machine learning; Neural networks; Scattering; scattering networks","Artificial intelligence; Computation theory; Convolution; Decay (organic); Energy conservation; Feature extraction; Learning systems; Neural networks; Scattering; Vector spaces; Convolutional neural network; Energy decay; Frame theory; Indexes; Scattering networks; Deep neural networks",2-s2.0-85030773944
"Van Dijk M., Ter Laak A.M., Wichard J.D., Capoferri L., Vermeulen N.P.E., Geerke D.P.","Comprehensive and Automated Linear Interaction Energy Based Binding-Affinity Prediction for Multifarious Cytochrome P450 Aromatase Inhibitors",2017,"Journal of Chemical Information and Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029911836&doi=10.1021%2facs.jcim.7b00222&partnerID=40&md5=e8eb165596e4b15b42030f2d44e252f5","Cytochrome P450 aromatase (CYP19A1) plays a key role in the development of estrogen dependent breast cancer, and aromatase inhibitors have been at the front line of treatment for the past three decades. The development of potent, selective and safer inhibitors is ongoing with in silico screening methods playing a more prominent role in the search for promising lead compounds in bioactivity-relevant chemical space. Here we present a set of comprehensive binding affinity prediction models for CYP19A1 using our automated Linear Interaction Energy (LIE) based workflow on a set of 132 putative and structurally diverse aromatase inhibitors obtained from a typical industrial screening study. We extended the workflow with machine learning methods to automatically cluster training and test compounds in order to maximize the number of explained compounds in one or more predictive LIE models. The method uses protein-ligand interaction profiles obtained from Molecular Dynamics (MD) trajectories to help model search and define the applicability domain of the resolved models. Our method was successful in accounting for 86% of the data set in 3 robust models that show high correlation between calculated and observed values for ligand-binding free energies (RMSE &lt; 2.5 kJ mol-1), with good cross-validation statistics. © 2017 American Chemical Society.",,"Chemical compounds; Learning systems; Ligands; Molecular dynamics; Aromatase inhibitors; Cross validation statistics; In-silico screening; Interaction energies; Linear interaction energies (LIE); Machine learning methods; Molecular dynamics trajectories; Protein-ligand interactions; Binding energy; aromatase; aromatase inhibitor; ligand; protein binding; automation; biology; chemistry; metabolism; molecular dynamics; procedures; protein conformation; statistical model; thermodynamics; Aromatase; Aromatase Inhibitors; Automation; Computational Biology; Ligands; Linear Models; Molecular Dynamics Simulation; Protein Binding; Protein Conformation; Thermodynamics",2-s2.0-85029911836
"Dubey P., Aditya K., Srivastava A., Khanuja A., Kawa J., Nguyen T.","A 0.42V high bandwidth synthesizable parallel access smart memory fabric for computer vision",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032675312&doi=10.1109%2fISCAS.2017.8050235&partnerID=40&md5=88419bed9dd0b906cb423a06122e3e8b","We present a design of a 2 to 12 port scalable multiport compiler with simultaneous read port access and closely packed graphics integration capability specially designed for low power high bandwidth, low latency stream vector processors and machine learning applications. Novel pipe-lined decoder and bitline repeater insertion helps to achieve a fast cycle time. Memory words can be accessed in different ways, serial, parallel or mixed. A wide supply range from 0.4V to 1.1V is supported without any complex write or read assist circuit. Design is non-self-timed and fully testable while timing and power views are generated through a static timing analysis (STA) approach. Layout is based on automatic place and route of standard cells in periphery and full custom standard cell compatible high density memory core. Full custom core is tightly bound with the common graphics processing operations, to enable low latency (< 1μs), high bandwidth operations at low voltage. Hybrid approach reduces the turn around time to just a few man weeks. Area penalty of a 2W2R 64 Kbit instance is up to 10% in comparison to a logic rule based full custom high speed 1W1R compiler, while doubling the throughput. Compared to complete RTL based synthesis approach, area is just 5% for 64 Kbit. A 2W2R 32×128 testchip instance in sub-20nm FinFET process, runs up-to 3 GHz on CAD at 1.1 V supply at -40 ° C. While measured speed of same instance on silicon is 86 MHz (at 0.42 V) for simultaneous access from both the ports and energy consumed is just 5 pJ/cycle in typical process corner. Architecture is scalable up to 64KB for more parallel architectures (64 cores) as demanded in ultra-high definition real time computational photography [1]. © 2017 IEEE.","Cell based design; Chip multi-processor; Data-Path; Energy Efficiency; Internet of Things; machine learning; Neuromorphic Computing; Register File; vector processors","Array processing; Artificial intelligence; Bandwidth; Color photography; Computation theory; Computer aided design; Computer vision; Energy efficiency; Green computing; Internet of things; Learning systems; Memory architecture; Multiprocessing systems; Parallel architectures; Parallel processing systems; Cell-based design; Chip multi-processors; Data paths; Neuromorphic computing; Register files; Vector processors; Program compilers",2-s2.0-85032675312
"Kim M., Mohanty A., Kadetotad D., Suda N., Wei L., Saseendran P., He X., Cao Y., Seo J.-S.","A real-time 17-scale object detection accelerator with adaptive 2000-stage classification in 65nm CMOS",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032692087&doi=10.1109%2fISCAS.2017.8050798&partnerID=40&md5=e50485ba7873fbb5597a5dad273ec0a8","This paper presents an object detection accelerator that features many-scale (17), many-object (up to 50), multi-class (e.g., face, traffic sign), and high accuracy (average precision of 0.79/0.65 for AFW/BTSD datasets). Employing 10 gradient/color channels, integral features are extracted, and the results of 2,000 simple classifiers for rigid boosted templates are adaptively combined to make a strong classification. By jointly optimizing the algorithm and the hardware architecture, the prototype chip implemented in 65nm CMOS demonstrates real-time object detection of 13-35 frames per second with low power consumption of 22-160mW at 0.58-1.0V supply. © 2017 IEEE.","classification; low-power; machine learning; object detection; real-time; special-purpose accelerator","Classification (of information); CMOS integrated circuits; Learning systems; Object recognition; Frames per seconds; Hardware architecture; High-accuracy; Integral features; Low Power; Low-power consumption; Prototype chip; Real time; Object detection",2-s2.0-85032692087
"Kulkarni A., Abtahi T., Shea C., Kulkarni A., Mohsenin T.","PACENet: Energy efficient acceleration for convolutional network on embedded platform",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032655402&doi=10.1109%2fISCAS.2017.8050342&partnerID=40&md5=4020716314bed04edc2532fad4875168","Lightweight convolutional neural network (CNN) on tiny embedded platforms can offer energy efficient solution for today's IoT devices. However, CNN implementation on embedded system faces processing bottleneck in convolutional layers and memory storage issues in fully connected layers. In past years, heterogeneous acceleration, where compute intensive tasks are performed on kernel specific cores, has gained attention. In this paper we propose, a domain specific and programmable accelerator PACENet-Programmable many-core ACcElerator for convolution neural Network architecture. It consists of neural network kernel specific instruction set architecture such as convolution, maxpool and relu. To demonstrate efficiency of the proposed PACENet, we implemented ResNet-20 for CIFAR-10 dataset, where PACENet performs convolution layer, Relu activations, Maxpool layer, and fully-connected layer. We also implemented ResNet-20 for CIFAR-10 dataset on NVIDIA TX1 mobile GPU platform using Tensorflow and cuDNN libraries. Compared to NVIDIA TX1 platform implementation PACENet platform implementation performs 1.4× to 4.5× faster and saves 2.8× to 9× energy consumption respectively. PACENet achieves 2.9× to 9.3× higher throughput per watt as compared to TX1 platform implementation. © 2017 IEEE.","Accelerator; Convolutional Neural Network (CNN); Domain-specific many-core; Energy efficient; Machine Learning","Acceleration; Computer architecture; Convolution; Embedded systems; Energy utilization; Learning systems; Network architecture; Neural networks; Particle accelerators; Compute-intensive tasks; Convolution neural network; Convolutional networks; Convolutional neural network; Energy efficient; Many core; Many-core accelerators; Platform implementations; Energy efficiency",2-s2.0-85032655402
"Navarrete-Perea J., Isasa M., Paulo J.A., Corral-Corral R., Flores-Bautista J., Hernández-Téllez B., Bobes R.J., Fragoso G., Sciutto E., Soberón X., Gygi S.P., Laclette J.P.","Quantitative multiplexed proteomics of Taenia solium cysts obtained from the skeletal muscle and central nervous system of pigs",2017,"PLoS Neglected Tropical Diseases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030626699&doi=10.1371%2fjournal.pntd.0005962&partnerID=40&md5=aee167aaf531999c5b9d62445a23d039","In human and porcine cysticercosis caused by the tapeworm Taenia solium, the larval stage (cysts) can infest several tissues including the central nervous system (CNS) and the skeletal muscles (SM). The cyst’s proteomics changes associated with the tissue localization in the host tissues have been poorly studied. Quantitative multiplexed proteomics has the power to evaluate global proteome changes in response to different conditions. Here, using a TMT-multiplexed strategy we identified and quantified over 4,200 proteins in cysts obtained from the SM and CNS of pigs, of which 891 were host proteins. To our knowledge, this is the most extensive intermixing of host and parasite proteins reported for tapeworm infections.Several antigens in cysticercosis, i.e., GP50, paramyosin and a calcium-binding protein were enriched in skeletal muscle cysts. Our results suggested the occurrence of tissue-enriched antigen that could be useful in the improvement of the immunodiagnosis for cysticercosis. Using several algorithms for epitope detection, we selected 42 highly antigenic proteins enriched for each tissue localization of the cysts. Taking into account the fold changes and the antigen/epitope contents, we selected 10 proteins and produced synthetic peptides from the best epitopes. Nine peptides were recognized by serum antibodies of cysticercotic pigs, suggesting that those peptides are antigens. Mixtures of peptides derived from SM and CNS cysts yielded better results than mixtures of peptides derived from a single tissue location, however the identification of the ‘optimal’ tissue-enriched antigens remains to be discovered. Through machine learning technologies, we determined that a reliable immunodiagnostic test for porcine cysticercosis required at least five different antigenic determinants. © 2017 Navarrete-Perea et al.",,"immunoglobulin G; paramyosin; peptide; synthetic peptide; helminth protein; proteome; animal experiment; animal model; antigenicity; Article; brain cyst; calcium binding; central nervous system; cysticercosis; discriminant analysis; Echinococcus; enzyme linked immunosorbent assay; gene ontology; high performance liquid chromatography; high throughput screening; liquid chromatography-mass spectrometry; machine learning; Mesocestoidea; microscopy; nonhuman; parasite; pig; protein expression; proteomics; sequence analysis; serodiagnosis; skeletal muscle; Taenia solium; tissue distribution; Western blotting; animal; central nervous system; chemistry; isolation and purification; parasitology; proteomics; skeletal muscle; swine disease; Taenia solium; taeniasis; veterinary; Animals; Central Nervous System; Helminth Proteins; Muscle, Skeletal; Proteome; Proteomics; Swine; Swine Diseases; Taenia solium; Taeniasis",2-s2.0-85030626699
"Ueyoshi K., Marukame T., Asai T., Motomura M., Schmid A.","Live demonstration: Feature extraction system using restricted Boltzmann machines on FPGA",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032685715&doi=10.1109%2fISCAS.2017.8050402&partnerID=40&md5=8b85f1b65be97db2d8b51b2ff997959a","Real-time results obtained from an unsupervised feature extraction system using Restricted Boltzmann Machines (RBMs) implemented on FPGA are presented. The feature extraction application is demonstrated using the MNIST dataset, and the weights storing features are visualized in real-time. A digit classification is also performed based on the learning results. Our demonstration system performs 134 times faster than the compared conventional CPU. © 2017 IEEE.",,"Extraction; Feature extraction; Digit classification; Real time; Restricted boltzmann machine; Field programmable gate arrays (FPGA)",2-s2.0-85032685715
"Eskofier B.M., Lee S.I., Baron M., Simon A., Martindale C.F., Gaßner H., Klucken J.","An overview of smart shoes in the internet of health things: Gait and mobility assessment in health promotion and disease monitoring",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030106032&doi=10.3390%2fapp7100986&partnerID=40&md5=d52d32d143923e1ef51510fe3b653764","New smart technologies and the internet of things increasingly play a key role in healthcare and wellness, contributing to the development of novel healthcare concepts. These technologies enable a comprehensive view of an individual's movement and mobility, potentially supporting healthy living as well as complementing medical diagnostics and the monitoring of therapeutic outcomes. This overview article specifically addresses smart shoes, which are becoming one such smart technology within the future internet of health things, since the ability to walk defines large aspects of quality of life in a wide range of health and disease conditions. Smart shoes offer the possibility to support prevention, diagnostic work-up, therapeutic decisions, and individual disease monitoring with a continuous assessment of gait and mobility. This overview article provides the technological as well as medical aspects of smart shoes within this rising area of digital health applications, and is designed especially for the novel reader in this specific field. It also stresses the need for closer interdisciplinary interactions between technological and medical experts to bridge the gap between research and practice. Smart shoes can be envisioned to serve as pervasive wearable computing systems that enable innovative solutions and services for the promotion of healthy living and the transformation of health care. © 2017 by the authors.","Digital health; EHealth; Machine learning; MHealth; Parkinson's disease; Sensor-based gait analysis; Smart shoes; Telehealth",,2-s2.0-85030106032
"Yee Chang A.T., Tan L.T., Duke S., Ng W.-T.","Challenges for quality assurance of target volume delineation in clinical trials",2017,"Frontiers in Oncology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029796556&doi=10.3389%2ffonc.2017.00221&partnerID=40&md5=c144903857eb6f46050d8e8326e97040","In recent years, new radiotherapy techniques have emerged that aim to improve treatment outcome and reduce toxicity. The standard method of evaluating such techniques is to conduct large scale multicenter clinical trials, often across continents. A major challenge for such trials is quality assurance to ensure consistency of treatment across all participating centers. Analyses from previous studies have shown that poor compliance and protocol violation have a significant adverse effect on treatment outcomes. The results of the clinical trials may, therefore, be confounded by poor quality radiotherapy. Target volume delineation (TVD) is one of the most critical steps in the radiotherapy process. Many studies have shown large inter-observer variations in contouring, both within and outside of clinical trials. High precision techniques, such as intensity-modulated radiotherapy, image-guided brachytherapy, and stereotactic radiotherapy have steep dose gradients, and errors in contouring may lead to inadequate dose to the tumor and consequently, reduce the chance of cure. Similarly, variation in organ at risk delineation will make it difficult to evaluate dose response for toxicity. This article reviews the literature on TVD variability and its impact on dosimetry and clinical outcomes. The implications for quality assurance in clinical trials are discussed. © 2017 Chang, Tan, Duke and Ng.","Clinical trial; Contouring guidelines; Education program; Peer review; Target volume delineation variability","brachytherapy; cancer radiotherapy; Clinical Target Volume; clinical trial (topic); dose response; dosimetry; human; intensity modulated radiation therapy; machine learning; medical ontology; multimodal imaging; oncological parameters; program impact; protocol compliance; quality control; Review; stereotactic treatment; systematic review (topic); target volume delineation; treatment outcome; tumor volume",2-s2.0-85029796556
"Liu R., Abdulhameed M.D.M., Wallqvist A.","Molecular Structure-Based Large-Scale Prediction of Chemical-Induced Gene Expression Changes",2017,"Journal of Chemical Information and Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029895062&doi=10.1021%2facs.jcim.7b00281&partnerID=40&md5=14c278d921458f178c0d8953547b4fa6","The quantitative structure-activity relationship (QSAR) approach has been used to model a wide range of chemical-induced biological responses. However, it had not been utilized to model chemical-induced genomewide gene expression changes until very recently, owing to the complexity of training and evaluating a very large number of models. To address this issue, we examined the performance of a variable nearest neighbor (v-NN) method that uses information on near neighbors conforming to the principle that similar structures have similar activities. Using a data set of gene expression signatures of 13 150 compounds derived from cell-based measurements in the NIH Library of Integrated Network-based Cellular Signatures program, we were able to make predictions for 62% of the compounds in a 10-fold cross validation test, with a correlation coefficient of 0.61 between the predicted and experimentally derived signatures - a reproducibility rivaling that of high-throughput gene expression measurements. To evaluate the utility of the predicted gene expression signatures, we compared the predicted and experimentally derived signatures in their ability to identify drugs known to cause specific liver, kidney, and heart injuries. Overall, the predicted and experimentally derived signatures had similar receiver operating characteristics, whose areas under the curve ranged from 0.71 to 0.77 and 0.70 to 0.73, respectively, across the three organ injury models. However, detailed analyses of enrichment curves indicate that signatures predicted from multiple near neighbors outperformed those derived from experiments, suggesting that averaging information from near neighbors may help improve the signal from gene expression measurements. Our results demonstrate that the v-NN method can serve as a practical approach for modeling large-scale, genomewide, chemical-induced, gene expression changes. © 2017 American Chemical Society.",,"Computational chemistry; Genes; Software testing; Statistical tests; 10-fold cross-validation; Areas under the curves; Correlation coefficient; Expression measurements; Gene expression signatures; Large-scale prediction; Quantitative structure-activity relationships; Receiver operating characteristics; Gene expression; biology; drug effects; gene expression regulation; human; machine learning; procedures; quantitative structure activity relation; Computational Biology; Gene Expression Regulation; Humans; Machine Learning; Quantitative Structure-Activity Relationship",2-s2.0-85029895062
"Mackay J., Gangopadhyay A., Chakrabartty S.","FPGA demonstration of spiking support vector networks based on growth transform neurons",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032677751&doi=10.1109%2fISCAS.2017.8050401&partnerID=40&md5=1b6ea6c93648b96440d70fc300926f1a","Growth transform neuron models provide a neuromorphic approach for implementing well established machine learning algorithms while producing neural and population dynamics similar to what have been observed in biology, for example, spiking, bursting and noise-shaping [1]. In this demonstration, we will show some of these dynamics in real-time using an FPGA based acceleration platform that implements a network of growth transform neurons. The demonstration setup (Fig 1) will consist of a custom printed circuit board (PCB) that will interface to a laptop display for real-time display. The PCB will host a USB module, a Spartan 6 field programmable gate array (FPGA), and a VGA adaptor such that it will be possible to output the VGA signal to an external monitor. The FPGA will implement the spiking support vector machine (SVM) using growth transform neuron models in the same manner as described in the appended paper. The inputs to the FPGA will include the network interconnection (synaptic) matrix and will correspond to a SVM kernel matrix. These parameters can be programmed using a laptop as shown in Fig.1. © 2017 IEEE.",,"Demonstrations; Interconnection networks (circuit switching); Laptop computers; Learning algorithms; Learning systems; Neurons; Population statistics; Printed circuit boards; Support vector machines; FPGA-based accelerations; Kernel matrices; Network interconnections; Neuromorphic approach; Noise-shaping; Printed circuit boards (PCB); Real time display; Support vector networks; Field programmable gate arrays (FPGA)",2-s2.0-85032677751
"Wang Z., Guo J., Zhang Y., Luo R.","Fault diagnosis for railway track circuit based on wavelet packet power spectrum and ELM",2017,"Proceedings of 2016 11th International Conference on Reliability, Maintainability and Safety: Integrating Big Data, Improving Reliability and Serving Personalization, ICRMS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032827611&doi=10.1109%2fICRMS.2016.8050089&partnerID=40&md5=032d99118c30257291a35bd9d575eee1","For enhancing the troubleshooting efficiency of a track circuit, a fault diagnosis method for the track circuit is proposed in this paper. First, a locomotive signal induced voltage model is established based on the transmission-line theory. Then, cases of the induced voltage envelope signals, when the track circuits are in the normal and fault conditions, respectively, are simulated. Next, a three-layer wavelet packet is adopted to decompose the induced voltage envelope signals and power spectrum analysis for the detail signal is realized. 16 time-domain indices of the β power spectrum including the standard deviation, variance, kurtosis value, and the variable coefficient are used as the failure features. Then, the information fusion of the time domain features is implemented using the principal component analysis (PCA) technology. Finally, the fusion features are input to an extreme learning machine (ELM) model to identify the failures. Case analyses show that the fault diagnosis method proposed in this paper can obtain a high accuracy and provide a scientific basis for the on-site maintenance of the track circuit. © 2016 IEEE.","Extreme learning machine; Fault diagnosis; Power spectrum analysis; Principal component analysis; Track circuit; Wavelet packet decomposition","Big data; Electric network analysis; Failure analysis; Fault detection; Knowledge acquisition; Learning systems; Maintainability; Power spectrum; Principal component analysis; Reliability; Spectrum analysis; Timing circuits; Wavelet analysis; Wavelet decomposition; Extreme learning machine; Fault diagnosis method; Induced voltages; Standard deviation; Time domain features; Track circuit; Variable coefficients; Wavelet Packet Decomposition; Time domain analysis",2-s2.0-85032827611
"Muller L.K., Nair M.V., Indiveri G.","Randomized unregulated step descent for limited precision synaptic elements",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032658161&doi=10.1109%2fISCAS.2017.8050217&partnerID=40&md5=47fb4895df37603d86bc23873eedd8fa","Training neural networks with low-resolution synaptic weights raised much interest recently and inference in neural networks with binary activation and binary weights has been shown to be able to achieve near state-of-the-art performance in a wide range of tasks. However, the current methods for training such networks rely on high-resolution gradients or update probabilities. Low resolution training methods would be useful for neuromorphic architectures that support lower power hardware implementations as well as emerging memory technologies based on memristive devices that do not always support fine-grained state changes. In this paper, we propose a training method, Randomized Unregulated Step Descent (RUSD), as an alternative to gradient descent that uses only a single bit of information about the gradient; we show how it is compatible with low-resolution integer arithmetic platforms and is resilient to some of the prominent non-idealities of memristive memories. We verify the performance of RUSD several standard machine-learning benchmarks. © 2017 IEEE.",,"Bins; Hardware; Learning systems; Memristors; Emerging memory technologies; Gradient descent; Hardware implementations; Integer arithmetic; Neuromorphic Architectures; Standard machines; State-of-the-art performance; Training methods; Benchmarking",2-s2.0-85032658161
"Bolme D., Mikkilineni A., Rose D., Yoginath S., Judy M., Holleman J.","Deep modeling: Circuit characterization using theory based models in a data driven framework",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032684125&doi=10.1109%2fISCAS.2017.8050752&partnerID=40&md5=3d873a3587f8f058b8af6eb7ed945719","Analog computational circuits have been demonstrated to provide substantial improvements in power and speed relative to digital circuits, especially for applications requiring extreme parallelism but only modest precision. Deep machine learning is one such area and stands to benefit greatly from analog and mixed-signal implementations. However, even at modest precisions, offsets and non-linearity can degrade system performance. Furthermore, in all but the simplest systems, it is impossible to directly measure the intermediate outputs of all sub-circuits. The result is that circuit designers are unable to accurately evaluate the non-idealities of computational circuits in-situ and are therefore unable to fully utilize measurement results to improve future designs. In this paper we present a technique to use deep learning frameworks to model physical systems. Recently developed libraries like TensorFlow make it possible to use back propagation to learn parameters in the context of modeling circuit behavior. Offsets and scaling errors can be discovered even for sub-circuits that are deeply embedded in a computational system and not directly observable. The learned parameters can be used to refine simulation methods or to identify appropriate compensation strategies. We demonstrate the framework using a mixed-signal convolution operator as an example circuit. © 2017 IEEE.",,"Backpropagation; Deep learning; Learning systems; Timing circuits; Analog and mixed signals; Circuit behaviors; Circuit designers; Compensation strategy; Computational circuits; Computational system; Convolution operators; Learning frameworks; Computation theory",2-s2.0-85032684125
"Wess M., Manoj P.D.S., Jantsch A.","Neural network based ECG anomaly detection on FPGA and trade-off analysis",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032658055&doi=10.1109%2fISCAS.2017.8050805&partnerID=40&md5=6f744e9917554bc1219c3a3b3ee816a9","This paper presents FPGA-based ECG arrhythmia detection using an Artificial Neural Network (ANN). The objective is to implement a neural network based machine learning algorithm on FPGA to detect anomalies in ECG signals, with a better performance and accuracy, compared to statistical methods. An implementation with Principal Component Analysis (PCA) for feature reduction and a multi-layer perceptron (MLP) for classification, proved superior to other algorithms. For implementation on FPGA, the effects of several parameters and simplification on performance, accuracy and power consumption were studied. Piecewise linear approximation for activation functions and fixed point implementation were effective methods to reduce the amount of needed resources. The resulting neural network with twelve inputs and six neurons in the hidden layer, achieved, in spite of the simplifications, the same overall accuracy as simulations with floating point number representation. An accuracy of 99.82% was achieved on average for the MIT-BIH database. © 2017 IEEE.",,"Digital arithmetic; Economic and social effects; Electrocardiography; Field programmable gate arrays (FPGA); Financial data processing; Learning algorithms; Learning systems; Neural networks; Piecewise linear techniques; Activation functions; ECG arrhythmia detection; Fixed-point implementation; Floating point numbers; Multi layer perceptron; Overall accuracies; Piecewise linear approximations; Trade-off analysis; Principal component analysis",2-s2.0-85032658055
"Zalivaka S.S., Ivaniuk A.A., Chang C.-H.","Low-cost fortification of arbiter PUF against modeling attack",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032663605&doi=10.1109%2fISCAS.2017.8050671&partnerID=40&md5=e2bbd8522b64fd5ac2b4ea5d7b449c43","Arbiter Physical unclonable function (A-PUF) with exponential number of challenges is an ideal candidate to realize lightweight and robust device authentication in Internet of Things applications. Unfortunately, it is particularly difficult to attain highly reliable responses and increase its modeling attack resistance simultaneously. This paper presents an approach to reduce the vulnerability of A-PUF to machine learning attacks without compromising its high reliability and uniqueness. It utilizes a multiple input signature register (MISR) to process the input challenges. Our experiment results show that the accuracy of predicting the responses of a MISR augmented 128-stage arbiter PUF in FPGA implementation by support vector machine and gradient boosting learning algorithms with a training set of 100,000 challenge-response pairs has reduced drastically from 98% to 50%. If design-for-testability is mandatory, the MISR can be reconfigured from an existing built-in logic block observer, making this approach virtually free. Otherwise, the MISR carries a negligible hardware overhead of only 0.4% of the total available resources in an Xilinx ZC706 FPGA chip. © 2017 IEEE.",,"Design for testability; Field programmable gate arrays (FPGA); Learning algorithms; Learning systems; Built in logic block observers; Challenge-response pair; Exponential numbers; FPGA implementations; Gradient boosting; Hardware overheads; High reliability; Multiple input signature registers; Cryptography",2-s2.0-85032663605
"Li Y., Schulze S., Saake G.","Reverse engineering variability from natural language documents: A systematic literature review",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032274023&doi=10.1145%2f3106195.3106207&partnerID=40&md5=abd2abcce09498f039201a967484f969","Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering & machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more e.ort need to be invested for making such approaches applicable in practice. © 2017 ACM.","Feature identification; Natural language documents; Reverse engineering; Software product lines; Systematic literature review; Variability extraction","Computer software; Extraction; Learning algorithms; Learning systems; Reverse engineering; Software design; Comprehensive evaluation; Feature identification; Natural languages; Software Product Line; Software product lines; Systematic literature review; Systematic literature review (SLR); Variability information; Feature extraction",2-s2.0-85032274023
"Szczupak J., Pinto L., Torres G.","Signal processing and climate understanding",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032659036&doi=10.1109%2fISCAS.2017.8050338&partnerID=40&md5=41306d24ef6f60e165f3334ea316e406","Clean energy, as generated by wind and sun, is a must nowadays. It has, however, to be competitive with other sources, offering acceptable costs and production risks. This work faces the usual lack of a long and reliable historic of climato-logical data and consequent uncertainty about expected plant production. Customized machine learning and signal processing techniques are used to map regional satellite measurements into local information, creating a virtual historical series of energy production. Two illustrative cases are presented for eolic and photovoltaic energy, where neural network inputs and operations are described. © 2017 IEEE.",,"Learning systems; Energy productions; Local information; Logical data; Photovoltaic energy; Plant production; Production risks; Satellite measurements; Signal processing technique; Signal processing",2-s2.0-85032659036
"González T., Sol D., Saenz J., Clavijo D., García H.","Urban multisensory laboratory, an approach to model urban space human perception",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032622949&doi=10.5194%2fisprs-archives-XLII-4-W3-29-2017&partnerID=40&md5=6900826228a70f05b3c64942605a711b","An urban sensory lab (USL or LUS an acronym in Spanish) is a new and avant-garde approach for studying and analyzing a city. The construction of this approach allows the development of new methodologies to identify the emotional response of public space users. The laboratory combines qualitative analysis proposed by urbanists and quantitative measures managed by data analysis applications. USL is a new approach to go beyond the borders of urban knowledge. The design thinking strategy allows us to implement methods to understand the results provided by our technique. In this first approach, the interpretation is made by hand. However, our goal is to combine design thinking and machine learning in order to analyze the qualitative and quantitative data automatically. Now, the results are being used by students from the Urbanism and Architecture courses in order to get a better understanding of public spaces in Puebla, Mexico and its interaction with people. © Authors 2017. CC BY 4.0 License.","Data analysis; Design Thinking; Emotional Response; Public space understanding; Qualitative analysis; Urban Knowledge; Urban Space Laboratory","Data handling; Data reduction; Education; Information analysis; Learning systems; Regional planning; Smart city; Space stations; Urban planning; Design thinking; Emotional response; Public space; Qualitative analysis; Urban Knowledge; Urban spaces; Laboratories",2-s2.0-85032622949
"Anantrasirichai N., Daniels K.A.J., Burn J.F., Gilchrist I.D., Bull D.R.","Fixation Prediction and Visual Priority Maps for Biped Locomotion",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030756101&doi=10.1109%2fTCYB.2017.2734946&partnerID=40&md5=546f2b5d65d35720ab34e5ac9135d5f7","This paper presents an analysis of the low-level features and key spatial points used by humans during locomotion over diverse types of terrain. Although, a number of methods for creating saliency maps and task-dependent approaches have been proposed to estimate the areas of an image that attract human attention, none of these can straightforwardly be applied to sequences captured during locomotion, which contain dynamic content derived from a moving viewpoint. We used a novel learning-based method for creating a visual priority map informed by human eye tracking data. Our proposed priority map is created based on two fixation types: first exploiting the observation that humans search for safe foot placement and second that they observe the edges of a path as a guide to safe traversal of the terrain. Texture features and the difference between them, observed at the region around an eye position, are employed within a support vector machine to create a visual priority map for biped locomotion. The results show that our proposed method outperforms the state-of-the-art, particularly for more complex terrains, where achieving smooth locomotion needs more attention on the traversing path. CCBY","Bioinspired; Concrete; eye tracking; Gaze tracking; Legged locomotion; locomotion; priority map; Rocks; salience; Support vector machines; Tracking; Visualization","Biped locomotion; Concretes; Flow visualization; Rocks; Surface discharges; Bioinspired; Eye-tracking; Gaze tracking; Legged locomotion; salience; Support vector machines",2-s2.0-85030756101
"Perez-Pena F., Lenero-Bardallo J.A., Linares-Barranco A., Chicca E.","Towards bioinspired close-loop local motor control: A simulated approach supporting neuromorphic implementations",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032655046&doi=10.1109%2fISCAS.2017.8050808&partnerID=40&md5=d47c67d333a60e01e198841ee0467c15","Despite being well established in robotics, classical motor controllers have several disadvantages: they pose a high computational load, therefore requiring powerful devices, they are not easy to tune and they are not suited for neuroprosthetics. In contrast, bio-inspired controller do not transform the output of the controller therefore no delays are introduced and a smooth response is achieved; they also have a high scalability. Finally, the most important feature of bio-inspired controllers is that they could integrate learning features to make them adaptable to new tasks within the same hardware robotic platform. We present the model and simulation of a spiking neural network for low-level motor control. The proposed neural network acts as a motor controller and produces pulsed signals which can be directly interfaced with commercial DC motors. The simulated network is compatible with neuromorphic VLSI implementation and paves the way to the implementation bio-inspired motor controller which are compact, low power, scalable and compatible with neuroprosthetic. The network presented is inspired by the current knowledge about biological motor control: it comprises alpha motoneuron for driving the motor and spindle populations to provide the feedback and close the loop. The spikes from the motoneuron population are time lengthen to a fixed amount of time and supplied to the simulated motor: Pulse Frequency Modulation (PFM) modulation is used. This paper presents the software simulations using the Brian simulator for a position controller. Our controller is a first step toward a novel bio-inspired motor control approach suitable for robotics as well as neuroprosthetic. © 2017 IEEE.","motoneurons; motor control; neuromorphic engineering; PFM; robotics","Computer software; Controllers; DC motors; Low power electronics; Modulation; Neural networks; Neural prostheses; Neurons; Pulse time modulation; Robotics; Biological motor control; Computational loads; Model and simulation; motoneurons; Motor control; Neuromorphic engineering; Position controller; Spiking neural networks; Electric machine control",2-s2.0-85032655046
"Diefenbach D., Lopez V., Singh K., Maret P.","Core techniques of question answering systems over knowledge bases: a survey",2017,"Knowledge and Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029782329&doi=10.1007%2fs10115-017-1100-y&partnerID=40&md5=f6e61a53637f060a29adedcce755dc2d","The Semantic Web contains an enormous amount of information in the form of knowledge bases (KB). To make this information available, many question answering (QA) systems over KBs were created in the last years. Building a QA system over KBs is difficult because there are many different challenges to be solved. In order to address these challenges, QA systems generally combine techniques from natural language processing, information retrieval, machine learning and Semantic Web. The aim of this survey is to give an overview of the techniques used in current QA systems over KBs. We present the techniques used by the QA systems which were evaluated on a popular series of benchmarks: Question Answering over Linked Data. Techniques that solve the same task are first grouped together and then described. The advantages and disadvantages are discussed for each technique. This allows a direct comparison of similar techniques. Additionally, we point to techniques that are used over WebQuestions and SimpleQuestions, which are two other popular benchmarks for QA systems. © 2017 Springer-Verlag London Ltd.","Knowledge base; QALD; Question answering; Semantic Web; SimpleQuestions; Survey; WebQuestions",,2-s2.0-85029782329
"Zhang H., Zeng L., Wu W., Zhang C.","How good are machine learning clouds for binary classification with good features?: Extended abstract",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032444740&doi=10.1145%2f3127479.3132570&partnerID=40&md5=01ac03f397bac6f30e8693a808a0e39a","In spite of the recent advancement of machine learning research, modern machine learning systems are still far from easy to use, at least from the perspective of business users or even scientists without a computer science background. Recently, there is a trend toward pushing machine learning onto the cloud as a ""service,"" a.k.a. machine learning clouds. By putting a set of machine learning primitives on the cloud, these services significantly raise the level of abstraction for machine learning. For example, with Amazon Machine Learning, users only need to upload the dataset and specify the type of task (classification or regression). The cloud will then train machine learning models without any user intervention.","Machine learning clouds","Artificial intelligence; Classification (of information); Cloud computing; Binary classification; Extended abstracts; Level of abstraction; Machine learning models; Machine learning research; Modern machines; Science background; User intervention; Learning systems",2-s2.0-85032444740
"Jo C., Cho Y., Egger B.","A machine learning approach to live migration modeling",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450247&doi=10.1145%2f3127479.3129262&partnerID=40&md5=35e95d739c5a8eae0e553b4dd94fb75a","Live migration is one of the key technologies to improve data center utilization, power efficiency, and maintenance. Various live migration algorithms have been proposed; each exhibiting distinct characteristics in terms of completion time, amount of data transferred, virtual machine (VM) downtime, and VM performance degradation. To make matters worse, not only the migration algorithm but also the applications running inside the migrated VM affect the different performance metrics. With service-level agreements and operational constraints in place, choosing the optimal live migration technique has so far been an open question. In this work, we propose an adaptive machine learning-based model that is able to predict with high accuracy the key characteristics of live migration in dependence of the migration algorithm and the workload running inside the VM. We discuss the important input parameters for accurately modeling the target metrics, and describe how to profile them with little overhead. Compared to existing work, we are not only able to model all commonly used migration algorithms but also predict important metrics that have not been considered so far such as the performance degradation of the VM. In a comparison with the state-of-the-art, we show that the proposed model outperforms existing work by a factor 2 to 5.","Live migration; Machine learning; Performance modeling; Virtualization","Artificial intelligence; Cloud computing; Outsourcing; Virtualization; Adaptive machine learning; Live migrations; Machine learning approaches; Migration algorithms; Operational constraints; Performance degradation; Performance Model; Service Level Agreements; Learning systems",2-s2.0-85032450247
"Zhang H., Stafman L., Or A., Freedman M.J.","SLAQ: Quality-driven scheduling for distributed machine learning",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032447493&doi=10.1145%2f3127479.3127490&partnerID=40&md5=53d88264c0d350671cb8404f3a20b224","Training machine learning (ML) models with large datasets can incur significant resource contention on shared clusters. This training typically involves many iterations that continually improve the quality of the model. Yet in exploratory settings, better models can be obtained faster by directing resources to jobs with the most potential for improvement. We describe SLAQ, a cluster scheduling system for approximate ML training jobs that aims to maximize the overall job quality. When allocating cluster resources, SLAQ explores the quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so, SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highlytailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average quality improvement of up to 73% and an average delay reduction of up to 44% on a large set of ML training jobs, compared to resource fairness schedulers. © 2017 Association for Computing Machinery.","Approximate computing; Machine learning; Quality; Resource management; Scheduling","Artificial intelligence; Cloud computing; Economic and social effects; Image quality; Iterative methods; Learning systems; Approximate computing; Cluster scheduling; Distributed machine learning; Driven scheduling; Quality improvement; Resource contention; Resource management; Training algorithms; Scheduling",2-s2.0-85032447493
"Syed U., Vassilvitskii S.","SQML: Large-scale in-database machine learning with pure SQL",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032452725&doi=10.1145%2f3127479.3132746&partnerID=40&md5=ed99a5c0f6b3a97f8bc280b9adcf4eaa","Many enterprises have migrated their data from an on-site database to a cloud-based database-as-a-service that handles all database-related administrative tasks while providing a simple SQL interface to the end user. Businesses are also increasingly relying on machine learning to understand their customers and develop new products. Given these converging trends, there is a pressing need for database-as-a-service providers to add support for sophisticated machine learning algorithms to the core functionality of their products.",,"Artificial intelligence; Cloud computing; Database systems; Learning systems; Administrative tasks; Cloud-based; Core functionality; Database as a service; End users; On-machines; Sophisticated machines; Learning algorithms",2-s2.0-85032452725
"Crisci C., Terra R., Pacheco J.P., Ghattas B., Bidegain M., Goyenola G., Lagomarsino J.J., Méndez G., Mazzeo N.","Multi-model approach to predict phytoplankton biomass and composition dynamics in a eutrophic shallow lake governed by extreme meteorological events",2017,"Ecological Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024108099&doi=10.1016%2fj.ecolmodel.2017.06.017&partnerID=40&md5=84d9f91c46f4b29e60ece98194052a1d","A multi-model approach to predict phytoplankton biomass and composition was performed in a eutrophic Uruguayan shallow lake which is the second drinking water source of the country. We combined statistical (spectral analysis and Machine learning techniques) and physically based models to generate, for the first time in this system, a predictive tool of phytoplankton biomass (chlorophyll-a) and composition (morphology-based functional groups). The results, based on a 11-year time series, revealed two alternating phases in the temporal dynamics of phytoplankton biomass. One phase is characterized by high inorganic turbidity and low phytoplankton biomass, and the other by low inorganic turbidity and variable (low and high) phytoplankton biomass. A threshold of turbidity (29 TNU), above which phytoplankton remains with low biomass (<15–20 ug/l) was established. The periods of high turbidity, which in total cover 30% of the time series, start abruptly and are related to external forcing. Meteorological conditions associated with the beginning of these periods were modeled through a regression tree analysis. These conditions consist of moderate to high wind intensities from the SW direction, in some cases combined with high antecedent precipitation or low water level. The results from the physically-based modeling indicated that the long decaying time-scale of turbidity and intermediate resuspension events could explain the prolonged length of the high turbidity periods (∼1.5 years). Random Forests models for the prediction of phytoplankton biomass and composition in periods of low turbidity resulted in a proportion of explained variance and a classification error over a test sample of 0.46 and 0.34 respectively. Turbidity, conductivity, temperature and water level were within the most important model predictors. The development and improvement of this type of modeling is needed to provide management tools to water managers in the current water supply situation. © 2017 Elsevier B.V.","Chlorophyll-a dynamics; Extreme meteorological events; Inorganic turbidity dynamics; Phytoplankton morphology-based functional groups; Predictions; Water quality","Biomass; Chlorophyll; Decision trees; Dynamics; Eutrophication; Forecasting; Lakes; Learning systems; Potable water; Spectrum analysis; Time series; Turbidity; Water; Water levels; Water quality; Water supply; Chlorophyll a; Drinking water sources; Extreme meteorological events; Machine learning techniques; Meteorological condition; Physically based modeling; Physically based models; Phytoplankton biomass; Phytoplankton; chlorophyll a; community composition; community dynamics; ecosystem modeling; eutrophic environment; extreme event; functional group; inorganic matter; lake ecosystem; meteorological hazard; morphology; phytomass; phytoplankton; prediction; statistical analysis; turbidity; water quality; Uruguay",2-s2.0-85024108099
"Wang Z., Gao L., Gu Y., Bao Y., Yu G.","FSP: Towards flexible synchronous parallel framework for expectation-maximization based algorithms on cloud",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436124&doi=10.1145%2f3127479.3128612&partnerID=40&md5=1c1f0dc0f6850b436019590cb6719f76","Myriad of parameter estimation algorithms can be performed by an Expectation-Maximization (EM) approach. Traditional synchronous frameworks can parallelize these EM algorithms on the cloud to accelerate computation while guaranteeing the convergence. However, expensive synchronization costs pose great challenges for efficiency. Asynchronous solutions have been recently designed to bypass high-cost synchronous barriers but at expense of potentially losing convergence guarantee. This paper first proposes a flexible synchronous parallel framework (FSP) that provides the capability of synchronous EM algorithms implementations, as well as significantly reduces the barrier cost. Under FSP, every distributed worker can immediately suspend local computation when necessary, to quickly synchronize with each other. That maximizes the time fast workers spend doing useful work, instead of waiting for slow, straggling workers. We then formally prove the algorithm convergence. Further, we analyze how to automatically identify a proper barrier interval to strike a nice balance between reduced synchronization costs and the convergence speed. Empirical results demonstrate that on a broad spectrum of real-world and synthetic datasets, FSP achieves as much as 3x speedup over the up-to-date synchronous solution. © 2017 Association for Computing Machinery.","Distributed iterative computation; Expectation-maximization; Flexible synchronous parallel; Machine learning; Straggler","Cloud computing; Cost reduction; Costs; Iterative methods; Learning systems; Network function virtualization; Algorithm convergence; Expectation - maximizations; Expectation-maximization approaches; Flexible synchronous parallel; Iterative computation; Parameter estimation algorithm; Straggler; Synchronous framework; Maximum principle",2-s2.0-85032436124
"Cenek M., Franklin M.","An adaptable agent-based model for guiding multi-species Pacific salmon fisheries management within a SES framework",2017,"Ecological Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024492365&doi=10.1016%2fj.ecolmodel.2017.06.024&partnerID=40&md5=4896966c1d2157dfe243fff3e6fb8f22","Informing fishery management decisions using coupled socio-ecological systems (CSES) models requires model construction that captures the systems interactions with high precision. Ecological uncertainty in fishery models is easily reduced using existing scientific literature, but social drivers are often poorly defined or understood. The lack of knowledge about fishermen behavior results in inaccurate models of questionable utility for fishery managers. We designed and constructed a high fidelity agent based model (ABM) using the socio-ecological framework that reduces social system uncertainty by capturing complex behaviors using data-driven bounded rationality and feedback. The resulting generalized ABM of CSES dynamics was instantiated to Pacific salmon fisheries at Kenai river in Upper Cook Inlet, Alaska. The data-driven model construction fuses multiple data-sets for classification of social and ecological fishery regimes into stochastic distributions; the agent behaviors were generalized by evolving parametrized equations using data-driven machine learning; multiple non-trivial metrics on multiple scales verified model's accuracy and predictive capacity. The verified model of CSES dynamics at the Kenai river revealed recent instability in the dipnet fishery coupled dynamics, historic instability in the drift gillnet fishery coupled dynamics due to a compensatory and aggressive fishing strategy, and in the future the model will be used for scenario-based studies to understand the outcomes of alternative management strategies. © 2017 Elsevier B.V.","Agent-based model; Coupled social–ecological system; Decision support tool; Fisheries management; Pacific salmon; Social–ecological system","Autonomous agents; Computational methods; Decision making; Decision support systems; Dynamics; Ecology; Stochastic models; Stochastic systems; Agent-based model; Decision support tools; Ecological systems; Fisheries management; Pacific salmon; Fisheries; adaptive management; decision making; decision support system; fishery management; gillnet; machine learning; parameterization; precision; salmonid; salmonid fishery; stochasticity; uncertainty analysis; Alaska; Cook Inlet; Kenai River; United States; Oncorhynchus",2-s2.0-85024492365
"Li C., Andersen D.G., Fu Q., Elnikety S., He Y.","Workload analysis and caching strategies for search advertising systems",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032446953&doi=10.1145%2f3127479.3129255&partnerID=40&md5=c44db10dce44a6edd8ab07dfb64798dd","Search advertising depends on accurate predictions of user behavior and interest, accomplished today using complex and computationally expensive machine learning algorithms that estimate the potential revenue gain of thousands of candidate advertisements per search query. The accuracy of this estimation is important for revenue, but the cost of these computations represents a substantial expense, e.g., 10% to 30% of the total gross revenue. Caching the results of previous computations is a potential path to reducing this expense, but traditional domain-agnostic and revenue-agnostic approaches to do so result in substantial revenue loss. This paper presents three domain-specific caching mechanisms that successfully optimize for both factors. Simulations on a trace from the Bing advertising system show that a traditional cache can reduce cost by up to 27.7% but has negative revenue impact as bad as -14.1%. On the other hand, the proposed mechanisms can reduce cost by up to 20.6% while capping revenue impact between -1.3% and 0%. Based on Microsoft's earnings release for FY16 Q4, the traditional cache would reduce the net profit of Bing Ads by $84.9 to $166.1 million in the quarter, while our proposed cache could increase the net profit by $11.1 to $71.5 million. © 2017 Association for Computing Machinery.","Caching; Sponsored search; Workload analysis","Behavioral research; Bins; Cloud computing; Costs; Learning algorithms; Learning systems; Marketing; Profitability; Publishing; Accurate prediction; Advertising systems; Caching; Caching mechanism; Expensive machines; Potential revenue; Sponsored searches; Workload analysis; Cost reduction",2-s2.0-85032446953
"Lugones D., Aroca J.A., Jin Y., Sala A., Hilt V.","AidOps: A data-driven provisioning of high-availability services in cloud",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032457903&doi=10.1145%2f3127479.3129250&partnerID=40&md5=1c860ce0307297d6d39e888125f306e6","The virtualization of services with high-availability requirements calls to revisit traditional operation and provisioning processes. Providers are realizing services in software on virtual machines instead of using dedicated appliances to dynamically adjust service capacity to changing demands. Cloud orchestration systems control the number of service instances deployed to make sure each service has enough capacity to meet incoming workloads. However, determining the suitable build-out of a service is challenging as it takes time to install new instances and excessive re-configurations (i.e. scale in/out) can lead to decreased stability. In this paper we present AidOps, a cloud orchestration system that leverages machine learning and domain-specific knowledge to predict the traffic demand, optimizing service performance and cost. AidOps does not require a conservative provisioning of services to cover for the worst-case demand and significantly reduces operational costs while still fulfilling service quality expectations. We have evaluated our framework with real traffic using an enterprise application and a communication service in a private cloud. Our results show up to 4X improvement in service performance indicators compared to existing orchestration systems. AidOps achieves up to 99.985% availability levels while reducing operational costs at least by 20%. © 2017 Association for Computing Machinery.","Cloud orchestration; High-availability; Workload forecasting","Cloud computing; Costs; Learning systems; Communication service; Domain-specific knowledge; Enterprise applications; High availability; Number of services; Re-configurations; Service capacity; Service performance; Cost reduction",2-s2.0-85032457903
"Karpathiotakis M., Floratou A., Özcan F., Ailamaki A.","No data left behind: Real-time insights from a complex data ecosystem",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032438714&doi=10.1145%2f3127479.3131208&partnerID=40&md5=0f79cadc0e519f2883afead9746bae3b","The typical enterprise data architecture consists of several actively updated data sources (e.g., NoSQL systems, data warehouses), and a central data lake such as HDFS, in which all the data is periodically loaded through ETL processes. To simplify query processing, state-of-the-art data analysis approaches solely operate on top of the local, historical data in the data lake, and ignore the fresh tail end of data that resides in the original remote sources. However, as many business operations depend on real-time analytics, this approach is no longer viable. The alternative is hand-crafting the analysis task to explicitly consider the characteristics of the various data sources and identify optimization opportunities, rendering the overall analysis non-declarative and convoluted. Based on our experiences operating in data lake environments, we design System-PV, a real-time analytics system that masks the complexity of dealing with multiple data sources while offering minimal response times. System-PV extends Spark with a sophisticated data virtualization module that supports multiple applications - from SQL queries to machine learning. The module features a location-aware compiler that considers source complexity, and a two-phase optimizer that produces and refines the query plans, not only for SQL queries but for all other types of analysis as well. The experiments show that System-PV is often faster than Spark by more than an order of magnitude. In addition, the experiments show that the approach of accessing both the historical and the remote fresh data is viable, as it performs comparably to solely operating on top of the local, historical data. © 2017 Association for Computing Machinery.","Data federation; Data virtualization; Distributed database systems; ETL; Real-time analytics; SQL-on-hadoop","Cloud computing; Data warehouses; Distributed database systems; Lakes; Learning systems; Query languages; Query processing; Virtual reality; Virtualization; Business operation; Data federation; Data virtualization; Multiple applications; Multiple data sources; Real-time analytics; Source complexity; SQL-on-hadoop; Real time systems",2-s2.0-85032438714
"Fan C., Huang Y.","Identification of novel potential scaffold for class I HDACs inhibition: An in-silico protocol based on virtual screening, molecular dynamics, mathematical analysis and machine learning",2017,"Biochemical and Biophysical Research Communications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026308515&doi=10.1016%2fj.bbrc.2017.07.051&partnerID=40&md5=19d10f51e046c6e792442f99f119356e","Histone deacetylases (HDACs) family has been widely reported as an important class of enzyme targets for cancer therapy. Much effort has been made in discovery of novel scaffolds for HDACs inhibition besides existing hydroxamic acids, cyclic peptides, benzamides, and short-chain fatty acids. Herein we set up an in-silico protocol which not only could detect potential Zn2+ chelation bonds but also still adopted non-bonded model to be effective in discovery of Class I HDACs inhibitors, with little human's subjective visual judgment involved. We applied the protocol to screening of Chembridge database and selected out 7 scaffolds, 3 with probability of more than 99%. Biological assay results demonstrated that two of them exhibited HDAC-inhibitory activity and are thus considerable for structure modification to further improve their bio-activity. © 2017","Histone deacetylases inhibitors; Molecular dynamics; Virtual screening","histone deacetylase; histone deacetylase inhibitor; molecular scaffold; zinc ion; chelating agent; histone deacetylase; histone deacetylase inhibitor; protein binding; zinc; Article; chelation; chemical bond; chemical modification; computer model; controlled study; enzyme inhibition; factual database; human; hydrophobicity; machine learning; mathematical analysis; molecular docking; molecular dynamics; molecular model; pharmacophore; priority journal; probability; screening; simulation; virtual screening; binding site; chemical model; chemistry; computer interface; computer simulation; drug development; enzyme activation; machine learning; preclinical study; procedures; protein analysis; ultrastructure; Binding Sites; Chelating Agents; Computer Simulation; Drug Discovery; Drug Evaluation, Preclinical; Enzyme Activation; Histone Deacetylase Inhibitors; Histone Deacetylases; Machine Learning; Models, Chemical; Molecular Docking Simulation; Protein Binding; Protein Interaction Mapping; User-Computer Interface; Zinc",2-s2.0-85026308515
"Lian C., Zeng Z., Su Y., Yao W.","Landslide displacement prediction based on error correction and ensemble of online sequential extreme learning machine",2017,"Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032272618&doi=10.13245%2fj.hust.170910&partnerID=40&md5=9390f0d053c7ef826c0f923b5d8b3648","A novel prediction approach based on error correction and ensemble of online sequential extreme learning machine (EOS-ELM) was proposed in this paper for the landslide displacement prediction. Following the proposed approach, landslide displacement time series was divided into the trend component displacement and the periodic component displacement to express the relations between landslide displacement and different affecting factors. An online sequential extreme learning machine (OS-ELM) algorithm was adopted to forecast the trend component and periodic component landslide displacements, respectively. The ensemble learning method was used to improve the generalization ability of OS-ELM. For further improving the forecasting accuracy, an error correction method was proposed. This method utilized error series to build a predictor which was used to correct the final outcomes. The effectiveness of the proposed method was evaluated by using real data from Baishuihe landslide in the Three Gorges Reservoir of China. © 2017, Editorial Board of Journal of Huazhong University of Science and Technology. All right reserved.","Ensemble learning; Error correction; Extreme learning machine; Landslide displacement prediction; Time series prediction","Error correction; Errors; Forecasting; Knowledge acquisition; Landslides; Learning systems; Time series; Displacement prediction; Displacement time series; Ensemble learning; Extreme learning machine; Generalization ability; Online sequential extreme learning machine; Three gorges reservoir; Time series prediction; E-learning",2-s2.0-85032272618
"Jang S., Tan G., Toh K., Teoh A.B.J.","Online Heterogeneous Face Recognition Based on Total-Error-Rate Minimization",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030671556&doi=10.1109%2fTSMC.2017.2724761&partnerID=40&md5=542ead77e7a50c9f14afd19bd9152f1b","In this paper, we propose a recursive learning formulation for online heterogeneous face recognition (HFR). The main task is to compare between images which are acquired from different sensing spectrums for identity recognition. Using an extreme learning machine, the proposed recursive formulation seeks a direct optimization to the classification error goal where the solution converges exactly to the batch mode solution. Due to the nonlinear nature of the classification error objective function, formulation of a recursive solution that converges is an important and nontrivial task. Based on this recursive formulation, an online HFR system is designed. The system is evaluated using two challenging heterogeneous face databases with images captured under visible, near infrared and infrared spectrums. The proposed system shows promising performance which is comparable with that of competing state-of-the-arts. IEEE","Extreme learning machine (ELM); heterogeneous face recognition (HFR); online learning; total-error-rate (TER) minimization","E-learning; Errors; Infrared devices; Knowledge acquisition; Learning systems; Classification errors; Direct optimization; Extreme learning machine; heterogeneous face recognition (HFR); Identity recognition; Online learning; Recursive formulation; Total error rates; Face recognition",2-s2.0-85030671556
"Vijayakumar V., Case M., Shirinpour S., He B.","Quantifying and Characterizing Tonic Thermal Pain across Subjects from EEG Data using Random Forest Models",2017,"IEEE Transactions on Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030667350&doi=10.1109%2fTBME.2017.2756870&partnerID=40&md5=53bec5498bf7e7ff7f84235fcf700e1a","<formula><tex>${Objective}$</tex></formula>: Effective pain assessment and management strategies are needed to better manage pain. In addition to self-report, an objective pain assessment system can provide a more complete picture of the neurophysiological basis for pain. In this study, a robust and accurate machine learning approach is developed to quantify tonic thermal pain across healthy subjects into a maximum of ten distinct classes. <formula><tex>${Methods}$</tex></formula>: A random forest model was trained to predict pain scores using time-frequency wavelet representations of independent components obtained from electroencephalography (EEG) data, and the relative importance of each frequency band to pain quantification is assessed. <formula><tex>${Results}$</tex></formula>: The mean classification accuracy for predicting pain on an independent test subject for a range of 1-10 is 89.45%, highest among existing state of the art quantification algorithms for EEG. The gamma band is the most important to both inter-subject and intra-subject classification accuracy. <formula><tex>${Conclusion}$</tex></formula>: The robustness and generalizability of the classifier is demonstrated. <formula><tex>${Significance}$</tex></formula>: Our results demonstrate the potential of this tool to be used clinically to help improve chronic pain treatment, and establish spectral biomarkers for future pain-related studies using EEG. IEEE","Bars; Brain modeling; cingulate cortex; Data models; Electroencephalography; Electroencephalography (EEG); gamma oscillations; Heating systems; machine learning; Pain; pain quantification; Time-frequency analysis","Artificial intelligence; Bars (metal); Brain models; Data structures; Decision trees; Electrophysiology; Frequency bands; Health; Learning systems; Cingulate cortex; Gamma oscillations; Heating system; Pain; pain quantification; Time frequency analysis; Electroencephalography",2-s2.0-85030667350
"Wang H., Zheng Z., Cai Y., Sun X.","Scene-Adaptive Vehicle Detection Algorithm based on a Composite Deep Structure",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030667577&doi=10.1109%2fACCESS.2017.2756081&partnerID=40&md5=9e07854cbbdea57e5eb675c727161ca8","Existing machine-learning-based vehicle detection algorithms for intelligent vehicles have an obvious disadvantage in that the detection effect decreases dramatically when the distribution of training samples and the scene target samples do not match. To address this issue, a scene-adaptive vehicle detection algorithm based on a composite deep structure is proposed in this paper. Inspired by the Bagging (Bootstrap aggregating) mechanism, multiple relatively independent source samples are firstly used to build multiple classifiers and then voting is used to generate target training samples with confidence scores. The automatic feature extraction ability of DCNN (Deep Convolutional Neural Network) is then used to perform source-target scene feature similarity calculations with a deep auto-encoder in order to design a composite deep-structure-based scene-adaptive classifier and its training method. Experiments on the KITTI dataset and a dataset captured by our group demonstrate that the proposed method performs better than existing machine-learning-based vehicle detection methods. Additionally, compared with existing scene-adaptive object detection methods, our method improves the detection rate by an average of approximately 3&#x0025;. OAPA","Composite deep structure; Deep convolutional neural network; Image recognition; Scene adaptive; Vehicle detection","Artificial intelligence; Convolution; Image recognition; Learning systems; Neural networks; Object detection; Object recognition; Sampling; Signal detection; Speech recognition; Statistical methods; Vehicles; Automatic feature extraction; Bootstrap aggregating; Convolutional neural network; Deep structure; Multiple classifiers; Object detection method; Scene adaptive; Vehicle detection; Deep neural networks",2-s2.0-85030667577
"Zhang C., Yao L., Song S., Wen X., Zhao X., Long Z.","Euler Elastica regularized Logistic Regression for whole-brain decoding of fMRI data",2017,"IEEE Transactions on Biomedical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030714382&doi=10.1109%2fTBME.2017.2756665&partnerID=40&md5=11b231510dd36b629484027a95095acd","Objective: Multivariate pattern analysis (MVPA) methods have been widely applied to functional magnetic resonance imaging (fMRI) data to decode brain states. Due to the &#x201C;high features, low samples&#x201D; in fMRI data, machine learning methods have been widely regularized using various regularizations to avoid overfitting. Both total variation (TV) using the gradients of images and Euler&#x0027;s elastica (EE) using the gradient and the curvature of images are the two popular regulations with spatial structures. In contrast to TV, EE regulation is able to overcome the disadvantage of TV regulation that favored piecewise constant images over piecewise smooth images. In this study, we introduced EE to fMRI-based decoding for the first time and proposed the EE regularized multinomial logistic regression (EELR) algorithm for multi-class classification. Methods: We performed experimental tests on both simulated and real fMRI data to investigate the feasibility and robustness of EELR. The performance of EELR was compared with sparse logistic regression (SLR) and TV regularized LR (TVLR). Results: The results showed that EELR was more robustness to noises and showed significantly higher classification performance than TVLR and SLR. Moreover, the forward models and weights patterns revealed that EELR detected larger brain regions that were discriminative to each task and activated by each task than TVLR. Conclusion: The results suggest that EELR not only performs well in brain decoding but also reveals meaningful discriminative and activation patterns. Significance: This study demonstrated that EELR showed promising potential in brain decoding and discriminative/activation pattern detection. IEEE","Brain modeling; Classification algorithms; Decoding; decoding; Euler&#x0027;s Elastica; fMRI; Logistic Regression; Logistics; Three-dimensional displays; TV","Brain; Brain models; Decoding; Learning systems; Logistics; Magnetic resonance imaging; Multivariant analysis; Pattern recognition; Regression analysis; Television; Three dimensional computer graphics; Classification algorithm; Elastica; fMRI; Logistic regressions; Three-dimensional display; Functional neuroimaging; brain region; classification algorithm; experimental test; feasibility study; functional magnetic resonance imaging; noise; simulation",2-s2.0-85030714382
"Subasi O., Di S., Balaprakash P., Unsal O., Labarta J., Cristal A., Krishnamoorthy S., Cappello F.","MACORD: Online adaptive machine learning framework for silent error detection",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032615470&doi=10.1109%2fCLUSTER.2017.128&partnerID=40&md5=bc9d274d6e8b16980907b7cfd796b224","Future high-performance computing (HPC) systems with ever-increasing resource capacity (such as compute cores, memory and storage) may significantly increase the risks on reliability. Silent data corruptions (SDCs) or silent errors are among the major sources that corrupt HPC execution results. Unlike fail-stop errors, SDCs can be harmful and dangerous in that they cannot be detected by hardware. To remedy this, we propose an online MAchine-learning-based silent data CORruption Detection framework (abbreviated as MACORD) for detecting SDCs in HPC applications. In our study, we comprehensively investigate the prediction ability of a multitude of machine-learning algorithms and enable the detector to automatically select the best-fit algorithms at runtime to adapt to the data dynamics. Because it takes only spatial features (i.e., neighboring data values for each data point in the current time step) into the training data, our learning framework exhibits low memory overhead (less than 1%). Experiments based on real-world scientific applications/benchmarks show that our framework can elevate the detection sensitivity (i.e., recall) up to 99%. Meanwhile the false positive rate is limited to 0.1% in most cases, which is one order of magnitude improvement compared with the latest state-of-The-Art spatial technique. © 2017 IEEE.",,"Artificial intelligence; Cesium compounds; Cluster computing; Computer architecture; Digital storage; Errors; Learning algorithms; Learning systems; Network function virtualization; Adaptive machine learning; Detection framework; Detection sensitivity; False positive rates; High performance computing systems; Learning frameworks; Scientific applications; Silent data corruptions; E-learning",2-s2.0-85032615470
"Choi J.Y., Logan J., Wolf M., Ostrouchov G., Kurc T., Liu Q., Podhorszki N., Klasky S., Romanus M., Sun Q., Parashar M., Churchill R.M., Chang C.","TGE: Machine Learning Based Task Graph Embedding for Large-Scale Topology Mapping",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032621616&doi=10.1109%2fCLUSTER.2017.67&partnerID=40&md5=dd9314405e365e8b4b9cf431b55fc3cd","Task mapping is an important problem in parallel and distributed computing. The goal in task mapping is to find an optimal layout of the processes of an application (or a task) onto a given network topology. We target this problem in the context of staging applications. A staging application consists of two or more parallel applications (also referred to as staging tasks) which run concurrently and exchange data over the course of computation. Task mapping becomes a more challenging problem in staging applications, because not only data is exchanged between the staging tasks, but also the processes of a staging task may exchange data with each other. We propose a novel method, called Task Graph Embedding (TGE), that harnesses the observable graph structures of parallel applications and network topologies. TGE employs a machine learning based algorithm to find the best representation of a graph, called an embedding, onto a space in which the task-To-processor mapping problem can be solved. We evaluate and demonstrate the effectiveness of TGE experimentally with the communication patterns extracted from runs of XGC, a large-scale fusion simulation code, on Titan. © 2017 IEEE.",,"Artificial intelligence; Cluster computing; Computer architecture; Distributed computer systems; Embedded systems; Learning systems; Mapping; Communication pattern; Fusion simulation; Graph structures; Large-scale topology; Network topology; Parallel and distributed computing; Parallel application; Representation of a graph; Topology",2-s2.0-85032621616
"Montgomery L., Damian D.","What do Support Analysts Know about Their Customers? On the Study and Prediction of Support Ticket Escalations in Large Software Organizations",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032831281&doi=10.1109%2fRE.2017.61&partnerID=40&md5=36fdf31218838a01a812842c83bdaba5","Understanding and keeping the customer happy is a central tenet of requirements engineering. Strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed. For the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests. Whenever insufficient attention is given to support issues, however, their escalation to management is time-consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets. Our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets. In a field study at our large industrial partner, IBM, we used a design science methodology to characterize the support process and data available to IBM analysts in managing escalations. Through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a Machine Learning model to predict support ticket escalations. We trained and evaluated our Machine Learning model on over 2.5 million support tickets and 10,000 escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation. Further on-site evaluations, through a prototype tool we developed to implement our Machine Learning techniques in practice, showed more efficient weekly support-ticket-management meetings. The features we developed in the Support Ticket Model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction. © 2017 IEEE.","Customer relationship management; customer support ticket; escalation prediction; machine learning","Artificial intelligence; Forecasting; Iterative methods; Learning systems; Public relations; Requirements engineering; Societies and institutions; Customer relationship management; Customer support; Design and evaluations; Design science methodologies; Large organizations; Machine learning models; Machine learning techniques; Software organization; Sales",2-s2.0-85032831281
"Klinkenberg J., Terboven C., Lankes S., Muller M.S.","Data mining-based analysis of HPC center operations",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032637445&doi=10.1109%2fCLUSTER.2017.23&partnerID=40&md5=5dbc2b7ee2eabde0788d44f920b5805f","Size and complexity of contemporary High Performance Computing (HPC) systems increases permanently. While the reliability of a single component and compute node is high, the huge amount of components comprising these systems results in the fact that defects happen regularly. This drives the need to manage failure situations. Common issues are component failures or node soft lock-ups that typically lead to crashes of the user jobs that are scheduled on the affected node, and may cause undesired downtime. One approach to mitigate the impact of such problems is to predict node failures with a sufficient lead time in order to take proactive measures. However, accurate prediction is a challenging task.The literature describes several approaches that focus on gathering and analyzing system event logs in order to create prediction models. In this paper, we present a different approach by using descriptive statistics and supervised machine learning to create a prediction model from monitoring data. Our approach is based on the assumption, that features of a certain time frame before a critical event (i. e., a failure or soft lock-up) can serve as an indicator. Consequently, our model is trained with monitoring data from critical and healthy time frames. The evaluation with standard monitoring data collected from the HPC systems at RWTH Aachen University shows that our classifier is able to locate potentially failing nodes with a 10-fold cross precision of 98% and recall of 91 %. © 2017 IEEE.","Big data; Failure prediction; Large scale; Machine learning; Monitoring","Artificial intelligence; Cluster computing; Computer architecture; Data mining; Digital storage; Forecasting; Learning systems; Locks (fasteners); Monitoring; Silicon compounds; Supervised learning; Accurate prediction; Component failures; Descriptive statistics; Failure prediction; High performance computing systems; Large scale; Proactive measures; Supervised machine learning; Big data",2-s2.0-85032637445
"Yang J., Liu M., Lu J., Miao Y., Hossain M.A., Alhamid M.F.","Botanical Internet of Things: Toward Smart Indoor Farming by Connecting People, Plant, Data and Clouds",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029738420&doi=10.1007%2fs11036-017-0930-x&partnerID=40&md5=df588df736d972fa4a13906f1cf81c3a","With rapid development of a new generation of communication technology, sensor technology as well as big data technology, the application scenes of Internet of Things (IOT) based on these technologies increase constantly in extensive fields, for instance, there are great contributions of Internet of Things in fields such as smart home, intelligent transportation, intelligent healthcare, intelligent monitoring as well as intelligent agriculture. At present, as for intelligent agriculture, the main focus is monitoring agricultural environment with IOT and M2M technology. In the era of population explosion, agricultural resources such as farmland become more and more insufficient, a indoor intelligent agricultural IOT system is designed and implemented by the author in order to attack this conundrum. And the system directs an new trend for agricultural development. With the capability of parallel extension, the system can connect to large-scale indoor farms gradually thus to make these farms combining with each other organically. Finally, information mining shall be achieved based on large amount of sensing data by utilizing the big data technology and machine learning algorithms, and those derived information shall be adopted as critical reference to support indoor agricultural activities. © 2017 Springer Science+Business Media, LLC","Big data; Cloud computing; Internet of things; Machine learning; Smart green house; Smart indoor farming","Agricultural machinery; Agriculture; Artificial intelligence; Automation; Cloud computing; Data mining; Engineering education; Green computing; Intelligent buildings; Internet of things; Learning algorithms; Learning systems; Agricultural activities; Agricultural development; Agricultural environments; Agricultural resources; Communication technologies; Intelligent transportation; Internet of Things (IOT); Smart indoor farming; Big data",2-s2.0-85029738420
"Papanikolaou Y., Tsoumakas G., Laliotis M., Markantonatos N., Vlahavas I.","Large-scale online semantic indexing of biomedical articles via an ensemble of multi-label classification models",2017,"Journal of Biomedical Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029832145&doi=10.1186%2fs13326-017-0150-0&partnerID=40&md5=a014eb849113807c2769dda211ff1c39","Background: In this paper we present the approach that we employed to deal with large scale multi-label semantic indexing of biomedical papers. This work was mainly implemented within the context of the BioASQ challenge (2013-2017), a challenge concerned with biomedical semantic indexing and question answering. Methods: Our main contribution is a MUlti-Label Ensemble method (MULE) that incorporates a McNemar statistical significance test in order to validate the combination of the constituent machine learning algorithms. Some secondary contributions include a study on the temporal aspects of the BioASQ corpus (observations apply also to the BioASQ's super-set, the PubMed articles collection) and the proper parametrization of the algorithms used to deal with this challenging classification task. Results: The ensemble method that we developed is compared to other approaches in experimental scenarios with subsets of the BioASQ corpus giving positive results. In our participation in the BioASQ challenge we obtained the first place in 2013 and the second place in the four following years, steadily outperforming MTI, the indexing system of the National Library of Medicine (NLM). Conclusions: The results of our experimental comparisons, suggest that employing a statistical significance test to validate the ensemble method's choices, is the optimal approach for ensembling multi-label classifiers, especially in contexts with many rare labels. © 2017 The Author(s).","BioASQ; Machine learning; Multi-label ensemble; Multi-label learning; Semantic indexing; Supervised learning",,2-s2.0-85029832145
"Mead N., Shull F., Spears J., Heibl S., Weber S., Cleland-Huang J.","Crowd Sourcing the Creation of Personae Non Gratae for Requirements-Phase Threat Modeling",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032787990&doi=10.1109%2fRE.2017.63&partnerID=40&md5=26973b26033497bc441334548ddddb9a","Security threats should be identified in the early phases of a project so that design solutions can be explored and mitigating requirements specified. In this paper, we present a crowd-sourcing approach for creating Personae non Gratae (PnGs), which model attack goals and techniques of unwanted, potentially malicious users. We present a proof of concept study that takes a diverse collection of potentially redundant PnGs and merges them into a single set. Our approach combines machine learning techniques and visualization. It is illustrated and evaluated using a collection of PnGs collected from undergraduate students for a drone-based rescue scenario. Lessons learned from the proof of concept study are discussed and lay the foundations for future work. © 2017 IEEE.","Personae non gratae; Security Requirements; Threat Modeling","Learning systems; Requirements engineering; Students; Machine learning techniques; Personae non gratae; Proof of concept; Requirements phase; Security requirements; Security threats; Threat modeling; Undergraduate students; Education",2-s2.0-85032787990
"Fan Y., Rich P., Allcock W.E., Papka M.E., Lan Z.","Trade-Off between Prediction Accuracy and Underestimation Rate in Job Runtime Estimates",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032626953&doi=10.1109%2fCLUSTER.2017.11&partnerID=40&md5=02949236cf212569c2e999af581525c2","Job runtime estimates provided by users are widely acknowledged to be overestimated and runtime overestimation can greatly degrade job scheduling performance. Previous studies focus on improving accuracy of job runtime estimates by reducing runtime overestimation, but fail to address the underestimation problem (i.e., the underestimation of job runtimes). Using an underestimated runtime is catastrophic to a job as the job will be killed by the scheduler before completion. We argue that both the improvement of runtime accuracy and the reduction of underestimation rate are equally important. To address this problem, we propose an online runtime adjustment framework called TRIP. TRIP explores the data censoring capability of the Tobit model to improve prediction accuracy while keeping a low underestimation rate of job runtimes. TRIP can be used as a plugin to job scheduler for improving job runtime estimates and hence boosting job scheduling performance. Preliminary results demonstrate that TRIP is capable of achieving high accuracy of 80% and low underestimation rate of 5%. This is significant as compared to other well-known machine learning methods such as SVM, Random Forest, and Last-2 which result in a high underestimation rate (20%-50%). Our experiments further quantify the amount of scheduling performance gain achieved by the use of TRIP. © 2017 IEEE.","Blue Gene systems; Job scheduling; Runtime prediction","Computer architecture; Decision trees; Economic and social effects; Forecasting; Learning systems; Scheduling; Blue Gene; Job scheduling; Machine learning methods; Prediction accuracy; Runtime adjustment; Runtime estimates; Runtimes; Scheduling performance; Cluster computing",2-s2.0-85032626953
"Shin Y.-S., Jang Y.-W., Kang M.-H., Chang J.-W.","A Novel Hybrid Transactional Memory Based on Abort Prediction and Adaptive Retry Policy",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032636599&doi=10.1109%2fCLUSTER.2017.41&partnerID=40&md5=ab9e0064a6ddb268262100cf2dbbc090","This paper proposes a novel hybrid transactional memory scheme based on both abort prediction and an adaptive retry policy. First, the proposed scheme can predict not only conflicts between transactions running concurrently, but also the capacity and other aborts of transactions by collecting the information of previously executed transactions. Second, the proposed scheme can provide an adaptive retry policy based on machine learning algorithms, according to the characteristic of a given workload. Finally, through our experimental performance analysis using STAMP, the proposed scheme shows about 20% better performance than Hybrid NOrec, a hybrid version of the efficient NOrec STM. © 2017 IEEE.","Concurrency control; HTM; Multi-core in-memory database; STM; Transactional memory","Cluster computing; Computer architecture; Forecasting; Learning algorithms; Learning systems; Storage allocation (computer); Experimental performance analysis; Memory database; On-machines; Policy-based; Transactional memory; Concurrency control",2-s2.0-85032636599
"Lang X., Li P., Li Y., Ren H.","Leak location of pipeline with multibranch based on a cyber-physical system",2017,"Information (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030457405&doi=10.3390%2finfo8040113&partnerID=40&md5=5bce911d5feb4d94b2f7758f1103d449","Data cannot be shared and leakage cannot be located simultaneously among multiple pipeline leak detection systems. Based on cyber-physical system (CPS) architecture, the method for locating leakage for pipelines with multibranch is proposed. The singular point of pressure signals at the ends of pipeline with multibranch is analyzed by wavelet packet analysis, so that the time feature samples could be established. Then, the Fischer-Burmeister function is introduced into the learning process of the twin support vector machine (TWSVM) in order to avoid the matrix inversion calculation, and the samples are input into the improved twin support vector machine (ITWSVM) to distinguish the pipeline leak location. The simulation results show that the proposed method is more effective than the back propagation (BP) neural networks, the radial basis function (RBF) neural networks, and the Lagrange twin support vector machine. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Cyber-physical system; Leak location; Pipeline leakage; Signal singular point; Twin support vector machine; Wavelet packet analysis","Backpropagation; Cyber Physical System; Leak detection; Location; Pipelines; Radial basis function networks; Support vector machines; Vectors; Wavelet analysis; Leak locations; Pipeline leakage; Singular points; Twin support vector machines; Wavelet Packet Analysis; Embedded systems",2-s2.0-85030457405
"Chen X., Benson J., Estrada T.","Keybin: Key-Based Binning for Distributed Clustering",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032642374&doi=10.1109%2fCLUSTER.2017.96&partnerID=40&md5=c2a166d1f93ebdbed20d89860da1959c","Traditional machine learning algorithms often require computations on centralized data, but modern datasets are collected and stored in a distributed way. In addition to the cost of moving data to centralized locations, increasing concerns about privacy and security warrant distributed approaches. We propose keybin, a distributed key-based binning clustering algorithm for high-dimensional spaces. keybin locally generates a spatial key for each data point across all dimensions without needing knowledge of other data. Then, it performs a conceptual Map-Reduce procedure in the index space to form a global clustering assignment. We present an implementation and a case study on the capabilities and limitations of this approach, showing that this algorithm can learn a global clustering structure with limited communication and can scale with the dimensionality and size of data sets. © 2017 IEEE.","Big Data; Distributed Clustering; Map-Reduce; Privacy Preserving","Big data; Bins; Cluster computing; Computer architecture; Data privacy; Learning algorithms; Learning systems; Distributed approaches; Distributed clustering; Global clustering; High dimensional spaces; Limited communication; Map-reduce; Privacy and security; Privacy preserving; Clustering algorithms",2-s2.0-85032642374
"Lei J., Jin T., Hao J., Li F.","Short-term load forecasting with clustering–regression model in distributed cluster",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029772927&doi=10.1007%2fs10586-017-1198-4&partnerID=40&md5=6ffe5f58587acbcc00f062a3fff8b52c","This paper tackles a new challenge in power big data: how to improve the precision of short-term load forecasting with large-scale data set. The proposed load forecasting method is based on Spark platform and “clustering–regression” model, which is implemented by Apache Spark machine learning library (MLlib). Proposed scheme firstly clustering the users with different electrical attributes and then obtains the “load characteristic curve of each cluster”, which represents the features of various types of users and is considered as the properties of a regional total load. Furthermore, the “clustering–regression” model is used to forecast the power load of the certain region. Extensive experiments show that the proposed scheme can predict reasonably the short-term power load and has excellent robustness. Comparing with the single-alone model, the proposed method has a higher efficiency in dealing with large-scale data set and can be effectively applied to the power load forecasting. © 2017 Springer Science+Business Media, LLC","Clustering–regression model; Distributed cluster; Load characteristic curve; Short-term load forecasting","Electric power plant loads; Forecasting; Learning systems; Regression analysis; Distributed clusters; Higher efficiency; Large scale data sets; Load characteristics; Load forecasting; Power load forecasting; Regression model; Short term load forecasting; Big data",2-s2.0-85029772927
"Guzman E., Ibrahim M., Glinz M.","A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032809108&doi=10.1109%2fRE.2017.88&partnerID=40&md5=4ef8fae0ed769ffbef63d7bf78a21625","Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements. © 2017 IEEE.","requirements elicitation; software evolution; text mining; Twitter; user feedback","Data mining; Learning systems; Requirements engineering; Social networking (online); Requirements elicitation; Software Evolution; Text mining; Twitter; User feedback; Application programs",2-s2.0-85032809108
"Liu X., Jiao L., Zhao J., Zhao J., Zhang D., Liu F., Yang S., Tang X.","Deep Multiple Instance Learning-Based Spatial-Spectral Classification for PAN and MS Imagery",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030682333&doi=10.1109%2fTGRS.2017.2750220&partnerID=40&md5=800951bc45b5264234e191c354e800ce","Panchromatic (PAN) and multispectral (MS) ima-gery classification is one of the hottest topics in the field of remote sensing. In recent years, deep learning techniques have been widely applied in many areas of image processing. In this paper, an end-to-end learning framework based on deep multiple instance learning (DMIL) is proposed for MS and PAN images' classification using the joint spectral and spatial information based on feature fusion. There are two instances in the proposed framework: one instance is used to capture the spatial information of PAN and the other is used to describe the spectral information of MS. The features obtained by the two instances are concatenated directly, which can be treated as simple fusion features. To fully fuse the spatial-spectral information for further classification, the simple fusion features are fed into a fusion network with three fully connected layers to learn the high-level fusion features. Classification experiments carried out on four different airborne MS and PAN images indicate that the classifier provides feasible and efficient solution. It demonstrates that DMIL performs better than using a convolutional neural network and a stacked autoencoder network separately. In addition, this paper shows that the DMIL model can learn and fuse spectral and spatial information effectively, and has huge potential for MS and PAN imagery classification. IEEE","Convolution; Deep learning; Feature extraction; feature fusion; Fuses; image classification; joint features; Machine learning; multiple instance learning; Neural networks; Spatial resolution","Convolution; Deep learning; Electric fuses; Feature extraction; Image classification; Image fusion; Image processing; Learning systems; Neural networks; Remote sensing; Convolutional neural network; Feature fusion; Learning techniques; Multiple instance learning; Spatial informations; Spatial resolution; Spectral classification; Spectral information; Classification (of information)",2-s2.0-85030682333
"Zhou S., Wang J., Shi R., Hou Q., Gong Y., Zheng N.","Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification",2017,"IEEE Transactions on Multimedia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708013&doi=10.1109%2fTMM.2017.2755983&partnerID=40&md5=0a5230f5cab126656ec7fc122d463b87","Person re-identification (Re-ID) aims at matching images of the same person across disjoint camera views, which is a challenging problem in multimedia analysis, multimedia editing and content-based media retrieval communities. The major challenge lies in how to preserve similarity of the same person across video footages with large appearance variations, while discriminating different individuals. To address this problem, conventional methods usually consider the pairwise similarity between persons by only measuring the point to point (P2P) distance. In this paper, we propose to use deep learning technique to model a novel set to set (S2S) distance, in which the underline objective focuses on preserving the compactness of intra-class samples for each camera view, while maximizing the margin between the intra-class set and inter-class set. The S2S distance metric is consisted of three terms, namely the class-identity term, the relative distance term and the regularization term. The class-identity term keeps the intra-class samples within each camera view gathering together, the relative distance term maximizes the distance between the intra-class class set and inter-class set across different camera views, and the regularization term smoothness the parameters of deep convolutional neural network (CNN). As a result, the final learned deep model can effectively find out the matched target to the probe object among various candidates in the video gallery by learning discriminative and stable feature representations. Using the CUHK01, CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over the state-of-the-art approaches. IEEE","Cameras; Deep Learning; Feature extraction; Learning systems; Machine learning; Measurement; Metric Learning; Neural networks; Person Re-identification; Robustness; Set to Set Similarity Comparison","Cameras; Deep learning; Feature extraction; Learning systems; Measurements; Neural networks; Robustness (control systems); Comparative evaluations; Convolutional neural network; Feature representation; Large-margin learning; Metric learning; Person re identifications; Set similarity; State-of-the-art approach; Deep neural networks",2-s2.0-85030708013
"Antoniades A., Spyrou L., Martin-Lopez D., Valentin A., Alarcon G., Sanei S., Took C.C.","Detection of Interictal Discharges with Convolutional Neural Networks Using Discrete Ordered Multichannel Intracranial EEG",2017,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030673317&doi=10.1109%2fTNSRE.2017.2755770&partnerID=40&md5=3c4b6c06affb2b74963099ec687e0bc9","Detection algorithms for electroencephalography (EEG) data, especially in the field of interictal epileptiform discharge (IED) detection, have traditionally employed handcrafted features which utilised specific characteristics of neural responses. Although these algorithms achieve high accuracy, mere detection of an IED holds little clinical significance. In this work, we consider deep learning for epileptic subjects to accommodate automatic feature generation from intracranial EEG data, while also providing clinical insight. Convolutional neural networks are trained in a subject independent fashion to demonstrate how meaningful features are automatically learned in a hierarchical process. We illustrate how the convolved filters in the deepest layers provide insight towards the different types of IEDs within the group, as confirmed by our expert clinicians. The morphology of the IEDs found in filters can help evaluate the treatment of a patient. To improve the learning of the deep model, moderately different score classes are utilised as opposed to binary IED and non-IED labels. The resulting model achieves state of the art classification performance and is also invariant to time differences between the IEDs. This study suggests that deep learning is suitable for automatic feature generation from intracranial EEG data, while also providing insight into the data. IEEE","Biological neural networks; Brain modeling; Convolution; Convolutional neural networks; Electrodes; Electroencephalography; epilepsy detection; Feature extraction; intracranial EEG; Machine learning; multi score class learning","Brain models; Convolution; Deep learning; Electrodes; Electrophysiology; Feature extraction; Learning systems; Neural networks; Patient treatment; Biological neural networks; Convolutional neural network; Epilepsy detection; Intracranial EEG; multi score class learning; Electroencephalography",2-s2.0-85030673317
"Kanjo E., Kuss D.J., Ang C.S.","NotiMind: Utilizing Responses to Smart Phone Notifications as Affective sensors",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030689454&doi=10.1109%2fACCESS.2017.2755661&partnerID=40&md5=c6fb71c00d77f0a67938722f2a8b7512","Today&#x2019;s mobile phone users are faced with large numbers of notifications on social media, ranging from new followers on Twitter and emails to messages received from WhatsApp and Facebook. These digital alerts continuously disrupt activities through instant calls for attention. This paper examines closely the way everyday users interact with notifications and their impact on users&#x2019; emotion. Fifty users were recruited to download our application NotiMind and use it over a five-week period. Users&#x2019; phones collected thousands of social and system notifications along with affect data collected via self-reported PANAS tests three times a day. Results showed a noticeable correlation between positive affective measures and keyboard activities. When large numbers of Post and Remove notifications occur, a corresponding increase in negative affective measures is detected. Our predictive model has achieved a good accuracy level using three different &#x201C;in the wild&#x201D; classifiers (F-measure 74-78&#x0025; within-subject model, 72-76&#x0025; global model). Our findings show that it is possible to automatically predict when people are experiencing positive, neutral or negative affective states based on interactions with notifications. We also show how our findings open the door to a wide range of applications in relation to emotion awareness on social and mobile communication. OAPA","Affective Computing; Electronic mail; Facebook; Machine Learning; Mobile communication; Mobile computing; Mobile Computing; Mobile handsets; Mobile Sensing; Mobile Social media","Electronic mail; Learning systems; Mobile computing; Smartphones; Social networking (online); Telephone sets; User interfaces; Affective Computing; Facebook; Mobile communications; Mobile handsets; Mobile sensing; Mobile social medias; Mobile telecommunication systems",2-s2.0-85030689454
"Hainc N., Federau C., Stieltjes B., Blatow M., Bink A., Stippich C.","The bright, artificial intelligence-augmented future of neuroimaging reading",2017,"Frontiers in Neurology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029717324&doi=10.3389%2ffneur.2017.00489&partnerID=40&md5=08cf2322fe404567ccb9e79d7a9a78b2","Radiologists are among the first physicians to be directly affected by advances in computer technology. Computers are already capable of analyzing medical imaging data, and with decades worth of digital information available for training, will an artificial intelligence (AI) one day signal the end of the human radiologist? With the ever increasing work load combined with the looming doctor shortage, radiologists will be pushed far beyond their current estimated 3 s allotted time-of-analysis per image; an AI with super-human capabilities might seem like a logical replacement. We feel, however, that AI will lead to an augmentation rather than a replacement of the radiologist. The AI will be relied upon to handle the tedious, time-consuming tasks of detecting and segmenting outliers while possibly generating new, unanticipated results that can then be used as sources of medical discovery. This will affect not only radiologists but all physicians and also researchers dealing with medical imaging. Therefore, we must embrace future technology and collaborate interdisciplinary to spearhead the next revolution in medicine. © 2017 Hainc, Federau, Stieltjes, Blatow, Bink and Stippich.","Artificial intelligence; Machine learning; Magnetic resonance imaging; Neuroimaging; Neuroradiology; Radiology","Article; artificial intelligence; computer system; coping behavior; decision making; diagnostic accuracy; factual database; futurology; history of medicine; human; image processing; medical technology; neuroimaging; nuclear magnetic resonance imaging; patient care; physician; radiologist; x-ray computed tomography",2-s2.0-85029717324
"Lee S., Lee Y., Kim J.","Automated Evaluation of Upper-limb Motor Function Impairment using Fugl-Meyer Assessment",2017,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030666116&doi=10.1109%2fTNSRE.2017.2755667&partnerID=40&md5=a2b50fd6a91d0a7e944f606ad8daf9e8","The Fugl-Meyer assessment (FMA) is the most popular instrument for evaluating upper extremity motor function in stroke patients. However, it is a labor-intensive and timeconsuming method. This paper proposes a novel automated FMA system to overcome these limitations of the FMA. For automation, we used Kinect v2 and force sensing resistor sensors owing to their convenient installation as compared with body-worn sensors. Based on the linguistic guideline of the FMA, a rule-based binary logic classification algorithm was developed to assign FMA scores using the extracted features obtained from the sensors. The algorithm is appropriate for clinical use because it is not based on machine learning, which requires additional learning processes with a large amount of clinical data. The proposed system was able to automate 79% of the FMA tests because of optimized sensor selection and the classification algorithm. In clinical trials conducted with nine stroke patients, the proposed system exhibited high scoring accuracy (92&#x0025;) and time efficiency (85&#x0025; reduction in clinicians&#x2019; required time). IEEE","automated upper-limb assessment; Elbow; Fugl-Meyer assessment; Read only memory; rule-based binary logic classification; Sensor systems; Shoulder; Stroke; Thumb; Wrist","Automation; Bins; Classification (of information); Computer circuits; Function evaluation; Learning algorithms; Learning systems; ROM; Shoulders (road); Binary logic; Elbow; Fugl-meyer assessments; Sensor systems; Stroke; Thumb; Upper limbs; Wrist; Wearable sensors",2-s2.0-85030666116
"Balamurugan V., Saravanan R.","Enhanced intrusion detection and prevention system on cloud environment using hybrid classification and OTS generation",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029704782&doi=10.1007%2fs10586-017-1187-7&partnerID=40&md5=172c4c2e2399e88e8515aa2f0f28b892","Cloud environment is an assembly of resources for furnishing on-demand services to cloud customers. Here access to cloud environment is via internet services in which data stored on cloud environment are easier to both internal and external intruders. To detect intruders, various intrusion detection systems and authentication systems was proposed in earlier researches which are primarily ineffective. Many existing researchers were concentrated on machine learning approaches for detecting intrusions using fuzzy clustering, artificial neural network, support vector machine, fuzzy with neural network and etc., which are not furnishing predominant results based on detection rate and false negative rates. Our proposed system directed on intrusion detection system and it uses cloudlet controller, trust authority and virtual machine management in cloud environment. We propose two novel algorithms such as (i) packet scrutinization algorithm which examines the packets from the users and (ii) hybrid classification model called “NK-RNN” which is a combination of normalized K-means clustering algorithm with recurrent neural network. For preventing the user from intruders, we propose a one time signature for cloud user in order to access the data on cloud environment. Our proposed classifier effectively detects the intruders which are experimentally proved by comparing with existing classification models. Thus our proposed results are expressed by packet loss ratio, average packet delay, throughput, detection rate, false positive rate and false negative rate. © 2017 Springer Science+Business Media, LLC","Cloud computing; Cloudlets; DDOS; Flood attacks; IDS; Queue modeling; RNN classifier; U2R attacks; Zero day attacks and R2L attacks","Cloud computing; Clustering algorithms; Computer crime; Learning systems; Mercury (metal); Network security; Neural networks; Packet networks; Recurrent neural networks; Cloudlets; DDOS; Flood attacks; Queue models; U2R attacks; Zero day attack; Intrusion detection",2-s2.0-85029704782
[No author name available],"Journal of Physics: Conference Series",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030154868&partnerID=40&md5=4c362927971f536d0bc819ae48e7f31d","The proceedings contain 19 papers. The topics discussed include: an integrated study of surface roughness in EDM process using regression analysis and GSO algorithm; machining parameters optimization using hybrid firefly algorithm and particle swarm optimization; towards an enhanced aspect-based contradiction detection approach for online review content; an element search ant colony technique for solving virtual machine placement problem; association rule-based predictive model for machine failure in industrial internet of things; replacing missing values using trustworthy data values from web data sources; selection input output by restriction using DEA models based on a fuzzy Delphi approach and expert information; towards an enhancement of organizational information security through threat factor profiling (TFP) model; appropriation of social media for fostering effective tacit knowledge sharing: developing conceptual model; the impacts of demographic variables on technological and contextual challenges of e-learning implementation; the awareness and challenges of cloud computing adoption on tertiary education in Malaysia; similarity measure for molecular structure: a brief review; and handling a small dataset problem in prediction model by employ artificial data generation approach: a review.",,,2-s2.0-85030154868
"Chen G., Li Y., Sun G., Zhang Y.","Application of deep networks to oil spill detection using polarimetric synthetic aperture radar images",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029809914&doi=10.3390%2fapp7100968&partnerID=40&md5=2191640ba3a9bb4789fa4299d7a24eb3","Polarimetric synthetic aperture radar (SAR) remote sensing provides an outstanding tool in oil spill detection and classification, for its advantages in distinguishing mineral oil and biogenic lookalikes. Various features can be extracted from polarimetric SAR data. The large number and correlated nature of polarimetric SAR features make the selection and optimization of these features impact on the performance of oil spill classification algorithms. In this paper, deep learning algorithms such as the stacked autoencoder (SAE) and deep belief network (DBN) are applied to optimize the polarimetric feature sets and reduce the feature dimension through layer-wise unsupervised pre-training. An experiment was conducted on RADARSAT-2 quad-polarimetric SAR image acquired during the Norwegian oil-on-water exercise of 2011, in which verified mineral, emulsions, and biogenic slicks were analyzed. The results show that oil spill classification achieved by deep networks outperformed both support vector machine (SVM) and traditional artificial neural networks (ANN) with similar parameter settings, especially when the number of training data samples is limited. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Autoencoder; Deep belief network; Oil spill; Polarimetric synthetic aperture radar (SAR); Remote sensing",,2-s2.0-85029809914
"Bonchanoski M., Zdravkova K.","Machine learning-based approach to automatic POS tagging of macedonian language",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032495633&doi=10.1145%2f3136273.3136275&partnerID=40&md5=8da910715c3c5409f6947068dc89e1b4","This paper presents the research that has contributed to the creation of an automatic part-of-speech (POS) tagger of Macedonian, a Slavic language that has a rich morphology, but limited language resources and contributions towards establishing of Natural Language Processing (NLP) tools. The created system automatically tags only the part-of-speech category for each word, without giving the full annotation. For the purposes of this research, the existing large online lexicon was combined together with a self-created crowd-sourcing system intended for manual disambiguation of POS tags. The joint technique resulted in a POS tagged corpus that was used during the training and testing phase. Four different models were produced, built using: TnT tagger, averaged perceptron, cyclic dependency network and guided learning framework for bidirectional sequence classification and the performance of all the models was exhaustively compared. The final accuracy that has been achieved is 96.37%, reaching a result which is comparable to more researched languages. © 2017 Association for Computing Machinery.","Macedonian language; Machine learning approach; Part-of-speech tagging","Artificial intelligence; Learning algorithms; Learning systems; Natural language processing systems; Syntactics; Averaged perceptron; Cyclic dependencies; Learning frameworks; Macedonian language; Machine learning approaches; Part of speech tagging; Sequence classification; Training and testing; Computational linguistics",2-s2.0-85032495633
"Sun W., Wang C., Zhang C.","Factor analysis and forecasting of CO2 emissions in Hebei, using extreme learning machine based on particle swarm optimization",2017,"Journal of Cleaner Production",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024108566&doi=10.1016%2fj.jclepro.2017.06.016&partnerID=40&md5=cc207e05ebc7890d83cdac651da499b1","In the prevailing low-carbon economy, China is under enormous pressure to control CO2 emissions, therefore, of great significance is the study to analyze what influential factors mainly contribute to emissions, so as to forecast emissions accurately and harness the growth from the source. In this paper, basing on 22 influencing factors identified by bivariate correlation analysis, factor analysis is then adopted to extract the latent factors which essentially affect emissions and 8 special factors transformed by scoring coefficients are acquired. Extreme learning machine (ELM) whose input weights and bias threshold were optimized by particle swarm optimization (PSO), hereafter referred as PSO-ELM, is established to predict CO2 emissions and testify the availability of the factor analysis. Case studies reveal that the factor analysis which generates 8 factors as input can highly improve prediction accuracy. And the simulation results demonstrate that the built model PSO-ELM outperforms the compared ELM and back propagation neural network in forecasting CO2 emissions. Eventually, the analysis made in this study can provide valuable policy implications for Hebei's CO2 emissions reduction and strategic low-carbon development. © 2017 Elsevier Ltd","CO2 emissions; Extreme learning machine; Factor analysis; Particle swarm optimization algorithm","Carbon dioxide; Education; Emission control; Forecasting; Knowledge acquisition; Learning systems; Multivariant analysis; Neural networks; Optimization; Particle swarm optimization (PSO); Public policy; Back propagation neural networks; Bivariate correlations; Extreme learning machine; Influential factors; Low-carbon development; Particle swarm optimization algorithm; Policy implications; Prediction accuracy; Factor analysis",2-s2.0-85024108566
"Silitonga A.S., Hassan M.H., Ong H.C., Kusumo F.","Analysis of the performance, emission and combustion characteristics of a turbocharged diesel engine fuelled with Jatropha curcas biodiesel-diesel blends using kernel-based extreme learning machine",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029602552&doi=10.1007%2fs11356-017-0141-9&partnerID=40&md5=73b05161c692246082c02fe2bd72606e","The purpose of this study is to investigate the performance, emission and combustion characteristics of a four-cylinder common-rail turbocharged diesel engine fuelled with Jatropha curcas biodiesel-diesel blends. A kernel-based extreme learning machine (KELM) model is developed in this study using MATLAB software in order to predict the performance, combustion and emission characteristics of the engine. To acquire the data for training and testing the KELM model, the engine speed was selected as the input parameter, whereas the performance, exhaust emissions and combustion characteristics were chosen as the output parameters of the KELM model. The performance, emissions and combustion characteristics predicted by the KELM model were validated by comparing the predicted data with the experimental data. The results show that the coefficient of determination of the parameters is within a range of 0.9805–0.9991 for both the KELM model and the experimental data. The mean absolute percentage error is within a range of 0.1259–2.3838. This study shows that KELM modelling is a useful technique in biodiesel production since it facilitates scientists and researchers to predict the performance, exhaust emissions and combustion characteristics of internal combustion engines with high accuracy. © 2017 Springer-Verlag GmbH Germany","Combustion; Engine performance; Exhaust emissions; Jatropha curcas biodiesel; Kernel-based extreme learning machine; Turbocharged diesel engine",,2-s2.0-85029602552
"Lee W., Huang J., Chang H., Lee K., Lai C.","Predicting Drug Side Effects Using Data Analytics and the Integration of Multiple Data Sources",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030642587&doi=10.1109%2fACCESS.2017.2755045&partnerID=40&md5=e8f62b23f5f9cc5b11f93e42e279a12a","The development of automated approaches employing computational methods using data from publicly available drugs datasets for the prediction of drug side effects has been proposed. This work presents the use of a hybrid machine learning approach to construct side effect classifiers using an appropriate set of data features. The presented approach utilizes the perspective of data analytics to investigate the effect of drug distribution in the feature space, categorize side effects into several intervals, adopt suitable strategies for each interval, and construct data models accordingly. To verify the applicability of the presented method in side effect prediction, a series of experiments were conducted. The results showed that this approach was able to take into account the characteristics of different types of side effects, thereby achieve better predictive performance. Moreover, different feature selection schemes were coupled with the modeling methods to examine the corresponding effects. Additionally, analyses were performed to investigate the task difficulty in terms of data distance and similarity. Examples of visualized networks of associations between drugs and side effects are also discussed to further evaluate the results. OAPA","data analytics; Drug side effect; feature selection; machine learning; predictive modeling","Artificial intelligence; Classification (of information); Feature extraction; Forecasting; Learning systems; Predictive analytics; Automated approach; Data analytics; Drug distribution; Drug side effects; Hybrid machine learning; Multiple data sources; Predictive modeling; Predictive performance; Drug interactions",2-s2.0-85030642587
"Patel G.K., Hahne J.M., Castellini C., Farina D., Dosen S.","Context-dependent adaptation improves robustness of myoelectric control for upper-limb prostheses",2017,"Journal of Neural Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029836801&doi=10.1088%2f1741-2552%2faa7e82&partnerID=40&md5=7ee4c8da2dbf7b013aab73703799bf2f","Objective. Dexterous upper-limb prostheses are available today to restore grasping, but an effective and reliable feed-forward control is still missing. The aim of this work was to improve the robustness and reliability of myoelectric control by using context information from sensors embedded within the prosthesis. Approach. We developed a context-driven myoelectric control scheme (cxMYO) that incorporates the inference of context information from proprioception (inertial measurement unit) and exteroception (force and grip aperture) sensors to modulate the outputs of myoelectric control. Further, a realistic evaluation of the cxMYO was performed online in able-bodied subjects using three functional tasks, during which the cxMYO was compared to a purely machine-learning-based myoelectric control (MYO). Main results. The results demonstrated that utilizing context information decreased the number of unwanted commands, improving the performance (success rate and dropped objects) in all three functional tasks. Specifically, the median number of objects dropped per round with cxMYO was zero in all three tasks and a significant increase in the number of successful transfers was seen in two out of three functional tasks. Additionally, the subjects reported better user experience. Significance. This is the first online evaluation of a method integrating information from multiple on-board prosthesis sensors to modulate the output of a machine-learning-based myoelectric controller. The proposed scheme is general and presents a simple, non-invasive and cost-effective approach for improving the robustness of myoelectric control. © 2017 IOP Publishing Ltd.","context-driven control; myoelectric control; upper-limb prosthesis","Artificial intelligence; Artificial limbs; Cost effectiveness; Learning systems; Prosthetics; Robust control; Robustness (control systems); Semantics; Units of measurement; Context information; Cost-effective approach; Inertial measurement unit; Integrating information; Myoelectric control; On-line evaluation; Realistic evaluations; Upper limb prosthesis; Myoelectrically controlled prosthetics; adaptation; adult; arm prosthesis; Article; controlled study; human; human experiment; information; machine learning; male; myoelectric control; normal human; online monitoring; priority journal; proprioception; reliability; task performance",2-s2.0-85029836801
"Boselli R., Cesarini M., Marrara S., Mercorio F., Mezzanzanica M., Pasi G., Viviani M.","WoLMIS: a labor market intelligence system for classifying web job vacancies",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029603583&doi=10.1007%2fs10844-017-0488-x&partnerID=40&md5=5523d9c7f15a8f170b8532f024337a89","In the last decades, an increasing number of employers and job seekers have been relying on Web resources to get in touch and to find a job. If appropriately retrieved and analyzed, the huge number of job vacancies available today on on-line job portals can provide detailed and valuable information about the Web Labor Market dynamics and trends. In particular, this information can be useful to all actors, public and private, who play a role in the European Labor Market. This paper presents WoLMIS, a system aimed at collecting and automatically classifying multilingual Web job vacancies with respect to a standard taxonomy of occupations. The proposed system has been developed for the Cedefop European agency, which supports the development of European Vocational Education and Training (VET) policies and contributes to their implementation. In particular, WoLMIS allows analysts and Labor Market specialists to make sense of Labor Market dynamics and trends of several countries in Europe, by overcoming linguistic boundaries across national borders. A detailed experimental evaluation analysis is also provided for a set of about 2 million job vacancies, collected from a set of UK and Irish Web job sites from June to September 2015. © 2017 Springer Science+Business Media, LLC","Information systems; Knowledge discovery; Labor market intelligence; Machine learning; Text classification","Commerce; Data mining; Education; Employment; Information systems; Intelligent systems; Learning systems; Text processing; European agency; European labor markets; Experimental evaluation; Job seekers; Labor markets; Text classification; Vocational education and training; Web resources; Classification (of information)",2-s2.0-85029603583
"Nakamura R., Uno A., Kumagai M., Morishita S., Takeda H.","Hypomethylated domain-enriched DNA motifs prepattern the accessible nucleosome organization in teleosts",2017,"Epigenetics and Chromatin",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029706486&doi=10.1186%2fs13072-017-0152-2&partnerID=40&md5=dd61c8a34748cd7849a279fc30b39e48","Background: Gene promoters in vertebrate genomes show distinct chromatin features such as stably positioned nucleosome array and DNA hypomethylation. The nucleosomes are known to have certain sequence preferences, and the prediction of nucleosome positioning from DNA sequence has been successful in some organisms such as yeast. However, at gene promoters where nucleosomes are much more stably positioned than in other regions, the sequence-based model has failed to work well, and sequence-independent mechanisms have been proposed. Results: Using DNase I-seq in medaka embryos, we demonstrated that hypomethylated domains (HMDs) specifically possess accessible nucleosome organization with longer linkers, and we reassessed the DNA sequence preference for nucleosome positioning in these specific regions. Remarkably, we found with a supervised machine learning algorithm, k-mer SVM, that nucleosome positioning in HMDs is accurately predictable from DNA sequence alone. Specific short sequences (6-mers) that contribute to the prediction are specifically enriched in HMDs and distribute periodically with approximately 200-bp intervals which prepattern the position of accessible linkers. Surprisingly, the sequence preference of the nucleosome and linker in HMDs is opposite from that reported previously. Furthermore, the periodicity of specific motifs at hypomethylated promoters was conserved in zebrafish. Conclusion: This study reveals strong link between nucleosome positioning and DNA sequence at vertebrate promoters, and we propose hypomethylated DNA-specific regulation of nucleosome positioning. © 2017 The Author(s).","DNA methylation; DNA sequence; Nucleosome positioning; Vertebrate","animal tissue; Article; chromatin; DNA methylation; DNA sequence; embryo; nonhuman; nucleosome; Oryzias; priority journal; promoter region; supervised machine learning; teleost; vertebrate; zebra fish",2-s2.0-85029706486
"Adhikari N., Amin S.A., Saha A., Jha T.","Exploring in house glutamate inhibitors of matrix metalloproteinase-2 through validated robust chemico-biological quantitative approaches",2017,"Structural Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029586412&doi=10.1007%2fs11224-017-1028-6&partnerID=40&md5=da553ad399c462879fcefbda1f68036f","Matrix metalloproteinase-2 (MMP-2) is established as one of the most important metalloenzymes for targeting cancer. However, homologous MMP-9 of the gelatinase family is implicated as an antitarget of cancer. Therefore, it is an important and challenging task to achieve MMP-2 selectivity over MMP-9. In this article, robust validated chemico-biological quantitative approaches were conducted on a series of in house glutamate-based selective MMP-2 inhibitors over MMP-9 for further refinement of our MMP-2 inhibitor designing approach. The two-dimensional quantitative structure-activity relationship (2D-QSAR) study suggested that arylsulfonamide moiety was better than arylcarboxamide function, which in turn, supported by the hologram QSAR (HQSAR), 3D-QSAR comparative molecular field analysis (CoMFA), and comparative molecular similarity analysis (CoMSIA) studies. Regarding the MMP-2 selectivity, glutamines were better than isoglutamines as evidenced by the quantitative activity-activity relationship (QAAR) and molecular docking studies. Favorable hydrophobic and steric features of aryl function directed towards the S1′ pocket were also well attributed. Naphthyl and p-bromophenoxyphenyl moieties in place of biphenyl function were found to be unfavorable for MMP-2 inhibition and selectivity over MMP-9. Linear or cyclic aliphatic group directed towards the S2′ pocket was favorable, whereas branching was unfavorable for MMP-2 inhibition and selectivity. The importance of biphenyl and 3,5-bistrifluoromethylbenzyl functions directed towards the S1′ and S2′ pockets, respectively, was well attributed for potent MMP-2 inhibition and selectivity over MMP-9. © 2017 Springer Science+Business Media, LLC","CoMFA; CoMSIA; Glutamate; Machine learning; MMP-2; Molecular docking; QAAR; QSAR; Selectivity",,2-s2.0-85029586412
"Küstner T., Liebgott A., Mauch L., Martirosian P., Bamberg F., Nikolaou K., Yang B., Schick F., Gatidis S.","Automated reference-free detection of motion artifacts in magnetic resonance images",2017,"Magnetic Resonance Materials in Physics, Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029592992&doi=10.1007%2fs10334-017-0650-z&partnerID=40&md5=984afb0ee1cfc2eb1173a8633b083b2b","Objectives: Our objectives were to provide an automated method for spatially resolved detection and quantification of motion artifacts in MR images of the head and abdomen as well as a quality control of the trained architecture. Materials and methods: T1-weighted MR images of the head and the upper abdomen were acquired in 16 healthy volunteers under rest and under motion. Images were divided into overlapping patches of different sizes achieving spatial separation. Using these patches as input data, a convolutional neural network (CNN) was trained to derive probability maps for the presence of motion artifacts. A deep visualization offers a human-interpretable quality control of the trained CNN. Results were visually assessed on probability maps and as classification accuracy on a per-patch, per-slice and per-volunteer basis. Results: On visual assessment, a clear difference of probability maps was observed between data sets with and without motion. The overall accuracy of motion detection on a per-patch/per-volunteer basis reached 97%/100% in the head and 75%/100% in the abdomen, respectively. Conclusion: Automated detection of motion artifacts in MRI is feasible with good accuracy in the head and abdomen. The proposed method provides quantification and localization of artifacts as well as a visualization of the learned content. It may be extended to other anatomic areas and used for quality assurance of MR images. © 2017 ESMRMB","Artifacts; Machine learning; Neural networks; Quality assurance",,2-s2.0-85029592992
"Cullen A., Harte N.","A longitudinal database of Irish political speech with annotations of speaker ability",2017,"Language Resources and Evaluation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029604440&doi=10.1007%2fs10579-017-9401-z&partnerID=40&md5=7f0bdcadd1b85c2b262476a36a9fe777","This paper presents the Irish Political Speech Database, an English-language database collected from Irish political recordings. The database is collected with automated indexing and content retrieval in mind, and thus is gathered from real-world recordings (such as television interviews and election rallies) which represent the nature and quality of recordings which will be encountered in practical applications. The database is labelled for six speaker attributes: boring; charismatic; enthusiastic; inspiring; likeable; and persuasive. Each of these traits is linked to the perceived ability or appeal of the speaker, and as such are relevant to a range of content retrieval and speech analysis tasks. The six base attributes are combined to form a metric of Overall Speaker Appeal. A set of baseline experiments is presented, which demonstrate the potential of this database for affective computing studies. Classification accuracies of up to 76% are achieved, with little feature or system optimisation. © 2017 Springer Science+Business Media B.V.","Affective computing; Charisma; Computational paralinguistics; Machine learning; Political speech; Speaker ability",,2-s2.0-85029604440
"Zhang L., Wang G., Romero D., Giannakis G.B.","Randomized Block Frank-Wolfe for Convergent Large-Scale Learning",2017,"IEEE Transactions on Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030623725&doi=10.1109%2fTSP.2017.2755597&partnerID=40&md5=68f32440964f6333e665c0ae5408dacb","Owing to their low-complexity iterations, Frank-Wolfe (FW) solvers are well suited for various large-scale learning tasks. When block-separable constraints are also present, randomized FW has been shown to further reduce complexity by updating only a fraction of coordinate blocks per iteration. In this context, the present work develops feasibility-ensuring step sizes, and provably convergent randomized block Frank-Wolfe (RB-FW) solvers that are flexible in selecting the number of blocks to update per iteration. Convergence rates of RBFW are established through computational bounds on a primal sub-optimality measure, and on the duality gap. Different from existing convergence analysis, which only applies to a step-size sequence that does not generally lead to feasible iterates, the analysis here includes two classes of step-size sequences that not only guarantee feasibility of the iterates, but also enhance flexibility in choosing decay rates. The novel convergence results are markedly broadened to encompass also nonconvex objectives, and further assert that RB-FW with exact line-search reaches a stationary point at rate O(1/\sqrt{t}). Performance of RB-FW with different step sizes and number of blocks is demonstrated in two applications, namely charging of electrical vehicles and structural support vector machines. Simulated tests demonstrate the impressive performance improvement of RB-FW relative to existing randomized single-block FW methods. IEEE","block coordinate; Conditional gradient descent; nonconvex optimization; parallel optimization","Decay (organic); Optimization; block coordinate; Conditional gradient; Convergence analysis; Electrical vehicles; Exact line searches; Large-scale learning; Nonconvex optimization; Parallel optimization; Iterative methods",2-s2.0-85030623725
"Banavar G., Basseda R., Rinaldi M.","The new era of AI will revolutionize our wellness",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032483010&doi=10.1145%2f3136273.3136622&partnerID=40&md5=9d7f86540279da7d94d556e7439227d5","The resurgence of AI is changing every aspect of our lives. Many new kinds of data are becoming available in every field, making possible new insights and approaches that were unthinkable a few years ago. In this paper, we provide a broad overview of the new capabilities of AI platforms that is making this revolution possible, and delve more deeply into one specific AI application for wellness we're building at Viome, Inc. This AI system helps us understand the biological ecosystem inside each of us by creating a high-resolution model of each individual's microbiome and metabolome. It then applies knowledge-based reasoning and machine learning to modern medical science to produce the best wellness plan for each individual. © 2017 Copyright held by the owner/author(s).","Learning Technologies; Logic Programming; Screencasts","Knowledge based systems; Learning systems; AI applications; Biological ecosystem; High-resolution models; Knowledge-based reasoning; Learning technology; Medical science; Metabolomes; Screencasts; Logic programming",2-s2.0-85032483010
"Liu D., Chen W., Lee K., Chavarriaga R., Bouri M., Pei Z., Millán J.D.R.","Brain-actuated gait trainer with visual and proprioceptive feedback",2017,"Journal of Neural Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029836175&doi=10.1088%2f1741-2552%2faa7df9&partnerID=40&md5=a92f09c1a0e828a4d0bef5340a34dc5d","Objective. Brain-machine interfaces (BMIs) have been proposed in closed-loop applications for neuromodulation and neurorehabilitation. This study describes the impact of different feedback modalities on the performance of an EEG-based BMI that decodes motor imagery (MI) of leg flexion and extension. Approach. We executed experiments in a lower-limb gait trainer (the legoPress) where nine able-bodied subjects participated in three consecutive sessions based on a crossover design. A random forest classifier was trained from the offline session and tested online with visual and proprioceptive feedback, respectively. Post-hoc classification was conducted to assess the impact of feedback modalities and learning effect (an improvement over time) on the simulated trial-based performance. Finally, we performed feature analysis to investigate the discriminant power and brain pattern modulations across the subjects. Main results. (i) For real-time classification, the average accuracy was % and % for the two online sessions. The results were significantly higher than chance level, demonstrating the feasibility to distinguish between MI of leg extension and flexion. (ii) For post-hoc classification, the performance with proprioceptive feedback (%) was significantly better than with visual feedback (%), while there was no significant learning effect. (iii) We reported individual discriminate features and brain patterns associated to each feedback modality, which exhibited differences between the two modalities although no general conclusion can be drawn. Significance. The study reported a closed-loop brain-controlled gait trainer, as a proof of concept for neurorehabilitation devices. We reported the feasibility of decoding lower-limb movement in an intuitive and natural way. As far as we know, this is the first online study discussing the role of feedback modalities in lower-limb MI decoding. Our results suggest that proprioceptive feedback has an advantage over visual feedback, which could be used to improve robot-assisted strategies for motor training and functional recovery. © 2017 IOP Publishing Ltd.","brain-machine interface (BMI); electroencephalography (EEG); lower-limb rehabilitation; proprioceptive feedback","Brain computer interface; Decision trees; Decoding; Electroencephalography; Electrophysiology; Neuromuscular rehabilitation; Visual communication; Visual servoing; Brain machine interface; Brain machine interface (BMIs); Closed loop applications; Discriminant power; Functional recovery; Lower limb; Neurorehabilitation; Random forest classifier; Feedback; accuracy; adult; Article; brain computer interface; classification; classifier; electroencephalograph electrode; electroencephalography; female; gait; human; human experiment; limb movement; male; normal human; priority journal; proprioceptive feedback; random forest; visual feedback; young adult",2-s2.0-85029836175
"Ploennigs J., Ba A., Barry M.","Materializing the Promises of Cognitive IoT: How Cognitive Buildings are Shaping the Way",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030636141&doi=10.1109%2fJIOT.2017.2755376&partnerID=40&md5=802e1fe672a6aeafa9e397abb0834e4b","Relatively tiny examples have demonstrated the potential of Cognitive IoT (CIoT) in its full-stack, namely semantic modelling, learning and reasoning over sensors data, and machine learning, to uncover and expose actionable insights via advanced user interfaces. In this paper, we make the case for the feasibility of CIoT in all of its dimensions. We devise a CIoT architecture that integrates thousands of sensors present in our buildings in order to learn the buildings&#x2019; behaviour and intuitively assist users in diagnosing and mitigating undesired events. With our architecture, we place emphasis on the scalability and flexibility that reduce the configuration effort. The solution shows the potential of CIoT to create highly scalable, adaptable and interactive IoT systems functioning for buildings and capable of addressing the challenges encountered in the realm of Homes, Smart Cities and Industry 4.0. IEEE","augmented reality.; automated analytics; Cognitive IoT; fog and cloud computing; learning and reasoning; semantic modeling","Augmented reality; Learning systems; Semantics; Smart city; User interfaces; Advanced user interfaces; automated analytics; Cognitive IoT; learning and reasoning; Semantic Model; Semantic modelling; Sensors data; Internet of things",2-s2.0-85030636141
"Heid T.","Event identification for KM3NeT/ARCA",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436204&doi=10.1088%2f1742-6596%2f888%2f1%2f012046&partnerID=40&md5=8e330e90af99751d9469ef225fba7504","KM3NeT is a large research infrastructure consisting of a network of deep-sea neutrino telescopes. KM3NeT/ARCA will be the instrument detecting high-energy neutrinos with energies above 100 TeV. This instrument gives a new opportunity to observe the neutrino sky with very high angular resolution to be able to detect neutrino point sources. Furthermore it will be possible to probe the flavour composition of neutrino fluxes, and hence production mechanisms, with so-far unreached precision. Neutrinos produce different event topologies in the detector according to their flavour, interaction channel and deposited energy. Machine-learning algorithms are able to learn features of topologies to discriminate them. In previous analyses only two event types were regarded, namely the shower and track topology. With good timing resolution and precise reconstruction algorithms it is possible to separate into more event types, for example the double bang topology produced by tau neutrinos. The final goal is to distinguish all three neutrino flavors as much as possible. To resolve this issue the KM3NeT collaboration uses deep neural networks trained with Monte Carlo events of all neutrino types. This contribution shows the ability of KM3NeT/ARCA to classify events in more than two neutrino event topologies. Furthermore, the borders between detectable classes are shown, such as the minimum distance the tau has to travel before decaying into a tau neutrino to be detected as double bang event. © Published under licence by IOP Publishing Ltd.",,"Astrophysics; Atomic physics; Deep neural networks; Learning algorithms; Learning systems; Neutrons; Topology; Deep sea neutrino telescopes; Event identification; High energy neutrinos; Interaction channel; Production mechanisms; Reconstruction algorithms; Research infrastructure; Very high angular resolution; Elementary particles",2-s2.0-85032436204
[No author name available],"ACM International Conference Proceeding Series",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032494658&partnerID=40&md5=0ed4808af9d568a4f3d161b3b95ac60f","The proceedings contain 29 papers. The topics discussed include: machine learning-based approach to automatic POS tagging of Macedonian language; a novel model for measuring component-based system agility; information retrieval with reinforced word classes; towards dynamic cross-platform component-driven applications with girders elements framework; a time-critical mobile application based on ECG medical monitoring; reasons for and benefits of teaching internet of things basics in the eve of the 4th industrial revolution; a systematic mapping study of computer vision approaches based on deep learning and neural network; extended Tuple constraint type in relational and xml data model - definition and enforcement; structured component and connector communication; Albanian dynamic dactyls recognition using Kinect technology and DTW; using lexical resources for irony and sarcasm classification; extracting text keywords using WordNet; the invalidity of validating emotional multi-agent systems simulations; the new era of AI will revolutionize our wellness; proposed architecture to manage critical states predictions in IoT applications; a survey on access control mechanisms in e-commerce environments; and stimulating intellectual activity with adaptive environment (SMILE).",,,2-s2.0-85032494658
"Zhao T., Zhang N., Zhang Y., Ren J., Xu P., Liu Z., Cheng L., Hu Y.","A novel method to identify pre-microRNA in various species knowledge base on various species",2017,"Journal of Biomedical Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029769535&doi=10.1186%2fs13326-017-0143-z&partnerID=40&md5=b487109c96625c3ee8472139c7a43bad","Background: More than 1/3 of human genes are regulated by microRNAs. The identification of microRNA (miRNA) is the precondition of discovering the regulatory mechanism of miRNA and developing the cure for genetic diseases. The traditional identification method is biological experiment, but it has the defects of long period, high cost, and missing the miRNAs that but also many other algorithms only exist in a specific period or low expression level. Therefore, to overcome these defects, machine learning method is applied to identify miRNAs. Results: In this study, for identifying real and pseudo miRNAs and classifying different species, we extracted 98 dimensional features based on the primary and secondary structure, then we proposed the BP-Adaboost method to figure out the overfitting phenomenon of BP neural network by constructing multiple BP neural network classifiers and distributed weights to these classifiers. The novel method we proposed, from the 4 evaluation terms, have achieved greatly improvement on the effect of identifying true pre-RNA compared to other methods. And from the respect of identifying species of pre-RNA, the novel method achieved more accuracy than other algorithms. Conclusions: The BP-Adaboost method has achieved more than 98% accuracy in identifying real and pseudo miRNAs. It is much higher than not only BP but also many other algorithms. In the second experiment, restricted by the data, the algorithm could not get high accuracy in identifying 7 species, but also better than other algorithms. © 2017 The Author(s).","Adaboost; BP neural network; Pre-miRNA identification",,2-s2.0-85029769535
"Hamed Alemohammad S., Fang B., Konings A.G., Aires F., Green J.K., Kolassa J., Miralles D., Prigent C., Gentine P.","Water, Energy, and Carbon with Artificial Neural Networks (WECANN): A statistically based estimate of global surface turbulent fluxes and gross primary productivity using solar-induced fluorescence",2017,"Biogeosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029767232&doi=10.5194%2fbg-14-4101-2017&partnerID=40&md5=ead2836331ee67437920be5391b3b95e","A new global estimate of surface turbulent fluxes, latent heat flux (LE) and sensible heat flux (<i>H</i>), and gross primary production (GPP) is developed using a machine learning approach informed by novel remotely sensed solar-induced fluorescence (SIF) and other radiative and meteorological variables. This is the first study to jointly retrieve LE, <i>H</i>, and GPP using SIF observations. The approach uses an artificial neural network (ANN) with a target dataset generated from three independent data sources, weighted based on a triple collocation (TC) algorithm. The new retrieval, named Water, Energy, and Carbon with Artificial Neural Networks (WECANN), provides estimates of LE, and GPP from 2007 to 2015 at 1 spatial resolution and at monthly time resolution. The quality of ANN training is assessed using the target data, and the WECANN retrievals are evaluated using eddy covariance tower estimates from the FLUXNET network across various climates and conditions. When compared to eddy covariance estimates, WECANN typically outperforms other products, particularly for sensible and latent heat fluxes. Analyzing WECANN retrievals across three extreme drought and heat wave events demonstrates the capability of the retrievals to capture the extent of these events. Uncertainty estimates of the retrievals are analyzed and the interannual variability in average global and regional fluxes shows the impact of distinct climatic events - such as the 2015 El Niño - on surface turbulent fluxes and GPP.",,"algorithm; artificial neural network; carbon; data set; drought; eddy covariance; energy; fluorescence; heat wave; latent heat flux; net primary production; sensible heat flux; solar radiation; spatial resolution; surface flux; turbulence; water resource",2-s2.0-85029767232
"Auer M., Griffiths M.D.","Cognitive Dissonance, Personalized Feedback, and Online Gambling Behavior: An Exploratory Study Using Objective Tracking Data and Subjective Self-Report",2017,"International Journal of Mental Health and Addiction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029596311&doi=10.1007%2fs11469-017-9808-1&partnerID=40&md5=927f8c7abb316b162198109c09e05bbb","Providing personalized feedback about the amount of money that gamblers have actually spent may—in some cases—result in cognitive dissonance due to the mismatch between what gamblers actually spent and what they thought they had spent. In the present study, the participant sample (N = 11,829) was drawn from a Norwegian population that had played at least one game for money in the past six months on the Norsk Tipping online gambling website. Players were told that they could retrieve personalized information about the amount of money they had lost over the previous 6-month period. Out of the 11,829 players, 4045 players accessed information about their personal gambling expenditure and were asked whether they thought the amount they lost was (i) more than expected, (ii) about as much as expected, or (iii) less than expected. It was hypothesized that players who claimed that the amount of money lost gambling was more than they had expected were more likely to experience a state of cognitive dissonance and would attempt to reduce their gambling expenditure more than other players who claimed that the amount of money lost was as much as they expected. The overall results contradicted the hypothesis because players without any cognitive dissonance decreased their gambling expenditure more than players experiencing cognitive dissonance. However, a more detailed analysis of the data supported the hypothesis because specific playing patterns of six different types of gambler using a machine-learning tree algorithm explained the paradoxical overall result. © 2017 The Author(s)","Behavioral tracking; Cognitive dissonance; Gambling; Gambling expenditure; Online gambling",,2-s2.0-85029596311
"Rong F.","Audio classification method based on machine learning",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032788363&doi=10.1109%2fICITBS.2016.98&partnerID=40&md5=1aba4f6f55d7cfddce0778b6c03c2ff3","Audio classification has very large theoretical and practical values in both pattern recognition and artificial intelligence. In this paper, we propose a novel audio classification method based on machine learning technique. Firstly, we illustrate the hierarchical structure of audio data, which is made up of four layers: 1) Audio frame, 2) Audio clip, 3) Audio shot, and 4) Audio high level semantic unit. Secondly, three types of audio data feature are extracted to construct feature vector, including 1) Short time energy, 2) Zero crossing rate and 3) Mel-Frequency cepstral coefficients. Thirdly, we discuss how to classify audio data using the SVM classifier with Gaussian kernel. Finally, experimental results demonstrate that the proposed method is able to achieve higher audio classification accuracy. © 2016 IEEE.","Audio classification; Audio frame; Audio shot; Machine learning; SVM","Artificial intelligence; Audio acoustics; Big data; Learning systems; Pattern recognition; Semantics; Smart city; Speech recognition; Audio classification; Audio frames; Audio shot; Hierarchical structures; High level semantics; Mel frequency cepstral co-efficient; Short-time energy; Zero crossing rate; Classification (of information)",2-s2.0-85032788363
"Patra S., Bhardwaj K., Bruzzone L.","A Spectral-Spatial Multicriteria Active Learning Technique for Hyperspectral Image Classification",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030633830&doi=10.1109%2fJSTARS.2017.2747600&partnerID=40&md5=ff18d6cff6a0cbceb4adbf094ed2d75c","Hyperspectral image classification with limited labeled samples is a challenging task and still an open research issue. In this article, a&#x00A0; novel technique is presented to address such an issue by exploiting dimensionality reduction, spectral-spatial information, and classification with active learning. The proposed technique is based on two phases. Considering the importance of dimensionality reduction and spatial information for the analysis of hyperspectral images, Phase I generates the patterns corresponding to each pixel of the image using both spectral and spatial information. To this end, first, principal component analysis is used to reduce the dimensionality of an hyperspectral image, then extended morphological profiles are exploited. The spectral-spatial information based patterns generated by extended morphological profiles are used as input to the Phase II. Phase II performs the classification task guided by an active learning technique. This technique is based on a novel query function that uses uncertainty, diversity, and cluster assumption criteria by exploiting the properties of <formula><tex>$k$</tex></formula>-means clustering, <formula><tex>$K$</tex></formula>-nearest neighbors algorithm, support vector machines, and genetic algorithms. Experiments on three benchmark hyperspectral datasets demonstrate that the proposed method outperforms five state-of-the-art active learning methods. IEEE","Active learning (AL); classification; Feature extraction; genetic algorithms (GAs); Hyperspectral imaging; k-means clustering; mathematical morphology; Principal component analysis; remote sensing; Support vector machines; support vector machines (SVM); Training","Artificial intelligence; Clustering algorithms; Feature extraction; Genetic algorithms; Hyperspectral imaging; Image analysis; Image classification; Independent component analysis; Learning algorithms; Learning systems; Mathematical morphology; Personnel training; Principal component analysis; Reduction; Remote sensing; Spectroscopy; Support vector machines; Active Learning; Active learning methods; Classification tasks; Dimensionality reduction; Extended morphological profiles; Genetic algorithm (GAs); K-means clustering; Spatial informations; Classification (of information)",2-s2.0-85030633830
"Lesnik K.L., Liu H.","Predicting Microbial Fuel Cell Biofilm Communities and Bioreactor Performance using Artificial Neural Networks",2017,"Environmental Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029697414&doi=10.1021%2facs.est.7b01413&partnerID=40&md5=7703f330d717121562f96f4cfa6487c4","The complex interactions that occur in mixed-species bioelectrochemical reactors, like microbial fuel cells (MFCs), make accurate predictions of performance outcomes under untested conditions difficult. While direct correlations between any individual waste stream characteristic or microbial community structure and reactor performance have not been able to be directly established, the increase in sequencing data and readily available computational power enables the development of alternate approaches. In the current study, 33 MFCs were evaluated under a range of conditions including eight separate substrates and three different wastewaters. Artificial Neural Networks (ANNs) were used to establish mathematical relationships between wastewater/solution characteristics, biofilm communities, and reactor performance. ANN models that incorporated biotic interactions predicted reactor performance outcomes more accurately than those that did not. The average percent error of power density predictions was 16.01 ± 4.35%, while the average percent error of Coulombic efficiency and COD removal rate predictions were 1.77 ± 0.57% and 4.07 ± 1.06%, respectively. Predictions of power density improved to within 5.76 ± 3.16% percent error through classifying taxonomic data at the family versus class level. Results suggest that the microbial communities and performance of bioelectrochemical systems can be accurately predicted using data-mining, machine-learning techniques. © 2017 American Chemical Society.",,"Biofilms; Data mining; Errors; Forecasting; Fuel cells; Learning systems; Microorganisms; Neural networks; Bio-electrochemical reactors; Bio-electrochemical systems; Bioreactor performance; Machine learning techniques; Mathematical relationship; Microbial communities; Microbial community structures; Microbial fuel cells (MFCs); Microbial fuel cells; antimicrobial activity; artificial neural network; biofilm; bioreactor; community structure; electrochemical method; fuel cell; microbial community; performance assessment; prediction; amplicon; Article; artificial neural network; biofilm; bioreactor; community structure; data mining; machine learning; microbial community; microbial fuel cell; nonhuman; prediction; reactor operation; taxonomy; waste water",2-s2.0-85029697414
"Yazgac B.G., Kirci M.","Embedded system application for sunn pest detection",2017,"2017 6th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032795705&doi=10.1109%2fAgro-Geoinformatics.2017.8047027&partnerID=40&md5=3b37151488d5e5f5f65b31c9cef4fd47","Wheat bugs are one of the most dangerous insect types for cereal plantations. These types of insect pests have a possibility of causing 100% product loss in wheat production. This insect type consists of members from Pentatomidea and Heterotoptera. Most notorious members of this group are known as sunn pest. This type of pest can be encountered on every plantation in Eurasia. Wheat bugs prefer plantations not only for feeding but also for breeding. In the absence of control measures, wheat plantations can become overpopulated with sunn pest. Recently cultural control and biological control studies have gained attention. These control method groups are preferred over chemical control, because of healthcare reasons. Moreover, integrated control methods can be projected as the feature of the pest control, as precision agriculture applications spread. Today, beginning from the spring, plant protection experts watch for sunn pest awakening to try to avoid possible sunn pest attack. Additionally, chemical poisons are sprayed on plantations for protection reasons. These hazardous insecticides are known to be seriously damaging to human health, fauna and flora. Therefore, precision techniques for spotting these pests gained undeniable importance. Audio detection, recognition and classification methods have been used for decision making about creatures. To present day, these methods are used on insects, pests, birds, reptiles etc. successfully. In this work, a successful sound detection algorithm is applied to sound recordings of different sunn pest classes on an embedded system. A capable microcomputer is programmed to perform segmentation, feature extraction and classification procedures. Mel Frequency Cepstral Coefficients (MFCC) and Line Spectral Frequencies (LSF) methods are applied for feature extraction. Following that, different classification algorithms such as k-Nearest Neighbors (kNN) and Support Vector Machine (SVM) are applied to feature vector set. The performances of the procedures are examined in the sense of accuracy, and time consumption. © 2017 IEEE.","audio processing; Embedded system; kNN; LSF; machine learning; MFCC; sunn pest; SVM; wheat bugs","Decision making; Embedded systems; Extraction; Feature extraction; Image retrieval; Learning systems; Nearest neighbor search; Speech recognition; Support vector machines; Audio processing; Embedded system applications; Feature extraction and classification; Line spectral frequencies; Mel-frequency cepstral coefficients; MFCC; sunn pest; wheat bugs; Program debugging",2-s2.0-85032795705
"Wang W., Shen J., Shao L.","Video Salient Object Detection via Fully Convolutional Networks",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030790744&doi=10.1109%2fTIP.2017.2754941&partnerID=40&md5=8b9e9fc862c3d77956fe7c642058d5f4","This paper proposes a deep learning model to efficiently detect salient regions in videos. It addresses two important issues: (1) deep video saliency model training with the absence of sufficiently large and pixel-wise annotated video data; and (2) fast video saliency training and detection. The proposed deep video saliency network consists of two modules, for capturing the spatial and temporal saliency information, respectively. The dynamic saliency model, explicitly incorporating saliency estimates from the static saliency model, directly produces spatiotemporal saliency inference without time-consuming optical flow computation. We further propose a novel data augmentation technique that simulates video training data from existing annotated image datasets, which enables our network to learn diverse saliency information and prevents overfitting with the limited number of training videos. Leveraging our synthetic video data (150K video sequences) and real videos, our deep video saliency model successfully learns both spatial and temporal saliency cues, thus producing accurate spatiotemporal saliency estimate. We advance the state-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of .07), and do so with much improved speed (2fps with all steps). IEEE","Computational modeling; Computer vision; deep learning; fully convolutional network; Machine learning; Object detection; Optical imaging; salient object detection; Spatiotemporal phenomena; synthetic video data; Training; Video saliency","Computer vision; Convolution; Deep learning; Learning systems; Object detection; Personnel training; Video recording; Computational model; Convolutional networks; Optical imaging; Salient object detection; Spatiotemporal phenomena; Video data; Video saliencies; Object recognition",2-s2.0-85030790744
"Hasegawa K., Yanagisawa M., Togawa N.","Hardware Trojans classification for gate-level netlists using multi-layer neural networks",2017,"2017 IEEE 23rd International Symposium on On-Line Testing and Robust System Design, IOLTS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032723104&doi=10.1109%2fIOLTS.2017.8046227&partnerID=40&md5=0aabc3033f141e4dd5c5c7bae3b7abf2","Recently, due to the increase of outsourcing in IC design and manufacturing, it has been reported that malicious third-party IC vendors often insert hardware Trojans into their products. Especially in IC design step, it is strongly required to detect hardware Trojans because malicious third-party vendors can easily insert hardware Trojans in their products. In this paper, we propose a machine-learning-based hardware-Trojan detection method for gate-level netlists using multi-layer neural networks. First, we extract 11 Trojan-net feature values for each net in a netlist. After that, we classify the nets in an unknown netlist into a set of Trojan nets and that of normal nets using multi-layer neural networks. We obtained at most 100% true positive rate with our proposed method. © 2017 IEEE.","Gate-level netlist; Hardware Trojan; Machine learning; Multi-layer; Neural network","Artificial intelligence; Hardware; Integrated circuits; Learning systems; Malware; Network layers; Neural networks; Outsourcing; Product design; Systems analysis; Design steps; Feature values; Gate levels; Hardware Trojan detection; Netlist; Third parties; Third party vendors; True positive rates; Hardware security",2-s2.0-85032723104
"Baron J.R., Payne N.","Dark archives and edemocracy: Strategies for overcoming access barriers to the public record archives of the future",2017,"Proceedings of the 7th International Conference for E-Democracy and Open Government, CeDEM 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032793829&doi=10.1109%2fCeDEM.2017.27&partnerID=40&md5=c1a6264841223edaa8adf6e3573b1dd5","Electronic records in public archives are increasingly in danger of being rendered inaccessible in light of the challenges posed by their hugely increased overall volume, coupled with the presence of sensitive content embedded within records which presently requires labor-intensive review for possible withholding. New advanced search technologies including in the form of machine learning promise to improve the review capabilities of public servants, for the purpose of protecting privacy as well as potentially accelerating access to archival record holdings in digital form. The emerging issue of how governments deal with the challenge to transparency posed by dark archives is a subject worthy of greater discussion in international open government and e-democracy forums. © 2017 IEEE.","Archives; E-democracy; Electronic records; Machine learning; Open government; Public access; Sensitive content","Artificial intelligence; Learning systems; Records management; Archives; E-democracy; Electronic records; Open government; Public Access; Sensitive content; Electronic document exchange",2-s2.0-85032793829
"Darton T.C., Baker S., Randall A., Dongol S., Karkey A., Voysey M., Carter M.J., Jones C., Trappl K., Pablo J., Hung C., Teng A., Shandling A., Le T., Walker C., Molina D., Andrews J., Arjyal A., Basnyat B., Pollard A.J., Blohmke C.J.","Identification of novel serodiagnostic signatures of typhoid fever using a Salmonella proteome array",2017,"Frontiers in Microbiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029747608&doi=10.3389%2ffmicb.2017.01794&partnerID=40&md5=f36cb552fbffd1935e13f907680f611e","Current diagnostic tests for typhoid fever, the disease caused by Salmonella Typhi, are poor. We aimed to identify serodiagnostic signatures of typhoid fever by assessing microarray signals to 4,445 S. Typhi antigens in sera from 41 participants challenged with oral S. Typhi. We found broad, heterogeneous antibody responses with increasing IgM/IgA signals at diagnosis. In down-selected 250-antigen arrays we validated responses in a second challenge cohort (n = 30), and selected diagnostic signatures using machine learning and multivariable modeling. In four models containing responses to antigens including flagellin, OmpA, HlyE, sipC, and LPS, multi-antigen signatures discriminated typhoid (n = 100) from other febrile bacteremia (n = 52) in Nepal. These models contained combinatorial IgM, IgA, and IgG responses to 5 antigens (ROC AUC, 0.67 and 0.71) or 3 antigens (0.87), although IgA responses to LPS also performed well (0.88). Using a novel systematic approach we have identified and validated optimal serological diagnostic signatures of typhoid fever. © 2017 Darton, Baker, Randall, Dongol, Karkey, Voysey, Carter, Jones, Trappl, Pablo, Hung, Teng, Shandling, Le, Walker, Molina, Andrews, Arjyal, Basnyat, Pollard and Blohmke.","Antibody response; Controlled human infection model; Enteric fever; Fever diagnostics; Machine learning; Rapid diagnostic tests; Salmonella typhi; Serodiagnostics","flagellin; immunoglobulin A; immunoglobulin class; immunoglobulin G; immunoglobulin M; lipopolysaccharide; n acetylmuramoylalanine amidase; outer membrane protein A; proteome; antibody response; Article; bacteremia; blood culture; controlled study; human; humoral immunity; microarray analysis; nonhuman; parasite serodiagnosis; partial least squares regression; principal component analysis; protein purification; receiver operating characteristic; Salmonella; Salmonella enterica serovar Typhi; support vector machine; typhoid fever",2-s2.0-85029747608
"Lu J., Zhang X., Junfeng W., Lingyun Y.","APT traffic detection based on time transform",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032835257&doi=10.1109%2fICITBS.2016.87&partnerID=40&md5=0bb3b84d408314794b9f58381a712813","APT(Advanced persist threat) is an emerging attack on the Internet. Attackers may combine phishing emails, malware, social engineering and botnets to create a series of attacks in one APT attack which makes it quite difficult for detection. In this way, attackers can remotely control the infected host, or steal sensitive information. In this paper, we proposed a time transform features approach for distinguishing APT attacks based on the observation that malicious payload must be transferred to the target hosts in an APT attack. By comparing the normal traffic with the traffic containing a malicious payload, we are able to catch the signal of malicious payload and further infer the existence of APT attacks. Then we use machine learning methods to detect APT attacks in big data. To verify this approach, we placed a device on the gateway of our university for catching the real Internet traffic of the university for one month. Then we mixed the APT traffic with these flows, and see whether our approach can identify the malicious payloads. We found our approach is not only accurate but also efficient for catching APT attacks. © 2016 IEEE.","APT; Detection; Time transform","Big data; Computer crime; Error detection; Gateways (computer networks); Learning systems; Smart city; Botnets; Internet traffic; Machine learning methods; Phishing; Sensitive informations; Social engineering; Traffic detection; Malware",2-s2.0-85032835257
"Jing L., Bin W.","Network intrusion detection method based on relevance deep learning",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032805816&doi=10.1109%2fICITBS.2016.132&partnerID=40&md5=00af6de1834275884070de2812f58a3d","With the development of science and technology, computer network has been more and more widely used, but its inherent characteristics cause that it is prone to various invasions. Its security research is very valuable. As an active security mechanism, intrusion detection technology is the key to ensure network security. This paper proposed an network intrusion detection method based on the relevance deep learning, which learned the learning principle of relevance deep and the training algorithm of restricted Boltzmann machine, which analyzed the principle of feasibility that the relevance deep learning was applied to the network intrusion detection system in the network intrusion detection technology. The relevance depth learning was applied to the network intrusion detection technology, which could obtain the higher detection accuracy. The simulation results show that the network intrusion detection method based on relevance deep learning has a high level of average detection rate and average false detection rate for unknown intrusion and attack. The experiment results show that the proposed method is reliable and effective. © 2016 IEEE.","Deep learning; Detection; Network intrusion","Big data; Deep learning; Error detection; Intrusion detection; Mercury (metal); Smart city; Development of science and technologies; Inherent characteristics; Intrusion detection technologies; Network intrusion detection; Network intrusion detection method; Network intrusion detection systems; Network intrusions; Restricted boltzmann machine; Network security",2-s2.0-85032805816
"Yalcin H.","Plant phenology recognition using deep learning: Deep-Pheno",2017,"2017 6th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032839225&doi=10.1109%2fAgro-Geoinformatics.2017.8046996&partnerID=40&md5=7b9ceed452fe08b38656a7349c4bb833","Monitoring phenology of agricultural plants is a critical understanding in precision agriculture. Vital improvements can be achieved with precise detection of phenological change of plants which would henceforth improve the timing for the harvest, pest control, yield prediction, farm monitoring, disaster warning etc. Many countries across the world have been developing initiatives to build national agriculture monitoring network systems, since inferring the phenological information contributes to a better understanding of relationships between productivity, vegetation health and environmental conditions. In this paper, we utilize a deep learning architecture to recognize and classify phenological stages of several types of plants purely based on the visual data captured every half an hour by cameras mounted on the ground agro-stations that have been planted all over Turkey as part of an agriculture monitoring network system. A pre-trained Convolutional Neural Network architecture (CNN) is employed to automatically extract the features of images. In order to evaluate the performance of the approach proposed in this paper, the results obtained through CNN model are compared with those obtained by employing hand crafted feature descriptors. Experimental results suggest that CNN architecture outperforms the machine learning algorithms based on hand crafted features for the discrimination of phenological stages. © 2017 IEEE.","computer vision; convolutional neural networks; deep learning; phenology recognition; precision agriculture","Agricultural machinery; Agriculture; Biology; Computer vision; Convolution; Deep learning; Learning systems; Monitoring; Network architecture; Neural networks; Vegetation; Agriculture monitoring; Convolutional neural network; Environmental conditions; Feature descriptors; Learning architectures; Phenological changes; phenology recognition; Precision Agriculture; Learning algorithms",2-s2.0-85032839225
"Fang X., Sastry A., Mih N., Kim D., Tan J., Yurkovich J.T., Lloyd C.J., Gao Y., Yang L., Palsson B.O.","Global transcriptional regulatory network for Escherichia coli robustly connects gene expression to transcription factor activities",2017,"Proceedings of the National Academy of Sciences of the United States of America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029549061&doi=10.1073%2fpnas.1702581114&partnerID=40&md5=70698452018e0ebf22091a544019343f","Transcriptional regulatory networks (TRNs) have been studied intensely for >25 y. Yet, even for the Escherichia coli TRN - probably the best characterized TRN - several questions remain. Here, we address three questions: (i) How complete is our knowledge of the E. coli TRN; (ii) how well can we predict gene expression using this TRN; and (iii) how robust is our understanding of the TRN? First, we reconstructed a high-confidence TRN (hiTRN) consisting of 147 transcription factors (TFs) regulating 1,538 transcription units (TUs) encoding 1,764 genes. The 3,797 high-confidence regulatory interactions were collected from published, validated chromatin immunoprecipitation (ChIP) data and RegulonDB. For 21 different TF knockouts, up to 63% of the differentially expressed genes in the hiTRN were traced to the knocked-out TF through regulatory cascades. Second, we trained supervised machine learning algorithms to predict the expression of 1,364 TUs given TF activities using 441 samples. The algorithms accurately predicted condition-specific expression for 86% (1,174 of 1,364) of the TUs, while 193 TUs (14%) were predicted better than random TRNs. Third, we identified 10 regulatory modules whose definitions were robust against changes to the TRN or expression compendium. Using surrogate variable analysis, we also identified three unmodeled factors that systematically influenced gene expression. Our computational workflow comprehensively characterizes the predictive capabilities and systems-level functions of an organism’s TRN from disparate data types. © 2017, National Academy of Sciences. All rights reserved.","Matrix factorization; Regression; Transcriptional regulation; Transcriptomics","transcription factor; transcriptome; algorithm; Article; binding affinity; chromatin immunoprecipitation; Escherichia coli; gene expression; gene regulatory network; genetic conservation; machine learning; nonhuman; priority journal; protein expression; protein function; protein structure; sensitivity analysis; transcriptomics; workflow",2-s2.0-85029549061
"Alphonse A.S., Dharma D.","Novel directional patterns and a Generalized Supervised Dimension Reduction System (GSDRS) for facial emotion recognition",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029606629&doi=10.1007%2fs11042-017-5141-8&partnerID=40&md5=3f2539af12d8bbbd17868519c1c2c43a","This paper presents two novel directional patterns, a Maximum Response-based Directional Texture Pattern (MRDTP) and a Maximum Response-based Directional Number Pattern (MRDNP), for recognizing the facial emotions in constrained as well as unconstrained situations. The intensity information obtained from the maximum of the edge responses, after applying eight Kirsch masks, is used for the calculation of facial features in MRDTP. In MRDNP, instead of intensity information, the direction number of the maximum response is used. After dividing MRDNP and MRDTP code images into grids, feature vectors are created from the concatenated histograms obtained from the grids. This paper also proposes an effective Generalized Supervised Dimension Reduction System (GSDRS) and uses Extreme Learning Machine with Radial Basis Function (ELM-RBF) classifier for rapid and efficient classification of emotions. Both the proposed patterns are more effective than the existing ones in removing random noise and providing good structural information using prominent edges which help to achieve high classification accuracy when tested with seven datasets. © 2017 Springer Science+Business Media, LLC","classification; dimension reduction; ELM; Emotion; feature extraction","Classification (of information); Concatenated codes; Feature extraction; Learning systems; Pattern recognition systems; Radial basis function networks; Classification accuracy; Classification of emotions; Dimension reduction; Emotion; Extreme learning machine; Radial basis functions; Structural information; Supervised dimension reductions; Face recognition",2-s2.0-85029606629
"Kang M., Lee J.-G.","An experimental analysis of limitations of MapReduce for iterative algorithms on Spark",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029577286&doi=10.1007%2fs10586-017-1167-y&partnerID=40&md5=258c85cb6a0228e22387e0ded0ff7e02","MapReduce is the most popular framework for distributed processing. Recently, the scalability of data mining and machine learning algorithms has significantly improved with help from MapReduce. However, MapReduce does not handle iterative algorithms very efficiently. The problem is that many data mining and machine learning algorithms are iterative by nature. In order to overcome the limitations of MapReduce, many advanced distributed systems have been developed, including HaLoop, iMapReduce, Twister, and Spark. In this paper, we identify and categorize the limitations of MapReduce in handling iterative algorithms, and then, experimentally investigate the consequences of these limitations by using the most flexible and stable distributed system, Spark. According to our experiment results, the network I/O overhead was the primary factor that affected system performance the most. The disk I/O overhead also affected system performance, but it was less significant than the network I/O overhead. For the synchronization overhead, it affected system performance only when the static data was not cached. © 2017 Springer Science+Business Media, LLC","Hadoop; HaLoop; iMapReduce; Iterative algorithms; Spark; Twister","Artificial intelligence; Data mining; Electric sparks; Iterative methods; Learning systems; Hadoop; HaLoop; iMapReduce; Iterative algorithm; Twister; Learning algorithms",2-s2.0-85029577286
"Cottet V., Alquier P.","1-Bit matrix completion: PAC-Bayesian analysis of a variational approximation",2017,"Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029593893&doi=10.1007%2fs10994-017-5667-z&partnerID=40&md5=3fad4b8b175b5157cf0fbeb30146b343","We focus on the completion of a (possibly) low-rank matrix with binary entries, the so-called 1-bit matrix completion problem. Our approach relies on tools from machine learning theory: empirical risk minimization and its convex relaxations. We propose an algorithm to compute a variational approximation of the pseudo-posterior. Thanks to the convex relaxation, the corresponding minimization problem is bi-convex, and thus the method works well in practice. We study the performance of this variational approximation through PAC-Bayesian learning bounds. Contrary to previous works that focused on upper bounds on the estimation error of M with various matrix norms, we are able to derive from this analysis a PAC bound on the prediction error of our algorithm. We focus essentially on convex relaxation through the hinge loss, for which we present a complete analysis, a complete simulation study and a test on the MovieLens data set. We also discuss a variational approximation to deal with the logistic loss. © 2017 The Author(s)","Matrix completion; Oracle inequalities; PAC-Bayesian bounds; Risk convexification; Supervised classification; Variational Bayes","Approximation algorithms; Learning systems; Relaxation processes; Statistical tests; Supervised learning; Convexification; Matrix completion; Oracle inequalities; PAC-Bayesian bounds; Supervised classification; Variational bayes; Matrix algebra",2-s2.0-85029593893
"Zhao H., Liu H., Ding Z., Fu Y.","Consensus Regularized Multi-View Outlier Detection",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708882&doi=10.1109%2fTIP.2017.2754942&partnerID=40&md5=fe54ea99f4b53621b0c7088a25db9e8b","Identifying different types of data outliers with abnormal behaviors in multi-view data setting is challenging due to the complicated data distributions across different views. Conventional approaches achieve this by learning a new latent feature representation with the pairwise constraint on different view data. In this paper, we argue that the existing methods are expensive in generalizing their models from two-view data to three-view (or more) data, in terms of the number of introduced variables and detection performance. To address this, we propose a novel multi-view outlier detection method with a consensus regularization on the latent representations. Specifically, we explicitly characterize each kind of outliers by the intrinsic cluster assignment labels and sample-specific errors. Moreover, we make a thorough discussion about the proposed consensusregularization and the pairwise-regularization. Correspondingly, an optimization solution based on augmented Lagrangian multiplier method is proposed and derived in details. In the experiments, we evaluate our method on five well-known machine learning datasets with different outlier settings. Further, to show its effectiveness in real-world computer vision scenario, we tailor our proposed model to saliency detection and face reconstruction applications. The extensive results of both standard multi-view outlier detection task and the extended computer vision tasks demonstrate the effectiveness of our proposed method. IEEE","Anomaly detection; Computational modeling; Computer vision; consensus regularization; Data models; Face; Feature extraction; multi-view learning; Optimization; outlier detection","Computer vision; Constrained optimization; Data handling; Data structures; Feature extraction; Lagrange multipliers; Learning systems; Optimization; Statistics; Anomaly detection; Computational model; consensus regularization; Face; Multi-view learning; Outlier Detection; Data mining",2-s2.0-85030708882
"Barkat M., Karan V., Pradeep N.","A Method of Reporting and Prioritizing Faults for Aircraft Downtime Reduction",2017,"SAE Technical Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029000004&doi=10.4271%2f2017-01-2125&partnerID=40&md5=c34e9d296454d71f1dd6e5c39bba3429","The exponential increase in the number of aircrafts and air travelers has triggered new innovations which aim to make airline services more reliable and consumer friendly. Quick and efficient maintenance actions with minimum downtime are the need of the hour. Areas that have a large potential for improvement in this regard are the real time use of diagnostic data, filtering/elimination of nuisance faults and machine learning capabilities with respect to maintenance actions. Although, numerous LRUs installed on the aircraft generate massive amounts of diagnostic data to detect any possible issue or LRU failure, it is seldom used in real time. The turnaround time for LRU maintenance can be greatly reduced if the results of the diagnostics conducted during LRU normal operation is relayed to ground stations in real-time. This enables the maintenance engineers to plan ahead and initiate maintenance actions well before the aircraft lands and becomes available for maintenance. Handling nuisance faults generated during the LRU diagnostic tests is another area with scope for improvement. The advancements in predictive analytics can be harnessed to identify the possibility of reported fault being a nuisance fault. The current method to identify nuisance faults involves a maintenance engineer performing an initiated test after the aircraft touches down. Any time spent in planning maintenance actions to rectify these faults and parts procured for the same is wasted. This paper discusses a novel method that addresses the aforementioned problems by the use of on-board automated FMEA, predictive analytics and machine learning to suggest actions for maintenance engineers. The on-board automated FMEA allows critical diagnostic data to be identified, transmitted and used in real time. Predictive analytics enables the forecasting of nuisance faults and prioritizing the reported faults. The paper also outlines the implementation challenges pertaining to data communication, security and integrity. © 2017 SAE International.",,"Air transportation; Aircraft; Artificial intelligence; Engineers; Learning systems; Predictive analytics; Airline services; Data-communication; Diagnostic tests; Downtime reduction; Exponential increase; Maintenance Action; Maintenance engineers; Normal operations; Maintenance",2-s2.0-85029000004
"Aktas A.F., Berk Ustundag B.","Phenology based NDVI time-series compensation for yield estimation analysis",2017,"2017 6th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032827333&doi=10.1109%2fAgro-Geoinformatics.2017.8047038&partnerID=40&md5=8a5dbe58854ffda01c1dbeef859bd145","Normalized difference vegetation index (NDVI) has been correlated with various vegetation parameters using different preprocessing methods, corrections and sampling time based on the aim of the study. In yield estimation studies, maximum NDVI value of the season and the same day of the year NDVI value, which are based on chronological sampling time, are used within different techniques from statistical analysis to machine learning. However, analysis of biological systems based on their chronological timing results in an error increase at data extraction phase due to the non-linearity among phenological stages, representing plant development and its variability. In this study, a phenology based optimum NDVI sampling time is determined and proposed as a replacement of chronologically sampled NDVI time for yield estimation analysis. It may not be possible to have or acquire satellite images for the desired NDVI date due to the temporal resolution of existing remote sensing satellites and meteorological limitations. Therefore, a compensation process based on Adaptive Savitzky-Golay filter and using the existing images is proposed to constitute a new NDVI value for the desired day of the season. The study area is situated in the Southeastern Anatolia region of Turkey within the Fertile Crescent where wheat was first cultivated 10000 years ago. The region has the highest durum wheat production, supplying %46 of the whole production in Turkey. 8-day interval, Landsat-7 and Landsat-8 NDVI time-series are analyzed for seasonal vegetation development with TIMESAT software for the 2014-2016 period. Ground-based ancillary data was obtained within the Turkish Agricultural and Environmental Informatics Research and Application Center (TARBIL) project. Trend analysis of NDVI time-series was performed using Adaptive Savitzky-Golay filter, form of a moving average, adapting to the upper envelope of the data points. Two different sampling methods representing chronological and phenological approaches in addition to the max NDVI value are used to determine the optimum NDVI day. Phenological sampling is carried out as 10-day intervals starting from the emergence phase indicating the start of the season whereas 15 April, representing the long-term annual mean peak NDVI date of the study area was used for chronological sampling. Adaptive Savitzky-Golay filtering and different sampling combinations were used to perform correlation analysis with annual yield data. Best sampling method along with the optimum NDVI sampling day of the season was determined based on the correlation analysis. It is observed that the combinations with phenological sampling corresponding to the first node stage according to Food and Agriculture Organization (FAO) guidelines have the highest correlation. Regression analysis between agrometeorological data with and without compensated NDVI and yield variables showed that the usage of compensated NDVI had higher correlation for wheat yield estimation. The results showed that, in comparison with the conventional approaches, the usage of phenology based compensated NDVI, enhanced the yield estimation percentage. Along with the possibility of producing ancillary data from remote sensing images, this approach will minimize the need for ground-based observations that are time and money consuming. © 2017 IEEE.","Data Compensation; Data Sampling; NDVI; Plant Phenology; time-series; TIMESAT; Yield Estimation","Adaptive filtering; Agricultural machinery; Agriculture; Biology; Correlation methods; Learning systems; Regression analysis; Remote sensing; Sampling; Signal filtering and prediction; Time series; Vegetation; Data compensation; Data sampling; NDVI; Plant phenology; TIMESAT; Yield estimation; Time series analysis",2-s2.0-85032827333
[No author name available],"Proceedings - 2017 NICOGRAPH International, NICOInt 2017",2017,"Proceedings - 2017 NICOGRAPH International, NICOInt 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032812446&partnerID=40&md5=d00342a3e2ebd2296959601c5fe1b0e3","The proceedings contain 35 papers. The topics discussed include: dynamic pressure cycle control: dynamic difficulty adjustment beyond the flow zone; a study on voice actor recommendation for game characters based on acoustic feature estimation and document co-occurrence; Japanese fingerspelling recognition based on classification tree and machine learning; tracking and short-term forecasting of typhoon structure; construction of indoor location search system using Bluetooth low energy; category classification of text data with machine learning technique for visualizing flow of conversation in counseling; estimating parameters of subsurface scattering using directional dipole model; a study of analytic method for distortion of rotational shape by using elliptic circularity; and interactive music modulation with micro-geometry of object epidermis.",,,2-s2.0-85032812446
"Gombolay M.C., Jensen R., Son S.","Machine Learning Techniques for Analyzing Training Behavior in Serious Gaming",2017,"IEEE Transactions on Computational Intelligence and AI in Games",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640462&doi=10.1109%2fTCIAIG.2017.2754375&partnerID=40&md5=1bf6a1d092b23fa654b019af52b84c3a","Training time is a costly, scarce resource across domains such as commercial aviation, healthcare, and military operations. In the context of military applications, serious gaming -- the training of warfighters through immersive, real-time environments rather than traditional classroom lectures -- offers benefits to improve training not only in its hands-on development and application of knowledge, but also in data analytics via machine learning. In this paper, we explore an array of machine learning techniques that allow teachers to visualize the degree to which training objectives are reflected in actual play. First, we investigate the concept of discovery: learning how warfighters utilize their training tools and develop military strategies within their training environment. Second, we develop machine learning techniques that could assist teachers by automatically predicting player performance, identifying player disengagement, and recommending personalized lesson plans. These methods could potentially provide teachers with insight to assist them in developing better lesson plans and tailored instruction for each individual student. IEEE","Games; Hidden Markov models; Measurement; Missiles; Tools; Training","Artificial intelligence; Hidden Markov models; Learning algorithms; Learning systems; Markov processes; Measurements; Military applications; Military operations; Missiles; Personnel training; Planning; Teaching; Tools; Classroom lecture; Data analytics; Development and applications; Games; Machine learning techniques; Real-time environment; Scarce resources; Serious gaming; Education",2-s2.0-85030640462
"Liouane Z., Lemlouma T., Roose P., Weis F., Messaoud H.","An improved extreme learning machine model for the prediction of human scenarios in smart homes",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029585389&doi=10.1007%2fs10489-017-1062-5&partnerID=40&md5=7691871f2b82322596240c2e45bfcafe","One of the main objectives of smart homes is healthcare monitoring and assistance, especially for elderly and disabled people. Therefore, an accurate prediction of the inhabitant behavior is very helpful to provide the required assistance. This work aims to propose a prediction model that satisfies the accuracy as well as the rapidity of the learning phase. To do so, we propose to improve the existing extreme learning machine (ELM) model by defining a recurrent form. This form ensures a temporal relationship of inputs between observations at different time steps. The new model uses feedback connections to the input layer from the output layer which allows the output to be included in the long-term prediction. A recurrent dynamic network, with feedback connections of the output of the network, is proposed to predict the future series representing future activities of the inhabitant. The resulting model, called Recurrent Extreme Learning Machine (RELM), provides the ability to learn the human behavior and ensures a good balance between the learning time and the prediction accuracy. The input data is based on the real data representing the activities of persons belonging to the profile of first level (i.e. P1) as measured by the dependency model called Functional Autonomy Measurement System (SMAF) used in the geriatric domain. The experimental results reveal that the proposed RELM model requires a minimum time during the learning phase with a better performance compared to existing models. © 2017 Springer Science+Business Media, LLC","Accuracy; Behavior prediction; Elderly; ELM; RELM; Smart home; Time series prediction","Automation; Behavioral research; Intelligent buildings; Knowledge acquisition; Learning systems; Accuracy; Behavior prediction; Elderly; RELM; Smart homes; Time series prediction; Forecasting",2-s2.0-85029585389
"Li H., Hou J., Adhikari B., Lyu Q., Cheng J.","Deep learning methods for protein torsion angle prediction",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029659778&doi=10.1186%2fs12859-017-1834-2&partnerID=40&md5=7f5a0c3f3f032f7316408a27b322ab5a","Background: Deep learning is one of the most powerful machine learning methods that has achieved the state-of-the-art performance in many domains. Since deep learning was introduced to the field of bioinformatics in 2012, it has achieved success in a number of areas such as protein residue-residue contact prediction, secondary structure prediction, and fold recognition. In this work, we developed deep learning methods to improve the prediction of torsion (dihedral) angles of proteins. Results: We design four different deep learning architectures to predict protein torsion angles. The architectures including deep neural network (DNN) and deep restricted Boltzmann machine (DRBN), deep recurrent neural network (DRNN) and deep recurrent restricted Boltzmann machine (DReRBM) since the protein torsion angle prediction is a sequence related problem. In addition to existing protein features, two new features (predicted residue contact number and the error distribution of torsion angles extracted from sequence fragments) are used as input to each of the four deep learning architectures to predict phi and psi angles of protein backbone. The mean absolute error (MAE) of phi and psi angles predicted by DRNN, DReRBM, DRBM and DNN is about 20-21° and 29-30° on an independent dataset. The MAE of phi angle is comparable to the existing methods, but the MAE of psi angle is 29°, 2° lower than the existing methods. On the latest CASP12 targets, our methods also achieved the performance better than or comparable to a state-of-the art method. Conclusions: Our experiment demonstrates that deep learning is a valuable method for predicting protein torsion angles. The deep recurrent network architecture performs slightly better than deep feed-forward architecture, and the predicted residue contact number and the error distribution of torsion angles extracted from sequence fragments are useful features for improving prediction accuracy. © 2017 The Author(s).","Deep learning; Deep recurrent neural network; Protein torsion angle prediction; Restricted Boltzmann machine","Deep neural networks; Dihedral angle; Errors; Forecasting; Learning systems; Network architecture; Proteins; Recurrent neural networks; Torsional stress; Feed-forward architectures; Learning architectures; Machine learning methods; Restricted boltzmann machine; Secondary structure prediction; State-of-the-art methods; State-of-the-art performance; Torsion angle; Deep learning",2-s2.0-85029659778
"Albrecht T., Slabaugh G., Alonso E., Al-Arif S.M.M.R.","Deep learning for single-molecule science",2017,"Nanotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030178612&doi=10.1088%2f1361-6528%2faa8334&partnerID=40&md5=6e5c4cbed78e1b17fd8ae54f9ca60b25","Exploring and making predictions based on single-molecule data can be challenging, not only due to the sheer size of the datasets, but also because a priori knowledge about the signal characteristics is typically limited and poor signal-to-noise ratio. For example, hypothesis-driven data exploration, informed by an expectation of the signal characteristics, can lead to interpretation bias or loss of information. Equally, even when the different data categories are known, e.g., the four bases in DNA sequencing, it is often difficult to know how to make best use of the available information content. The latest developments in machine learning (ML), so-called deep learning (DL) offer interesting, new avenues to address such challenges. In some applications, such as speech and image recognition, DL has been able to outperform conventional ML strategies and even human performance. However, to date DL has not been applied much in single-molecule science, presumably in part because relatively little is known about the 'internal workings' of such DL tools within single-molecule science as a field. In this Tutorial, we make an attempt to illustrate in a step-by-step guide how one of those, a convolutional neural network (CNN), may be used for base calling in DNA sequencing applications. We compare it with a SVM as a more conventional ML method, and discuss some of the strengths and weaknesses of the approach. In particular, a 'deep' neural network has many features of a 'black box', which has important implications on how we look at and interpret data. © 2017 IOP Publishing Ltd.","data analysis; deep learning; machine learning; nanoscience; nanotechnology; single-molecule","Artificial intelligence; Data reduction; Deep learning; DNA sequences; Gene encoding; Image recognition; Learning systems; Molecules; Nanoscience; Nanotechnology; Neural networks; Signal to noise ratio; Speech recognition; Technology transfer; Convolutional neural network; Data exploration; Human performance; Information contents; Latest development; Priori knowledge; Signal characteristic; Single molecule; Deep neural networks",2-s2.0-85030178612
"Trivedi H., Mesterhazy J., Laguna B., Vu T., Sohn J.H.","Automatic Determination of the Need for Intravenous Contrast in Musculoskeletal MRI Examinations Using IBM Watson’s Natural Language Processing Algorithm",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029578434&doi=10.1007%2fs10278-017-0021-3&partnerID=40&md5=c3a8a92de497c212c38ba88b7d031896","Magnetic resonance imaging (MRI) protocoling can be time- and resource-intensive, and protocols can often be suboptimal dependent upon the expertise or preferences of the protocoling radiologist. Providing a best-practice recommendation for an MRI protocol has the potential to improve efficiency and decrease the likelihood of a suboptimal or erroneous study. The goal of this study was to develop and validate a machine learning-based natural language classifier that can automatically assign the use of intravenous contrast for musculoskeletal MRI protocols based upon the free-text clinical indication of the study, thereby improving efficiency of the protocoling radiologist and potentially decreasing errors. We utilized a deep learning-based natural language classification system from IBM Watson, a question-answering supercomputer that gained fame after challenging the best human players on Jeopardy! in 2011. We compared this solution to a series of traditional machine learning-based natural language processing techniques that utilize a term-document frequency matrix. Each classifier was trained with 1240 MRI protocols plus their respective clinical indications and validated with a test set of 280. Ground truth of contrast assignment was obtained from the clinical record. For evaluation of inter-reader agreement, a blinded second reader radiologist analyzed all cases and determined contrast assignment based on only the free-text clinical indication. In the test set, Watson demonstrated overall accuracy of 83.2% when compared to the original protocol. This was similar to the overall accuracy of 80.2% achieved by an ensemble of eight traditional machine learning algorithms based on a term-document matrix. When compared to the second reader’s contrast assignment, Watson achieved 88.6% agreement. When evaluating only the subset of cases where the original protocol and second reader were concordant (n = 251), agreement climbed further to 90.0%. The classifier was relatively robust to spelling and grammatical errors, which were frequent. Implementation of this automated MR contrast determination system as a clinical decision support tool may save considerable time and effort of the radiologist while potentially decreasing error rates, and require no change in order entry or workflow. © 2017 Society for Imaging Informatics in Medicine","Artificial intelligence; Deep learning; IBM Watson; Imaging protocol; Machine learning; Natural language processing (NLP); Quality improvement; Workflow efficiency","Artificial intelligence; Classification (of information); Decision support systems; Deep learning; Efficiency; Errors; Learning systems; Magnetic levitation vehicles; Magnetic resonance imaging; Musculoskeletal system; Natural language processing systems; Supercomputers; Automatic determination; Classification system; Clinical decision support; IBM Watson; Imaging protocol; Improving efficiency; Intravenous contrast; Quality improvement; Learning algorithms",2-s2.0-85029578434
"Crocetta T.B., de Araújo L.V., Guarnieri R., Massetti T., Ferreira F.H.I.B., de Abreu L.C., de Mello Monteiro C.B.","Virtual reality software package for implementing motor learning and rehabilitation experiments",2017,"Virtual Reality",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029584434&doi=10.1007%2fs10055-017-0323-2&partnerID=40&md5=0b3d669f2fe938f67b9fe8351cf59627","Virtual reality games for rehabilitation are attracting increasing growth. In particular, there is a demand for games that allow therapists to identify an individual’s difficulties and customize the control of variables, such as speed, size, distance, as well as visual and auditory feedback. This study presents and describes a virtual reality software package (Bridge Games) to promote rehabilitation of individuals living with disabilities and highlights preliminary researches of its use for implementing motor learning and rehabilitation. First, the study presents seven games in the software package that can be chosen by the rehabilitation team, considering the patient’s needs. All game characteristics are described including name, function presentation, objective and valuable measurements for rehabilitation. Second, preliminary results illustrate some applications of two games, considering 343 people with various disabilities and health status. Based on the results, in the Coincident Timing game, there was a main effect of movement sensor type (in this instance the most functional device was the keyboard when compared with Kinect and touch screen) on average time reached by sample analyzed, F(2, 225) = 4.42, p < 0.05. Similarly, in the Challenge! game, a main effect was found for movement sensor type. However, in this case, touch screen provided better performance than Kinect and Leap Motion, F(2, 709) = 5.90, p < 0.01. Thus, Bridge Games is a possible software game to quantify motor learning. Moreover, the findings suggest that motor skills might be practiced differently depending on the environmental interface in which the game may be used. © 2017 Springer-Verlag London Ltd.","Man–machine interface; Rehabilitation games; Virtual reality rehabilitation","E-learning; Software packages; Touch screens; Virtual reality; Auditory feedback; Environmental interfaces; Functional devices; Health status; Machine interfaces; Motor learning; Movement sensors; Software games; Patient rehabilitation",2-s2.0-85029584434
"Bae Y., Kumarasamy K., Ali I.M., Korfiatis P., Akkus Z., Erickson B.J.","Differences Between Schizophrenic and Normal Subjects Using Network Properties from fMRI",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029583461&doi=10.1007%2fs10278-017-0020-4&partnerID=40&md5=6673928ff85464d5a43f09f18e39ccf7","Schizophrenia has been proposed to result from impairment of functional connectivity. We aimed to use machine learning to distinguish schizophrenic subjects from normal controls using a publicly available functional MRI (fMRI) data set. Global and local parameters of functional connectivity were extracted for classification. We found decreased global and local network connectivity in subjects with schizophrenia, particularly in the anterior right cingulate cortex, the superior right temporal region, and the inferior left parietal region as compared to healthy subjects. Using support vector machine and 10-fold cross-validation, nine features reached 92.1% prediction accuracy, respectively. Our results suggest that there are significant differences between control and schizophrenic subjects based on regional brain activity detected with fMRI. © 2017 Society for Imaging Informatics in Medicine","fMRI; Machine learning; Network properties; Schizophrenia","Artificial intelligence; Brain; Diseases; 10-fold cross-validation; fMRI; Functional connectivity; Functional MRI (fMRI); Local parameters; Network properties; Prediction accuracy; Schizophrenia; Learning systems",2-s2.0-85029583461
"Jiang Q., Christakos G.","Space-time mapping of ground-level PM2.5 and NO2 concentrations in heavily polluted northern China during winter using the Bayesian maximum entropy technique with satellite data",2017,"Air Quality, Atmosphere and Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029581957&doi=10.1007%2fs11869-017-0514-8&partnerID=40&md5=fd86f242a4b4c3ad39c7bee95c8a427b","The accurate and informative space-time mapping of air pollutants is a crucial component of many human exposure studies. In the present work, space-time maps of daily distributions of PM2.5 and NO2 concentrations were generated in the severely polluted northern China region using the Bayesian maximum entropy (BME) method. This method can incorporate hard PM2.5 and NO2 data (obtained at ground-level monitoring sites), and various kinds of soft (uncertain) data, including satellite data processed in terms of machine learning techniques, meteorological variables, and geographical predictors. The BME maps of space-time PM2.5 and NO2 concentrations over northern China generated during the winter season (when severe haze episodes occur frequently) were realistic and informative. As regards their numerical accuracy, for the space-time PM2.5 estimates, the tenfold cross-validation R2 and the RMSE were, respectively, 0.86 and 14.37 μg/m3; for the space-time NO2 estimates, the R2 and RMSE values were, respectively, 0.85 and 6.93 μg/m3. Lastly, it was shown that the BME method performed better than the mainstream spatiotemporal ordinary kriging technique in terms of the higher R2 values of both the predicted PM2.5 and NO2 concentration maps. © 2017 Springer Science+Business Media B.V.","Bayesian maximum entropy; Machine learning; NO2; PM2.5; Space-time mapping",,2-s2.0-85029581957
"Gale T.V., Horton T.M., Grant D.S., Garry R.F.","Metabolomics analyses identify platelet activating factors and heme breakdown products as Lassa fever biomarkers",2017,"PLoS Neglected Tropical Diseases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030695716&doi=10.1371%2fjournal.pntd.0005943&partnerID=40&md5=8dbd540e76fa1712bfa0d33c4ee25600","Lassa fever afflicts tens of thousands of people in West Africa annually. The rapid progression of patients from febrile illness to fulminant syndrome and death provides incentive for development of clinical prognostic markers that can guide case management. The small molecule profile of serum from febrile patients triaged to the Viral Hemorrhagic Fever Ward at Kenema Government Hospital in Sierra Leone was assessed using untargeted Ultra High Performance Liquid Chromatography Mass Spectrometry. Physiological dysregulation resulting from Lassa virus (LASV) infection occurs at the small molecule level. Effects of LASV infection on pathways mediating blood coagulation, and lipid, amino acid, nucleic acid metabolism are manifest in changes in the levels of numerous metabolites in the circulation. Several compounds, including platelet activating factor (PAF), PAF-like molecules and products of heme breakdown emerged as candidates that may prove useful in diagnostic assays to inform better care of Lassa fever patients. © 2017 Gale et al.",,"heme; immunoglobulin G; immunoglobulin M; methylinosine; peptides and proteins; ribavirin; thrombocyte activating factor; unclassified drug; urobilinogen; biological marker; heme; immunoglobulin M; thrombocyte activating factor; virus antibody; virus antigen; virus RNA; adult; amino acid metabolism; Article; blood clotting; electrospray mass spectrometry; enzyme linked immunosorbent assay; female; human; Lassa fever; Lassa virus; lipid metabolism; liquid chromatography-mass spectrometry; machine learning; male; metabolome; metabolomics; nucleic acid metabolism; principal component analysis; protein degradation; receiver operating characteristic; reverse transcription polymerase chain reaction; sensitivity and specificity; ultra performance liquid chromatography; adolescent; Africa; blood; chemistry; immunology; isolation and purification; Lassa fever; mass spectrometry; metabolism; metabolomics; physiology; procedures; Sierra Leone; young adult; Adolescent; Adult; Africa, Western; Antibodies, Viral; Antigens, Viral; Biomarkers; Female; Heme; Humans; Immunoglobulin M; Lassa Fever; Lassa virus; Male; Mass Spectrometry; Metabolomics; Platelet Activating Factor; RNA, Viral; Sierra Leone; Young Adult",2-s2.0-85030695716
"Lupetti M.L., Yao Y., Mi H., Germak C.","Design for children's playful learning with robots",2017,"Future Internet",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030147855&doi=10.3390%2ffi9030052&partnerID=40&md5=2d9d5343237a1e8937a04814d177ecd0","This article presents an investigation of the implications of designing for children's playful learning with robots. This study was carried out by adopting a Research through Design approach that resulted in the development of a novel low-anthropomorphic robot called Shybo. The article reports the main phases of the project: the preliminary and exploratory research that was carried out to define a list of design requirements; the design of the robot and its supplementary materials for carrying out playful learning experiences; and the evaluation of the project that involved both parents and children. The robot, in fact, was finally tested as part of a two-hour experience that engaged children in activities related to the associations between sounds and colours. The article presents and discusses the results of this evaluation to point out positive aspects of the experience, emerging issues and hints for future works. These are documented to share lessons learned that might be supportive of the general development of children's playful learning and cognitive experiences with robots. © 2017 by the authors.","Child-robot interaction; Edutainment robotics; Participatory design; Play; Research through design; Robot design","Anthropomorphic robots; Robots; Child-robot interactions; Edutainment; Participatory design; Play; Robot designs; Machine design",2-s2.0-85030147855
"Batmanov K., Wang J.","Predicting variation of DNA shape preferences in protein-DNA interaction in cancer cells with a new biophysical model",2017,"Genes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030539666&doi=10.3390%2fgenes8090233&partnerID=40&md5=795ca9e72c1fae85ad38875519406860","DNA shape readout is an important mechanism of transcription factor target site recognition, in addition to the sequence readout. Several machine learning-based models of transcription factor–DNA interactions, considering DNA shape features, have been developed in recent years. Here, we present a new biophysical model of protein–DNA interactions by integrating the DNA shape properties. It is based on the neighbor dinucleotide dependency model BayesPI2, where new parameters are restricted to a subspace spanned by the dinucleotide form of DNA shape features. This allows a biophysical interpretation of the new parameters as a position-dependent preference towards specific DNA shape features. Using the new model, we explore the variation of DNA shape preferences in several transcription factors across various cancer cell lines and cellular conditions. The results reveal that there are DNA shape variations at FOXA1 (Forkhead Box Protein A1) binding sites in steroid-treated MCF7 cells. The new biophysical model is useful for elucidating the finer details of transcription factor–DNA interaction, as well as for predicting cancer mutation effects in the future. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","DNA shape; Protein–DNA interaction; Transcription factors","DNA; hepatocyte nuclear factor 3alpha; transcription factor; Article; binding affinity; binding site; biophysics; cancer cell; controlled study; DNA shape; DNA structure; MCF-7 cell line; prediction; protein DNA interaction",2-s2.0-85030539666
"Min H., Mobahi H., Irvin K., Avramovic S., Wojtusiak J.","Predicting activities of daily living for cancer patients using an ontology-guided machine learning methodology",2017,"Journal of Biomedical Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029495342&doi=10.1186%2fs13326-017-0149-6&partnerID=40&md5=06b0a8f424e0a7560094d2ff3d7f50c2","Background: Bio-ontologies are becoming increasingly important in knowledge representation and in the machine learning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to the SEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities of daily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to ""understand"" biomedical data. Results: This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods were applied to create predictive models for ADL disabilities for the first year after a patient's cancer diagnosis. The first method is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods for reasoning with ontologies. The models showed that a patient's race, ethnicity, smoking preference, treatment plan and tumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performance levels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADL performance levels (P < 0.1) than methods without ontologies. Conclusions: This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for ML algorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rule learning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. The ontology-guided ML method achieved better performance than the method without ontologies. The presented method can also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledge and consistency with existing clinical expertise is critical. © 2017 The Author(s).","Activities of daily living; Bio-ontologies; Machine learning; Quality of life; SEER-MHOS",,2-s2.0-85029495342
