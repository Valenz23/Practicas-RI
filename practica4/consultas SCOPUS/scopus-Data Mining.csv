Authors,Title,Year,Source title,Cited by,Link,Abstract,Author Keywords,Index Keywords,EID
"Liu W., Yang X., Tao D., Cheng J., Tang Y.","Multiview dimension reduction via Hessian multiset canonical correlations",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028972232&doi=10.1016%2fj.inffus.2017.09.001&partnerID=40&md5=d0937224860436347425758b6f8fb2f9","Canonical correlation analysis (CCA) is a main technique of linear subspace approach for two-view dimension reduction by finding basis vectors with maximum correlation between the pair of variables. The shortcoming of the traditional CCA lies that it only handles data represented by two-view features and cannot reveal the nonlinear correlation relationship. In recent years, many variant algorithms have been developed to extend the capability of CCA such as discriminative CCA, sparse CCA, kernel CCA, locality preserving CCA and multiset canonical correlation analysis (MCCA). One representative work is Laplacian multiset canonical correlations (LapMCC) that employs graph Laplacian to exploit the nonlinear correlation information for multiview high-dimensional data. However, it possibly leads to poor extrapolating power because Laplacian regularization biases the solution towards a constant function. In this paper, we present Hessian multiset canonical correlations (HesMCC) for multiview dimension reduction. Hessian can properly exploit the intrinsic local geometry of the data manifold in contrast to Laplacian. HesMCC takes the advantage of Hessian and provides superior extrapolating capability and finally leverage the performance. Extensive experiments on several popular datasets for handwritten digits classification, face classification and object classification validate the effectiveness of the proposed HesMCC algorithm by comparing it with baseline algorithms including TCCA, KMUDA, MCCA and LapMCC. © 2017 Elsevier B.V.","Canonical correlation analysis; Dimension reduction; Hessian; Multiview","Character recognition; Classification (of information); Clustering algorithms; Correlation methods; Data mining; Extrapolation; Laplace transforms; Canonical correlation analysis; Canonical correlations; Dimension reduction; Hessian; Laplacian regularizations; Multi-views; Multiset canonical correlation analysis; Non-linear correlations; Data reduction",2-s2.0-85028972232
"Hassan M.M., Huda S., Yearwood J., Jelinek H.F., Almogren A.","Multistage fusion approaches based on a generative model and multivariate exponentially weighted moving average for diagnosis of cardiovascular autonomic nerve dysfunction",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028998217&doi=10.1016%2fj.inffus.2017.08.004&partnerID=40&md5=99262c3acd2b27e264e8df98d507832c","Like many medical diagnoses, clinical decision support system (CDSS) is essential to diagnose the cardiovascular autonomic neuropathy (CAN). However, diagnosis of CAN using the traditional ‘Ewing battery test’ becomes very difficult due to the inherent imbalanced and incompleteness condition in the collected clinical data. This influences the health professionals to investigate other related diagnostic reports of patients, including Electrocardiogram (ECG) data from ECG sensors, blood chemistry, podiatry and endocrinology features. However, additional components increase the dimensionality of the feature set as well as its heterogeneity and modality in the clinical data which may limit the applications of traditional data mining approaches for an accurate diagnosis of CAN in the CDSS. To address the aforementioned problem, in this paper, we have proposed, a novel multistage fusion approach based on a generative model and a statistical process control (SPC) technique to diagnose CAN more accurately. The proposed approach develops two different generative models by using a shared and a separated Independent Component Analysis (ICA) to overcome the incompleteness and modality of the data. Due to the heterogeneous and non-normality features, statistical correlations and multivariate control limits in relation to the CAN diagnosis parameters are determined by fusioning of a series of exponentially weighted moving average (MEWMA) control processes. Fusioned features from both component analyses and SPC are applied in an ensemble classification system. The proposed multistage fusion approach is experimentally verified to justify its performance by using a large dataset collected from the diabetes screening research initiative (DiScRi) project at Charles Sturt University, NSW, Australia. Our comprehensive experimental results show that the proposed fusion approach performs better than the standard classifier for both ‘Ewing’ feature set and ‘Ewing and additional feature set’ with significant improvement in accuracy. © 2017 Elsevier B.V.","Autonomic nerve dysfunction classification; Blind source separation; Fusion of features and decisions; Fusion of multiple statistical process control techniques; Multivariate exponentially weighted moving average","Artificial intelligence; Blind source separation; Data mining; Decision support systems; Diagnosis; Electrocardiography; Independent component analysis; Multivariable control systems; Process control; Charles sturt universities; Clinical decision support systems; Ensemble classification; Exponentially weighted moving average; Fusion of features; Independent component analysis(ICA); Multivariate exponentially weighted moving averages; Statistical process controls (SPC); Statistical process control",2-s2.0-85028998217
"Piri S., Delen D., Liu T., Paiva W.","Development of a new metric to identify rare patterns in association analysis: The case of analyzing diabetes complications",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032491288&doi=10.1016%2fj.eswa.2017.09.061&partnerID=40&md5=de990003e919c0213bdc92f5774bd86b","Diabetes, one of the most serious and fast growing chronic health conditions, often leads to other serious complications such as neurological, renal, ophthalmic, and heart diseases. Research has shown that more than 85% of diabetic patients develop at least one of these complications. Therefore, studying comorbidities among diabetic patients using association analysis is a worthy research endeavor. Association analysis is a well-known data mining method that aims to reveal the association/affinity patterns/rules among various items (objects or events) that occur together. One of the most critical problems in association analysis is the difficulty with the identification of rare items/patterns. In ordinary association analysis, specifying a large minimum-support leads to not discovering rare rules, while setting a small minimum-support leads to over-generating rules that may not be strong and beneficial. In this study, we propose a new assessment metric, called adjusted_support, to address this problem. Applying this new metric can retrieve rare patterns without over-generating association rules. To test the proposed metric, we extracted data from a large and feature-rich electronic medical records data warehouse and performed association analysis on the resultant data set that included 492,025 unique patients diagnosed with diabetes and related complications. By applying adjusted_support, we discovered interesting associations among diabetes complications such as neurological manifestations with diabetic arthropathy and gastroparesis; renal manifestations with retinopathy; gastroparesis with ketoacidosis and retinopathy; and skin complications with hyperglycemia, peripheral circulatory disorder, heart disease, and neurological manifestations. We also performed association analysis in various demographic groups at more granular levels. Besides association analysis, we also analyzed the comorbidity situation among different demographic groups of diabetics. Finally, we studied and compared the prevalence of diabetes complications in every demographic group of patients. © 2017 Elsevier Ltd","Adjusted_support; Association rule mining; Comorbidity; Data mining; Diabetes; Rare-pattern identification","Association rules; Cardiology; Data warehouses; Diseases; Epidemiology; Eye protection; Ketones; Medical computing; Medical problems; Neurology; Patient monitoring; Population statistics; Statistical tests; Adjusted_support; Association analysis; Circulatory disorders; Co morbidities; Data mining methods; Electronic medical record; Neurological manifestations; Pattern identification; Data mining",2-s2.0-85032491288
"Cai G., Lee K., Lee I.","Itinerary recommender system with semantic trajectory pattern mining from geo-tagged photos",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032280838&doi=10.1016%2fj.eswa.2017.10.049&partnerID=40&md5=a97aa384397357fa4e6aa36703fc5cd5","A large number of geo-tagged photos become available online due to the advances in geo-tagging services and Web technologies. These geo-tagged photos are indicative of photo-takers’ trails and movements, and have been used for mining people movements and trajectory patterns. These geo-tagged photos are inherently spatio-temporal, sequential and implicitly containing aspatial semantics. and recommender systems are collaborative filtering based. There have been some studies to build itinerary recommender systems from these geo-tagged photos, but they fail to consider these dimensions and share some common drawbacks, especially lacking aspatial semantics or temporal information. This paper proposes an itinerary recommender system with semantic trajectory pattern mining from geo-tagged photos by discovering sequential points-of-interest with temporal information from other users’ visiting sequences and preferences. Our system considers spatio-temporal, sequential, and aspatial semantics dimensions, and also takes into account user-specified preferences and constraints to customise their requests. It generates a set of customised and targeted semantic-level itineraries meeting the user specified constraints. The proposed method generates these semantic itineraries from historic people's movements by mining frequent travel patterns from geo-tagged photos. Experimental results demonstrate the informativeness, efficiency and effectiveness of our proposed method over traditional approaches. © 2017","Geotagged photos; Recommender systems; Semantics; Trajectory pattern mining","Collaborative filtering; Data mining; Semantics; Trajectories; Geotagged photos; Informative ness; Points of interest; Semantic trajectories; Temporal information; Traditional approaches; Trajectory pattern; User-specified constraints; Recommender systems",2-s2.0-85032280838
"Abu-Aisheh Z., Raveaux R., Ramel J.-Y., Martineau P.","A parallel graph edit distance algorithm",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032512961&doi=10.1016%2fj.eswa.2017.10.043&partnerID=40&md5=e0955d97039a6d8c020baa630dfa70fc","Graph edit distance (GED) has emerged as a powerful and flexible graph matching paradigm that can be used to address different tasks in pattern recognition, machine learning, and data mining. GED is an error-tolerant graph matching problem which consists in minimizing the cost of the sequence that transforms a graph into another by means of edit operations. Edit operations are deletion, insertion and substitution of vertices and edges. Each vertex/edge operation has its associated cost defined in the vertex/edge cost function. Unfortunately, Unfortunately, the GED problem is NP-hard. The question of elaborating fast and precise algorithms is of first interest. In this paper, a parallel algorithm for exact GED computation is proposed. Our proposal is based on a branch-and-bound algorithm coupled with a load balancing strategy. Parallel threads run a branch-and-bound algorithm to explore the solution space and to discard misleading partial solutions. In the mean time, the load balancing scheme ensures that no thread remains idle. Experiments on 4 publicly available datasets empirically demonstrated that under time constraints our proposal can drastically improve a sequential approach and a naive parallel approach. Our proposal was compared to 6 other methods and provided more precise solutions while requiring a low memory usage. © 2017 Elsevier Ltd","Graph edit distance; Graph matching; Load balancing; Parallel computing; Pattern recognition","Branch and bound method; Cost functions; Costs; Data mining; Learning systems; Parallel processing systems; Pattern recognition; Resource allocation; Branch-and-bound algorithms; Error-tolerant graph matching; Graph edit distance; Graph matchings; Load balancing strategy; Load-balancing schemes; Precise solutions; Sequential approach; Pattern matching",2-s2.0-85032512961
"Andrzejewski W., Boinski P.","Efficient spatial co-location pattern mining on multiple GPUs",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032276569&doi=10.1016%2fj.eswa.2017.10.025&partnerID=40&md5=b09bf116d653845f6af4c0691423a805","In this paper, we investigate Co-location Pattern Mining (CPM) from big spatial datasets. CPM consists in searching for types of objects that are frequently located together in a spatial neighborhood. Knowledge about such patterns is very important in fields like biology, environmental sciences, epidemiology etc. However, CPM is computationally challenging, mainly due to the large number of pattern instances hidden in spatial data. In this work, we propose a new solution that can utilize the power of multiple GPUs to increase the performance of CPM. The proposed solution is also capable of coping with the GPU memory limits by dividing the work into multiple packages and compressing internal data structures. Experiments performed on large synthetic and real-world datasets prove that we can achieve an order of magnitude speedups in comparison to the efficient multithreaded CPU implementation. Our solution can greatly improve the performance of data analysis, using widely available and energy efficient graphics cards. As a result, CPM in large datasets is more viable for university researchers as well as smaller companies and organizations. © 2017 Elsevier Ltd","Co-location pattern mining; Compression; Data mining; GPGPU; Parallel computing; Spatial co-location","Compaction; Data compression; Energy efficiency; Location; Parallel processing systems; Program processors; Co-location patterns; Colocations; Environmental science; GPGPU; Real-world datasets; Spatial co-location patterns; Spatial neighborhoods; University researchers; Data mining",2-s2.0-85032276569
"Marshall J.W., Schmitt-Kopplin P., Schuetz N., Moritz F., Roullier-Gall C., Uhl J., Colyer A., Jones L.L., Rychlik M., Taylor A.J.","Monitoring chemical changes during food sterilisation using ultrahigh resolution mass spectrometry",2018,"Food Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029511083&doi=10.1016%2fj.foodchem.2017.09.074&partnerID=40&md5=5d80fed71f2a5c48aa7e0a4653dd0033","Sterilised food products undergo chemical changes during processing that ultimately determine the product quality. To provide detailed information on the chemistry of each stage of a pet-food sterilisation process, a laboratory-scale system was developed, which allowed sampling under the high temperatures and pressures associated with sterilisation. Products from the laboratory-scale system were representative of the factory process. Sample extracts were analysed by Fourier Transform-Ion Cyclotron Resonance-Mass Spectrometry (FT-ICR-MS), which delivered the molecular formulae and ion intensities of the compounds present. Data were examined to determine the coverage of this method, the degree of chemical change occurring during pet food thermal processing, and the level of identification possible with FT-ICR-MS. Data visualisation and statistical analysis identified significant chemical changes in pet food as a result of processing, and allowed tentative identification of the compounds involved. Insights generated using FT-ICR-MS analysis can be confirmed and further explored using conventional, targeted analyses. © 2017 Elsevier Ltd","Data mining FT-ICR-MS; Data visualisation; Maillard reaction; Van Krevelen","Chemical analysis; Chemical compounds; Data mining; Data visualization; Food products; Mass spectrometry; Spectrometry; Visualization; Fourier transform ion cyclotron resonance mass spectrometry; FT-ICR MS; High temperature; Maillard reaction; Molecular formulae; Sterilisation process; Ultrahigh resolution mass spectrometries; Van Krevelen; Thermal processing (foods); animal food; Article; data mining; food processing; food sterilization; Fourier transformation; glycation; ion cyclotron resonance mass spectrometry",2-s2.0-85029511083
"Tommasel A., Godoy D.","A Social-aware online short-text feature selection technique for social media",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019594702&doi=10.1016%2fj.inffus.2017.05.003&partnerID=40&md5=7314a51a50b4631b275c9e90879de89a","Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques. © 2017 Elsevier B.V.","Classification; Micro-blogging communities; Online feature selection","Classification (of information); Clustering algorithms; Data mining; Learning systems; Social networking (online); Text processing; Classification results; High dimensional data; Micro blogging; Online feature selection; Selection techniques; Social applications; Textual information; Topological information; Feature extraction",2-s2.0-85019594702
"Hu L., Gao W., Zhao K., Zhang P., Wang F.","Feature selection considering two types of feature relevancy and feature interdependency",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032290189&doi=10.1016%2fj.eswa.2017.10.016&partnerID=40&md5=130152106c5e16c0a042e1886690e105","Feature selection based on information theory, which is used to select a group of the most informative features, has extensive application fields such as machine learning, data mining and natural language processing. However, numerous previous methods suffer from two common defects. (1) Feature relevancy is defined without distinguishing candidate feature relevancy and selected feature relevancy. (2) Some interdependent features may be misinterpreted as redundant features. In this study, we propose a feature selection method named Dynamic Relevance and Joint Mutual Information Maximization (DRJMIM) to address these two defects. DRJMIM includes four stages. First, the relevancy is divided into two categories: candidate feature relevancy and selected feature relevancy. Second, according to candidate feature relevancy that is joint mutual information, some redundant features are selected. Third, the redundant features are combined with a dynamic weight to reduce the selection possibility of true redundant features while increasing the false ones. Finally, the most informative and interdependent features are selected and true redundant features are eliminated simultaneously. Furthermore, our method is compared with five competitive feature selection methods on 12 publicly available data sets. The classification results show that DRJMIM performs better than other five methods. Its statistical significance is verified by a paired two-tailed t-test. Meanwhile, DRJMIM obtains few number of selected features when it achieves the highest classification accuracy. © 2017 Elsevier Ltd","Feature interdependency; Feature relevancy; Feature selection; Information theory","Classification (of information); Data mining; Defects; Information theory; Learning algorithms; Learning systems; Natural language processing systems; Application fields; Classification accuracy; Classification results; Feature interdependency; Feature relevancy; Feature selection methods; Joint mutual informations; Statistical significance; Feature extraction",2-s2.0-85032290189
"Yadav D., Chowdary C.R.","OOIMASP: Origin based association rule mining with order independent mostly associated sequential patterns",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031746647&doi=10.1016%2fj.eswa.2017.10.015&partnerID=40&md5=8ed2953c4ec9732643f69181ed942a77","Efficient mining of association rules on a transaction dataset is an interesting and a challenging problem. The state-of-the-art MASP algorithm is dependent on the order of items in the transaction. We propose OOIMASP algorithm, which has two novel properties- 1) order independence and 2) it takes into consideration the origin of items to calculate unbiased support and unbiased confidence values. Order dependence is one of the drawbacks of MASP. OOIMASP addresses this issue by rearranging the items in transactions using a greedy frequency based approach. We compare the performance of our system with MASP on five synthetic data sets and three public data sets. The results show that our proposed approach outperforms the MASP in both the comparison metrics, i.e., the number of association rules generated and the length of the longest association rule. Both these metrics are important to evaluate the performance of an algorithm. On an average, OOIMASP algorithm generates 632% longer rules and 457% more association rules than MASP algorithm. The disadvantage of the proposed algorithm is, it requires more computational resources in terms of time, approximately 5 times more than MASP. We claim that the extra information extracted using our method compensates for the increase in time complexity as compared to MASP. The proposed method produces multiple trees which can be very useful in the visual analysis of data. © 2017 Elsevier Ltd","Association rule mining; Mostly associated sequential patterns; Unbiased confidence; Unbiased support","Association rules; Information analysis; Comparison metrics; Computational resources; Frequency-based approaches; Mining of association rules; Order independents; Sequential patterns; Synthetic datasets; Unbiased confidence; Trees (mathematics)",2-s2.0-85031746647
"Wan C., Zhu Y., Yu J., Shen Y.","SMOPAT: Mining semantic mobility patterns from trajectories of private vehicles",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032815148&doi=10.1016%2fj.ins.2017.10.043&partnerID=40&md5=b08e5c0e7d7ac3252518a37221d0e0c1","With the increasing use of private vehicles with positioning services, GPS trajectory data of vehicles has become one of the major sources of big data about urban life. Existing studies on mobility pattern mining from trajectories share a common limitation, i.e., they fail to capture the semantics of trajectories. Automatic derivation of semantic information for every trajectory is a challenging task. In this paper, we propose an approach, called SMOPAT (Semantic MObility PATterns), for mining spatial-temporal semantic mobility patterns from trajectories of private vehicles. We design a probabilistic generative model with latent variables to characterize the semantic mobility of vehicles. Based on the model, SMOPAT labels each location in a trajectory with a visit purpose by using a polynomial-time dynamic programming algorithm. It then employs an efficient algorithm to find the most frequent semantic mobility patterns. We evaluate our approach on a large data set of real trajectories of private vehicles spanning a time duration of over ten months with 114 million records in Shanghai, China. The experimental results show that our approach produces meaningful patterns and outperforms the two competing methods in terms of diversity, coherence, and coverage. © 2017 Elsevier Inc.","Private vehicles; Semantic mobility pattern; Trajectory","Dynamic programming; Polynomial approximation; Semantics; Trajectories; Vehicles; Automatic derivation; Gps trajectories; Mobility pattern; Polynomial-time dynamic programming; Private vehicles; Real trajectories; Semantic information; Spatial temporals; Big data",2-s2.0-85032815148
"de Sá C.R., Azevedo P., Soares C., Jorge A.M., Knobbe A.","Preference rules for label ranking: Mining patterns in multi-target relations",2018,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024868885&doi=10.1016%2fj.inffus.2017.07.001&partnerID=40&md5=e208f5bf1b21fcce7224097fa6a2b6fb","In this paper, we investigate two variants of association rules for preference data, Label Ranking Association Rules and Pairwise Association Rules. Label Ranking Association Rules (LRAR) are the equivalent of Class Association Rules (CAR) for the Label Ranking task. In CAR, the consequent is a single class, to which the example is expected to belong to. In LRAR, the consequent is a ranking of the labels. The generation of LRAR requires special support and confidence measures to assess the similarity of rankings. In this work, we carry out a sensitivity analysis of these similarity-based measures. We want to understand which datasets benefit more from such measures and which parameters have more influence in the accuracy of the model. Furthermore, we propose an alternative type of rules, the Pairwise Association Rules (PAR), which are defined as association rules with a set of pairwise preferences in the consequent. While PAR can be used both as descriptive and predictive models, they are essentially descriptive models. Experimental results show the potential of both approaches. © 2017 Elsevier B.V.","Association rules; Label ranking; Pairwise comparisons","Equivalence classes; Sensitivity analysis; Class association rules; Descriptive Model; Label rankings; Multi-targets; Pair-wise comparison; Predictive models; Preference data; Support and confidence; Association rules",2-s2.0-85024868885
"Wodecki J., Michalak A., Zimroz R.","Optimal filter design with progressive genetic algorithm for local damage detection in rolling bearings",2018,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032878036&doi=10.1016%2fj.ymssp.2017.09.008&partnerID=40&md5=474e2b1d129a81c5557b9fe3fc4b2385","Harsh industrial conditions present in underground mining cause a lot of difficulties for local damage detection in heavy-duty machinery. For vibration signals one of the most intuitive approaches of obtaining signal with expected properties, such as clearly visible informative features, is prefiltration with appropriately prepared filter. Design of such filter is very broad field of research on its own. In this paper authors propose a novel approach to dedicated optimal filter design using progressive genetic algorithm. Presented method is fully data-driven and requires no prior knowledge of the signal. It has been tested against a set of real and simulated data. Effectiveness of operation has been proven for both healthy and damaged case. Termination criterion for evolution process was developed, and diagnostic decision making feature has been proposed for final result determinance. © 2017 Elsevier Ltd","Digital filter; Genetic algorithm; Local damage; Vibration","Bandpass filters; Bearings (machine parts); Decision making; Digital filters; Filtration; Genetic algorithms; Machinery; Roller bearings; Diagnostic decision makings; Evolution process; Industrial conditions; Local damage; Optimal filter design; Termination criteria; Underground mining; Vibration; Damage detection",2-s2.0-85032878036
"Nie S., Wang Z., Pujia W., Nie Y., Lu P.","Big data prediction of durations for online collective actions based on peak's timing",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032256845&doi=10.1016%2fj.physa.2017.09.059&partnerID=40&md5=cd47d2a00f843003cb974a12603608ca","Peak Model states that each collective action has a life circle, which contains four periods of “prepare”, “outbreak”, “peak”, and “vanish” and the peak determines the max energy and the whole process. The peak model's re-simulation indicates that there seems to be a stable ratio between the peak's timing (TP) and the total span (T) or duration of collective actions, which needs further validations through empirical data of collective actions. Therefore, the daily big data of online collective actions is applied to validate the model; and the key is to check the ratio between peak's timing and the total span. The big data is obtained from online data recording & mining of websites. It is verified by the empirical big data that there is a stable ratio between TP and T; furthermore, it seems to be normally distributed. This rule holds for both the general cases and the sub-types of collective actions. Given the distribution of the ratio, estimated probability density function can be obtained, and therefore the span can be predicted via the peak's timing. Under the scenario of big data, the instant span (how long the collective action lasts or when it ends) will be monitored and predicted in real-time. With denser data (Big Data), the estimation of the ratio's distribution gets more robust, and the prediction of collective actions’ spans or durations will be more accurate. © 2017 Elsevier B.V.","Big data; Collective actions; Peak's timing; Prediction; Ratio; Span","Data mining; Forecasting; Normal distribution; Probability density function; Probability distributions; Timing circuits; Collective action; Data prediction; Empirical data; Model state; Peak's timing; Ratio; Span; Whole process; Big data",2-s2.0-85032256845
"He W.","Improved block redundancy mining based reversible data hiding using multi-sub-blocking",2018,"Signal Processing: Image Communication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032684180&doi=10.1016%2fj.image.2017.10.007&partnerID=40&md5=5559cd99b8471f00f1b08227b36b37fb","In difference/prediction error expansion, an efficient capacity-distortion trade-off generally indicates excellence in embedding performance. From this point of view, existing schemes which generate difference/prediction error in block-by-block manner are far from efficient and uniform-blocking may be the main reason. To overcome this drawback, a novel multi-sub-blocking (MSB) strategy which enables three or even more various-sized blocks based embedding is proposed in this paper. After initial division, the dispersion of pixel values within block is predicted to identify exceptional block which may produce to-be-shifted pixels. Then further division would be inevitable for those blocks to obtain closer correlation of pixels within smaller block to achieve improved block redundancy mining. To verify the advantage of MSB, this work extends two existing RDH schemes by incorporating MSB. Experimental results demonstrate that the two proposed schemes both achieve improved embedding performance and outperform previous related state-of-the-art schemes. © 2017 Elsevier B.V.","Block redundancy mining; Multi-sub-blocking; Reversible data hiding","Economic and social effects; Redundancy; Steganography; Block by blocks; Block redundancy; Efficient capacities; Error expansion; Multi-sub-blocking; Pixel values; Reversible data hiding; State-of-the-art scheme; Pixels",2-s2.0-85032684180
"Mazurek P., Wagner J., Morawski R.Z.","Use of kinematic and mel-cepstrum-related features for fall detection based on data from infrared depth sensors",2018,"Biomedical Signal Processing and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029698638&doi=10.1016%2fj.bspc.2017.09.006&partnerID=40&md5=f2a78c3a275b24bcc3e9fc8dac4c743b","A methodology for acquisition and preprocessing of measurement data from infrared depth sensors, when applied for fall detection, combined with several approaches to the classification of those data, is proposed. Data processing is initiated with extraction of the silhouette from the depth image and estimation of the coordinates of the center of that silhouette. Next, two groups of features to be applied for a fall/non-fall classification are extracted: kinematic features (various statistics defined on the position, velocity and acceleration trajectories of the monitored person) and mel-cepstrum-related features (components of the mel-cepstrum obtained by means of an unconventional set of mel-filters). Finally, the utility of these features in fall detection is assessed using three classification algorithms − viz. support vector machine, artificial neural network, and naïve Bayes classifier − trained and tested on two datasets consisting of, respectively, 160 data sequences (representative of 80 falls and 80 other human behaviours) and 264 data sequences (representative of 132 falls and 132 other human behaviours). The application of the combination of the kinematic and mel-cepstrum-related features yields highly accurate classification results − all classifiers achieved, depending on the dataset, 98.6–100% and 93.9–97.7% sensitivity. Thus, infrared depth sensors can be promising tools for unobtrusive fall detection. They provide data which can be in various ways preprocessed to form a basis for reliable fall detection. Appropriate selection of the feature sets directly affects the reliability of unobtrusive monitoring systems, and − indirectly − the quality of life of the monitored persons. © 2017","Classification algorithms; Data acquisition; Data processing; Event detection; Infrared image sensors; Public healthcare; Sensor systems and applications","Behavioral research; Data acquisition; Data handling; Data mining; Data processing; Feature extraction; Image processing; Image retrieval; Infrared imaging; Kinematics; Neural networks; Social sciences; Bayes Classifier; Classification algorithm; Classification results; Event detection; Measurement data; Public healthcares; Sensor systems and applications; Unobtrusive monitoring; Classification (of information); acceleration; Article; artificial neural network; classification; classification algorithm; controlled study; data analysis; kinematics; near infrared imaging system; priority journal; sensitivity analysis; sensor; signal detection; support vector machine",2-s2.0-85029698638
"Giacalone M., Cusatelli C., Romano A., Buondonno A., Santarcangelo V.","Big Data and forensics: An innovative approach for a predictable jurisprudence",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032201353&doi=10.1016%2fj.ins.2017.10.036&partnerID=40&md5=52da63e1ab7059182d51c5b29177b7df","Nowadays, it is easy to trace a large amount of information on the web, to access documents and produce a digital storage. The current work is submitted as an introduction to an innovative system for the investigation about notoriety of web data which is based on the evaluation of judicial sentences and it is implemented to reduce the duration of all processes. This research also aims to open some new conjoint debates about the study and application of statistical and computational methods to web data on new forensics topics: text mining techniques enable us to obtain information which may be helpful to establish a statistical index in order to describe the quality and the efficiency in terms of law. It is also possible to develop an intelligent system about facts and judgments. © 2017 Elsevier Inc.","Big data; Efficiency in law; Literal text similarity; Quality in law; Semantic text similarity","Data mining; Digital storage; Efficiency; Electronic document exchange; Intelligent systems; Semantics; Innovative approaches; Innovative systems; Large amounts; Statistical indices; Study and applications; Text mining techniques; Text similarity; Web data; Big data",2-s2.0-85032201353
"Demšar J., Bosnić Z.","Detecting concept drift in data streams using model explanation",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030859587&doi=10.1016%2fj.eswa.2017.10.003&partnerID=40&md5=c39740f79f7c2432a84f58ffb58aa786","Learning from data streams (incremental learning) is increasingly attracting research focus due to many real-world streaming problems and due to many open challenges, among which is the detection of concept drift – a phenomenon when the data distribution changes and makes the current prediction model inaccurate or obsolete. Current state-of-the art detection methods can be roughly split into performance monitoring algorithms and distribution comparing algorithms. In this work we propose a novel concept drift detector that can be combined with an arbitrary classification algorithm. The proposed concept drift detector is based on computing multiple model explanations over time and observing the magnitudes of their changes. The model explanation is computed using a methodology that yields attribute-value contributions for prediction outcomes and thus provides insight into the model's decision-making process and enables its transparency. The evaluation has revealed that the methods surpass the baseline methods in terms of concept drift detection, accuracy, robustness and sensitivity. To even further augment interpretability, we visualized the detection of concept drift, enabling macro and micro views of the data. © 2017 Elsevier Ltd","Concept drift; Data stream; Explanation; Visualization","Data visualization; Decision making; Flow visualization; Classification algorithm; Comparing algorithm; Concept drifts; Data stream; Decision making process; Explanation; Incremental learning; Performance monitoring; Data mining",2-s2.0-85030859587
"Rekik R., Kallel I., Casillas J., Alimi A.M.","Assessing web sites quality: A systematic literature review by text and association rules mining",2018,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031735902&doi=10.1016%2fj.ijinfomgt.2017.06.007&partnerID=40&md5=73789b1ed7be471cf18ed82d68845c8a","Nowadays society is deeply affected by web content. A web site, regardless of its category, can provide or not for users their needs. To identify its strengths and weaknesses, a process of analyzing and assessing its quality, via some criteria, is necessary. Assessing web sites is considered as a Multiple Criteria Decision Making problem (MCDM), with a massive number of criteria; a reduction phase is needed. This paper presents, firstly a Systematic Literature Review (SLR) to identify the purposes of recent researches from the assessment and determine the affected categories; secondly, it proposes a process of collecting and extracting data (criteria featuring web sites) from a list of studies. Text mining is applied for this SLR to construct a dataset. Then, a method based on Apriori algorithm is assigned and implemented to find association rules between criteria and the category of the web site, and to get a set of frequent criteria. This paper also presents a review on soft computing assessing methods. It aims to help the research community to have a scope in existing research and to derive future developments. The obtained results motivate us to further probe datasets and association rule mining. © 2017 Elsevier Ltd","Assessing web sites; Association rules mining; Multiple criteria decision making; Systematic literature review; Text mining","Association rules; Decision making; Soft computing; Websites; Apriori algorithms; Association rules mining; Multiple criteria decision making; Multiple criteria decision-making problems; Research communities; Systematic literature review; Systematic literature review (SLR); Text mining; Data mining",2-s2.0-85031735902
"Zhang D., Lee K., Lee I.","Hierarchical trajectory clustering for spatio-temporal periodic pattern mining",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029718478&doi=10.1016%2fj.eswa.2017.09.040&partnerID=40&md5=a4bc1ceb160791672e8140d926b9c61b","Spatio-temporal periodic pattern mining is to find temporal regularities for interesting places. Many real world spatio-temporal phenomena present sequential and hierarchical nature. However, traditional spatio-temporal periodic pattern mining ignores the consideration of sequence, and fails to take into account inherent hierarchy. This paper proposes a hierarchical trajectory clustering based periodic pattern mining that overcomes the two common drawbacks from traditional approaches: hierarchical reference spots and consideration of sequence. We propose a new trajectory clustering algorithm which considers semantic spatio-temporal information such as direction, speed and time based on Traclus and present comparative experimental results with three popular clustering methods: Kernel function, Grid-based, and Traclus. We further extend the proposed trajectory clustering to hierarchical clustering with the use of the single linkage approach to generate a hierarchy of reference spots. Experimental results reveal various hierarchical periodic patterns, and demonstrate that our algorithm outperforms traditional reference spot detection algorithms. © 2017 Elsevier Ltd","Hierarchical trajectory clustering; Periodic pattern mining; Reference spots; Single-linkage; Traclus","Data mining; Semantics; Trajectories; Hierarchical trajectory; Periodic pattern; Reference spots; Single linkage; Traclus; Clustering algorithms",2-s2.0-85029718478
"Zhou Y., Wu J., Ji L., Yu Z., Lin K., Hao L.","Transient stability preventive control of power systems using chaotic particle swarm optimization combined with two-stage support vector machine",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031784844&doi=10.1016%2fj.epsr.2017.10.007&partnerID=40&md5=43b6d9505a3e9214500cc62b4fddc5dd","This paper presents a chaotic particle swarm optimization (CPSO) algorithm combined with data mining method for transient stability preventive control. The data mining method is utilized to approximate the security region considering transient stability. Therefore, the application effects of different input features and data-mining classifiers are compared first. Then, a two-stage support vector machine (SVM) approach is proposed to generate two models, including a linear SVM model with controllable features provides preventive adjustment rules, and a more accurate SVM model to approximate the actual security region. Finally, the CPSO in combination with the two-stage SVM is proposed to calculate the optimal preventive control strategies. Comprehensive studies are conducted on a 16-machine 68-bus system and 48-machine 140-bus system to verify the effectiveness. © 2017 Elsevier B.V.","Chaotic particle swarm optimization; Data mining; Preventive control; Support vector machine; Transient stability","Classification (of information); Data mining; Electric power system stability; Particle swarm optimization (PSO); Stability; Adjustment rules; Application effect; Chaotic particle swarm optimizations; Data mining methods; Input features; Linear SVM; Preventive control; Security region; Support vector machines",2-s2.0-85031784844
"Morel M., Achard C., Kulpa R., Dubuisson S.","Time-series averaging using constrained dynamic time warping with tolerance",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032284026&doi=10.1016%2fj.patcog.2017.08.015&partnerID=40&md5=02eb8fa47deadf93b6a27d2c2993f5ca","In this paper, we propose an innovative averaging of a set of time-series based on the Dynamic Time Warping (DTW). The DTW is widely used in data mining since it provides not only a similarity measure, but also a temporal alignment of time-series. However, its use is often restricted to the case of a pair of signals. In this paper, we propose to extend its application to a set of signals by providing an average time-series that opens a wide range of applications in data mining process. Starting with an existing well-established method called DBA (for DTW Barycenter Averaging), this paper points out its limitations and suggests an alternative based on a Constrained Dynamic Time Warping. Secondly, an innovative tolerance is added to take into account the admissible variability around the average signal. This new modeling of time-series is evaluated on a classification task applied on several datasets and results show that it outperforms state of the art methods. © 2017 Elsevier Ltd","Constrained DTW barycenter averaging; Dynamic time warping; Local constraints; Time-series averaging; Time-series classification","Classification (of information); Time series; Barycenters; Classification tasks; Constrained dynamics; Data mining process; Dynamic time warping; Local constraints; State-of-the-art methods; Time series classifications; Data mining",2-s2.0-85032284026
"Zhang C., Zhu L., Xu C., Lu R.","PPDP: An efficient and privacy-preserving disease prediction scheme in cloud-based e-Healthcare system",2018,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029543737&doi=10.1016%2fj.future.2017.09.002&partnerID=40&md5=5ce951a391e8b7b0d96a45ffa60df7e2","Disease prediction systems have played an important role in people's life, since predicting the risk of diseases is essential for people to lead a healthy life. The recent proliferation of data mining techniques has given rise to disease prediction systems. Specifically, with the vast amount of medical data generated every day, Single-Layer Perceptron can be utilized to obtain valuable information to construct a disease prediction system. Although the disease prediction system is quite promising, many challenges may limit it in practical use, including information security and prediction efficiency. In this paper, we propose an efficient and privacy-preserving disease prediction system, called PPDP. In PPDP, patients’ historical medical data are encrypted and outsourced to the cloud server, which can be further utilized to train prediction models by using Single-Layer Perceptron learning algorithm in a privacy-preserving way. The risk of diseases for new coming medical data can be computed based on the prediction models. In particular, PPDP builds on new medical data encryption, disease learning and disease prediction algorithms that novelly utilize random matrices. Security analysis indicates that PPDP offers a required level of privacy protection. In addition, real experiments on different datasets show that computation costs of data encryption, disease learning and disease prediction are several magnitudes lower than existing disease prediction schemes. © 2017 Elsevier B.V.","Cloud computing; Disease prediction; Privacy-preserving; Single-Layer Perceptron","Cloud computing; Cryptography; Data mining; Data privacy; Forecasting; Learning algorithms; Security of data; Computation costs; Prediction algorithms; Prediction schemes; Prediction systems; Privacy preserving; Privacy protection; Security analysis; Single layer perceptron; Medical information systems",2-s2.0-85029543737
"Yang J., Kim E., Hur M., Cho S., Han M., Seo I.","Knowledge extraction and visualization of digital design process",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030116887&doi=10.1016%2fj.eswa.2017.09.002&partnerID=40&md5=f7e99b67b7f84b50c0f9009135763ca9","After digitally designing components of vehicles, a design team creates a virtual manufacturing environment that resembles actual manufacturing facilities. During this digital pre-assembly process, a review team examines each component, and records its problems and requirements in part verification reports. Once these reports are delivered to specific design team responsible for each part, the design team can make appropriate adjustments to their designs. This digital pre-assembly process can evaluate and prevent flaws in design prior to actual manufacturing, improving production quality and reducing manufacturing cost. As these reports are written in free text form, they, however, are not fully utilized for understanding problems arising from the design process. This paper proposes a method of applying text mining techniques on verification reports to extract insights for quality improvement. In this paper, following three text mining approaches are proposed: (1) Extracting n-grams for text preprocessing and constructing domain ontology; (2) Extracting meaningful insights from text preprocessing; (3) Creating intuitive visual tools to understand the extracted insights. The proposed method is applied on approximately 140,000 reports, and is validated through the quality of the answers obtained for the questions posed by the domain experts. The proposed method successfully extracts useful information from the text database, and provides intuitive graphical interface, thereby satisfying the need of the domain experts. This paper proposes a systematic framework of transforming huge amount of raw text data into intuitive visualization. Through this framework, meaningful knowledge can be extracted, analyzed and shared to improve the quality of the products. Main contribution of our paper is that it proposes a framework for knowledge extraction from pre-assembly process. Not only does it systematically arrange the data, but it also combines various data sources and creates a knowledge system to improve efficiency of the design process. © 2017 Elsevier Ltd","Digital design; Knowledge extraction; Visualization","Agile manufacturing systems; Assembly; Data visualization; Design; Extraction; Flow visualization; Manufacture; Metadata; Text processing; Visualization; Designing components; Digital designs; Knowledge extraction; Manufacturing facility; Quality improvement; Systematic framework; Text mining techniques; Virtual manufacturing; Data mining",2-s2.0-85030116887
"Abellán J., Mantas C.J., Castellano J.G.","AdaptativeCC4.5: Credal C4.5 with a rough class noise estimator",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030540807&doi=10.1016%2fj.eswa.2017.09.057&partnerID=40&md5=e1d4ee2da5a99bb2c5c0aea3912cc103","The application of classifiers on data represents an important help in a process of decision making. Any classifier, or other method used for knowledge extraction, suffers a deterioration when it is applied on data with noise. Credal C4.5 (CC4.5) is a recent method of classification, that introduces imprecise probabilities in the algorithm of the classic C4.5. It is very suitable in classification noise tasks, but it has a clear dependency of a parameter. It has been proved that this parameter is related with the level of overfitting of the model on the data used for training. In noisy domains, this characteristic is important in the sense that variations of this parameter can reduce the variance of the model. Depending on the degree of noise that a data set has, the application of different values of this parameter can produce different performance of the CC4.5 model. Hence, the use of the correct parameter is fundamental to attain a high level of performance for this model. In this paper, that problem is solved via a rough procedure to estimate the level of class noise in the training data. Combining this new noise estimation process with the CC4.5, it is presented a direct method that has an equivalent performance than the one of the CC4.5 when it is used with the best value of its parameter for each level of class noise. © 2017 Elsevier Ltd","Classification; Decision support system; Decision tree; Imprecise probabilities; Noisy data; Uncertainty measures","Artificial intelligence; Classification (of information); Decision making; Decision support systems; Decision trees; Equivalence classes; Probability; Trees (mathematics); Degree of noise; Direct method; Imprecise probabilities; Knowledge extraction; Noise estimation; Noisy data; Training data; Uncertainty measures; Data mining",2-s2.0-85030540807
"Varrica D., Dongarrà G., Alaimo M.G., Monna F., Losno R., Sanna E., De Giudici G., Tamburo E.","Lead isotopic fingerprint in human scalp hair: The case study of Iglesias mining district (Sardinia, Italy)",2018,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029379443&doi=10.1016%2fj.scitotenv.2017.09.106&partnerID=40&md5=03d45893adcd73c7d7039c098df758b0","The Sulcis-Iglesiente district (SW Sardinia, Italy) has been, until recently, one of the most important Italian polymetallic mining areas for the extraction of lead. Epidemiological studies conducted over several decades have indicated this site at high risk of environmental crisis with possible adverse effects on the public health. In the present paper we discuss Pb isotope signatures in human scalp hair and road dust collected from the Sulcis-Iglesiente area in order to trace the exposure of populations to potential Pb sources. A total of 23 determinations (20 on hair samples and 3 on road dust samples) of lead isotope ratios (206Pb/207Pb and 208Pb/206Pb) were carried out. The obtained results were integrate with literature data regarding the total content of Pb in hair samples from the same study area. Hair from children living in Sant'Antioco exhibited lead isotope ratios in the ranges 1.152–1.165 for 206Pb/207Pb and 2.101–2.108 for 208Pb/206Pb, while hair samples from Iglesias resulted less radiogenic: 206Pb/207Pb ~ 1.147–1.154 and 208Pb/206Pb ~ 2.106–2.118. These values pointed to a multi-source mixing between the less radiogenic sources, corresponding to the Pb ore deposits, and the more radiogenic sources identified in local background. © 2017 Elsevier B.V.","Environmental geochemistry; Human biomonitoring; Lead isotope ratios; Mining district","Dust; Geochemistry; Health risks; Lead; Metamorphic rocks; Ore deposits; Roads and streets; Environmental crisis; Environmental geochemistry; Epidemiological studies; Human biomonitoring; Lead isotope ratios; Literature data; Mining district; Road dust samples; Isotopes; isotope; lead; biomonitoring; dust; geochemistry; hair; isotopic ratio; lead isotope; mine; pollution exposure; adolescent; Article; child; dust; finger dermatoglyphics; hair; human; Italy; literature; mining; population exposure; priority journal; sampling; scalp hair; Italy; Sardinia",2-s2.0-85029379443
"Karami A., Dahl A.A., Turner-McGrievy G., Kharrazi H., Shaw G., Jr.","Characterizing diabetes, diet, exercise, and obesity comments on Twitter",2018,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029596678&doi=10.1016%2fj.ijinfomgt.2017.08.002&partnerID=40&md5=8f5f3e857978cb45b99fe8d9e1c37372","Social media provide a platform for users to express their opinions and share information. Understanding public health opinions on social media, such as Twitter, offers a unique approach to characterizing common health issues such as diabetes, diet, exercise, and obesity (DDEO); however, collecting and analyzing a large scale conversational public health data set is a challenging research task. The goal of this research is to analyze the characteristics of the general public's opinions in regard to diabetes, diet, exercise and obesity (DDEO) as expressed on Twitter. A multi-component semantic and linguistic framework was developed to collect Twitter data, discover topics of interest about DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8% of tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity. The strongest correlation among the topics was determined between exercise and obesity (p <.0002). Other notable correlations were: diabetes and obesity (p <.0005), and diet and obesity (p <.001). DDEO terms were also identified as subtopics of each of the DDEO topics. The frequent subtopics discussed along with “Diabetes”, excluding the DDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer. The non-DDEO subtopics for “Diet” included vegetarian, pregnancy, celebrities, weight loss, religious, and mental health, while subtopics for “Exercise” included computer games, brain, fitness, and daily plan. Non-DDEO subtopics for “Obesity” included Alzheimer, cancer, and children. With 2.67 billion social media users in 2016, publicly available data such as Twitter posts can be utilized to support clinical providers, public health experts, and social scientists in better understanding common public opinions in regard to diabetes, diet, exercise, and obesity. © 2017 Elsevier Ltd","Diabetes; Diet; Exercise; Health; Obesity; Text mining; Topic model; Twitter","Blood pressure; Computer games; Data mining; Medical problems; Nutrition; Public health; Semantics; Social networking (online); Exercise; Obesity; Text mining; Topic Modeling; Twitter; Health",2-s2.0-85029596678
"Pásztor L., Laborczi A., Bakacsi Z., Szabó J., Illés G.","Compilation of a national soil-type map for Hungary by sequential classification methods",2018,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019019185&doi=10.1016%2fj.geoderma.2017.04.018&partnerID=40&md5=f39acc1138d010dda3a08e7e5ad7aef7","Traditionally in Hungary the soil cover under agricultural and forestry management is typically characterized independently and just approximately identically. Soil data collection is carried out and the databases of soil features are managed irrespectively. As a consequence, nationwide soil maps cannot be considered homogeneously predictive for soils of croplands and forests, plains and hilly/mountainous regions. In order to compile a national soil type map with harmonized legend as well as with spatially relatively homogeneous predictive power and accuracy, the authors unified their resources. Soil profile data originating from the two sources (agriculture and forestry) were cleaned up and harmonized according to a common soil type classification. Various methods were tested for the compilation of the target map: segmentation of a synthesized image consisting of the predictor variables, multi stage classification by Classification and Regression Trees, Random Forests and Artificial Neural Networks. Evaluation of the results showed that the object based, multi-level mapping approach performs significantly better than the simple classification techniques. A combination of best performing classifiers, when each classifier's vote on the same object is weighted according to its confidence in the voted class, led to the final product: a unified, national, soil type map with spatially consistent predictive capabilities. © 2017 Elsevier B.V.","Data mining; Digital soil mapping; Multi stage classification; Segmentation; Soil type map","Agriculture; Data mining; Decision trees; Forestry; Image segmentation; Mapping; Neural networks; Soil surveys; Timber; Classification and regression tree; Classification methods; Classification technique; Digital soil mappings; Multi stage; Predictive capabilities; Predictor variables; Soil types; Soils; agricultural land; data mining; data set; digital mapping; forest management; forest soil; image classification; mountain region; numerical method; segmentation; soil cover; soil profile; soil type; Hungary",2-s2.0-85019019185
"Wang D., Zhang Z., Bai R., Mao Y.","A hybrid system with filter approach and multiple population genetic algorithm for feature selection in credit scoring",2018,"Journal of Computational and Applied Mathematics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020173958&doi=10.1016%2fj.cam.2017.04.036&partnerID=40&md5=9d4c5d7405318c299c54f293e2d6e2df","With the financial crisis happened in 2007, massive credit risks are exposed to the banking sectors. So credit scoring has attracted more and more attention. Bank owns a lot of customer data. By using those data, credit scoring model can judge the applicants’ credit risk accurately. But those data are often high dimensional, and have some irrelevant features. Those irrelevant features will affect classifiers accuracy. Therefore, feature selection is an important topic. This paper proposes a two-phase hybrid approach based on filter approach and multiple population genetic algorithm-HMPGA. In phase 1, it introduces the idea of wrapper approach into three filter approaches to acquire some important prior information for initial populations setting of MPGA. In phase 2, it takes advantage of MPGA's characteristics of global optimization and quick convergence to find optimal feature subset. This paper uses two real credit scoring datasets of UCI databases to compare HMPGA, MPGA and GA. It verifies that the accuracies of feature subsets acquired from HMPGA, MPGA and GA are superior to three filter approaches. Meanwhile, nonparametric Wilcoxon signed rank test is held to confirm that HMPGA is better than MPGA and GA. HMPGA not only can be applied to feature selection of credit scoring, but also can be applied to more fields of data mining. © 2017 Elsevier B.V.","Credit scoring; Feature selection; HMPGA; Hybrid approach","Bandpass filters; Data mining; Feature extraction; Filtration; Genetic algorithms; Global optimization; Hybrid systems; Optimization; Risk assessment; Credit scoring; Credit scoring model; HMPGA; Hybrid approach; Initial population; Multiple population genetic algorithms; Prior information; Wilcoxon signed rank test; Classification (of information)",2-s2.0-85020173958
"Grasso M., Demir A.G., Previtali B., Colosimo B.M.","In situ monitoring of selective laser melting of zinc powder via infrared imaging of the process plume",2018,"Robotics and Computer-Integrated Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022345139&doi=10.1016%2fj.rcim.2017.07.001&partnerID=40&md5=da9d3950d687d8043250208d2646e0a0","Despite continuous technological improvements in metal additive manufacturing (AM) systems, process stability is still affected by several possible sources of defects especially in the presence of challenging materials. Thus, both the research community and the major AM system developers have focused an increasing attention on in situ sensing and monitoring tools in the last years. However, there is still a lack of statistical methods to automatically detect the onset of a defect and signal an alarm during the part's layer-wise production. This study contributes to this framework with two levels of novelty. First, it presents an in situ monitoring method that integrates the acquisition of infrared images with a data mining approach for feature extraction and a statistical process monitoring technique to design a data-driven and automated alarm rule. Second, the method is aimed at monitoring powder bed fusion processes for difficult-to-process materials like zinc and its alloys, which impose several challenges to the process stability and quality because of their low melting and boiling points. To this aim, the proposed approach analyzes the byproducts generated by the interaction between the energy source and the material. In particular, it detects unstable behaviors by analyzing the salient properties of the process plume to detect unstable melting conditions. This case study entails an SLM process on zinc powder, where different sets of process parameters were tested leading either to in-control or out-of-control quality conditions. A comparison analysis highlights the effectiveness of plume-based stability monitoring. © 2017 Elsevier Ltd","In situ monitoring; Infrared imaging; Metal additive manufacturing; Process plume; Selective laser melting; Zinc","3D printers; Byproducts; Data mining; Defects; Infrared imaging; Manufacture; Melting; Monitoring; Powder metals; Process monitoring; Quality control; Statistical process control; Thermography (imaging); Zinc; Zinc alloys; Comparison analysis; In- situ monitoring; Metal additives; Research communities; Selective laser melting; Stability monitoring; Statistical process monitoring; Technological improvements; Image processing",2-s2.0-85022345139
"Massawe B.H.J., Subburayalu S.K., Kaaya A.K., Winowiecki L., Slater B.K.","Mapping numerically classified soil taxa in Kilombero Valley, Tanzania using machine learning",2018,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007173765&doi=10.1016%2fj.geoderma.2016.11.020&partnerID=40&md5=d91cb82e890b8cc632052a5c831f4077","Inadequacy of spatial soil information is one of the limiting factors to making evidence-based decisions to improve food security and land management in the developing countries. Various digital soil mapping (DSM) techniques have been applied in many parts of the world to improve availability and usability of soil data, but less has been done in Africa, particularly in Tanzania and at the scale necessary to make farm management decisions. The Kilombero Valley has been identified for intensified rice production. However the valley lacks detailed and up-to-date soil information for decision-making. The overall objective of this study was to develop a predictive soil map of a portion of Kilombero Valley using DSM techniques. Two widely used decision tree algorithms and three sources of Digital Elevation Models (DEMs) were evaluated for their predictive ability. Firstly, a numerical classification was performed on the collected soil profile data to arrive at soil taxa. Secondly, the derived taxa were spatially predicted and mapped following SCORPAN framework using Random Forest (RF) and J48 machine learning algorithms. Datasets to train the model were derived from legacy soil map, RapidEye satellite image and three DEMs: 1 arc SRTM, 30 m ASTER, and 12 m WorldDEM. Separate predictive models were built using each DEM source. Mapping showed that RF was less sensitive to the training set sampling intensity. Results also showed that predictions of soil taxa using 1 arc SRTM and 12 m WordDEM were identical. We suggest the use of RF algorithm and the freely available SRTM DEM combination for mapping the soils for the whole Kilombero Valley. This combination can be tested and applied in other areas which have relatively flat terrain like the Kilombero Valley. © 2016 Elsevier B.V.","Decision tree analysis; DEM; Kilombero Valley; Machine learning; Numerical classification; Soil mapping","Artificial intelligence; Data mining; Decision making; Decision trees; Developing countries; Food supply; Forestry; Landforms; Learning algorithms; Learning systems; Mapping; Soil surveys; Surveying; Tracking radar; Decision tree analysis; Decision-tree algorithm; Digital elevation model; Digital soil mappings; Evidence- based decisions; Kilombero Valley; Numerical classification; Soil mapping; Soils; algorithm; crop production; decision support system; developing world; digital elevation model; land management; machine learning; mapping; numerical method; RapidEye; rice; satellite imagery; Shuttle Radar Topography Mission; soil analysis; soil biota; soil classification; soil profile; terrain; Kilombero Valley; Morogoro [Tanzania]; Tanzania",2-s2.0-85007173765
"Vincent S., Lemercier B., Berthier L., Walter C.","Spatial disaggregation of complex Soil Map Units at the regional scale based on soil-landscape relationships",2018,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994357191&doi=10.1016%2fj.geoderma.2016.06.006&partnerID=40&md5=cfeb68a146b8f2aeec2e1b03eaece9b7","Digital soil mapping is becoming a powerful tool to increase the spatial detail of soil information over large areas, which is essential to address agronomic and environmental issues. When it exists, information about soil is often sparse or available at a coarser resolution than required. The spatial distribution of soil at the regional scale is usually represented as a set of polygons defining Soil Map Units (SMUs), each including several Soil Type Units (STUs), which are not spatially delineated but semantically described in a database. Delineation of STUs within SMUs, i.e. spatial disaggregation of SMU, should improve the precision of soil information derived from legacy and ancillary data. The aim of this study was to predict STUs by spatially disaggregating SMUs through a decision-tree approach that considered expert knowledge about soil-landscape relationships embedded in soil databases. In a 27,376 km2 study area in north-western France (Brittany), 434 SMUs were delineated at 1:250,000 scale, and 320 STUs, their relative area in the SMUs, and their geomorphological and geological contexts were described. A calibration dataset of points was established using stratified random sampling (n = 352,188). To retrieve soil-landscape relationships, expert rules for soil distribution defined by soil surveyors and based on topography, parent material and waterlogging index were considered in order to allocate an STU to 83% of the calibration dataset. The calibration dataset and covariates (i.e. pedological, geological and terrain attributes; land use; airborne gamma-ray spectrometry) were then used to build and extrapolate the decision tree using the C.5 algorithm in DSMART software. Several iterations were performed, providing a probability of occurrence of each possible STU within the study area. External validation was performed by comparing predictions of the disaggregation procedure to available soil maps at scales of 1:25,000 or 1:50,000 and observed profiles. Overall accuracies ranged from 41 to 72%, depending on the validation method (per pixel vs. 3 × 3 windows of pixels, per STU vs. STU grouped by semantic proximity (n = 204)). Introducing expert rules based on soil-landscape relationships to allocate STUs to calibration samples enabled production of a soil map with clear spatial structures, yielding expected spatial patterns of soil organisation. Future work notably concerns estimating soil properties at multiple depths deriving from STU predictions, according to the GlobalSoilMap project. © 2016","Classification trees; Digital soil mapping; Regional scale; Soil map units; Soil type units; Soil-landscape ruleset; Spatial disaggregation","Calibration; Data mining; Decision trees; Forecasting; Forestry; Gamma ray spectrometers; Gamma rays; Geology; Land use; Mapping; Pixels; Semantics; Soils; Trees (mathematics); Classification trees; Digital soil mappings; Regional scale; Soil map units; Soil types; Spatial disaggregation; Soil surveys; classification; database; digital map; knowledge; landscape; map; parent material; pixel; regional pattern; soil; soil type; spatial analysis; spatial distribution; topography; waterlogging; Bretagne; France",2-s2.0-84994357191
"Eliacik A.B., Erdogan N.","Influential user weighted sentiment analysis on topic based microblogging community",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030679791&doi=10.1016%2fj.eswa.2017.10.006&partnerID=40&md5=25c2bcf59481245ad6b31d7f611e6c13","Nowadays, social microblogging services have become a popular expression platform of what people think. People use these platforms to produce content on different topics from finance, politics and sports to sociological fields in real-time. With the proliferation of social microblogging sites, the massive amount of opinion texts have become available in digital forms, thus enabling research on sentiment analysis to both deepen and broaden in different sociological fields. Previous sentiment analysis research on microblogging services generally focused on text as the unique source of information, and did not consider the social microblogging service network information. Inspired by the social network analysis research and sentiment analysis studies, we find that people's trust in a community have an important place in determining the community's sentiment polarity about a topic. When studies in the literature are examined, it is seen that trusted users in a community are actually influential users. Hence, we propose a novel sentiment analysis approach that takes into account the social network information as well. We concentrate on the effect of influential users on the sentiment polarity of a topic based microblogging community. Our approach extends the classical sentiment analysis methods, which only consider text content, by adding a novel PageRank-based influential user finding algorithm. We have carried out a comprehensive empirical study of two real-world Twitter datasets to analyze the correlation between the mood of the financial social community and the behavior of the stock exchange of Turkey, namely BIST100, using Pearson correlation coefficient method. Experimental results validate our assumptions and show that the proposed sentiment analysis method is more effective in finding topic based microblogging community's sentiment polarity. © 2017 Elsevier Ltd","Influential user; Microblogging service; Sentiment analysis; Social network analysis","Correlation methods; Data mining; Empirical studies; Finding algorithm; Influential users; Micro-blogging services; Pearson correlation coefficients; Sentiment analysis; Social communities; Social network informations; Social networking (online)",2-s2.0-85030679791
"Dou P., Chen Y., Yue H.","Remote-sensing imagery classification using multiple classification algorithm-based AdaBoost",2018,"International Journal of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032872235&doi=10.1080%2f01431161.2017.1390276&partnerID=40&md5=aa51eabc9f0d1f791d4c3787e3003662","AdaBoost demonstrates excellent performance in remote sensing (RS) image classification, but as it works on only one classification algorithm, the disadvantage of the classification algorithm itself is difficult to overcome, resulting in limitations in the improvement of classification accuracy. In this article, a modified AdaBoost, a multiple classification algorithm-based AdaBoost (MCA AdaBoost), is proposed to improve remote sensing image classification. The new method works on more than one classification algorithm and can make full use of the advantages of different learning algorithms. Based on a Landsat 8 Operational Land Imager (OLI) image whose spatial resolution was enhanced to 15 m with a panchromatic band, a C4.5 decision tree, Naïve Bayes, and artificial neural network were used as objects to verify and compare the performance of both AdaBoost and MCA AdaBoost. The experimental results show that MCA AdaBoost successfully inherits the benefits of the original AdaBoost, combines the advantages of different classification algorithms and lowers overfitting. By increasing diversity and complementarity among base classifiers, MCA AdaBoost outperforms AdaBoost in terms of RS classification accuracy improvement. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Adaptive boosting; Data mining; Decision trees; Image enhancement; Neural networks; Remote sensing; C4.5 decision trees; Classification accuracy; Classification algorithm; Multiple Classification; Operational land imager; Remote sensing image classification; Remote sensing imagery; Remote sensing images; Image classification; accuracy assessment; algorithm; artificial neural network; image classification; Landsat; panchromatic image; remote sensing",2-s2.0-85032872235
"Chen X., Breslow L., DeBoer J.","Analyzing productive learning behaviors for students using immediate corrective feedback in a blended learning environment",2018,"Computers and Education",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031107366&doi=10.1016%2fj.compedu.2017.09.013&partnerID=40&md5=0c4e2ee41fe570bf9dad78bd2c3bca05","Undergraduate classes in many science and engineering courses are utilizing asynchronous computer platforms to host educational materials such as lecture videos or discussion forums. These platforms also have the ability to provide immediate feedback to students on formative assessment tasks such as homework problems, reading questions, or weekly quizzes. Although there have been a number of studies on computer-based feedback, there is more we need to know about how students interact with immediate feedback, and how those interactions influence their learning. In this study, we characterize introductory physics students' interactions with one computer-based immediate simple corrective feedback tool, the “checkable answer feature” (CAF), powered by the institutional version of the edX platform. We investigate how much students interact with the CAF, the patterns of interaction, and, ultimately, how these patterns are associated with course performance. We utilize rich quantitative data, including a large volume of server tracking logs that show students’ use the CAF, as well as performance metrics. Our findings show certain patterns of engagement with feedback reflect productive study strategies and significantly predict higher performance. The findings provide guidance for instructional practice and the continued development of online feedback tools in introductory STEM courses. © 2017","Blended learning; Computer-mediated feedback; Human-computer interface; Physics education","Computer aided instruction; Curricula; Human computer interaction; Room and pillar mining; Students; Blended learning; Blended learning environments; Computer-mediated feedbacks; Educational materials; Human computer interfaces; Instructional practices; Physics education; Science and engineering; Education",2-s2.0-85031107366
"Lepšová-Skácelová O., Fibich P., Wild J., Lepš J.","Trophic gradient is the main determinant of species and large taxonomic groups representation in phytoplankton of standing water bodies",2018,"Ecological Indicators",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032362780&doi=10.1016%2fj.ecolind.2017.10.034&partnerID=40&md5=165ce892541b88a72d187cc5c34b8fa9","We used the newly formed database of phytoplankton samples from the Czech Republic, containing 696 taxa from 662 samples of various types of stagnant waters (fishponds, alluvial backwaters, flooded sand- and gravel pits, lakes in abandoned quarries in former coal mines, reservoirs and others) to test for the relationships between phytoplankton composition and productivity, geographical and climatic variables (elevation, temperature, precipitation) and season of the year. As a surrogate of productivity, the number of cells/ml and the total biovolume[μm3]/ml, both spanning over more than six orders of magnitude (from 101 to 107 for the number of cells and from 102 to 109 for the biovolume) were used. The phytoplankton was characterized by its species composition and also by the composition of large taxonomic groups. The data were analysed by constrained ordination (Canonical Correspondence Analysis), including variation partitioning, and by ANOVA of the estimates of species optima based on weighted averages of environmental characteristics. All the explanatory variable groups, i.e. productivity, geography/climate, and season have significant effects on both the species composition and the composition of large taxonomic groups. Productivity is the best predictor of both species and large taxonomic group composition, followed by climatic variables and finally season. The relative effectiveness of productivity as a predictor was considerably greater for large taxonomic groups. The productivity characterized by the number of cells was always a better predictor than when characterized by biovolume. The species optima estimated as weighted averages of corresponding environmental variables show consistent patterns according to large taxonomic groups, but also according to the genera within the groups: in particular, the cyanobacteria and Chrysophyceae preferred on average the most and the least productive environments respectively, however, there were large differences in species preferences also within groups and even within genera. The optima of species on the trophic gradient are suitable characteristics for ecological indication and are presented for more than 400 taxa in the appendix together with estimates of species tolerance. © 2017 Elsevier Ltd","Constrained ordination; Fishpond; Phylogenetic niche conservativism; Phytoplankton composition database; Variation partitioning; Weighted averages as indicators","Abandoned mines; Coal mines; Fish ponds; Mining laws and regulations; Productivity; Quarries; Reservoirs (water); Statistical methods; Water pollution; Constrained ordination; Phylogenetic niche conservativism; Phytoplankton composition; Variation partitioning; Weighted averages; Phytoplankton; climate effect; community composition; cyanobacterium; environmental factor; estimation method; geographical distribution; niche partitioning; ordination; phylogenetics; phytoplankton; pond; taxonomy; trophic conditions; weight; Czech Republic; Chrysophyceae; Cyanobacteria",2-s2.0-85032362780
"Nuss P., Blengini G.A.","Towards better monitoring of technology critical elements in Europe: Coupling of natural and anthropogenic cycles",2018,"Science of the Total Environment",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029544490&doi=10.1016%2fj.scitotenv.2017.09.117&partnerID=40&md5=fab67e70d57263a26bc3687f899511da","The characterization of elemental cycles has a rich history in biogeochemistry. Well known examples include the global carbon cycle, or the cycles of the ‘grand nutrients’ nitrogen, phosphorus, and sulfur. More recently, efforts have increased to better understand the natural cycling of technology critical elements (TCEs), i.e. elements with a high supply risk and economic importance in the EU. On the other hand, tools such as material-flow analysis (MFA) can help to understand how substances and goods are transported and accumulated in man-made technological systems (‘anthroposphere’). However, to date both biogeochemical cycles and MFA studies suffer from narrow system boundaries, failing to fully illustrate relative anthropogenic and natural flow magnitude and the degree to which human activity has perturbed the natural cycling of elements. We discuss important interconnections between natural and anthropogenic cycles and relevant EU raw material dossiers. Increased integration of both cycles could help to better capture the transport and fate of elements in nature including their environmental/human health impacts, highlight potential future material stocks in the anthroposphere (in-use stocks) and in nature (e.g., in soils, tailings, or mining wastes), and estimate anticipated emissions of TCEs to nature in the future (based on dynamic stock modeling). A preliminary assessment of natural versus anthropogenic element fluxes indicates that anthropogenic fluxes induced by the EU-28 of palladium, platinum, and antimony (as a result of materials uses) might be greater than the respective global natural fluxes. Increased combination of MFA and natural cycle data at EU level could help to derive more complete material cycles and initiate a discussion between the research communities of biogeochemists and material flow analysts to more holistically address the issues of sustainable resource management. © 2017 The Authors","Anthropogenic cycles; Elemental cycles; EU raw materials information systems; EU raw materials policy; Material flow analysis; Natural cycles","Antimony; Biogeochemistry; Carbon; Fluxes; Information management; Soil pollution; Anthropogenic cycles; Anthropogenic elements; Elemental cycles; Material flow analysis; Natural cycle; Preliminary assessment; Sustainable resource management; Technological system; Characterization; antimony; element; palladium; platinum; biogeochemical cycle; biogeochemistry; environmental monitoring; European Union; holistic approach; information system; material flow analysis; resource management; sustainable development; anthropogenic cycle; Article; biogeochemical cycle; environmental economics; environmental impact; environmental monitoring; Europe; life cycle assessment; natural cycle; priority journal; resource management; risk management; waste management; Europe",2-s2.0-85029544490
"Suo T., Wang H., Shi X., Feng L., Cai J., Duan Y., Bao H., Wu X., Zhang Y., Yu H., Li Z.","Combining near infrared spectroscopy with predictive model and expertise to monitor herb extraction processes",2018,"Journal of Pharmaceutical and Biomedical Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031797714&doi=10.1016%2fj.jpba.2017.10.004&partnerID=40&md5=8bee27f983ebe32992170a8873171b16","Albeit extensively utilized, herb extraction process (HEP) is hard to be monitored because of its batch nature and the fluctuating quality of raw materials. Process analytical tools like near infrared spectroscopy (NIRS) can offer nondestructive examinations and collect abundant data of the process, which in principle contain the information about the quality of both the product and the process itself. However, extra effort is often required for the data mining of such process measurements, and extracting knowledge of the quality of process can be even harder. In this study, we take the extraction process of licorice as a typical HEP instance, and combine NIRS with classical partial least squared regression (PLSR) and expertise for its on-line monitoring. We show that our scheme effectively extracts information with clear physical meanings, through which we can even uncover the process fault that does not induce evident abnormalities in the product quality. Moreover, the constructed model can continuously evolve with more process data from daily operations, and the idea of the whole framework can be directly generalized to other HEP. © 2017 Elsevier B.V.","Batch process monitoring; Herb extraction process; Near infrared spectroscopy; Partial least squared regression; Quality of process; Traditional Chinese medicine","Glycyrrhiza extract; liquiritin; solvent; algorithm; Article; batch process; calibration; chemometrics; extraction; Glycyrrhiza; herb; herb extraction; near infrared spectroscopy; nonhuman; online monitoring; power supply; prediction; priority journal; ultra performance liquid chromatography; workflow",2-s2.0-85031797714
"Xu C., Yang J., Gao J., Lai H., Yan S.","SRNN: Self-regularized neural network",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028723597&doi=10.1016%2fj.neucom.2017.07.051&partnerID=40&md5=379e2307e735315f36f7dc41b93060e5","In this work, we address to boost the discriminative capability of deep neural network by alleviating the over-fitting problem. Previous works often deal with the problem of learning a neural network by optimizing one or more objective functions with some existing regularization methods (such as dropout, weight decay, stochastic pooling, data augmentation, etc.). We argue that these approaches may be difficult to further improve the classification performance of a neural network, due to not well employing its own learned knowledge. In this paper, we introduce a self-regularized strategy for learning a neural network, named as a Self-Regularized Neural Network (SRNN). The intuition behind the SRNN is that the sample-wise soft targets of a neural network may have potentials to drag its own neural network out of its local optimum. More specifically, an initial neural network is firstly pre-trained by optimizing one or more objective functions with ground truth labels. We then gradually mine sample-wise soft targets, which enables to reveal the correlation/similarity among classes predicted from its own neural network. The parameters of neural network are further updated for fitting its sample-wise soft targets. This self-regularization learning procedure minimizes the objective function by integrating the sample-wise soft targets of neural network and the ground truth label of training samples. Three characteristics in this SRNN are summarized as: (1) gradually mining the learned knowledge from a single neural network, and then correcting and enhancing this part of learned knowledge, resulting in the sample-wise soft targets; (2) regularly optimizing the parameters of this neural network with their sample-wise soft targets; (3) boosting the discriminative capability of a neural network with the self-regularization strategy. Extensive experiments on four public datasets, i.e., CIFAR-10, CIFAR-100, Caltech101 and MIT, well demonstrate the effectiveness of the proposed SRNN for image classification. © 2017","Image classification; Neural network; Sample-wise soft targets; Self-regularized learning","Classification (of information); Deep learning; Image classification; Neural networks; Stochastic systems; Classification performance; Objective functions; Over fitting problem; Regularization methods; Regularization strategies; Regularized neural networks; Self-regularized learning; Soft targets; Deep neural networks",2-s2.0-85028723597
"Pakzad P., Mofarahi M., Izadpanah A.A., Afkhamipour M., Lee C.-H.","An experimental and modeling study of CO2 solubility in a 2-amino-2-methyl-1-propanol (AMP) + N-methyl-2-pyrrolidone (NMP) solution",2018,"Chemical Engineering Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031751512&doi=10.1016%2fj.ces.2017.10.015&partnerID=40&md5=7e33b42541cf7615f7f6c6c33317f0db","In this study, an experimental setup based on the static-synthetic method was used to measure the new experimental data of CO2 solubility in an aqueous solution of 2-amino-2-methyl-1-propanol (AMP) + N-methyl-2-pyrrolidone (NMP) solution. For the static-synthetic method, the mass balance of compositions and the pressure–volume–temperature conditions were used for measuring the amount of absorbed CO2 by the AMP+NMP solution. The measurements were performed over a temperature range of 313.15–353.15 K, CO2 partial pressure up to 316.7 kPa, and in different concentrations of the AMP+NMP solution. Two models, modified Kent–Eisenberg, and Deshmukh–Mather, based on the empirical correlations and activity-fugacity approach, respectively, were used for the prediction of experimental data. The parameters of the equilibrium constants of the protonation and carbamate reactions for the modified Kent–Eisenberg model and the interaction parameters for Deshmukh–Mather model were obtained. For validation of our setup, a new set of experimental data for the solubility of CO2 in an aqueous solution of AMP, methyldiethanolamine (MDEA) and diethanolamine (DEA) were measured and compared with existing experimental data in the literature, and good results were obtained. The results of the modeling study showed that the Deshmukh–Mather model gave a better prediction of experimental CO2 loadings data than the modified Kent–Eisenberg. Also, the results showed that the solubility of CO2 in an aqueous solution of AMP+NMP increases as the CO2 partial pressure increases while the temperature decreases. © 2017 Elsevier Ltd","AMP; CO2 absorption; Deshmukh–Mather; Modified Kent–Eisenberg; NMP","Carbon dioxide; Equilibrium constants; Ionic liquids; Solubility; Solutions; 2-amino-2-methyl-1-propanol; CO2 absorption; Empirical correlations; Interaction parameters; Methyldiethanolamine; N-methyl-2-pyrrolidone; Static-synthetic method; Temperature conditions; Solution mining",2-s2.0-85031751512
"Tian R.-Y., Wu L., Liang X.-H., Zhang X.-F.","Opinion data mining based on DNA method and ORA software",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029668848&doi=10.1016%2fj.physa.2017.08.093&partnerID=40&md5=a2a78840b35e01001397df7e9c7136e2","Public opinion, especially the online public opinion is a critical issue when it comes to mining its characteristics. Because it can be formed directly and intensely in a short time, and may lead to the outbreak of online group events, and the formation of online public opinion crisis. This may become the pushing hand of a public crisis event, or even have negative social impacts, which brings great challenges to the government management. Data from the mass media which reveal implicit, previously unknown, and potentially valuable information, can effectively help us to understand the evolution law of public opinion, and provide a useful reference for rumor intervention. Based on the Dynamic Network Analysis method, this paper uses ORA software to mine characteristics of public opinion information, opinion topics, and public opinion agents through a series of indicators, and quantitatively analyzed the relationships between them. The results show that through the analysis of the 8 indexes associating with opinion data mining, we can have a basic understanding of the public opinion characteristics of an opinion event, such as who is important in the opinion spreading process, the information grasping condition, and the opinion topics release situation. © 2017 Elsevier B.V.","Dynamic Network Analysis; Opinion data mining; ORA; Rumor intervention","Social aspects; Software agents; Critical issues; Dynamic network analysis; Government management; Online public opinions; Public crisis; Public opinion informations; Public opinions; Rumor intervention; Data mining",2-s2.0-85029668848
"Jamaati M., Mehri A.","Text mining by Tsallis entropy",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029597339&doi=10.1016%2fj.physa.2017.09.020&partnerID=40&md5=07cf9816fd59299df137924f2788a29f","Long-range correlations between the elements of natural languages enable them to convey very complex information. Complex structure of human language, as a manifestation of natural languages, motivates us to apply nonextensive statistical mechanics in text mining. Tsallis entropy appropriately ranks the terms’ relevance to document subject, taking advantage of their spatial correlation length. We apply this statistical concept as a new powerful word ranking metric in order to extract keywords of a single document. We carry out an experimental evaluation, which shows capability of the presented method in keyword extraction. We find that, Tsallis entropy has reliable word ranking performance, at the same level of the best previous ranking methods. © 2017 Elsevier B.V.","Keyword extraction; Long-range correlation; Text mining; Tsallis entropy","Entropy; Extraction; Large scale systems; Statistical mechanics; Experimental evaluation; Keyword extraction; Long range correlations; Nonextensive statistical mechanics; Spatial correlations; Statistical concepts; Text mining; Tsallis entropies; Data mining",2-s2.0-85029597339
"Yan X., Minnhagen P.","The dependence of frequency distributions on multiple meanings of words, codes and signs",2018,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028936217&doi=10.1016%2fj.physa.2017.08.133&partnerID=40&md5=7f57ceb7ab08afa4e7065131b7ece03e","The dependence of the frequency distributions due to multiple meanings of words in a text is investigated by deleting letters. By coding the words with fewer letters the number of meanings per coded word increases. This increase is measured and used as an input in a predictive theory. For a text written in English, the word-frequency distribution is broad and fat-tailed, whereas if the words are only represented by their first letter the distribution becomes exponential. Both distribution are well predicted by the theory, as is the whole sequence obtained by consecutively representing the words by the first L=6,5,4,3,2,1 letters. Comparisons of texts written by Chinese characters and the same texts written by letter-codes are made and the similarity of the corresponding frequency-distributions are interpreted as a consequence of the multiple meanings of Chinese characters. This further implies that the difference of the shape for word-frequencies for an English text written by letters and a Chinese text written by Chinese characters is due to the coding and not to the language per se. © 2017 Elsevier B.V.","Codes; Maximum entropy; Multiple meanings; Random Group Formation; Word-frequency distributions","Data mining; Linguistics; Maximum entropy methods; Chinese characters; Chinese text; Codes; Frequency distributions; Multiple meanings; Random groups; Word frequencies; Codes (symbols)",2-s2.0-85028936217
"Søndergaard M., Lauridsen T.L., Johansson L.S., Jeppesen E.","Gravel pit lakes in Denmark: Chemical and biological state",2018,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027996014&doi=10.1016%2fj.scitotenv.2017.08.163&partnerID=40&md5=4d8607ad92237dc4929ad626abdf7e5d","Mining of gravel and sand for construction purposes is big business and gravel pit lakes have become increasingly common all over the world. In Denmark, hundreds of gravel pit lakes have been created during the past decades. We investigated the chemical and biological status of 33–52 gravel pit lakes and compared the results with data from similar-sized natural Danish lakes. The area of the lakes ranged from 0.2 to 13 ha and their age from 0.5 to 26 years. Generally, the gravel pit lakes were clear with low nutrient concentrations, the median concentrations of total phosphorus and total nitrogen being 0.023 mg/l and 0.30 mg/l compared with 0.115 mg/l and 1.29 mg/l, respectively, in natural lakes. Correspondingly, median chlorophyll a was 5 μg/l in the gravel pit lakes and 36 μg/l in the natural lakes. Submerged macrophytes were found in all gravel pit lakes, with particularly high cover in the shallow ones. Most gravel pit lakes were deeper than the natural lakes, which may restrict the area potentially to be covered by submerged macrophytes, with implications also for the biological quality of the lakes. Fish were found in most of the gravel pit lakes, roach (Rutilus rutilus), perch (Perca fluviatilis) and rudd (Scardinius erythrophalmus) being the most frequently observed species. Fish stocking was common and included also non-native species such as carp (Cyprinus carpio) and rainbow trout (Oncorchynchus mykiss). Compared with the natural lakes, fish species richness and catch per gillnet were overall lower in the gravel pit lakes. Groundwater-fed gravel pit lakes add importantly to the number of high-quality lakes in Denmark and with an optimised design and by avoiding negative side effects, they can be positive for both nature and society. © 2017 Elsevier B.V.","Chlorophyll; Fish; Groundwater; Macrophytes; Phosphorus","Chlorophyll; Fish; Forestry; Gravel; Groundwater; Phosphorus; Chemical and biologicals; Fish species richness; Macrophytes; Median concentration; Negative side effects; Non-native species; Nutrient concentrations; Submerged macrophytes; Lakes; Cyprinidae; Cyprinus carpio; Oncorhynchus mykiss; Perca fluviatilis; Rutilus rutilus; Scardinius",2-s2.0-85027996014
"Ding W., Lin C.-T., Chen S., Zhang X., Hu B.","Multiagent-consensus-MapReduce-based attribute reduction using co-evolutionary quantum PSO for big data applications",2018,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023638265&doi=10.1016%2fj.neucom.2017.06.059&partnerID=40&md5=e8978ef7b64c6205fefad605ae289171","The attribute reduction for big data applications has become an urgent challenge in pattern recognition, machine learning and data mining. In this paper, we introduce the multi-agent consensus MapReduce optimization model and co-evolutionary quantum PSO with self-adaptive memeplexes for designing the attribute reduction method, and propose a multiagent-consensus-MapReduce-based attribute reduction algorithm (MCMAR). Firstly, the co-evolutionary quantum PSO with self-adaptive memeplexes is designed for grouping particles into different memeplexes, which aims to explore the search space and locate the global best region during the attribute reduction of big datasets. Secondly, the four layers neighborhood radius framework with compensatory scheme is constructed to partition big attribute sets by exploiting the interdependency among multiple-relevant-attribute sets. Thirdly, a novel multi-agent consensus MapReduce optimization model is adopted to perform the multiple-relevance-attribute reduction, in which five kinds of agents are used to conduct the ensemble co-evolutionary optimization. So the uniform reduction framework of different agents’ co-evolutionary game under the bounded rationality is further refined. Fourthly, the approximation MapReduce parallelism mechanism is permitted to formalize to the multi-agent co-evolutionary consensus structure, interaction and adaptation, which enhances different agents to share their solutions. Finally, extensive experimental studies substantiate the effectiveness and accuracy of MCMAR on some well-known benchmark datasets. Moreover, successful applications in big medical datasets are expected to dramatically scaling up MCMAR for complex infant brain MRI in terms of efficiency and feasibility. © 2017 Elsevier B.V.","Co-evolutionary quantum PSO; Ensemble co-evolutionary optimization of attribute reduction; Multi-agent consensus MapReduce model; Neighborhood radius with compensatory scheme; Self-adaptive memeplexes","Big data; Data mining; Magnetic resonance imaging; Multi agent systems; Optimization; Particle swarm optimization (PSO); Pattern recognition; Rough set theory; Attribute reduction; Co-evolutionary; MapReduce models; Neighborhood radius with compensatory scheme; Self-adaptive memeplexes; Data reduction",2-s2.0-85023638265
"Wang Z., Ye X.","Social media analytics for natural disaster management",2018,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028571071&doi=10.1080%2f13658816.2017.1367003&partnerID=40&md5=ebc3b395dc14367ffab2657a50c64ba2","Social media analytics has become prominent in natural disaster management. In spite of a large variety of metadata fields in social media data, four dimensions (i.e. space, time, content and network) have been given particular attention for mining useful information to gain situational awareness and improve disaster response. In this article, we review how existing studies analyze these four dimensions, summarize common techniques for mining these dimensions, and then suggest some methods accordingly. We then propose a schema to categorize the gathered articles into 15 classes and facilitate the generation of data analysis tasks. We find that (1) a large part of studies involve multiple dimensions of social media data in their analyses, (2) there are both separate analyses for each dimension and simultaneous analyses for multiple dimensions and (3) there are fewer simultaneous analyses as dimensions increase. Finally, we suggest research opportunities and challenges in fusing social media data with authoritative datasets, i.e. census data and remote-sensing data. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","census data; dimensions; natural disasters; remote sensing; Social media",,2-s2.0-85028571071
"Furtado A.S., Alvares L.O.C., Pelekis N., Theodoridis Y., Bogorny V.","Unveiling movement uncertainty for robust trajectory similarity analysis",2018,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029431791&doi=10.1080%2f13658816.2017.1372763&partnerID=40&md5=e594c90c81ac87b58354ac5a56019547","Trajectory data analysis and mining require distance and similarity measures, and the quality of their results is directly related to those measures. Several similarity measures originally proposed for time-series were adapted to work with trajectory data, but these approaches were developed for well-behaved data that usually do not have the uncertainty and heterogeneity introduced by the sampling process to obtain trajectories. More recently, similarity measures were proposed specifically for trajectory data, but they rely on simplistic movement uncertainty representations, such as linear interpolation. In this article, we propose a new distance function, and a new similarity measure that uses an elliptical representation of trajectories, being more robust to the movement uncertainty caused by the sampling rate and the heterogeneity of this kind of data. Experiments using real data show that our proposal is more accurate and robust than related work. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","dynamic threshold similarity; elliptical trajectory representation; Movement similarity; parameter-free similarity measure; raw trajectory similarity",,2-s2.0-85029431791
"Pouranvari M., Ekrami A., Kokabi A.H.","Role of base-metal composition in isothermal solidification during diffusion brazing of nickel-based superalloys",2018,"Science and Technology of Welding and Joining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019540812&doi=10.1080%2f13621718.2017.1327934&partnerID=40&md5=d52f30f26d314d8b76274c8a160d05a0","The key feature of diffusion brazing, also referred to as transient liquid phase bonding, is isothermal solidification which precludes the formation of intermetallic in the joint centreline. Analysing the available data published in the literature showed that the composition of the nickel-based superalloys plays a strong role in determining the required time for obtaining intermetallic-free joint during diffusion brazing. This effect is not predictable by the standard conventional models. It is proposed that increasing the boride-forming elements in the base superalloy which promotes in situ boride precipitation at the diffusion-affected zone can accelerate the diffusion flux of the boron into the base superalloy, leading to faster isothermal solidification. The higher the Cr + Mo + Nb + Ta + W content in base superalloy, the shorter the isothermal solidification time. © 2017 Institute of Materials, Minerals and Mining. Published by Taylor & Francis on behalf of the Institute.","boride precipitation; Diffusion brazing; isothermal solidification; nickel-based superalloy; TLP bonding","Borides; Brazing; Diffusion; Isotherms; Nickel; Solidification; Superalloys; Conventional models; Diffusion brazing; Diffusion fluxes; Diffusion-affected zone; Isothermal solidification; Nickel- based superalloys; TLP bonding; Transient liquid phase bonding; Precipitation (chemical)",2-s2.0-85019540812
"Siddiqui T., Ahmad A.","Data mining tools and techniques for mining software repositories: A systematic review",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031419113&doi=10.1007%2f978-981-10-6620-7_70&partnerID=40&md5=95f708aa7cd049a4b654e8098f5c4d5f","A software repository contains a historical and valuable wealth of information about overall development of software system (project’s status, progress, and evolution). Mining software repositories (MSR) are one of the interesting and fastest growing fields within software engineering. It focuses on extracting and analyzing the heterogeneous data available in software repositories to uncover interesting, useful, and actionable information about software system and projects. Using well-established data mining tools and techniques, professionals, practitioners, and researchers can explore the potential of this valuable data in order to better understand and manage their complicated projects and also to produce high reliable software system delivered on time and within estimated budget. This paper is an effort to discover problems encountered during development of software projects and the role of mining software repositories to resolve these problems. A comparative study of data mining tools and techniques for mining software repositories has been presented. © 2018, Springer Nature Singapore Pte Ltd.","Heterogeneous data; Mining software repositories (MSR); Software mining","Budget control; Computer software; Data mining; Information management; Software design; Software engineering; Comparative studies; Data-mining tools; Heterogeneous data; Mining software repositories; Mining software repository (MSR); Software minings; Software repositories; Wealth of information; Big data",2-s2.0-85031419113
"Wei Z., Jia K., Sun Z.","Sensor Data Mining for Gas Station Online Monitoring",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030869154&doi=10.1007%2f978-3-319-68527-4_28&partnerID=40&md5=ae99c604eec4a6b7a126a372073a6d3d","Aiming at the geography and manpower inconvenience during the monitoring and inspection of the gas station, a remote online monitoring system for the gas station based on the sensor network is proposed in this paper, and the early warning analysis of the abnormal state of the gas station is proposed based on the data mining technology. Firstly, a gas station senor dataset is built based on the sensor network of the gas station. Then, based on the B/S architecture, a gas station online monitoring system is built. Finally, based on the sensor dataset data mining, an abnormal state of the gas station analysis method is proposed. Experiments show that the classifier method proposed in this paper has the generalization ability, it can analysis and alarm the abnormal state of the gas station which improve the intelligence and convenience of the gas station monitoring. © 2018, Springer International Publishing AG.","B/S architecture; Data mining; Gas station; Sensor network","Data handling; Data mining; Gases; Information analysis; Network architecture; Partial discharges; Sensor networks; B/S architecture; Data mining technology; Early warning analysis; Gas stations; Generalization ability; On-line monitoring system; Online monitoring; Sensor-data mining; Monitoring",2-s2.0-85030869154
"Li L., Li P., Xu H., Chen F.","A bayes classifier-based OVFDT algorithm for massive stream data mining on big data platform",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026310606&doi=10.1007%2f978-3-319-61566-0_49&partnerID=40&md5=28672531ac07e89be38f8776a751079b","Recently, online incremental data mining has become an immensely growing area of research for stream data mining. VFDT algorithm, as an excellent incremental decision tree classification algorithm, is widely used in online data mining. To optimize VFDT algorithm, a dynamic tie-breaking threshold strategy and a pre-pruning mechanism strategy are utilized to achieve the reduction of the scale of decision tree. Furthermore, Bayes classifier is applied to leaf nodes of Hoeffding decision tree, which promotes the improvement of classification accuracy. In this paper, this improved algorithm is called OVFDT (Optimized VFDT) algorithm. To improve the performance of OVFDT for massive streaming data processing, an implementation scheme of OVFDT Algorithm on MapReduce Platform is proposed in our paper. Considering the need for real-time computing, the implementation scheme on Storm Platform is designed. Three comparison experiments are designed to compare the scale, the classification accuracy and the execution time of decision tree of three algorithm generate. The simulation results reveal that compared with C4.5 and VFDT algorithm, OVFDT algorithm can effectively reduce the scale of the decision tree, achieves the improvement of classification accuracy as well. © Springer International Publishing AG 2018.",,"Big data; Data handling; Decision trees; Trees (mathematics); Classification accuracy; Decision tree classification; Implementation scheme; Incremental data; Real time computing; Stream data mining; Streaming data processing; Threshold strategy; Data mining",2-s2.0-85026310606
"Jani R., Bhatt N., Shah C.","A survey on issues of data stream mining in classification",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028414168&doi=10.1007%2f978-3-319-63673-3_17&partnerID=40&md5=034705688df48c0c72b10f7fdbc3f3df","As Data Stream Mining is trending topic for Research nowadays and more users increases day by day with online stuff, the size of big data is also getting larger. In traditional data mining extracting knowledge is done mostly using offline phase. While in data stream, Extracting data is from the continuous arriving data or we can say from the online streams. Due to continuously arriving data, it cannot be stored in the memory for processing permanently. So examining of data as fast as possible is important. In this paper we would be interested to discuss about the data stream mining and the issues of stream classification, like Single scan, Load shedding, Memory Space, Class imbalance problem, Concept drift, and possible ways to solve those issues. © 2018, Springer International Publishing AG.","Concept drift; Data stream classification; Imbalance class; Noise","Big data; Classification (of information); Data communication systems; Intelligent systems; Class imbalance problems; Concept drifts; Data stream classifications; Data stream mining; Imbalance class; Noise; Stream classification; Trending topics; Data mining",2-s2.0-85028414168
"Kularbphettong K.","Analysis of Students’ Behavior Based on Educational Data Mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029580957&doi=10.1007%2f978-3-319-67621-0_15&partnerID=40&md5=5b8065c02896d4626714a37ff91164f1","This research aims to develop a model for analysis of student behavior through e-Learning based on data mining technique in case of Suan Sunandha Rajabhat University. The student data set was composed of 5392 personal records and, to compare the effective of algorithm, the model was created under decision tree and Bayesian networks techniques. The result found that showed that Bayesian networks technique showed higher performance and the percentage of prediction is accurate 91.32%. © 2018, Springer International Publishing AG.","Bayesian networks; Decision tree; Educational data mining; Student’s behavior","Bayesian networks; Computational methods; Decision trees; Education; Students; Trees (mathematics); Behavior-based; Data set; Educational data mining; Student behavior; Data mining",2-s2.0-85029580957
"Hange U., Selvaraj R., Galani M., Letsholo K.","A data-mining model for predicting low birth weight with a high AUC",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020403611&doi=10.1007%2f978-3-319-60170-0_8&partnerID=40&md5=1ac52e463072ab1fdd38ebdc943ada2e","Birth weight is a significant determinant of a newborn’s probability of survival. Data-mining models are receiving considerable attention for identifying low birth weight risk factors. However, prediction of actual birth weight values based on the identified risk factors, which can play a significant role in the identification of mothers at the risk of delivering low birth weight infants, remains unsolved. This paper presents a study of data-mining models that predict the actual birth weight, with particular emphasis on achieving a higher area under the receiver operating characteristic (AUC). The prediction is based on birth data from the North Carolina State Center for Health Statistics of 2006. The steps followed to extract meaningful patterns from the data were data selection, handling missing values, handling imbalanced data, model building, feature selection, and model evaluation. Decision trees were used for classifying birth weight and tested on the actual imbalanced dataset and the balanced dataset using synthetic minority oversampling technique (SMOTE). The results highlighted that models built with balanced datasets using the SMOTE algorithm produce a relatively higher AUC compared to models built with imbalanced datasets. The J48 model built with balanced data outperformed REPTree and Random tree with an AUC of 90.3%, and thus it was selected as the best model. In conclusion, the feasibility of using J48 in birth weight prediction would offer the possibility to reduce obstetric-related complications and thus improving the overall obstetric health care. © Springer International Publishing AG 2018.","Birth weight; Data-mining; Imbalanced dataset; Low birth weight; SMOTE","Classification (of information); Data handling; Decision trees; Forecasting; Trees (mathematics); Birth weight; Birth weight prediction; Handling missing values; Imbalanced dataset; Low birth weights; Receiver operating characteristics; SMOTE; Synthetic minority over-sampling techniques; Data mining",2-s2.0-85020403611
"Scardapane S., Altilio R., Ciccarelli V., Uncini A., Panella M.","Privacy-preserving data mining for distributed medical scenarios",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029180413&doi=10.1007%2f978-3-319-56904-8_12&partnerID=40&md5=8ef124d43fffc427c3a5ae6873624712","In this paper, we consider the application of data mining methods in medical contexts, wherein the data to be analysed (e.g. records from different patients) is distributed among multiple clinical parties. Although inference procedures could provide meaningful medical information (such as optimal clustering of the subjects), each party is forbidden to disclose its local dataset to a centralized location, due to privacy concerns over sensible portions of the dataset. To this end, we propose a general framework enabling the parties involved to perform (in a decentralized fashion) any data mining procedure relying solely on the Euclidean distance among patterns, including kernel methods, spectral clustering, and so on. Specifically, the problem is recast as a decentralized matrix completion problem, whose proposed solution does not require the presence of a centralized coordinator, and full privacy of the original data can be ensured by the use of different strategies, including random multiplicative updates for secure computation of distances. Experimental results support our proposal as an efficient tool for performing clustering and classification in distributed medical contexts. As an example, on the known Pima Indians Diabetes dataset, we obtain a Rand-Index for clustering of 0.52 against 0.54 of the (unfeasible) centralized solution, while on the Parkinson speech database we increase from 0.45 to 0.50. © Springer International Publishing AG 2018.","Biomedicine; Distributed learning; Kernel methods; Privacy; Spectral clustering","Clustering algorithms; Data mining; Medical computing; Biomedicine; Distributed learning; Kernel methods; Matrix completion problems; Medical information; Multiplicative updates; Privacy preserving data mining; Spectral clustering; Data privacy",2-s2.0-85029180413
"Goyal A., Khandelwal I., Anand R., Srivastava A., Swarnalatha P.","A comparative analysis of the different data mining tools by using supervised learning algorithms",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028581672&doi=10.1007%2f978-3-319-60618-7_11&partnerID=40&md5=296529e57676b3a9114fad421f03ea18","These days a lot of raw data is generated from various common sources. This large amount of data, which would appear useless at first glance, is very important for companies and researchers as could provide a lot of helpful information. The data could be mined to get useful knowledge that could be used to make fruitful decisions. A lot of online tools and proprietary toolkits are available to the users and it becomes all the more cumbersome for them to know which is the best tool among these for the supervised learning algorithm and datasets they are applying. In order to aid this process, the paper progresses in this direction by doing a comparison of various data mining tools on the basis of their classification finesse. The various tools used in the paper are weka, knime and tanagra. Rigorous work on this has given the result that the performance of the tools is affected by the kind of datasets used and the way in which the supervised learning is done. © Springer International Publishing AG 2018.","Knime; Mining tools; Supervised learning; Tanagra; Weka","Data mining; Pattern recognition; Soft computing; Supervised learning; Common source; Comparative analysis; Data-mining tools; Knime; Large amounts; On-line tools; Tanagra; Weka; Learning algorithms",2-s2.0-85028581672
"Muchová M., Paralič J., Nemčík M.","Using predictive data mining models for data analysis in a logistics company",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029522398&doi=10.1007%2f978-3-319-67220-5_15&partnerID=40&md5=e5fccdc6b47b0bbf38197b79c1e46725","The aim of this paper is to apply predictive data mining (DM) techniques in order to predict the average fuel consumption for trucks and drivers resp., to identify the key factors that affect fuel consumption of vehicles and also to identify best practices and driving styles of drivers. For this purpose different models have been proposed to provide an overview of the key factors affecting fuel consumption for individual vehicles and their drivers. Predictive models enabled us to identify main influencing factors and provide recommendations for a logistics company to reduce the fuel consumption. Data were collected from Dynafleet information system of a small transport company. The company is dealing with freight traffic, particularly trucks. We first describe selected projects dealing with similar tasks in this area. Next, we explore and analyze data using CRISP-DM methodology by appropriate methods designed for data mining and then evaluate the results of the experiments. © 2018, Springer International Publishing AG.","CRISP-DM; Data mining; Dynafleet; Fuel consumption; Naive bayes; Neural network; Predictive data mining",,2-s2.0-85029522398
"Szeląg B., Studziński J.","Modelling and forecasting the sludge bulking in biological reactors of wastewater treatment plants by means of data mining methods",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593297&doi=10.1007%2f978-3-319-64465-3_29&partnerID=40&md5=7d105ec43b83c24208f375532333ffa3","The bulking of active sludge in treatment plant bioreactors occurs very often in communal wastewater works what leads to worsening the abilities of sludge sedimentation and the efficiency of works operation. Because of that there is useful and suitable to model and predict the sludge bulking events in order to take some counteractions. In the paper the data mining methods of Support Vector Machines (SVM), Boosted Trees, Random Forests and Multivariate Adaptive Regression Splines (MARS) have been used for modelling and forecasting the sludge bulking events. By the calculation the measurement data series from 4 years concerning the physical and chemical parameters of wastewater flowing into the treatment plant investigated and the technological parameters of the plant bioreactor were used. The calculation results show that the best sludge bulking model containing the best prediction ability has been received by the MARS method and on another side the worst models have been generated by the Random Forests method. © Springer International Publishing AG 2018.","Data mining methods; Mathematical modelling; Sewage treatment processes","Biological water treatment; Bioreactors; Data mining; Decision trees; Intelligent systems; Maintenance; Mathematical models; Production; Sewage sludge; Sewage treatment; Support vector machines; Wastewater treatment; Calculation results; Data mining methods; Modelling and forecasting; Multivariate adaptive regression splines; Physical and chemical parameters; Sewage treatment process; Technological parameters; Wastewater treatment plants; Forecasting",2-s2.0-85028593297
"Sánchez-Silva D.M., Acosta-Mesa H.G., Romo-González T.","Semi-Automatic Analysis for Unidimensional Immunoblot Images to Discriminate Breast Cancer Cases Using Time Series Data Mining",2018,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026352050&doi=10.1142%2fS0218001418600042&partnerID=40&md5=b052b03e133d9175781c358d56064a26","Breast cancer (BC) is one of the leading causes of death in adult women worldwide and the best way to reduce mortality and improve prognosis is through early diagnosis. Thus, it is necessary to optimize diagnostic methods; one option could be the automatic detection of patterns in 1D-II. In that respect, through recent analysis of unidimensional Immunoblot Images (1D-II), it was possible to distinguish between women with and without breast disease using as a discrimination criterion the presence of autoantibodies (bands) in their blood. However, the analysis of 1D-II is a difficult task even for an expert, generating great subjectivity and complexity in the process of interpretation. In the present study, a semi-automatic methodology for the bands' analysis contained in the 1D-II's was implemented and evaluated, the bands were extracted using digital image processing techniques. This was possible through the recognition of banding patterns represented as time series to distinguish between three classes: women with breast cancer (BC), women with benign breast pathology (BBP) and women without breast pathology (H). The classification was performed using the machine learning algorithm k-nearest neighbors (KNN) with different parameters over the time series representation. The semi-automatic method here presented was able to reduce the time, complexity and subjectivity of the image analysis with the performance metrics compared, obtaining similar percentages for both representations. With the traditional analysis, binary representation [Accuracy 72.8%, Precision 73.42% for three classes (BC, BBP and H) and Accuracy 90.91% Accuracy 92.55% Sensitivity 93.57% and Specificity 92.99% for two classes (BC and H)], versus Time series representation [Accuracy 66.4%, Precision 67.07% for three classes (BC, BBP and H) and Accuracy 86.36% Accuracy 87.31% Sensitivity 95.86% and Specificity 85.56% for two classes (BC and H)]. © 2018 World Scientific Publishing Company.","Breast cancer; digital image processing; protein bands; semi-automatic method; time series data mining; unidimensional immunoblot","Automation; Data mining; Diagnosis; Diseases; Image analysis; Image processing; Learning algorithms; Medical imaging; Nearest neighbor search; Pathology; Pattern recognition; Time series; Breast Cancer; Immunoblots; Protein bands; Semiautomatic methods; Time series data mining; Time series analysis",2-s2.0-85026352050
"Sun G., Jiang C., Cheng P., Liu Y., Wang X., Fu Y., He Y.","Short-term wind power forecasts by a synthetical similar time series data mining method",2018,"Renewable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028696683&doi=10.1016%2fj.renene.2017.08.071&partnerID=40&md5=6f187b266f0d8eae46a3cba549ba8b15","As the aggravating influence of growing wind power, wind power forecasting research becomes more important in economic operation and safety management of power system. A novel short-term wind power forecasting methodology consists of a hybrid clustering method and a wavelet based neural network is introduced. The clustering similar measure function combines the Euclidean Distance and Angle Cosine together, aims to identify the similar wind speed days which are close in space distance and have similar variance trend synthetically. Then similar daily samples as the predicting days are treated as training samples of an improved particle swarm optimization based wavelet neural network. The proposed forecasting strategy is applied to two real wind farms in China. The results demonstrate that the strategy can identify the similar time series and improve the predicting accuracy effectively, compared with some other forecasting models. © 2017 Elsevier Ltd","Hybrid clustering method; Similarity measure; Wavelet neural network; Wind power forecasts","Cluster analysis; Data mining; Forecasting; Particle swarm optimization (PSO); Time series; Wavelet analysis; Wind; Wind effects; Wind power; Hybrid clustering; Short-term wind power forecast; Short-term wind power forecasting; Similarity measure; Time series data mining; Wavelet neural networks; Wavelet-based neural network; Wind power forecast; Weather forecasting; accuracy assessment; artificial neural network; cluster analysis; data mining; economic analysis; forecasting method; optimization; time series; wavelet analysis; wind farm; wind power; wind velocity; China",2-s2.0-85028696683
"Chen Y.-S., Lin C.-K., Chen W.-S.","Performance evaluation of data mining technologies: An example of ERP system adoption",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421864&doi=10.1007%2f978-981-10-3187-8_94&partnerID=40&md5=1ea5de574ba2dd78e9fa428b318bdcec","Traditionally, most studies highlight on exploring the critical success factors (CSFs) of ERP system implementation, and the lack of effective methods for identifying ERP system adoption. In addition, it is an important and interesting issue to help ERP system vendor selecting a suitable customer who will survey an appropriate decision to adopt ERP system. We compare the results of the decisional feature database constructed by two classification prediction models, Models 1 and 2, and find out the critical factors of industrial evaluation for ERP system summarized through the empirical results and hypothesis. The empirical results include Model 1: the accuracy of percentage split without feature selection can reach 89.7810% at maximum, with the minimum value of 57.6642%, and Model 2: the accuracy of percentage split with expert feature selection can reach 89.7810% at maximum, with the minimum value of 54.0146%. © Springer Nature Singapore Pte Ltd. 2018.","Classification model; Enterprise resource planning (ERP) system; Expert feature selection","Classification (of information); Computation theory; Data mining; Feature extraction; Classification models; Classification prediction; Critical success factor; Data mining technology; Enterprise resource planning systems; ERP system adoption; Erp system implementations; Industrial evaluations; Enterprise resource planning",2-s2.0-85031421864
"Chovatiya F., Prajapati P., Vasa J., Patel J.","A research direction on data mining with IOT",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028421063&doi=10.1007%2f978-3-319-63673-3_22&partnerID=40&md5=9a72b2dfc6aae128fb5a38c6c71b84b1","The mission of connecting everything on the earth together via internet seems to be impossible. There will be the great effect on human life by Internet of Things (IOT), because with the help of IOT, many impossible things will become possible. IOT devices generates big data having useful, valuable and highly accurate data. It is difficult to extract the required information or data from the set of big data discovered by any device. For this purpose, data mining is used. Data mining will plays important role in constructing smart system that provides convenient services. It is required to extract data and knowledge from the connected things. For this purpose, various data mining techniques are used. Various algorithms such as classification, clustering, association rule mining etc. helps to mine data. This paper represents the different Data mining techniques, challenges, and Data mining issues with IOT. © 2018, Springer International Publishing AG.","Classification; Clustering; Data mining; Frequent pattern; Internet of things","Big data; Classification (of information); Intelligent systems; Internet of things; Clustering; Frequent pattern; Highly accurate; Human lives; Internet of Things (IOT); Smart System; Data mining",2-s2.0-85028421063
"Chowdhury M., Rahman A., Islam R.","Malware analysis and detection using data mining and machine learning classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032708717&doi=10.1007%2f978-3-319-67071-3_33&partnerID=40&md5=ccfe68567e910edcb3f082a3fed13ec1","Exfiltration of sensitive data by malicious software or malware is a serious cyber threat around the world that has catastrophic effect on businesses, research organizations, national intelligence, as well as individuals. Thousands of cyber criminals attempt every day to attack computer systems by employing malicious software with an intention to breach crucial data, damage or manipulate data, or to make illegal financial transfers. Protection of this data is therefore, a critical concern in the research community. This manuscript aims to propose a comprehensive framework to classify and detect malicious software to protect sensitive data against malicious threats using data mining and machine learning classification techniques. In this work, we employ a robust and efficient approach for malware classification and detection by analyzing both signature-based and anomaly-based features. Experimental results confirm the superiority of the proposed approach over other similar methods. © 2018, Springer International Publishing AG.","Classification; Cyber threat; Data security; Machine learning; Malware","Artificial intelligence; Classification (of information); Computer crime; Crime; Data mining; Learning systems; Network security; Security of data; Catastrophic effects; Cyber criminals; Cyber threats; Machine learning classification; Malware analysis; Malware classifications; Research communities; Research organization; Malware",2-s2.0-85032708717
"Alaiz-Moreton H., Fernández-Robles L., Alfonso-Cendón J., Castejón-Limas M., Sánchez-González L., Pérez H.","Data mining techniques for the estimation of variables in health-related noisy data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028689016&doi=10.1007%2f978-3-319-67180-2_47&partnerID=40&md5=7127115fe651f09f2492c99e3591c3e9","Public health in developed countries is heavily affected by pollution specially in highly populated areas. Amongst the pollutants with greatest impact in health, ozone is particularly addressed in this paper due to importance of its effect on cardiovascular and respiratory problems and their prevalence on developed societies. Local authorities are compelled to provide satisfactory predictions of ozone levels and thus the need of proper estimation tools rises. A data driven approach to prediction demands high quality data but those observations collected by weather stations usually fail to meet this requirement. This paper reports a new approach to robust ozone levels prediction by using an outlier detection technique in an innovative way. The aim is to assess the feasibility of using raw data without preprocessing in order to obtain similar or better results than with traditional outlier removal techniques. An experimental dataset from a location in Spain, Ponferrada, is used through an experimental stage in which such approach provides satisfactory results in a difficult case. © 2018, Springer International Publishing AG.","Outlier detection; Ozone; Weighted regression","Air pollution; Data handling; Forecasting; Health; Ozone; Pollution; Soft computing; Statistics; Data-driven approach; Developed countries; High quality data; Local authorities; Outlier Detection; Respiratory problems; Satisfactory predictions; Weighted regression; Data mining",2-s2.0-85028689016
"Tran T., Le U.","Predicting breast cancer risk: A data mining approach",2018,"IFMBE Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030831476&doi=10.1007%2f978-981-10-4361-1_37&partnerID=40&md5=98a8e120fbdcc9a590ccc0f90810ab25","Early prediction of breast cancer plays a critical role in successful treatment and saving lives of thousands of patients every year. Although massive clinical data related to the patients is being collected and stored by healthcare organizations, only a small subset of the predictive factors has been used in predicting outcomes. Most of the existing approaches focus on applying statistical techniques on small set of attributes recommended by the domain-experts disease diagnostics. These conventional approaches usually make unrealistic assumptions, e.g., normality, independence or linearity relationships, which may not be always true in practical data. On the other hand, advanced statistical approaches may address some of the above shortcomings; however, they are computationally expensive and may not applicable to massive datasets. In this study, we use a data mining approach which offers significant advantages over conventional techniques to address the existing limitations. Our data-driven approach can efficiently process clinical dataset to discover patterns and reveal hidden information for early detection and successfully treatment of breast cancer patients. © Springer Nature Singapore Pte Ltd. 2018.","Breast cancer prediction; Data mining; Informatics; Massive datasets","Biomedical engineering; Diagnosis; Diseases; Forecasting; Patient treatment; Breast Cancer; Conventional approach; Conventional techniques; Data-driven approach; Healthcare organizations; Informatics; Massive data sets; Statistical techniques; Data mining",2-s2.0-85030831476
"Manuel J., Cordeiro R., Silva C.","Between Data Mining and Predictive Analytics Techniques to Cybersecurity Protection on eLearning Environments",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029599904&doi=10.1007%2f978-3-319-67621-0_17&partnerID=40&md5=c2d6825e018575bcd7ed8c375b500dc5","This paper aims to present a hypothetic theory of intelligent security system. In society the threat of cyber-attacks is getting louder and the use of computers, criminal activity has also changed from physical to cybernetic intrusion. There had been many cyber security solutions used to counteract these attacks, however we highlight the importance of self-protected systems in defense and in a correct analysis of cyber attacks. The internet is vulnerable to cyber-attacks as well as the information found in data systems and through a form of recognition and extraction of relevant information, we can represent data as shared data and integrated to intelligent system. What was used us a static firewall is now intended to be dynamic and self-critical. By techniques of data analysis, statistics, machine learning, data mining, the cybersecurity and privacy challenges are within our reach. This paper examines data mining techniques in order to predict pathways of Internet security and which considerations are involved in the theoretical solutions presented for the privacy systems such as the e-Learning environments. © 2018, Springer International Publishing AG.","Cybersecurity; Data mining; Intelligent firewall; Intrusion detection systems; Predictive models","Artificial intelligence; Computation theory; Computational methods; Computer aided instruction; Computer crime; Computer system firewalls; Crime; Data privacy; Distributed computer systems; E-learning; Intelligent systems; Intrusion detection; Learning systems; Mercury (metal); Network security; Predictive analytics; Criminal activities; Cyber security; E-learning environment; Intelligent firewall; Intelligent security systems; Intrusion Detection Systems; Predictive models; Theoretical solutions; Data mining",2-s2.0-85029599904
"Micek M., Pacholczyk M.","Searching for cancer signatures using data mining techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030786738&doi=10.1007%2f978-3-319-67792-7_16&partnerID=40&md5=a4b5d45268140ac61a1e146de5404a12","Data mining finds many uses in biotechnology and one of them may be to analyze multi-platform data in order to allow searching for genomic cancer signatures. The importance of the topic arises as nowadays cancer is noted one of the leading causes of deaths in highly developed countries. The goal of this work was to search for colorectal cancer signatures, consisting of somatic mutations, somatic gene copy number alterations (SCNAs) as well as abnormal expression levels. After acquiring mutation, SCNA and expression data from cBioPortal, frequent itemset mining was performed using basket analysis and apriori algorithm. We also performed survival analysis of colorectal cancer patients using the discovered signatures as differentiating factor for Kaplan-Meier curve comparison. Frequent itemset mining returned modifications of genes that can be regarded as potential colorectal cancer signatures or signatures of carcinogenic processes in general. While methods used in the project consisted of use of simple or even basic tools, the results suggest that searching for cancer signatures amidst multi-platform data may be worth developing and improving. © 2018, Springer International Publishing AG.","Apriori; Basket analysis; Colorectal cancer; Data mining; Survival analysis","Bioinformatics; Data mining; Gene expression; Genes; Apriori; Apriori algorithms; Basket analysis; Carcinogenic process; Colorectal cancer; Developed countries; Frequent itemset mining; Survival analysis; Diseases",2-s2.0-85030786738
"Tan Z., Wang J., Peng Y., Ma F.","The admissions big data mining research based on real data from a normal university",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030844854&doi=10.1007%2f978-981-10-6496-8_49&partnerID=40&md5=997b886e4b37cb1edf8a3a1db4e8ad06","In this paper, a Normal University’s 2011–2016 real admissions data are analyzed by the Apriori, K-MEANS and KNN algorithm. The result shows that the university’s normal students are more likely to choose other normal majors than to choose other non-normal majors related the normal majors and the overall situation of the Normal University’s student enrollment is relatively stable. Liberal arts college is the most popular college. Chinese language and Literature (normal) and English (normal) are more popular in the Normal University. The result reveals the internal connection between the various majors and has a guiding role for specialties setup in the university. © 2018, Springer Nature Singapore Pte Ltd.","Admissions data; Apriori algorithm; K-MEANS algorithm; KNN algorithm","Big data; Intelligent systems; Learning algorithms; Admissions data; Apriori algorithms; Chinese language; Internal connections; k-Means algorithm; k-NN algorithm; Normal students; Student enrollments; Data mining",2-s2.0-85030844854
"Yang H.-H., Huang M.-L., Lai C.-M., Jin J.-R.","An approach combining data mining and control charts-based model for fault detection in wind turbines",2018,"Renewable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029081047&doi=10.1016%2fj.renene.2017.09.003&partnerID=40&md5=445d704fcc6bd6df78016bd9b65ddd77","Wind energy is growing to be one of main sources of renewable energy. As the operational and maintenance costs of wind turbines are adversely affected by the occurrence of faults, the early detection of potential faults can help reduce such costs. In this study, we propose a method for detecting potential faults sooner and identifying the probable variables contributing to the faults over a certain period as well as at a specific time. The proposed method uses data mining techniques to select the more important variables from the supervisory control and data acquisition (SCADA) systems of the turbine to improve the prediction accuracy and employs an exponentially weighted moving average (EWMA) model-based control chart to implement the residual approach, in order to remove the autocorrelation in the data. Both EWMA and multivariate EWMA (MEWMA) control charts are constructed so that their detection capabilities as well as the types of errors generated can be compared. We evaluated the proposed method by using both the SCADA data and the alarm log of a turbine. It was observed that the MEWMA chart is more suitable than the EWMA chart for the early detection and avoidance of errors. © 2017 Elsevier Ltd","Fault diagnosis; Feature extraction; Statistical process control; Wind power","Control charts; Data acquisition; Data mining; Failure analysis; Feature extraction; Flowcharting; SCADA systems; Statistical process control; Wind power; Wind turbines; Detection and avoidances; Detection capability; Exponentially weighted moving average; Model-based control chart; Potential faults; Prediction accuracy; Renewable energies; Supervisory control and dataacquisition systems (SCADA); Fault detection; accuracy assessment; control system; data acquisition; data mining; detection method; numerical model; operations technology; renewable resource; statistical analysis; wind power; wind turbine",2-s2.0-85029081047
"Khan S., Dembla D.","Implementation of modified K-means approach for privacy preserving in data mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031395248&doi=10.1007%2f978-981-10-3773-3_58&partnerID=40&md5=1d3ed1531f18eb0cdc95d4f9a0269086","Recent concerns regarding privacy breach issues have motivated the development of data mining methods, which preserve the privacy of individual data item. A cluster is gathering of information in such a way that the objects with similar properties are grouped into similar clusters and objects with dissimilar properties are placed into different clusters. The K-Means clustering algorithm is a broadly utilized plan to solve the clustering problem. In this paper, a comparative study of three clustering algorithms—K-means, Hierarchical and Cobweb across two different datasets is being performed. To form Clusters WEKA API has been used. The comparison is made with the variant of standard K-means technique that is Modified K-means technique. The Modified K-means technique has been developed to give better results as compared to existing K-means, Hierarchical and Cobweb techniques. This work also includes encryption and decryption of the formed clusters using AES algorithm to provide privacy to the data while transferring over networks. Experimental result proves that the performance of Modified K-means algorithm is better as compared to the existing K-Means and better than the hierarchical and Cobweb when tested on two datasets. K-Means and Hierarchical clustering is forming less number of clusters. In contrast, Cobweb is forming many clusters, which create memory issues. Therefore, Modified K-means forms an appropriate number of clusters in an organized manner and also takes minimum amount of time. © Springer Nature Singapore Pte Ltd. 2018.","AES; Clustering; File joining; File splitting; K-means","Cryptography; Data mining; Data privacy; Clustering; Clustering problems; Comparative studies; Encryption and decryption; File splitting; Hier-archical clustering; K-means; K-Means clustering algorithm; Clustering algorithms",2-s2.0-85031395248
"Korycki Ł., Krawczyk B.","Combining active learning and self-labeling for data stream mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019231123&doi=10.1007%2f978-3-319-59162-9_50&partnerID=40&md5=7668e5e6f072d4b4bcc05a2e78add71c","Data stream mining is among the most vital contemporary data science challenges. In this work we concentrate on the issue of actual availability of true class labels. Assumption that the ground truth for each instance becomes known right after processing it is far from being realistic, due to usually high costs connected with its acquisition. Active learning is an attractive solution to this problem, as it selects most valuable instances for labeling. In this paper, we propose to augment the active learning module with self-labeling approach. This allows classifier to automatically label instances for which it displays the highest certainty and use them for further training. Although in this preliminary work we use a static threshold for self-labeling, the obtained results are encouraging. Our experimental study shows that this approach complements the active learning strategy and allows to improve data stream classification, especially in scenarios with very small labeling budget. © Springer International Publishing AG 2018.","Active learning; Data stream mining; Machine learning; Self-labeling; Semi-supervised learning","Budget control; Data communication systems; Learning algorithms; Learning systems; Supervised learning; Active Learning; Active learning strategies; Attractive solutions; Data stream classifications; Data stream mining; Further trainings; Semi- supervised learning; Static thresholds; Artificial intelligence",2-s2.0-85019231123
"Park J., Kim Y., Jung W.","Use of a big data mining technique to extract relative importance of performance shaping factors from event investigation reports",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023204086&doi=10.1007%2f978-3-319-60645-3_23&partnerID=40&md5=afca447315aaefd90da4d370c8075fce","In this study, the relative importance of significant performance shaping factors (PSFs), which is critical for estimating the human error probability (HEP) of a given task environment is extracted from event investigation reports of domestic nuclear power plants (NPPs). Each event was caused by one or more human performance related problems (i.e., human errors), and its investigation report includes detailed information describing why the corresponding event has occurred. Based on 10 event reports, 47,220 data records were identified, which represent the task environment of 11 human errors in terms of significant PSFs. After that, the relative importance of the associated PSFs was analyzed by using a CART (Classification and Regression Tree) method that is one of the representative techniques to scrutinize the characteristics of big data. © Her Majesty the Queen in Right of United Kingdom 2018.","Classification and regression tree; Event investigation report; Human reliability analysis; Nuclear power plant; Performance shaping factors","Big data; Errors; Forestry; Nuclear energy; Nuclear fuels; Nuclear power plants; Reliability; Reliability analysis; Trees (mathematics); Classification and regression tree; Event investigation report; Event report; Human error probability; Human performance; Human reliability analysis; Performance shaping factors; Task environment; Data mining",2-s2.0-85023204086
"Czyczyn-Egird D., Wojszczyk R.","The effectiveness of data mining techniques in the detection of DDoS attacks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022177249&doi=10.1007%2f978-3-319-62410-5_7&partnerID=40&md5=5679e685a01dc34befb01a8a6f324786","The term of online attacks appeared in public space in the area of computer networks long ago. The effects of these actions can be difficult to rectify and also very expensive. For early detection of such attacks, one can use different methods to analyze the input data generated by the network communication interfaces. The article presented the results of the research on effectiveness of data mining techniques in the detection of DDoS attacks on the selected network resources. © Springer International Publishing AG 2018.","Computer networks; Data mining; DDoS attack; Design patterns","Artificial intelligence; Computer networks; Denial-of-service attack; Distributed computer systems; DDoS Attack; Design Patterns; Input datas; Network communications; Network resource; Public space; Data mining",2-s2.0-85022177249
"Solanki P., Gopal G.","Image categorization using improved data mining technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031405458&doi=10.1007%2f978-981-10-6620-7_19&partnerID=40&md5=884aad2d0dd8c6dbd78481b324203d8b","Image categorization is one of the important branches of artificial intelligence. Categorization of images is a way of grouping images according to their similarity. Image categorization uses various features of images like texture, color component, shape, edge, etc. Categorization process has various steps like image preprocessing, object detection, object segmentation, feature extraction, and object classification. For the past few years, researchers have been contributing different algorithms in the two most common machine learning categories to either cluster or classify images. The goal of this paper is to discuss two of the most popular machine learning algorithms: Nearest Neighbor (k-NN) for image classification and Means clustering algorithm. After that, a Hybrid model of both the above algorithms is proposed. These algorithms are implemented in MATLAB; finally, the experimental results of each algorithm are presented and discussed. © 2018, Springer Nature Singapore Pte Ltd.","Image categorization; Means algorithm; Nearest neighbor (NN) algorithm","Artificial intelligence; Big data; Data mining; Feature extraction; Image classification; Image enhancement; Image segmentation; Imaging systems; Learning algorithms; Learning systems; Nearest neighbor search; Object detection; Color component; Image Categorization; Image preprocessing; Means clustering algorithm; Nearest neighbor algorithm; Nearest neighbors; Object classification; Object segmentation; Clustering algorithms",2-s2.0-85031405458
"Kaczmarek-Majer K., Hryniewicz O.","Data-mining approach to finding weights in the model averaging for forecasting of short time series",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029445618&doi=10.1007%2f978-3-319-66824-6_28&partnerID=40&md5=4f1e568037847364c66c612cdb22a5f9","Selection of an appropriate time series model and estimation of its parameters may become very challenging tasks for short series of observations. The state-of-the-art information criteria often fail to adequately identify the predictive model for the small sample sizes, for which real-life applications are routinely made. Within this research, we propose a forecasting approach for averaging across multiple predictive models and demonstrate its usefulness especially for small samples. The proposed method incorporates selected data-mining techniques and similarity measures to find most appropriate weights. The performance of the proposed method is illustrated with simulation study for stationary processes and the experimental study for the benchmark datasets. © 2018, Springer International Publishing AG.","Forecasting; Similarity measures; Small samples; Time series","Benchmarking; Fisher information matrix; Forecasting; Fuzzy logic; Fuzzy sets; Pattern matching; Time series; Benchmark datasets; Information criterion; Predictive modeling; Real-life applications; Similarity measure; Simulation studies; Small samples; Time series modeling; Data mining",2-s2.0-85029445618
"Rasel R.I., Sultana N., Meesad P.","An application of data mining and machine learning for weather forecasting",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022174793&doi=10.1007%2f978-3-319-60663-7_16&partnerID=40&md5=c9f3ce4f582f27d856a34ae99c152782","Weather forecasting for an area where the weather and climate changes occurs spontaneously is a challenging task. Weather is non-linear systems because of various components having a grate impact on climate change such as humidity, wind speed, sea level and density of air. A strong forecasting system can play a vital role in different sectors like business, agricultural, tourism, transportation and construction. This paper exhibits the performance of data mining and machine learning techniques using Support Vector Regression (SVR) and Artificial Neural Networks (ANN) for a robust weather prediction purpose. To undertake the experiments 6-years historical weather dataset of rainfall and temperature of Chittagong metropolitan area were collected from Bangladesh Meteorological Department (BMD). The finding from this study is SVR can outperform the ANN in rainfall prediction and ANN can produce the better results than the SVR. © Springer International Publishing AG 2018.","ANN; Data mining; Machine learning; Rainfall; SVM; Temperature; Weather forecasting","Agricultural machinery; Artificial intelligence; Climate change; Data mining; Education; Forecasting; Learning systems; Linear systems; Neural networks; Rain; Sea level; Temperature; Bangladesh; Forecasting system; Machine learning techniques; Metropolitan area; Rainfall prediction; Support vector regression (SVR); Weather prediction; Wind speed; Weather forecasting",2-s2.0-85022174793
"Peng H., Bai X.","Recovering area-to-mass ratio of resident space objects through data mining",2018,"Acta Astronautica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032294438&doi=10.1016%2fj.actaastro.2017.09.030&partnerID=40&md5=d7d83a4cc3a3e5c62478f1abae245bb5","The area-to-mass ratio (AMR) of a resident space object (RSO) is an important parameter for improved space situation awareness capability due to its effect on the non-conservative forces including the atmosphere drag force and the solar radiation pressure force. However, information about AMR is often not provided in most space catalogs. The present paper investigates recovering the AMR information from the consistency error, which refers to the difference between the orbit predicted from an earlier estimate and the orbit estimated at the current epoch. A data mining technique, particularly the random forest (RF) method, is used to discover the relationship between the consistency error and the AMR. Using a simulation-based space catalog environment as the testbed, this paper demonstrates that the classification RF model can determine the RSO's category AMR and the regression RF model can generate continuous AMR values, both with good accuracies. Furthermore, the paper reveals that by recording additional information besides the consistency error, the RF model can estimate the AMR with even higher accuracy. © 2017 IAA","Area-to-mass ratio; Consistency error; Data mining; Decision tree; Random forest; Resident space object","Decision trees; Drag; Errors; Area-to-mass ratios; Consistency error; Non-conservative forces; Random forests; Solar radiation pressure; Space catalogs; Space objects; Space situation awareness; Data mining",2-s2.0-85032294438
"Vaghela R.S., Gonsai A., Gami P.","Measurement (data mining) of real mobile signals data in weka tools for interference detection",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412781&doi=10.1007%2f978-3-319-63645-0_73&partnerID=40&md5=53215f1e28a89353beba10136220163b","In this paper we have collected the data from selected population of Rajkot city by the way of android and iPhone application after collecting all radio signals data like Wi-Fi signal power, GPS signal power, 4g signal power, 3g signal power, and Signal to noise ratio in different mobile device in different geographical location we can apply datamining technique by which can measure the different type of the scenario. After applying different method we can find hidden pattern and many insight to deal with interference situation. © Springer International Publishing AG 2018.","GPS; LTE; SNR; WEKA; WIFI","Data mining; Global positioning system; Intelligent systems; Mobile devices; Population statistics; Signal interference; Wi-Fi; Wireless local area networks (WLAN); Geographical locations; Hidden patterns; Interference detection; Interference situation; Mobile signals; Radio signals; WEKA; Wi-Fi signals; Signal to noise ratio",2-s2.0-85028412781
"Szeląg B., Gawdzik J., Studziński J.","Sludge volume index (SVI) modelling: Data mining approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029485254&doi=10.1007%2f978-3-319-67223-6_31&partnerID=40&md5=b561881b3b6660a843c61b40401d4138","In this paper, statistical models to forecast based on the sludge volume index (SVI) with the continuous measurements carried out in the period from 2013 to 2016 for waste water treatment Sitkowka-Nowiny was developed at the same, for two variants of analyses. In the first one, a model of SVI predicting based on the quality indicators of wastewater flowing into the treatment plant, i.e. Biochemical (BOD) and chemical oxygen demand (COD), the content of total nitrogen (TN) and ammonia nitrogen (NH4), total suspended solids, total phosphorus (TP) and the operating parameters of the bioreactor (pH, temperature, oxygen concentration in the nitrification chamber). In the second case, the possibility of replacing individual measurements of the quality of wastewater values calculated on the basis of daily sewage flows to the treatment plant was examined. The above mentioned models statistical analysis was performed using the method of k-nearest neighbor (k-NN), cascading neural network (CNN) and boosted tree (BT). To evaluate the predictive ability of these models the average relative error (MAE) and absolute error (MAPE) were used. The conducted analysis showed that based on the above mentioned indicators of effluent quality and technological parameters of the biological reactor it is possible to modeling of sediment volume index with satisfactory accuracy. In the case under consideration methods of lower values of the prediction error of SVI obtained using a cascade neural networks (MAE = 17.49 ml/g and MAPE = 9.80%) than for the method k-nearest neighbor (MAE = 27.85 ml/g and MAPE = 14.50%). Furthermore, based on the performed simulation, it was found that it is possible to model the analyzed work of the quality of waste water on the basis of the daily flow with reasonable accuracy, it is confirmed by the calculated value of the average and absolute and relative error, and the better ability predictive characterized by the models obtained on the basis CNN than k-NN. In examined cases, the MAP in a set of validation did not exceed 10.13%. The simulation results of quality indicators obtained by CNN were substituted in place of the explanatory variables of sludge volume index in the model for prediction index of sediment and conducted simulations SVI, set out the error MAE = 25.15 ml/g and MAPE = 15.26%. On this basis, it is possible to replace the measured values of the quality of the results of their simulation, thereby reducing the cost of testing, but also gives you continuous control of SVI and adjustments discussed in this work of technological parameters of the biological reactor. © 2018, Springer International Publishing AG.","Cascade neural network; K–nearest neighbor method; Sludge volume index; Wastewater treatment","Bioinformatics; Biological water treatment; Chemical oxygen demand; Data mining; Effluents; Errors; Forecasting; Indicators (chemical); Information systems; Motion compensation; Nearest neighbor search; Network architecture; Neural networks; Nitrogen; Sewage; Waste treatment; Wastewater treatment; Water quality; Water treatment; Average relative error; Cascade neural networks; Continuous measurements; Explanatory variables; Nearest neighbor method; Sludge volume index; Technological parameters; Total suspended solids; Quality control",2-s2.0-85029485254
"Kularbphettong K.","Enrichment Ontology Instance by Using Data Mining Techniques: A Case of Thai Tourist Interest in Culture Tourism",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029585381&doi=10.1007%2f978-3-319-67621-0_13&partnerID=40&md5=7256a8fa94df9cd9e3c3469d3053d29c","Ontology is an agreement about a shared conceptualization, which includes frameworks for modeling domain knowledge and agreements about the representation of particular domain theories, often captured in some form of a semantic web formally. However, building ontology is a time consuming task. however, the paper was presented an approach to enrich instances into the exiting ontology and this research presented the technique to extract information from the unstructured text from websites. Support vector machine was used to create model. The results showed that feature reduction and SVM techniques presented the highest precision than SVM approach. © 2018, Springer International Publishing AG.","Cultural tourism; Ontology enrichment; Support vector machine","Computation theory; Computational methods; Ontology; Support vector machines; Cultural tourism; Domain theory; Extract informations; Feature reduction; Model domains; Ontology enrichment; Time-consuming tasks; Unstructured texts; Data mining",2-s2.0-85029585381
"Dišek M., Šperka R., Kolesár J.","Conversion of real data from production process of automotive company for process mining analysis",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020381992&doi=10.1007%2f978-3-319-59394-4_22&partnerID=40&md5=b766b7eb8b302e52c3db7156788b9381","The aim of this paper is to convert the real data from the raw format from different information systems (log files) to the format, which is suitable for process mining analysis of a production process in a large automotive company. The conversion process will start with the import from several relational databases. The motivation is to use the DISCO tool for importing real pre-processed data and to conduct process mining analysis of a production process. DISCO generates process models from imported data in a comprehensive graphical form and provides different statistical features to analyse the process. This makes it possible to examine the production process in detail, identify bottlenecks, and streamline the process. The paper firstly presents a brief introduction of a manufacturing process in a company. Secondly, it provides a description of a conversion and pre-processing of chosen real data structures for the DISCO import. Then, it briefly describes the DISCO tool and proper format of pre-processed log file, which serves as desired input data. This data will be the main source for all consecutive operations in generated process map. Finally, it provides a sample analysis description with emphasis on one production process (process map and few statistics). To conclude, the results obtained show high demands on pre-processing of real data for suitable import format into DISCO tool and vital possibilities of process mining methods to optimize a production process in an automotive company. © Springer International Publishing AG 2018.","Data cleaning; Data cleaning tools; DISCO; Process mining","Data handling; Data mining; Mining; Multi agent systems; Automotive companies; Data cleaning; DISCO; Manufacturing process; Pre-processed data; Process mining; Relational Database; Statistical features; Data flow analysis",2-s2.0-85020381992
"Zhao L.J., Huang L., Lv Q., Yang T., Wei D.","WAMS/SCADA Data Fusion Method Study Based on Time-Series Data Correlation Mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028660064&doi=10.1007%2f978-3-319-67349-3_11&partnerID=40&md5=d052442f1f572880e79caca250ebf4cd","Hybrid measurement state estimation of WAMS data and the SCADA system is an effective method to improve the traditional state estimation. However, as the WAMS data and the SCADA data belong to different systems, there are great differences between them. To solve this problem, WAMS/SCADA data fusion method based on the correlation mining of time-series data is proposed in this paper. Firstly, WAMS/SCADA correlation estimation is done with the derivation of Pearson correlation coefficient. Then, solving the function model for the time difference issue and the alignment problem of correlation curves. After that, analyzing the measurement precision by considering the measurement weight and calculate the matrix of time series data weight to complete the optimization for the measurement precision. Finally, forming the effective fusion scheme based on the correlation of timing data. Simulation results on the IEEE 118 nodes system, with set a comparison of different hybrid measurement state estimation and different state estimation algorithm, effectiveness and stability of the proposed method has been proved. © 2018, Springer International Publishing AG.","correlation mining; Time-series data; WAMS/SCADA data fusion","Biomedical engineering; Correlation methods; Data fusion; SCADA systems; State estimation; Alignment Problems; Correlation estimation; Correlation mining; Data fusion methods; Measurement precision; Pearson correlation coefficients; State estimation algorithms; Time-series data; Time series",2-s2.0-85028660064
"Mishra B.K., Sahoo A.K., Misra R.","Recommendation for selecting smart village in India through opinion mining using big data analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031424559&doi=10.1007%2f978-981-10-6602-3_11&partnerID=40&md5=e3c1af51d74038ea692b3a4cbee31ba7","In India, most of the people are staying in below poverty line. Nowadays, village people are also most inadequate with mobile phones. To develop this village as smart, we emphasize on different factors like agriculture, employment, nutrition security, environment, natural resource utilization, and conservation, etc. We select smart village based on collection of different opinion from village people in terms of forms, questionnaire, views and surveys, etc. Opinion mining extracts useful knowledge about village from ample of opinions. This mining process gives us a right direction for creating smart village. In this paper, we are trying to create digitalized village which is basically an application of Information and Communications Technology to define the major function of Government in order to bring about Small, Moral, Accurate, Reliable and Transparent. To get accurate response, we use big data analytic concept after mining opinions using map reduce approach. © 2018, Springer Nature Singapore Pte Ltd.","Big data analytics; Map reduce; Opinion mining; Smart village","Conservation; Data mining; Rural areas; Surveys; Accurate response; Data analytics; Information and communications technology; Map-reduce; Mining process; Natural resource utilization; Opinion mining; Smart village; Big data",2-s2.0-85031424559
"Chang Y.","A parallel algorithm of mining frequent pattern on uncertain data streams",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032653230&doi=10.1007%2f978-3-319-67071-3_47&partnerID=40&md5=fb62f7969527f890c2c956473ef45864","At present, more and more data are generated every day and the actual application requirements for the mining algorithm efficiency have become higher. In such a situation, one of the hot research topics on the frequent pattern mining over uncertain data is the spatiotemporal efficiency improvement of mining algorithms. Aiming at solving the frequent pattern mining problems over dynamic uncertain data streams, based on the existing algorithm researches, the paper proposes a parallel mining approximation algorithm based on the MapReduce framework by combining a highly efficient algorithm for static data. If this algorithm is used to mine frequent patterns, all the frequent patterns can be mined from a sliding window by using MapReduce at most twice. In the experiments conducted for this paper, in most cases the frequent item set was accurately discovered after MapReduce is used once. The experiments have shown that the spatiotemporal efficiency of the algorithm proposed in this paper is much better than those of the other algorithms. © 2018, Springer International Publishing AG.","Data mining; Frequent pattern; Parallel algorithm; Uncertain data","Approximation algorithms; Efficiency; Parallel algorithms; Algorithm researches; Application requirements; Efficiency improvement; Frequent pattern; Frequent pattern mining; Mapreduce frameworks; Uncertain data streams; Uncertain datas; Data mining",2-s2.0-85032653230
"AlJadda K., Korayem M., Ortiz C., Grainger T., Miller J.A., Rasheed K.M., Kochut K.J., Peng H., York W.S., Ranzinger R., Porterfield M.","Mining massive hierarchical data using a scalable probabilistic graphical model",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497883&doi=10.1016%2fj.ins.2017.10.014&partnerID=40&md5=cb95de85dab5d2790b16b6a4f077b6db","Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and data mining. The crucial limitation of those models, however, is their scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and data mining, demonstrates this limitation when the training data consists of random variables, in which each of them has a large set of possible values. In the big data era, one could expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data - data that is arranged in a treelike structure with several levels - one may see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, unrestricted Bayesian Networks may become infeasible for representing the probability distributions. In this paper, we introduce an extension to Bayesian Networks that can handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves high precision and high recall when used as a multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by CareerBuilder.com, the model was able to predict latent semantic relationships among search keywords with high accuracy. © 2017","Big data; Large scale machine learning; Mass spectrometry annotation; Probabilistic model; Smantic discovery","Artificial intelligence; Bayesian networks; Classification (of information); Data mining; Graphic methods; Knowledge based systems; Learning systems; Mass spectrometry; Probability distributions; Semantics; Spectrometry; Speech recognition; Electronic device; Large-scale machine learning; Mass spectrometry data; Probabilistic graphical models; Probabilistic graphical models (PGM); Probabilistic modeling; Smantic discovery; Tree-like structures; Big data",2-s2.0-85031497883
"Weichbroth P.","Frequent sequence mining in web log data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030758775&doi=10.1007%2f978-3-319-67792-7_45&partnerID=40&md5=1292c6b7ceb2e6d516430c869e0b7b3e","The amount of information available even on a single web server can be huge. On the other hand, the amount of visitors (users) can often reach a number of at least six digits. Users vary in gender, age and education, and in consequence their information needs are different. Moreover, they subconsciously expect to get more adequate content after visiting the first few pages. The scope of this kind of problem relates to the domain of information filtering, as a method for delivering relevant information. To solve such a problem, different sources of unstructured or structured data can be used, one of the latter type being web server log data. Executed logging processes on the server side can gather valuable data showing requests sent by users to available resources shared on a particular web site. In this paper, we introduce the Apriori-like FWP algorithm for frequent sequence mining in web log data. Discovered sequences present reconstructed navigation paths across shared web pages by a number of users satisfying a defined minimum. Such knowledge can primarily be used for content recommendation, as well as in cross-marketing strategies and email promotion campaigns. © 2018, Springer International Publishing AG.","Mining; Sequence; Usage; Web","Blogs; Data mining; Marketing; Mining; Web services; Websites; Amount of information; Content recommendations; Frequent sequences; Marketing strategy; Navigation paths; Sequence; Structured data; Usage; Information filtering",2-s2.0-85030758775
"Watada J., Tan S.C., Matsumoto Y., Vasant P.","Rough set-based text mining from a large data repository of experts’ diagnoses for power systems",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020405285&doi=10.1007%2f978-3-319-59424-8_13&partnerID=40&md5=005bf543c13251000ba83b0cfa00e38d","Usually it is hard to classify the situation where uncertainty of randomness and fuzziness exists simultaneously. This paper presents a rough set approach applying fuzzy random variable and statistical t-test to text-mine a large data repository of experts’ diagnoses provided by a Japanese power company. The algorithms of rough set and statistical t-test are used to distinguish whether a subset can be classified in the object set or not. The expected-value-approach is also applied to calculate the fuzzy value with probability into a scalar value. © Springer International Publishing AG 2018.","Expected-value-approach; Fuzzy statistical test; Randomness and fuzziness; Rough set","Data mining; Electric utilities; Fuzzy set theory; Fuzzy systems; Random processes; Expected values; Fuzzy random variable; Large data; Power company; Randomness and fuzziness; Rough-set based; Scalar values; Text mining; Rough set theory",2-s2.0-85020405285
"Ribeiro C., Pinto T., Vale Z., Baptista J.","Data mining for prosumers aggregation considering the self-generation",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022227545&doi=10.1007%2f978-3-319-62410-5_12&partnerID=40&md5=469df361d2af50e6be332bc0ae28c621","Several challenges arrive with electrical power restructuring, liberalized electricity markets emerge, aiming to improve the system’s efficiency while offering new economic solutions. Privatization and liberalization of previously nationally owned systems are examples of the transformations that have been applied. Microgrids and smart grids emerge and new business models able to cope with new opportunities start being developed. New types of players appear, allowing aggregating a diversity of entities, e.g. generation, storage, electric vehicles, and consumers, Virtual Power Players (VPPs) are a new type of player that allows aggregating a diversity of players to facilitate their participation in the electricity markets. A major task of VPPs is the remuneration of generation and services (maintenance, market operation costs and energy reserves), as well as charging energy consumption. The paper proposes a normalization method that supports a clustering methodology for the remuneration and tariffs definition. This model uses a clustering algorithm, applied on normalized load values, the value of the micro production, generated in the bus associated to the same load, was subtracted from the value of the consumption of that load. This calculation is performed in a real smart grid on buses with associated micro production. This allows the creation of sub-groups of data according to their correlations. The clustering process is evaluated so that the number of data sub-groups that brings the most added value for the decision making process is found, according to players characteristics. © Springer International Publishing AG 2018.",,"Artificial intelligence; Commerce; Digital storage; Distributed computer systems; Electric power system economics; Electric power transmission networks; Energy utilization; Power markets; Privatization; Smart power grids; Clustering process; Decision making process; Economic solutions; Liberalized electricity market; Micro productions; New business models; Normalization methods; Virtual power players; Clustering algorithms",2-s2.0-85022227545
"Chen S.-H., Lin H.-W., Bucciarelli E., Muratore F., Odoardi I.","A data mining analysis of the Chinese inland-coastal inequality",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021737589&doi=10.1007%2f978-3-319-60882-2_12&partnerID=40&md5=3c5fe2bbd04ccb2f6c54551395f70c82","As in many countries, even in China the socio-economic changes have affected income inequality in recent decades. The various economic opportunities have led to different paths of development causing severe disparities in GDP per capita level. In addition to the well-known Chinese rural/urban inequality, in this work we study the inland/coastal differences. There are many known causes of inequality, but we aim to discover the actual determinants of the local GDP and, therefore, of income in a period that includes the international economic crisis started in 2007. With this aim, we use different variables to obtain clusters of the Chinese provinces in the period 2004–2015 and, subsequently, we investigate the determinants of income with a multivariate adaptive regression splines (MARS). There is an extensive economic literature on the Chinese case: MARS allows us to integrate this literature enabling us to find which GDP determinants are the most relevant in the certain areas of China. © Springer International Publishing AG 2018.","Chinese provinces; Inland/coastal income inequality; MARS","Artificial intelligence; Distributed computer systems; Chinese provinces; Economic opportunities; Income inequality; International economics; MARS; Multivariate adaptive regression splines; Per capita; Socio-economic change; Economics",2-s2.0-85021737589
"Radhika D., Aruna Kumari D.","Adding big value to big businesses: A present state of the art of big data, frameworks and algorithms",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410362&doi=10.1007%2f978-981-10-6602-3_17&partnerID=40&md5=b75f49c5784441f308740b814b8f8962","Data plays a pivotal role in business growth. In fact, data is considered to be an asset to organizations. This is more evident in the enterprises where the data is preserved and mined for discovering knowledge. The data with exponential growth and characterized by volume, velocity, and variety is termed as big data. Mining such voluminous data can give comprehensive business intelligence for making strategic decisions. The emergence of cloud computing technology, parallel processing power of servers, and the distributed programming frameworks like Hadoop with new programming paradigm “MapReduce” pave way for mining massive-scale data. Data mining domain is rich in algorithms that are used to mine data for discovering trends. The era of big data has arrived and mining such data is beyond the capability of conventional data mining techniques. The unprecedented exponential growth of data needs a platform for effective data analysis in real time with fast response. In this paper, we present an overview of big data, mechanisms or algorithms and environment or tools needed to execute them. The rationale behind this paper is that big data mining is the need of the hour in all sectors like finance, biology, healthcare, banking, insurance, and environmental research to name few. Review of various aspects of big data mining can help readers to gain know-how in the context of globalization, business collaborations where mining cross-organization data is essential. This paper also throws light into the relationship among big data, cloud computing technology, Hadoop, and Big data storage systems. In future, we intend to propose and implement algorithms for big data mining. © 2018, Springer Nature Singapore Pte Ltd.","Algorithms; Big data; Big data mining; Distributed programming frameworks; Hadoop","Algorithms; Cloud computing; Data mining; Data storage equipment; Digital storage; Distributed computer systems; Information analysis; Technology transfer; Business collaboration; Cloud computing technologies; Conventional data mining; Data storage systems; Distributed programming; Environmental researches; Hadoop; Programming paradigms; Big data",2-s2.0-85031410362
"Yadav P.K., Rizvi S.","Query optimization: Issues and challenges in mining of distributed data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031397567&doi=10.1007%2f978-981-10-6620-7_67&partnerID=40&md5=0c244977c587d3084caa163d92f2a46f","The technique of finding the optimal processing method to answer a query is called Query optimization, whereas a collection of various sites, distributed over a computer network is called Distributed Database. In Distributed Database, the site communicates with each other through networks. There are various issues arise during evaluation of query cost, among which the processing cost and a transmission cost are important. There are several algorithms developed to find the best possible solution for a particular query, but they all have their certain limitations. The optimizer is mainly concern on search space, search strategy, and the cost model. It primarily focuses on these three factors. The mining cost of a query depends on the order of evaluation of the operators, for the same query we can have different cost if the order is changed. Hence, to find the optimal cost for a particular query is emerging as an open challenge for many researchers. Therefore, the cost-based query optimization technique has emerged as an important concept for dealing with the query optimization. This paper explores the issues and challenges of query optimization in mining of distributed data. © 2018, Springer Nature Singapore Pte Ltd.","Cost-based optimization; Distributed database; Query optimization","Big data; Costs; Database systems; Distributed computer systems; Distributed database systems; Processing; Cost-based optimization; Distributed data; Distributed database; Issues and challenges; Optimal processing; Query optimization; Search strategies; Transmission costs; Query processing",2-s2.0-85031397567
"Wu T.-Y., Lin J.C.-W., Zhang Y.","Mining of Multiple Fuzzy Frequent Itemsets with Transaction Insertion",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030863228&doi=10.1007%2f978-3-319-68527-4_15&partnerID=40&md5=b9936984d8fc2ebb80c3e8f1ff07a379","In this paper, we thus present an algorithm to efficiently update the multiple fuzzy frequent itemsets from the quantitative dataset with transaction insertion. The designed approach is based on the Fast UPdated (FUP) concept to divide the transformed linguistic terms into four cases, and each case is performed by the designed approach for updating the discovered information. Also, the fuzzy-list (FL) structure is adopted to reduce the generation of candidates without multiple database scans. Experiments are conducted to show that the proposed algorithm outperforms the state-of-the-art approach. © 2018, Springer International Publishing AG.","Dynamic database; FL-strcutrue; Fuzzy data mining; Incremetal; Insertion","Data mining; Information analysis; Linguistics; Dynamic database; FL-strcutrue; Fuzzy-data mining; Incremetal; Insertion; Data handling",2-s2.0-85030863228
"Chen P., Zhu L.","Research on personalized recommendation of electronic commerce website based on text mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028419525&doi=10.1007%2f978-3-319-60744-3_38&partnerID=40&md5=91749da3d09378969f2187e7cc6ff9ca","With the growing popularity of electronic commerce, for enterprises to enhance the competitiveness, enterprise decision makers and active innovation, to build a distinctive, individualized and user characteristics of e-commerce network platform, e-commerce personalized recommendation was thus born. Enterprises make use of the data mining technology, search, record, analysis server, browser the user’s browsing history and purchase records, selected users a sense of interest from the market information, provide differentiated network marketing strategy. Presented in this paper based on text mining e-commerce website personalized recommendation technology is the A, this paper through the comparison of text mining and Web Mining Technology Analysis of the text mining technology of personalized recommendation technology of precision, which information data of the study of mining technology has great reference value. © 2018, Springer International Publishing AG.","Electronic commerce; Personalized service; Text mining; User modeling","Commerce; Competition; Decision making; Electronic commerce; Intelligent systems; Marketing; Real time systems; Websites; Data mining technology; E-commerce websites; Personalized recommendation; Personalized service; Text mining; User characteristics; User Modeling; Web mining technology; Data mining",2-s2.0-85028419525
"Gregoriades A., Christodoulides A., Michael H.","Mining traffic data for the development of an accident warning application for tourists",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022205699&doi=10.1007%2f978-3-319-60525-8_60&partnerID=40&md5=80ed790235008c415f6e6e712416f2f2","Tourist drivers belong to a category of drivers that are more vulnerable to road accidents due to their unfamiliarity of the road network at a destination. This paper presents a method followed to develop a tool that alert tourist drivers of their accident risks based on situational factors obtained from mobile phone sensors and knowledge distilled from historical records of traffic accidents. The knowledge necessary for the development of a context aware mobile accident warning application was extracted from a spatiotemporal analysis of historical accidents data, to identify patterns of conditions that lead to accidents. Results from this analysis were used to develop heuristics rules that were programmed in a mobile application. The developed system warns travelers of possible threats on the road network of Nicosia, given driver’s location and situational factors. The system aims to improve tourists’ safety. © Springer International Publishing AG 2018.","Accident prediction; Association rules; Self-organizing maps; Tourist safety","Association rules; Conformal mapping; Highway accidents; Highway administration; Human engineering; Motor transportation; Roads and streets; Self organizing maps; Transportation; Accident prediction; Accident risks; Accident warning; Historical records; Mobile applications; Mobile phone sensors; Situational factors; Spatiotemporal analysis; Accidents",2-s2.0-85022205699
"Cao X.-L., Xi X.-Q.","A Method to Evaluate the Research Direction of University",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030848321&doi=10.1007%2f978-3-319-68527-4_17&partnerID=40&md5=5a00511b062cc4aa54f9ed57dc97bd2e","The era is changing from information technology to data technology, big data is used very well in the field of financial, medical, e-commerce and so on, but not very well in the field of education. The idea of “Data - driven schools, analysis of change education” make the need for the educational data mining more and more prominent. Data mining in education can help us to connect the relevant areas of education and find the key educational variables, which can make the education and teaching decision simple and accurate. In this paper, by using the Chinese word segmentation algorithm, association rule and RStudio tool, we analyse the title of master’s thesis in four universities that have same discipline structure. The title data is obtained from http://www.cnki.net, which is an authority database in China. The results show that the research directions of the four university tend to be wireless network, mobile communication and algorithms. © 2018, Springer International Publishing AG.","Association rules; Chinese word segmentation algorithm; Education data mining; RStudio","Association rules; Computational linguistics; Data handling; Data mining; Education; Engineering education; Information analysis; Teaching; Chinese word segmentation; Data driven; Data technologies; Educational data mining; Mobile communications; RStudio; Big data",2-s2.0-85030848321
"Madala S.R., Rajavarman V.N., Venkata Satya Vivek T.","Analysis of different pattern evaluation procedures for big data visualization in data analysis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021211593&doi=10.1007%2f978-981-10-3223-3_44&partnerID=40&md5=b61ed1bb338b9d3576e124e9a193e4c0","Data visualization is the main focusing concept in big data analysis for processing and analyzing multi variate data, because of rapid growth of data size and complexity of data. Basically data visualization may achieve three main problems, i.e. 1. Structured and Unstructured pattern evaluation in big data analysis. 2. Shrink the attributes in data indexed big data analysis. 3. Rearrange of attributes in parallel index based data storage. So in this paper we analyze different techniques for solving above three problems with feasibility of each client requirement in big data analysis for visualization in real time data stream extraction based on indexed data arrangement. We have analyzed different prototypes in available parallel co-ordinate and also evaluate quantitative exert review in real time configurations for processing data visualization. Report different data visualization analysis results for large and scientific data created by numerical simulation in practice sessions analysed in big data presentation. © Springer Nature Singapore Pte Ltd. 2018.","Big data analysis; Data visualization; Parallel co-ordinate analysis; Pattern evaluation","Data handling; Data mining; Data visualization; Digital storage; Information analysis; Intelligent computing; Visualization; Client requirement; Data presentation; Data storage; Parallel co-ordinate analysis; Pattern evaluation; Real-time data streams; Scientific data; Visualization analysis; Big data",2-s2.0-85021211593
"Saravanan D.","Image frame mining using indexing technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021185279&doi=10.1007%2f978-981-10-3223-3_12&partnerID=40&md5=9f04defc3593beedfd8a1c0401d81b45","Data mining is a technique the bring out hidden information effectively from an available data set. Most of this extraction works well when performed for binary and character information. Mining information form images is a challenge today for many researchers. Creating of images and videos is easy as it does not require any domain knowledge, but extracting the required knowledge is difficult. For this reason, today video data mining is an interesting area for many researchers. To overcome these problems many researchers are motivated for finding an effective retrieval and indexing technique. This research paper brings a new technique for video content retrieval using hierarchical clustering technique. Objective of this work is to extract image key frames from the trained image set and use this as an image input query. The experiment proved that the proposed technique provided better results than existing video retrieval and indexing technique. © Springer Nature Singapore Pte Ltd. 2018.","Clustering; Data mining; Hierarchical clustering; Histogram; Image mining; Key frame selection; Video data mining","Image processing; Indexing (of information); Intelligent computing; Video recording; Clustering; Hier-archical clustering; Histogram; Image mining; Key frame selection; Video data mining; Data mining",2-s2.0-85021185279
"Leung C.K.","Data and Visual Analytics for Emerging Databases",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032507899&doi=10.1007%2f978-981-10-6520-0_21&partnerID=40&md5=eb1f94a4c7c585673b67ff4a52029f35","With advances in technology, high volumes of valuable data of different veracity can be generated at a high velocity in wide varieties of data sources in various real-life applications. Examples of these big data include social media data. As a popular data mining tasks, frequent pattern mining discovers implicit, previously unknown and potentially useful knowledge in the form of sets of frequently co-occurring items or events. Many existing data mining algorithms return to users with long textual lists of frequent patterns, which may not be easily comprehensible. Given a picture is worth a thousand words, having a visual means for humans to interact with computers would be beneficial. In this paper, we present a framework for data and visual analytics for emerging databases. In particular, our data and visual analytic framework focuses on mining and analyzing social media data, as well as visualizing the mined ‘following’ patterns that reveal those groups of frequently followed social entities in a social network. © 2018, Springer Nature Singapore Pte Ltd.","Data analytics; Data mining; Emerging databases; Frequent patterns; Knowledge discovery in databases; Social media data; Social network; Visual analytics; Visualization; ‘following’ patterns","Data mining; Data visualization; Database systems; Flow visualization; Social networking (online); Visualization; Data analytics; Frequent patterns; Knowledge discovery in database; Social media datum; Visual analytics; Big data",2-s2.0-85032507899
"Tyagi N., Gupta S.K.","Web structure mining algorithms: A survey",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031415327&doi=10.1007%2f978-981-10-6620-7_30&partnerID=40&md5=90c0106983c548fa7c46806bf0909001","World Wide Web (WWW) is a massive collection of information and due to its rapid growing size, information retrieval becomes more challenging task to the user. Web mining techniques such as web content mining, web usage mining, and web structure mining are used to make the information retrieval more efficient. In this paper, study is focused on the web structure mining and different link analysis algorithms. Further, a comparative review of these algorithms is given. © 2018, Springer Nature Singapore Pte Ltd.","Information retrieval; Link analysis; Web content mining; Web mining; Web structure mining; Web usage mining","Big data; Information retrieval; Websites; Link analysis; Web content mining; Web Mining; Web structure mining; Web usage mining; Data mining",2-s2.0-85031415327
"Dinkić N., Džaković N., Joković J., Stoimenov L., Đukić A.","Using sentiment analysis of Twitter data for determining popularity of city locations",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031736676&doi=10.1007%2f978-3-319-68855-8_15&partnerID=40&md5=7eba897c329101f0fcc5e9b6737e8be3","The paper considers mining and analyzing data generated by Twitter social network, regarding content classification, language determination and sentiment analysis of tweets. Analyzes are based on geospatial tweets collected in timespan of four months within region Vračar in Belgrade, Serbia. All of collected data is first being preprocessed, filtered and classified by given criteria, by using “Twitter search engine” (TSE) application, that has been upgraded in order to detect tweet language and execute sentiment analysis of the tweets written in English. This type of analysis can be used for determining popularity of city locations of interest and public spaces in general. © Springer International Publishing AG 2018.","Geospatial data; Natural language processing; Sentiment analysis and opinion mining; Twitter social network","Cognitive systems; Data mining; Natural language processing systems; Social networking (online); Belgrade , Serbia; Content classification; Geo-spatial; Geo-spatial data; Opinion mining; Public space; Sentiment analysis; Twitter social networks; Search engines",2-s2.0-85031736676
"Claypo N., Hanskunatai A., Jaiyen S.","A new streaming learning for stream chunk data classification based on incremental learning and adaptive boosting algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022179182&doi=10.1007%2f978-3-319-60663-7_15&partnerID=40&md5=5287ad2927f6f65a2bb1c1684b64b284","Currently, stream data classification is a challenge task to discover new useful knowledge from massive and dynamic data in big data era. This paper proposes a streaming learning method based on the incremental learning using a new adaptive boosting algorithm for stream data. The proposed adaptive boosting consists of a new method for updating distribution weight and the new weight voting. This learning method concentrates on learning from sequential chunks of data stream. The distribution weight updating method uses error of previous hypothesis to update the weight. The learning method uses only one data chunk to create a new hypothesis at a time and after learning, the learned data chunk can be thrown away and can learn the new data chunk without using the previous learned data. The experimental results show that the accuracy of the proposed method is higher than other methods in all datasets. © Springer International Publishing AG 2018.","Adaptive boosting; Classification; Incremental learning; Stream data","Adaptive boosting; Big data; Classification (of information); Data mining; Learning algorithms; Learning systems; Adaptive boosting algorithms; Data classification; Dynamic data; Incremental learning; Learning methods; Stream data; Stream data classifications; Updating methods; Education",2-s2.0-85022179182
"Duong H., Truong T., Le B.","Efficient algorithms for simultaneously mining concise representations of sequential patterns based on extended pruning conditions",2018,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032285989&doi=10.1016%2fj.engappai.2017.09.024&partnerID=40&md5=a5f129669d1c36148a13db457081c6d2","The concise representations of sequential patterns, including maximal sequential patterns, closed sequential patterns and sequential generator patterns, play an important role in data mining since they provide several benefits when compared to sequential patterns. One of the most important benefits is that their cardinalities are generally much less than the cardinality of the set of sequential patterns. Therefore, they can be mined more efficiently, use less storage space, and it is easier for users to analyze the information provided by the concise representations. In addition, the set of all maximal sequential patterns can be utilized to recover the complete set of sequential patterns, while closed sequential patterns and sequential generators can be used together to generate non-redundant sequential rules and to quickly recover all sequential patterns and their frequencies. Several algorithms have been proposed to mine the concise representations separately, i.e., each of them has been designed to discover only a type of the concise representation. However, they remain time-consuming and memory intensive tasks. To address this problem, we propose three novel efficient algorithms named FMaxSM, FGenCloSM and MaxGenCloSM to exploit only maximal sequential patterns, to simultaneously mine both the sets of closed sequential patterns and generators, and to discover all three concise representations during the same process. To our knowledge, MaxGenCloSM is the first algorithm for concurrently mining the three concise representations of sequential patterns. The proposed algorithms are based on two novel local pruning strategies called LPMAX and LPMaxGenClo that are designed to prune non-maximal, non-closed and non-generator patterns earlier and more efficiently at two and three successive levels of the prefix tree without subsequence relation checking. Extensive experiments on real-life and synthetic databases show that FMaxSM, FGenCloSM and MaxGenCloSM are up to two orders of magnitude faster than the state-of-the-art algorithms and that the proposed algorithms consume much less memory, especially for low minimum support thresholds and for dense databases. © 2017 Elsevier Ltd","Frequent closed sequence; Frequent generator sequences; Frequent sequence; Maximal frequent sequence; Sequential pattern mining; Vertical data format","Digital storage; Closed sequences; Frequent generator sequences; Frequent sequences; Maximal frequent sequences; Sequential-pattern mining; Vertical data formats; Data mining",2-s2.0-85032285989
"Sethi K.K., Dharavath R., Nyakotey S.","PPS: Parallel pincer search for mining frequent itemsets based on spark",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028590336&doi=10.1007%2f978-3-319-60618-7_35&partnerID=40&md5=0529ac535409e366921e642f1f300398","Association rule mining is one of prominent techniques to discover the relation between data items of a transactional data. The process of mining has been simplified by considering only the frequent itemsets. Pincer search is one of the frequent itemset mining method which combines top-down and bottom-up search techniques to get the benefits of both. Top-down approach in Pincer search reduces the number of candidates in pass of iterations and saves a lot of computing resources. In this work, we present a Parallel Pincer Search (PPS) which is based on distributed implementation on Spark framework. We have converted the search algorithm according to the Spark framework to make it run in parallel. Spark provides a lot of features for the iterative algorithm such as in-memory execution, efficient data structure, better fault tolerant method, etc. We implemented the PPS on a Spark cluster with multiple datasets and analysed the performance. © Springer International Publishing AG 2018.","Apriori algorithm; Frequent itemset mining; Maximal itemset; Pincer search; Spark","Clustering algorithms; Data mining; Electric sparks; Learning algorithms; Mining; Pattern recognition; Soft computing; Apriori algorithms; Distributed implementation; Efficient data structures; Fault-tolerant method; Frequent itemset mining; Itemset; Mining frequent itemsets; Pincer search; Iterative methods",2-s2.0-85028590336
"Reder M., Yürüşen N.Y., Melero J.J.","Data-driven learning framework for associating weather conditions and wind turbine failures",2018,"Reliability Engineering and System Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031740982&doi=10.1016%2fj.ress.2017.10.004&partnerID=40&md5=28afa616dbad1bc11aaf60b80633e882","The need for cost effective operation and maintenance (O&M) strategies in wind farms has risen significantly with the growing wind energy sector. In order to decrease costs, current practice in wind farm O&M is switching from corrective and preventive strategies to rather predictive ones. Anticipating wind turbine (WT) failures requires sophisticated models to understand the complex WT component degradation processes and to facilitate maintenance decision making. Environmental conditions and their impact on WT reliability play a significant role in these processes and need to be investigated profoundly. This paper is presenting a framework to assess and correlate weather conditions and their effects on WT component failures. Two approaches, using (a) supervised and (b) unsupervised data mining techniques are applied to pre-process the weather and failure data. An apriori rule mining algorithm is employed subsequently, in order to obtain logical interconnections between the failure occurrences and the environmental data, for both approaches. The framework is tested using a large historical failure database of modern wind turbines. The results show the relation between environmental parameters such as relative humidity, ambient temperature, wind speed and the failures of five major WT components: gearbox, generator, frequency converter, pitch and yaw system. Additionally, the performance of each technique, associating weather conditions and WT component failures, is assessed. © 2017 Elsevier Ltd","Association rule mining; Big data; Data mining; Failure; k-means clustering; Machine learning; Operation & maintenance; Weather; Wind turbine","Big data; Computer system recovery; Cost effectiveness; Decision making; Electric utilities; Learning systems; Maintenance; Meteorology; Weathering; Wind; Wind power; Wind turbines; Environmental conditions; Environmental parameter; K-means clustering; Logical interconnections; Maintenance decision making; Operation and maintenance; Preventive strategies; Rule mining algorithms; Data mining",2-s2.0-85031740982
"Vasavi S.","Extracting hidden patterns within road accident data using machine learning techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760909&doi=10.1007%2f978-981-10-5508-9_2&partnerID=40&md5=b40024edc7a4f44caa4c78266a591951","Road accidents may not be stopped altogether, but can be reduced. Driver emotions such as sad, happy, and anger can be one reason for accidents. At the same time, environment conditions such as weather, traffic on the road, load in the vehicle, type of road, health condition of driver, and speed can also be the reasons for accidents. Hidden patterns in accidents can be extracted so as to find the common features between accidents. This paper presents the results of the framework from the research study on road accident data of major national highways that pass through Krishna district for the year 2013 by applying machine learning techniques into analysis. These datasets collected from police stations are heterogeneous. Incomplete and erroneous values are corrected using data cleaning measures, and relevance attributes are identified using attribute selection measures. Clusters that are formed using K-medoids, and expectation maximization algorithms are then analyzed to discover hidden patterns using a priori algorithm. Results showed that the selected machine learning techniques are able to extract hidden patterns from the data. Density histograms are used for accident data visualization. © Springer Nature Singapore Pte Ltd. 2018.","Association rule mining; Clustering; Machine learning techniques; Preprocessing; Road accident data analysis; Visualization","Artificial intelligence; Data mining; Data visualization; Flow visualization; Highway accidents; Image segmentation; Learning algorithms; Learning systems; Maximum principle; Motor transportation; Roads and streets; Transportation; Visualization; Attribute selection; Clustering; Environment conditions; Expectation-maximization algorithms; Machine learning techniques; Preprocessing; Research studies; Road accident data; Accidents",2-s2.0-85031760909
"Anjana K., Radhika K., Darshana P.","Imbalanced data stream classification: Analysis and solution",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028400475&doi=10.1007%2f978-3-319-63645-0_35&partnerID=40&md5=47334b1f0688a8f77ae0c1486daa26f2","Through the progress in each hardware and software system technologies, automatic data creation and storage have become quicker than ever. Such data is called as a data stream. Streaming information is present everywhere and it’s usually a difficult problem to visualize, collect and examine such huge volumes of information. Data stream mining has become a unique experimental area in information finding because of the large size and rapid speed of data in the data stream, due to this reason conventional classification methods are not effective. In today`s a substantial amount of analysis has been done on this issue whose main aim is to efficiently solve the difficulty of information stream mining with concept drift. Class imbalance is one of the problems of machine learning and data processing fields. Imbalance data sets reduce the performance as well as the overall accuracy of data mining methods. Decision making towards the majority class, which lead to misclassifying the minority class examples or moreover considered them as noise. © Springer International Publishing AG 2018.","Data stream; Ensemble method; Hoeffding tree; K-nearest neighbor; MSMOTE; SMOTE","Classification (of information); Data communication systems; Data handling; Decision making; Digital storage; Intelligent systems; Learning systems; Nearest neighbor search; Trees (mathematics); Data stream; Ensemble methods; Hoeffding tree; K-nearest neighbors; MSMOTE; SMOTE; Data mining",2-s2.0-85028400475
"Tusor B., Várkonyi-Kóczy A.R., Tóth J.T.","A fuzzy data structure for variable length data and missing value classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802680&doi=10.1007%2f978-3-319-67459-9_37&partnerID=40&md5=23f35515f80573da2824ed2c52be4759","Variable length data classification is an important field of machine learning. However, while there are plenty of classifiers in literature that can efficiently handle fixed length data, not many can also handle data with varying length samples. In this paper, a structure is proposed for quick and robust classification of such data, as well as data sets with occasionally missing values. It builds on the principle of look-up table classifiers, realizing a direct assignment between the attribute values of the given data samples and their corresponding classes. The proposed data structure solves this problem by decomposing the problem space into a sequence of integer value combinations, thus creating and maintaining a layered structure in the combined form of 1D and 2D arrays. Furthermore, a simple analysis regarding the data structure can reveal functional dependencies considering the attributes of the data set, offering an option to simplify the structure thus reduce its complexity. © Springer International Publishing AG 2018.","Classification; Data mining; Data structure; Machine learning; Missing data; Pattern recognition","Artificial intelligence; Autocorrelation; Data mining; Data structures; Education; Learning systems; Pattern recognition; Table lookup; Attribute values; Data classification; Functional dependency; Layered Structures; Missing data; Robust classification; Simple analysis; Variable length; Classification (of information)",2-s2.0-85029802680
"Parra-Arnau J.","Optimized, direct sale of privacy in personal data marketplaces",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032346071&doi=10.1016%2fj.ins.2017.10.009&partnerID=40&md5=85539574011aa9e2e9c545a5a4a92d6a","Very recently, we are witnessing the emergence of a number of start-ups that enables individuals to sell their private data directly to brokers and businesses. While this new paradigm may shift the balance of power between individuals and companies that harvest and mine data, it raises some practical, fundamental questions for users of these services: how they should decide which data must be vended and which data protected, and what a good deal is. In this work, we investigate a mechanism that aims at helping users address these questions. The investigated mechanism relies on a hard-privacy model and allows users to share partial or complete profile data with broker and data-mining companies in exchange for an economic reward. The theoretical analysis of the trade-off between privacy and money posed by such mechanism is the object of this work. We adopt a generic measure of privacy although part of our analysis focuses on some important examples of Bregman divergences. We find a parametric solution to the problem of optimal exchange of privacy for money, and obtain a closed-form expression and characterize the trade-off between profile-disclosure risk and economic reward for several interesting cases. Finally, we evaluate experimentally how our approach could contribute to privacy protection in a real-world data-brokerage scenario. © 2017 Elsevier Inc.","data brokers; disclosure risk; disclosure-money trade-off; User privacy","Data mining; Economic and social effects; Human computer interaction; Bregman divergences; Closed-form expression; Data broker; Disclosure risk; Parametric solutions; Privacy protection; Trade off; User privacy; Data privacy",2-s2.0-85032346071
"Rasel M.K., Lee Y.-K.","An Efficient Subgraph Compression-Based Technique for Reducing the I/O Cost of Join-Based Graph Mining Algorithms",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032469875&doi=10.1007%2f978-981-10-6520-0_9&partnerID=40&md5=5b579ed8f235c40220c5e844ecdd4da6","Many join-based graph mining algorithms such as triangle listing and clique enumeration output a large size of intermediate or final data that sometimes dominates the mining cost. A few researches highlighted on the size of output data. However, those techniques have limitation that they are highly specific to their corresponding graph mining algorithms. In this paper, through the careful observations of the output patterns, we propose a general compression solution that can be applied to any join-based graph algorithm. It first categorizes the overlapping and non-overlapping vertices in a resultant subgraph set of a join-based graph mining algorithm. Then it compresses the output data by removing the redundancy from the overlapping vertices and by encoding the non-overlapping vertices using a non-aligned hybrid bit vector compression technique. Our proposed technique performs the compression on-the-fly and can easily be adopted by the join-based graph mining algorithms. Experiments on the real datasets show that our proposed technique, which is adopted in a triangle listing algorithm, reduces the size of the output data and the running time by three times and more than two times, respectively. The proposed technique also reduces the I/O cost for a maximal clique listing algorithm. © 2018, Springer Nature Singapore Pte Ltd.","Bitmap compression; Graph I/O compression; Graph mining algorithms","Cost reduction; Costs; Compression solutions; Compression techniques; Graph algorithms; Graph mining; Listing algorithms; Maximal clique; Real data sets; Running time; Data mining",2-s2.0-85032469875
"Zhou Y.","Public security big data processing support technology",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032685533&doi=10.1007%2f978-3-319-67071-3_22&partnerID=40&md5=5c856125d88f7ea98f294d6d576288e7","Fourth times a million police officers held in October 21, 2016 at the Central Political Committee learning seminars, the Political Bureau of the CPC Central Committee and the central politics and Law Committee Secretary Meng Jianzhu pointed out, we are in the era of big data, modern science and technology in the mobile Internet, big data, cloud computing and artificial intelligence as the representative is changing our mode of life, everything experience of human life are changing. Big data development of the human “third eyes”, through massive data analysis, processing, mining, allows us to penetrate into the unknown world. To cultivate data culture, good at using big data thinking analysis, problem solving, decision support. © 2018, Springer International Publishing AG.","Big data; Public security","Data handling; Data mining; Decision support systems; Law enforcement; Problem solving; Data development; Decision supports; Human lives; Mobile Internet; Modern science; Police officers; Public security; Support technology; Big data",2-s2.0-85032685533
"Li J., Fong S., Wong R.K., Chu V.W.","Adaptive multi-objective swarm fusion for imbalanced data classification",2018,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016392535&doi=10.1016%2fj.inffus.2017.03.007&partnerID=40&md5=6b582a17fdabebe7881a6f2f7424cbf8","Learning a classifier from an imbalanced dataset is an important problem in data mining and machine learning. Since there is more information from the majority classes than the minorities in an imbalanced dataset, the classifier would become over-fitted to the former and under-fitted to the latter classes. Previous attempts to address the problem have been focusing on increasing the learning sensitivity to the minorities and/or rebalancing sample sizes among classes before learning. However, how to efficiently identify their optimal mix in rebalancing is still an unresolved problem. Due to non-linear relationships between attributes and class labels, merely to rebalance sample sizes rarely comes up with optimal results. Moreover, brute-force search for the perfect combination is known to be NP-hard and hence a smarter heuristic is required. In this paper, we propose a notion of swarm fusion to address the problem – using stochastic swarm heuristics to cooperatively optimize the mixtures. Comparing with conventional rebalancing methods, e.g., linear search, our novel fusion approach is able to find a close to optimal mix with improved accuracy and reliability. Most importantly, it has found to be with higher computational speed than other coupled swarm optimization techniques and iteration methods. In our experiments, we first compared our proposed solution with traditional methods on thirty publicly available imbalanced datasets. Using neural network as base learner, our proposed method is found to outperform other traditional methods by up to 69% in terms of the credibility of the learned classifiers. Secondly, we wrapped our proposed swarm fusion method with decision tree. Notably, it defeated six state-of-the-art methods on ten imbalanced datasets in all evolution metrics that we considered. © 2017 Elsevier B.V.","Crossover rebalancing; Imbalanced data classification; Multi-objective; Swarm fusion; Swarm intelligence algorithm","Artificial intelligence; Data mining; Decision trees; Iterative methods; Learning systems; Optimization; Stochastic systems; Swarm intelligence; Computational speed; Imbalanced data; Imbalanced Data-sets; Multi objective; Non-linear relationships; Rebalancing; State-of-the-art methods; Swarm intelligence algorithms; Classification (of information)",2-s2.0-85016392535
"Yu T., Wang S., Yu X.","A preamble mining algorithm oriented to binary protocol using random probes",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026636405&doi=10.1007%2f978-3-319-63859-1_39&partnerID=40&md5=c6bdbc7c684bffff6786c23c5632eea7","At present, most of the researches on the protocol reverse are on the basis of segmented frames and lack of effective methods to analyze the raw data stream. Several existing frame segmentation algorithms based on AC have the problem of large space overhead and low time efficiency. In this paper, we study on frames segmentation algorithms based on preamble mining and propose a preamble mining algorithm based on random probes oriented to binary protocol. We extract the correct preamble by randomly inserting some probes into the data stream, from which to find continuous short mode strings, after which extracting the most frequently repeated strings as the candidate units, and then filtering them with the help of structural characteristics of the preamble. Experiment shows that the algorithm has higher time efficiency compared with the preamble mining algorithm based on AC algorithm. © Springer International Publishing AG 2018.","Frames segmentation; Preamble mining; Protocol reverse; Random probes","Bins; Data communication systems; Data mining; Efficiency; Probes; Signal processing; Binary protocols; Large spaces; Mining algorithms; Random probes; Repeated strings; Segmentation algorithms; Structural characteristics; Time efficiencies; Multimedia signal processing",2-s2.0-85026636405
"Dittert M., Härting R.-C., Reichstein C., Bayer C.","A data analytics framework for business in small and medium-sized organizations",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020425798&doi=10.1007%2f978-3-319-59424-8_16&partnerID=40&md5=0f6104614cc74274c0e70a362cd5b319","Data Analytics and derived Data Mining are powerful approaches for the analysis of Big Data. There are a lot of commercial Data Analytics applications enterprises can take advantage of. In the past, many firms were still critical of Data Analytics. Through efforts made in the field of the establishment of process standards, managers might be convinced of Data Analytics advantages. Many small and medium-sized organizations are still exempt from this development. The main reasons are a lack of business prioritization, a lack of (IT) knowledge, and a lack of overview of Data Analytics issues. To reduce that problem, we developed a useful process framework. It resembles with existing frameworks, but is highly simplified and easy to use. To exemplify, how this framework can be put into action by the means of a retail site location analysis, we set up a case study as best practice. There we are focusing on Data Mining because it is the most important domain of Data Analytics. © Springer International Publishing AG 2018.","Data analytics; Data mining; Location analysis; Process framework; SME","Data mining; Location; Network function virtualization; Best practices; Business prioritization; Data analytics; Location analysis; Medium sized organizations; Process framework; Process standards; Site location; Big data",2-s2.0-85020425798
"Tyagi K., Thakur S.","Predictive classification of ECG parameters using association rule mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031396430&doi=10.1007%2f978-981-10-3773-3_60&partnerID=40&md5=4a8ebbd1bc0be14a09be0336a538c256","Data mining is the procedure of extricating valuable information from the tremendous information stored in the database. Association rule mining is one of the most important and powerful data mining techniques. Association rule mining is normally carried out in two stages: first is to find frequent item set and second is to utilize those item sets to recognize the association rules. In recent medical history of cardiac arrests it has been observed that a huge gap exists in interpreting ECG data among differently skilled doctors. In this paper we will use the principle of meta-analysis and will reduce the gap between the interpretations of different doctors by employing statistical techniques like correlation and multiple linear regression. We would also generate rules using predictive apriori association rule mining among the various attributes of ECG to classify whether a patient requires an ECG before a cardiac arrest or not. The purpose of carrying out this work is to reduce the fatality rate and be able to predict that whether a patient requires ECG before actually facing a cardiac arrest and to minimize the cases of wrong interpretation. © Springer Nature Singapore Pte Ltd. 2018.","Association rule mining; Data mining; ECG data; Predictive apriori","Association rules; Electrocardiography; Linear regression; Apriori; Cardiac arrest; ECG data; ECG parameters; Fatality rates; Medical history; Multiple linear regressions; Statistical techniques; Data mining",2-s2.0-85031396430
"Jariyavajee C., Sirinaovakul B., Polvichai J.","Bodily posture recognition with weighted dimension on kinect data stream",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022212268&doi=10.1007%2f978-3-319-60663-7_14&partnerID=40&md5=80d55b230f875d8657d6d9bd1495495c","The characteristic of the data stream is continuous, non-stationary, and very large or infinite size. Data stream classification requires the algorithm that able to classify data instance and learn from data incrementally. In this paper, the algorithm with Weighted Dimension is proposed and applied for the Kinect bodily posture recognition. The human body portions, as the input features, are calculated from Skeleton Joint data. The proposed algorithm successes in recognizing three human postures: stand, sit_on_chair, and sit_on_floor. The result of classification is 99.02% on average and 100% on moving accuracy. Moreover, the algorithm always learns from the data instances and some labels so the algorithm is able to learn whether the data instances are changed. In the other words, the algorithm could handle the concept drift in the data stream. © Springer International Publishing AG 2018.","Concept drift; Data stream classification; Kinect; Posture recognition","Data communication systems; Data mining; Concept drifts; Data stream classifications; Human postures; Input features; Kinect; Nonstationary; Posture recognition; Skeleton joints; Classification (of information)",2-s2.0-85022212268
"Chauhan R., Jangade R., Mudunuru V.K.","A cloud based environment for big data analytics in healthcare",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028608804&doi=10.1007%2f978-3-319-60618-7_31&partnerID=40&md5=df2cfe08b865b2a7429aa0c6ce2e5ceb","A large amount of modern healthcare data is generated through imaging, Electronic Health Report (EHR), sensor based technology and other various healthcare processes. An elaborative perspective in technological advancement has enabled practitioners to answer questions for governance and future decision making. However, very few tools exist to critically analyze such big data for future knowledge discovery. We can further say that cloud computing technology can be a benchmark to substantiate big data which may lead to discover of hidden patterns and trends to enhance knowledge for progression of disease. This paper approached various aspects of cloud based services to enable big data analytic in healthcare data management system. © Springer International Publishing AG 2018.","Analytics as service; Big data; Cloud computing; Healthcare","Cloud computing; Data mining; Decision making; Health care; Information management; Pattern recognition; Soft computing; Analytics as service; Cloud computing technologies; Data analytics; Data management system; Electronic health; Healthcare process; Hidden patterns; Technological advancement; Big data",2-s2.0-85028608804
"Bryant C., Schuh S., Schoenstein N., Meza D.","Understanding the international space station crew perspective following long duration missions through data analytics and visualization of crew feedback",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022327713&doi=10.1007%2f978-3-319-60492-3_7&partnerID=40&md5=66b1152eb18176eec5771da0af89488e","This paper will discuss how the use of an analytical framework in conjunction with the current human interface, improved the understanding of the International Space Station (ISS) crew perspective data and shortened analysis time, allowing for more informed decisions and rapid development of improvements. These data analytics and visualization methods significantly optimize valuable ISS postflight crew debrief qualitative data, yielding results that can be applied to both current and future spaceflight design and development, and other domains. This paper will discuss a collaboration that has allowed a team of Human Factors engineers at NASA’s Johnson Space Center (JSC) to analyze and share data in a more automated and accurate fashion, thanks to the efforts of the JSC Chief Knowledge Office (CKO). Trends are no longer manually derived and are visualized effectively to assist in presenting these evolving techniques and subsequent results to an ever-growing population of human spaceflight end users. © Springer International Publishing AG 2018.","Data Science; Data Visualization; Habitability; Human Factors; Human-Systems Integration; Information Extraction & Clustering; Knowledge Architecture; Knowledge Informatics; Knowledge Management; Qualitative Data Analysis; Sentiment Analysis; Text Analytics; Trend Analysis","Data visualization; Human computer interaction; Human engineering; Information analysis; Information management; Knowledge management; Manned space flight; NASA; Space platforms; Space stations; Visualization; Data Science; Habitability; Human systems integration; Informatics; Knowledge architecture; Qualitative data analysis; Sentiment analysis; Text analytics; Trend analysis; Data mining",2-s2.0-85022327713
"Siahroudi S.K., Moodi P.Z., Beigy H.","Detection of evolving concepts in non-stationary data streams: A multiple kernel learning approach",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028812415&doi=10.1016%2fj.eswa.2017.08.033&partnerID=40&md5=f8abdea1fbf972bcbae4ae15dbb74056","Due to the unprecedented speed and volume of generated raw data in most of applications, data stream mining has attracted a lot of attention recently. Methods for solving these problems should address challenges in this area such as infinite length, concept-drift, recurring concepts, and concept-evolution. Moreover, due to the speedy intrinsic of data streams, the time and space complexity of the methods are extremely important. This paper proposes a novel method based on multiple-kernels for classifying non-stationary data streams, which addresses the mentioned challenges with special attention to the space complexity. By learning multiple kernels and specifying the boundaries of classes in the feature (mapped) space of combined kernels, the required amount of memory will be decreased. These kernels will be updated regularly throughout the stream when the true labels of instances are received. Newly arrived instances will be classified with respect to their distance to boundaries of the previously known classes in the feature spaces. Due to the efficient memory usage, the computation time does not increase significantly through the stream. We evaluate the performance of the proposed method using a set of experiments conducted on both real and synthetic benchmark data sets. The experimental results show the superiority of the proposed method over the state-of-the-art methods in this area. © 2017 Elsevier Ltd","Concept drift; Data stream; Data stream classification; Novel class detection","Benchmarking; Data communication systems; Concept drifts; Data stream; Data stream classifications; Data stream mining; Multiple Kernel Learning; State-of-the-art methods; Synthetic benchmark; Time and space complexity; Classification (of information)",2-s2.0-85028812415
"Wei Y., Zhang X., Shi Y., Xia L., Pan S., Wu J., Han M., Zhao X.","A review of data-driven approaches for prediction and classification of building energy consumption",2018,"Renewable and Sustainable Energy Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030703701&doi=10.1016%2fj.rser.2017.09.108&partnerID=40&md5=d744ae502a07a34823268f17785ceccd","A recent surge of interest in building energy consumption has generated a tremendous amount of energy data, which boosts the data-driven algorithms for broad application throughout the building industry. This article reviews the prevailing data-driven approaches used in building energy analysis under different archetypes and granularities, including those methods for prediction (artificial neural networks, support vector machines, statistical regression, decision tree and genetic algorithm) and those methods for classification (K-mean clustering, self-organizing map and hierarchy clustering). The review results demonstrate that the data-driven approaches have well addressed a large variety of building energy related applications, such as load forecasting and prediction, energy pattern profiling, regional energy-consumption mapping, benchmarking for building stocks, global retrofit strategies and guideline making etc. Significantly, this review refines a few key tasks for modification of the data-driven approaches in the context of application to building energy analysis. The conclusions drawn in this review could facilitate future micro-scale changes of energy use for a particular building through the appropriate retrofit and the inclusion of renewable energy technologies. It also paves an avenue to explore potential in macro-scale energy-reduction with consideration of customer demands. All these will be useful to establish a better long-term strategy for urban sustainability. © 2017 Elsevier Ltd","Building; Classification; Data driven approach; Energy consumption; Prediction","Benchmarking; Buildings; Classification (of information); Cluster analysis; Clustering algorithms; Conformal mapping; Construction industry; Data mining; Decision trees; Energy management; Energy utilization; Forecasting; Genetic algorithms; Neural networks; Renewable energy resources; Retrofitting; Self organizing maps; Speech recognition; Trees (mathematics); Building energy analysis; Building energy consumption; Data-driven algorithm; Data-driven approach; Hierarchy clustering; Regional energy consumption; Renewable energy technologies; Statistical regression; Energy conservation",2-s2.0-85030703701
"Mehran N., Movahhedinia N.","Non-uniform EWMA-PCA based cache size allocation scheme in Named Data Networks",2018,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027715781&doi=10.1007%2fs11432-016-0501-5&partnerID=40&md5=11763df00f315ddb93bf5c11016ec56c","As a data-centric cache-enabled architecture, Named Data Networking (NDN) is considered to be an appropriate alternative to the current host-centric IP-based Internet infrastructure. Leveraging in- network caching, name-based routing, and receiver-driven sessions, NDN can greatly enhance the way Internet resources are being used. A critical issue in NDN is the procedure of cache allocation and management. Our main contribution in this research is the analysis of memory requirements to allocate suitable Content-Store size to NDN routers, with respect to combined impacts of long-term centrality-based metric and Exponential Weighted Moving Average (EWMA) of short-term parameters such as users behaviours and outgoing traffic. To determine correlations in such large data sets, data mining methods can prove valuable to researchers. In this paper, we apply a data-fusion approach, namely Principal Component Analysis (PCA), to discover relations from short- and long-term parameters of the router. The output of PCA, exploited to mine out raw data sets, is used to allocate a proper cache size to the router. Evaluation results show an increase in the hit ratio of Content-Stores in sources, and NDN routers. Moreover, for the proposed cache size allocation scheme, the number of unsatisfied and pending Interests in NDN routers is smaller than the Degree-Centrality cache size scheme. © 2017, Science China Press and Springer-Verlag GmbH Germany.","content centric networks; exponential weighted moving average; future Internet; Named Data Networks; NDN cache size; principal component analysis","Data fusion; Data mining; Long short-term memory; Network architecture; Routers; Cache size; Content-centric networks; Exponential weighted moving average; Future internet; Named data networks; Principal component analysis",2-s2.0-85027715781
"Guo W., Xu C., Tan J., Li L.","The optimization of intersection signal in the situation of data loss",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026748820&doi=10.1007%2f978-981-10-3551-7_84&partnerID=40&md5=d74e9f0e54146c540b0ccbf5a38e9596","There is a problem of intersection detector data missing in many cities because of various reasons, such as construction damage, line failure, raining, snowing or fogging, and processing errors, which will inevitably bring adverse effects on traffic flow data analysis as well as deep data mining. As a result, the signal timing plan at the intersection will be unreasonable which will exacerbate the problem of traffic jam. The intersection of Pingan Road and Xingfu Street was taken as an example, which has a critical defect problem of data missing. Experiment and analysis were made, and tireless efforts were tried to implement assumption and completion from the minimum to maximum for the southern traffic data. What’s more, based on the traditional timing model calculation of Webster and HCM, the initial optimization plan can be drawn, and then, the total delay was taken as the evaluation index. Furthermore, comparison between the suggested timing plan with verification and data processing via VISSIM software and current timing plan will also be carried out. Based on this, simulation experiment in different timing plan and flow will also be carried out, and the result will be matrix-arranged. Finally, a timing plan with relatively small delay for the whole intersection was hoped to be find out no matter the southern direction traffic flow and without field investigation. © Springer Science+Business Media Singapore 2018.","Data loss; Delay; Signal optimization; Traffic control","Computer aided software engineering; Data mining; Intelligent systems; Intelligent vehicle highway systems; Traffic congestion; Traffic control; Traffic signals; Transportation; Verification; Construction damages; Data loss; Delay; Experiment and analysis; Field investigation; Initial optimization; Signal optimization; Signal timing plan; Data handling",2-s2.0-85026748820
"Mane R.V., Ghorpade V.R.","Association rule mining for finding admission tendency of engineering student with pattern growth approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031413932&doi=10.1007%2f978-981-10-6620-7_73&partnerID=40&md5=b75334836a4d1d282be330c91e580390","Association Rule Mining is one of the important techniques in data mining. Generation of the rule involves two phases where the first phase finds the frequent itemsets and second phase generates the rule. Many algorithms are specified to find frequent item set from the sequential patterns. There are mainly two approaches for finding frequent item sets. First approach is with candidate sequence generation, i.e., Apriori approach and second is the pattern growth method. If the sequence length is less, pattern growth method performs better than that of Apriori approach. In this paper, we have analyzed the pattern growth approach for the database of an engineering student. With finding associations among the attributes we can find the tendency of taking admission and prioritizing an engineering branch. To find strong and valid association rules, different measures like minInterest, lift, leverage, and conviction are considered during finding rules. © 2018, Springer Nature Singapore Pte Ltd.","Association rule mining; Constraint; Measure; Pattern growth","Association rules; Data mining; Candidate sequences; Constraint; Frequent item sets; Measure; Pattern growth; Second phase; Sequence lengths; Sequential patterns; Big data",2-s2.0-85031413932
"Wu S.-J., Chiang R.-D., Chang W.-T.","Extracting new opinion elements in the semi-automatic chinese opinion-mining system from internet forums",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402847&doi=10.1007%2f978-981-10-3187-8_51&partnerID=40&md5=52b0d8740edfa4e6045fa42cd63f2dae","Articles posted on a forum often contain new Internet terms related to opinion elements. Consequently, existing Chinese opinion-mining systems may exhibit low recall and precision because they cannot recognize these new Internet terms. Therefore, we designed an algorithm to elaborate on the opinion elements of such articles by extracting the new terms. By ignoring any uncommon opinion element that appears only once, we determine whether the new term identified through manual judgment is a useful opinion element for a specific domain and add it to the thesaurus. In comparison with semi-automatic annotation methods, our approach can save considerable labor. The same Chinese word may have different meanings depending on the context, and this fact is prone to cause difficulties by changing the polarity or meaning of certain opinion elements, leading to errors in the analysis results of many Chinese systems. We designed appropriate algorithms to address this problem. Meanwhile, this system extracts the opinion elements from an article based on its established thesaurus and simultaneously considers various sentence patterns, the default topic, and clause priority to determine the opinion tendency of the author. © Springer Nature Singapore Pte Ltd. 2018.","Customer review; Opinion mining system; Sentiment analysis","Computation theory; Internet; Mining machinery; Thesauri; Customer review; Internet forums; New terms; Opinion mining; Recall and precision; Semi-automatic annotation; Semi-automatics; Sentiment analysis; Data mining",2-s2.0-85031402847
"Velampalli S., Jonnalagedda V.R.M.","Frequent subgraph mining algorithms: Framework, classification, analysis, comparisons",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021236825&doi=10.1007%2f978-981-10-3223-3_31&partnerID=40&md5=0cfb820909b98e9f32607c872bec4949","Graphs and Trees are non-linear data structures used to organise, model and solve many real world problems and becoming more popular both in scientific as well as commercial domains. They have wide number of applications ranging from Telephone networks, Internet, Social Networks, Program flow, Chemical Compounds, BioInformatics, XML data, Terrorist networks etc. Graph Mining is used for finding useful and significant patterns. Frequent subgraph Mining mines for frequent patterns and subgraphs and they form the basis for Graph clustering, Graph classification, Graph Based Anomaly Detection. In this paper, classification of FSM algorithms is done and popular frequent subgraph mining algorithms are discussed. Comparative study of algorithms is done by taking chemical compounds dataset. Further, this paper provides a framework which acts as strong foundation in understanding any frequent subgraph mining algorithm. © Springer Nature Singapore Pte Ltd. 2018.","Apriori; Chemical compounds; Frequent subgraph mining; Graph mining; Graphs; Pattern growth; Trees","Application programs; Chemical compounds; Flow graphs; Intelligent computing; Trees (mathematics); Apriori; Frequent subgraph mining; Graph mining; Graphs; Pattern growth; Trees; Data mining",2-s2.0-85021236825
"Sethy R., Dash S.K., Panda M.","Performance comparison between apache hive and oracle SQL for big data analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028589407&doi=10.1007%2f978-3-319-60618-7_14&partnerID=40&md5=71f2354ff1576f4bedd46f8f97fd4d77","Big data shall mean the massive volume of data that could not be stored, processed and managed by any traditional database management systems. Big Data Analytics becoming a comprehensive research area today this has attracted to all academia and industry to extract knowledge and information from a large amount of data. Oracle SQL is a prominent DBMS and is used worldwide. As the data goes bigger the running time is increasing in Oracle SQL. With the help of Apache Hive, we can do a large scale of data analysis in minimal time period. Apache Hive expedites for reading, writing and managing big datasets in distributed environment using SQL. Whereas Oracle SQL provides integrated development domain for running queries and scripts. In this paper, we have taken few queries for analysis for some smaller data sets as well as larger data sets and we have done an analysis for both Apache Hive and Oracle SQL environment. © Springer International Publishing AG 2018.","Apache hadoop; Apache hive; Big data; Oracle SQL; Query processing","Computer software; Data mining; Database systems; Information management; Pattern recognition; Query processing; Soft computing; Apache hadoop; Apache hive; Comprehensive research; Data analytics; Distributed environments; Integrated development; Oracle SQL; Performance comparison; Big data",2-s2.0-85028589407
"Hasti C., Hasti A.","Data security in cloud-based analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031416571&doi=10.1007%2f978-981-10-6620-7_11&partnerID=40&md5=9ce1a3661212006e3703091a517b47c2","Cloud computing platforms have grown in prominence in last few years, as they have made business applications and information accessible on the move without the need to purchase, set up, and maintain necessary hardware and software. The organizations are churning enormous gains due to scalability, agility, and efficiency achieved through the use of clouds. Data analytics involves voluminous data crunching to determine trends and patterns for business intelligence, scientific studies, and data mining. The incessant outburst of data from multiple sources such as web applications, social media, and other Internet-based sources motivate leveraging cloud technology for data analytics. Different strategies are being studied and incorporated to use the subscription-based cloud for serving analytics systems. The paper focusses on understanding the security threats associated with cloud-based analytics and approaches to cloud security assurance in data analytics systems. © 2018, Springer Nature Singapore Pte Ltd.","Analytics as a Service (AaaS); Cryptography; Multi-tenancy; Trusted third-party auditor; VMWare","Application programs; Cryptography; Data mining; Analytics as a Service (AaaS); Business applications; Cloud computing platforms; Hardware and software; Multi tenancies; Scientific studies; Trusted third parties; VMWare; Big data",2-s2.0-85031416571
"Birjali M., Beni-Hssane A., Erritali M.","Learning with big data technology: The future of education",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028648712&doi=10.1007%2f978-3-319-60834-1_22&partnerID=40&md5=e8f8f581a93904bc3f4ef2c47605cdc9","The use of Big Data systems in the field of education allows to envisage new approaches and new learning contexts. Indeed, the rapid emergence of the new e-learning platforms have been presented in many interests. However, the quality of the teaching service rendered depends on the capacity of the learning approaches to be provided to learners, content and learning path tailored to their needs. In this paper, we will present how Big Data helps to solve education issues through reaching the objective of learning. Then, we will introduce some opportunities of Big Data analytics to develop the efficiency and effectiveness of students learning and maximize their knowledge retention. Finally, our research method show that students can generate personalized activities and offer academic advising. Big Data can expose the capabilities of learners, predict their future performances and offer assistance for educational organizations to make strategic decisions. © 2018, Springer International Publishing AG.","Big Data; Data mining; e-Learning; Education; Learning analytics","Big data; Data mining; E-learning; Students; Data technologies; E-learning platforms; Educational organizations; Future performance; Knowledge retention; Learning analytics; Strategic decisions; Students learning; Education",2-s2.0-85028648712
"Ahuja B., Anuradha, Juneja D.","Hidden data extraction using url templates processing",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031424567&doi=10.1007%2f978-981-10-6620-7_13&partnerID=40&md5=01d0af10acc262845641370a585a51f0","A lot of work has been carried out in the deep web. Deep web is like a golden apple in the eyes of the researchers. Most of the deep web search engines extract the data from the deep web, store them in the database, and index them. So, such kind of techniques have the disadvantage of less freshness, large repository requirement and need of frequent updating of the deep web database to give accurate and correct results. In order to overcome these drawbacks, we propose a new technique “Hidden Data Extraction using URL Template processing” where the fresh results from the website server database are fetched dynamically and are served to the users. © 2018, Springer Nature Singapore Pte Ltd.","Hidden web; Query interfaces; Search engines; Surface web","Data mining; Database systems; Extraction; Query processing; Search engines; Websites; Data extraction; Deep web; Hidden web; Query interfaces; Server database; Big data",2-s2.0-85031424567
"Li F., Zhang X., Zhang X., Du C., Xu Y., Tian Y.-C.","Cost-sensitive and hybrid-attribute measure multi-decision tree over imbalanced data sets",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029044394&doi=10.1016%2fj.ins.2017.09.013&partnerID=40&md5=26e8b23e26cefe8fc110cd51d26fe33b","One of the most popular algorithms for classification is the decision tree. However, existing binary decision tree models do not handle well the minority class over imbalanced data sets. To address this difficulty, a Cost-sensitive and Hybrid attribute measure Multi-Decision Tree (CHMDT) approach is presented in this paper. It penalizes misclassification through a hybrid attribute measure, which is defined from the combination of the Gini index and information gain measure. It further builds a multi-decision tree consisting of multiple decision trees each with different root node information. The overall objective of the approach is to maximize the classification performance with the hybrid attribute measure while minimizing the total misclassification cost. Experiments are conducted over twelve KEEL imbalanced data sets to demonstrate the CHMDT approach. They show that the classification performance of the minority class is improved significantly without sacrifice of the overall classification accuracy of the majority class. © 2017 Elsevier Inc.","Cost sensitivity; Hybrid attribute measure; Imbalanced data set; Minority class; Multi-decision tree","Binary trees; Costs; Decision trees; Trees (mathematics); Binary decision trees; Classification accuracy; Classification performance; Hybrid attributes; Imbalanced Data-sets; Minority class; Misclassification costs; Multi decisions; Data mining",2-s2.0-85029044394
"Geman O., Chiuchisan I., Covasa M., Doloc C., Milici M.-R., Milici L.-D.","Deep learning tools for human microbiome big data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029511447&doi=10.1007%2f978-3-319-62521-8_21&partnerID=40&md5=b9ac13c2e213a207bfb61319fd1705fe","Deep Learning is a branch of Machine Learning, which focuses on a set of algorithms that model high-level abstractions in data by using a deep representation of multiple processing layers. The goal of Machine Learning is to map input patterns to output values. This paper will suggest a potential application of Deep Learning Algorithms for the analysis of large amounts of data produced by the research of the Human Microbiome. Humans have coevolved with microbes in the environment, and each body habitat has a unique set of microorganisms (microbiota). The most abundant and well-studied microbiota are found in the gut, where the bacterial density reaches 1011–1012Â cells/g in the distal human colon. The number of bacteria in the human gut has been estimated to exceed the number of somatic cells in the body by an order of magnitude and that the biomass of the gut microbiota may reach up to 1.5Â kg. This paper presents different methods that have been implemented and tested on a Human Microbiome Dataset. Besides the findings concerning accuracy and runtime, the results suggest that the Deep Learning algorithms could be successfully used to analyze large amounts of Microbiota data. © 2018, Springer International Publishing AG.","Big Data; Data Mining; Deep learning; Human Microbiome; Machine learning","Artificial intelligence; Bacteria; Big data; Data mining; Deep learning; Learning systems; Microorganisms; Soft computing; Bacterial density; Gut microbiota; High-level abstraction; Human microbiome; Input patterns; Large amounts of data; Learning tool; Multiple processing; Learning algorithms",2-s2.0-85029511447
"Kamanksha D.P., Sanjay A.","A critical analysis of twitter data for movie reviews through ‘random forest’ approach",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028391349&doi=10.1007%2f978-3-319-63645-0_52&partnerID=40&md5=f6a23e307d84998c9fade0e754715dcd","Using Sentiment analysis one can understand interaction of a user with the movies through their feedback. Here analysis is done based on the movie reviews that can be collected from many sources. Twitter is one among the foremost frequent on-line social media and micro blogging services. Due to the popularity of twitter it has become a useful resource for collecting sentiments through API or other data mining techniques. Our work here presents an examination on the evaluation of the machine learning algorithms (Random Forest, bagging, SVM and Naïve Bayes) in R together the public opinion for example opinion about ‘Civil War’ Movie. Here we have used ‘Random Forest’ to show its better performance in the analysis of movie reviews. © Springer International Publishing AG 2018.","Natural language processing; Opinion mining; Sentiment analysis; Sentiment classification; Twitter","Decision trees; Intelligent systems; Learning algorithms; Learning systems; Motion pictures; Natural language processing systems; Social aspects; Social networking (online); Critical analysis; Micro-blogging services; Opinion mining; Public opinions; Random forests; Sentiment analysis; Sentiment classification; Twitter; Data mining",2-s2.0-85028391349
"Zhang Q., Yang L.T., Chen Z., Li P.","High-order possibilistic c-means algorithms based on tensor decompositions for big data in IoT",2018,"Information Fusion",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017458048&doi=10.1016%2fj.inffus.2017.04.002&partnerID=40&md5=569110bcdcbdd58580addf105afe097a","Internet of Things (IoT) connects the physical world and the cyber world to offer intelligent services by data mining for big data. Each big data sample typically involves a large number of attributes, posing a remarkable challenge on the high-order possibilistic c-means algorithm (HOPCM). Specially, HOPCM requires high-performance servers with a large-scale memory and a powerful computing unit, to cluster big samples, limiting its applicability in IoT systems with low-end devices such as portable computing units and embedded devises which have only limited memory space and computing power. In this paper, we propose two high-order possibilistic c-means algorithms based on the canonical polyadic decomposition (CP-HOPCM) and the tensor-train network (TT-HOPCM) for clustering big data. In detail, we use the canonical polyadic decomposition and the tensor-train network to compress the attributes of each big data sample. To evaluate the performance of our algorithms, we conduct the experiments on two representative big data datasets, i.e., NUS-WIDE-14 and SNAE2, by comparison with the conventional high-order possibilistic c-means algorithm in terms of attributes reduction, execution time, memory usage and clustering accuracy. Results imply that CP-HOPCM and TT-HOPCM are potential for big data clustering in IoT systems with low-end devices since they can achieve a high compression rate for heterogeneous samples to save the memory space significantly without a significant clustering accuracy drop. © 2017","Big data; Canonical polyadic decomposition; IoT; Possibilistic c-means clustering; Tensor-train network","Cluster computing; Clustering algorithms; Data mining; Embedded systems; Internet of things; Tensors; Attributes reduction; Canonical polyadic decompositions; Intelligent Services; Internet of Things (IOT); Possibilistic C-means; Possibilistic c-means clustering; Tensor decomposition; Tensor trains; Big data",2-s2.0-85017458048
"Chatterjee R., Goyal M.","Optima (opinionated tweet implied mining and analysis): An innovative tool to automate sentiment analysis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031404463&doi=10.1007%2f978-981-10-6620-7_36&partnerID=40&md5=8ee72442e3967ba299a1ea46f4689dae","The prevalent social media usage ramification has recently directed the research into the area of “Sentiment Analysis” producing potpourri of interesting results to analyze and apply in diverse domains. Among its existence in several forms, “Twitter” (an instance of the social media) is the most popular micro-blogging platform. Hence, contextually, “Sentiment analysis” connotes to assortment of user’s opinions expressed over the “Twitter,” into positive and negative classes. To exemplify and establish the essence, significance, and incidence of Opinion Mining, a corpus of tweets on “Windows 10” has been collected and built, with an objective to provide aid in customer feedback loop during its beta release. This paper focuses on the significance of sentiment analysis in a reasoned manner by epitomizing the working methodology of opinion mining techniques via its automated implementation. To address the purpose, an innovative tool has been developed named OPTIMA (OPinionated Tweet Implied Mining and Analysis) ver. 1.0.0., to automate the process of sentiment analysis and the results have been presented in a graphical form, in an analytical and comprehensive manner. © 2018, Springer Nature Singapore Pte Ltd.","Naïve Bayesian; NLP; Opinion mining; Package; Sentiment analysis; SVM; Tweets","Data mining; Packaging; Social networking (online); Bayesian; Customer feedback; Diverse domains; Graphical forms; Micro-blogging platforms; Opinion mining; Sentiment analysis; Tweets; Big data",2-s2.0-85031404463
"Pujari C., Aiswarya, Shetty N.P.","Comparison of classification techniques for feature oriented sentiment analysis of product review data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021248935&doi=10.1007%2f978-981-10-3223-3_14&partnerID=40&md5=e7d2452b6425d3b63d82433744168254","With the rapid increase in popularity of e-commerce services over the years, all varieties of products are sold online today. Posting online reviews has become a common means for people to express their impressions on any product, while serving as a recommendation for others. To enhance customer satisfaction and buying experience, often the sellers provide a platform for the customers to express their views. Due to the explosion of these opinion rich sites where numerous opinions about a product are expressed, a potential customer finds it difficult to read all the reviews and form an intelligent opinion about the product. In this research, a new framework comprising of the inbuilt packages of python is designed which mines many customers’ opinions about a product and groups them accordingly based on their sentiments, which aids the potential buyers to form a capitalized view on the product. Here classification of the reviews is done using three different classification algorithms i.e. Naïve Bayes Algorithm, Maximum Entropy Classifier and SVM (Support Vector Machine), and their performance is compared. The methodology showcased in this work can be extended easily in all domains. © Springer Nature Singapore Pte Ltd. 2018.","Anaconda; Bigram collocation; Classification; Feature based opinion mining; Nltk; Sentiment analysis; Tokenization","Customer satisfaction; Data mining; Intelligent computing; Sales; Support vector machines; Anaconda; Bigram collocation; Nltk; Opinion mining; Sentiment analysis; Tokenization; Classification (of information)",2-s2.0-85021248935
"Nguyen L.T.T., Nguyen L.T.T., Vo B.","An improved algorithm for mining top-k association rules",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025160120&doi=10.1007%2f978-3-319-61911-8_11&partnerID=40&md5=279c05637f2171a2b5973cda0b94a367","This paper proposes an improved algorithm of TopKRules algorithm which was proposed by Philippe et al. in 2012 to mine top-k association rules (ARs). To impove the perfomance of TopKRules, we develop two propositions to reduce search space and runtime in the mining process. Experimental results on standard databases show that our algorithm need less time than TopKRules algorithm to generate usefull rules. © Springer International Publishing AG 2018.","Association rule mining; Data mining; Rule expansion; Top-k association rules","Aluminum alloys; Data mining; Mining process; Perfomance; Rule expansions; Runtimes; Search spaces; Association rules",2-s2.0-85025160120
"Singh S.P., Kumar A., Darbari H., Kaur B., Tiwari K., Joshi N.","Intelligent text mining model for english language using deep neural network",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028391622&doi=10.1007%2f978-3-319-63645-0_54&partnerID=40&md5=f7dc4949fd99a7982e8a34f885c05ea2","Today there exist various sources that provide information in very massive amount to serve the demand over the internet, which creates huge collection of heterogeneous data. Thus existing data can be categorized as unstructured and structured data. In this paper we propose an idea of a tool which intelligently preprocesses the unstructured data by segmenting the whole document into number of sentences, using deep learning concepts with word2vec [11] and a Recurrent Neural Network [13]. At the beginning step we use word2vec which was introduced by Tomas Mikolov with his team at Google, to generate vectors of the inputted text content which will be further forwarded to Recurrent Neural Network. RNN takes this series of vectors as input and trained Data Cleaning Recurrent Neural Network model will perform preprocessing task (including cleaning of missing, grammatically incorrect, misspelled data) to produce structured results, which then passed into automatic summarization module to generate desired summary. © Springer International Publishing AG 2018.","Deep neural network (DNN); Recurrent neural network (RNN); Sentence processing; Summarization; Text processing","Data mining; Intelligent systems; Natural language processing systems; Recurrent neural networks; Text processing; Automatic summarization; English languages; Heterogeneous data; Recurrent neural network (RNN); Recurrent neural network model; Sentence processing; Summarization; Unstructured data; Deep neural networks",2-s2.0-85028391622
"Surya Prasanthi L., Kiran Kumar R., Srinivas K.","A novel random forest approach using specific under sampling strategy",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021206181&doi=10.1007%2f978-981-10-3223-3_24&partnerID=40&md5=55aa7a09bcf736c081b7a82227278b70","In Data Mining the knowledge is discovered from the existing real world data sets. In real time scenario, the category of datasets varies dynamically. One of the emerging categories of dataset is class imbalance data. In Class Imbalance data, the percentages of instances in one class are far greater than the other class. The traditional data mining algorithms are well applicable for knowledge discovery from balance datasets. Efficient knowledge discovery is hampered in the case of class imbalance datasets. In this paper, we propose a novel approach dubbed as Under Sampling using Random Forest (USRF) for efficient knowledge discovery from imbalance datasets. The proposed USRF approach is verified on the 11 benchmark datasets from UCI repository. The experimental observations show that an improved accuracy and AUC is achieved with the proposed USRF approach with a good reduction in RMS error. © Springer Nature Singapore Pte Ltd. 2018.","Data mining; Imbalance data; Knowledge discovery; Random forest","Decision trees; Intelligent computing; Benchmark datasets; Class imbalance; Data mining algorithm; Imbalance datum; Random forests; RMS errors; UCI repository; Under-sampling; Data mining",2-s2.0-85021206181
"Tidke B., Mehta R., Dhanani J.","A comprehensive survey and open challenges of mining bigdata",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028417766&doi=10.1007%2f978-3-319-63673-3_53&partnerID=40&md5=9ed7dc537757d5a2b6a18286010f0577","Bigdata comes into big picture in early 2000, since it becomes focus of researchers and data scientist. Main purpose of research and development in the field of Bigdata is to extract and predicts meaningful information from large amount of structured as well as unstructured real world data. In this paper, systematic review of background, existing related technologies used by various big enterprises, data researchers, government officials has been discussed. In addition, presented standardized complex processes to extract useful information such as data generation, storage, modeling/analysis, visualization and interpretation. Finally discusses open issues, challenges and point out the emerging directions in which researchers can work in the age of Bigdata © 2018, Springer International Publishing AG.","Bigdata; Bigdata analysis; Bigdata storage; Data mining","Data visualization; Digital storage; Intelligent systems; Bigdata; Bigdata analysis; Complex Processes; Data generation; Government officials; Large amounts; Research and development; Systematic Review; Data mining",2-s2.0-85028417766
"Gupta A., Gusain K., Goyal L.M.","Improved FP-linked list algorithm for association rule mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031415725&doi=10.1007%2f978-981-10-3773-3_54&partnerID=40&md5=913d9dd8a8ad6f475ba8dfb9662a0138","One of the more important techniques used in Data Mining is, Association Rule Mining and it involves the finding of frequent item sets from the database. A linked list version of one of the most widely used association mining algorithms that are the FP-Growth algorithm was proposed, called the FPBitLink Algorithm. It uses a bit matrix along with linked lists to find the desired item sets. In this paper, we propose two things, first is a variant of the FPBitLink Algorithm, in which instead of treating the items in the datasets as individual nodes, we take a single transaction to be the node in the linked list, and finally use UNION set operation to obtain the frequent pattern set. Since this transactional version is a highly efficient alternative when the number of transactions are greater than the number of items, as it saves valuable space and time, whereas the original FPBitLink algorithm is more efficient when the number of items is greater than the transactions, we further propose the installation of a checkpoint in the beginning, such that depending upon the data either of the two algorithms can be chosen. This way we arrive at a frequent pattern set finding procedure, which is both, highly efficient and extremely efficacious. © Springer Nature Singapore Pte Ltd. 2018.","Algorithm; Association rules; Data mining; FP-growth algorithm; Frequent itemset; Linked list","Algorithms; Association rules; Association mining; FP-growth algorithm; Frequent item sets; Frequent itemset; Linked list; List algorithms; Set operation; Space and time; Data mining",2-s2.0-85031415725
"Nakata T., Sohrab M.","Detection of typical progress patterns of industrial incidents by text mining technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023209560&doi=10.1007%2f978-3-319-60645-3_22&partnerID=40&md5=060c89aa189a153b89c97360dc2b809d","To prevent accidents, it is very important to learn why and how past accidents occurred and escalated. The information of accidents is mostly recorded in natural language texts, which is not convenient to analyze the flow of events in the accidents. This paper proposes a method to recognize typical flow of events in a large set of text reports. By focusing two adjacent sentences, our system succeeded to detect typical pairs of predecessor word and successor word. Then we can recognize the typical flows of accidents. © Her Majesty the Queen in Right of United Kingdom 2018.","Human factors; Incident report; Safety engineering; Text mining","Accidents; Data mining; Errors; Human engineering; Natural language processing systems; Reliability; Safety engineering; Incident reports; Industrial incidents; Natural language text; Text mining; Text mining techniques; Character recognition",2-s2.0-85023209560
"Okoye K., Tawil A.-R.H., Naeem U., Islam S., Lamine E.","Semantic-based model analysis towards enhancing information values of process mining: Case study of learning process domain",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028594873&doi=10.1007%2f978-3-319-60618-7_61&partnerID=40&md5=97958985f7a812d26c753825b7211889","Process mining results can be enhanced by adding semantic knowledge to the derived models. Information discovered due to semantic enrichment of the deployed process models can be used to lift process analysis from syntactic level to a more conceptual level. The work in this paper corroborates that semantic-based process mining is a useful technique towards improving the information value of derived models from the large volume of event logs about any process domain. We use a case study of learning process to illustrate this notion. Our goal is to extract streams of event logs from a learning execution environment and describe formats that allows for mining and improved process analysis of the captured data. The approach involves mapping of the resulting learning model derived from mining event data about a learning process by semantically annotating the process elements with concepts they represent in real time using process descriptions languages, and linking them to an ontology specifically designed for representing learning processes. The semantic analysis allows the meaning of the learning objects to be enhanced through the use of property characteristics and classification of discoverable entities, to generate inference knowledge which are used to determine useful learning patterns by means of the Semantic Learning Process Mining (SLPM) algorithm - technically described as Semantic-Fuzzy Miner. To this end, we show how data from learning processes are being extracted, semantically prepared, and transformed into mining executable formats to enable prediction of individual learning patterns through further semantic analysis of the discovered models. © Springer International Publishing AG 2018.","Event logs; Knowledge discovery; Learning process; Ontology; Process mining; Semantic annotation","Inference engines; Learning algorithms; Learning systems; Ontology; Pattern recognition; Semantics; Soft computing; Event logs; Execution environments; Individual learning; Learning process; Process descriptions; Process mining; Semantic annotations; Semantic enrichment; Data mining",2-s2.0-85028594873
"Kushwaha N., Merlino G., Longo F., Dario B., Puliafito A., Vyas O.P.","Providing sensor services by data correlation: The #SmartME approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026317817&doi=10.1007%2f978-3-319-61566-0_82&partnerID=40&md5=7b4cb65c4b09ea03df77bbea893f4002","In the current era Internet is the most used medium for sharing and retrieving the information for building applications which are commonly developed for enhancing the user experience in terms of comfort, communication. For this, the need of real-time sensor data gains importance. The data collected from the physical objects should be easily available for different applications. Semantic representation of the sensor data directly addresses the problem of storing it in logical, easily accessible and extensible manner. Our paper works towards converting the already collected sensor data of the #SmartME project into semantic format and also proposes real-time storage of semantically enriched sensor data. To build applications using these sensor data the authors consider mainly three kinds of sensors, i.e., Temperature, Humidity, Pressure. Predicting the observed value of any sensor data is the main aim of this work. The analysis leverages other sensors & environmental parameters such as Date, Time, Longitude, Latitude, Altitude etc. Correlation among these parameters and the accuracy of the predicted results showed the suitability of our proposed idea. © Springer International Publishing AG 2018.","Correlations; Data mining; IoT; Semantic web; Sensor network; SmartME; Stack4Things","Complex networks; Data mining; Semantic Web; Sensor networks; Building applications; Correlations; Data correlations; Environmental parameter; Real time sensors; Semantic representation; SmartME; Stack4Things; Digital storage",2-s2.0-85026317817
"Gao X., Sun Q., Yang J.","MRCCA: A novel CCA based method and its application in feature extraction and fusion for matrix data",2018,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032458500&doi=10.1016%2fj.asoc.2017.10.008&partnerID=40&md5=3d049b9bcbde02f295ab916a6d9d9bb9","Multiset features extracted from the same pattern usually represent different characteristics of data, meanwhile, matrices or 2-order tensors are common forms of data in real applications. Hence, how to extract multiset features from matrix data is an important research topic for pattern recognition. In this paper, by analyzing the relationship between CCA and 2D-CCA, a novel feature extraction method called multiple rank canonical correlation analysis (MRCCA) is proposed, which is an extension of 2D-CCA. Different from CCA and 2D-CCA, in MRCCA k pairs left transforms and k pairs right transforms are sought to maximize correlation. Besides, the multiset version of MRCCA termed as multiple rank multiset canonical correlation analysis (MRMCCA) is also developed. Experimental results on five real-world data sets demonstrate the viability of the formulation, they also show that the recognition rate of our method is higher than other methods and the computing time is competitive. © 2017 Elsevier B.V.","Canonical correlation analysis (CCA); Feature extraction; Feature fusion; Matrix data; Multiset canonical correlation analysis (MCCA); Pattern recognition; Two-dimensional CCA (2D-CCA)","Correlation methods; Data mining; Extraction; Feature extraction; Pattern recognition; Canonical correlation analysis; Computing time; Feature extraction methods; Feature fusion; ITS applications; Multiset canonical correlation analysis; Real applications; Research topics; Matrix algebra",2-s2.0-85032458500
"Bodyanskiy Y., Vynokurova O., Pliss I., Peleshko D., Rashkevych Y.","Deep stacking convex neuro-fuzzy system and its on-line learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020830756&doi=10.1007%2f978-3-319-59415-6_5&partnerID=40&md5=63875cd5f8b81609a16937f94b8b4079","In the paper the architecture of Deep Stacking Convex Neuro-Fuzzy System for data stream processing in on-line mode is proposed. The advantage of proposed system is that its layers are formed by multivariate modification of hybrid generalized additive neuro-fuzzy system. Such system is characterized by simplicity of computational implementation, high speed learning, increased approximation properties. For learning of the proposed system both conventional least squares method (including its recurrent version) and specialized learning procedures, which have tracking and smoothing properties are used. Proposed system is aimed at solving of wide range of Data Stream Mining problems, which are connected with processing of nonstationary stochastic and chaotic processes under conditions when information is fed to the system in on-line mode. © Springer International Publishing AG 2018.","Computational intelligence; Deep learning; Deep stacking convex neuro-fuzzy system; Hybrid systems","Artificial intelligence; Data communication systems; Data handling; Data mining; Deep learning; E-learning; Fuzzy inference; Fuzzy neural networks; Hybrid systems; Least squares approximations; Stochastic systems; Approximation properties; Computational implementations; Data stream mining; Data stream processing; Generalized additives; Learning procedures; Least squares methods; Neurofuzzy system; Fuzzy systems",2-s2.0-85020830756
"Fong S., Deb S., Yang X.-S.","How meta-heuristic algorithms contribute to deep learning in the hype of big data analytics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026775964&doi=10.1007%2f978-981-10-3373-5_1&partnerID=40&md5=ad6fcb2e35de3e2aa77873ed8dc3d955","Deep learning (DL) is one of the most emerging types of contemporary machine learning techniques that mimic the cognitive patterns of animal visual cortex to learn the new abstract features automatically by deep and hierarchical layers. DL is believed to be a suitable tool so far for extracting insights from very huge volume of so-called big data. Nevertheless, one of the three “V” or big data is velocity that implies the learning has to be incremental as data are accumulating up rapidly. DL must be fast and accurate. By the technical design of DL, it is extended from feed-forward artificial neural network with many multi-hidden layers of neurons called deep neural network (DNN). In the training process of DNN, it has certain inefficiency due to very long training time required. Obtaining the most accurate DNN within a reasonable run-time is a challenge, given there are potentially many parameters in the DNN model configuration and high dimensionality of the feature space in the training dataset. Meta-heuristic has a history of optimizing machine learning models successfully. How well meta-heuristic could be used to optimize DL in the context of big data analytics is a thematic topic which we pondered on in this paper. As a position paper, we review the recent advances of applying meta-heuristics on DL, discuss about their pros and cons and point out some feasible research directions for bridging the gaps between meta-heuristics and DL. © Springer Nature Singapore Pte Ltd. 2018.","Algorithm design; Deep learning; Meta-heuristic algorithm; Nature-inspired computing algorithms; Neural network training","Artificial intelligence; Computation theory; Data mining; Deep learning; Deep neural networks; Heuristic algorithms; Intelligent computing; Learning algorithms; Learning systems; Neural networks; Optimization; Algorithm design; Feed-forward artificial neural networks; High dimensionality; Machine learning models; Machine learning techniques; Meta heuristic algorithm; Nature inspired computing; Neural network training; Big data",2-s2.0-85026775964
"Karpio K., Łukasiewicz P.","Association rules in data with various time periods",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030780466&doi=10.1007%2f978-3-319-67792-7_38&partnerID=40&md5=bc5a5c9d9823ecf11e994c7628c1ee6a","In this paper an Association Rules data mining technique is adopted to explore the co-movement between sector indices listed on the Warsaw Stock Exchange. The indices are related to the various sectors of the economy. Because of the different time ranges the various indices are traded, the special approach has been used. That allowed us to analyze data in a wide range of time. The results were compared to those obtained using the tradi-tional approach. We observed higher values of measures and smaller errors for a majority of rules. © 2018, Springer International Publishing AG.","Association rules; Data mining; Sector indices","Association rules; Co movements; Sector indices; Stock exchange; Time range; Time-periods; Data mining",2-s2.0-85030780466
"Lawanont W., Inoue M.","A development of classification model for smartphone addiction recognition system based on smartphone usage data",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020456424&doi=10.1007%2f978-3-319-59424-8_1&partnerID=40&md5=b97139c6849d68bf7ca60dce9c97d813","The rapid growth of smartphone in recent years has resulted in many syndromes. Most of these syndromes are caused by excessive use of smartphone. In addition, people who tends to use smartphone excessively are also likely to have smartphone addiction. In this paper, we presented the system architecture for e-Health system. Not only we used the architecture for our smartphone addiction recognition system, but we also pointed out important benefits of the system architecture, which also can be adopted by other system. Later on, we presented a development of the classification model for recognizing likelihood of having smartphone addiction. We trained the classification model based on data retrieved from subjects’ smartphone. The result showed that the best model can correctly classify the instance up to 78%. © Springer International Publishing AG 2018.","Activity recognition; Data mining; e-Health system; Smartphone addiction; Smartphone application","Architecture; Computer architecture; Data mining; Activity recognition; Classification models; e-Health systems; Rapid growth; Recognition systems; Smart-phone applications; System architectures; Usage data; Smartphones",2-s2.0-85020456424
"Chowdhury K., Chaudhuri D., Pal A.K.","Seed point selection algorithm in clustering of image data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032618235&doi=10.1007%2f978-981-10-3376-6_13&partnerID=40&md5=978cca64957ca892951c68c2e852123c","Massive amount of data are being collected in almost all sectors of life due to recent technological advancements. Various data mining tools including clustering is often applied on huge data sets in order to extract hidden and previously unknown information which can be helpful in future decision-making processes. Clustering is an unsupervised technique of data points which is separated into homogeneous groups. Seed point is an important feature of a clustering technique, which is called the core of the cluster and the performance of seed-based clustering technique depends on the choice of initial cluster center. The initial seed point selection is a challenging job due to formation of better cluster partition with rapidly convergence criteria. In the present research we have proposed the seed point selection algorithm applied on image data by taking the RGB features of color image as well as 2D data based on the maximization of Shannon’s entropy with distance restriction criteria. Our seed point selection algorithm converges in a minimum number of steps for the formation of better clusters. We have applied our algorithm in different image data as well as discrete data and the results appear to be satisfactory. Also we have compared the result with other seed selection methods applied through K-Means algorithm for the comparative study of number of iterations and CPU time with the other clustering technique. © Springer Nature Singapore Pte Ltd. 2018.","Clustering; Data mining; K-means; Seed point; Shannon’s entropy",,2-s2.0-85032618235
"Ewing S.M., Boring R.L., Rasmussen M., Ulrich T.","Text mining for procedure-level primitives in human reliability analysis",2018,"Advances in Intelligent Systems and Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023169126&doi=10.1007%2f978-3-319-60645-3_24&partnerID=40&md5=49d3b10f77d2b92998b9ebac85510932","The classification of nuclear power plant procedures at the sub-task level can be accomplished via text mining. This method can inform dynamic human reliability calculations without manual coding. Several approaches to text classification are considered with results provided. When a discrete discriminant analysis is applied to the text, this results in clear identification procedure primitive greater than 88% of the time. Other analysis methods considered are Euclidian difference, principal component analysis, and single value decomposition. The text mining approach automatically decomposes procedure steps as Procedure Level Primitives, which are mapped to task level primitives in the Goals, Operation, Methods, and Section Rules (GOMS) human reliability analysis (HRA) method. The GOMS-HRA method is used as the basis for estimating operator timing and error probability. This approach also provides a tool that may be incorporated in dynamic HRA methods such as the Human Unimodel for Nuclear Technology to Enhance Reliability (HUNTER) framework. © Her Majesty the Queen in Right of United Kingdom 2018.","Computation-based human reliability analysis; GOMS-HRA; Human error; Human reliability analysis; Text mining","Classification (of information); Data mining; Discriminant analysis; Errors; Nuclear fuels; Nuclear power plants; Principal component analysis; Reliability; Text processing; Error probabilities; GOMS-HRA; Human errors; Human reliability analysis; Identification procedure; Single value decompositions; Text classification; Text mining; Reliability analysis",2-s2.0-85023169126
"López-Sánchez D., Arrieta A.G., Corchado J.M.","Deep neural networks and transfer learning applied to multimedia web mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022199440&doi=10.1007%2f978-3-319-62410-5_15&partnerID=40&md5=00db7c063d0faf35a4e9cf93abd0328c","The growth in the amount of multimedia content available online supposes a challenge for search and recommender systems. This information in the form of visual elements is of great value to a variety of web mining tasks; however, the mining of these resources is a difficult task due to the complexity and variability of the images. In this paper, we propose applying a deep learning model to the problem of web categorization. In addition, we make use of a technique known as transfer or inductive learning to drastically reduce the computational cost of the training phase. Finally, we report experimental results on the effectiveness of the proposed method using different classification methods and features from various depths of the deep model. © Springer International Publishing AG 2018.","Deep learning; Transfer learning; Web mining","Artificial intelligence; Complex networks; Data mining; Deep neural networks; Distributed computer systems; Education; Online systems; Classification methods; Computational costs; Inductive learning; Learning models; Multimedia contents; Transfer learning; Web categorization; Web Mining; Deep learning",2-s2.0-85022199440
"Itani S., Lecron F., Fortemps P.","A multi-level classification framework for multi-site medical data: Application to the ADHD-200 collection",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028500199&doi=10.1016%2fj.eswa.2017.08.044&partnerID=40&md5=7cdfa8fdc386b982509d1b16c624de5d","Recently, the culture of sharing medical data has emerged impressively, reducing significantly the barrier to the development of medical research accordingly. As open-access large datasets result from this significant initiative, data mining techniques can be considered for the development of interpretable expert systems to help in diagnosis. However, the collaborative effort of information gathering yields heterogeneous databases because of technical and geographical factors. Indeed, on the one hand, the harmonization of protocols for data collection is still missing. On the other hand, cultural and social factors impact locally both the epidemiology and etiology of a given disease. Ignoring these factors could weaken the credibility of studies based on multi-site data. Thereby, our work tackles the development of computer-aided diagnosis systems relying on heterogeneous data. For such a purpose, we propose a multi-level approach (inspired by multi-level statistical modeling) based on decision trees (in the sense of machine learning). This framework is applied on the public ADHD-200 collection for the study of Attention Deficit Hyperactivity Disorder (ADHD). © 2017 Elsevier Ltd","Attention Deficit Hyperactivity Disorder (ADHD); Clinical decision support systems; Decision trees; Multi-level approach","Artificial intelligence; Computer aided diagnosis; Data mining; Decision support systems; Decision trees; Diseases; Distributed computer systems; Expert systems; Forestry; Learning systems; Attention deficit hyperactivity disorder; Clinical decision support systems; Computer aided diagnosis systems; Heterogeneous database; Information gathering; Multi-level classifications; Multilevels; Statistical modeling; Diagnosis",2-s2.0-85028500199
"Sprint G., Cook D.J., Schmitter-Edgecombe M.","Unsupervised detection and analysis of changes in everyday physical activity data",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032015752&doi=10.1007%2f978-3-319-67513-8_6&partnerID=40&md5=e51f058485b3bd203455980ecaae3b1b","Sensor-based time series data can be utilized to monitor changes in human behavior as a person makes a significant lifestyle change, such as progress toward a fitness goal. Recently, wearable sensors have increased in popularity as people aspire to be more conscientious of their physical health. Automatically detecting and tracking behavior changes from wearable sensor-collected physical activity data can provide a valuable monitoring and motivating tool. In this paper, we formalize the problem of unsupervised physical activity change detection and address the problem with our Physical Activity Change Detection (PACD) approach. PACD is a framework that detects changes between time periods, determines significance of the detected changes, and analyzes the nature of the changes. We compare the abilities of three change detection algorithms from the literature and one proposed algorithm to capture different types of changes as part of PACD. We illustrate and evaluate PACD on synthetic data and using Fitbit data collected from older adults who participated in a health intervention study. Results indicate PACD detects several changes in both datasets. The proposed change algorithms and analysis methods are useful data mining techniques for unsupervised, window-based change detection with potential to track users’ physical activity and motivate progress toward their health goals. © Springer International Publishing AG 2018.","Change point detection; Data mining; Physical activity monitoring; Unsupervised learning; Wearable sensors",,2-s2.0-85032015752
"Liu X., Deng R.H., Yang Y., Tran H.N., Zhong S.","Hybrid privacy-preserving clinical decision support system in fog–cloud computing",2018,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016064811&doi=10.1016%2fj.future.2017.03.018&partnerID=40&md5=7b0d6f4967ca62aad18b354e3c554eee","In this paper, we propose a framework for hybrid privacy-preserving clinical decision support system in fog–cloud computing, called HPCS. In HPCS, a fog server uses a lightweight data mining method to securely monitor patients’ health condition in real-time. The newly detected abnormal symptoms can be further sent to the cloud server for high-accuracy prediction in a privacy-preserving way. Specifically, for the fog servers, we design a new secure outsourced inner-product protocol for achieving secure lightweight single-layer neural network. Also, a privacy-preserving piecewise polynomial calculation protocol allows cloud server to securely perform any activation functions in multiple-layer neural network. Moreover, to solve the computation overflow problem, a new protocol called privacy-preserving fraction approximation protocol is designed. We then prove that the HPCS achieves the goal of patient health status monitoring without privacy leakage to unauthorized parties by balancing real-time and high-accurate prediction using simulations. © 2017 Elsevier B.V.","Clinical decision support system; Cloud computing; Fog computing; Neural networks; Privacy-preserving","Artificial intelligence; Cloud computing; Data mining; Data privacy; Decision support systems; Fog; Internet protocols; Network function virtualization; Network layers; Network security; Neural networks; Patient monitoring; Product design; Accurate prediction; Activation functions; Calculation protocols; Clinical decision support systems; Data mining methods; Health condition; Piecewise polynomials; Privacy preserving; Distributed computer systems",2-s2.0-85016064811
"Mehrotra L., Saxena P.S.","An assessment report on: Statistics-based and signature-based intrusion detection techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031727857&doi=10.1007%2f978-981-10-5508-9_31&partnerID=40&md5=a59a5a20e941147ca6dcd8f531792651","With the growing size of data, its security has become a great challenge, and security of data is a major issue in most of the research areas. A detailed study of existing IDS is presented in the current paper so as to detect threats or intrusions on the data residing on system/network. It is a bit difficult to stop security threats and breaches entirely using present security technologies. Detecting the presence of intruder is very crucial for maintaining the network security. It is found that intrusion detection systems (IDSs) that are signature-based are restricted in their areas of detecting intrusions, because of the fact that the signature-based intrusion detection system is based on matching a signature with the network details. The system using signatures or patterns can detect only known attacks and threats, but they mostly fail when it comes to novel attacks.﻿ Thus preventing/detecting the new or special types of attracts whose signature is not specified. Although signature-based IDS does not give false alarms at genuine cases, but still is inept for unknown attacks or masked attacks. Later in the paper, another category of IDS is discussed which is statistical-based intrusion detection system (SBIDS). The statistical-based intrusion detection systems have an upper hand when it is compared with the signature-based intrusion detection system. During the study, it has been found that many researchers have solved this problem by data mining classification algorithms. © Springer Nature Singapore Pte Ltd. 2018.","Data mining; HIDS; IDS; NIDS; SBIDS","Computer crime; Data mining; Intrusion detection; Mercury (metal); Security of data; Security systems; Data mining classification algorithms; HIDS; Intrusion Detection Systems; NIDS; SBIDS; Security technology; Security threats; Unknown attacks; Network security",2-s2.0-85031727857
"Długosz M.M., Kurzydło W.","Anatomy trains modelling based on photogrammetric data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028719367&doi=10.1007%2f978-3-319-66905-2_23&partnerID=40&md5=03cf14f4a91745247ff009b4b24cd8b5","The research undertakes the issue of objectification of the human body posture (HBP) assessment, for the clinical practice. There are no tools to support physicians in HPB assessment. In our study models of the four major Anatomy Trains (AT) and a method of studying them were developed, to enable an objective assessment of HBP. To this end, the photogrammetric measurement results obtained using a Photogrammetrical Body Explorer system were used. In a healthy person, all AT are balanced, and maintain the correct posture. The occurrence of a specific AT is indicated by shortenings, which result from excessive tension of the AT, or a part thereof. Therefore, standards for the healthy individual parameters chosen to describe the AT were developed. Deviations from the designated standards testify to the occurrence of shortening on the corresponding section of one of the two cooperating AT. Created models of AT became the basis for the development of rules for a decision support system for physiotherapists. The system is based on fuzzy logic. Its main task is to identify which of the considered AT is dominant for a specific patient. The idea and the gathered experience can be the starting point for further research and development in the field of using data mining and artificial intelligence methods in supporting medical diagnosis based on the assessment of the HBP. © 2018, Springer International Publishing AG.","Anatomy trains; Assessment of the human body posture; Expert system; Myofascial chains","Artificial intelligence; Biocybernetics; Biomedical engineering; Biophysics; Data mining; Decision support systems; Diagnosis; Expert systems; Fuzzy logic; Photogrammetry; Anatomy trains; Artificial intelligence methods; Clinical practices; Healthy individuals; Human body postures; Objective assessment; Photogrammetric measurements; Research and development; Medical computing",2-s2.0-85028719367
"Kamiran F., Mansha S., Karim A., Zhang X.","Exploiting reject option in classification for social discrimination control",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376541&doi=10.1016%2fj.ins.2017.09.064&partnerID=40&md5=8564aac182aeca8e3baa1b6832f5737e","Social discrimination is said to occur when an unfavorable decision for an individual is influenced by her membership to certain protected groups such as females and minority ethnic groups. Such discriminatory decisions often exist in historical data. Despite recent works in discrimination-aware data mining, there remains the need for robust, yet easily usable, methods for discrimination control. In this paper, we utilize reject option in classification, a general decision theoretic framework for handling instances whose labels are uncertain, for modeling and controlling discriminatory decisions. Specifically, this framework permits a formal treatment of the intuition that instances close to the decision boundary are more likely to be discriminated in a dataset. Based on this framework, we present three different solutions for discrimination-aware classification. The first solution invokes probabilistic rejection in single or multiple probabilistic classifiers while the second solution relies upon ensemble rejection in classifier ensembles. The third solution integrates one of the first two solutions with situation testing which is a procedure commonly used in the court of law. All solutions are easy to use and provide strong justifications for the decisions. We evaluate our solutions extensively on four real-world datasets and compare their performances with previously proposed discrimination-aware classifiers. The results demonstrate the superiority of our solutions in terms of both performance and flexibility of applicability. In particular, our solutions are effective at removing illegal discrimination from the predictions. © 2017","Classification; Decision theory; Discrimination-aware data mining; Fairness in machine learning","Data mining; Decision theory; Learning systems; All solutions; Classifier ensembles; Decision boundary; Decision-theoretic; Ethnic groups; Historical data; Probabilistic classifiers; Real-world datasets; Classification (of information)",2-s2.0-85032376541
"Loher P., Telonis A.G., Rigoutsos I.","Accurate profiling and quantification of tRNA fragments from RNA-Seq Data: A Vade Mecum for MINTmap",2018,"Methods in Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031492250&doi=10.1007%2f978-1-4939-7339-2_16&partnerID=40&md5=b351203a656685f2d21a8dbdce7b232a","There is an increasing interest within the scientific community in identifying tRNA-derived fragments (tRFs) and elucidating the roles they play in the cell. Such endeavors can be greatly facilitated by mining the numerous datasets from many cellular contexts that exist publicly. However, the standard mapping tools cannot be used for the purpose. Several factors complicate this endeavor including: the presence of multiple identical or nearly identical isodecoders at various genomic locations; the presence of identical sequence segments that are shared by isodecoders of the same or even different anticodons; the existence of numerous partial tRNA sequences across the genome; the existence of hundreds of “lookalike” sequences that resemble true tRNAs; and others. This is generating a need for specialized tools that can mine deep sequencing data to identify and quantify tRFs. We discuss the various complicating factors and their ramifications, and how to use and run MINTmap, a tool that addresses these considerations. © 2018, Springer Science+Business Media LLC.","3′-halves; 5′-halves; 5′-tRFs; i-tRFs; internal tRFs; MINTbase; MINTcodes; MINTmap; MINTsubmit; Transfer RNA; tRF license plate; tRFs; tRNA; tRNA-derived fragments; tRNA-lookalikes","transfer RNA; accuracy; anticodon; data mining; genome; genomics; quantitative analysis; RNA sequence",2-s2.0-85031492250
"Peterkova A., Michalconok G., Bohm A.","Overview and Comparison of Machine Learning Methods to Build Classification Model for Prediction of Categorical Outcome Based on Medical Data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029593565&doi=10.1007%2f978-3-319-67618-0_20&partnerID=40&md5=a89122151109291f09dbe0f9d6e0c803","In this paper a classification model is proposed to predict a future state of patient’s cardiac diagnosis based on a large amount of medical data. The methodology of building a prediction model can be applied also to the other areas, such as industrial processes. In our research, we focus on cardiologic datasets of selected patients who were indicated for the ischemic heart disease. The selected sample of patients is divided into four stages of clinical diagnosis. Some of the parameters have a significant impact on the probability of the occurrence of the myocardial infraction. For building a classification model to predict categorical class output was used STATISTICA 13 software. © 2018, Springer International Publishing AG.","Classification model; Clinical dataset; Data mining","Classification (of information); Computational methods; Data mining; Diagnosis; Forecasting; Intelligent systems; Learning systems; Classification models; Clinical dataset; Clinical diagnosis; Industrial processs; Ischemic heart disease; Machine learning methods; Myocardial infraction; Prediction model; Computer aided diagnosis",2-s2.0-85029593565
"Nguyen T.-L., Vo B., Huynh B., Snasel V., Nguyen L.T.T.","Constraint-based method for mining colossal patterns in high dimensional databases",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029511563&doi=10.1007%2f978-3-319-67220-5_18&partnerID=40&md5=9b802e82ac33da4746814395e88a77a0","Constraint-based methods for mining patterns have been developed in recent years. They are based on top-down manner to prune candidate patterns. However, for colossal pattern mining, bottom-up manners are efficient methods, so the previous approaches for pruning candidate patterns based on top-down manner cannot apply to colossal pattern mining with constraint when using bottom-up manner. In this paper, we state the problem of mining colossal pattern with pattern constraints. Next, we develop a theorem for efficient pruning candidate patterns with bottom-up manner. Finally, we propose an efficient algorithm for mining colossal patterns with pattern constraints based on this theorem. © 2018, Springer International Publishing AG.","Bottom up; Colossal patterns; Data mining; High dimensional databases; Itemset constraint",,2-s2.0-85029511563
"Sureshan S., Penumacha A., Jain S., Vanahalli M., Patil N.","Mining closed colossal frequent patterns from high-dimensional dataset: Serial versus parallel framework",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026763613&doi=10.1007%2f978-981-10-3373-5_32&partnerID=40&md5=169861e24d16a1b44d82a3eed6105f97","Mining colossal patterns is one of the budding fields with a lot of applications, especially in the field of bioinformatics and genetics. Gene sequences contain inherent information. Mining colossal patterns in such sequences can further help in their study and improve prediction accuracy. The increase in average transaction length reduces the efficiency and effectiveness of existing closed frequent pattern mining algorithm. The traditional algorithms expend most of the running time in mining huge amount of minute and midsize patterns which do not enclose valuable information. The recent research focused on mining large cardinality patterns called as colossal patterns which possess valuable information. A novel parallel algorithm has been proposed to extract the closed colossal frequent patterns from high-dimensional datasets. The algorithm has been implemented on Hadoop framework to exploit its inherent distributed parallelism using MapReduce programming model. The experiment results highlight that the proposed parallel algorithm on Hadoop framework gives an efficient performance in terms of execution time compared to the existing algorithms. © Springer Nature Singapore Pte Ltd. 2018.","Closed colossal frequent patterns; Closed patterns; Frequent patterns; Hadoop; High-dimensional datasets; MapReduce; Minimum support","Computation theory; Intelligent computing; Parallel algorithms; Closed colossal frequent patterns; Closed pattern; Frequent patterns; Hadoop; High dimensional datasets; Map-reduce; Minimum support; Data mining",2-s2.0-85026763613
"Priyanka G., Darshana P., Radhika K.","Privacy-preserving associative classification",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028393017&doi=10.1007%2f978-3-319-63645-0_27&partnerID=40&md5=37bdeaba951977e56216291ed871970e","The massive amount of data, if publicly available, can be stored and shared securely for analysis and advancement. Mining of association rule besides classification technique is skilled of discovering useful patterns from big datasets. This technique results in the if-then form of rules and these rules are simple for end users to understand and easy for prediction. But it is apparent that the gathering and analysis of such data causes a serious menace to confidentiality and freedom. Hence, it interprets a field of privacy-preservation of data mining, which deals with efficient conduction and application of data mining without scarifying the privacy of data. This paper puts effort on the construction of class association rules generated by associative classification and applying privacy-preserving techniques on these rules to prevent its disclosure to the uncertified population. © Springer International Publishing AG 2018.","Associative classification; Classification; Data mining; Privacy-preservation","Association rules; Big data; Classification (of information); Data mining; Intelligent systems; Associative classification; Class association rules; Classification technique; End users; Mining of association rules; Privacy preservation; Privacy preserving; Useful patterns; Data privacy",2-s2.0-85028393017
"Wu T.-Y., Lin J.C.-W., Ren S.","Efficient mining of high average-utility itemsets with multiple thresholds",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026752837&doi=10.1007%2f978-3-319-63856-0_25&partnerID=40&md5=e95cb2f0b553fc43053995444662636c","In this paper, we propose an efficient algorithm to discover HAUIs based on the compact average-utility list structure. A tighter upper-bound model is used to instead of the traditional auub model used in HAUIM to lower the upper-bound value. Three pruning strategies are also respectively developed to facilitate mining performance of HAUIM. Experiments show that the proposed algorithm outperforms the state-of-the-art HAUIM-MMAU algorithm in terms of runtime and memory usage. © Springer International Publishing AG 2018.","Data mining; High average-utility itemsets; List structure; Multiple thresholds","Data mining; Signal processing; Average utilities; Item sets; List structures; Multiple threshold; Pruning strategy; Runtime and memory usage; State of the art; Upper bound models; Multimedia signal processing",2-s2.0-85026752837
"Park A., Conway M., Chen A.T.","Examining thematic similarity, difference, and membership in three online mental health communities from reddit: A text mining and visualization approach",2018,"Computers in Human Behavior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029859953&doi=10.1016%2fj.chb.2017.09.001&partnerID=40&md5=2caece11041fbd2d761de9ee539c6a75","Objectives Social media, including online health communities, have become popular platforms for individuals to discuss health challenges and exchange social support with others. These platforms can provide support for individuals who are concerned about social stigma and discrimination associated with their illness. Although mental health conditions can share similar symptoms and even co-occur, the extent to which discussion topics in online mental health communities are similar, different, or overlapping is unknown. Discovering the topical similarities and differences could potentially inform the design of related mental health communities and patient education programs. This study employs text mining, qualitative analysis, and visualization techniques to compare discussion topics in publicly accessible online mental health communities for three conditions: Anxiety, Depression and Post-Traumatic Stress Disorder. Methods First, online discussion content for the three conditions was collected from three Reddit communities (r/Anxiety, r/Depression, and r/PTSD). Second, content was pre-processed, and then clustered using the k-means algorithm to identify themes that were commonly discussed by members. Third, we qualitatively examined the common themes to better understand them as well as their similarities and differences. Fourth, we employed multiple visualization techniques to form a deeper understanding of the relationships among the identified themes for the three mental health conditions. Results The three mental health communities shared four themes: sharing of positive emotion, gratitude for receiving emotional support, and sleep- and work-related issues. Depression clusters tended to focus on self-expressed contextual aspects of depression, whereas the Anxiety Disorders and Post-Traumatic Stress Disorder clusters addressed more treatment- and medication-related issues. Visualizations showed that discussion topics from the Anxiety Disorders and Post-Traumatic Stress Disorder subreddits shared more similarities to one another than to the depression subreddit. Conclusions We observed that the members of the three communities shared several overlapping concerns (i.e., sleep- and work-related problems) and discussion patterns (i.e., sharing of positive emotion and showing gratitude for receiving emotional support). We also highlighted that the discussions from the r/Anxiety and r/PTSD communities were more similar to one another than to discussions from the r/Depression community. The r/Anxiety and r/PTSD subreddit members are more likely to be individuals whose experiences with a condition are long-term, and who are interested in treatments and medications. The r/Depression subreddit members may be a comparatively diffuse group, many of whom are dealing with transient issues that cause depressed mood. The findings from this study could be used to inform the design of online mental health communities and patient education programs for these conditions. Moreover, we suggest that researchers employ multiple methods to fully understand the subtle differences when comparing similar discussions from online health communities. © 2017","Anxiety disorders; Consumer health information; Consumer health information; Depression; Post-traumatic; Stress disorders; Unsupervised machine learning","Data mining; Education; Learning systems; Social networking (online); Visualization; Anxiety disorders; Consumer health information; Depression; Post-traumatic; Stress disorders; Unsupervised machine learning; Health",2-s2.0-85029859953
"Anuradha T.","Parallel mining of frequent itemsets from memory-mapped files",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402727&doi=10.1007%2f978-981-10-6620-7_43&partnerID=40&md5=71cc6d4b121e0690e57f5123964c4f17","Due to digitization of data in different fields, data are increasing in leaps and bounds. Mining of these large amounts of data requires two major issues to deal with. The first is the potential to deal with huge data which can be dealt with parallel algorithms as serial algorithms may take very long time or sometimes may not process. The second is the I/O overhead which can be dealt with memory mapping of files. This chapter brings together both parallelization and memory mapping of files concepts in mining the frequent itemsets. Our experiments proved that there is almost 20% more speedup on parallelizing our frequent itemset mining algorithm with memory mapping when compared to conventional I/O without memory mapping. © 2018, Springer Nature Singapore Pte Ltd.","Apriori; Frequent itemset; Memory-mapped file; Parallel mining","Mapping; Apriori; Frequent itemset; Frequent itemset mining; Large amounts of data; Memory-Mapped file; Parallel minings; Parallelizations; Serial algorithms; Big data",2-s2.0-85031402727
"Chern-Tong H., Aziz I.A.","A Performance Evaluation of Chi-Square Pruning Techniques in Class Association Rules Optimization",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029598698&doi=10.1007%2f978-3-319-67621-0_18&partnerID=40&md5=68d8481b92f9b67593940bb6869e206e","Associative classification is recognized by its high accuracy and strong flexibility in managing unstructured data. However, the performance is still induced by low quality dataset which comprises of noised and distorted data during data collection. The noisy data affected support value of an itemset and so it influenced the performance of an associative classification. The performance of associative classification is relied on the classification where the classification is worked based on the class association rules which generated from frequent rule mining process. To optimize the frequent itemsets based on the support value, in this research, we proposed a new optimization pruning technique to prune decision tree according to the correlation of each decision tree branches using genetic algorithm. © 2018, Springer International Publishing AG.","Association rules mining; Associative classification; Data mining; Decision tree; Genetic algorithm; Pruning","Association rules; Computational methods; Data acquisition; Decision trees; Genetic algorithms; Optimization; Trees (mathematics); Association rules mining; Associative classification; Class association rules; Data collection; Pruning; Pruning techniques; Support value; Unstructured data; Data mining",2-s2.0-85029598698
"Nithin Y.R., Poornalatha G.","Feature based opinion mining for restaurant reviews",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030157594&doi=10.1007%2f978-3-319-67934-1_27&partnerID=40&md5=be9f87b3df11e9cf38afb0cccb66a78c","Product reviews or customer feedback has become a platform for retailers to plan marketing strategy and also for new customers to select their appropriate product. Since the trend of e-commerce is increasing, an amount of customer reviews also has been increased to a greater extent. Consequently, it becomes a tough task for retailers as well as customers to read the reviews associated with the product. Sentiment analysis resolves this issue by scanning through free text reviews and providing the opinion summary. However, it does not provide detailed information, such as features on which the product is reviewed. Feature-based sentiment analysis methods increases the granularity of sentiment analysis by analyzing polarity associated with features in the given free text. The main objective of this work is to design a system that predicts polarity at aspect level and to design a score calculating scheme that defines the extent of polarity. Obtained feature - level scores are summarized according to users’ priority of interest. © Springer International Publishing AG 2018.","Aspects; Free text; Natural language processing; Reviews; Star rating","Customer satisfaction; Data mining; Marketing; Natural language processing systems; Reviews; Sales; Aspects; Customer feedback; Customer review; Free texts; Marketing strategy; Restaurant reviews; Sentiment analysis; Star ratings; Signal processing",2-s2.0-85030157594
"Srivastava S.","Novel method for predicting academic performance of students by using modified particle swarm optimization (PSO)",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031431276&doi=10.1007%2f978-981-10-6747-1_21&partnerID=40&md5=86ce303aadc6355e616a31f3d2fb9a6f","There are numerous methods for extracting useful information from data. This paper describes a method for predicting performance of students. This method modifies the basic particle swarm optimization (PSO) algorithm using a set of rules. An attribute is selected from a set of performance attributes of the students. This attribute is used to frame rules. These rules determine the value of a modifying factor. This factor changes the mathematical expression of the function used in PSO for finding the solution. These rules are based on number of students in a particular shift. Other attributes are assigned different indexes. These indexes indicate number of students deviating from average value. The modified PSO algorithm takes the values of these indexes as inputs and generates a solution set which minimizes the values of indexes. A comparison of the solution set given by modified PSO and the solution set with unmodified PSO is presented. A brief outline of the modified PSO is given. The selection of the modifying factor and design of rules is described. These rules are based on the number of students in a particular shift. The different possible classes for the shift attribute are given. Thus, a decision strategy for predicting performance is described. © 2018, Springer Nature Singapore Pte Ltd.","Modified PSO; Performance attributes; Rules, knowledge discovery, data mining","Data mining; Forecasting; Functions; Optimization; Students; Mathematical expressions; Modified particle swarm optimization; Modified pso; Modified pso algorithms; Modifying factors; Particle swarm optimization algorithm; Performance attributes; Predicting academic performance; Particle swarm optimization (PSO)",2-s2.0-85031431276
"Muro N., Larburu N., Bouaud J., Belloso J., Cajaraville G., Urruticoechea A., Séroussi B.","Augmenting guideline knowledge with non-compliant clinical decisions: Experience-based decision support",2018,"Smart Innovation, Systems and Technologies",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019702600&doi=10.1007%2f978-3-319-59397-5_23&partnerID=40&md5=b2bd5a6d831d0ed00be05444ec2ce05e","Guideline-based clinical decision support systems (CDSSs) are expected to improve the quality of care by providing best evidence-based recommendations. However, because clinical practice guidelines (CPGs) may be incomplete and often lag behind the publication time of very last scientific results, CDSSs may not provide up-to-date treatments. It happens that clinical decisions made for specific patients do not comply with CDSS recommendations, whereas they comply with the state of the art. They may also be non-compliant because they rely on some implicit knowledge not covered by CPGs. We propose to capitalize the clinical know-how built from such non-compliant decisions and allow physicians to use it in future similar cases by the development of a decisional event structure that allows the modelling, storage, processing, and reuse of all the information related to a decision-making process. This structure allows the analysis of non-compliant decisions, which generates new experience-based rules. These new rules augment the knowledge embedded in CPGs supporting clinician decision for specific patients poorly covered by CPGs. This work is applied to the management of breast cancer within the EU Horizon 2020 project DESIREE. © Springer International Publishing AG 2018.","Breast cancer; Clinical guidelines evolution; Data mining techniques; DESIREE; Experience-based clinical decision support system","Artificial intelligence; Data mining; Decision making; Digital storage; Diseases; Health care; Medical applications; Technology transfer; Breast Cancer; Clinical decision support systems; Clinical guideline; Clinical practice guidelines; Decision making process; DESIREE; Implicit knowledge; Scientific results; Decision support systems",2-s2.0-85019702600
"Chon K.-W., Kim M.-S.","SSDMiner: A Scalable and Fast Disk-Based Frequent Pattern Miner",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032467262&doi=10.1007%2f978-981-10-6520-0_11&partnerID=40&md5=cf004b37b0012ee5f5f38bd21fa25fec","Frequent itemset mining is widely used as a fundamental data mining technique. Recently, there have been proposed a number of disk-based methods. However, the existing methods still do not have a good scalability due to large-scale intermediate data and non-trivial disk I/Os. We propose SSDMiner, a new fast and scalable disk-based method for frequent itemset mining that is based on Apriori-like method and has no intermediate data and small disk I/O overheads by exploiting SSD. We propose a concept of bitmap chunks for storing transactional database in disks and a fast support counting based on bitmap chunks. Through experiments, we demonstrate that SSDMiner has the enhanced scalability and the good performance similar to that in memory-based methods with robustness. © 2018, Springer Nature Singapore Pte Ltd.","Disk-based algorithm; Frequent pattern mining; Scalable algorithm; SSD","Database systems; Scalability; Disk I/O; Disk-based; Disk-based algorithm; Frequent itemset mining; Frequent pattern mining; Non-trivial; Scalable algorithms; Transactional database; Data mining",2-s2.0-85032467262
"Vidyavathi B.M., Neha D.","Prediction of crime trends using Mk-MC technique",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021236727&doi=10.1007%2f978-981-10-3223-3_40&partnerID=40&md5=f65d547ca86c76e5dbeb6241cceb2472","Day by day the quantum of data has been increasing not only in terms of user generated content in social media but also outside the social media, due to which the data has gone from scarce to superabundant that conveys new advantages to users. This explosion of data has made it difficult to handle and analyze huge datasets. Therefore, the techniques of Data Mining assist in exploring and analyzing enormous datasets and helps in discovering meaningful patterns. Clustering is one such task of Data Mining that gathers all the data and partitions it into various groups taking into account their similarity or closeness measure. Clustering in the field of Social Science is used in identification, analysis and detection of various crime patterns. This paper proposes the Modified k-means clustering technique which is applied on the fictitious crime data in order to identify various crime patterns or trends and make a variety of predictions from the analysis of different crime patterns. © Springer Nature Singapore Pte Ltd. 2018.","Data cleaning; K-means clustering; Modified k-means clustering; Pre-processing","Cleaning; Crime; Data mining; Intelligent computing; Social networking (online); Crime data; Data cleaning; K-means clustering; Modified k-means clustering; Pre-processing; Social media; User-generated content; Cluster analysis",2-s2.0-85021236727
"Agrawal R.","Integrated effect of nearest neighbors and distance measures in k-nn algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031411767&doi=10.1007%2f978-981-10-6620-7_74&partnerID=40&md5=fe57f0cb618373fa54fc7213dd12953f","Supervised learning or classification is the cornerstone of Data Mining. A well-known, simple, and effective algorithm for supervised classification is k-Nearest Neighbor (k-NN). A distance measure provides significant support in the process of classification and the correct choice of distance measure is the most influential process in the classification technique. Also, the choice of k in k-Nearest Neighbor algorithm plays an effective role in the accuracy of the classifier. The aim of this paper is to analyze the integrated effect of various distance measures on different values of k in k-Nearest Neighbor algorithm on different data sets taken from UCI machine learning repository. © 2018, Springer Nature Singapore Pte Ltd.","Cityblock; Classification; Cosine; Data sets; Distance measure; Euclidean; K-Nearest neighbor; Mahalanobis","Big data; Data mining; Learning algorithms; Learning systems; Motion compensation; Nearest neighbor search; Pattern recognition; Supervised learning; Cityblock; Cosine; Data sets; Distance measure; Euclidean; K-nearest neighbors; Mahalanobis; Classification (of information)",2-s2.0-85031411767
"Kanimozhi K.V., Venkatesan M.","A novel map-reduce based augmented clustering algorithm for big text datasets",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021221326&doi=10.1007%2f978-981-10-3223-3_41&partnerID=40&md5=cd4a538c9de839ce8eecb61d47e2f5d1","Text clustering is a well known technique for improving quality in information retrieval, In Today’s real world data is not organized in the essential manner for a precise mining, given a large unstructured text document collection it is essential to organize into clusters of related documents. It is a contemporary challenge to explore compact and meaning insights from large collections of the unstructured text documents. Although many frequent item mining algorithms have been discovered yet most do not scale for “Big Data” and also takes more processing time. This paper presents a high scalable speedy and efficient map reduce based augmented clustering algorithm based on bivariate n-gram frequent item to reduce high dimensionality and derive high quality clusters for Big Text documents and also the comparative analysis is shown for the sample text datasets with stop word removal the proposed algorithm performs better than without stop word removal. © Springer Nature Singapore Pte Ltd. 2018.","Clustering; Frequent item; Similarity; Text documents","Big data; Data mining; Intelligent computing; Clustering; Comparative analysis; Frequent item; Frequent item minings; High dimensionality; Similarity; Text document; Unstructured texts; Clustering algorithms",2-s2.0-85021221326
"Serrano E., del Pozo-Jiménez P., Suárez-Figueroa M.C., González-Pachón J., Bajo J., Gómez-Pérez A.","Predicting the risk of suffering chronic social exclusion with machine learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022188445&doi=10.1007%2f978-3-319-62410-5_16&partnerID=40&md5=a100b82cba6e8b506fa0ad42396abd17","The fight against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a common practice. This paper describes a data analysis of over 16K cases with over 60 predictors from the Spanish region of Castilla y León. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion. The paper is complemented with a responsive web available online that allows social workers to calculate the risk of a social exclusion case to become chronic through a smartphone. © Springer International Publishing AG 2018.","Data analysis; Data mining; Machine learning; Social exclusion; Social services","Artificial intelligence; Data handling; Data mining; Data reduction; Decision trees; Distributed computer systems; Forecasting; Health risks; Information analysis; Insurance; Learning systems; Health services; High-precision; Insurance companies; Logistic regressions; Random forests; Risk prediction models; Social exclusion; Social service; Education",2-s2.0-85022188445
"Olarte-Valentín R., Múgica-Vidal R., Sainz-García E., Alba-Elías F., Fernández-Robles L.","Analysis of the online interactions of students in the project management learning process",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028635521&doi=10.1007%2f978-3-319-67180-2_73&partnerID=40&md5=1b12e4a5816b14938412fbcd56c6eb51","This paper analyses the use of discussion forums by students that work together in the same project, and investigates their relationship with the final success of the project. Furthermore, the application of text mining techniques as a tool for identifying problems in the development of the learning experience is studied. Analyses were carried out using real data from students’ participation in project management and communication tools. The results show a strong positive correlation between the frequency of use of the discussion forums and the success of the project, as well as a low variability in the terminology used by the students. Despite the potential of text mining, the reduced use of vocabulary by the students makes the process of classifying messages into a complex problem. © 2018, Springer International Publishing AG.","Discussion forums; Project management; Project performance; Text mining","Data mining; E-learning; Education computing; Project management; Soft computing; Students; Communication tools; Discussion forum; Learning experiences; On-line interactions; Positive correlations; Project performance; Text mining; Text mining techniques; Education",2-s2.0-85028635521
"Desarkar A., Das A.","A smart air pollution analytics framework",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031719930&doi=10.1007%2f978-981-10-5508-9_19&partnerID=40&md5=39482ac0cc2e432dba43805eb60c5c11","Air pollution which is the worst environmental health risk across the world takes millions of lives every year both in developing and developed countries. These huge premature deaths happen due to long-term exposure to air pollutants as most of the cities do not meet the acceptable pollution level suggested by World Health Organization (WHO). So there is an urgent need to reduce the air pollution level across the globe. This paper proposes a state-of-the-art approach and proposes a layered air pollution reduction framework. The methodology of the proposed framework also suggests the action plans to reduce air pollution level with an innovative Rule Base and mining appropriate data from the huge dataset which is basically a data warehouse. It also discusses the expected outcome of the proposed framework beneficial to the citizens. © Springer Nature Singapore Pte Ltd. 2018.","Action plan; Air pollution; Data mining; Data warehouse; Knowledge discovery; Predictive analysis","Air pollution; Data mining; Data warehouses; Health risks; Knowledge based systems; Network function virtualization; Pollution; Predictive analytics; Action plan; Developed countries; Environmental health risks; Long term exposure; Pollution reduction; Premature death; State-of-the-art approach; World Health Organization; Pollution control",2-s2.0-85031719930
"Chaabani Y., Toujani R., Akaichi J.","Sentiment analysis method for tracking touristics reviews in social media network",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020455910&doi=10.1007%2f978-3-319-59480-4_30&partnerID=40&md5=84c3edabe6c576f90215bf2ac10758be","The touristic sector in Tunisia has declined after the “Arabic Spring”. Therefore, the number of comments published by tourists to give their opinions about it has increased. Consequently, this resulted in a high volume of data in the different social networks such as Facebook and Twitter. In this case, the opinion mining plays an important role to more understanding and then ameliorating the situation of tourism in Tunisia. In this paper, the main goal is to select the tourists’ viewpoints in Twitter after the revolution. For this reason, we create a sentiment lexicon based on the emoticons and interjections as well as acronyms. We also use a sentiWordnet to build lexical scales for sentiment analysis of different tourist reviews with reference to a travel agency page on Facebook. Then, we propose a method relying on Support Vector Machine (SVM), Maximum entropy and Naive Bayes. Our approach is efficient as it gives encouraging results. © Springer International Publishing AG 2018.","Machine learning; Medias networks; NP-Complete; Reviews; Sentiment analysis; Text mining","Data mining; Interactive computer systems; Learning systems; Multimedia services; Multimedia systems; Natural language processing systems; Reviews; Social networking (online); Support vector machines; High volumes; NP Complete; Opinion mining; Sentiment analysis; Sentiment lexicons; Social media networks; Text mining; Travel agency; Maximum entropy methods",2-s2.0-85020455910
"Sebu M.L., Ciocarlie H.","Collaborative business process solution considering an ontological dimension of process models",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031398186&doi=10.1007%2f978-3-319-62524-9_14&partnerID=40&md5=6a530aeb5f49b9e16c65084124f6a387","Considering the potential of collaborative business as a trend, we start from the assumption that collaborations between companies have a higher success rate if a good compatibility from process perspective is detected. The solution is based on the business processes used internally in all participating organizations to collaborations with common business objectives. For un-standardized organizations, we used process mining techniques to formalize processes and to attach them to business scenarios. The process models are reduced to graph format and graph comparison algorithms are used to compute a compatibility factor. Syntactic similarity is unfeasible for real business process comparison in an un-standardized environment. Semantic similarity is resource consuming and could lead to un-predictable results. We propose in the current paper a methodology based on domain specific ontology to calculate the similarity between organizations from process perspective and to offer a collaborative solution, business process based. © Springer International Publishing AG 2018.","Business process management; Ontology domain; Process mining; Similarity","Administrative data processing; Data mining; Enterprise resource management; Natural language processing systems; Semantics; Societies and institutions; Soft computing; Business objectives; Business process management; Collaborative business process; Domain-specific ontologies; Process mining; Semantic similarity; Similarity; Syntactic similarities; Ontology",2-s2.0-85031398186
"Ali N., Amer E., Zayed H.","Understanding medical text related to breast cancer: A review",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539660&doi=10.1007%2f978-3-319-64861-3_26&partnerID=40&md5=4c9af8f732c3774c15aea2d7edd3e4f1","Breast Cancer is a harmful disease that has caused millions of women deaths. There are a huge number of publications on breast cancer research which offers a good source of information. Identifying breast cancer biomarkers is not a trivial task. There are many approaches used to identify and extract the needed information more efficiently from structured/unstructured text, uncover relationships and hidden rules from the huge amount of information such as text mining, machine learning and data mining. This paper reviews some of research literature on breast cancer using these approaches. © 2018, Springer International Publishing AG.","Breast cancer; Data mining; NLP; Text mining",,2-s2.0-85029539660
"Wiercioch M.","Feature selection in texts",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019213117&doi=10.1007%2f978-3-319-59162-9_35&partnerID=40&md5=7a8b2df79eb5e7bf52ca3c03c577e1b4","Feature selection is used in many application areas relevant to expert and intelligent systems, such as machine learning, data mining, cheminformatics and natural language processing. In this study we propose methods for feature selection and features analysis based on Support Vector Machines (SVM) with linear kernels. We explore how these techniques can be used to obtain some interesting information for further exploration of text data. The results provide satisfactory observations which may lead to progress in feature selection field. © Springer International Publishing AG 2018.","Dimension reduction; Feature selection; Support vector machines; Text classification","Classification (of information); Data mining; Intelligent systems; Learning algorithms; Learning systems; Natural language processing systems; Support vector machines; Text processing; Application area; Cheminformatics; Dimension reduction; Interesting information; Linear kernel; NAtural language processing; Text classification; Text data; Feature extraction",2-s2.0-85019213117
"Li Z., Shang C., Shen Q.","Fuzzy Connected-Triple for Predicting Inter-variable Correlation",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029580543&doi=10.1007%2f978-3-319-66939-7_5&partnerID=40&md5=9cc5259dc69ac6126800bd1ff36d488a","Identifying relationship between attribute variables from different data sources is an emerging field in data mining. However, currently there seldom exist effective methods designed for this particular problem. In this paper, a novel approach for inter-variable correlation prediction is proposed through the employment of the concept of connected-triple, and implemented with fuzzy logic. By the use of link strength measurements and fuzzy inference, the job of detecting similar or related variables can be accomplished via examining the link relation patterns. Comparative experimental investigations are carried out, demonstrating the potential of the proposed work in generating acceptable predicted results, while involving only simple computations. © 2018, Springer International Publishing AG.","Connected-triple; Fuzzy inference; Link analysis","Artificial intelligence; Computation theory; Data mining; Fuzzy logic; Connected triples; Data-sources; Experimental investigations; Link analysis; Related variables; Fuzzy inference",2-s2.0-85029580543
"Lalwani A., Banerjee S., Kindo M.M., Ali S.Z.","An obscure method for clustering in android using k-medoid and apriori algorithm",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028382894&doi=10.1007%2f978-3-319-63673-3_9&partnerID=40&md5=fd90b19bb12863efc85a086bfbde810f","In today’s scenario, there is quick evolution in each field which contains majority and distinctive sorts of information. In order to differentiate sample data from the other, the amalgamation of data mining techniques with other useful algorithms is done. Android development is one of the major arena where there is tremendous need to execute these calculations. Combining frequent pattern calculation with clustering is extremely efficacious for android. In this paper the work is done in two levels, initial stage concentrates on generation of clusters and final stage deals with finding the frequent patterns. © 2018, Springer International Publishing AG.","Android; Clustering; Itemsets","Data mining; Intelligent systems; Metals; Android; Apriori algorithms; Clustering; Item sets; K-medoid; Sample data; Android (operating system)",2-s2.0-85028382894
"Fatima J., Arora D.","Classification approach to extract strongly liked and disliked features through online user opinions",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031427595&doi=10.1007%2f978-981-10-3773-3_13&partnerID=40&md5=f721842c8939bbf8d93c767e40e901ee","In recent years, with the advent of emergence and growth of various web technologies and paradigms, an exponential increase can be seen regarding its usage and applicability. This growth has impacted tremendously the way of managing and analysis of data generated on web and how it is being utilized for further planning of any large business organization by exploring different hidden patterns and associated knowledge. Nowadays, internet is being popularized among its users with different dimensions of way of expressing their opinions. As a result, there are various sources of information in form of large repositories is being generated all around, such as social networking sites, e-commerce sites (Amazon, Flipkart, etc.) and forums, etc., which is beneficial to the customers as well as the manufacturers. Feature-based opinion mining aims to produce a feature-based summary of reviews and classifying it as positive, negative, and neutral. In this paper, a method has been proposed to extract the strongly liked and disliked features of product based on customers’ online reviews. The Stanford POS tagger has been used to tag the sentences to extract information to identify the required features of any product. This work is implemented on Eclipse using JAVA. © Springer Nature Singapore Pte Ltd. 2018.","Feature-based opinion mining; Implicit features; Opinion mining","Computational linguistics; Websites; Classification approach; Exponential increase; Extract informations; Implicit features; Opinion mining; Social networking sites; Sources of informations; Web technologies; Data mining",2-s2.0-85031427595
"Kumar N., Kumar U.","Anomaly-based network intrusion detection: An outlier detection techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028582402&doi=10.1007%2f978-3-319-60618-7_26&partnerID=40&md5=6b7e8338ea8327e5e21fae4c954d9836","A robust Network Intrusion Detection System (NIDS) has become the need of today’s era. To provide a robust mechanism require to distinguish between normal and anomalous activities, outliers detection with the help of data mining, play an important role in detection and distinction of such activities in the midst of enhanced performance in detection of false alarm. Now day’s researchers focus on applying outlier detection techniques for anomaly detection because of its promising results in discover true attacks and in sinking false alarm rate. So this paper contributed a enhanced mechanism of outlier detection to enhance accuracy in intrusion detection by introducing Density based Outlier detection into Data Mining using Hamming Densities of a data point. Hamming density is k-nearest neighbour divided by Hamming-distance. Analyzed the outcomes of our proposed by doing experiment using UCI repository KDD Cup’99 Intrusion data-set on our simulator work and compare the result with other such existing algorithms like LOF, LOF′ and found more accuracy and increase in detecting the number of true positive alarm in our proposed work. © Springer International Publishing AG 2018.","Attack; Density based; Hamming distance; NIDS; Outlier","Alarm systems; Data handling; Data mining; Errors; Hamming distance; Mercury (metal); Nearest neighbor search; Pattern recognition; Soft computing; Statistics; Anomalous activity; Attack; Density-based; K-nearest neighbours; Network intrusion detection; NIDS; Outlier; Outliers detection; Intrusion detection",2-s2.0-85028582402
"Andrat H., Ansari N.","Analyzing game stickiness using clustering techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031427092&doi=10.1007%2f978-981-10-3773-3_63&partnerID=40&md5=e458da395358df73bc79ae24e28a40ac","The popularity of computer games has led to tremendous generation of gaming data. Such gaming data consists of gamer’s personal information along with the game genres played and the time spent by them on a particular game. This gaming data can be utilized by the gaming industry for the purpose of extracting the knowledge needed to monitor the stickiness of the games. The raw data related to computer games can be refined, which could provide game developers the number of the gamers attracted towards a particular game. If the count of the gamers for a specific game decreases as the time passes by, then game developers need to improve the game, in order to retain the gamers. As gaming industry adds to our country’s revenue to a great extent, certain technological advancements are required. Therefore, this study aims to use a data mining approach, i.e., clustering, for monitoring computer games stickiness. © Springer Nature Singapore Pte Ltd. 2018.","Clustering; Computer games; Data mining; DBSCAN; Gaming data; k-means","Data mining; Information dissemination; Clustering; Clustering techniques; DBSCAN; Gaming data; K-means; Personal information; Technological advancement; Time spent; Computer games",2-s2.0-85031427092
"Chen Y.-S., Chou J.C.-L., Huang H.-C.","Predicting earning per share with financial ratios",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031425769&doi=10.1007%2f978-981-10-3187-8_93&partnerID=40&md5=15213c755795ad87579eeff25ee0a489","Predicting EPS (earning per share) with financial ratios is of high practical value. In this study, we applied two methods of data mining to predict EPS and compared their performance. We used three classifiers to find rules for predicting EPS from financial ratios, and compared the difference of prediction performance with and without data discretization. The experimental data set was extracted from the online financial database of Taiwan Economic Journal (TEJ) and collected from the 2009–2013 financial statements of six different industries. 26 condition attributes were selected from financial statements of listed public companies. The decision attribute, EPS, was classified into two and three classes. The result shows that there are three key determinants: Operating income per share, Times interest earned, and Total assets growth rate, and the method of with data discretization is of better prediction accuracy. © Springer Nature Singapore Pte Ltd. 2018.","Classification; Data discretization; Data mining; EPS; Financial ratio","Classification (of information); Computation theory; Finance; Forecasting; Condition attributes; Data discretization; Decision attribute; Earning per shares; Financial ratios; Financial statements; Prediction accuracy; Prediction performance; Data mining",2-s2.0-85031425769
"Wang X., Zuo M., Song L.","A feature selection method based on information gain and BP neural network",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030834457&doi=10.1007%2f978-981-10-6496-8_3&partnerID=40&md5=6634e51afb5306b905d91af51a1f5d3c","Data mining and machine learning fields are facing with a great challenge of mass data with high dimensionality. Feature selection can contribute a lot to address this issue with the concept of reducing the number of features by eliminating the redundant and irrelevant ones while preserving the information of original features maximally. This paper analyzes and compares two common feature selection methods, then puts forward a novel method for feature selection based on information gain and BP neural network (IGBP). The experimental result shows that IGBP method can reduce the time cost and improve the accuracy of the model at the meantime. The scientificity and superiority of IGBP are demonstrated in this paper, making it an efficient approach to deal with high-dimensional data. © 2018, Springer Nature Singapore Pte Ltd.","BP neural network; Data mining; Feature selection; IGBP method; Information gain","Clustering algorithms; Feature extraction; Intelligent systems; Learning systems; Neural networks; BP neural networks; Common features; Feature selection methods; High dimensional data; High dimensionality; IGBP method; Information gain; Time cost; Data mining",2-s2.0-85030834457
"Shastri S.S., Nair P.C., Gupta D., Nayar R.C., Rao R., Ram A.","Breast cancer diagnosis and prognosis using machine learning techniques",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032663199&doi=10.1007%2f978-3-319-68385-0_28&partnerID=40&md5=ff402391fcb3375df38765307cd09c3c","Breast cancer is one of the major type of cancer which is the leading cause of death in women. The research work is carried out on the real data of patient records obtained from HealthCare Global Enterprises Ltd (HCG) hospitals. The work analyzes the four major class variables in the dataset, namely death, progression, recurrence and metastasis. The influence of the same 11 predictor variables is explored for each of the class. Various machine algorithms namely Support Vector Machine, Decision Tree, Multi-layer Perceptron and Naive Bayes have been explored for classification of the patient data into various classes. The imbalance in the data is handled using an over sampling technique. The contribution of various attributes in classifying the instances into different classes is also being explored. The model helps in predicting various factors and thus helps in early diagnosis in the breast cancer. © Springer International Publishing AG 2018.","Attribute ranking; Breast cancer; Data imbalance; Machine learning","Artificial intelligence; Data mining; Decision trees; Diagnosis; Diseases; Hospital data processing; Intelligent systems; Trees (mathematics); Attribute ranking; Breast Cancer; Breast cancer diagnosis; Data imbalance; Global Enterprises; Machine learning techniques; Multi layer perceptron; Predictor variables; Learning systems",2-s2.0-85032663199
"Leal F., González-Vélez H., Malheiro B., Burguillo J.C.","Semantic profiling and destination recommendation based on crowd-sourced tourist reviews",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022178258&doi=10.1007%2f978-3-319-62410-5_17&partnerID=40&md5=e6662da5e2b75e796f96fb1eae62d4a9","Nowadays tourists rely on technology for inspiration, research, booking, experiencing and sharing. Not only it provides access to endless sources of information, but has become an unbounded source of tourist-related data. In such crowd-sourced data-intensive scenario, we argue that new approaches are required to enrich current and new travelling experiences. This work, which supports the “dreaming stage”, proposes the automatic recommendation of personalised destinations based on textual reviews, i.e., a semantic content-based filter of crowd-sourced information. Our approach relies on Topic Modelling – to extract meaningful information from textual reviews – and Semantic Similarity – to identify relevant recommendations. Our main contribution is the processing of crowd-sourced tourism information employing data mining techniques in order to automatically discover untapped destinations on behalf of tourists. © Springer International Publishing AG 2018.","Crowdsourcing; Profiling; Recommendation; Topic modelling; Tourism","Artificial intelligence; Crowdsourcing; Distributed computer systems; Filtration; Semantics; Data intensive; Destination recommendations; Profiling; Recommendation; Semantic content; Semantic similarity; Sources of informations; Tourism; Data mining",2-s2.0-85022178258
"Boto F., Lizuain Z., Cortadi A.J.","Intelligent maintenance for industrial processes, a case study on cold stamping",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028642214&doi=10.1007%2f978-3-319-67180-2_15&partnerID=40&md5=12c92d13c3f13d388f8727fa8823f81b","The correct diagnosis of tool breakage is fundamental to improve productivity, minimizing the number of unproductive hours and avoiding expensive repairs. The use of Data Mining techniques provides a significant added value in terms of improvements in the robustness, reliability and flexibility of the monitored systems. In this work, a general view of a diagnosis and prognosis of tool breakage in Industrial Processes is proposed. The important issues identified will be analyzed: filtering, process characterization and data based modeling. A case study has been implemented to carry out the prognosis of tool breakage in the cold stamping process. The results provided are qualitative trends and hypothesis to perform the prognosis. Although a validation in real operation is needed, these results are promising and demonstrate the goodness of using these type of techniques in real processes. © 2018, Springer International Publishing AG.","Cold stamping; Data mining; Fault diagnosis; Time series analysis","Failure analysis; Fault detection; Productivity; Soft computing; Stamping; Time series analysis; Cold stamping; Data based model; Diagnosis and prognosis; Industrial processs; Intelligent maintenance; Minimizing the number of; Monitored systems; Process characterization; Data mining",2-s2.0-85028642214
"Reda A., Fakharany E., Hazman M.","Early prediction of wheat diseases using SVM multiclass",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029532692&doi=10.1007%2f978-3-319-64861-3_24&partnerID=40&md5=85270fa931412aee5831e3a6419609ba","The early prediction of Plant diseases based on learning algorithms is one of promising research areas. Several types of classification techniques can be utilized on such data to early predict the different kinds of wheat diseases. However, the high dimension of the dataset in our case study and how selecting of the best data mining classifiers is one of the challenges. For that, Principle Component Analysis (PCA) technique was carried out for reducing the dimension by combining a set of correlated features as preprocessing step. Then, the Support Vector Machine (SVM) classifier with different multiclass techniques has been applied to predict of wheat diseases. The results have been combined with different voting methods in conjunction with PCA. The proposed system evaluated by several measurements and the classification accuracy reached to 96%. © 2018, Springer International Publishing AG.","Data mining; Data preprocessing; Principle component analysis; Support vector machine; Wheat diseases",,2-s2.0-85029532692
"Wu Y., Ke Y., Xu C., Xiao X., Hu Y.","Eco-efficiency measurement of coal-fired power plants in China using super efficiency data envelopment analysis",2018,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032278238&doi=10.1016%2fj.scs.2017.10.011&partnerID=40&md5=72a1ee74f9e0b37038ddbc86cc701aee","The measurement of coal-fired power plant eco-efficiency and identification of influencing variables are of great importance for the government to develop relative policy decisions. In this article, an improved two-stage analysis model is employed to analyze eco-efficiency of 58 Chinese coal-fired power plants: firstly, principal component analysis is selected for pre-treatment of variables in order to reduce dimensionality and distinguish prioritized factors; secondly, the super efficiency data envelopment analysis is chosen to assess eco-efficiency with overall ranking; thirdly, Kruskal-Wallis rank sum tests are adopted to figure out macro-environmental factors; finally, considering interactions in the coal power industry chain, plus time effect, we apply the Tobit regression to determine direct external factors. According to the result, over 60% of the coal-fired power plants work in an acceptable productive condition, while some still face improper investment problems. Plants with large installed capacity tend to stably operate. Then, macro-environmental factors like policy preference and economic situation can greatly affect local plants’ performance, while resource distribution has few impacts because of the perfect transport condition. In addition, the increasing thermal coal price help improve eco-efficiency while the high power unit age and feed-in tariff do harm to production performance. Based on the analysis, this paper also proposes some effective suggestions for the government. © 2017 Elsevier Ltd","Coal-fired power plant; Eco-efficiency analysis; External factors; Super efficiency data envelopment analysis; Tobit regression","Coal; Coal fueled furnaces; Coal industry; Data envelopment analysis; Efficiency; Factor analysis; Fire tube boilers; Investments; Mining; Principal component analysis; Coal-fired power plant; Eco-efficiency analysis; External factors; Super efficiency; Tobit regression; Fossil fuel power plants",2-s2.0-85032278238
"Hosseini B., Kiani K.","FWCMR: A scalable and robust fuzzy weighted clustering based on MapReduce with application to microarray gene expression",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028982337&doi=10.1016%2fj.eswa.2017.08.051&partnerID=40&md5=1ef4ea2c1bc4ff88072d26ab99dfe82f","Data clustering is a very useful data mining technique to find groups of similar objects present in the dataset. Scalability to handle immense volumes, robustness to intrinsic outlier data and validity of clustering results are the main challenges of any data clustering approach. In order to address these challenges, a fuzzy weighted clustering approach which is comprehensibly parallel and distributed in every phase, is proposed in this research. Although the proposed method can be used for various data clustering purposes, it has been applied in gene expression clustering to reveal functional relationships of genes in a biological process. Conforming to MapReduce, the proposed method also presents a novel similarity measure which benefits from combining ordered weighted averaging and Spearman correlation coefficient. In the proposed method, density reachable genes were joined to establish subclusters. Afterwards, final cluster results were obtained by merging these subclusters. A voting system detects the best weights and consequently the most valid clusters among all possible results for each distinct dataset. The whole algorithm is implemented on a distributed processing platform and it is scalable to process any size of data stored in cloud infrastructures. Precision of resulting clusters were evaluated using some of the well-known cluster validity indexes in the literature. Also, the efficiency of the proposed method in scalability and robustness was compared with recently published similar researches. In all the mentioned comparisons, the proposed method outperformed recent works on the same datasets. © 2017 Elsevier Ltd","Big data; Decision making; Distributed density based clustering; Fuzzy weighted clustering; Gene expression microarray; MapReduce","Big data; Cluster analysis; Clustering algorithms; Data mining; Decision making; Genes; Scalability; Voting machines; Density-based Clustering; Fuzzy weighted clustering; Gene expression clustering; Gene expression microarray; Map-reduce; Microarray gene expression; Ordered weighted averaging; Spearman correlation coefficients; Gene expression",2-s2.0-85028982337
"Setlak G., Bodyanskiy Y., Pliss I., Vynokurova O., Peleshko D., Kobylin I.","Adaptive fuzzy clustering of multivariate short time series with unevenly distributed observations based on matrix neuro-fuzzy self-organizing network",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029437434&doi=10.1007%2f978-3-319-66827-7_28&partnerID=40&md5=64ac3302b5b87d2d1b63d962c90d3be1","In the paper the method of fuzzy clustering task for multivariate short time series with unevenly distributed observations is proposed. Proposed method allows to process the time series both in batch mode and sequential on-line mode. In the first case we can use the matrix modification of fuzzy C-means method, and in second case we can use the matrix modification of neuro-fuzzy network by T. Kohonen, which is learned using the rule “Winner takes more”. Proposed fuzzy clustering algorithms are enough simple in computational implementation and can be used for solving of wide class of Big Data and Data Stream Mining problems. The effectiveness of proposed approach is confirmed by many experiments based on real data sets. © 2018, Springer International Publishing AG.","Adaptive fuzzy clustering; Matrix neuro-fuzzy self-organizing network; Multivariate short time series; Unevenly distributed observations","Big data; Clustering algorithms; Computation theory; Data mining; Fuzzy clustering; Fuzzy inference; Fuzzy neural networks; Fuzzy sets; Matrix algebra; Pattern matching; Time series; Time series analysis; Computational implementations; Fuzzy C means method; Matrix modification; Neuro-Fuzzy; Neuro-fuzzy network; Self-organizing network; Short time series; Unevenly distributed observations; Fuzzy logic",2-s2.0-85029437434
"Agarwal R., Singh S., Vats S.","Review of parallel apriori algorithm on mapreduce framework for performance enhancement",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410450&doi=10.1007%2f978-981-10-6620-7_38&partnerID=40&md5=71785ff38c80ffff5e91b3fffe84e638","Finding frequent itemsets in the large transactional database is considered as one of the most and significant issues in data mining. Apriori is one of the popular algorithms that widely used as a solution of addressing the same issue. However, it has computing power shortage to deal with large data sets. Various modified Apriori-like algorithms have been proposed to enhance the performance of traditional Apriori algorithm that works on distributed platform. Developing efficient and fast computing algorithm to handle large data sets becomes a challenging task due to load balancing, synchronisation and fault-tolerance issue. In order to overcome these problems, MapReduce model comes into existence, originally introduced by Google. MapReduce model-based parallel Apriori algorithm finds the frequent itemsets from large data sets using a large number of computers in distributed computational environment. In this paper, we mainly focused on parallel Apriori algorithm and its different versions based on approaches used to implement them. We also explored on current major open issues and extensions of MapReduce framework along with future research directions. © 2018, Springer Nature Singapore Pte Ltd.","Big data; Frequent itemsets; Hadoop; MapReduce; Parallel Apriori","Computer supported cooperative work; Data mining; Fault tolerance; Learning algorithms; Apriori; Computational environments; Future research directions; Hadoop; Item sets; Map-reduce; Performance enhancements; Transactional database; Big data",2-s2.0-85031410450
"Patel S., Suthar S., Patel S., Patel N., Patel A.","Topic detection and tracking in news articles",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028376449&doi=10.1007%2f978-3-319-63645-0_48&partnerID=40&md5=ca61a66b3af8c0d64fb47897b76c236b","We have presented an idea in this paper for detecting and tracking topics from news articles. Topic detection and tracking are used in text mining process. From data which are unstructured in text mining we pluck out information which are previously unknown. The objective of this paper is to recognize tasks occurred in different news sources. We are going to use agglomerative clustering based on average linkage for detecting the topics, calculate the similarity of topics using cosine similarity and KNN classifier for tracking the topics. © Springer International Publishing AG 2018.","Agglomerative; Article; Detecting; Extract; Information; KNN classifier; Similarity; Text mining; Tracking; Unstructured; Vector space model (VSM)","Classification (of information); Intelligent systems; Surface discharges; Vector spaces; Agglomerative; Article; Detecting; Extract; Information; K-NN classifier; Similarity; Text mining; Unstructured; Vector space models; Data mining",2-s2.0-85028376449
"Kumar V., Pujari A.K., Padmanabhan V., Sahu S.K., Kagita V.R.","Multi-label classification using hierarchical embedding",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029144279&doi=10.1016%2fj.eswa.2017.09.020&partnerID=40&md5=1c9b9d7f0cdc3e133f9054e6d0dafcef","Multi-label learning is concerned with the classification of data with multiple class labels. This is in contrast to the traditional classification problem where every data instance has a single label. Multi-label classification (MLC) is a major research area in the machine learning community and finds application in several domains such as computer vision, data mining and text classification. Due to the exponential size of the output space, exploiting intrinsic information in feature and label spaces has been the major thrust of research in recent years and use of parametrization and embedding have been the prime focus in MLC. Most of the existing methods learn a single linear parametrization using the entire training set and hence, fail to capture nonlinear intrinsic information in feature and label spaces. To overcome this, we propose a piecewise-linear embedding which uses maximum margin matrix factorization to model linear parametrization. We hypothesize that feature vectors which conform to similar embedding are similar in some sense. Combining the above concepts, we propose a novel hierarchical matrix factorization method for multi-label classification. Practical multi-label classification problems such as image annotation, text categorization and sentiment analysis can be directly solved by the proposed method. We compare our method with six well-known algorithms on twelve benchmark datasets. Our experimental analysis manifests the superiority of our proposed method over state-of-art algorithm for multi-label learning. © 2017 Elsevier Ltd","Label correlation; Matrix factorization; Multi-label learning","Data mining; Factorization; Learning algorithms; Learning systems; Matrix algebra; Piecewise linear techniques; Text processing; Classification of data; Hierarchical matrix factorizations; Label correlations; Linear parametrization; Machine learning communities; Matrix factorizations; Multi label classification; Multi-label learning; Classification (of information)",2-s2.0-85029144279
"Kim J., Kim D.","Personalized Information Visualization of Online Product Reviews",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032514459&doi=10.1007%2f978-981-10-6520-0_29&partnerID=40&md5=0e2d7a41a23240fdab0e36219392ae41","This paper presents a new method for visualizing online product reviews considering customer profiles. Typically, product review data are unstructured and have no fixed format or structure. The review data can be used by customers and also an e-business company. Potential consumers can acquire useful information on product characteristics and decide whether to buy or not depending on the review data. Also, the company can understand customers’ experiences or opinions on the product and reflect them in developing marketing strategies. In order to provide valuable information to the customers from enormous and unstructured review data, the process of collecting, storing, and preprocessing of review data should be performed firstly. And then text mining and personalization techniques can be integrated to extract properly visualized data. Thus, customers can utilize review data conveniently with the assistance of the proposed system. © 2018, Springer Nature Singapore Pte Ltd.","Information visualization; Personalization; Product review; Text mining","Information analysis; Information systems; Marketing; Sales; Visualization; Information visualization; Marketing strategy; Online product reviews; Personalizations; Personalized information; Product characteristics; Product reviews; Text mining; Data mining",2-s2.0-85032514459
"Wei Y.-C., Wu W.-C., Chu Y.-C.","Performance evaluation of information security risk identification",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031413711&doi=10.1007%2f978-981-10-3187-8_76&partnerID=40&md5=0bd83ec90b599a45fcd3f6f59ce82107","In recent decade, information security becomes a crucial issue on protecting the benefits of business operation. Many organizations perform information security risk management in order to analysis their weakness, and ensure the security of the business processes. However, identifying the threat-vulnerability pairs for each asset during the processes of risk assessment is both difficult and time-consuming for the risk assessor. Furthermore, if the identified results diverged from the real situation, the organization may put emphasis on the unnecessary controls to prevent the non-existing risk. In order to resolve the problem mentioned above, we utilize the data mining approach to discover the relationship between asset and threat-vulnerability pair. And then, we propose a risk recommendation system for assisting user identifying threat and vulnerability. The experiment result shows that the risk recommendation system can improve the performance of efficiency and accuracy of the risk assessment. We also develop a risk assessment system in order to collect the historical selection records and measure the elapsed time for further research. © Springer Nature Singapore Pte Ltd. 2018.","Risk recommendation; Security; Threat; Vulnerability","Computation theory; Data mining; Recommender systems; Risk management; Security of data; Assessment system; Business operation; Business Process; Information security risk managements; Information security risks; Security; Threat; Vulnerability; Risk assessment",2-s2.0-85031413711
"Eustáquio F., Camargo H., Rezende S., Nogueira T.","On fuzzy cluster validity indexes for high dimensional feature space",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029415048&doi=10.1007%2f978-3-319-66824-6_2&partnerID=40&md5=e5d601a872f1d76b5154776e9be26ff2","Fuzzy document clustering aims at automatically organizing related documents into clusters in a flexible way. At this context, the topics identification addressed by documents in every cluster is performed by automatically discovering cluster descriptors, which are relevant terms present in these documents. Since documents are represented by a high-dimensional feature space, the extraction of good descriptors is a big problem to be solved. This problem is even bigger using fuzzy clustering, since the same descriptor can be representative for more than one cluster. Moreover, it is well-known that the Fuzzy C-Means clustering algorithm is also affected by documents dimensionality and the choice of correct partition of a given document collection into clusters is still a challenging problem. In order to overcome this drawback, we have investigated the most common fuzzy clustering validity indexes to validate the organization of data with high dimensional feature space, since they are commonly used to evaluate fuzzy clusters from low dimensional data sets. © 2018, Springer International Publishing AG.","Documents; Flexible organization; Fuzzy clustering; Text mining; Validity indexes","Clustering algorithms; Data mining; Fuzzy clustering; Fuzzy sets; Pattern matching; Document Clustering; Documents; Fuzzy c-means clustering algorithms; High-dimensional feature space; Organization of datum; Text mining; Topics identifications; Validity index; Fuzzy logic",2-s2.0-85029415048
"Rahmani A.M., Gia T.N., Negash B., Anzanpour A., Azimi I., Jiang M., Liljeberg P.","Exploiting smart e-Health gateways at the edge of healthcare Internet-of-Things: A fog computing approach",2018,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012875280&doi=10.1016%2fj.future.2017.02.014&partnerID=40&md5=db88c41aaa12560ca3777c5d7a46a66b","Current developments in ICTs such as in Internet-of-Things (IoT) and Cyber–Physical Systems (CPS) allow us to develop healthcare solutions with more intelligent and prediction capabilities both for daily life (home/office) and in-hospitals. In most of IoT-based healthcare systems, especially at smart homes or hospitals, a bridging point (i.e., gateway) is needed between sensor infrastructure network and the Internet. The gateway at the edge of the network often just performs basic functions such as translating between the protocols used in the Internet and sensor networks. These gateways have beneficial knowledge and constructive control over both the sensor network and the data to be transmitted through the Internet. In this paper, we exploit the strategic position of such gateways at the edge of the network to offer several higher-level services such as local storage, real-time local data processing, embedded data mining, etc., presenting thus a Smart e-Health Gateway. We then propose to exploit the concept of Fog Computing in Healthcare IoT systems by forming a Geo-distributed intermediary layer of intelligence between sensor nodes and Cloud. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, our Fog-assisted system architecture can cope with many challenges in ubiquitous healthcare systems such as mobility, energy efficiency, scalability, and reliability issues. A successful implementation of Smart e-Health Gateways can enable massive deployment of ubiquitous health monitoring systems especially in clinical environments. We also present a prototype of a Smart e-Health Gateway called UT-GATE where some of the discussed higher-level features have been implemented. We also implement an IoT-based Early Warning Score (EWS) health monitoring to practically show the efficiency and relevance of our system on addressing a medical case study. Our proof-of-concept design demonstrates an IoT-based health monitoring system with enhanced overall system intelligence, energy efficiency, mobility, performance, interoperability, security, and reliability. © 2017 Elsevier B.V.","Edge/Fog computing; Healthcare; Home care; Internet of Things; Mobility; Sensor network; Smart gateway; Smart hospital","Automation; Carrier mobility; Computer circuits; Cyber Physical System; Data handling; Data mining; Digital storage; Distributed computer systems; Embedded systems; Energy efficiency; Fog; Health; Health care; Home networks; Hospitals; Intelligent buildings; Internet of things; Interoperability; Monitoring; Sensor networks; Sensor nodes; Cyber-physical systems (CPS); Edge/Fog computing; Health monitoring system; Home care; Internet of Things (IOT); Smart hospital; Ubiquitous health monitoring; Ubiquitous healthcare system; Gateways (computer networks)",2-s2.0-85012875280
"Sakouhi T., Akaichi J., Ahmed U.","Computing semantic trajectories: Methods and used techniques",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020422740&doi=10.1007%2f978-3-319-59480-4_39&partnerID=40&md5=3d7b2f68b40155cfad87f658e69b94b9","The widespread use of mobile devices generates huge amount of location data. The generated data is useful for many applications, including location-based services such as outdoor sports forums, routine prediction, location-based activity recognition and location-based social networking. Sharing individuals’ trajectories and annotating them with activities, for example a tourist transportation mode during his trip, helps bringing more semantics to the GPS data. Indeed, this provides a better understanding of the user trajectories, and then more interesting location-based services. To address this issue, diverse range of novel techniques in the literature are explored to enrich this data with semantic information, notably, machine learning and statistical algorithms. In this work, we focused, at a first level, on exploring and classifying the literature works related to semantic trajectory computation. Secondly, we capitalized and discussed the benefits and limitations of each approach. © Springer International Publishing AG 2018.","Activity recognition; Data mining; Machine learning; Mobility data; Ontology; Semantic modeling; Trajectory","Artificial intelligence; Data mining; Interactive computer systems; Learning systems; Location; Mobile devices; Mobile telecommunication systems; Multimedia services; Multimedia systems; Ontology; Pattern recognition; Semantics; Telecommunication services; Trajectories; Activity recognition; Mobility datum; Routine prediction; Semantic information; Semantic Model; Semantic trajectories; Statistical algorithm; Transportation mode; Location based services",2-s2.0-85020422740
"Li H., Dai Y., Jin X., Sun G., Li T., Xu Z.","Risk evaluation of financial websites based on structure mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032683991&doi=10.1007%2f978-3-319-67071-3_11&partnerID=40&md5=8f876d2b6f4c59cec6ea56c2e19cd4b9","With the development of network communication and security authentication technologies, Internet finance, a new financial business model which allows customers to achieve online financing, payment, investment and lending, becomes more and more popular. Risk of internet finance is much higher than that of traditional financial system because of the rapid fund flowing, lack of personal credit audit, deficiency of standard network operation and imperfect of information security. Current risk control of Internet finance mainly contains network security, financial self-discipline, investor education and governmental regulatory. In this paper, we propose a risk evaluation method based on structure mining for financial website after analyzing a large number of Internet financial sites. The kernel functions algorithm of natural language syntax tree is introduced to classify the security level of website. While the URL is considered as a long sentence and the path segments are defined as keywords. The experimental results demonstrate that the structure mining method can simply evaluate the risk of Internet financial website to achieve acceptable accuracy. © 2018, Springer International Publishing AG.","Internet finance; Natural language syntax tree; Risk evaluation; Structure mining","Finance; Investments; Mining; Security of data; Syntactics; Trees (mathematics); Websites; Business modeling; Financial system; Natural languages; Network communications; Network operations; Risk evaluation; Security authentication; Structure mining; Network security",2-s2.0-85032683991
"Ji W., Guo Q., Lei Y.","HM-AprioriAll algorithm improvement based on Hadoop environment",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020409236&doi=10.1007%2f978-3-319-60170-0_12&partnerID=40&md5=87faeda161f9a9e4ce338123ea1a8c4e","In order to improve the efficiency of the mining frequent item-sets of AprioriAll algorithm, the Hadoop environment and MapReduce model are introduced to improve AprioriAll algorithm, a new algorithm of mining frequent item-sets under the environment of big data HM-AprioriAll algorithm is designed. Compared with the original algorithm, the new algorithm introduces user attributes and pruning technology, which reduces the number of the elements in the candidate sets and reduces the number of the scanning times on the data sets, greatly reduces the time complexity and space complexity of computing, gives rules model in large scale. After testing HM-AprioriAll algorithm on Hadoop platform, the results prove that the insertion of this technology makes HM-AprioriAll algorithm have higher efficiency of expanding. © Springer International Publishing AG 2018.","AprioriAll algorithm; Big data; Hadoop; Map Reduce; Recommendation systems","Data mining; Efficiency; Recommender systems; AprioriAll algorithm; Frequent item sets; Hadoop; Hadoop platforms; Higher efficiency; Map-reduce; Original algorithms; Space complexity; Big data",2-s2.0-85020409236
"Grzenda M., Kwasiborska K., Zaremba T.","Combining stream mining and neural networks for short term delay prediction",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028635540&doi=10.1007%2f978-3-319-67180-2_18&partnerID=40&md5=a2e8507ecff94eca2a9f00830ba78930","The systems monitoring the location of public transport vehicles rely on wireless transmission. The location readings from GPS-based devices are received with some latency caused by periodical data transmission and temporal problems preventing data transmission. This negatively affects identification of delayed vehicles. The primary objective of the work is to propose short term hybrid delay prediction method. The method relies on adaptive selection of Hoeffding trees, being stream classification technique and multilayer perceptrons. In this way, the hybrid method proposed in this study provides anytime predictions and eliminates the need to collect extensive training data before any predictions can be made. Moreover, the use of neural networks increases the accuracy of the predictions compared with the use of Hoeffding trees only. © 2018, Springer International Publishing AG.","Data stream classification; Hoeffding tree; IoT data streams; Multilayer perceptron","Classification (of information); Data communication systems; Data transfer; Forecasting; Forestry; Multilayer neural networks; Multilayers; Soft computing; Vehicle transmissions; Adaptive selection; Data stream; Data stream classifications; Hoeffding tree; Public transport vehicles; Short-term delays; Stream classification; Wireless transmissions; Trees (mathematics)",2-s2.0-85028635540
"Barot P.A., Jethva H.B.","Statistical study to prove importance of causal relationship extraction in rare class classification",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028376118&doi=10.1007%2f978-3-319-63673-3_51&partnerID=40&md5=4704a65b8bb109d58c167e3ec63e4058","Rare class classification is important technique in real-world domains like medical diagnosis, bioinformatics, detection of oil spills in satellite images, road accident analysis etc. Imbalanced data classification resides among the top research area of current time and attracted huge interest of researcher. For rare class classification most of researchers have concentrated their study on use of methods like sampling techniques, one-class learning and ensemble based methods. But these methods suffer by some drawbacks. Here we first present an overview of imbalanced data and rare class classification, and then extract unique causes of target class using association rule mining and show importance of causal relationship in determining target class of an instance. © 2018, Springer International Publishing AG.","Association rule; Imbalanced data; Rare class","Association rules; Computer aided diagnosis; Data mining; Diagnosis; Intelligent systems; Medical imaging; Oil spills; Causal relationships; Ensemble-based method; Imbalanced data; One-class learning; Rare class; Rare-class classification; Road accident analysis; Sampling technique; Classification (of information)",2-s2.0-85028376118
"Jadon M.K., Agarwal P., Nag A.","Pico-nym cloud (PNC): A method to devise and peruse semantically related biological patterns",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031400259&doi=10.1007%2f978-981-10-6747-1_17&partnerID=40&md5=9b945cc2cedd39154e8a7e6e98a948fb","Text mining works widely in the field of research techniques, which allow an individual to store text and its important terms in form of electronic document (.doc,.txt). Obliviously, one cannot remember such huge amount of text; moreover, the manual approach is more time-taking, unreliable, and accessible to that person only. Text mining techniques optimize this approach by extracting and storing this data. Computational comparison, file read, file write are more efficiently done. With the help of Pico-Nym Cloud (PNC), we generated more semantically similar, related, and significant patterns. The give, generate, and get sequence modeling is adopted. Over the other available Web applications, we present our application with improved stemming, relation, and average case consideration. This approach does not limit the displayed number of words as all the generated sets can be traversed with the GUI, with opted size of patterns. This PNC is highly applicable in bioinformatics, related information retrieval from document, sentimental analysis using social Web sites (Twitter and Facebook), query expansion (Google) and many more. © 2018, Springer Nature Singapore Pte Ltd.","Cosine similarity; Patterns; Stemming; Tag cloud; Text mining","Natural language processing systems; Search engines; Social networking (online); Word processing; Cosine similarity; Patterns; Stemming; Tag clouds; Text mining; Data mining",2-s2.0-85031400259
"Walkowiak T.","Language processing modelling notation – Orchestration of NLP microservices",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020785890&doi=10.1007%2f978-3-319-59415-6_44&partnerID=40&md5=a429f935ac40248556817e8d5b077409","The paper presents Language Processing Modelling Notation (LPMN). It is a formal language used to orchestrate a set of NLP microservices. The LPMN allows modeling and running complex workflows of language and machine learning tools. The scalability of the solution was achieved by a usage of message-oriented middleware. LPMN is used for developing text mining application with web-based interface and performing research experiments that requires a usage of NLP and machine learning tools. © Springer International Publishing AG 2018.","Microservices; Natural language processing; Orchestration; Text mining; Web-based application","Artificial intelligence; Data mining; Formal languages; Learning algorithms; Learning systems; Middleware; Multimedia systems; Natural language processing systems; Websites; Complex workflows; Language processing; Message oriented middleware; Microservices; Orchestration; Text mining; Web-based applications; Web-based interface; Modeling languages",2-s2.0-85020785890
"Harrison R.W., Freas C.","Fuzzy restricted boltzmann machines",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030696010&doi=10.1007%2f978-3-319-67137-6_43&partnerID=40&md5=47a9e7174e122867cc70865b277726b8","Restricted Boltzmann Machines are a reconstructive neural network. They derive an implicitly probabilistic model of data which can be used to reconstruct or filter missing data as well as to classify data. This paper develops a deterministic training algorithm and shows how to use that algorithm to automatically derive fuzzy membership classes. The algorithm developed in this paper combines many of the best features of fuzzy learning algorithms and Restricted Boltzmann machines. © Springer International Publishing AG 2018.","Data mining; Deep learning; Energy-based learning; Fuzzy machine learning; Restricted boltzmann machines","Classification (of information); Data mining; Deep learning; Filtration; Learning systems; Energy-based; Fuzzy membership; Missing data; Probabilistic modeling; Restricted boltzmann machine; Training algorithms; Learning algorithms",2-s2.0-85030696010
"Oswald C., Akshay Vyas V., Arun Kumar K., Vijay Sri L., Sivaselvan B.","Hierarchical clustering approach to text compression",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026769679&doi=10.1007%2f978-981-10-3373-5_35&partnerID=40&md5=4fbbd4e2f5babde7a8524e570f12e11e","A novel data compression perspective is explored in this paper and focus is given on a new text compression algorithm based on clustering technique in Data Mining. Huffman encoding is enhanced through clustering, a non-trivial phase in the field of Data Mining for lossless text compression. The seminal hierarchical clustering technique has been modified in such a way that optimal number of words (patterns which are sequence of characters with a space as suffix) are obtained. These patterns are employed in the encoding process of our algorithm instead of single character-based code assignment approach of conventional Huffman encoding. Our approach is built on an efficient cosine similarity measure, which maximizes the compression ratio. Simulation of our proposed technique over benchmark corpus clearly shows the gain in compression ratio and time of our proposed work in relation to conventional Huffman encoding. © Springer Nature Singapore Pte Ltd. 2018.","Compression ratio; Cosine similarity measure; Hierarchical clustering; Huffman encoding; Lossless compression","Cluster analysis; Compression ratio (machinery); Computation theory; Data mining; Encoding (symbols); Intelligent computing; Signal encoding; Clustering techniques; Code assignments; Cosine similarity measures; Hier-archical clustering; Hierarchical clustering approach; Huffman encoding; Lossless compression; Text compressions; Clustering algorithms",2-s2.0-85026769679
"Solanki N., Panchal G.","A novel machine learning based approach for rainfall prediction",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028368663&doi=10.1007%2f978-3-319-63673-3_38&partnerID=40&md5=145253f9acf270c1f88722c04ff2d651","The climate changes effortlessly nowadays, prediction of climate is very hard. However, the forecasting mechanism is the vital process. It is also a valuable thing as it is the important part of the human life. Accordingly to the research, the weather forecast of rainfall intensity conducted. The remarkable commitment of this proposal is in the implementation of a hybrid intelligent system data mining technique for solving novel practical problems, Hybrid Intelligent system data mining consists of the combination of Artificial Neural Network and the proper usage of Genetic Algorithm. In this research, Genetic algorithm is utilized the type of inputs, the connection structure between the inputs and the output layers and make the training of neural network more efficient. In ANN, Multi-layer Perceptron (MLP) serves as the center data mining (DM) engine in performing forecast tasks. Back Propagation algorithm used for the trained the neural network. During the training phase of the proposed approach, it gains the optimal values of the connection weights which, in fact, utilized as the part of the testing phase of the MLP. Here, the testing phase is used to bring about the rainfall prediction accuracy. It may be noted that the information/data is used to cover the information from the variables namely temperature, cloud fraction, wind, humidity, and rainfall. © 2018, Springer International Publishing AG.","Artificial neural network; Genetic algorithm; Rainfall prediction","Backpropagation; Backpropagation algorithms; Climate change; Data mining; Forecasting; Genetic algorithms; Intelligent systems; Learning algorithms; Learning systems; Neural networks; Rain; Connection structures; Connection weights; Hybrid intelligent system; Multi layer perceptron; Practical problems; Rainfall intensity; Rainfall prediction; Training phase; Weather forecasting",2-s2.0-85028368663
"Afanasieva T., Egorov Y., Savinov N.","About transformations of a numerical time series using a linguistic variable",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031418200&doi=10.1007%2f978-3-319-68321-8_23&partnerID=40&md5=c11c970e856404c51458c8f4b6cedb47","Time series transforming is considered as a preprocessing stage in various data mining techniques. To obtain relevant and accurate result in time series analysis it is needed to apply relevant time series representation by suitable transformation. In the paper at the first time five transformations of a numerical time series derived on the basis of a single linguistic variable of time series values are described systematically. The formal notion of five kinds (fuzzy matrix, fuzzy vectors, fuzzy linguistic, numerical and linguistic) of time series produced by these transformations and general scheme of their computing are represented. Applications of these five representations of a numerical time series in data mining techniques are given and discussed. © Springer International Publishing AG 2018.","Fuzzy time series; Linguistic variable of values; Time series representation; Time series transformation","Data mining; Linear transformations; Linguistics; Metadata; Time series; Fuzzy linguistics; Fuzzy matrix; Fuzzy time series; Fuzzy vectors; Linguistic variable; Time series transformations; Time series analysis",2-s2.0-85031418200
"Gupta R., Jivani A.G.","Analyzing the stemming paradigm",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028402595&doi=10.1007%2f978-3-319-63645-0_37&partnerID=40&md5=e1f337af2dd840201d9b85a14e6969e6","This paper discusses affix removal and statistical based Stemming algorithms in detail with stemmer-generated output from some Standard English text and dictionary. Comparative empirical studies of all these stemmers are also discussed here with respect to number of stem token generation from single root morphed word variants and computation time. First part of the paper deals with introductory discussion of stemming and lemmatization. Second part of the paper focuses on algorithms of affix and statistical based stemmers with their empirical output. Last part describes the steps of the comparative tool for the same. Finally conclusion section wraps up whole discussion about stemming. This paper can assist researchers working in the field of text mining. © Springer International Publishing AG 2018.","Index compression factor (ICF); Lemmatizing; Stemming; Text mining","Data mining; Intelligent systems; Text processing; Computation time; Empirical studies; Index compression; Lemmatization; Lemmatizing; Stemming; Stemming algorithms; Text mining; Natural language processing systems",2-s2.0-85028402595
"Wang X., Guo Y., Wang Z.","Multi-view discriminative manifold embedding for pattern classification",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028381667&doi=10.1007%2f978-3-319-60744-3_18&partnerID=40&md5=fb55249c9f6fcd16bae687abeb79901a","While many dimensionality reduction algorithms have been proposed in recent years, most of them are designed for single view data and cannot cope with multi-view data directly. Dimensionality reduction algorithms in recent ten years, both in theory and application have great breakthrough. In the face of dozens, hundreds or even thousands of dimension by dimension reduction to the data from high dimensional space to a low dimensional space and extract the essential characteristics of low dimensional data. In many real-world pattern applications such as face recognition, multiple feature descriptors can provide complementary information in characterizing image from different viewpoints. Motivated by this concern, we propose a new multi-view discriminative manifold embedding (MDME) method for classification by making use of intra-class geometry and inter-class marginal information as well as complementary information of multiple feature representations. Experimental results on face recognition demonstrate the effectiveness of the proposed algorithm. © 2018, Springer International Publishing AG.","Dimensionality reduction; Discriminative manifold embedding; Multi-view learning; Pattern classification","Classification (of information); Data mining; Face recognition; Intelligent systems; Pattern recognition; Real time systems; Dimensionality reduction; Dimensionality reduction algorithms; Discriminative manifold embedding; Essential characteristic; High dimensional spaces; Low-dimensional spaces; Multi-view learning; Multiple feature descriptors; Data reduction",2-s2.0-85028381667
"Sharma D.","Study of sentiment analysis using hadoop",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031424428&doi=10.1007%2f978-981-10-6620-7_35&partnerID=40&md5=89ddd40d565f2a809a17feb3e1f89067","In the current world of Internet people express themselves, present their views and feelings about specific topics or entities using various social media application. These posts from users present a huge opportunity for the organizations to increase their market value by analyzing the posts and using information in decision making. These posts can be studied using various machine learning and lexicon-based approaches for extracting its sentiments. With more and more people moving to internet, huge data is being produced every second and challenge is to store this large data and process it efficiently in real time to infer knowledge from this data. This paper presents different approaches for real-time and scalable ways of performing sentiment analysis using Hadoop in a time efficient manner. Hadoop and its component tools like MapReduce, Mahout, and Hive are being surveyed in different scholar articles for this paper. © 2018, Springer Nature Singapore Pte Ltd.","Hadoop; Hive; Mahout; MapReduce; Sentiment analysis; Twitter","Behavioral research; Data mining; Decision making; Learning systems; Social networking (online); Hadoop; Hive; Mahout; Map-reduce; Sentiment analysis; Twitter; Big data",2-s2.0-85031424428
"He Q., Zhou W., Xu H., Cui L., Li X., Liu J.","A Distributed Network Alarm Correlation Analysis Mechanism for Heterogeneous Networks",2018,"Journal of Circuits, Systems and Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020701261&doi=10.1142%2fS0218126618500123&partnerID=40&md5=1621c16a82b1520b84fd5cd644c09991","The current China State Grid network consists of a set of heterogeneous and distributed networks. In such a heterogeneous environment, it is a challenging task to provide efficient network alarm management, and further to analyze the correlation of alarms. Thus, an efficient and distributed network alarm analysis scheme is becoming necessary and indispensable. To this end, we have attached our emphasis on two aspects of the aforementioned problem and proposed the corresponding algorithms, respectively. Firstly, we introduced an intra-network alarm treatment process to manage the heterogeneous network alarms. Especially, we leverage on the fuzzy clustering method and fuse alarms within the same cluster into comprehensive alarms by utilizing the Dempster-Shafer theory. Secondly, we proposed an inter-network alarm analysis process to mine the correlation rules of alarms through a distributed scheme based on the Frequent Pattern-Growth (FP-Growth) association rule mining algorithm. Compared with the traditional centralized scheme and another Apriori-based distributed algorithm, our proposed scheme has a higher time efficiency for the effective management of network alarms. With the aid of such a two-step network alarm management scheme, it is easy for the network management system to make a better management of alarms in heterogeneous and distributed networks. © 2018 World Scientific Publishing Company.","alarm fusion; correlation analysis; distributed and heterogeneous network; Network alarms","Correlation methods; Data mining; Formal logic; Fuzzy clustering; Heterogeneous networks; Network management; Alarm correlation analysis; Correlation analysis; Effective management; Frequent pattern growth; Fuzzy clustering method; Heterogeneous environments; Network management systems; Rule mining algorithms; Alarm systems",2-s2.0-85020701261
"Monroy-Tenorio F., Batyrshin I., Gelbukh A., Solovyev V., Kubysheva N., Rudas I.","Correlation measures for bipolar rating profiles",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669431&doi=10.1007%2f978-3-319-67137-6_3&partnerID=40&md5=5148531bc4c84a8b441cdd4484aee28f","We introduce new correlation measures for measuring similarity and association of rating profiles obtained from bipolar rating scales. Instead of the measurement based approach when the user’s rating is considered as a number measured in ordinal, interval or ratio scales we use model based approach when user’s rating is modeled by bipolar score function that can be nonlinear. This approach can use different models of preferences for different users. The values of utility function can be adjusted in machine learning procedure to obtain better solutions on the output of recommender or decision making system. We show that Pearson’s correlation coefficient often used for measuring similarity between bipolar rating profiles in recommender systems has some drawbacks. New correlation measures proposed in the paper have not these drawbacks. These measures are obtained using general methods of construction of association measures from similarity measures on sets with involutive operation. Proposed measures can be used in recommender systems, in opinion mining and in sociological research for analysis of possible relationships between opinions of users and ratings of items. © Springer International Publishing AG 2018.","Association measure; Bipolar scale; Correlation; Opinion mining; Rating scale; Recommender system; Sentient analysis","Correlation methods; Data mining; Decision making; Learning systems; Recommender systems; Association measures; Bipolar scale; Opinion mining; Rating scale; Sentient analysis; Rating",2-s2.0-85030669431
"Mannai M., Karâa W.B.A., Ghezala H.H.B.","Information extraction approaches: A survey",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031746627&doi=10.1007%2f978-981-10-5508-9_28&partnerID=40&md5=8a75af5f9348849c8f7a9b53e692f49e","In the recent years, the amount of available information in the Web is growing. Thereby, the search of pertinent information through those large documents has become a difficult task. That’s why, we need to develop information extraction systems in order to facilitate the treatment and the representation of data according to the user’s need. These systems should adopt an extraction approach for its implementation. In this paper, we provide an overview of the basic information extraction (IE) approaches used in the developed systems. We survey a specific class of IE approaches based on semantics, due to the importance of semantic processing of the data. © Springer Nature Singapore Pte Ltd. 2018.","Information extraction; Ontology-based knowledge; Vector space model","Artificial intelligence; Data handling; Information analysis; Information retrieval; Information retrieval systems; Ontology; Semantics; Surveys; Vector spaces; Information extraction systems; Ontology-based; Semantic processing; Specific class; Vector space models; Data mining",2-s2.0-85031746627
"Patil A.P., Doshi D., Dalsaniya D., Rashmi B.S.","Applying machine learning techniques for sentiment analysis in the case study of indian politics",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030151462&doi=10.1007%2f978-3-319-67934-1_31&partnerID=40&md5=d280f2d160ca1c71f8471f52c79ce2da","In the recent era, humans have become detached from their surroundings, immediate peers and more addicted to their social media platforms and micro-blogging sites. Technology is digitalizing at a very fast pace and this has led to man being social, but only on technological forefront. Social media platforms like twitter, facebook, whatsapp, instagram are in trend. In our paper, we will concentrate on data generated through Twitter (tweets). People express their opinions, perspectives within a 140 character tweet, which is subjective. We try to analyze their emotion by tweet classification followed by sentiment analysis. On an average, with 328 million Twitter users, 6000 tweets are generated every second. This tremendous amount of data can be used to assess general public’s views in economy, politics, environment, product reviews, feedbacks etc. and so many other sectors. Here, we take into account the political data from Tweets. The data obtained can be images, videos, links, emoticons, text, etc. The results obtained could help the government function better, improve their flaws, plan out better strategies to empower the nation. © Springer International Publishing AG 2018.","Machine learning; Polarity; Sentiment analysis; Subjective data; Twitter","Artificial intelligence; Data mining; Image enhancement; Learning systems; Social networking (online); General publics; Machine learning techniques; Polarity; Product reviews; Sentiment analysis; Social media platforms; Subjective data; Twitter; Signal processing",2-s2.0-85030151462
"Kao J.-H., Lin R.-T., Hsu L.-M., Wang T.-C.","A practical study on the relation of out-of-hospital cardiac arrest and chronic diseases",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031403433&doi=10.1007%2f978-981-10-3187-8_78&partnerID=40&md5=3bf5590bc14b709ac668a9dfcd13dc21","This study has several segments: reviewing and collecting the OHCA patients with Registry Data of New Taipei city from 2010 to 2011: analyzing and appling International Utestein formula standards for data extraction; using regression analysis to search for the relationship between the variations and OHCA 2 h survival rate; appliing OHCA modeling to identify risk factors coefficient; using Apriori to analize the association rules, and adapting decision tree to create a Risk Decision Model. The correlation chronic diseases: Heart Disease, Diabetes Disease, and Hypertensive Disease have a significant correction on OHCA 2 h survival risk factors. Meanwhile, senior citizens have had high risk could not be revive form standard emergency measure. OHCA Decision tree model helped EMDand rescue units to allocate pre-hospital medical resources and contributes to an accurate clinical quality assessments to assist researchers to analyze medical policy for different OHCA patients. © Springer Nature Singapore Pte Ltd. 2018.",,"Computation theory; Data mining; Decision trees; Hospitals; Regression analysis; Risk assessment; Trees (mathematics); Cardiac arrest; Chronic disease; Data extraction; Decision tree modeling; Emergency measures; Heart disease; Quality assessment; Senior citizens; Diseases",2-s2.0-85031403433
"García-Pablos A., Cuadros M., Rigau G.","W2VLDA: Almost unsupervised system for Aspect Based Sentiment Analysis",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028727045&doi=10.1016%2fj.eswa.2017.08.049&partnerID=40&md5=dd04cb9e0a8f4e2e874de90a088fe6c3","With the increase of online customer opinions in specialised websites and social networks, automatic systems to help organise and classify customer reviews by domain-specific aspect categories and sentiment polarity are more needed than ever. Supervised approaches for Aspect Based Sentiment Analysis achieve good results for the domain and language they are trained on, but manually labelling data to train supervised systems for all domains and languages is very costly and time consuming. In this work, we describe W2VLDA, an almost unsupervised system based on topic modelling that, combined with some other unsupervised methods and a minimal configuration step, performs aspect category classification, aspect-term and opinion-word separation and sentiment polarity classification for any given domain and language. We evaluate its domain aspect and sentiment classification performance in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several domains (hotels, restaurants, electronic devices) and languages (English, Spanish, French and Dutch). © 2017 Elsevier Ltd","Almost unsupervised; Aspect Based Sentiment Analysis; Multidomain; Multilingual; Opinion mining","Classification (of information); Data mining; Modeling languages; Online systems; Almost unsupervised; Multi domains; Multilingual; Opinion mining; Sentiment analysis; Natural language processing systems",2-s2.0-85028727045
"Pang Y., Jiang X., Zou F., Gan Z., Wang J.","Research on Energy Consumption of Building Electricity Based on Decision Tree Algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030872666&doi=10.1007%2f978-3-319-68527-4_29&partnerID=40&md5=e36fbf9cf18a382b3951798960226010","The consumption of building energy was increasing every year in the past in China. It is a big problem to be solved about how to make the building energy consumption to be more reasonable. In order to obtain the feature of building energy consumption with its electrical instructions, a decision-tree algorithm is designed to mine the data of energy consumption of electricity. According to the analysis of the attribute quantity factors, comparing the factors classification information gain value, and analysis the proportion of power consumption factors, a typical energy consumption sample of a laboratory in university was modeled and analyzed in this paper. Experimental results show that the seasonal change factor has a great influence on the energy consumption of the building. And according to this conclusion, a suitable energy-efficiency device is planed. © 2018, Springer International Publishing AG.","Decision-tree; Energy consumption; ID3 algorithm; Information entropy","Buildings; Classification (of information); Data handling; Data mining; Decision trees; Energy efficiency; Energy utilization; Information analysis; Trees (mathematics); Building energy; Building energy consumption; Classification informations; Decision-tree algorithm; ID3 algorithm; Information entropy; Seasonal changes; Energy conservation",2-s2.0-85030872666
"Schlachter J., Lautenschlager J.","A fast track approach towards automatic detection of gray zone activities from text",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021743646&doi=10.1007%2f978-3-319-60747-4_14&partnerID=40&md5=381f9b8268abc64ccfd3d1551fe43379","Gray Zone Operations (GZ Ops) are increasingly being used by countries to achieve national security objectives that have historically only been achieved through armed conflict. GZ Ops employ actions that are diverse, covert, and adversarial but fall below the threshold required to trigger an aggressive response from major powers. We aim to help analysts with early identification of Gray Zone Operations by automatically detecting taxonomy-backed Gray Zone activities in news stories and text. We map our proven geo-political event extraction and data processing capabilities previously developed under the World-Wide Integrated Crisis Early Warning System (W-ICEWS) program to Gray Zone activities and apply these mappings to text from a corpus of Russian news stories from 2015 to 2016 to generate a set of sentences labeled with specific Gray Zone activities. We evaluate the validity of these matches to determine if the sentences and stories accurately reflect their label. © Springer International Publishing AG 2018.","Data mining; Event coding; Gray zone; NLP; Taxonomy; Text analytics","Data handling; Decision making; National security; Taxonomies; Automatic Detection; Crisis early warnings; Event coding; Gray zone; Political events; Processing capability; Security objectives; Text analytics; Data mining",2-s2.0-85021743646
"Midha N., Singh V.","Classification of e-commerce products using reptree and k-means hybrid approach",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031425401&doi=10.1007%2f978-981-10-6620-7_26&partnerID=40&md5=e576d4e39022b38059b45b5580fd9bed","The paper discusses an algorithm that groups the items on the basis of their attributes and then classifies the clusters. In other words, the proposed algorithms first cluster the items on the basis of property, i.e., attributes available for the dataset. The clustering is performed by K-means clustering. Then this clustered data is classified using the RepTree. In other words, the proposed algorithm is the hybrid algorithm of K-means clustering and the RepTree classification. The proposed algorithm is compared with the RepTree algorithm using the WEKA tool. The comparison is done over clothing dataset downloaded from Internet. The proposed algorithm decreases the mean absolute error as well as the root-mean-square error. The decrease in error results in accurate classification. So the proposed algorithm clusters the items and classifies them on the basis of their attributes more accurately. © 2018, Springer Nature Singapore Pte Ltd.","Clustering; Data mining; K-means; RepTree","Big data; Data mining; Errors; Mean square error; Clustering; Hybrid algorithms; Hybrid approach; K-means; K-means clustering; Mean absolute error; RepTree; Root mean square errors; Clustering algorithms",2-s2.0-85031425401
"Kato T., Kambayashi Y., Terawaki Y., Kodama Y.","Analysis of students’ behaviors in programming exercises using deep learning",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020473678&doi=10.1007%2f978-3-319-59451-4_4&partnerID=40&md5=df2be401c8401d568f60c525eb0844ee","Programming exercises are time-consuming activities for many students. Therefore, many classes provide meticulous supports for students, in the form of teaching assistants (TAs). However, individual students’ programming behaviors are quite different from each other, even when they are solving the same problem. It can be hard for TAs to understand the unique features of each student’s programming behavior. Using data mining (specifically autoencoding of deep learning), we have analyzed students’ programming behaviors in order to determine their varied features. The purpose of this study is to present such behavioral features for TAs, to improve the effectiveness of the assistance they can provide. © Springer International Publishing AG 2018.","Deep learning; Learning analytics; Programming exercises; Teaching assistants","Behavioral research; Computer programming; Data mining; Deep learning; E-learning; Education; Behavioral features; Learning analytics; Programming exercise; Teaching assistants; Unique features; Students",2-s2.0-85020473678
"Araya R., Aljovin E.","The effect of teacher questions on elementary school students’ written responses on an online stem platform",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022231961&doi=10.1007%2f978-3-319-60018-5_36&partnerID=40&md5=eca57e0246beee77a807e68a9f1a3655","In this paper, we analyze thousands of elementary school students’ written responses to open-ended questions on an online STEM platform. We found that certain key words in the teachers’ questions, such as “explain”, had a significant effect on the length of the students’ responses and on the type of words used for reasoning. We also found that including emotional words in the teachers’ questions had a significant effect on the emotional content of the students’ responses. These findings have practical implications. They suggest that the use of key words in teachers’ questions is an important element that must be taken into account when planning a class and when designing instructional strategies using online STEM platforms. © Springer International Publishing AG 2018.","Learner engagement; Online STEM platforms; Text mining","Data mining; Education; Human engineering; Students; Teaching; Elementary schools; Emotional words; Instructional strategy; Key words; Learner engagement; Online STEM platforms; Open-ended questions; Text mining; STEM (science, technology, engineering and mathematics)",2-s2.0-85022231961
"Rana T.A., Cheah Y.-N.","Improving aspect extraction using aspect frequency and semantic similarity-based approach for aspect-based sentiment analysis",2018,"Advances in Intelligent Systems and Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022207221&doi=10.1007%2f978-3-319-60663-7_30&partnerID=40&md5=57bf70d558f5048c8abdf30f8dc80eb8","Identifying the targets of users’ opinions, referred as aspects, in aspect-based sentiment analysis, is the most important and crucial task. A large number of approaches have been proposed to accomplish this task. These approaches identify a huge amount of potential aspects from customer reviews. But not all the extracted aspects are interesting and include terms which are not related to the product and these irrelevant terms affect the performance of the aspect extraction approaches. Therefore, in this paper, we are proposing a two-level aspect pruning approach to eliminate irrelevant aspects. The proposed approach performs the task of aspect pruning in two steps: (a) by calculating the frequency of each word and selecting the most frequent aspects; and (b) by calculating the semantic similarity of non-frequent words and eliminate aspects which are not semantically related to the product. Our experimental evaluation has shown a significant improvement of the proposed approach over the compared approaches. © Springer International Publishing AG 2018.","Aspect pruning; Aspect-based sentiment analysis; Explicit aspects; Opinion mining","Data mining; Extraction; Semantics; Aspect pruning; Customer review; Experimental evaluation; Explicit aspects; Opinion mining; Semantic similarity; Sentiment analysis; Natural language processing systems",2-s2.0-85022207221
"Oza K.S., Kamat R.K., Naik P.G.","Student feedback analysis: A neural network approach",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028395560&doi=10.1007%2f978-3-319-63673-3_42&partnerID=40&md5=bf75e9ff1caf3e0dc6cd9e722104d1c4","With a specific end goal to excel in teaching-learning the students and the educator must be responsible to each other. The trust built in such an environment will permit the educator to create a conducive teaching-learning environment. A fundamental tool which can be for the above said purpose is the feedback of students on various aspects of teaching-learning. Feedback is ultimate necessity to ensure effective learning. It helps the students to comprehend the subject being examined and gives a clear direction on the most proficient method to enhance their learning. The present paper depicts the analysis of the feedback using Artificial Neural Network (ANN). The feedback of the students in the form of text messages is converted to numerical vectors by considering the positive and negative keywords. Thereafter the ANN is trained using the above said input to predict whether the feedback is positive or negative. We could really enhance student’s accomplishment and achievement all the more viably in an effective way utilizing the aforesaid approach. It paper conveys the advantages and effects of this approach to the students, educators and scholarly establishments. © 2018, Springer International Publishing AG.","Artificial neural network; Classification; Feedback; Text mining","Classification (of information); Computer aided instruction; Data mining; Feedback; Intelligent systems; Neural networks; Students; Text processing; Effective learning; Fundamental tools; Student feedback; Teaching-learning; Teaching-learning environment; Text mining; Education",2-s2.0-85028395560
"Rahab H., Zitouni A., Djoudi M.","SIAAC: Sentiment Polarity Identification on Arabic Algerian Newspaper Comments",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029574192&doi=10.1007%2f978-3-319-67621-0_12&partnerID=40&md5=5aa1edec5b5b104f1ffeb4f527d6d716","It is a challenging task to identify sentiment polarity in Arabic journals comments. Algerian daily newspapers interest more and more people in Algeria, and due to this fact they interact with it by comments they post on articles in their websites. In this paper we propose our approach to classify Arabic comments from Algerian Newspapers into positive and negative classes. Publicly-available Arabic datasets are very rare on the Web, which make it very hard to carring out studies in Arabic sentiment analysis. To reduce this gap we have created SIAAC (Sentiment polarity Identification on Arabic Algerian newspaper Comments) a corpus dedicated for this work. Comments are collected from website of well-known Algerian newspaper Echorouk. For experiments two well known supervised learning classifiers Support Vector Machines (SVM) and Naïve Bayes (NB) were used, with a set of different parameters for each one. Recall, Precision and F_measure are computed for each classifier. Best results are obtained in term of precision in both SVM and NB, also the use of bigram increase the results in the two models. Compared with OCA, a well know corpus for Arabic, SIAAC give a competitive results. Obtained results encourage us to continue with others Algerian newspaper to generalize our model. © 2018, Springer International Publishing AG.","Arabic comments; Machine learning; Natural Language Processing; Naïve Bayes; Newspaper; Opinion mining; Sentiment analysis; Support Vector Machines","Artificial intelligence; Computational methods; Data mining; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Newsprint; Sodium; Support vector machines; Websites; Algeria; Arabic comments; Daily newspapers; Learning classifiers; Newspaper; Opinion mining; Sentiment analysis; Taxonomies",2-s2.0-85029574192
"Dalal V., Malik L.","Semantic graph based automatic text summarization for hindi documents using particle swarm optimization",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028421290&doi=10.1007%2f978-3-319-63645-0_31&partnerID=40&md5=90e6604d7abfaab1b9b43c50760ec2b2","Automatic text summarization can be defined as a process of extracting and describing important information from given document using computer algorithms. A number of techniques have been proposed by researchers in the past for summarization of English text. Automatic summarization of Indian text has received a very little attention so far. In this paper, we propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm. PSO is one of the most powerful bio-inspired algorithms used to obtain optimal solution. The subject-object-verb (SOV) triples are extracted from the document. These triples are used to construct semantic graph of the document. A classifier is trained using PSO algorithm which is then used to generate semantic sub-graph and to obtain document summary. © Springer International Publishing AG 2018.","Bio-inspired algorithms; PSO; Semantic graph; Text mining; Text summarization","Data mining; Graphic methods; Intelligent systems; Natural language processing systems; Optimization; Semantics; Text processing; Automatic summarization; Automatic text summarization; Bio-inspired algorithms; Particle swarm optimization algorithm; Semantic graphs; Subject object verbs (SOV); Text mining; Text summarization; Particle swarm optimization (PSO)",2-s2.0-85028421290
"Łancucki R., Foszner P., Polanski A.","Searching through scientific PDF files supported by bi-clustering of key terms matrices",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764368&doi=10.1007%2f978-3-319-67792-7_15&partnerID=40&md5=2ddf00ff23700682e3aa6d0ba905934a","We describe an original approach for exploring corpora of pdf format scientific texts in the area of bio-medical research, created over a wide topic of interest, e.g., cancer, thyroid cancer, biological process etc. Our methodology is based on indexing large lists of appropriate key-terms and additionally performing bi-clustering of term occurrence matrices. In our approach the position of phrase inside text (abstract or text) is not considered, but we include statistics based on occurrences frequency. We treat documents as a bags of words and the results are processed toward unique list of values. Bi-clustering is used to achieve separating character of lists of key-terms, characterizing sub-types of the studied category, e.g., different cancers or different sub-classes of a given cancer. We prove usefulness of the algorithm by searching for lists of genes characteristic for cancer types. © 2018, Springer International Publishing AG.","Bi-clustering; Rule-based; Text mining","Abstracting; Data mining; Natural language processing systems; Bi-clustering; Bio-medical; Biological process; Rule based; Scientific texts; Term occurrences; Text mining; Thyroid cancers; Diseases",2-s2.0-85030764368
"Chandra Shekar K., Venugopala Rao K., Chandra P.","Hidden decision tree based pattern evaluation using regression models for health diagnosis",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028393767&doi=10.1007%2f978-3-319-63673-3_4&partnerID=40&md5=1d07ea1987a74b91e82ac8990537d671","Regression Models evolved since in Health Diagnosis contributes to predict the health condition through various approaches and analysis. These analytical approaches require statistical methodology for the better diagnosis in data mining applications, utilizing numerical predictions through regression coefficients’ approximations. Here, we propose a model to diagnose the Health condition through error pattern discovery from the known and unknown databases; by designing a hidden layer to extract hidden patterns through Hidden Decision Tree approach. Our proposed model evaluates the database for (a) resulting the different health diagnosis, (b) patternising these to optimized queries based on returned results, (c) effective implementation and extraction of data being supplied by the end users, to handle data regularities and irregularities and (d) evaluate the pattern through HDT approach for the accuracy of the knowledge discovery. Proposed model can be implemented on a medical data to evaluate the error pattern for numeric additive quantities to obtain a better health diagnosis model. © 2018, Springer International Publishing AG.","Health diagnosis; Hidden decision trees; Pattern evaluation; Predictive accuracy; Regression models","Decision trees; Diagnosis; Health; Intelligent systems; Query languages; Regression analysis; Health diagnosis; Hidden decisions; Pattern evaluation; Predictive accuracy; Regression model; Data mining",2-s2.0-85028393767
"Hernández-Torruco J., Canul-Reich J., Román D.L.","Rule based classifiers for diagnosis of mechanical ventilation in Guillain-Barré syndrome",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022178988&doi=10.1007%2f978-3-319-62410-5_22&partnerID=40&md5=83dc3fdfd2ac7cd2e5d4f068bd292bfe","Breathing difficulty is a complication present in almost a third of Guillain-Barré Syndrome (GBS) patients. To alleviate this condition a mechanical respiratory device is needed. Anticipating this need is crucial for patients’ recovery. This can be achieved by means of machine learning predictive models. We investigated whether clinical, serological, and nerve conduction features separately can predict the need of mechanical ventilation with high accuracy. In this work, three rule based classifiers are applied to create a diagnostic model for this necessity. JRip, OneR and PART algorithms are analyzed using a real dataset. We performed classification experiments using train-test evaluation scheme. Clinical features were found as the best predictors. © Springer International Publishing AG 2018.","Data mining and processing; JRip; OneR; PART; Performance evaluation; Train-test","Artificial intelligence; Data handling; Distributed computer systems; Patient rehabilitation; Ventilation; Clinical features; JRip; Mechanical ventilation; OneR; PART; Performance evaluation; Predictive models; Rule-based classifier; Diagnosis",2-s2.0-85022178988
"Bock C.","Generating load profiles using smart metering time series",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029424720&doi=10.1007%2f978-3-319-66830-7_20&partnerID=40&md5=21bbbaeb6560dd68d2f1aae732b3aa56","In this work we present a practice-oriented approach for generating load profiles as a means to forecast energy demand by using smart metering time series. The general idea is to apply fuzzy clustering on historic consumption time series. The segmentation yielded helps electricity companies to identify customers with similar consumption behavior. This knowledge can be used to plan available energy capacities in advance. What makes this approach special is that this approach segments consumption time series by time in addition to identifying customer groups. This is done not only to accommodate for customers potentially behaving completely different on working days than on local holidays for example, but also to build the resulting load profiles in a way the electricity companies can adapt with minimal adjustments. We also evaluate our approach using two real world smart metering datasets and discuss potential improvements. © 2018, Springer International Publishing AG.","Big data; Clustering; Data mining; Knowledge discovery; Load profiles; Smart metering; Time series","Big data; Computer circuits; Data mining; Electric measuring instruments; Fuzzy sets; Sales; Time series; Available energy; Clustering; Electricity companies; Energy demands; Load profiles; Real-world; Smart metering; Fuzzy logic",2-s2.0-85029424720
"Keshwani K., Agarwal P., Kumar D., Ranvijay","Prediction of market movement of gold, silver and crude oil using sentiment analysis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031410637&doi=10.1007%2f978-981-10-3773-3_11&partnerID=40&md5=c79889467c6cec38181aba8438dd087f","Prediction of stock movements and share market has always remained an area of great curiosity and concern for investors. It has already been established that the movement of market shares a big correlation with the sentiments about it. In this paper, we have applied sentiments analysis techniques and machine learning principles to foretell the stock market trends of three major commodities, Gold, Silver and Crude oil. We have used the SentiWordNet library to quantify the emotions expressed in the text. Further neural network has been trained over the calculated readings. Thereafter, the trained neural network is used to forecast the future values. The efficacy of the proposed model is measured on the basis of mean absolute percentage error. The results clearly reflect that there in fact lies a strong correlation between public mood and stock market variations. © Springer Nature Singapore Pte Ltd. 2018.","Microblogging data; Neural network; Sentiment analysis; SentiWordNet; Stock market","Commerce; Competition; Crude oil; Data mining; Electronic trading; Finance; Financial markets; Forecasting; Gold; Learning systems; Neural networks; Petroleum analysis; Silver; Analysis techniques; Market variations; Mean absolute percentage error; Microblogging; Sentiment analysis; SentiWordNet; Strong correlation; Trained neural networks; Investments",2-s2.0-85031410637
"Mangathayaru N., Mathura Bai B., Srikanth P.","Clustering and classification of effective diabetes diagnosis: computational intelligence techniques using PCA with kNN",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028438585&doi=10.1007%2f978-3-319-63673-3_52&partnerID=40&md5=994f9bb6766535134367c3dc02c13136","The fourth leading disease in the world today is Diabetes and there are number of challenges to predict and identify the disease. Data mining proposes effective approaches to identify the diabetic patients. This paper proposes clustering and classification of effective diabetes diagnosis based on computational intelligence techniques using PCA with kNN. Diabetes disease data is used to identify feature of clusters. Diabetes disease diagnosis proposes novel distribution function applied to classify each patient. This proposed procedure defines clusters and similarity measure based on classifying with each cluster using computational intelligence techniques. PCA using diabetes disease data for dimensionality reduction. Novel similarity measure is proposed in kNN for classification. Accuracy measures are computed for each patient. © 2018, Springer International Publishing AG.","Classification; Clustering; Diabetes disease; Distribution function and PCA with kNN","Artificial intelligence; Classification (of information); Computer aided diagnosis; Data mining; Distribution functions; Intelligent systems; Accuracy measures; Clustering; Computational intelligence techniques; Diabetes diagnosis; Dimensionality reduction; Disease diagnosis; Effective approaches; Similarity measure; Diagnosis",2-s2.0-85028438585
"Klimenko A., Gorelova G., Korobkin V., Bibilo P.","The Cognitive Approach to the Coverage-Directed Test Generation",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029604606&doi=10.1007%2f978-3-319-67621-0_34&partnerID=40&md5=c123133de02f264798edc7ce75d1fd29","The important contemporary issue of VLSI design verification is its time-consuming. The hardware model, written, for instance, with VHDL, is verified by formal and dynamic verification approaches. Dynamic verification (simulation) is widely-used due to the possibility of full automation of the process, but takes too much time due to its redundancy. The concept of the coverage-directed test generation is to redirect the test generator such as to reach uncovered metric points. There are several approaches for this, including genetic algorithms using, Bayesian network, Data mining, etc. The new cognitive approach to the coverage-directed test generation (CA CDG) is proposed within this paper. It is based on a cognitive map usage. The CA CDG is described, some simulation results are given. Also the future work areas are outlined. © 2018, Springer International Publishing AG.","Cognitive map; Coverage-directed test generation; Simulation; Verification; VHDL","Bayesian networks; Cognitive systems; Computational methods; Computer hardware description languages; Data mining; Genetic algorithms; Testing; Verification; Cognitive approaches; Cognitive maps; Contemporary issues; Dynamic verifications; Hardware models; Simulation; Test generations; VLSI design; Statistical tests",2-s2.0-85029604606
"Afshar J., Haghighian Roudsari A., Lee C.C.G., Eom C.S.-H., Lee W., Arora N.","Harmonic Mean Based Soccer Team Formation Problem",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032484718&doi=10.1007%2f978-981-10-6520-0_25&partnerID=40&md5=2a4a4f826050a01b2328134a81f51d07","This paper provides a review of the literature on team formation problem. We illustrate the review based on two different classifications; operations research and data mining. The papers in each class are then described according to their contributions to the literature. We aim to facilitate the chasing of conducted works in relevant area of interest and to identify trends and fields which should be topic to future studies. Also, we present the problem of the existing Team Formation method and showed our approach outperformed the conventional approaches, where we explained not only the abilities of the players but also the harmony has been one of the key factors that has been worked in the real world soccer datasets. © 2018, Springer Nature Singapore Pte Ltd.","Data mining; Operations research; Team formation problem","Operations research; Sports; Area of interest; Conventional approach; Harmonic mean; Real-world; Soccer team; Team formation; Data mining",2-s2.0-85032484718
"Srivastava D.K., Nair P.","Employee attrition analysis using predictive techniques",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028428954&doi=10.1007%2f978-3-319-63673-3_35&partnerID=40&md5=d4363b592d85e8869b4c9778510afe86","Employee churn is an unsolicited aftermath of our blooming economy. Attrition may be defined as voluntary or involuntary resignation of a serving employee from an organization. Employee churn can incur a colossal cost to the firm. However, furtherance to prediction and control over attrition can give quality results. Earmarking the risk of attrition, the management can take required steps to retain the high valued talent. Workforce Analytics can be applied to reduce the overall business risk by predicting the employee churn. Predictive Analytics is the field of study that employs statistical analysis, data mining techniques and machine learning to predict the future events with accuracy based on past and current situation. The paper presents a framework for predicting the employee attrition with respect to voluntary termination employing predictive analytics. © 2018, Springer International Publishing AG.","Data mining; Employee attrition; Predictive algorithms; Predictive analytics; Turnover prediction","Data mining; Forecasting; Intelligent systems; Learning systems; Quality control; Business risks; Current situation; Prediction and control; Predictive algorithms; Predictive techniques; Risk of attritions; Predictive analytics",2-s2.0-85028428954
"Li L., Zhang J., Zheng Y., Ran B.","Real-time traffic incident detection with classification methods",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026736139&doi=10.1007%2f978-981-10-3551-7_62&partnerID=40&md5=b53cc395914df39e212ac8a87b65dbf9","It is well known that traffic incident detection is essential to intelligent transportation system (ITS) and modern traffic management. Compared to traditional models based on traffic theory, some data mining computational algorithms are believed more appropriate and flexibility for automatic incident detection. In this paper, four classification models were introduced and their parameters were selected by tenfold cross-validation. Using an open dataset their predictive performance was compared based on five criteria. The results show that the classification models perform well to detect traffic incidents and no over-fitting problem. What’s more, AdaBoost-Cart and Naïve Bayes models seem to outperform support vector machine and Cart models since they provide superior detection rate. However, they cost long time to train. © Springer Science+Business Media Singapore 2018.","Classification method; Data mining; Traffic incident detection","Adaptive boosting; Advanced traffic management systems; Bayesian networks; Computation theory; Data mining; Highway traffic control; Intelligent systems; Transportation; Automatic incident detection; Classification methods; Classification models; Computational algorithm; Intelligent transportation systems; Over fitting problem; Predictive performance; Traffic incident detections; Intelligent vehicle highway systems",2-s2.0-85026736139
"Khawaja S.G., Khan A.M., Akram M.U., Khan S.A.","A novel architecture for k-means clustering algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028652714&doi=10.1007%2f978-3-319-60834-1_31&partnerID=40&md5=119624485bc57603b0d9a9d76a0b4749","Technological advancements in today information age has helped the researchers to capture digital footprints of humans with regards to their daily activities. These logs of information posses valuable information for the data analytics who process it to find hidden pattern and unique behavior. Among the many algorithms k-means clustering is one of the very popular and widely used algorithm in the field of data mining and machine learning. k-means provides natural segments of dataset provided for clustering. It uses proximity to assign data points to a specific cluster, here the criteria of allocation is the minimum distance from the cluster center. Unfortunately, the rate of data growth has not been met by the speed of the algorithms. A number of hardware based solutions have been proposed to increase the processing power of different algorithms. In this paper, we present a novel algorithm for k-mean clustering which exploits the data redundancy occurring in the dataset. The proposed algorithm performs computations for the available unique items in the dataset and uses its frequency to finalize the results. Furthermore, FPGA based hardware architecture for the proposed algorithm is also presented in the paper. The performance of the proposed algorithm and its hardware implementation is evaluated using execution time, speedup and throughput. The proposed architecture provides speedup of 23 times and 2600 times against sequential hardware architecture and software implementation with a very small area requirement. © 2018, Springer International Publishing AG.",,"Data mining; Hardware; Learning systems; Hardware architecture; Hardware implementations; K-means clustering; K-Means clustering algorithm; Novel architecture; Proposed architectures; Software implementation; Technological advancement; Clustering algorithms",2-s2.0-85028652714
"Hui L., Keh H.C., Chiang M.C., Liu Z.Y.","Clinical application of decision support system for treatment of migraine",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031423642&doi=10.1007%2f978-981-10-3187-8_53&partnerID=40&md5=b7d139a5b2494cfc933d9137028123ba","In this modern society, migraine belongs to a kind of common disease. The hospitals begin to use clinical decision support system a lot to improve the accuracy of diagnosis, this kind of system can help the physicians make the decisions and give a treatment by entering the information of the patients in advance. In the treatment of migraine, the patients need to keep diaries of headache, the physicians can make a diagnosis and trace according to the diaries. In this paper, we come up with constructing a clinical decision support system for treatment of migraine. We use the headache diaries as data, in order to store the data easily, we transform paper diaries to electronic diaries first, store the data in the databases of the servers or the mobile platforms (e.g. smartphones, tablet computers), and the data can be shown on the front-end interface. We also make use of data mining to analyze the factors, medicines and associations of migraine, and the result will be recorded in the system to improve the efficiency of the physicians’ inspection. © Springer Nature Singapore Pte Ltd. 2018.","Clinical decision support system; Data mining; Migraine","Artificial intelligence; Computation theory; Data mining; Diagnosis; Patient treatment; Clinical application; Clinical decision support systems; Common disease; Electronic diary; Front end; Migraine; Mobile platform; Tablet computer; Decision support systems",2-s2.0-85031423642
"Hodhod R., Fleenor H.","A text mining based literature analysis for learning theories and computer science education",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029511664&doi=10.1007%2f978-3-319-64861-3_52&partnerID=40&md5=692c98f93a63b09eaa269eb0d7c60535","Text mining has been successfully used to discover interesting patterns and extract useful information from analyzing massive text data exists on the internet, books and other text sources. Computer science education has become an initiative for The National Science Foundation (NSF) and the White House Office of Science and Technology Policy (OSTP) in the United States. Finding the right tools and technologies that can support that initiative and help students succeed and do well while studying computer science is a plus. Although the literature is rich with research for computer science education, there is no clear guide on the use of learning theories to design educational games to teach computer science. Text mining can analyze the literature to find trends for designing educational games for computer science education, in addition to identifying existing gaps. The paper presents the results from analyzing 204 papers to discover the current state of research related to computer science education, using Voyant, a text mining tool. Analysis of results should provide an insight on how learning theories have been considered/used in computer science education and guide future research through identifying what learning theories need to be considered in designing educational games to teach computer science topics, such as data structures. © 2018, Springer International Publishing AG.","Computer science education; Learning theories; Teaching data structures; Text mining",,2-s2.0-85029511664
"Jenhani F., Gouider M.S., Said L.B.","Social stream clustering to improve events extraction",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020383090&doi=10.1007%2f978-3-319-59424-8_30&partnerID=40&md5=191e98e899000c24dabe6be6eace4ab8","Events extraction from social media data is a tedious task because of their volume, velocity and informality. In a previous work [25], we proposed a successful approach for events extraction from social data. However, messages were processed individually which generates many meaningless events because of missing details scattered within millions of text segments. In addition, many unnecessary texts were analyzed which increased processing time and decreased the performance of the system. In this paper, we aim to cope with the abovementioned weaknesses and ameliorate the performance of the system. We propose clustering to group semantically-related text segments, filter noise, reduce the volume of data to process and promote only relevant text segments to the information extraction pipeline. We port the clustering algorithm to a stream processing framework namely Storm in order to build a stream clustering solution and scale up to continuously growing volumes of data. © Springer International Publishing AG 2018.","Apache storm; Clustering; Events extraction; Social stream; Twitter","Data mining; Information analysis; Social networking (online); Storms; Clustering; Events extractions; Processing time; Social media datum; Social streams; Stream clustering; Stream processing; Twitter; Clustering algorithms",2-s2.0-85020383090
"Jabbar M.A., Srinivas K., Sai Satyanarayana Reddy S.","A novel intelligent ensemble classifier for network intrusion detection system",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028617368&doi=10.1007%2f978-3-319-60618-7_48&partnerID=40&md5=9be019111b209797e0e15a8934ac94e4","The objective of this research paper is to propose a novel ensemble Intrusion Detection System (IDS) to classify intrusions. Network security is important and challenging, as there is a tremendous growth of network-based services and sensitive information shared on the network. Intrusion detection system (IDS) is a security mechanism used to detect, prevent unauthorized access to computer networks. IDS plays a vital role in maintaining the secure network. Therefore there is a need to develop reliable and robust IDS. Various data mining techniques are used to implement network intrusion detection. Recently, ensemble classifiers are used to implement it. A group of classifiers known as ensemble classifiers outperforms base classifiers. This paper deals with a novel ensemble classifier based on naïve Bayes and ADTree for intrusion detection. ADTree is a well known supervised boosting decision tree algorithm. Naive bayes is a linear classifier and assumes that all features are independent. Naive Bayes will not perform well, where complex attribute dependencies are present. The proposed ensemble combines ADTree and Naïve Bayes to improve classification accuracy of the detection system. Our experimental results show that the proposed ensemble classifier outperforms other classifiers in terms of accuracy. © Springer International Publishing AG 2018.","ADTree; Ensemble classifier; IDS; IQR; Naïve bayes","Classifiers; Computer crime; Data mining; Decision trees; Intrusion detection; Mercury (metal); Pattern recognition; Sodium; Soft computing; Trees (mathematics); ADTree; Classification accuracy; Decision-tree algorithm; Ensemble classifiers; Intrusion Detection Systems; Network intrusion detection; Network intrusion detection systems; Sensitive informations; Network security",2-s2.0-85028617368
"de la Hoz C.F., Ramos E., Puente A., Méndez F., Menéndez M., Juanes J.A., Losada Í.J.","Ecological typologies of large areas. An application in the Mediterranean Sea",2018,"Journal of Environmental Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030163539&doi=10.1016%2fj.jenvman.2017.09.058&partnerID=40&md5=124331c18ed714908f55734e2c203ee1","One approach to identifying and mapping the state of marine biophysical conditions is the identification of large-scale ecological units for which conditions are similar and the strategies of management may also be similar. Because biological processes are difficult to directly record over large areas, abiotic characteristics are used as surrogate parameters. In this work, the Mediterranean Sea was classified into homogeneous spatial areas based on abiotic variables. Eight parameters were selected based on salinity, sea surface temperature, photosynthetically active radiation, sea-wave heights and depth variables. The parameters were gathered in grid points of 0.5° spatial resolution in the open sea and 0.125° in coastal areas. The typologies were obtained by data mining the eight parameters throughout the Mediterranean and combining two clustering techniques: self-organizing maps and the k-means algorithm. The result is a division of the Mediterranean Sea into seven typologies. For these typologies, the classification recognizes differences in temperature, salinity and radiation. In addition, it separates coastal from deep areas. The influence of river discharges and the entrance of water from other seas are also reflected. These results are consistent with the ecological requirements of the five studied seagrasses (Posidonia oceanica, Zostera marina, Zostera noltei, Cymodocea nodosa, Halophila stipulacea), supporting the suitability of the resulting classification and the proposed methodology. The approach thus provides a tool for the sustainable management of large marine areas and the ability to address not only present threats but also future conditions, such as climate change. © 2017 Elsevier Ltd","Environmental management; K-means; Meteo-oceanographic variables; Regions; Seagrasses; SOM","sea water; abiotic factor; algorithm; climate change; data mining; environmental management; mapping method; marine environment; parameter estimation; river discharge; seagrass; self organization; spatial resolution; Article; biotypology; climate change; Cymodocea nodosa; data mining; deep sea; environmental parameters; Halophila stipulacea; Mediterranean Sea; photosynthetically active radiation; Posidonia oceanica; salinity; sea surface temperature; sea wave height; seagrass; seashore; Zostera marina; Zostera noltei; Mediterranean Sea; Cymodocea nodosa; Halophila stipulacea; Posidonia oceanica; Zostera; Zostera marina",2-s2.0-85030163539
"Amezzane I., Fakhri Y., El Aroussi M., Bakhouya M.","Analysis and Effect of Feature Selection Over Smartphone-Based Dataset for Human Activity Recognition",2018,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032669864&doi=10.1007%2f978-3-319-67837-5_20&partnerID=40&md5=320073374d3e994dcba5607a7cd9d4f6","The availability of diverse and powerful sensors that are embedded in modern smartphones has created exciting opportunities for developing context-aware services and applications. For example, Human activity recognition (HAR) is an important feature that could be applied to many applications and services, such as those in healthcare and transportation. However, recognizing relevant human activities using smartphones remains a challenging task and requires efficient data mining approaches. In this paper, we present a comparison study for HAR using features selection methods to reduce the training and classification time while maintaining significant performance. In fact, due to the limited resources of Smartphones, reducing the feature set helps reducing computation costs, especially for real-time continuous online applications. We validated our approach on a publicly available dataset to classify six different activities. Results show that Recursive Feature Elimination algorithm works well with Radial Basis Function Support Vector Machine and significantly improves model building time without decreasing recognition performance. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Feature selection; Human Activity Recognition; Smartphone sensors","Classification (of information); Data mining; Developing countries; Image segmentation; Information services; mHealth; Pattern recognition; Radial basis function networks; Smartphones; Classification time; Context aware services; Features selection; Human activity recognition; Important features; On-line applications; Radial basis functions; Recursive feature elimination; Feature extraction",2-s2.0-85032669864
"Svetsky S., Moravcik O.","The empirical research on human knowledge processing in natural language within engineering education",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025826926&doi=10.1007%2f978-3-319-60937-9_1&partnerID=40&md5=7584ae2b87ac1ca085571e9287fc4ea7","The state-of-the art of the computer supported or enhanced teaching and learning processes is mostly focused on content acquisition, processing, and management; virtual learning environments; learning analytics; educational data mining. There is an absence of approaches based on knowledge exchange between machines and humans in knowledge based processes. This paper describes such an approach. Within an empirical participatory action research on technology enhanced learning the issue of knowledge is understood as a key element of any automation of human knowledge processing. Teaching is thus considered as a typical knowledge based process. In this context, the utility model is also used, which implemented “virtual knowledge” as a universal construct, which “inter-disciplinarilly” bridges the mental processes of humans and the physical processes performed by computers. Examples of such solutions are briefly mentioned. It is also emphasized that a complex computer supported collaborative learning requires, in the real practice, to execute a combined pedagogic (didactic) and informatics research. © Springer International Publishing AG 2018.","Computer supported collaborative learning; Engineering education; Human computer interactions; Human knowledge processing; Technology enhanced learning","Computer aided instruction; Education; Human computer interaction; Knowledge based systems; Knowledge management; Teaching; Virtual reality; Computer Supported Collaborative Learning; Educational data mining; Human knowledge; Knowledge-based process; Participatory action research; Teaching and learning; Technology enhanced learning; Virtual learning environments; Engineering education",2-s2.0-85025826926
"Mondal T., Ragot N., Ramel J.-Y., Pal U.","Comparative study of conventional time series matching techniques for word spotting",2018,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027530047&doi=10.1016%2fj.patcog.2017.07.011&partnerID=40&md5=33e32cb6d2e1a5ef2303b268d05646d2","In word spotting literature, many approaches have considered word images as temporal signals that could be matched by classical Dynamic Time Warping algorithm. Consequently, DTW has been widely used as a on the shelf tool. However there exists many other improved versions of DTW, along with other robust sequence matching techniques. Very few of them have been studied extensively in the context of word spotting whereas it has been well explored in other application domains such as speech processing, data mining etc. The motivation of this paper is to investigate such area in order to extract significant and useful information for users of such techniques. More precisely, this paper has presented a comparative study of classical Dynamic Time Warping (DTW) technique and many of its improved modifications, as well as other sequence matching techniques in the context of word spotting, considering both theoretical properties as well as experimental ones. The experimental study is performed on historical documents, both handwritten and printed, at word or line segmentation level and with a limited or extended set of queries. The comparative analysis is showing that classical DTW remains a good choice when there is no segmentation problems for word extraction. Its constrained version (e.g. Itakura Parallelogram) seems better on handwritten data, as well as Hilbert transform also shows promising performances on handwritten and printed datasets. In case of printed data and low level features (pixel's column based), the aggregation of features (e.g. Piecewise-DTW) seems also very important. Finally, when there are important word segmentation errors or when we are considering line segmentation level, Continuous Dynamic Programming (CDP) seems to be the best choice. © 2017 Elsevier B.V.","Bentham dataset; Degraded historical documents; George Washington dataset; Hand-written documents; Japanese handwriting recognition; Word spotting","Character recognition; Data mining; History; Mathematical transformations; Speech processing; Bentham dataset; Handwriting recognition; Historical documents; Washington; Word Spotting; Written documents; Dynamic programming",2-s2.0-85027530047
"Lequerica-Fernández P., Peña I., Sánchez Lasheras F., Iglesias Rodrigez F.J., González Gutiérrez C., De Vicente J.C.","Outcome prediction for salivary gland cancer using Multivariate Adaptative Regression Splines (MARS) and Self-Organizing Maps (SOM)",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028658908&doi=10.1007%2f978-3-319-67180-2_35&partnerID=40&md5=8748a9b6c1becdfefa9d8d34c2f8e4b3","Over the last decades, advances in diagnosis and tissue microsurgical reconstruction of soft tissues have modified the therapeutic approach to salivary gland cancers, but long term survival rates have increased only marginally. Due to the relatively low frequency of these tumors together with their diverse histopathological types, it is not easy to perform a prognosis assessment. Multivariate adaptative regression splines (MARS) is a data mining technique with a well-known ability to describe a response starting from a large number of predictors. In this work MARS was used for determining the prognosis of cancers of salivary glands using clinical and histological variables, as well as molecular markers. Here, we have generated four different models combining different sets of variables, with sensitivities and specificities that ranging from 95.45 to 100%. Specifically, one of these models which combined five clinical variables (Tumor size – T-, neck node metastasis – N-, distant metastasis – M-, age, and number of tumor recurrences) plus one molecular factor (gelatinase B -MMP-9-) showed a sensitivity and a specificity of 100%. Therefore, the MARS model was applied to the modelling of the influence of several clinical and molecular variables on the prognosis of salivary gland cancers with success. A self-organizing map (SOM) is a type of neural network what was used here to determine a prognostic model composed for four variables: N, M, number of recurrences and tumor type. The sensitivity of this model was that of 97%, and its specificity was that of 94.7%. © 2018, Springer International Publishing AG.","Data mining; Multivariate Adaptative Regression Splines (MARS); Prognosis; Salivary gland cancer","Conformal mapping; Data mining; Diagnosis; Diseases; Pathology; Regression analysis; Soft computing; Tissue; Tumors; Distant metastasis; Histopathological type; Molecular factors; Outcome prediction; Prognosis; Regression splines; Salivary glands; Tumor recurrences; Self organizing maps",2-s2.0-85028658908
"Knauf R.","Knowledge engineering of system refinement what we learnt from software engineering",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030712018&doi=10.1007%2f978-3-319-64161-4_5&partnerID=40&md5=d76fd28082b2c13f81f6b32def623e8b","Formal methods are a usual means to avoid errors or bugs in the development, adjustment and maintenance of both software and knowledge bases. This chapter provides a formal method to refine a knowledge base based on insides about its correctness derived from its use in practice. The objective of this refinement technique is to overcome particular invalidities revealed by the application of a case-oriented validation technology, i.e. it is some kind of “learning by examples”. Approaches from AI or Data Mining to solve such problems are often not useful for a system refinement that aims at is an appropriate modeling of the domain knowledge in way humans would express that, too. Moreover, they often lead to a knowledge base which is difficult to interpret, because it is too far from a natural way to express domain knowledge. The refinement process presented here is characterized by (1) using human expertise that also is a product of the validation technique and (2) keeping as much as possible of the original humanmade knowledge base. At least the second principle is pretty much adopted from Software Engineering. This chapter provides a brief introduction to AI rule base refinement approaches so far as well as an introduction to a validation and refinement framework for rulebased systems. It also states some basic principles for system refinement, which are adopted from Software Engineering. The next section introduces a refinement approach based on these principles. Moreover, it considers this approach from the perspective of the principles. Finally, some more general conclusions for the development, employment, and refinement of complex systems are drawn. The developed technology covers five steps: (1) test case generation, (2) test case experimentation, (3) evaluation, (4) validity assessment, and (5) system refinement. These steps can be performed iteratively, where the process can be conducted again after the improvements have been made. © 2018, Springer International Publishing AG.",,"Data mining; Engineering education; Iterative methods; Knowledge based systems; Knowledge engineering; Potassium compounds; Program debugging; Software engineering; Appropriate models; Learning by examples; Refinement process; Refinement techniques; System refinement; Test case generation; Validation technologies; Validity assessment; Formal methods",2-s2.0-85030712018
"Proskuryakova L.N., Saritas O., Sivaev S.","Global water trends and future scenarios for sustainable development: The case of Russia",2018,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860823&doi=10.1016%2fj.jclepro.2017.09.120&partnerID=40&md5=c53249a410ae30c54104e7c1bb924239","The grand challenge of accessing fresh water and sanitation is a global concern. The intensity of challenge depends on the geographical location as well as the level of socio-economic development of individual countries. The present paper first reviews the key water-related global trends and examines the global agenda on water issues. Next the focus is turned on Russia. Despite of being one of the water-rich countries in the world, Russia faces a number of substantial administrative and structural issues in the water sector. Therefore, it is crucial to develop a long-term strategy for the management of this infinite, but strategic resource. The present paper develops long-term scenarios and strategies for the Russian water sector towards the year 2030. The study draws upon an earlier horizon scanning activity that identified a set of global trends and uncertainties related to water sector. This horizon scanning work is extended into alternative futures for the Russian water sector by using a combination of Foresight methods including scenario analysis, data mining, and various expert methods. Scenarios developed are characterized by a set of qualitative and quantitative factors and indicators of future developments in three key domains for the water sector: (i) the sustainability of water systems; (ii) water use by households and industry; and (iii) new water products and services. Scenarios present four alternative trajectories for the water sector that may also be applied for certain countries whose water sector is comparable with the Russia. Among the scenarios developed in the study, it is concluded that the most probable ones are Problem conservation and Losses and accidents. However, there is a possibility to revert these scenarios into more desirable trajectories, which are presented in other scenarios. For instance, a variety of new clean water technologies may be widely applied to achieve the Nearly perfect future (visionary) scenario. © 2017 Elsevier Ltd","Foresight; Sustainable water use; Water policy; Water supply and sanitation; Water tariff; Water technologies","Data mining; Economics; Sanitation; Sustainable development; Water conservation; Water management; Foresight; Sustainable water use; Water policies; Water tariffs; Water technologies; Water supply",2-s2.0-85032860823
"Sheik R., Philip S.S., Sajeev A., Sreenivasan S., Jose G.","Entity level contextual sentiment detection of topic sensitive influential twitterers using senticircles",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021192875&doi=10.1007%2f978-981-10-3223-3_19&partnerID=40&md5=cd9824fcccb2bca2814e537929bc3ea1","Sentiment analysis, when combined with the vast amounts of data present in the social networking domain like Twitter data, becomes a powerful tool for opinion mining. In this paper we focus on identifying ‘the most influential sentiment’ for topics extracted from tweets using Latent Dirichlet Allocation (LDA) method. The most influential twitterers for various topics are identified using the TwitterRank algorithm. Then a SentiCircle based approach is used for capturing the dynamic context based entity level sentiment. © Springer Nature Singapore Pte Ltd. 2018.","Entity level sentiment; LDA; SentiCircle; Sentiment analysis; Twitterers; Twitterrank","Intelligent computing; Statistics; Entity-level; SentiCircle; Sentiment analysis; Twitterers; Twitterrank; Data mining",2-s2.0-85021192875
"Djenouri Y., Belhadi A., Fournier-Viger P., Lin J.C.-W.","An hybrid multi-core/GPU-based mimetic algorithm for big association rule mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032622722&doi=10.1007%2f978-981-10-6487-6_8&partnerID=40&md5=dfddf0a164fade7e1e9c555b7a0a07d2","This paper addresses the problem of big association rule mining using an evolutionary approach. The mimetic method has been successfully applied to small and medium size databases. However, when applied on larger databases, the performance of this method becomes an important issue and current algorithms have very long execution times. Modern CPU/GPU architectures are composed of many cores, which are massively threaded and provide a large amount of computing power, suitable for improving the performance of optimization techniques. The parallelization of such method on GPU architecture is thus promising to deal with very large datasets in real time. In this paper, an approach is proposed where the rule evaluation process is parallelized on GPU, while the generation of rules is performed on a multi-core CPU. Furthermore, an intelligent strategy is proposed to partition the search space of rules in several independent sub-spaces to allow multiple CPU cores to explore the search space efficiently and without performing redundant work. Experimental results reveal that the suggested approach outperforms the sequential version by upto at 600 times for large datasets. Moreover, it outperforms the-state-of-the-art high performance computing based approaches when dealing with the big WebDocs dataset. © Springer Nature Singapore Pte Ltd. 2018.","Association rule mining; Big data; GPU algorithm; Mimetic algorithm; Multi-core algorithm",,2-s2.0-85032622722
"Zhang L., Wen M., Ashuri B.","BIM Log Mining: Measuring Design Productivity",2018,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032629269&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000721&partnerID=40&md5=11205660a5d20cd767e7cea104417f96","There has been a long debate on how to measure design productivity. Compared to construction productivity, design productivity is much more difficult to measure because design is an iterative and innovative process. Today, with rapid extension of building information modeling (BIM) applications, tremendous volumes of design logs have been generated by design software systems, such as Autodesk Revit. A systematic approach composed of a detailed step-by-step procedure is developed to deeply mine design logs in order to monitor and measure the productivity of the design process. A pattern retrieval algorithm is proposed to identify the most frequent design sequential patterns in building design projects. A novel metric for measuring design productivity based on the discovered sequential patterns is put forward. A large data set of design logs, provided by a large international design firm, is used as a case study to demonstrate the feasibility and applicability of the developed approach. Results indicate that: (1) typically, each designer executes specific commands more than any other commands; for instance, it is shown for a designer that the accumulative frequency of three commands can reach up to 56.15% of the entire number of commands executed by the designer; (2) a particular sequential pattern of design commands (\""pick lines →\""trim/extend two lines or walls to make a corner→ \""finish sketch"") has been executed 2,219 times, accounting for 46.75% of instances associated with the top five discovered sequential patterns of design commands; (3) the identified sequential patterns can be used as a project control mean to detect outlier performers that may require additional attention from project leaders; and (4) productivity performance within the discovered sequential patterns varies significantly among different designers; for instance, one of the designers (designer #6 in the case study) is identified as the most productive designer in executing both Patterns I and II, whereas another designer (Designer #1) is found to be the most productive designer in executing both Patterns III and IV. It is also uncovered that designers, on average, spend less time running the most observed sequential patterns of design commands as they gain more experience. This research contributes: (1) to the body of knowledge by providing a novel approach to monitoring, measuring, and analyzing design productivity; and (2) to the state of practice by providing new insights into what additional design process information can be retrieved from Revit journal files. © 2017 American Society of Civil Engineers.","Building information modeling (BIM); Log data; Pattern discovery; Process mining; Productivity measurement","Application programs; Design; Edge detection; Information theory; Iterative methods; Productivity; Project management; Building Information Model - BIM; Log data; Pattern discovery; Process mining; Productivity measurements; Architectural design",2-s2.0-85032629269
"Tefrie K.G., Sohn K.-A.","Autonomous text summarization using collective intelligence based on nature-inspired algorithm",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022175933&doi=10.1007%2f978-981-10-5281-1_50&partnerID=40&md5=7a0b135efee118e43757f5104fdb6775","Thousands of years ago written language was introduced as a way of enhancing and facilitating communication. Fast forward to the twenty first century much has changed, especially the flow of data incrementing at fast rate and we should use the power of algorithms and hardware technology to understand text more clearly. With the Information age rising we are being cluttered with humongous data each day with no sign of it slowing. Humans have been trying to create ways on how to handle this continuous flow of text, image and video. And one of the categories of subjects regarding text is text summarization, given a document coming up with a reasonable summarized version of the original document. People have tried different aspects of summarizing to get a shorter yet an informative definition of document. This paper tries to utilize using nature inspired algorithms to implement an auto summarizer of text using pseudo-selected features. The main objective of this research is to use of cooperative nature-inspired algorithm specifically ant colony algorithm in text mining problems, in our case, text summarization. And throughout the paper we will try to show how this system can be achieved as well as show the performance and effectiveness of the measurement. We have used the standard data used to test summarization techniques, DUC data and at last comparing it to two algorithms for further analysis. © Springer Science+Business Media Singapore 2018.","Ant colony system; Automated text summarization; Natural language processing","Ant colony optimization; Data mining; Text processing; Wireless telecommunication systems; Ant colony algorithms; Ant colony systems; Collective intelligences; Continuous flows; Hardware technology; Nature inspired algorithms; Test summarization; Text summarization; Natural language processing systems",2-s2.0-85022175933
"Masrani M., Gou P.","Twitter sentiment analysis using a modified naïve bayes algorithm",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029521435&doi=10.1007%2f978-3-319-67220-5_16&partnerID=40&md5=f32949e93459776fabc4f083afd1c721","Microblogging has emerged as a popular platform and a powerful communication tool among people nowadays. A clear majority of people share their opinions about various aspects of their lives online every day. Thus, microblogging websites offer rich sources of data in order to perform sentiment analysis and opinion mining. Because microblogging has emerged relatively recently there are only some research works which are devoted to this field. In this paper, the focus is on performing the task of sentiment analysis using Twitter which is one of the most popular microblogging platforms. Twitter is a very popular microblogging site where its users write status messages called tweets to express themselves. These status updates mostly express their opinions about various topics. The objective of this paper is to build a system that can classify these Twitter status updates as positive, negative, or neutral with respect to any query term thereby giving an idea about the overall sentiment of the people towards that topic. This type of sentiment analysis is useful for advertisers, consumers researching a service or product, companies, governments, marketers, or any organization who are researching public opinion. © 2018, Springer International Publishing AG.","Data mining; Sentiment analysis; Twitter",,2-s2.0-85029521435
"Rathod D., Khanna S., Singh M.","Smart two level K-means algorithm to generate dynamic user pattern cluster",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028368807&doi=10.1007%2f978-3-319-63645-0_19&partnerID=40&md5=0774a4bc92661625b2b8a02387b1bc53","Data cleaning perform in the Data Preprocessing and Mining. The clean data work of web server logs irrelevant items and useless data can not completely removed and Overlapped data causes difficulty during retrieving data from datasource. Previous paper had given 30% performance of datasource. So We have Implemented Smart Two-level clustering method to get pattern data for mining. This paper presents WebLogCleaner can filter out much irrelevant, inconsistent data based on the common of their URLs and it is going to improving 8% of the data quality, performance, Accuracy and efficiency of any Datasource. © Springer International Publishing AG 2018.","Data cleaning; Pattern cluster; Preprocessing; Web log mining; Web page mining; Web usage mining (WUM)","Intelligent systems; Websites; Data cleaning; Pattern cluster; Preprocessing; Web log mining; Web usage mining; Filtration",2-s2.0-85028368807
"Li J., Zhao Z., Liu Y., Cheng Z., Wang X.","A comparative study on machine classification model in lung cancer cases analysis",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402914&doi=10.1007%2f978-981-10-3187-8_34&partnerID=40&md5=f77505f529ecf7eb1b01988c52ffe0c0","Due to the differences of machine classification models in the application of medical data, this paper selected different classification methods to study lung cancer data collected from HIS system with experimental analysis, applying the R language on decision tree algorithm, Bagging algorithm, Adaboost algorithm, SVM, KNN and neural network algorithm for lung cancer data analysis, in order to explore the advantages and disadvantages of each machine classification algorithm. The results confirmed that in lung cancer data research, Adaboost algorithm and neural network algorithm have relatively high accuracy, with a good diagnostic performance. © Springer Nature Singapore Pte Ltd. 2018.","Adaboost algorithm; Cross validation; Machine classification model; Neural network","Adaptive boosting; Biological organs; Classification (of information); Computation theory; Decision trees; Diagnosis; Diseases; Neural networks; Trees (mathematics); AdaBoost algorithm; Classification methods; Cross validation; Decision-tree algorithm; Diagnostic performance; Experimental analysis; Machine classifications; Neural network algorithm; Data mining",2-s2.0-85031402914
"Drake B., Huang T., Beavers A., Du R., Park H.","Event detection based on nonnegative matrix factorization: Ceasefire violation, environmental, and malware events",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021658769&doi=10.1007%2f978-3-319-60585-2_16&partnerID=40&md5=20b065693c3200ff64e3f1205ae5f728","Event detection is a very important problem across many domains and is a broadly applicable encompassing many disciplines within engineering systems. In this paper, we focus on improving the user’s ability to quickly identify threat events such as malware, military policy violations, and natural environmental disasters. The information to perform these detections is extracted from text data sets in the latter two cases. Malware threats are important as they compromise computer system integrity and potentially allow the collection of sensitive information. Military policy violations such as ceasefire policies are important to monitor as they disrupt the daily lives of many people within countries that are torn apart by social violence or civil war. The threat of environmental disasters takes many forms and is an ever-present danger worldwide, and indiscriminate regarding who is harmed or killed. In this paper, we address all three of these threat event types using the same underlying technology for mining the information that leads to detecting such events. We approach malware event detection as a binary classification problem, i.e., one class for the threat mode and another for non-threat mode. We extend our novel classifier utilizing constrained low rank approximation as the core algorithm innovation and apply our Nonnegative Generalized Moody-Darken Architecture (NGMDA) hybrid method using various combinations of input and output layer algorithms. The new algorithm uses a nonconvex optimization problem via the nonnegative matrix factorization (NMF) for the hidden layer of a single layer perceptron and a nonnegative constrained adaptive filter for the output layer estimator. We first show the utility of the core NMF technology for both ceasefire violation and environmental disaster event detection. Next NGMDA is applied to the problem of malware threat events, again based on the NMF as the core computational tool. Also, we demonstrate that an algorithm should be appropriately selected for the data generation process. All this has critical implications for design of solutions for important threat/event detection scenarios. Lastly, we present experimental results on foreign language text for ceasefire violation and environmental disaster events. Experimental results on a KDD competition data set for malware classification are presented using our new NGMDA classifier. © Springer International Publishing AG 2018.","Adaptive filtering; Classification; Clustering; Event detection; Hybrid classifier; Malware detection; Nonnegative matrix factorization; Perceptron; Topic modeling","Adaptive filtering; Adaptive filters; Approximation algorithms; Classification (of information); Computer crime; Constrained optimization; Data mining; Disasters; Environmental technology; Factorization; Filtration; Human engineering; Malware; Neural networks; Optimization; Clustering; Event detection; Hybrid classifier; Malware detection; Nonnegative matrix factorization; Topic Modeling; Matrix algebra",2-s2.0-85021658769
"Huang N.-F., Lee C.-A., Huang Y.-W., Ou P.-W., Hsu H.-H., Chen S.-C., Tzengßer J.-W.","On the automatic construction of knowledge-map from handouts for MOOC courses",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026728498&doi=10.1007%2f978-3-319-63856-0_13&partnerID=40&md5=2345d9aec648fd3786d1643a308b20f3","Massive open online courses (MOOCs) offer valuable opportunities for freedom in learning; however, many learners face cognitive overload and conceptual and navigational disorientation. In this study, we used handouts to automatically build domain-specific knowledge maps for MOOCs. We considered handouts as conceptual models created by teachers, and we performed text mining to extract keywords from MOOC handouts. Each knowlege map is based on the structure of the handouts, each consisting of an outline, title, and content. The findings suggest that using handouts to build knowledge maps is feasible. © Springer International Publishing AG 2018.","Knowledge maps; Learning styles; Massive open online courses; Open learning","Data mining; E-learning; Education; Knowledge engineering; Multimedia signal processing; Natural language processing systems; Teaching; Automatic construction; Cognitive overload; Conceptual model; Domain-specific knowledge; Knowledge map; Learning Style; Massive open online course; Open learning; Signal processing",2-s2.0-85026728498
"Polig R., Atasu K., Giefers H., Hagleitner C., Chiticariu L., Reiss F., Zhu H., Hofstee P.","A hardware compilation framework for text analytics queries",2018,"Journal of Parallel and Distributed Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021130573&doi=10.1016%2fj.jpdc.2017.05.015&partnerID=40&md5=ad693120e2734fad6831a9eb511e9e33","Unstructured text data is being generated at an unprecedented rate in the form of Twitter feeds, machine logs or medical records. The analysis of this data is an important step to gaining significant insight regarding innovation, security and decision-making. The performance of traditional compute systems struggles to keep up with the rapid data growth and the expected high quality of information extraction. To cope with this situation, a compilation framework is presented that can transform text analytics queries into a hardware description. Deployed on an FPGA, the queries can be executed 60 times faster on average compared to a multi-threaded software implementation. The performance has been evaluated on two generations of high-end server systems including two generations of FPGAs, demonstrating the performance gains from advanced technology. © 2017 Elsevier Inc.","Accelerator; FPGA; Query compilation; Text analytics","Computer hardware; Field programmable gate arrays (FPGA); Hardware; Particle accelerators; Advanced technology; Hardware compilation; Hardware descriptions; Multithreaded softwares; Performance Gain; Query compilation; Text analytics; Unstructured texts; Data mining",2-s2.0-85021130573
"Dubey R.K., Pandey U.K.","Identifying student for customized tutoring",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412087&doi=10.1007%2f978-3-319-63645-0_70&partnerID=40&md5=141d7872d63a59a4fef292279e0ad51f","Improving quality of student is related with increasing their learning and skills in the concerned field. The benchmark used to measure the learning and skill is obtained score of student and participation in various activities. During a session, this session may be a semester in short term and in longer term it may be program duration, student generates huge size of data. If this data is studied properly then learning and skill easily grouped. After knowing a person’s weakest zone a customized solution will provide to nurture him/her in required area of development. In this paper data will be studied to identify weaker student so that customized tutoring offered to him/her. © Springer International Publishing AG 2018.","Clustering; Customized tutoring; Data mining; k-means","Data mining; Education; Intelligent systems; Clustering; Customized solutions; Customized tutoring; K-means; Short term; Students",2-s2.0-85028412087
"Praveenkumar T., Sabhrish B., Saimurugan M., Ramachandran K.I.","Pattern recognition based on-line vibration monitoring system for fault diagnosis of automobile gearbox",2018,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541323&doi=10.1016%2fj.measurement.2017.09.041&partnerID=40&md5=b7f6b1a00fb15e31f48e13a3250898a0","Gearbox is an important equipment in an automobile to transfer power from the engine to the wheels with various speed ratios. The maintenance of the gearbox is a top criterion as it is prone to a number of failures like tooth breakage and bearing cracks. Techniques like vibration monitoring have been implemented for the fault diagnosis of the gearbox over the years. But, the experiments are usually conducted in lab environment where the actual conditions are simulated using setup consisting of an electric motor, dynamometer, etc. This work reports the feasibility of performing vibrational monitoring in real world conditions, i.e. by running the vehicle on road and performing the analysis. The data was acquired for the various conditions of the gearbox and features were extracted from the time-domain data and a decision tree was trained for the time-domain analysis. Fast Fourier Transform was performed to obtain the frequency domain which was divided into segments of equal size and the area covered by the data in each segment was calculated for every segment to train decision trees. The classification efficiencies of the decision trees were obtained and in an attempt to improve the classification efficiencies, the time-domain and frequency-domain analysis was also performed on the normalised time-domain data. From, the results obtained, it was found that performing time-domain analysis on normalised data had a higher efficiency when compared with the other methods. Instantaneous processing of the acquired data from the accelerometer enables faster diagnosis. Hence, online condition monitoring has gained importance with the advent of powerful microprocessors. A windows application that has been developed to automate the process was found to be essential and accurate. © 2017","Automobile gearbox; Frequency-domain; On-road; Pattern recognition; Real-time; Time-domain; Vibration monitoring","Automobiles; Condition monitoring; Data mining; Decision trees; Efficiency; Failure analysis; Fast Fourier transforms; Fault detection; Frequency domain analysis; Gears; Pattern recognition; Pattern recognition systems; Roads and streets; Trees (mathematics); Vibration analysis; Vibration measurement; Vibrations (mechanical); Automobile gearbox; Frequency domains; On-road; Real time; Time domain; Vibration monitoring; Time domain analysis",2-s2.0-85030541323
"Trinh S., Nguyen L., Vo M.","Combining lexicon-based and learning-based methods for sentiment analysis for product reviews in Vietnamese language",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020456695&doi=10.1007%2f978-3-319-60170-0_5&partnerID=40&md5=b8d81bf8c8bd61878f9e56e2ae5a1f3c","Social media websites are a major hub for users to express their opinions online. Businesses spend an enormous amount of time and money to understand their customer opinions about their products and services. Sentiment analysis which is also called opinion mining, involves in building a system to collect and examine opinions about the product made in blog posts, comments, or reviews. In this paper, we propose a framework for sentiment analysis based on combining lexicon-based and learning-based methods for product review sentiment analysis in Vietnamese language. Text analytics, Linguistic analysis and Vietnamese emotional dictionary were built, proposing features which adapted with the language was proposed. The experimental show that our system has very well performance when combine advantage of lexicon-based and learning based and can be applied in online systems for sentiment analysis product reviews. © Springer International Publishing AG 2018.","Learning-based; Lexicon-based; Linguistic analysis; Product review; Proposing features; Sentiment analysis; Text analytics; Vietnamese; Vietnamese emotional dictionary","Linguistics; Online systems; Learning-based; Lexicon-based; Linguistic analysis; Product reviews; Proposing features; Sentiment analysis; Text analytics; Vietnamese; Data mining",2-s2.0-85020456695
"Hora K.","Classifying exoplanets as potentially habitable using machine learning",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031398516&doi=10.1007%2f978-981-10-6602-3_20&partnerID=40&md5=3f60aac209445a097d716c3b57dcc0d0","With the launch of the ESA Gaia satellite observatory and the planned LSST, and the torrent of data coming through the Kepler space observatory, scientists will be able to collect data for more than 1 billion astronomical objects, including millions of exoplanets in the coming years. In this study, several predictive models are built using machine learning algorithms to classify exoplanets as potentially habitable, based on various characteristics of the planet and its star. I applied six supervised learning algorithms for the classification of planets, which include two decision trees, CART and Random Forest, Support Vector Machines, Logistic Regression, Feed-Forward Neural Network, and Naïve Bayes. I further applied CART to create a regression model to predict the value of the ESI (Earth Similarity Index) for an exoplanet. © 2018, Springer Nature Singapore Pte Ltd.","Artificial neural networks; Data mining; Decision trees; Logarithmic regression; Naïve bayes; Random forests; Support vector machines","Artificial intelligence; Data mining; Decision trees; Extrasolar planets; Forestry; Learning systems; Neural networks; Observatories; Regression analysis; Satellites; Sodium; Support vector machines; Astronomical objects; Logarithmic regression; Logistic regressions; Predictive models; Random forests; Regression model; Similarity indices; Space observatories; Learning algorithms",2-s2.0-85031398516
"Islam M.Z., Estivill-Castro V., Rahman M.A., Bossomaier T.","Combining K-MEANS and a genetic algorithm through a novel arrangement of genetic operators for high quality clustering",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029490496&doi=10.1016%2fj.eswa.2017.09.005&partnerID=40&md5=5c4d8c6906f85879015ff37dea613a78","Knowledge discovery from data can be broadly categorized into two types: supervised and unsupervised. A supervised knowledge discovery process such as classification by decision trees typically requires class labels which are sometimes unavailable in datasets. Unsupervised knowledge discovery techniques such as an unsupervised clustering technique can handle datasets without class labels. They aim to let data reveal the groups (i.e. the data elements in each group) and the number of groups. For the ubiquitous task of clustering, K-MEANS is the most used algorithm applied in a broad range of areas to identify groups where intra-group distances are much smaller than inter-group distances. As a representative-based clustering approach, K-MEANS offers an extremely efficient gradient descent approach to the total squared error of representation; however, it not only demands the parameter k, but it also makes assumptions about the similarity of density among the clusters. Therefore, it is profoundly affected by noise. Perhaps more seriously, it can often be attracted to local optima despite its immersion in a multi-start scheme. We present an effective genetic algorithm that combines the capacity of genetic operators to conglomerate different solutions of the search space with the exploitation of the hill-climber. We advance a previous genetic-searching approach called GENCLUST, with the intervention of fast hill-climbing cycles of K-MEANS and obtain an algorithm that is faster than its predecessor and achieves clustering results of higher quality. We demonstrate this across a series of 18 commonly researched datasets. © 2017 Elsevier Ltd","Cluster evaluation; Clustering; Data mining; Genetic algorithm; K-MEANS","Classification (of information); Data mining; Decision trees; Distributed computer systems; Genetic algorithms; Cluster evaluations; Clustering; Clustering results; K-means; Knowledge discovery process; Knowledge discovery techniques; Total squared errors; Unsupervised clustering technique; Clustering algorithms",2-s2.0-85029490496
"Guo G., Wang C., Chen J., Ge P.","Who Is Answering to Whom? Finding “Reply-To” Relations in Group Chats with Long Short-Term Memory Networks",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032462041&doi=10.1007%2f978-981-10-6520-0_17&partnerID=40&md5=c968309adbc5d7e60e696760f01b9e62","Social networks enjoy great popularity among Internet users while generating large volumes of online short-text conversations every day. It leads to a huge number of free-style asynchronous conversations where multiple users are involved and multiple topics are discussed at the same time in the same place, e.g., an instant group chat in WeChat. Here emerges an interesting problem: As a result of a large number of users and topics, the conversation structure may get into a mess, which interferes with our access to the messages we are interested in. For example, when we open the chat records, we do not want to read all the historical messages. We just want to get the messages that are the most relevant with the messages we care about. Therefore, it is an essential task to understand the logical correlations among messages, which benefits the text mining, the natural language processing and the web intelligence techniques. In this paper, we present the concept of “reply-to” relations to capture most kinds of logical correlations between messages, such as Q&A or complement. Also, we propose a model called LSTM-RT to predict the “reply-to” relations between messages, which is based on the high-quality vector representations of words and LSTM networks. In addition, we give two versions of LSTM-RT based on word level and sentence level, respectively. Experiments conducted on two real-world group chat datasets demonstrate the effectiveness of our proposed models. © 2018, Springer Nature Singapore Pte Ltd.","Group chat; LSTM network; “reply-to” Relations","Data mining; Natural language processing systems; Group chat; High quality; Internet users; Large volumes; Multiple user; Sentence level; Short term memory; Web intelligence; Long short-term memory",2-s2.0-85032462041
"Silva S., Costa P., Gouvea M., Lacerda A., Alves F., Leite D.","High impedance fault detection in power distribution systems using wavelet transform and evolving neural network",2018,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029810257&doi=10.1016%2fj.epsr.2017.08.039&partnerID=40&md5=5b57771efe5f0b87f9ac75dc8f9ef5a6","This paper concerns how to apply an incremental learning algorithm based on data streams to detect high impedance faults in power distribution systems. A feature extraction method, based on a discrete wavelet transform that is combined with an evolving neural network, is used to recognize spatial–temporal patterns of electrical current data. Different wavelet families, such as Haar, Symlet, Daubechie, Coiflet and Biorthogonal, and different decomposition levels, were investigated in order to provide the most discriminative features for fault detection. The use of an evolving neural network was shown to be a quite appropriate approach to fault detection since high impedance faults is a time-varying problem. The performance of the proposed evolving system for detecting and classifying faults was compared with those of well-established computational intelligence methods: multilayer perceptron neural network, probabilistic neural network, and support vector machine. The results showed that the proposed system is efficient and robust to changes. A classification performance in the order of 99% is exhibited by all classifiers in situations where the fault patterns do not significantly change during tests. However, a performance drop of about 13–24% is exhibited by non-evolving classifiers when fault patterns suffer from gradual or abrupt change in their behavior. The evolving system is capable, after incremental learning, of maintaining its detection and classification performance even in such situations. © 2017 Elsevier B.V.","Evolving neural network; High impedance fault detection; Pattern recognition; Power distribution system; Wavelet transform","Data mining; Discrete wavelet transforms; Electric fault currents; Electric fault location; Electric power system protection; Feature extraction; Learning algorithms; Neural networks; Pattern recognition; Pattern recognition systems; Wavelet decomposition; Wavelet transforms; Classification performance; Computational intelligence methods; Evolving neural network; Feature extraction methods; High impedance fault detection; Multi-layer perceptron neural networks; Power distribution system; Probabilistic neural networks; Fault detection",2-s2.0-85029810257
"Nabwey H.A.","A hybrid approach for extracting classification rules based on rough set methodology and fuzzy inference system and its application in groundwater quality assessment",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029422029&doi=10.1007%2f978-3-319-66824-6_54&partnerID=40&md5=75cb8d727a6b290ba0b5a15aab79e13c","This work proposes a hybrid approach based on rough set methodology and fuzzy inference system, for extraction of classification rules. Rough set methodology was used to eliminate abundant information of chemical indices while maintaining a lesser degree of information loss from the raw data; then the relationships among the variables under consideration were created using the fuzzy inference system which takes into consideration the experience of the decision-maker during fitting the membership functions. An application to assessment of groundwater quality is also given. Results demonstrate that the proposed approach has the ability to deal with the nonlinearity of the variables, as well as it deal with the highly subjective nature of the variables with highest degree of efficiency and so it can be used for finding the solution of multiple criteria decision making problems. © 2018, Springer International Publishing AG.","Attribute reduction; Data mining; Fuzzy inference system; Groundwater quality assessment; Rough sets theory","Data mining; Decision making; Fuzzy logic; Fuzzy sets; Fuzzy systems; Groundwater; Membership functions; Pattern matching; Rough set theory; Water quality; Attribute reduction; Chemical indices; Classification rules; Fuzzy inference systems; Groundwater quality assessment; Information loss; ITS applications; Multiple criteria decision-making problems; Fuzzy inference",2-s2.0-85029422029
"Fatkulin T., Butakov N., Dzhafarov B., Petrov M., Voloshin D.","An approach to location extraction from russian online social networks: Road accidents use case",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028668109&doi=10.1007%2f978-3-319-67180-2_14&partnerID=40&md5=0bdc14068affd246425ce896fb8ac617","Modern cities are a subject for various threats like terrorist attacks or natural disasters. Effective response on them requires fast delivering of information as close as possible to the source of events. Online social networks can play a role of monitoring system for such kind events with its users as particular sensors. But to exploit such a system one requires to have capabilities to process noisy, distorted data where desired information represented as compound entities scattered across the text of users’ messages, consisting of specific keywords, location names and service words heavily affected by usage styles of online social networks. This paper presents effective approach to handle with the problem of compound location extraction from online social networks in Russian language. © 2018, Springer International Publishing AG.","Data mining; Decision support; Neuro computing; Social sensing","Data mining; Decision support systems; Disasters; Extraction; Highway accidents; Location; Soft computing; Decision supports; Effective approaches; Monitoring system; Neurocomputing; On-line social networks; Russian languages; Social sensing; Terrorist attacks; Social networking (online)",2-s2.0-85028668109
"Patsadu O., Watanapa B., Nukoolkit C.","A multiple-stage classification of fall motions using kinect camera",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022188800&doi=10.1007%2f978-3-319-60663-7_11&partnerID=40&md5=190415f79f27a2b45bf4487c391a46bd","This paper proposes a model of fall detection using hybrid classification methods in video streaming. In particular, we are interested in a stream of data representing time sequential frames of fifteen body joint positions capturable by Kinect camera. A set of features is then extracted and fed into the designated multiple-stage classification. The first stage classifies a fall as a different event from normal activities of daily living (ADLs). The second stage is to classify types of fall once the fall was detected in the first stage, for aiding the diagnosis and treatment of a fall by a physician. We selected a number of reliable machine learning algorithms (MLP, SVM, and decision tree) in forming a hybrid model. Experimental results show that the first stage classifier can differentiate falls and ADLs with 99.98% accuracy and the second stage classifier can identify type of fall with 99.35% accuracy. © Springer International Publishing AG 2018.","Fall detection; Hybrid classification methods; Kinect camera; Multiple-stage classifier; Smart home system","Automation; Cameras; Decision trees; Intelligent buildings; Learning algorithms; Motion analysis; Video streaming; Fall detection; Hybrid classification; Kinect cameras; Multiple stages; Smart-home system; Data mining",2-s2.0-85022188800
"Morente-Molinera J.A., Pérez I.J., Cabrerizo F.J., Alonso S., Herrera-Viedma E.","Using group decision making methods to extract experts knowledge",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029420385&doi=10.1007%2f978-3-319-66824-6_50&partnerID=40&md5=19ca7622feaf745190179d6dc893ef01","Group Decision Making methods are interesting tools that can be used for extracting subjective information from experts that are familiar with an specific topic. Although they are typically used for carrying out an alternatives ranking, a novel approach that can extract information from the experts and store it in a fuzzy ontology is presented in this paper. Thanks to this, it is possible to create a knowledge database using subjective information extracted from experts in the dealt topic. This database can be helpful for other users that are interested in the stored data. The obtained information is stored in a fuzzy ontology due to its capacity of using queries to access the information and because they are able to handle the information in a organized way. © 2018, Springer International Publishing AG.","Fuzzy ontologies; Group decision making; Linguistic modelling; Soft computing","Computation theory; Data mining; Fuzzy logic; Fuzzy sets; Ontology; Pattern matching; Soft computing; Extract informations; Fuzzy ontology; Group Decision Making; Knowledge database; Linguistic Modelling; Subjective information; Decision making",2-s2.0-85029420385
"Nguyen T.T., Nguyen M.P., Pham X.C., Liew A.W.-C.","Heterogeneous classifier ensemble with fuzzy rule-based meta learner",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029004946&doi=10.1016%2fj.ins.2017.09.009&partnerID=40&md5=2340894899ff5296644690b848d72eb1","In heterogeneous ensemble systems, each learning algorithm learns a classifier on a given training set to describe the relationship between a feature vector and its class label. As each classifier outputs different result on an observation, uncertainty is introduced. In this paper, we introduce a heterogeneous ensemble system with a fuzzy IF-THEN rule inference engine as the combiner to capture the uncertainty in the outputs of the base classifiers. In our method, fuzzy rules are generated on the outputs of an ensemble of base classifiers, which can be viewed as the class posterior probability of the observations. The performance of our method was evaluated on thirty datasets and in comparison with nine ensemble methods (AdaBoost, Decision Template, Decision Tree on meta-data, and six fixed combiners) and two single learning algorithms (SVM with L2-loss function and Decision Tree), and was shown to significantly outperforms these algorithms in terms of classification accuracy. © 2017 Elsevier Inc.",,"Adaptive boosting; Classification (of information); Data mining; Decision trees; Fuzzy inference; Fuzzy rules; Trees (mathematics); Base classifiers; Classification accuracy; Decision template; Fuzzy if-then rules; Fuzzy rule based; Heterogeneous classifiers; Heterogeneous ensembles; Posterior probability; Learning algorithms",2-s2.0-85029004946
"Lin Z., Du J., Li Y., Ye L., Luo A.","Scenic negative comment clustering based on balance weighted comment topic model",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030838127&doi=10.1007%2f978-981-10-6496-8_28&partnerID=40&md5=c0e9ee889c4c15d3495768e34da24cc3","The scenic comment information from visitors often hidden the different aspects of the recommendations and expectations of the attractions, the extraction of these key information will help the spot managers find their own shortcomings and improve themselves. In this paper, we improved the author topic model and proposed a model of clustering the negative comments of the scenic spots. There are two improvements from our proposed model. Firstly, we added the importance of the comment category to the text clustering. Secondly, in order to prevent the stop words accumulating in the sampling process, we introduced the balance weight to the proposed model. Experiments showed that the model could not only effectively cluster these data, but also could extract the rich information related to different comment categories from the clustering results, which could help the managers of the scenic spots to better manage the attractions and attract tourists. © 2018, Springer Nature Singapore Pte Ltd.","Clustering analysis; Data mining; Scenic negative comment; Topic model","Clustering algorithms; Intelligent systems; Managers; Clustering analysis; Clustering results; Sampling process; Scenic negative comment; Scenic spot; Stop word; Text Clustering; Topic Modeling; Data mining",2-s2.0-85030838127
"Kilingaru K., Nedic Z., Tweedale J., Thatcher S.","Smart evaluation of instrument scan pattern using state transition model during flight simulator training",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020405350&doi=10.1007%2f978-3-319-59451-4_29&partnerID=40&md5=ee962eb1b742efb27aeb51c26da560b6","Trainee pilots are expected to be thoroughly trained on both technical and nontechnical skills. Technical skills are relatively easy to evaluate. However, non-technical skills are hard to monitor and assess. This study investigates the feasibility of providing a smart evaluation technique that can generate feedback on trainee pilots’ instrument scan behavior. The authors conducted gaze monitoring experiments with a number of trainee pilots while operating a flight simulator in order to isolate patterns of behavior associated with Situation Awareness (SA). Recorded data on eye movements are then processed and transitions are extracted using a state transition model. The sequence of transitions are then processed to retrieve repeated instrument scan pattern. The results are verified using chart visualizations. © Springer International Publishing AG 2018.","Flight simulator; Instrument scan pattern; Knowledge discovery; Pilot training; Situation awareness; Smart training","Data mining; E-learning; Eye movements; Flight simulators; Simulators; Non-technical skills; Pilot training; Scan patterns; Simulator training; Situation awareness; State transition models; Technical skills; Personnel training",2-s2.0-85020405350
"Allaf Z., Adda M., Gegov A.","A Comparison Study on Flush+Reload and Prime+Probe Attacks on AES Using Machine Learning Approaches",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029600557&doi=10.1007%2f978-3-319-66939-7_17&partnerID=40&md5=47e5a7d2dbc0f99cb191010a81fd20bb","AES, ElGamal are two examples of algorithms that have been developed in cryptography to protect data in a variety of domains including native and cloud systems, and mobile applications. There has been a good deal of research into the use of side channel attacks on these algorithms. This work has conducted an experiment to detect malicious loops inside Flush+Reload and Prime+Prob attack programs against AES through the exploitation of Hardware Performance Counters (HPC). This paper examines the accuracy and efficiency of three machine learning algorithms: Neural Network (NN); Decision Tree C4.5; and K Nearest Neighbours (KNN). The study also shows how Standard Performance Evaluation Corporation (SPEC) CPU2006 benchmarks impact predictions. © 2018, Springer International Publishing AG.","AES; Flush+Reload; Machine learning; Prime+Probe; Side-channel attack","Artificial intelligence; Benchmarking; Cryptography; Data mining; Decision trees; Learning algorithms; Learning systems; Nearest neighbor search; Probes; Comparison study; Flush+Reload; Hardware performance counters; K nearest neighbours (k-NN); Machine learning approaches; Mobile applications; Neural network (nn); Standard performance; Side channel attack",2-s2.0-85029600557
"Zhao Y., Altman K., Chaudhury K., Anandika M., Smidts C.","A systematic method to build a knowledge base to be used in a human reliability analysis model",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023184069&doi=10.1007%2f978-3-319-60645-3_6&partnerID=40&md5=b958200bd1125969b1daa38e63ec5587","A human’s knowledge base is a key component for the development of a mechanistic model of human response to be used for human reliability analysis. This paper proposes a new method for constructing this knowledge base. The proposed method is comprised of three steps: (1) systematic literature review, which is used to collect data pertinent to the subject under study; (2) summarization, the goal of which is to extract key points that are expressed in the literature; (3) qualitative coding, a process in which codes closely related to the topic are derived and the relationships between these codes are expressed. As a case study, the proposed method is being applied to construct an operator’s knowledge base concerning severe accident phenomenology in a nuclear power plant. Part of this application is explored in this paper. With the proposed method and the resulting knowledge base, it is expected that an individual’s response when presented with a specific context can be modeled in more detail. © Her Majesty the Queen in Right of United Kingdom 2018.","Human reliability analysis; Knowledge base; Knowledge representation; Mechanistic model; Severe nuclear accident","Accidents; Codes (symbols); Data mining; Errors; Knowledge based systems; Knowledge representation; Nuclear fuels; Nuclear power plants; Nuclear reactor accidents; Reliability; Human reliability analysis; Human response; Knowledge base; Mechanistic modeling; Severe accident; Severe nuclear accident; Systematic literature review; Systematic method; Reliability analysis",2-s2.0-85023184069
"Lajmert P., Sikora M., Kruszynski B., Ostrowski D.","Application of principal component analysis and decision trees in diagnostics of cylindrical plunge grinding process",2018,"Lecture Notes in Mechanical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032699645&doi=10.1007%2f978-3-319-68619-6_68&partnerID=40&md5=3fdea270a83f4d18aa07df57b5ef8215","In the grinding process, information about the process state may be derived from many measurement signals. As a result of these signals preprocessing, it is possible to obtain a high number of features of which only a part is related to the monitored process. This paper deals with the feature selection problem and modeling of relationships of selected features with grinding process states and grinding results. Firstly, time–frequency signal processing techniques are analyzed. Using the Hilbert-Huang transform, force, vibration, and acoustic emission signals are decomposed into separate intrinsic mode functions, and then the statistical features are extracted from these functions. Next, principal component analysis is used to select the most relevant features and to remove redundant data. Finally, decision trees are applied to additionally decrease the number of features and to model the grinding process. Using the proposed approach, it is possible to automate the feature selection process and to effectively diagnose the process state and predict final part quality parameters. © Springer International Publishing AG 2018.","Classification; Data mining; Diagnostics; Plunge grinding",,2-s2.0-85032699645
"Ma H., Wang Y., Wang K.","Automatic detection of false positive RFID readings using machine learning algorithms",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029489992&doi=10.1016%2fj.eswa.2017.09.021&partnerID=40&md5=330aa10e4b9a268753a2f7a356018754","Radio frequency identification (RFID) has been widely used for the automatic identification, tracking and tracing of goods throughout the supply chain from the manufacturer to the customer. However, one technological problem that impedes the productive and reliable use of RFID is the constraint of false positive readings, which refers to tags that are detected accidentally by the reader but not the ones of interest. This paper focuses on the use of machine learning algorithms to identify such RFID readings. A total of 11 statistical features are extracted from received signal strength (RSS) and phase rotations derived from the raw RFID data. Each of the features is highly statistically different to distinguish the false positive readings, but satisfactory classification cannot be achieved when these features are considered individually. Classifiers based on logistic regression (LR), support vector machine (SVM) and decision tree (DT) are constructed, which combine all of the extracted features to classify the RFID readings more effectively. The performance of the classifiers is evaluated in a real-world factory. Results show that SVM provides the highest accuracy of up to 95.3%. DT shows slightly better accuracy (92.85%) than LR (92.75%), while LR has the larger area under the curve (0.976) than DT (0.949). Overall, machine learning algorithms could achieve accuracy of 93% on average. The proposed methodology provides a much more reliable RFID application as false-positive readings are detected immediately without human intervention, which enables a significant potential of fully automatic identification and tracking of goods throughout the supply chain. © 2017 Elsevier Ltd","Classification; False positive readings; Machine learning; RFID","Artificial intelligence; Automation; Classification (of information); Data mining; Decision trees; Learning systems; Radio frequency identification (RFID); Supply chains; Support vector machines; Area under the curves; Automatic Detection; Automatic identification; False positive; Logistic regressions; Received signal strength; Statistical features; Tracking and tracing; Learning algorithms",2-s2.0-85029489992
"Wang C., Yang S., Liu C.-M., Jiang T.-T., Chen Z.-L., Tu H.-H., Mao L.-G., Li Z.-J., Li J.-C.","Screening and identification of four serum miRNAs as novel potential biomarkers for cured pulmonary tuberculosis",2018,"Tuberculosis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031800003&doi=10.1016%2fj.tube.2017.08.010&partnerID=40&md5=9182b60d2f6b5a033cfdb9a8ef0bffb1","Rapid and efficient methods for the determination of cured pulmonary tuberculosis (TB) are lacking. We screened serum miRNAs using the Solexa sequencing method among untreated TB patients, two-month treated TB patients, cured TB patients, and healthy controls. A total of 100 differentially expressed miRNAs were identified in cured TB patients, including 37 up-regulated (fold change >1.50, P < 0.05) and 63 down-regulated (fold change <0.60, P < 0.05) miRNAs. Gene ontology (GO) enrichment analysis revealed that most of the predicted genes were present in the nucleus with a strong protein binding function. Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis strongly suggested alterations in the metabolic pathways. Following quantitative real time chain reaction (qRT-PCR), significantly reduced expression levels of miR-21-5p (0.30, P < 0.001), miR-92a-3p (0.63, P < 0.001), and miR-148b-3p (0.17, P < 0.001) were found in the cured TB patients compared with the untreated TB patients, while significantly increased expression levels of miR-21-5p (2.09, P = 0.001), miR-92a-3p (1.40, P = 0.005), and miR-148b-3p (4.80, P = 0.003) were found in the untreated TB patients compared with the healthy controls. And significantly increased level of miR-125a-5p was found between two-month treated TB patients and untreated TB patients (1.81, P = 0.004). We established a cured TB model with 83.96% accuracy by four miRNAs (miR-21-5p, miR-92a-3p, miR-148b-3p, and miR-125a-5p), and also established a diagnostic model with 70.09% accuracy. Our study provides experimental data for establishing objective indicators of cured TB, and also provides a new experimental basis to understand the pathogenesis and prognosis of TB. © 2017 Elsevier Ltd","Biomarkers; Efficacy evaluation; Pulmonary tuberculosis; Solexa sequencing","biological marker; ethambutol; isoniazid; microRNA; microRNA 125a 5p; microRNA 148b 3p; microRNA 21 5p; microRNA 92a 3p; pyrazinamide; rifampicin; unclassified drug; adult; aged; Article; bioinformatics; blood level; case control study; controlled study; data mining; down regulation; drug efficacy; female; gene expression regulation; gene function; gene ontology; human; lung tuberculosis; major clinical study; male; measurement accuracy; metabolism; priority journal; protein binding; quantitative analysis; real time polymerase chain reaction; RNA sequence; screening; Solexa sequencing; transcriptomics; treatment outcome; upregulation",2-s2.0-85031800003
"Anderson K.E., Glenn N.F., Spaete L.P., Shinneman D.J., Pilliod D.S., Arkle R.S., McIlroy S.K., Derryberry D.R.","Estimating vegetation biomass and cover across large plots in shrub and grass dominated drylands using terrestrial lidar and machine learning",2018,"Ecological Indicators",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030456606&doi=10.1016%2fj.ecolind.2017.09.034&partnerID=40&md5=2a2219373f5e5114ae9d1235f48b53f4","Terrestrial laser scanning (TLS) has been shown to enable an efficient, precise, and non-destructive inventory of vegetation structure at ranges up to hundreds of meters. We developed a method that leverages TLS collections with machine learning techniques to model and map canopy cover and biomass of several classes of short-stature vegetation across large plots. We collected high-definition TLS scans of 26 1-ha plots in desert grasslands and big sagebrush shrublands in southwest Idaho, USA. We used the Random Forests machine learning algorithm to develop decision tree models predicting the biomass and canopy cover of several vegetation classes from statistical descriptors of the aboveground heights of TLS points. Manual measurements of vegetation characteristics collected within each plot served as training and validation data. Models based on five or fewer TLS descriptors of vegetation heights were developed to predict the canopy cover fraction of shrubs (R2 = 0.77, RMSE = 7%), annual grasses (R2 = 0.70, RMSE = 21%), perennial grasses (R2 = 0.36, RMSE = 12%), forbs (R2 = 0.52, RMSE = 6%), bare earth or litter (R2 = 0.49, RMSE = 19%), and the biomass of shrubs (R2 = 0.71, RMSE = 175 g) and herbaceous vegetation (R2 = 0.61, RMSE = 99 g) (all values reported are out-of-bag). Our models explained much of the variability between predictions and manual measurements, and yet we expect that future applications could produce even better results by reducing some of the methodological sources of error that we encountered. Our work demonstrates how TLS can be used efficiently to extend manual measurement of vegetation characteristics from small to large plots in grasslands and shrublands, with potential application to other similarly structured ecosystems. Our method shows that vegetation structural characteristics can be modeled without classifying and delineating individual plants, a challenging and time-consuming step common in previous methods applying TLS to vegetation inventory. Improving application of TLS to studies of shrub-steppe ecosystems will serve immediate management needs by enhancing vegetation inventories, environmental modeling studies, and the ability to train broader datasets collected from air and space. © 2017 Elsevier Ltd","Biomass; Carbon; Classification; Land cover; Lidar; Machine learning; Point cloud; Rangelands; Remote sensing; Structure from motion (SfM); Vegetation type","Artificial intelligence; Biomass; Carbon; Classification (of information); Data mining; Decision trees; Ecology; Ecosystems; Forecasting; Learning algorithms; Learning systems; Optical radar; Plants (botany); Remote sensing; Seebeck effect; Surveying instruments; Land cover; Point cloud; Rangelands; Structure from motion; Vegetation type; Vegetation; algorithm; environmental modeling; grass; lidar; machine learning; phytomass; shrub; shrubland; vegetation cover; vegetation structure; Idaho; United States; Artemisia tridentata; Poaceae",2-s2.0-85030456606
"Muthukumaran K., Dasgupta A., Abhidnya S., Neti L.B.M.","On the effectiveness of cost sensitive neural networks for software defect prediction",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028598796&doi=10.1007%2f978-3-319-60618-7_55&partnerID=40&md5=7d2b78371d8de6737b1e4f4011576a1d","The cost of fixing a software defect varies with the phase in which it is uncovered. Defect found during post-release phase costs much more than the defect that is uncovered in pre-release phase. Hence defect prediction models have been proposed to predict bugs in pre-release phase. For any prediction model, there are two kinds of misclassification errors - Type I and Type II errors. Type II errors are found to be more costly than Type I errors for defect prediction problem. However there have been only few studies that have considered misclassifications costs while building or evaluating defect predictions models. We have built classification models using three cost-sensitive boosting Neural Network methods, namely, CSBNN-TM, CSBNN-WU1 and CSBNN-WU2. We have compared the performance of these cost sensitive Neural Networks with the traditional machine learning algorithms like Logistic Regression, Naive Bayes, Random Forest, Bayesian Network, Neural Networks, k-Nearest Neighbors and Decision Tree. We have compared the performance of the resultant models using cost centric measure - Normalized Expected Cost of Misclassification (NECM). © Springer International Publishing AG 2018.","Cost-sensitive neural networks; Misclassification cost; Software defect prediction","Bayesian networks; Data mining; Decision trees; Defects; Errors; Forecasting; Learning algorithms; Learning systems; Nearest neighbor search; Pattern recognition; Soft computing; Classification models; Cost-sensitive neural networks; Defect prediction models; Expected cost of misclassification; Misclassification costs; Misclassification error; Software defect prediction; Type I and type II errors; Costs",2-s2.0-85028598796
"Jabreel M., Hassan F., Moreno A.","Target-dependent sentiment analysis of tweets using bidirectional gated recurrent neural networks",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340873&doi=10.1007%2f978-3-319-66790-4_3&partnerID=40&md5=67209f7eec457255ee01d42cc7be5663","The task of target-dependent sentiment analysis aims to identify the sentiment polarity towards a certain target in a given text. All the existing models of this task assume that the target is known. This fact has motivated us to develop an end-to-end target-dependent sentiment analysis system. To the extent of our knowledge, this is the first system that identifies and extract the target of the tweets. The proposed system is composed of two main steps. First, the targets of the tweet to be analysed are extracted. Afterwards, the system identifies the polarities of the tweet towards each extracted target. We have evaluated the effectiveness of the proposed model on a benchmark dataset from Twitter. The experiments show that our proposed system outperforms the state-of-the-are methods for target-dependent sentiment analysis. © 2018, Springer International Publishing AG.",,"Data mining; Benchmark datasets; End to end; First systems; Sentiment analysis; Recurrent neural networks",2-s2.0-85032340873
"Al-Obeidat F., Kafeza E., Spencer B.","Opinions Sandbox: Turning Emotions on Topics into Actionable Analytics",2018,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032366433&doi=10.1007%2f978-3-319-67837-5_11&partnerID=40&md5=2b6b668e87ece2d3f4b3ee102cf02557","The Opinions Sandbox is a running prototype that accesses comments collected from customers of a particular product or service, and calculates the overall sentiment toward that product or service. It performs topic extraction, displays the comments partitioned into topics, and presents a sentiment for each topic. This helps to quickly digest customers’ opinions, particularly negative ones, and sort them by the concerns expressed by the customers. These topics are now considered issues to be addressed. The Opinions Sandbox does two things with this list of issues. First, it simulates the social network of the future, after rectifying each issue. Comments with positive sentiment regarding this rectified issues are synthesized, they are injected into the comment corpus, and the effect on overall sentiment is produced. Second, it helps the user create a plan for addressing the issues identified in the comments. It uses the quantitative improvement of sentiment, calculated by the simulation in the first part, and it uses user-supplied cost estimates of the effort required to rectify each issue. Sets of possible actions are enumerated and analysed showing both the costs and the benefits. By balancing these benefits against these costs, it recommends actions that optimize the cost/benefit tradeoff. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Actionable analytics; Opinion extraction; Social commerce; Topic extraction","Cost benefit analysis; Cost estimating; Costs; Developing countries; Sales; Actionable analytics; Cost/benefit; Opinion extraction; Social commerces; Topic extraction; Data mining",2-s2.0-85032366433
"Kuhnert N., Lindenmayr O., Maier A.","Classification of body regions based on MRI log files",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019193266&doi=10.1007%2f978-3-319-59162-9_11&partnerID=40&md5=f84b95d33716aeb4c1894b1d7be3b663","Every Siemens Magnetic Resonance Imaging (MRI) system consistently writes events into log files while the system is running. The log files and their contents are constantly refined by software developers. This results in different information contents depending on the software version. One information that is missing in some log files is the examined body region. As the body region is crucial for usage analysis, we used pattern recognition methods to estimate the examined body region for software versions not logging it automatically. We learned the examined body region from a set of used MRI acquisition parameters such as grid and voxel size and could classify body region information with a classification rate up to 94.7%. We compared Bayesian Network augmented Naïve Bayes, Decision Trees, and Neural Networks, and found Neural Networks resulting in the best classification rate. © Springer International Publishing AG 2018.","Classification; Data mining; Log file analysis; Pattern recognition; System usage","Bayesian networks; Computer software; Data mining; Decision trees; Magnetic resonance imaging; Pattern recognition; Pattern recognition systems; Acquisition parameters; Classification rates; Information contents; Log-file analysis; Pattern recognition method; Software developer; Software versions; System usages; Classification (of information)",2-s2.0-85019193266
"Bernabé-Moreno J., Tejeda-Lorente A., Porcel C., Herrera-Viedma E.","A fuzzy linguistics supported model to measure the contextual bias in sentiment polarity",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029430796&doi=10.1007%2f978-3-319-66830-7_19&partnerID=40&md5=9bcdda9b2750af1bcfd7ff92fd1d07e1","The polarity detection problem typically relies on experimental dictionaries, where terms are assigned polarity scores lacking contextual information. As a matter of fact, the polarity is highly dependant on the domain or community it is analysed, so we can speak of a contextual bias. We propose a method supported by fuzzy linguistic modelling to quantify this contextual bias and to enable the bias-aware sentiment analysis. To show how our approach work, we measure the bias of common concepts in two different domains and discuss the results. © 2018, Springer International Publishing AG.","Contextual bias; Fuzzy logic; Linguistic modelling; Polarity; Sentiment analysis","Computer circuits; Data mining; Fuzzy sets; Linguistics; Contextual bias; Contextual information; Detection problems; Different domains; Fuzzy linguistics; Linguistic Modelling; Polarity; Sentiment analysis; Fuzzy logic",2-s2.0-85029430796
"Chao A.F.Y., Yang H.-L.","Using Chinese radical parts for sentiment analysis and domain-dependent seed set extraction",2018,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027865139&doi=10.1016%2fj.csl.2017.07.007&partnerID=40&md5=d1cef2bf20ff6017f5117b5ad9d57565","Although there has been good progress in English sentiment analysis and resources, studies in English cannot be directly used in Chinese owing to the nature of Chinese language. Previous studies suggested adopting linguistic information, such as grammar and morpheme information, to assist in sentiment analysis for Chinese text. However, morpheme-based approaches have a problem in identifying seeds. In addition, these methods do not take advantage of radicals in the characters, which contain a great deal of semantic information. A Chinese word is composed of one or more characters, each of which has its radical part. We can interpret the partial meaning of a character by analyzing that of the radical in the character. Therefore, we not only consider the radical information as the semantic root of a character, but also consider the radical parts between characters in a word as an appropriate linguistic unit for conducting sentiment analysis. In this study, we conducted a series of experiments using radicals as the feature unit in sentiment analysis. Using segmented results from part-of-speech tools as a meaningful linguistic unit (word) in Chinese, we conducted analyses of single-feature word (unigram) and frequently seen two words (pointwise mutual information collocated bigrams) through various sentiment analysis measures. It is concluded that radical features could work better than word features and would consume less computing memory and time. An extended study of the extraction of seeds was also conducted, and the results indicated that 50 seed radical features performed well. A cross-corpus comparison was also conducted; the results demonstrated that the use of 50 extracted radical features as domain-dependent keywords worked better than other sentiment analysis strategies. This study confirmed that radical information could be adopted as a feature unit in sentiment analysis and that domain-dependent radicals could be reused in different corpora. © 2018 Elsevier Ltd","Chinese radical; Domain-dependent seed; Restaurant review analysis; Sentiment analysis","Extraction; Linguistics; Semantics; Chinese language; Chinese radical; Linguistic information; Linguistic units; Pointwise mutual information; Restaurant reviews; Semantic information; Sentiment analysis; Data mining",2-s2.0-85027865139
"Patel K., Dubey S.K., Singh A.S.","A clustering techniques to detect e-mail spammer and their domains",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028426820&doi=10.1007%2f978-3-319-63645-0_71&partnerID=40&md5=d5c6880f3265093d1da7c1ce9bcb40ef","The latest internet has become a collaboration and communications platform, in that e-mail system is one of the most reliable internet services. Sending a spam e-mail is an economically useful commerce for intruders, with the very good earning of millions of dollars. The spam e-mail has become a critical issue to web and society, to stop/reduce the spam e-mails filtering techniques is not sufficient. This paper proposes to recognize spam domain by reading spam e-mails. These spam domains are nothing but Uniform Resource Locator (URL) of the website that intruder is promoting. The approach is based on extracting mail content; links from URL injected e-mail and subject of spam e-mails. These extracted parameters are grouped together through clustering algorithms and evaluated. This proposed work can be help as additional accessory to already available anti-spam tool to recognize intruders. © Springer International Publishing AG 2018.","Clustering algorithms; Data mining; Spam; Spam e-mail; Spam URL","Clustering algorithms; Data mining; Intelligent systems; Clustering techniques; Communications platform; Critical issues; E-mail systems; Filtering technique; Internet services; Spam; Spam e-mails; Electronic mail",2-s2.0-85028426820
"Gomes D.C., Carvalho C.M.G., Cubas M.R., Carvalho D.R., Malucelli A., Zahra F.M.","Use of a computational tool to support content analysis in qualitative research",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021767944&doi=10.1007%2f978-3-319-61121-1_10&partnerID=40&md5=58324e8f2f4c284ced583dd6f73c4d1e","The present article aims to discuss how the computational tool known as Poronto can help in the stage of material exploration, in qualitative investigations in the health area. Two studies in the Nursing area used Poronto for the identification of terms in analysis documents – one study is on a database of Terms of Nursing Jargon from a University Hospital in Southern Brazil, and the other is on the identification of terms in scientific articles and patient records from a University Hospital in Northeastern Brazil for nursing care for individuals with intestinal elimination stoma. When the identification of terms and the quantification of the frequency of occurrence are performed manually, in addition to becoming exhaustive, they are subject to failure. Poronto software facilitates the exploration of material in the content analysis, especially regarding the identification and quantification of categories, and in the identification of context units. © Springer International Publishing AG 2018.","Informatics; Qualitative analysis; Qualitative research","Computational methods; Hospitals; Nursing; Computational tools; Content analysis; Informatics; Northeastern Brazil; Qualitative analysis; Qualitative research; Scientific articles; Southern Brazil; Data mining",2-s2.0-85021767944
"Xu Z., Liu W., Zhu Y., Zhang S.","Building domain keywords using cognitive based sentences framework",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031430425&doi=10.1007%2f978-981-10-3187-8_2&partnerID=40&md5=890bc65d1d3506624a478a4027dc15a9","As the novel web social media emerges on the web, large scale unordered sentences are springing up in the forms: news headlines, microblogs, comments and so on. Domain keywords extraction is very important for information extraction, information retrieval, classification, clustering, topic detection and tracking, and so on. Although these massive sentences contain rich information, their loose semantic association and highly unordered semantic organization make web users extremely difficult to capture the rich information due to the lack of semantic coherence. Sentence ordering is a significant research area focusing on obtaining coherent sentence orders which could assist web user to easily understand these unordered sentences. TextRank is a common graph-based algorithm for keywords extraction. For TextRank, only edge weights are taken into account. We proposed a new text ranking formula that takes into account both edge and node weights of words, named F2N-Rank. The results show our model can obtain coherent sentence orders with higher accuracy in less iterations. The proposed sentence ordering model can be applied in automatic text organization and summarization. © Springer Nature Singapore Pte Ltd. 2018.","Domain keywords; Sentence ordering; TextRank","Computation theory; Data mining; Graphic methods; Information retrieval systems; Semantics; Websites; Domain keywords; Graph-based algorithms; Keywords extraction; Semantic associations; Sentence ordering; Text rankings; TextRank; Topic detection and tracking; Classification (of information)",2-s2.0-85031430425
"Bichindaritz I., Breen C., Cole E., Keshan N., Parimi P.","Feature selection and machine learning based multilevel stress detection from ECG signals",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019722181&doi=10.1007%2f978-3-319-59397-5_22&partnerID=40&md5=451508e3a7976f8619067d37db05d218","Physiological sensor analytics aims at monitoring health as the availability of sensor-enabled portable, wearable, and implantable devices become ubiquitous in the growing Internet of Things (IoT). Physiological multi-sensor studies have been conducted previously to detect stress. In this study, we focus on electrocardiography (ECG) monitoring that can now be performed with minimally invasive wearable patches and sensors, to develop an efficient and robust mechanism for accurate stress identification, for example in automobile drivers. A unique aspect of our research is personalized individual stress analysis including three stress levels: low, medium and high. Using machine learning algorithms from the ECG signals alone, our system achieves up to 100% accuracy and area under ROC curve of 1 depending on the experimental setting in detecting three classes of stress using feature selection from a combination of fiducial points and multiscale entropy as a fine-grained indicator of stress level. © Springer International Publishing AG 2018.","Data mining; ECG; Machine learning; Sensors; Stress medicine","Artificial intelligence; Automobile drivers; Data mining; Electrocardiography; Feature extraction; Health care; Implants (surgical); Internet of things; Learning algorithms; Learning systems; Physiology; Sensors; Stress analysis; Wearable technology; Area under roc curve (AUC); Implantable devices; Internet of Things (IOT); Minimally invasive; Multi-scale entropies; Physiological sensors; Robust mechanisms; Stress detection; Wearable sensors",2-s2.0-85019722181
"Rusnok P., Burda M.","Global quality measures for fuzzy association rule bases",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029451571&doi=10.1007%2f978-3-319-66827-7_24&partnerID=40&md5=d616923e9386ae30b9e748785ee2b1ee","Association rules and fuzzy association rules are vastly studied topics. Various measures for quantifying a quality of a (fuzzy) association rule were proposed in the past. In this article, we survey existing and propose some new quality measures for the whole rule bases of fuzzy association rules. © 2018, Springer International Publishing AG.","Data mining; Fuzzy associations; Fuzzy rules; Global measures; Rule bases","Association rules; Data mining; Fuzzy inference; Fuzzy rules; Fuzzy sets; Pattern matching; Fuzzy association rule; Fuzzy associations; Global measures; Global quality; Quality measures; Rule basis; Fuzzy logic",2-s2.0-85029451571
"Nguyen D., Vo K., Pham D., Nguyen M., Quan T.","A deep architecture for sentiment analysis of news articles",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025142034&doi=10.1007%2f978-3-319-61911-8_12&partnerID=40&md5=df466e55f96e9c9a6b0637c7fa18c5d6","In this paper, we present a deep architecture to perform aspect-level sentiment analysis for news articles. We combine some neural networks models proposed in various deep learning approaches, aiming at tackling specific issues commonly occurring for news articles. In this paper, we explain why our architecture can handle typically-long and content-specific news articles, which often cause overfitting when trained with neural networks. Moreover, the proposed architecture can also effectively process the case when the subject to be analyzed sentimentally is not the main topic of the concerned article, which is also a common issue when performing aspect-level sentiment processing. Experimental results with real dataset demonstrated advantages of our approach as compared to the existing approaches. © Springer International Publishing AG 2018.","Aspect-level sentiment analysis; Convolution neural network; Deep learning; LSTM network; News sentiment analysis; Word embedding network","Data mining; Deep learning; Education; Network architecture; Convolution neural network; Deep architectures; Embedding network; Learning approach; Neural networks model; News articles; Proposed architectures; Sentiment analysis; Deep neural networks",2-s2.0-85025142034
"Muley A.A.","Exploration of small scale wood industries in Nanded district, Maharashtra, India using statistical technique",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028384157&doi=10.1007%2f978-3-319-63645-0_57&partnerID=40&md5=fc5d47c2b71b558fa50b1c1629a9e965","The present study is an attempt for exploring parameters which represents the overall performance of Small Scale Wood Industries (SSWI) in Nanded district using statistical technique. This study specially focuses on some important parameters viz. availability of human resources, financial, production, transportation and marketing management aspect. To study the various in sights of the SSWI based on the focused parameters. The importance of this study on both the academic and the application levels is attributed to SSWIs, despite their contributions to the economy, have not been given due attention as the research of performance has been biased towards large enterprises. © Springer International Publishing AG 2018.","Data mining; Nanded; Small and medium scale enterprises; Statistical analysis; Wood industries","Data mining; Intelligent systems; Marketing; Transportation personnel; Wood; Wood products; Woodworking; Application level; Large enterprise; Maharashtra , India; Marketing management; Nanded; Small and medium scale enterprise; Statistical techniques; Wood industry; Statistical methods",2-s2.0-85028384157
"Anoop V.S., Asharaf S., Deepak P.","Topic modeling for unsupervised concept extraction and document ranking",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032702838&doi=10.1007%2f978-3-319-68385-0_11&partnerID=40&md5=dba6eef1c6debd15c958ac49e68479d0","This paper proposes a framework which induces semantically rich concepts from probabilistically generated topics by a topic modeling algorithm. In this method an off-the-shelf tool has been used to extract noun-phrases as word bi-grams and tri-grams from the static document corpus and then models the topics using Latent Dirichlet Allocation algorithm. Additionally, we show that a small extension to our proposed framework can better rank documents in a large collection, which is a well studied area in information retrieval. Experiments conducted on three real world datasets show that this proposed framework outperforms state-of-the-art methods used for extracting concepts and ranking documents. When compared with the baselines chosen, our proposed concept extraction method showed an increased f-measure in the range of 16.65% to 22.04% and the proposed topic modeling guided document retrieval method showed 7.6%–16.61% increase in f-measure. © Springer International Publishing AG 2018.","Concept extraction; Document ranking; Latent dirichlet allocation; Topic modeling","Extraction; Information retrieval; Intelligent systems; Search engines; Statistics; Concept extraction; Document ranking; Extracting concept; Latent Dirichlet allocation; Real-world datasets; State-of-the-art methods; Topic Modeling; Topic modeling algorithms; Data mining",2-s2.0-85032702838
"Dudarin P.V., Yarushkina N.G.","An approach to fuzzy hierarchical clustering of short text fragments based on fuzzy graph clustering",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031402952&doi=10.1007%2f978-3-319-68321-8_30&partnerID=40&md5=231819b1de1e3104aee11b73f5737e2d","In this paper a novel approach to fuzzy hierarchical clustering of short text fragments is presented. Nowadays dataset which contains a large and even huge amount of short text fragments becomes quite a common object. Different kinds of short messages, paper or news headers are examples of this kind of objects. Authors have taken another similar object which is a dataset of key process indicators of Strategic Planning System of Russian Federation. In order to reveal structure and thematic variety, fuzzy clustering approach is proposed. Fuzzy graph as a model has been chosen as the most natural view of connected set of words. Finally, hierarchy as a result of clustering obtained as desirable presentation structure of large amount of information. © Springer International Publishing AG 2018.","Clustering; Data mining; Fuzzy clustering; Fuzzy graph; Hierarchical clustering; Machine learning; Soft computing; Text analysis","Data mining; Fuzzy clustering; Fuzzy sets; Fuzzy systems; Graph theory; Learning systems; Soft computing; Clustering; Clustering approach; Connected sets; Fuzzy graph; Hier-archical clustering; Large amounts; Russian federation; Text analysis; Cluster analysis",2-s2.0-85031402952
"Muley A., Bhalchandra P., Joshi M., Wasnik P.","Academic analytics implemented for students performance in terms of canonical correlation analysis and chi-square analysis",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031758042&doi=10.1007%2f978-981-10-5508-9_26&partnerID=40&md5=eb64b662460dccf16f97961364121bc0","In this research study, we were interested to test the significant association between selected variables which otherwise called as invisible and have indirect impact on the performance of the students. We have devised out our own dataset for the experimental purpose. Our study has made these variables and their relationship visible. The results enable us to determine characteristics of learning environment related to performance. © Springer Nature Singapore Pte Ltd. 2018.","Data mining; Patterns; Statistical analysis","Computer aided instruction; Data mining; Statistical methods; Canonical correlation analysis; Chi-square analysis; Learning environments; Patterns; Research studies; Education",2-s2.0-85031758042
"Tselykh A., Tselykh L., Vasilev V., Barkovskii S.","Expert system with extended knowledge acquisition module for decision making support",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031307399&doi=10.1007%2f978-3-319-68324-9_3&partnerID=40&md5=27967b7c8af30c3487d52211271ab312","This study is an attempt to develop an expert system containing an advanced knowledge acquisition module to support decision-making in socio-economic systems. In this article, we demonstrate the operation of the effective controls algorithm for generating the attributes of expert systems. The selection of factors for the creation of production rules is automated and is based on the discovery of knowledge from fuzzy cognitive map, which is a system representation of the problem being solved, using numerical methods of linear algebra. The synergy of three tools - fuzzy cognitive maps, numerical methods and fuzzy inference - is realized in the architecture of the expert system. © 2018, Springer International Publishing AG.","Effective controls; Expert system; Knowledge acquisition; Knowledge discovery","Cognitive systems; Data mining; Decision making; Fuzzy inference; Fuzzy rules; Fuzzy systems; Knowledge acquisition; Large scale systems; Linear algebra; Mergers and acquisitions; Numerical methods; Decision making support; Fuzzy cognitive map; Production rules; Socio-economic systems; System representation; Expert systems",2-s2.0-85031307399
"Lin K., Lin D., Cao D.","Sentiment Analysis Model Based on Structure Attention Mechanism",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572811&doi=10.1007%2f978-3-319-66939-7_2&partnerID=40&md5=75edf2793d2076b2a56ed1bdb7a34857","Since the long short-term memory (LSTM) network is a sequential structure, it is difficult to effectively represent the structural level information of the context. Sentiment analysis based on the original LSTM causes a problem of structural level information loss, and its capacity to capture the context information is finite. To address this problem, we proposed a novel structure-attention-based LSTM as a hierarchical structure model. It may capture relevant information in the context as much as possible. We propose HM (ht matrix) to storage the structural information of the context. Furthermore, we introduce the attention mechanism to realize vector selection. Compared with the original LSTM and normal attention-based sentiment classification methods, our model obtains a higher classification precision. It is proved that the structure-attention-based method proposed in this study has an advantage in capturing the potential semantic structure. © 2018, Springer International Publishing AG.","Structural information; Structure-attention-based LSTM; Vector selection","Artificial intelligence; Data mining; Semantics; Attention mechanisms; Classification precision; Context information; Hierarchical structure model; Sentiment classification; Sequential structure; Structural information; Vector selections; Long short-term memory",2-s2.0-85029572811
"Tomar N., Manjhvar A.K.","Role of clustering in crime detection: Application of fuzzy K-means",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031419114&doi=10.1007%2f978-981-10-3773-3_57&partnerID=40&md5=4b7f4506298b0fc2456a2c98d8528f06","The rising rate of crime has devastated everything seriously. The reason of working on crime dataset is to make a better system, which can make people more aware about the increasing type of crime and crime rate in various fields. The proposed paper works on the detection of crime count and the factors that decide the increasing nature of crime in a better way. Utilization of fuzzy k-means has lead to a better technology that detects the crime rate in a better and effective way. The termination measure is an important factor to define the clusters that are formed over the years. They help in easy detection whether the crime is increasing over the years or not. The dataset from the Indian government’s website is taken and been processed so that the results that are calculated can be as correct and near to reality as possible. © Springer Nature Singapore Pte Ltd. 2018.","Clustering; Crime detection; Data mining; Fuzzy; K-means; Termination measure","Data mining; Clustering; Crime detection; Fuzzy; K-means; Termination measure; Crime",2-s2.0-85031419114
"Batra M., Agrawal R.","Comparative analysis of decision tree algorithms",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031416085&doi=10.1007%2f978-981-10-6747-1_4&partnerID=40&md5=42fc5ba2eb82e0846a0f70fcb4fb7af6","Decision trees are outstanding tools to help anyone to select the best course of action. They generate a highly valuable arrangement in which one can place options and study possible outcomes of those options. They also facilitate users to make a fair idea of the pros and cons related to each possible action. A decision tree is used to represent graphically the decisions, the events, and the outcomes related to decisions and events. Events are probabilistic and determined for each outcome. The aim of this paper is to do detailed analysis of decision tree and its variants for determining the best appropriate decision. For this, we will analyze and compare various decision tree algorithms such as ID3, C4.5, CART, and CHAID. © 2018, Springer Nature Singapore Pte Ltd.","C4.5; CART; CHAID; Ensemble; ID3","Decision trees; C4.5; CART; CHAID; Comparative analysis; Course of action; Decision-tree algorithm; Ensemble; Data mining",2-s2.0-85031416085
"Pei S., Hu Q.","Partially monotonic decision trees",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030725250&doi=10.1016%2fj.ins.2017.10.006&partnerID=40&md5=1c69c210003bba7b32c2f6f5bbb6fe0b","In multicriteria decision tasks, certain features are linearly ordered according to the decision and are called criteria, whereas others, called regular attributes, are not. In practice, regular attributes and criteria coexist in most classification tasks. In this paper, we propose a rank-inconsistent rate that distinguishes attributes from criteria. Furthermore, it represents the directions of the monotonic relationships between criteria and decisions. We design a partially monotonic decision tree algorithm to extract decision rules for partially monotonic classification tasks. Experimental results show that the proposed algorithm is effective and efficient. © 2017","Decision tree; Monotonic directions; Partially monotonic; Rank inconsistency","Decision trees; Trees (mathematics); Classification tasks; Decision rules; Decision-tree algorithm; Monotonic classifications; Monotonic directions; Multicriteria decision; Partially monotonic; Rank inconsistency; Data mining",2-s2.0-85030725250
"Ramírez-Tinoco F.J., Alor-Hernández G., Sánchez-Cervantes J.L., Olivares-Zepahua B.A., Rodríguez-Mazahua L.","A brief review on the use of sentiment analysis approaches in social networks",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032659009&doi=10.1007%2f978-3-319-69341-5_24&partnerID=40&md5=5e39316f5d4975d5591b2e71728ef45c","Sentiment analysis is the study of subjective information, for example, opinions, sentiments and beliefs that people express about different topics. In recent years, its importance has grown because a large amount of this type of information has been generated daily in social networks, which can be used to obtain various benefits. There are several research works about sentiment analysis, but very few of them compare the use of sentiment analysis approaches and methods among various social networks. Therefore, the objective of this document is to provide a brief review of the most relevant works related to sentiment analysis and social networks, which shows the main findings regarding the tendencies of using the main sentiment analysis approaches, methods and aspects detected in the different social networks. The review can provide a guide for researchers to know the approaches that exist and how they were used specifically in social networks. © Springer International Publishing AG 2018.","Facebook; Sentiment analysis; Sentiment analysis approaches; Social networks; Twitter","Application programs; Data mining; Process engineering; Software engineering; Facebook; Large amounts; Sentiment analysis; Subjective information; Twitter; Social networking (online)",2-s2.0-85032659009
"Kaing D., Medsker L.","Competitive hybrid ensemble using neural network and decision tree",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669103&doi=10.1007%2f978-3-319-67137-6_16&partnerID=40&md5=ecaac6d6d425069629dccdc972c9ef51","A group of experts can offer a more-informed opinion than any individual expert. In machine learning, the ensemble algorithm mirrors this real-world approach by combining predictions of multiple models, yielding higher performance than any individual model. However, having many models does not ensure optimal performance, the challenge is to choose the best set of models that are both diverse and accurate. In this paper, we propose an ensemble model selection algorithm for a hybrid ensemble, called competitive hybrid ensemble (CHE). CHE first creates a population of models, and then ranks the performance of each model on the validation set. From this ranking, CHE assembles the ensemble candidates and evaluates them on the training set. Finally, the best performing candidate is selected as the final hybrid ensemble. We tested our algorithm using neural network and decision tree as the base models. We compared CHE with random forest, a simple hybrid ensemble without the proposed method, and four types of neural network ensembles. Results show that CHE significantly outperforms or is on-par with most of the other methods. © Springer International Publishing AG 2018.","Decision tree; Ensemble; Hybrid; Neural network","Decision trees; Learning systems; Neural networks; Ensemble; Ensemble algorithms; Ensemble modeling; Hybrid; Individual modeling; Neural network ensembles; Optimal performance; Random forests; Data mining",2-s2.0-85030669103
"Uskov V.L., Bakken J.P., Karri S., Uskov A.V., Heinemann C., Rachakonda R.","Smart university: Conceptual modeling and systems’ design",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019575088&doi=10.1007%2f978-3-319-59454-5_3&partnerID=40&md5=a5d93db28939367f3a7fa0d69b4ba078","The development of Smart University concepts started just several years ago. Despite obvious progress in this area, the concepts and principles of this new trend are not clarified in full yet. This can be attributed to the obvious novelty of the concept and numerous types of smart systems, technologies and devices available to students, learners, faculty and academic institutions. This paper presents the outcomes of a research project aimed at conceptual modeling of smart universities as a system based on smartness levels of a smart system, smart classrooms, smart faculty, smart pedagogy, smart software and hardware systems, smart technology, smart curriculum, smart campus technologies and services, and other distinctive components. The ultimate goal of this ongoing research project is to develop smart university concepts and models, and identify the main distinctive features, components, technologies and systems of a smart university—those that go well beyond features, components and systems used in a traditional university with predominantly face-to-face classes and learning activities. This paper presents the up-to-date outcomes and findings of conceptual modeling of smart university. © 2018, Springer International Publishing AG.","Conceptual modeling; Smart university; Smart university components; Smartness levels; Software systems for a smart university","Data mining; Education; Teaching; Academic institutions; Conceptual model; Learning Activity; Smart technology; Smart universities; Smartness levels; Technologies and systems; Traditional universities; Engineering education",2-s2.0-85019575088
"Atibu E.K., Lacroix P., Sivalingam P., Ray N., Giuliani G., Mulaji C.K., Otamonga J.-P., Mpiana P.T., Slaveykova V.I., Poté J.","High contamination in the areas surrounding abandoned mines and mining activities: An impact assessment of the Dilala, Luilu and Mpingiri Rivers, Democratic Republic of the Congo",2018,"Chemosphere",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436282&doi=10.1016%2fj.chemosphere.2017.10.052&partnerID=40&md5=3997340e18002e75471899e4011a7b4b","Abandoned mines and mining activities constitute important sources of toxic metals and Rare Earth Elements (REEs) affecting surrounding environmental compartments and biota. This study investigates the contamination degree and distribution of toxic metals and REEs in contrasting sediment, soil and plant samples surrounding rivers in the African copperbelt area characterized by the presence of numerous abandoned mines, artisanal and industrial mining activities. ICP-MS results highlighted the highest concentration of Cu, Co and Pb in sediments reaching values of 146,801, 18,434 and 899 mg kg−1, respectively. In soil, the values of 175,859, 21,134 and 1164 mg kg−1 were found for Cu, Co and Pb, respectively. These values are much higher than the sediment guidelines for the protection of aquatic life and international soil clean-up standards. Enrichment factor and geoaccumulation index results indicated important contribution of mining activities to the study sites pollution in addition to natural background. Highest metal accumulation in leaves of Phalaris arundinacea L., was observed, reaching values of 34,061, 5050 and 230 mg kg−1 for Cu, Co, and Pb, respectively. The ∑REE concentration reached values of 2306, 733, 2796 mg kg−1 in sediment, soil and plant samples, respectively. The above results were combined with geographical information including satellite imagery, hydrography and mining concessions. Maps were produced to present the results in a comprehensive and compelling visual format. The results will be disseminated through an innovative mapping online platform to simplify access to data and to facilitate dialogue between stakeholders. © 2017 Elsevier Ltd","Abandoned mines and mining activities; Human risks; MaPX; Plant accumulation; Toxic metals; Water and soil pollution","Lead; Metals; Plants (botany); Pollution; River pollution; Satellite imagery; Sediments; Soil pollution; Soils; Water pollution; Environmental compartment; Geo-accumulation index; Geographical information; Human risks; MaPX; Plant accumulation; Rare earth elements (REEs); Toxic metals; Abandoned mines; abandoned mine; bioaccumulation; concentration (composition); environmental impact assessment; geoaccumulation index; hydrography; industrial emission; metal; mining; phytotoxicity; pollutant source; pollution incidence; pollution monitoring; public health; rare earth element; risk assessment; river pollution; satellite imagery; sediment pollution; soil pollution; toxic substance; water pollution; Congo; Democratic Republic Congo; Katanga; Luilu River; Phalaris arundinacea",2-s2.0-85032436282
"Kozielski M., Matyszok P., Sikora M., Wróbel Ł.","Decision rule learning from stream of measurements—a case study in methane hazard forecasting in coal mines",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030789462&doi=10.1007%2f978-3-319-67792-7_30&partnerID=40&md5=0372bd28d6a64af2e53272b0ac25b4a6","The approach based on the Very Fast Decision Rules algorithm in application to prediction of alarm state resulting from methane hazard in coal mines is presented in this work. The approach introduces the modification of rule induction process due to application of the Correlation rule quality measure. An evaluation of the introduced method on a real life stream data collected from coal mine sensors is performed. The results show advantages of the introduced method considering both the classification quality and the rule-based knowledge representation. © 2018, Springer International Publishing AG.","Classification; Data stream mining; Rule-based learning","Classification (of information); Coal; Hazards; Knowledge representation; Methane; Classification quality; Correlation rule; Data stream mining; Decision rules; Quality measures; Rule induction; Rule-based knowledge; Rule-based learning; Coal mines",2-s2.0-85030789462
"Kumar A., Kumar D., Jarial S.K.","A novel hybrid K-means and artificial bee colony algorithm approach for data clustering",2018,"Decision Science Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019015096&doi=10.5267%2fj.dsl.2017.4.003&partnerID=40&md5=84d0746679eafffcfd271cc59e4ab61c","Clustering is a popular data mining technique for grouping a set of objects into clusters so that objects in one cluster are very similar and objects in different clusters are quite distinct. K-means (KM) algorithm is an efficient data clustering method as it is simple in nature and has linear time complexity. However, it has possibilities of convergence to local minima in addition to dependence on initial cluster centers. Artificial Bee Colony (ABC) algorithm is a stochastic optimization method inspired by intelligent foraging behavior of honey bees. In order to make use of merits of both algorithms, a hybrid algorithm (MABCKM) based on modified ABC and KM algorithm is proposed in this paper. The solutions produced by modified ABC are treated as initial solutions for the KM algorithm. The performance of the proposed algorithm is compared with the ABC and KM algorithms on various data sets from the UCI repository. The experimental results prove the superiority of the MABCKM algorithm for data clustering applications. © 2018 Growing Science Ltd. All rights reserved.","Artificial bee colony; Data clustering; F-measure; K-means; Objective function value; Tournament selection",,2-s2.0-85019015096
"Gahlot G., Patil N.","A pragmatics-oriented high utility mining for itemsets of size two for boosting business yields",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032646581&doi=10.1007%2f978-981-10-3376-6_9&partnerID=40&md5=f6e17f8a858f79407b8b35f43b771eb9","Retail market has paced with an enormous rate, sprawling its effect over the nations. The B2C companies have been putting lucrative offers and schemes to fetch the customers’ attractions in the awe of upbringing the business profits, but with the mindless notion of the same. Knowledge discovery in the field of data mining can be well harnessed to achieve the profit benefits. This article proposes the novel way for determining the items to be given on sale, with the logical clubs, thus extending the Apriori algorithm. The dissertation proposes the high-utility mining for itemsets of size two (HUM-IS2) Algorithm using the transactional logs of the superstores. The pruning strategies have been introduced to remove unnecessary formations of the clubs. The essence of the algorithm has been proved by experimenting with various datasets. © Springer Nature Singapore Pte Ltd. 2018.","Apriori algorithm; B2C companies; Business yields; High-utility itemsets; Knowledge discovery; Log mining; Offers; Pragmatics; Retail",,2-s2.0-85032646581
"Flath C.M., Stein N.","Towards a data science toolbox for industrial analytics applications",2018,"Computers in Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029686258&doi=10.1016%2fj.compind.2017.09.003&partnerID=40&md5=26b352d1c51e409dacc55194364f5426","Manufacturing companies today have access to a vast number of data sources providing gigantic amounts of process and status data. Consequently, the need for analytical information systems is ever-growing to guide corporate decision-making. However, decision-makers in production environments are still very much focused on static, explanatory modeling provided by business intelligence suites instead of embracing the opportunities offered by predictive analytics. We develop a data science toolbox for manufacturing prediction tasks to bridge the gap between machine learning research and concrete practical needs. We provide guidelines and best practices for modeling, feature engineering and interpretation. To this end, we leverage tools from business information systems as well as machine learning. We illustrate the usage of this toolbox by means of a real-world manufacturing defect prediction case study. Thereby, we seek to enhance the understanding of predictive modeling. In particular, we want to emphasize that simply dumping data into “smart” algorithms is not the silver bullet. Instead, constant refinement and consolidation are required to improve the predictive power of a business analytics solution. © 2017 Elsevier B.V.","Manufacturing; Predictive analytics; Process mining","Artificial intelligence; Decision making; Industrial research; Information systems; Learning systems; Manufacture; Business information systems; Feature engineerings; Machine learning research; Manufacturing companies; Manufacturing defects; Predictive modeling; Process mining; Production environments; Predictive analytics",2-s2.0-85029686258
"Murinová P., Burda M., Pavliska V.","An algorithm for intermediate quantifiers and the graded square of opposition towards linguistic description of data",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029444687&doi=10.1007%2f978-3-319-66824-6_52&partnerID=40&md5=169f16313e6b04a604c3e50ceff333d8","The aim of this paper is to apply main theories of fuzzy natural logic together with fuzzy GUHA method for a linguistic characterization of relationships in data. Namely, we utilize the theory of intermediate quantifiers, which provides mathematical interpretation of natural language expressions describing quantity such as “Almost all”, “Few” etc., to describe relationships in data using vague terms that are natural in human expression. We provide an algorithm for computation of truth degrees of expressions containing such quantifiers. Moreover, we discuss some basic properties of intermediate quantifiers (contraries, contradictories, sub-contraries and sub-alterns), which formulate the graded Peterson’s square of opposition, and which can be used to infer new expressions from existing ones. © 2018, Springer International Publishing AG.","Fuzzy GUHA; Fuzzy natural logic; Generalized square of opposition; Intermediate quantifiers; Linguistic associations mining","Computation theory; Computer circuits; Fuzzy sets; Linguistics; Pattern matching; Fuzzy GUHA; Intermediate quantifiers; Linguistic associations; Natural logic; Square-of-opposition; Fuzzy logic",2-s2.0-85029444687
"Valencia-Avellan M., Slack R., Stockdale A., Mortimer R.J.G.","Evaluating water quality and ecotoxicology assessment techniques using data from a lead and zinc effected upland limestone catchment",2018,"Water Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032280111&doi=10.1016%2fj.watres.2017.10.031&partnerID=40&md5=189a5ac8d2e4625c714b735926f906e7","Point and diffuse sources associated with historical metal ore mining are major causes of metal pollution. The understanding of metal behaviour and fate has been improved by the integration of water chemistry, metal availability and toxicity. Efforts have been devoted to the development of efficient methods of assessing and managing the risk posed by metals to aquatic life and meeting national water quality standards. This study focuses on the evaluation of current water quality and ecotoxicology techniques for the metal assessment of an upland limestone catchment located within a historical metal (lead ore) mining area in northern England. Within this catchment, metal toxicity occurs at circumneutral pH (6.2–7.5). Environmental Quality Standards (EQSs) based on a simple single concentration approach like hardness based EQS (EQS-H) are more overprotective, and from sixteen sites monitored in this study more than twelve sites (&gt;75%) failed the EQSs for Zn and Pb. By increasing the complexity of assessment tools (e.g. bioavailability-based (EQS-B) and WHAM-FTOX), less conservative limits were provided, decreasing the number of sites with predicted ecological risk to seven (44%). Thus, this research supports the use of bioavailability-based approaches and their applicability for future metal risk assessments. © 2017 Elsevier Ltd","Bioavailability; Ecotoxicology; Limestone catchment; Metal assessment; River water; Water quality standards","Biochemistry; Ecology; Limestone; Metals; Ores; Risk assessment; River pollution; Runoff; Standards; Toxicity; Water quality; Zinc; Assessment technique; Bioavailability; Conservative limits; Eco-toxicology; Environmental quality standards; Research support; River water; Water quality standard; Catchments; aluminum; copper; humic acid; lead; limestone; zinc; aquatic environment; assessment method; bioavailability; catchment; data set; ecotoxicology; environmental risk; heavy metal; lead; limestone; risk assessment; river water; toxicity; water chemistry; water quality; zinc; Article; catchment; ecotoxicology; macroinvertebrate; pH; priority journal; species richness; surface property; water quality; England; United Kingdom",2-s2.0-85032280111
"Sandeep K.S., Patil N.","A multidimensional approach to blog mining",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032643596&doi=10.1007%2f978-981-10-3376-6_6&partnerID=40&md5=4d5650002a21671bbedca06ea80277f9","Blogs are textual web documents published by bloggers to share their experience or opinion about a particular topic(s). These blogs are frequently retrieved by the readers who are in need of such information. Existing techniques for text mining and web document mining can be applied to blogs to ease the blog retrieval. But these existing techniques consider only the content of the blogs or tags associated with them for mining topics from these blogs. This paper proposes a Multidimensional Approach to Blog Mining which defines a method to combine the Blog Content and Blog Tags to obtain Blog Patterns. These Blog Patterns represent a blog better when compared to Blog Content Patterns or Blog Tag Patterns. These Blog Patterns can either be used for Blog Clustering or used by Blog Retrieval Engines to compare with user queries. The proposed approach has been implemented and evaluated on real-world blog data. © Springer Nature Singapore Pte Ltd. 2018.","Blog clustering; Blog mining; Blogs; Tags",,2-s2.0-85032643596
"Zheng L., Liu H., Ding T., Xing R., Hu X.","Mining method for road traffic network synchronization control area",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026759552&doi=10.1007%2f978-981-10-3551-7_76&partnerID=40&md5=8e763b054f48ca3354abd734b8c1ca2f","To ensure the road network synchronization control effect, it needs the correct division of synchronous control area. Therefore, according to the topology and traffic flow characteristics of road traffic network, this paper constructs road network modularity based on weights which are defined by the correlation degree between the intersections and designs synchronization control zoning method with the goal of maximizing modularity. At the same time, this paper uses road topology, traffic flow data, and tidal characteristics of the morning and evening peak time of business district of Guilin Road in Changchun and apples MATLAB and VISSIM to verify the analysis. The results show that the method is feasible and effective. © Springer Science+Business Media Singapore 2018.","Condensation algorithm; Road traffic network modularity; The key node","Intelligent systems; Intelligent vehicle highway systems; Mining; Roads and streets; Synchronization; Topology; Traffic control; Transportation; Business district; Condensation algorithm; Correlation degree; Key nodes; Road traffic network; Synchronization control; Synchronous control; Traffic flow characteristics; Street traffic control",2-s2.0-85026759552
"Chang T.-M., Hsu M.-F., Lin S.-J.","Integrated news mining technique and AI-based mechanism for corporate performance forecasting",2018,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030978851&doi=10.1016%2fj.ins.2017.10.004&partnerID=40&md5=2aacdbba0b549c54ec970e130093d0a7","The deterioration in a corporation's profitability not only threatens its interests and sustainable development but also causes tremendous losses to other investors. Hence, constructing an effective pre-warning model for performance forecasting is an urgent requirement. Most previous studies only analyzed monetary-based ratios, but merely considering such ratios does not depict the full perspective of a corporation's business conditions. This study thus extends monetary-based ratios to non-monetary-based ratios and aggregates them through the analytic network process (ANP) with a risk-adjusted strategy to establish performance ranks of corporations. Analyzing a corporation's business relationships can help it to react to changes in the market and improve profit margins, as it draws upon such relationship networks for the transfer of scarce resources and knowledge. We believe that no current study adopts such a method to construct a forecasting model. To fill this gap in the literature, this study implements the social network (SN) technique to examine a corporation's competitive edge from seemingly noisy big media data, which are subsequently fed into an artificial intelligence (AI)-based technique to construct the model. The introduced model, examined through real-life cases under numerous conditions, offers a promising alternative for performance forecasting. © 2017 Elsevier Inc.","Decision making; News mining; Performance measure; Risk management; Social network","Forecasting; Profitability; Risk assessment; Risk management; Social networking (online); Analytic network process; Business conditions; Business relationships; Corporate performance; Forecasting modeling; Performance forecasting; Performance measure; Relationship networks; Decision making",2-s2.0-85030978851
"Jin Z., Wang L., Chang Y.","Clustering XML documents using frequent edge-sets",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032663906&doi=10.1007%2f978-3-319-67071-3_50&partnerID=40&md5=bd2f1103b602d2fc04cc72dc11b1f59f","Clustering of XML documents is a useful technique for knowledge discovery in XML databases. However, the process of clustering XML documents is always time-consuming due to the semi-structured characteristics of the documents. In this paper, we present an efficient clustering algorithm called Frequent Edge-based XML Clustering (FEXC) to cluster XML documents using frequent edge sets. First, we represent XML documents using edge sets, and then discover the frequent edge sets for each document employing a traditional frequent pattern mining approach. Second, for each frequent edge set, we find all the documents containing it, and then compute a measure called entropy overlap, which indicates the document relevance (overlap) with the ones containing all other frequent edge sets. Clustering is then performed using the entropy overlap measure. Third, we perform a merging process which removes redundant clusters, therefore reducing the number of clusters. Experimental results show that our proposed method outperforms the traditional distance-based XML clustering algorithm in terms of efficiency without compromising the quality of clustering. © 2018, Springer International Publishing AG.","Clustering; Frequent edge set; Semi-structured data; XML","Entropy; XML; Clustering; Edge-sets; Frequent pattern mining; Merging process; Number of clusters; Overlap measures; Quality of clustering; Semi structured data; Clustering algorithms",2-s2.0-85032663906
"Heritage J., McDonald S., McGarry K.","Integrating Association Rules Mined from Health-Care Data with Ontological Information for Automated Knowledge Generation",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029584429&doi=10.1007%2f978-3-319-66939-7_1&partnerID=40&md5=03c814c562a9e326ac5517a694de8de8","Association rule mining can be combined with complex network theory to automatically create a knowledge base that reveals how certain drugs cause side-effects on patients when they interact with other drugs taken by the patient when they have two or more diseases. The drugs will interact with on-target and off-target proteins often in an unpredictable way. A computational approach is necessary to be able to unravel the complex relationships between disease comorbidities. We built statistical models from the publicly available FAERS dataset to reveal interesting and potentially harmful drug combinations based on side-effects and relationships between co-morbid diseases. This information is very useful to medical practitioners to tailor patient prescriptions for optimal therapy. © 2018, Springer International Publishing AG.","Association rules; Comorbidity; Confidence; Pharmaco-epidemiology; Side-effect; Support","Artificial intelligence; Association rules; Complex networks; Computation theory; Knowledge based systems; Patient treatment; Supports; Co morbidities; Complex relationships; Computational approach; Confidence; Health care datum; Knowledge generations; Medical practitioner; Side effect; Drug interactions",2-s2.0-85029584429
"Błaszczyński J., Stefanowski J.","Local data characteristics in learning classifiers from imbalanced data",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030102225&doi=10.1007%2f978-3-319-67946-4_2&partnerID=40&md5=3c40621a15ea4fa2fc419bfa65ead934","Learning classifiers from imbalanced data is still one of challenging tasks in machine learning and data mining. Data difficulty factors referring to internal and local characteristics of class distributions deteriorate performance of standard classifiers. Many of these factors may be approximated by analyzing the neighbourhood of the learning examples and identifying different types of examples from the minority class. In this paper, we follow recent research on developing such methods for assessing the types of examples which exploit either k-nearest neighbours or kernels. We discuss the approaches to tune the size of both kinds of neighborhoods depending on the data set characteristics and evaluate their usefulness in series of experiments with real-world and synthetic data sets. Furthermore, we claim that the proper analysis of these neighborhoods could be the basis for developing new specialized algorithms for imbalanced data. To illustrate it, we study generalizations of over-sampling in pre-processing methods and neighbourhood based ensembles. © Springer International Publishing AG 2018.",,,2-s2.0-85030102225
"Lazo P., Steinnes E., Qarri F., Allajbeu S., Kane S., Stafilov T., Frontasyeva M.V., Harmens H.","Origin and spatial distribution of metals in moss samples in Albania: A hotspot of heavy metal contamination in Europe",2018,"Chemosphere",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030687290&doi=10.1016%2fj.chemosphere.2017.09.132&partnerID=40&md5=17a32519a0f1d2ad78713c49f098b281","This study presents the spatial distribution of 37 elements in 48 moss samples collected over the whole territory of Albania and provides information on sources and factors controlling the concentrations of elements in the moss. High variations of trace metals indicate that the concentrations of elements are affected by different factors. Relations between the elements in moss, geochemical interpretation of the data, and secondary effects such as redox conditions generated from local soil and/or long distance atmospheric transport of the pollutants are discussed. Zr normalized data, and the ratios of different elements are calculated to assess the origin of elements present in the current moss samples with respect to different geogenic and anthropogenic inputs. Factor analysis (FA) is used to identify the most probable sources of the elements. Four dominant factors are identified, i.e. natural contamination; dust emission from local mining operations; atmospheric transport of contaminants from local and long distance sources; and contributions from air borne marine salts. Mineral particle dust from local emission sources is classified as the most important factor affecting the atmospheric deposition of elements accumulated in the current moss samples. The open slag dumps of mining operation in Albania is probably the main factor contributing to high contents of Cr, Ni, Fe, Ti and Al in the moss. Enrichment factors (EF) were calculated to clarify whether the elements in the present moss samples mainly originate from atmospheric deposition and/or local substrate materials. © 2017 Elsevier Ltd","Anthropogenic sources; Atmospheric deposition; Geochemical interpretation; Moss biomonitoring; Trace metals","Air pollution; Atmospheric chemistry; Atmospheric movements; Deposition; Dust; Geochemistry; Heavy metals; Marine pollution; Meteorological problems; Slags; Soil pollution; Spatial distribution; Anthropogenic sources; Atmospheric depositions; Geochemical interpretation; Moss biomonitoring; Trace metal; Trace elements; aluminum; antimony; calcium; cerium; cesium ion; chromium; cobalt; heavy metal; hydrofluoric acid; iron; lanthanum; lithium ion; magnesium; molybdenum; nickel; potassium ion; rubidium ion; scandium; selenium; sodium ion; strontium; tantalum; thorium; titanium; trace metal; tungsten; unindexed drug; uranium; ytterbium; zirconium; anthropogenic source; atmospheric deposition; atmospheric transport; biomonitoring; data interpretation; geochemistry; heavy metal; moss; pollutant source; pollution effect; redox conditions; spatial distribution; trace metal; Albania; Article; atmospheric deposition; atmospheric transport; chemical analysis; concentration (parameters); contamination; differentiation; dust; geochemical analysis; microwave radiation; mining; moss; oxidation reduction state; particle size; pollutant; spatial analysis; Albania; Bryophyta",2-s2.0-85030687290
[No author name available],"50th Annual Convention of Computer Society of India: Big Data Analytics, CSI 2015",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031396107&partnerID=40&md5=e81b61ed1bbc118908ceb5da8e95c962","The proceedings contain 74 papers. The special focus in this conference is on of Computer Society of India: Big Data Analytics. The topics include: Need for developing intelligent interfaces for big data analytics in the microfinance industry; unified resource descriptor over KAAS framework; an adaptable and secure intelligent smart card framework for internet of things and cloud computing; a framework for ontology learning from taxonomic data; leveraging mapreduce with column-oriented stores: study of solutions and benefits; task-based load balancing algorithm by efficient utilization of VMs in cloud computing; a load balancing algorithm based on processing capacities of VMs in cloud computing; package-based approach for load balancing in cloud computing; workload prediction of E-business websites on cloud using different methods of ANN; data security in cloud-based analytics; ontology-based ranking in search engine; hidden data extraction using URL templates processing; automatic generation of ontology for extracting hidden web pages; importance of SLA in cloud computing; a survey on cloud computing; adapting and reducing cost in cloud paradigm (ARCCP); power aware-based workflow model of grid computing using ant-based heuristic approach; image categorization using improved data mining technique; an effective hybrid encryption algorithm for ensuring cloud data security; an analysis of resource-aware adaptive scheduling for HPC clusters with hadoop; analytical and perspective approach of big data in cloud computing; implementation of couchDBViews; a study of factors affecting mapreduce scheduling and a framework for twitter data analysis.",,,2-s2.0-85031396107
"Cadenas J.M., Garrido M.C.","Intelligent data analysis, soft computing and imperfect data",2018,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027284622&doi=10.1007%2f978-3-319-64286-4_2&partnerID=40&md5=faaea2d4e208e7b66bbccb770f8bd987","In different real problems the available information is not as precise or as accurate as we would like. Due to possible imperfection in the data (understanding that these contain data where not all the attributes are precisely known, such as missing, imprecise, uncertain, ambiguous, etc. values), tools provided by Soft Computing are quite adequate, and the hybridization of these tools with the Intelligent Data Analysis is a field that is gaining more importance. In this paper, first we present a brief overview of the different stages of Intelligent Data Analysis, focusing on two core stages: data preprocessing and data mining. Second, we perform an analysis of different hybridization approaches of the Intelligent Data Analysis with the Soft Computing for these two stages. The analysis is performed from two levels: If elements of Soft Computing are incorporated in the design of the method/model, or if they are also incorporated to be able to deal with imperfect information. Finally, in a third section, we present in more detail several methods which allow the use of imperfect data both for their learning phase and for the prediction. © Springer International Publishing AG 2018.",,,2-s2.0-85027284622
"Piparo D., Tejedor E., Mato P., Mascetti L., Moscicki J., Lamanna M.","SWAN: A service for interactive analysis in the cloud",2018,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009512558&doi=10.1016%2fj.future.2016.11.035&partnerID=40&md5=48f4f827a03f0d408bea98dc4b833aa9","SWAN (Service for Web based ANalysis) is a platform to perform interactive data analysis in the cloud. SWAN allows users to write and run their data analyses with only a web browser, leveraging on the widely-adopted Jupyter notebook interface. The user code, executions and data live entirely in the cloud. SWAN makes it easier to produce and share results and scientific code, access scientific software, produce tutorials and demonstrations as well as preserve analyses. Furthermore, it is also a powerful tool for non-scientific data analytics. This paper describes how a pilot of the SWAN service was implemented and deployed at CERN. Its backend combines state-of-the-art software technologies with a set of existing IT services such as user authentication, virtual computing infrastructure, mass storage, file synchronisation and sharing, specialised clusters and batch systems. The added value of this combination of services is discussed, with special focus on the opportunities offered by the CERNBox service and its massive storage backend, EOS. In particular, it is described how a cloud-based analysis model benefits from synchronised storage and sharing capabilities. © 2016 Elsevier B.V.","Analysis; Cloud; File sharing; Mining; Notebook; Storage synchronisation","Authentication; Clouds; Mining; Synchronization; Analysis; File Sharing; Interactive analysis; Interactive data analysis; Notebook; Scientific softwares; Software technology; User authentication; Digital storage",2-s2.0-85009512558
"Pedrayes F., Norniella J.G., Melero M.G., Menéndez-Aguado J.M., del Coz-Díaz J.J.","Frequency domain characterization of torque in tumbling ball mills using DEM modelling: Application to filling level monitoring",2018,"Powder Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031999464&doi=10.1016%2fj.powtec.2017.10.026&partnerID=40&md5=bad31cdb5927c3083319c02466c20d6b","Ball mills have a low efficiency rate partially due to the lack of a proper method to monitor the mill filling level, which makes it difficult to control the grinding process. The purpose of this paper is to explore the possibilities that DEM offers to characterize the load torque of tumbling ball mills in the frequency domain and, from this characterization, to establish the basis of a methodology capable of estimating the mill filling level. To achieve this, a pilot scale ball mill was considered and a campaign of simulations and experimental tests was carried out in dry grinding conditions, considering variables such as the rotation speed of the mill, the type of particle to be grinded, and the mill filling level. For each simulation, spectral analysis of torque data generated by DEM software was performed. The results obtained from simulations and subsequent torque data processing show the possibility of characterizing the load torque of ball mills in the frequency domain without the influence of components alien to the grinding process. In this paper, it is shown that load torque signal in ball mills contains enough information to characterize, unambiguously, the load level of the mill. Thus, a methodology to evaluate the mill filling level based on torque spectral analysis is proposed. © 2017 The Author(s)","Ball mill; Discrete element method; Load torque; Mill filling; Spectral analysis","Ball milling; Computer software; Data handling; Filling; Finite difference method; Frequency domain analysis; Grinding (machining); Grinding mills; Spectrum analysis; Torque; Torque measurement; Experimental test; Filling levels; Frequency domains; Grinding process; Load torques; Mill fillings; Rotation speed; Tumbling ball mills; Ball mills; Article; ball mill; discrete element method; energy cost; grinding; mathematical analysis; mill filling; mining; rotation; simulation; spectroscopy; torque",2-s2.0-85031999464
[No author name available],"4th Euro-China Conference on Intelligent Data Analysis and Applications, ECC 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030838003&partnerID=40&md5=f13b01ba2adbee96355a0e73b44a6f21","The proceedings contain 42 papers. The special focus in this conference is on Intelligent Data Analysis and Applications. The topics include: Research on eye detection and fatigue early warning technologies; face recognition based on HOG and fast PCA algorithm; day and night image stitching and rendering; application of image preprocessing in soil mesostructure; application of image proccessing in soil’s shear zone mesostructure; an improved scheme of secure access and detection of cloud front-end device; the order and failure estimation of redundancy system based on cobweb model; comments on yu et al’s shared data integrity verification protocol; efficient anonymous password-authenticated key exchange scheme using smart cards; digital certificate based security payment for QR code applications; a novel region selection algorithm for auto-focusing method based on depth from focus; graph theory modeling - a petri nets based approach; sampling as a method of comparing real and generated networks; the relative homogeneity between-class thresholding method based on shape measure; mining of multiple fuzzy frequent itemsets with transaction insertion; efficient mobile middleware for seamless communication of prehospital emergency medicine; a method to evaluate the research direction of university; a prototype system for three-dimensional cardiac modeling and printing with clinical computed tomography angiography; design of wisdom home system for the aged living alone; wireless sensor network routing protocol research for high voltage transmission line monitoring system and car collision warning system for cornering on mountain roads.",,,2-s2.0-85030838003
"Jiang L., He F., Zou Z., Wang Z., Sun L.","A hot area mobility model for ad hoc networks based on mining real traces of human",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026285649&doi=10.1007%2f978-3-319-61566-0_64&partnerID=40&md5=d85c04173beb309927186692bb9825d3","Mobility Model decides how the users move has a great impact on the topology of the Ad hoc networks. In order to reveal human mobile preferences in the real world, by mining two location-based social network data sets, we discover three regular patterns: (1) the complementary cumulative distribution of the number of hot areas of users and the degree of hot areas follows heavy-tail flight distribution; (2) the number of hot areas of the user has stability across time slots; (3) the user’s hot area and user’s social relationship has a weak correlation. A new model called HAMM (Hot Area Movement Model) is put forward based on the above three regular patterns. To validate HAMM, we implement it by extending ONE platform. The experimental results show that HAMM is superior to the baseline SLAW and TLW that relies on more parameters with higher complexity. © Springer International Publishing AG 2018.",,"Ad hoc networks; Topology; Complementary cumulative distributions; Heavy-tails; Location-based social networks; Mobility model; Movement model; Regular patterns; Social relationships; Weak correlation; Complex networks",2-s2.0-85026285649
"Toujani R., Dhouioui Z., Akaichi J.","Mobility based machine learning modeling for event mining in social networks",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020437326&doi=10.1007%2f978-3-319-59480-4_31&partnerID=40&md5=1887fa2a59762b8e397535c39de3e7df","Social networks sounds to be a rich source to discover events mobility and analyzing their trends. Hence, the mobility of events refers to the movement of users’ opinions, location, velocity and the continuous change over time. Despite the ability of existing methods to deal with the event mobility and evolution. To the best of our knowledge, there is no research able to show the relation between mobility and social interactions. In this work, we associate mobility into event mining issue. We also describe the movement of opinions in social network and we aim at extracting useful information from tweeter posts, especially during the economic and political event “TUNISIA 2020”. To achieve this task, we focused on the use of machine learning techniques to analyze tunisian tweeter posts and classify their opinions temporally about this event for each Tunisian region. We introduced decision tree method to model and analyze event mobility and to predict the change of opinions from its spatial and temporal co-occurrence. Therefore, an entropy measure has been proposed based on spatio-temporal attributes as branching attributes. Finally, in order to validate our solution, we used real data and we performed some comparative experiments to show the effectiveness of our method. © Springer International Publishing AG 2018.","Decision tree; Entropy; Event detection; Machine learning; Opinion change; Social networks; Spatial mobility; Spatio-temporal attributes; Temporal mobility","Artificial intelligence; Decision trees; Entropy; Interactive computer systems; Multimedia services; Multimedia systems; Social networking (online); Comparative experiments; Decision tree method; Event detection; Machine learning models; Machine learning techniques; Opinion change; Social interactions; Spatio temporal; Learning systems",2-s2.0-85020437326
"Devarapalli D.D., Srikanth P.","A novel cluster algorithms of analysis and predict for brain derived neurotrophic factor (BDNF) using diabetes patients",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021208356&doi=10.1007%2f978-981-10-3223-3_11&partnerID=40&md5=ce1d9661527545ebb0210e4d053df323","Brain Derived Neurotrophic Factor (BDNF) is involved Diabetes disease is associated with metabolic syndrome. Disease is mainly Type-2 Diabetes Mellitus (T2DM) parameters related to BDNF also. Today’s most people suffered Diabetes Disease. Diabetes Mellitus is a metabolic disorder. Current research is Cluster analyses of T2DM of BDNF data based on predicting the diabetes and identify patients. In this paper, Evaluated as a clustering method for the cluster regarding T2DM of BDNF dataset classifies several clusters. Data Mining is one of the primary methods in clustering. This method examines measurements based on compute minimum, maximum and average values based predict of patients. These algorithms and mathematical problems applied into dataset, evaluate Normalize data and similarity measures based on identifying accurate results. Identification of the BDNF Korley et al. (J Neurotrauma, 33(2):215–225, 2015, [1]) gene these factors help the neurological affected, Change Behavior thing and Mind Depression. © Springer Nature Singapore Pte Ltd. 2018.","Clustering algorithms; Euclidean distance measure; Manhattan distance measure; T2DM of BDNF data","Cluster analysis; Forecasting; Intelligent computing; Metabolism; Brain-derived neurotrophic factors; Euclidean distance measure; Manhattan distance; Mathematical problems; Metabolic disorders; Metabolic syndromes; Similarity measure; T2DM of BDNF data; Clustering algorithms",2-s2.0-85021208356
"Zhang X., Zhang L.","The research on key technique of raw coal management information system",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031400925&doi=10.1007%2f978-981-10-3773-3_26&partnerID=40&md5=d6af6da67e3c84f5b5852e5401008cac","The author analyzed the business process of a large coal enterprise on raw coal quality. A raw coal quality MIS (Management Information System) was designed and implemented using four-tier structure which built by Freemarker + Struts2 + Spring + Mybatis. The system uses network as data transmission medium, and manages the audit flow of different mining’s data and reports according to different permissions. It provides the visualization display of coal quality data, too. The paper analyzed the characteristics of raw coal quality data and established a reasonable database structure. Then it makes an exposition of function design scheme of the system and a method about permission control with configurable function. It also introduces the implementation strategy of reports and graphics. The system has been applied in the enterprise and work well. The work efficiency of coal quality management and audit are improved effectively. © Springer Nature Singapore Pte Ltd. 2018.","Coal quality management; Four-tier structure; Permission control; Report and graphic generation","Coal; Data visualization; Management information systems; Quality control; Quality management; Business Process; Coal enterprise; Coal quality; Database structures; Function designs; Implementation strategies; Report and graphic generation; Work efficiency; Information management",2-s2.0-85031400925
"Hamadeh M.W., Abdallah S.","Discover trending topics of interest to governments",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029498891&doi=10.1007%2f978-3-319-64861-3_34&partnerID=40&md5=2f35462bcc8dc1d6d0232757e644eccc","Users on Twitter post millions of tweets every day from all over the world. Analyzing such data has received significant attention in the last decade. While most of the previous work focused on business-related analysis, our study focuses on the perspective of governments, with Dubai as a case study. We collected corpus of tweets related to Dubai, spanning over the period of 4 months. We then used text mining and clustering techniques to analyze the tweets. We show that existing techniques for detecting, automatically, the best number of clusters would fail in such data. The empirical study was able to discover trending topics and events about Dubai in the period of study. © 2018, Springer International Publishing AG.","Clustering; Social networks; Text mining",,2-s2.0-85029498891
"Sathya Bama S., Irfan Ahmed M.S., Saravanan A.","Relevance re-ranking through proximity based term frequency model",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031764412&doi=10.1007%2f978-3-319-68855-8_22&partnerID=40&md5=e134d96af06ad8212f9eaf7dbe5fd973","In this internet era, people rely on the most significant tool called search engine for retrieving attractive information from the web. Also, the rapid growth in the usage of the web increases the volume of data on the web, due to which most of the documents retrieved by the search engine is overwhelmed with inappropriate and redundant information called outliers. This not only increases the result space, but also roots in wasting the user’s time and effort that makes them to surf uninteresting data. Consequently, a method is essential for the web user community to remove uninteresting information and to present the interesting data in an organized manner based on their request. Web content outlier mining is promising research area that serves these features to the web users. In this research work, proximity based term frequency model has been developed for retrieving the appropriate information and for refining the quality of the results offered by the search engine. Experimental results indicate that proximity based term frequency model improves the performance in terms of relevancy re-ranking of the retrieved documents. © Springer International Publishing AG 2018.","Proximity; Relevance ranking; Search engine; Term frequency; Web content outlier","Cognitive systems; Statistics; Websites; Proximity; Rapid growth; Re-ranking; Relevance ranking; Retrieved documents; Term Frequency; Web content; Web content outlier mining; Search engines",2-s2.0-85031764412
"Butakova M.A., Chernov A.V., Guda A.N.","Algorithms of sequential pattern generation with noise using stochastic and fuzzy models",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421084&doi=10.1007%2f978-3-319-68321-8_21&partnerID=40&md5=ca9aa469fa3b8ffdde67d8914f36cbb3","A task of sequential pattern generation can be considered as a problem which is inverse to sequential pattern mining. This paper presents two novel approaches to the sequential pattern generation with noise, namely the approach based on stochastic automata and context-free grammars and the approach based on Hidden Markov model. The distinctive feature of these methods is the suitability to produce an output in the noisy and fuzzy input data. Also, we present the detailed calculation algorithms to the proposed approaches. © Springer International Publishing AG 2018.","Fuzzy pattern; Hidden markov model; Sequential pattern; Stochastic automata model","Automata theory; Context free grammars; Hidden Markov models; Inverse problems; Markov processes; Stochastic systems; Calculation algorithms; Fuzzy input; Fuzzy models; Fuzzy pattern; Sequential patterns; Sequential-pattern mining; Stochastic Automata; Stochastic models",2-s2.0-85031421084
"Kanse L., Parkes K., Hodkiewicz M., Hu X., Griffin M.","Are you sure you want me to follow this? A study of procedure management, user perceptions and compliance behaviour",2018,"Safety Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028016402&doi=10.1016%2fj.ssci.2017.08.003&partnerID=40&md5=3be67a119e6d874aaaa2d359102ee66d","Adherence to procedures is critical to the safety and performance of maintenance tasks; however, few studies of procedure compliance among maintenance personnel have been reported. The present study evaluated a theoretical model in which management approaches to procedure compliance were linked to compliance outcomes through user perceptions of positive and negative procedure attributes. New scales were developed to assess these variables; hypotheses derived from the model were tested in survey data collected from maintainers in the mining industry (N = 176). A structural equation model showed acceptable fit statistics; findings were broadly consistent with the initial hypotheses. As predicted, positive and negative dimensions of procedure attributes and compliance/non-compliance were perceived as distinct constructs, and were implicated in different pathways of the model. Also supporting the initial hypotheses, user involvement and managers’ learning-oriented responses to non-compliance were linked to favourable compliance outcomes through perceived procedure attributes. Learning-oriented responses were also directly associated with greater compliance. In addition, and contrary to prediction, punitive management responses positively predicted compliance. As discussed in the paper, these findings contribute new insights, relevant in both research and industry contexts, to understanding procedure compliance among maintainers. © 2017 Elsevier Ltd","Compliance; Maintenance; Mining; Procedure; Safety; User perceptions; Violations","Accident prevention; Industrial research; Mining; Compliance; Maintenance personnel; Procedure; Procedure managements; Structural equation modeling; Theoretical modeling; User perceptions; Violations; Maintenance",2-s2.0-85028016402
"Vaněk A., Grösslová Z., Mihaljevič M., Ettler V., Trubač J., Chrastný V., Penížek V., Teper L., Cabala J., Voegelin A., Zádorová T., Oborná V., Drábek O., Holubík O., Houška J., Pavlů L., Ash C.","Thallium isotopes in metallurgical wastes/contaminated soils: A novel tool to trace metal source and behavior",2018,"Journal of Hazardous Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029603960&doi=10.1016%2fj.jhazmat.2017.09.020&partnerID=40&md5=a2008a58619008e2551aaa2a6935ea24","Thallium (Tl) concentration and isotope data have been recorded for contaminated soils and a set of industrial wastes that were produced within different stages of Zn ore mining and metallurgical processing of Zn-rich materials. Despite large differences in Tl levels of the waste materials (1–500 mg kg−1), generally small changes in ε205Tl values have been observed. However, isotopically lighter Tl was recorded in fly ash (ε205Tl ∼ −4.1) than in slag (ε205Tl ∼ −3.3), implying partial isotope fractionation during material processing. Thallium isotope compositions in the studied soils reflected the Tl contamination (ε205Tl ∼ −3.8), despite the fact that the major pollution period ended more than 30 years ago. Therefore, we assume that former industrial Tl inputs into soils, if significant, can potentially be traced using the isotope tracing method. We also suggest that the isotope redistributions occurred in some soil (subsurface) horizons, with Tl being isotopically heavier than the pollution source, due to specific sorption and/or precipitation processes, which complicates the discrimination of primary Tl. Thallium isotope analysis proved to be a promising tool to aid our understanding of Tl behavior within the smelting process, as well as its post-depositional dynamics in the environmental systems (soils). © 2017 Elsevier B.V.","Fractionation; Isotopes; Soil; Thallium; Waste","Fly ash; Fractionation; Industrial wastes; Isotopes; Metallurgy; Pollution; Slags; Smelting; Soils; Thallium; Trace elements; Wastes; Zinc; Environmental systems; Isotope compositions; Isotope fractionation; Isotope tracing methods; Material processing; Metallurgical processing; Metallurgical waste; Precipitation process; Soil pollution; isotope; thallium; zinc; concentration (composition); contaminated land; industrial waste; isotopic composition; isotopic fractionation; metallurgy; mineral processing; pollutant source; precipitation (chemistry); sorption; thallium; trace metal; zinc; Article; controlled study; fractionation; industrial waste; isotope tracing; metallurgy; mining; precipitation; soil pollution",2-s2.0-85029603960
"Deng J., Lei C., Xiao Y., Cao K., Ma L., Wang W., Laiwang B.","Determination and prediction on “three zones” of coal spontaneous combustion in a gob of fully mechanized caving face",2018,"Fuel",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802150&doi=10.1016%2fj.fuel.2017.09.027&partnerID=40&md5=66c6ebcff0da7705c30f55b9f1a29bf1","The precise division into “three zones” of coal spontaneous combustion in the gob plays a key role for coal fire fighting. This paper presents three-dimensional distribution maps and contour plots for the gases and temperature in the gob by the method of griddata interpolation according to the data (O2, CO, CO2, CH4, and temperature) acquired from in-situ test, and the variation of gases and temperature. It is proposed to comprehensively divide “three zones” by using O2 concentration of 5–18 vol%, the appearance and disappearance of CO, and the heating rate K = 0 °C/m. The gas explosion conditions were considered to determine the danger zone of coal spontaneous combustion. The minimum mining speed was calculated to be 4.8 m/day based on the division of the “three zones” in the gob in order to prevent spontaneous combustion phenomenon. Particle swarm optimization (PSO) was employed to optimize the parameters of support vector regression (SVR); the PSO-SVR model was established to predict the temperature of coal spontaneous combustion based on the gases’ concentration in the gob and distance from the measuring points to the working face. Prediction results and performance of PSO-SVR model were compared with standard SVR, back propagation neural network (BPNN), and multiple linear regression (MLR). The results indicated that PSO-SVR model had greater prediction accuracy and generalization ability, which can predict the temperature of coal spontaneous combustion in the gob. © 2017 Elsevier Ltd","Coal explosion; Minimum mining speed; Modelling; Spontaneous combustion; Support vector regression","Backpropagation; Coal; Coal combustion; Combustion; Fire extinguishers; Fire fighting equipment; Forecasting; Linear regression; Models; Neural networks; Particle swarm optimization (PSO); Regression analysis; Back-propagation neural networks; Coal spontaneous combustion; Distribution maps; Fully-mechanized caving faces; Generalization ability; Multiple linear regressions; Prediction accuracy; Support vector regression (SVR); Spontaneous combustion",2-s2.0-85029802150
"Vats G., Bhatnagar V., Sharma R., Setiya I., Jain A.","Approach for an opinion wrapping system–using focused web crawler",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421994&doi=10.1007%2f978-981-10-3773-3_1&partnerID=40&md5=72773539a89bea60a71f6cfeecbb5ed8","Most of the search engine depends on web crawler to go through a large number of Webpages. Web crawler (i.e. web spider or scutter or bot) is used to fetch content and URL from the Webpages. It also indexes them so that browser can easily and quickly fetch pages related to the searched word. Tons of data is produced every day, 90% of data has been created in the last 2 years. This data contains opinions and thoughts of the people in unstructured form. Opinion Scutter goes through the content and fetch reviews and comments so that they can be grilled and processed to find useful information. While shopping online or searching any game to buy we are largely dependent on the reviews provided by people. If we can keep track of such reviews and opinion, it will be easy to track a good product and increase efficiency and efficacy of the search engine. The proposed system is a generic crawler which fetches all the reviews from a given site. © Springer Nature Singapore Pte Ltd. 2018.","Opinion mining; Product monitoring; Reviews; Web crawler; Webpage parser; World wide web","Reviews; Search engines; Websites; World Wide Web; Focused web crawler; Keep track of; Opinion mining; Product monitoring; Web crawler",2-s2.0-85031421994
"Ben Abdessalem Karaa W., Mannai M., Dey N., Ashour A.S., Olariu I.","Gene-disease-food relation extraction from biomedical database",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029490497&doi=10.1007%2f978-3-319-62521-8_34&partnerID=40&md5=434088ab4f62df39f927f4d6eb6b83c9","Through the past years, an incredible increase in the biomedical data amount presented on the web is enlarged due to the increased data volume in the medical and biological domains. Hence, the search for documents and information on the internet became increasingly complicated. In the current work, a new approach for information extraction using the Natural Language Processing (NLP) tools and ontology was proposed. It described a system to extract relations between the concepts from biomedical texts using morphological analysis and information extraction techniques. In the first step, the system segmented the input text into sentences. Each sentence is then segmented into words that were tagged with part-of-speech labels and concept classes (food, drug, and gene). A set of relation extraction rules (regular expression patterns) are applied on the annotated sentences. If a pattern matches, the concepts and relations are extracted. The system has been tested on a set of 700 MEDLINE abstracts. For performance evaluation, the precision, recall and F-score were calculated. The proposed approach created by information retrieval from MEDLINE to gather a set of abstracts related to a given domain. Then, these texts were annotated using an automaton and ontology via recognizing interesting concepts for morphological analysis. After the annotation step, some rules were summarizing in an automaton that help gene-disease-food relationships discovery. This work proposed an approach for identifying relations between medical concepts using NLP tools. An evaluation experiment reported good effectiveness results. © 2018, Springer International Publishing AG.","Biomedical information; Information extraction; MEDLINE; Natural language processing (NLP); Semantic reasoning techniques; Text mining",,2-s2.0-85029490497
"Abdelhade N., Soliman T.H.A., Ibrahim H.M.","Detecting twitter users’ opinions of arabic comments during various time episodes via deep neural network",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029504404&doi=10.1007%2f978-3-319-64861-3_22&partnerID=40&md5=201694403c2e79ed7878370334f2ec43","Due to the revolution of web 2.0, the amount of opinionated data has been extremely increased, produced by online users through sharing comments, videos, pictures, reviews, news and opinions. Although Twitter is one of the most prevalent social networking, the gathered data from Twitter is highly disorganized. However, extracting useful information from tweets is considered a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. There has been a lot of work on sentiment analysis in English texts. However, the datasets and the publications of Arabic tweets analysis are still somewhat limited. In addition, one of the main important issues is that users can change their opinions on different subjects over time. In this work, two main points are discussed. First, a deep neural network (DNN) approach (back propagation algorithm) is applied to Arabic tweets to two different domains: Egyptian stock exchange and sports’ tweets. Second, DNN is implemented to detect users’ attitude in a time period of two years for each dataset (2014 and 2015) and (2012 and 2013). The datasets are manually annotated via constructing a lexicon from the two already existing ones. When DNN performance is evaluated an average value of accuracy 90.22%, precision 90.56%, recall 90.90%, and F-measure of 90.68%, when compared to other three machine learning algorithms Naïve Bayes (NB), Decision Tree, and K-Nearest. © 2018, Springer International Publishing AG.","Arabic tweets; Back propagation algorithm; Deep neural network; Opinion mining and time episodes; Sentiment analysis",,2-s2.0-85029504404
"Kowalczuk P.B., Snook B., Kleiv R.A., Aasly K.","Efficient extraction of copper and zinc from seafloor massive sulphide rock samples from the Loki's Castle area at the Arctic Mid-Ocean Ridge",2018,"Minerals Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032026201&doi=10.1016%2fj.mineng.2017.10.015&partnerID=40&md5=5776331146373776fb04dfe781fee573","Seafloor massive sulphide (SMS) deposits have been identified as important marine metal resources for the future. However, literature on the recovery/extraction of metals from SMS is currently limited, and to date, no research has been published on the processing of SMS from the active hydrothermal vent field at the Arctic Mid-Ocean Ridge. In this paper extraction of copper and zinc, as economically important metals, from the seafloor massive sulphide rock samples from the Loki's Castle area at the Arctic Mid-Ocean Ridge was investigated during nitric acid leaching. The results presented are of the various leaching experiments conducted under different conditions to optimise the extraction of copper and zinc. The mineralogical analysis indicated that the main copper and zinc bearing minerals were chalcopyrite and sphalerite, respectively. It was shown that the leaching efficiency and extraction of copper and zinc can be controlled mainly by temperature and acid concentration. The elemental composition and mineralogical data indicated that 95% of copper and zinc bearing minerals were leached out after 3 h, at the solid-to-liquid ratio of 1:10, temperature of 90 °C and acid concentration of 10%. © 2017 Elsevier Ltd","Copper; Deep sea mining; Extraction; Leaching; Marine minerals; Nitric acid; Seafloor massive sulphide; Zinc","Copper compounds; Deep sea mining; Extraction; Hot springs; Leaching; Metal recovery; Minerals; Nitric acid; Submarine geology; Sulfur compounds; Underwater mineral resources; Zinc; Zinc sulfide; Elemental compositions; Leaching experiments; Marine minerals; Mineralogical analysis; Nitric acid leaching; Sea floor; Seafloor massive sulphide deposit (SMS); Solid-to-liquid ratio; Copper",2-s2.0-85032026201
"Yang H., Duan L., Li Q., Tian Z., Li G.","Experimental and modeling investigation on the rheological behavior of collagen solution as a function of acetic acid concentration",2018,"Journal of the Mechanical Behavior of Biomedical Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028976624&doi=10.1016%2fj.jmbbm.2017.09.003&partnerID=40&md5=9aa0574dc2161a76174bc471e6dbac65","Systematic analysis of the rheological behavior of collagen solution (10 mg/mL) as a function of acetic acid (AA) concentration (0.1–10 M) was performed to achieve a deeper understanding about the interaction between collagen molecules and acidic solvent. Steady shear tests showed that all samples exhibited pseudo-plasticity with shear-thinning behavior. Viscosity decreased from 236.448 to 0.792 Pa·s at 0.1 s−1 suggesting the flow ability of collagen solution improved with increasing AA concentration. Dynamic frequency sweep analysis revealed that the storage modulus, loss modulus, and complex viscosity decreased with the increased AA concentration due to the disentanglement of collagen molecules, while the loss tangent increased. Hysteresis loop areas of collagen solutions were determined by thixotropic measurement, which demonstrated that weaker thixotropic behavior was associated with higher AA concentrations. Furthermore, the ability to resist deformation and elasticity was lower at higher AA concentration. Maximum compliance values increased from 0.042 to 376.407 Pa−1, and the recovery percentage decreased from 97.670% to 0.315%. Finally, corresponding mathematical models were employed to simulate and quantitatively assess the experimental data. © 2017 Elsevier Ltd","Acetic acid concentration; Collagen; Mechanical models; Rheological behavior","Acetic acid; Ascorbic acid; Collagen; Digital storage; Molecules; Organic acids; pH; Rheology; Shear thinning; Viscosity; Acetic acid concentration; Collagen molecules; Mechanical model; Recovery percentages; Rheological behaviors; Shear-thinning behavior; Systematic analysis; Thixotropic behavior; Solution mining; acetic acid; collagen; Article; collagen degradation; comparative study; compliance (physical); concentration (parameters); controlled study; flow kinetics; hysteresis; mathematical model; molecular weight; priority journal; shear rate; shear stress; thixotropy; viscosity; Young modulus",2-s2.0-85028976624
"Iosif E., Klasinas I., Athanasopoulou G., Palogiannidi E., Georgiladakis S., Louka K., Potamianos A.","Speech understanding for spoken dialogue systems: From corpus harvesting to grammar rule induction",2018,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029429337&doi=10.1016%2fj.csl.2017.08.002&partnerID=40&md5=fd779fef8739106d0c5def8d2512eb62","We investigate algorithms and tools for the semi-automatic authoring of grammars for spoken dialogue systems (SDS) proposing a framework that spans from corpora creation to grammar induction algorithms. A realistic human-in-the-loop approach is followed balancing automation and human intervention to optimize cost to performance ratio for grammar development. Web harvesting is the main approach investigated for eliciting spoken dialogue textual data, while crowdsourcing is also proposed as an alternative method. Several techniques are presented for constructing web queries and filtering the acquired corpora. We also investigate how the harvested corpora can be used for the automatic and semi-automatic (human-in-the-loop) induction of grammar rules. SDS grammar rules and induction algorithms are grouped into two types, namely, low- and high-level. Two families of algorithms are investigated for rule induction: one based on semantic similarity and distributional semantic models, and the other using more traditional statistical modeling approaches (e.g., slot-filling algorithms using Conditional Random Fields). Evaluation results are presented for two domains and languages. High-level induction precision scores up to 60% are obtained. Results advocate the portability of the proposed features and algorithms across languages and domains. © 2018 Elsevier Ltd","Corpora creation; Crowdsourcing; Grammar induction; Semantic similarity; Spoken dialogue systems; Web mining","Automation; Computational grammars; Crowdsourcing; Nematic liquid crystals; Query processing; Semantics; Speech processing; Corpora creation; Grammar induction; Semantic similarity; Spoken dialogue system; Web Mining; Harvesting",2-s2.0-85029429337
"Xu Z., Xuan J., Zhu Y., Wei X.","Building the profile of web events based on website measurement",2018,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031401873&doi=10.1007%2f978-981-10-3187-8_1&partnerID=40&md5=86f592a330534fbfacdb9ae1fb07c959","Nowadays, Web makes it possible to study emergencies from web information due to its real-time, open, and dynamic features. After the emergence of a web event, there will be numerous websites publishing webpages to cover this web event. Measuring temporal features in evolution course of web events can help people timely know and understand which events are emergencies, so harms to the society caused by emergencies can be reduced. In this paper, website preference is formally defined and mined by three proposed strategies which are all explicitly or implicitly based on the three-level networks: website-level, webpage-level and keyword-level. An iterative algorithm is firstly introduced to calculate outbreak power of web events, and increased web pages of events, increased attributes of events, distribution of attributes in web pages and the relationships of attributes are embedded into this iterative algorithm as the variables. By means of prior knowledge, membership grade of web events belong to each type can be calculated, and then the type of web events can be discriminated. Experiments on real data set demonstrate the proposed algorithm is both efficient and effective, and it is capable of providing accurate results of discrimination. © Springer Nature Singapore Pte Ltd. 2018.","Web events; Web mining; Website preference","Computation theory; Iterative methods; Dynamic features; Iterative algorithm; Membership grade; Temporal features; Three-level networks; Web events; Web information; Web Mining; Websites",2-s2.0-85031401873
"Hoogendoorn M., Funk B.","Clustering",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030686515&doi=10.1007%2f978-3-319-66308-1_5&partnerID=40&md5=570fc8294b232c48d7fa8d23e61eb0a8","This chapter focuses on clustering of the data resulting from quantified selves. It introduces distance functions that can be used to compare individual data points, but also entire datasets of users. Among these are dynamic time warping and the cross-correlation coefficient. The chapter provides a brief discussion of popular clustering techniques. In addition, it explains more specialized clustering techniques that are better suited for the quantified self, including subspace clustering and data stream mining. © Springer International Publishing AG 2018.",,,2-s2.0-85030686515
[No author name available],"38th International Conference on Information Systems Architecture and Technology, ISAT 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029534548&partnerID=40&md5=52c2b949e91e8c827e114c5897f80726","The proceedings contain 101 papers. The special focus in this conference is on Information Systems Architecture and Technology. The topics include: A deep learning approach for valve defect recognition in heart acoustic signal; population-based algorithm with selectable evolutionary operators for nonlinear modeling; evaluation of gated recurrent neural networks in music classification tasks; neuro-fuzzy system for medium-term electric energy demand forecasting; an evolutionary optimization method based on scalarization for multi-objective problems; measuring cognitive workload in arithmetic tasks based on response time and EEG features; a method for genetic selection of the dynamic signature global features’ subset; multivariate regression tree for pattern-based forecasting time series with multiple seasonal cycles; active protocol discoverer based on grammatical evolution; speaker diarization using deep recurrent convolutional neural networks for speaker embeddings; classification tree for material defect detection using active thermography; interactive visualization of query results set from information retrieval using concept lattices; PID-fuzzy controllers with dynamic structure and evolutionary method for their construction; community detection in bibsonomy using data clustering; big data, knowledge discovery and data mining, knowledge based management; using predictive data mining models for data analysis in a logistics company; twitter sentiment analysis using a modified naive bayes algorithm; cost-sensitive feature selection for class imbalance problem; cost-sensitive feature selection for class imbalance problem; a dynamic packed approach for analytic data warehouse in ad-Hoc queries; risk-based decision making in iot systems and on implementation of energy-aware MPTCP scheduler.",,,2-s2.0-85029534548
"Abdulrahman S.M., Cachada M.V., Brazdil P.","Impact of feature selection on average ranking method via metalearning",2018,"Lecture Notes in Computational Vision and Biomechanics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364887&doi=10.1007%2f978-3-319-68195-5_121&partnerID=40&md5=c400f2c4868237280d7dcd3b707d0485","Selecting appropriate classification algorithms for a given dataset is crucial and useful in practice but is also full of challenges. In order to maximize performance, users of machine learning algorithms need methods that can help them identify the most relevant features in datasets, select algorithms and determine their appropriate hyperparameter settings. In this paper, a method of recommending classification algorithms is proposed. It is oriented towards the average ranking method, combining algorithm rankings observed on prior datasets to identify the best algorithms for a new dataset. Our method uses a special case of data mining workflow that combines algorithm selection preceded by a feature selection method (CFS). © 2018, Springer International Publishing AG.",,,2-s2.0-85032364887
[No author name available],"International Conference on Computational Methods in Systems and Software, CoMeSySo 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572094&partnerID=40&md5=08b7fd5d1a8fc4ea6f33ca7f50f25a0b","The proceedings contain 35 papers. The special focus in this conference is on Computational Methods in Systems and Software. The topics include: Spatially augmented analysis of macroeconomic convergence with application to the czech republic and its neighbors; trend-cycle decomposition of economic activity in the czech republic; parallel matrix multiplication for business applications; content generation for massively multiplayer online games with genetic algorithms; an autonomous architecture for managing vertical elasticity in the IaaS cloud using memory over-subscription; a security framework for cloud data storage(CDS) based on agent; proposal for the design of a new technological infrastructure for the efficient management of network services and applications in a high complexity clinic in colombia; initial centroid selection optimization for k-means with genetic algorithm to enhance clustering of transcribed arabic broadcast news documents; an imperialist competitive algorithm to solve the manufacturing cell design problem; optical character recognition system for czech language using hierarchical deep learning networks; a percentile transition ranking algorithm applied to knapsack problem; enrichment ontology instance by using data mining techniques; solving the set covering problem using cat swarm optimization algorithm with a variable mixture rate and population restart; inference algorithms in latent dirichlet allocation for semantic classification; between data mining and predictive analytics techniques to cybersecurity protection on elearning environments; a performance evaluation of chi-square pruning techniques in class association rules optimization and information retrieval based on the extracted social network.",,,2-s2.0-85029572094
[No author name available],"14th International Conference on Distributed Computing and Artificial Intelligence, DCAI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021772460&doi=10.1007%2f978-3-319-60882-2_1&partnerID=40&md5=8b3e06bd5c1351c02f582443f5415fab","The proceedings contain 16 papers. The special focus in this conference is on Distributed Computing and Artificial Intelligence. The topics include: The Bayesian cost–effectiveness decision problem; evaluation of scientific production without using bibliometric indicators; information aggregation in big data; a decision framework for understanding data-aware business process models; cluster analysis as a decision-making tool; similar patterns of cultural and creative industries; a basic algorithm to support decision-making behaviour; looking for regional convergence; information manipulation and web credibility; a data mining analysis of the Chinese inland-coastal inequality; a unified framework for multicriteria evaluation of intangible capital assets inside organizations; processing and analysing experimental data using a tensor-based method and the mediating effect of the absorptive capacity in the international entrepreneurial orientation of family firms.",,,2-s2.0-85021772460
"Odoardi I., Muratore F., Bucciarelli E., Chen S.-H.","Looking for regional convergence: evidence from the italian case with multivariate adaptive regression splines",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021729032&doi=10.1007%2f978-3-319-60882-2_10&partnerID=40&md5=54069f738f575bd73c5f1b54450387e5","This paper examines the role of data mining analysis in explaining the Italian regional dualism with the aim of suggesting economic policies to fill the existing socio-economic gaps. We analyze the 2004–2014 period exploiting the capacity of MARS model in finding relationships among data. In Italy, the presence of a North-South divide is well-known for decades and present for several social and economic aspects. Recent studies prove that strong differences exist also in the regional human capital. Thus, we search for the causes of the local differences, also considering the entrepreneurial vitality and the international trade leverage. Among several variables, MARS is useful in showing the actual determinants on which to intervene. This is possible by comparing regions grouped homogeneously into clusters using recent data. MARS results are used for policy suggestions with the aim of filling the income gap. © Springer International Publishing AG 2018.","Clusters; MARS; Regional convergence; Regional policy","Artificial intelligence; Distributed computer systems; International trade; Clusters; Economic policies; MARS; Multivariate adaptive regression splines; Policy suggestions; Regional convergence; Regional policies; Several variables; Economics",2-s2.0-85021729032
"Hoogendoorn M., Funk B.","Predictive modeling without notion of time",2018,"Cognitive Systems Monographs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030663789&doi=10.1007%2f978-3-319-66308-1_7&partnerID=40&md5=736294a0c2acd8a706fb9c3cc35f04bf","Supervised learning approaches that do not explicitly take the time component into account are briefly discussed in this chapter. The approaches explained include feedforward neural networks, support vector machines, k-nearest neighbor, decision trees, naïve bayes and ensembles. Guidelines are provided on how to apply these algorithms to quantified self data, including the learning setup (e.g. learning for single users or across multiple users) and other practical considerations such as feature selection and regularization. Data stream mining approaches for predictive modeling are also briefly discussed. © Springer International Publishing AG 2018.",,,2-s2.0-85030663789
"Obukhova N., Motyko A.","Image analysis in clinical decision support system",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032574778&doi=10.1007%2f978-3-319-67994-5_10&partnerID=40&md5=7d58781efd80cd4c946d1b3337e42f03","In this chapter, the methods of medical image processing and analysis in Clinical Decision Support Systems (CDSS) are discussed. The main principles of image analysis with the aim of differential diagnostics in the CDSS are determined. The implementation is given through the method of multispectral images automatic processing and analysis for TV system of cervix oncological changes diagnostics. The method provides differential diagnostics of the following changes in cervical tissues as Norm, Chronic Nonspecific Inflammation (CNI), Cervical Intraepithelial Neoplasia in various types of oncological changes (CIN I, CIN II, CIN III). In proposed method, images of different type (fluorescent images and images obtained in white light illumination) are analyzed. The decision rules in the classification task are based on data mining methods. For the border CIN/CNI sensitivity 87% and specificity 75% are achieved. The detail description of main steps is given in the chapter. © 2018, Springer International Publishing AG.","Classification; Clinical decision support system; Image color analysis; Medical images processing; Multispectral images processing; Texture analysis",,2-s2.0-85032574778
[No author name available],"14th International Symposium on Distributed Computing and Artificial Intelligence, DCAI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022209944&partnerID=40&md5=81508107ebc6721ead3e7a1dc4922276","The proceedings contain 29 papers. The special focus in this conference is on Distributed Computing and Artificial Intelligence. The topics include: Optimization of urban freight distribution with different time constraints; artificial bee colony algorithms for two-sided assembly line worker assignment and balancing problem; cyclic steady state behaviour subject to grid-like network constraints; application of fuzzy logic and genetic algorithms in automated works transport organization; quality assessment of implementation of strategy design pattern; minimizing energy consumption in a straight robotic assembly line using differential evolution algorithm; statistics-based approach to enable consumer profile definition for demand response programs; feature extraction-based method for voltage sag source location in the context of smart grids; a multi-agent system for energy trading between prosumers; smart grids data management; data mining for prosumers aggregation considering the self- generation; control of accuracy of forming elastic deformable shafts with low rigidity; a negotiation algorithm for decision-making in the construction domain; deep neural networks and transfer learning applied to multimedia web mining; predicting the risk of suffering chronic social exclusion with machine learning; semantic profiling and destination recommendation based on crowd-sourced tourist reviews; robustness of coordination mechanisms in distributed problem solving against erroneous communication; a sentiment analysis model to analyze students reviews of teacher performance using support vector machines; proposal of wearable sensor-based system for foot temperature monitoring and energy analyzer emulation for energy management simulators.",,,2-s2.0-85022209944
[No author name available],"15th International Conference on Practical Applications of Agents and Multi-Agent Systems, PAAMS 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026890662&partnerID=40&md5=565e78b0ae4b1ef97ddc22090d090a9e","The proceedings contain 54 papers. The special focus in this conference is on Practical Applications of Agents and Multi-Agent Systems. The topics include: Understanding homophily and more-becomes-more through adaptive temporal-causal network models; towards a framework for agent-based simulation of user behaviour in e-commerce context; low cost architecture of autonomous subsystems for internet of things; long term reliability analysis of a microgrid on isolated mode using CPN formalism; photovoltaic inverter scheduler with the support of storage unit to minimize electricity bill; real-time emulation and simulation system of asynchronous motor consumption; economic evaluation of predictive dispatch model in MAS-based smart home; gravitational search algorithm applied for residential demand response using real-time pricing; learning frequent behaviors patterns in intelligent environments for attentiveness level; indoor children location system using BLE technology; learning to colorize infrared images; automatic construction of domain-specific sentiment lexicons for polarity classification; a hash based image matching algorithm for social networks; using twitter data to monitor political campaigns and predict election results; applying data mining for sentiment analysis in music; acceleration of dissimilarity-based classification algorithms using multi-core computation; a study on IoT technologies in smart cities; malware propagation software for wireless sensor networks; new perspectives in the study of advanced persistent threats; towards modelling organisational dynamics for large-scale multiagent systems; active ageing agents; rethinking posts through emotion awareness; big data in efficient smart grids management and multisensor indoor location system.",,,2-s2.0-85026890662
[No author name available],"AHFE 2017 International Conference on Human Error, Reliability, Resilience, and Performance, 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023185437&partnerID=40&md5=339d17314db13a36e20d6b024b2527e0","The proceedings contain 35 papers. The special focus in this conference is on Human Error, Reliability, Resilience, and Performance. The topics include: Evaluation and consolidation of the heart human reliability assessment principles; some practical considerations; simulation-based optimization of resilient communication protocol for nuclear power plant outages; task and procedure level primitives for modeling human error; operator timing of task level primitives for use in computation-based human reliability analysis; a systematic method to build a knowledge base to be used in a human reliability analysis model; task analysis as a cornerstone technique for human reliability analysis; combination of human reliability analysis and task analysis in human factors engineering; styles of thinking and human reliability in high hazard industries; a systematic framework for root-cause analysis of the Aliso canyon gas leak using the accimap methodology; a prospective study in resilience engineering; the impact of human errors on the estimation of uncertainty of measurements in water monitoring; human performance variability in task execution times under generic human-system integration conditions in naval operations; characteristics analysis for the effect of mental fatigue in monitoring task; detection of typical progress patterns of industrial incidents by text mining technique; use of a big data mining technique to extract relative importance of performance shaping factors from event investigation reports; the virtual human reliability analyst and a dynamic mechanistic model of human response proposed for human reliability analysis.",,,2-s2.0-85023185437
"Vavougios G.D., Zarogiannis S.G., Krogfelt K.A., Gourgoulianis K., Mitsikostas D.D., Hadjigeorgiou G.","Novel candidate genes of the PARK7 interactome as mediators of apoptosis and acetylation in multiple sclerosis: An in silico analysis",2018,"Multiple Sclerosis and Related Disorders",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032297806&doi=10.1016%2fj.msard.2017.10.013&partnerID=40&md5=456ef5468ded298a4c0805741089b422","Background currently only 4 studies have explored the potential role of PARK7's dysregulation in MS pathophysiology Currently, no study has evaluated the potential role of the PARK7 interactome in MS. Objective The aim of our study was to assess the differential expression of PARK7 mRNA in peripheral blood mononuclears (PBMCs) donated from MS versus healthy patients using data mining techniques. Methods The PARK7 interactome data from the GDS3920 profile were scrutinized for differentially expressed genes (DEGs); Gene Enrichment Analysis (GEA) was used to detect significantly enriched biological functions. Results 27 differentially expressed genes in the MS dataset were detected; 12 of these (NDUFA4, UBA2, TDP2, NPM1, NDUFS3, SUMO1, PIAS2, KIAA0101, RBBP4, NONO, RBBP7 AND HSPA4) are reported for the first time in MS. Stepwise Linear Discriminant Function Analysis constructed a predictive model (Wilk's λ = 0.176, χ2 = 45.204, p = 1.5275e−10) with 2 variables (TIDP2, RBBP4) that achieved 96.6% accuracy when discriminating between patients and controls. Gene Enrichment Analysis revealed that induction and regulation of programmed / intrinsic cell death represented the most salient Gene Ontology annotations. Cross-validation on systemic lupus erythematosus and ischemic stroke datasets revealed that these functions are unique to the MS dataset. Conclusions Based on our results, novel potential target genes are revealed; these differentially expressed genes regulate epigenetic and apoptotic pathways that may further elucidate underlying mechanisms of autorreactivity in MS. © 2017 Elsevier B.V.",,,2-s2.0-85032297806
"Miyauchi Y., Nishimura H.","Bayesian network modeling for specific health checkups on metabolic syndrome",2018,"Intelligent Systems Reference Library",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031994216&doi=10.1007%2f978-3-319-67513-8_5&partnerID=40&md5=9cf83570781267cbed4508911dfc6df2","Metabolic syndrome has become a significant public health problem worldwide, and Specific Health Checkup and Guidance on this syndrome began for people aged 40 to 74 in Japan in 2008. Through this guidance, people considered at high risk of developing metabolic syndrome are expected to be made aware of their own problems in terms of their daily lifestyle choices and to improve their daily life behaviors by themselves. To support this large undertaking with information technology, we have introduced ideas based on the Bayesian estimation in data mining technology and proposed a Bayesian network (BN) scheme connecting the information from physical examinations and daily lifestyle questionnaires. By applying this network model to the field data on 11,947 anonymized individuals, the proposed method was found to provide better performance and show its potentiality for the system of specific health checkup. We introduced a novel 4-bit representation with 16 states, treating body shape, blood lipids, blood glucose, and blood pressure as equal binary factors, and analyzed relationships among the support level, physical examination, and daily lifestyle questionnaire. In addition, we applied this BN to individual cases and showed its utility in allowing an examinee to improve his/her lifestyle by demonstrating individual predictions. Through the efforts described above, we confirmed that the Bayesian network for Specific Health Checkup and Guidance has the potential to be an effective support tool for health promotion regarding metabolic syndrome. © Springer International Publishing AG 2018.","Bayesian networks; Health guidance; Health promotion; Metabolic syndrome; Specific health checkup",,2-s2.0-85031994216
[No author name available],"9th KES International Conference on Intelligent Decision Technologies, KES-IDT 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020387681&partnerID=40&md5=69bf3362fb461d501cdd83184cc836c6","The proceedings contain 63 papers. The special focus in this conference is on Intelligent Decision Technologies. The topics include: The Shapley value and consistency axioms of cooperative games under incomplete information; decremental subset construction; using alloy for verifying the integration of OLAP preferences in a hybrid what-if scenario application; electrohydrodynamic effect simulation and method of its optimization; specialized decision techniques for data mining, transportation and project management; applying the intelligent decision heuristic to solve large scale technician and task scheduling problems; manipulability of majority relation-based collective decision rules; stacking-based integrated machine learning with data reduction; a hybrid approach to conceptual classification and ranking of resumes and their corresponding job posts; chaotic nature of eye movement signal; generational feature elimination to find all relevant feature subset; optimization of exact decision rules relative to length; evaluating importance for numbers of bins in discretised learning and test sets; assessing the similarity of situations and developments by using metrics; interval-valued intuitionistic fuzzy cognitive maps for supplier selection; heuristic method of air defense planning for an area object with the use of very short range air defense; an optimization problem of air defense planning for an area object; forecasting social security revenues in Jordan using fuzzy cognitive maps; fuzzy cognitive maps employing ARIMA components for time series forecasting; adjustment from inconsistent comparisons in AHP to perfect consistency and managerial decisions modelling for the company development strategy.",,,2-s2.0-85020387681
[No author name available],"8th International Conference on Soft Computing and Pattern Recognition, SoCPaR 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028586241&partnerID=40&md5=50a17fe310a0338b25c3ccc69a1a0259","The proceedings contain 70 papers. The special focus in this conference is on Soft Computing and Pattern Recognition. The topics include: Toward real-time high-frequency stock monitoring system using node.js; sensitivity analysis on effect of biomechanical factors for classifying vertebral deformities; soft feature based personal recognition; a new form of fuzzy reasoning tool to ensure both accuracy and readability; a novel edge based image steganography technique; RDE - reconstructed mutation strategy for differential evolution algorithm; predicting mobile application ratings using artificial neural network; dimensionality reduction of sift descriptor using vector decomposition for image classification; a comparative analysis of the different data mining tools by using supervised learning algorithms; a well organized phrase-based document clustering using ASCII values and adjacency list; reconstruction of 3-dimensional scenes using depth from defocus and artificial neural networks trained on fractals; performance comparison between apache hive and oracle SQL for big data analytics; design of wide beam hexagonal shaped circularly polarized patch antenna for WLAN application; an experimental comparison with time-series methods; advanced deep neural networks for pattern recognition; security enabled cluster head selection for wireless sensor network using improved firefly optimization; chaperoning the optimization of symmetric finFET circuits; solving machine part cell formation problem using genetic algorithm based evolutionary computing; DWT based source localization using microphone array; prevention of illegal content sharing in peer to peer systems; towards designing a framework for practical keystroke dynamics based authentication and exudates in detection and classification of diabetic retinopathy.",,,2-s2.0-85028586241
[No author name available],"16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020401591&partnerID=40&md5=79b0ded500f350573c2550682b6a191c","The proceedings contain 17 papers. The special focus in this conference is on Computer and Information Science. The topics include: Big data and IoT for u-healthcare security; retrospection and perspectives on pragmatic software architecture design; distributed coding and transmission scheme for wireless communication of satellite images; experimental evaluation of HoRIM to improve business strategy models; combining lexicon-based and learning-based methods for sentiment analysis for product reviews in Vietnamese language; reducing misclassification of true defects in defect classification of electronic board; a data-mining model for predicting low birth weight with a high AUC; a formal approach for maintaining forest topologies in dynamic networks; a multicriteria approach for selecting the optimal location of waste electrical and electronic treatment plants; localization strategy for island model genetic algorithm to preserve population diversity; HM-aprioriall algorithm improvement based on hadoop environment; architecture of a real-time weather monitoring system in a space-time environment using wireless sensor networks; a transducing system between hichart and XC on a visual software development environment; development of an interface for volumetric measurement on a ground-glass opacity nodule and efficient similarity measurement by the combination of distance algorithms to identify the duplication relativity.",,,2-s2.0-85020401591
[No author name available],"5th International Conference on Man-Machine Interactions, ICMMI 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030781966&partnerID=40&md5=4df79cf9a65856cd567664b343f5dc2e","The proceedings contain 56 papers. The special focus in this conference is on Man-Machine Interactions. The topics include: Deep learning with dense random neural networks; a perceptually inspired method for enhancing contrast in uneven lighting images.; advances in hand-eye robot interactions; human perception of the pattern strength measure; typing braille code in the air with the leap motion controller; touchless virtual keyboard controlled by eye blinking and EEG signals; an alternative virtual keyboard for blind people; how increasing machine agency affects human agency; eye movement traits in differentiating experts and laymen; mobile application using embedded sensors as a three dimensional motion registration method; virtual reality application to study the visual influences on human balance; improvements in DNA reads correction; semantic-based clustering of gene ontology terms on the biotest platform; comparative analysis of MicroRNA-target gene interaction prediction algorithms based on integrated P-value calculation; searching through scientific PDF files supported by bi-clustering of key terms matrices.; searching for cancer signatures using data mining techniques; consensus approach for detection of cancer somatic mutations; cancer clonal evolution simulation program; image denoising using backward stochastic differential equations; gabor filters generalization based on ateb-functions for information security; hierarchical agglomerative clustering of time-warped series; averaging of nonlinearly aligned evoked potentials in impulsive noise environment; linguistically described covariance matrix estimation; DBpedia and YAGO as knowledge base for natural language based question answering - the evaluation and expressing the notion of a mathematical structure in the formal language of mizar.",,,2-s2.0-85030781966
[No author name available],"27th Italian Workshop on Neural Networks, WIRN 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029380932&partnerID=40&md5=800bc5bf37c19022510aafbf3ca5bfd2","The proceedings contain 36 papers. The special focus in this conference is on Neural Networks. The topics include: Redefining information processing through neural computing models; a neural approach for hybrid events discrimination at stromboli volcano; fully automatic multispectral MR image segmentation of prostate gland based on the fuzzy C-means clustering algorithm; accurate computation of drude-lorentz model coefficients of single negative magnetic metamaterials using a micro-genetic algorithm approach; effective blind source separation based on the adam algorithm; depth-based hand pose recognizer using learning vector quantization; correlation dimension-based recognition of simple juggling movements; cortical phase transitions as an effect of topology of neural network; human fall detection by using an innovative floor acoustic senso; an improved hilbert-huang transform for non-linear and time-variant signals; privacy-preserving data mining for distributed medical scenarios; rule base reduction using conflicting and reinforcement measures; an application of internet traffic prediction with deep neural network; convolutional neural networks with 3-D kernels for voice activity detection in a multiroom environment; a hybrid variable selection approach for NN-based classification in industrial context; advanced neural networks systems for unbalanced industrial datasets; quantum-inspired evolutionary multiobjective optimization for a dynamic production scheduling approach; a neural network-based approach for steam turbine monitoring; a predictive model of artificial neural network for fuel consumption in engine control system; SOM-based analysis to relate non-uniformities in magnetic measurements to hot strip mill process conditions and artificial neural network analysis and erp in intimate partner violence.",,,2-s2.0-85029380932
[No author name available],"2nd International Conference on Information and Communication Technology for Intelligent Systems, ICTIS 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028371578&partnerID=40&md5=07c9889ba165e7ae20546d44b5a0dd80","The proceedings contain 147 papers. The special focus in this conference is on Information and Communication Technology for Intelligent Systems. The topics include: Survey on online social media networks facebook forensics; cooperative biometric multimodal approach for identificationp; a hardware based technique with an android application to avoid road accidents; hidden decision tree based pattern evaluation using regression models for health diagnosis; a security approach and prevention technique against ARP poisoning; a high-speed image fusion method using hardware and software co-simulation; personalized indian bschool counsellor system; proposed model for an expert system for diagnosing degenerative diseases - using digital image processing with neural network; an obscure method for clustering in android using k-medoid and apriori algorithm; extended BB84 protocol using lucas series and identity based encryption; performance analysis of WSN routing protocols with effective buffer management technique; comparison of accelerator coherency port (acp) and high performance port (HP) for data transfer in DDR memory using xilinx ZYNQ SoC; matrix factorization and regression-based approach for multi-criteria recommender system; matrix factorization and regression-based approach for multi-criteria recommender system; an ontology based recommender system to mitigate the cold start problem in personalized web search; a study on e-marketing and e-commerce for tourism development in hadoti region of rajasthan and a survey on issues of data stream mining in classification.",,,2-s2.0-85028371578
"Malafeev S.I., Malafeev S.S., Tikhonov Y.V.","Intelligent Diagnostics of Mechatronic System Components of Career Excavators in Operation",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029217890&doi=10.1007%2f978-3-319-66604-4_17&partnerID=40&md5=98a2857a46363ddb8d9a0490f71ecc50","The article provides the results of application of artificial neural networks for diagnosis of the condition of electrical mining machinery as well as the description of data collection and processing of intelligent system structure and a condition of components of mechatronic systems analysis algorithms using neural networks. Information is provided on practical implementation of algorithms in information and diagnostic systems of career excavators developed by Joint Power Co. Ltd., Moscow. © 2018, Springer International Publishing AG.","Automation; Control; Controller; Converter; Diagnosis; Efficiency; Electrical equipment; Mechatronics; Neural network; Reliability; Resource; Simulation",,2-s2.0-85029217890
"Kacprzyk J., Zadrożny S.","Reaching consensus in a group of agents: Supporting a moderator run process via linguistic summaries",2018,"Studies in Fuzziness and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021692722&doi=10.1007%2f978-3-319-60207-3_27&partnerID=40&md5=cfd78088d58586d2f75d783cde63d273","We present some account of our works on consensus reaching processes in a set of agents (individuals, decision makers, etc.), notably those which are driven by a moderator, a “super-agent” who is in charge of running the process in an effective and efficient way. We assume the classic approach to the evaluation of a degree of consensus due to Kacprzyk and Fedrizzi [19–21] in which a soft degree of consensus has been introduced as a degree to which, for instance, “most of the important individuals agree as to almost all of the relevant options”, using the fuzzy majority introduced into group decision making by Kacprzyk [17, 18] equated with a fuzzy linguistic quantifier (most, almost all,..) and handled via Zadeh’s [53] classic calculus of linguistically quantified propositions or some other method, notably Yager’s [50] OWA (ordered weighted average) operators. The consensus reaching process is run in a group of agents, which is assumed to be relatively small (e.g. human experts), by a moderator for whom some support, i.e. additional information may be useful. In our case, it is provided by a novel combination of, first, the use of the a soft degree of consensus alone within a decision support system setting along the lines of Fedrizzi et al. [5], Fedrizzi et al. [4], Kacprzyk and Zadrożny [28, 31]. Second, the linguistic data summaries in the sense of Yager [49], Kacprzyk and Yager [24], Kacprzyk et al. [25], in particular in its protoform based version proposed by Kacprzyk and Zadrożny [30, 32], are employed to indicate in a natural language some interesting relations between individuals and options to help the moderator identify crucial (pairs of) individuals and/options which pose some threats to the reaching of consensus. Third, we mention the use of some results obtained in our recent paper (Kacprzyk et al. [40]) on the use of a novel data mining tool, a so-called action rule proposed by Raś and Wieczorkowska [46], which are meant in our context to find best concessions to be offered to the individuals for changing their preferences to increase the degree of consensus. Fourth, our new results of the use of the concepts of a consensory and dissensory agent (cf. Kacprzyk and Zadrożny [37]) are summarized. © 2018, Springer International Publishing AG.","Action rule; Consensory agent; Consensus; Consensus reaching support; Dissensory agent; Fuzzy logic; Fuzzy majority; Fuzzy preference; Linguistic quantifier; Linguistic summary; OWA (ordered weighted averaging) operator",,2-s2.0-85021692722
"Fadl D., Abbas S., Aref M.","Automatic arabic ontology construction framework: An insectivore’s animal case study",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029531211&doi=10.1007%2f978-3-319-64861-3_43&partnerID=40&md5=6c3d297209a3f5f56f16f275dafdb2f7","Arabic is the official language of millions of people in the Middle East and northern African countries. Arabic ontology is the foundation of the formulation of Arabic Semantic Web. Surprisingly, little has been done in the field of computerized language and lexical resources. This paper introduces a framework that generates an Arabic Ontology from a semi-structured data (XML documents associated with graph schema), in which, XML schema is created and utilized in the graph schema development (XSG). Finally, the paper provides a case study, insectivore’s case, where the developed Arabic ontology is applied. The results consist of 143 words, 10 Concepts, 10 elements and 20 relations. The generated ontology is evaluated using data-driven evaluation method and tree based mining. 65% of the source XML documents have been included in the insectivore’s case study. This result can be refined more to reach satisfying results. © 2018, Springer International Publishing AG.","Arabic ontology; Ontology learning; XML schema",,2-s2.0-85029531211
[No author name available],"11th KES International Conference on Agent and Multi-Agent Systems - Technologies and Applications, KES-AMSTA 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020473243&partnerID=40&md5=ffa2b026650b4c96c08d46733b1d765c","The proceedings contain 23 papers. The special focus in this conference is on Agent and Multi-Agent Systems - Technologies and Applications. The topics include: Personalized healthcare and agent technologies; multiagent environments for dynamic transportation applications; microservices as agents in IoT systems; enhancing tactical information assessment using an agent-based cognitive architecture; a self-adaptive system for improving autonomy and public spaces accessibility for elderly; meaning negotiation with defeasible logic; artificial intelligence techniques for the Puerto Rico strategy game; simple bounded MTLK model checking for timed interpreted systems; an algorithm for allocating structured tasks in multi-robot scenarios; SAT-versus SMT-based BMC for TWIS and the existential fragment of WCTL with knowledge; communication and autonomous control of multi-UAV system in disaster response tasks; decision function implementation in MAREA simulations influencing financial balance of small-sized enterprise; application of I-fuzzy approach to prediction of blockability values in real world data; generalized dynamic model of rating alternatives by agents with interactions; the soft tissue implementation with triangulated mesh for virtual surgery system; pseudorehearsal in value function approximation; towards robot fall detection and management for Russian humanoid AR-601; modelling of the logistic supplier consumer behavior; conversion of real data from production process of automotive company for process mining analysis and multi-agent BPMN decision footprint.",,,2-s2.0-85020473243
[No author name available],"50th Annual Convention of Computer Society of India: ICT Based Innovations, CSI 2015",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031407324&partnerID=40&md5=c70a438c4f8e24c58797cf374ee1ba50","The proceedings contain 25 papers. The special focus in this conference is on of Computer Society of India: ICT Based Innovations. The topics include: Minimax (Maximin) with special approach of gamification in higher education; comparing the behavior of oversampling and undersampling approach of class imbalance learning by combining class imbalance problem with noise; proposed ICT-based transportation model; multi-criteria rating using fuzzy ranking for improving soil recommendation system; a way to connect farmer community to agriculture market for betterment of rural development; a methodical study on behavior of different seeds using an iterative technique with evaluation of cluster validity; bharatanatyam dance classification with rough set tools; effective and efficient digital advertisement algorithms; automation of patient information in healthcare system; recommendation for selecting smart village in india through opinion mining using big data analytics; analyzing online groups or the communities in social media networks by algorithmic approach; open source EJBCA public key infrastructure for e-governance enabled software systems in RRCAT; anticipation of gross domestic product using world development indicators; an efficacious matching of finger knuckle print images using gabor feature; an ensemble-based decision support system for the students' academic performance prediction; content-based social network aggregation; collaborative filtering-based recommender system; classifying exoplanets as potentially habitable using machine learning; towards understanding preference of use of emoticons for effective online communication and promotion; computer simulation using GPSC package MATLAB, simulink for bioinformatics professional.",,,2-s2.0-85031407324
[No author name available],"14th International Conference on Formal Concept Analysis, ICFCA 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021200552&partnerID=40&md5=b204ed4718ebe93d8f90d751e60fe442","The proceedings contain 63 papers. The special focus in this conference is on Formal Concept Analysis. The topics include: Analysis of genetic algorithm for effective power delivery and with best upsurge; edge detection of degraded stone inscription Kannada characters using fuzzy logic algorithm; performance evaluation of leaf disease measure; ear recognition using self-adaptive wavelet with neural network classifier; an efficient algorithm for mining sequential patterns; feature extraction of cervical pap smear images using fuzzy edge detection method; hybrid models for offline handwritten character recognition system without using any prior database images; image frame mining using indexing technique; a novel adaptive threshold and ISNT rule based automatic glaucoma detection from color fundus images; comparison of classification techniques for feature oriented sentiment analysis of product review data; review of texture descriptors for texture classification; improvisation in HEVC performance by weighted entropy encoding technique; design and implementation of high speed VLSI architecture of online clustering algorithm for image analysis; entity level contextual sentiment detection of topic sensitive influential twitterers using senticircles; prediction of human ethnicity from facial images using neural networks; glyph segmentation for offline handwritten Telugu characters; Hadoop framework for entity recognition within high velocity streams using deep learning; a novel random forest approach using specific under sampling strategy; real time audio steganographic countermeasure and a hybrid method for extraction of events from natural language text.",,,2-s2.0-85021200552
[No author name available],"11th International Conference on Genetic and Evolutionary Computing, 2017",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032647950&partnerID=40&md5=8186ccc542d18bdcba5ad6d90098331e","The proceedings contain 24 papers. The special focus in this conference is on Genetic and Evolutionary Computing. The topics include: Analysis of the dynamic co-purchase network based on image shape feature; vQ compression enhancer with huffman coding; adaptive steganography method based on two tiers pixel value differencing; a house price prediction for integrated web service system of Taiwan Districts; commonsense-knowledge based inference engine; analysis of users’ emotions through physiology; research on temperature rising prediction of distribution transformer by artificial neural networks; development of audio and visual attention assessment system in combination with brain wave instrument: Apply to children with attention deficit hyperactivity disorder; freeway travel time prediction by using the GA-based hammerstein recurrent neural network; markov queuing theory approach to internet of things reliability; some characteristics of nanyaseik area corundum and other assorted gemstones in Myanmar; exploring gemstones in northern part of Myanmar; attacks and solutions of a mutual authentication with anonymity for roaming service with smart cards in wireless communications; comments on Islam et al.’s certificateless designated server based public key encryption with keyword search scheme; a PIP-based approach for optimizing a group stock portfolio by grouping genetic algorithm; a novel genetic algorithm for resource allocation optimization in device-to-device communications; mining erasable itemsets using bitmap representation; identifying suspicious cases in the hong kong stock market using commentators’ stock news; a new conceptual model for big data analysis; an hybrid multi-core/GPU-based mimetic algorithm for big association rule mining; updating the discovered high average-utility patterns with transaction insertion.",,,2-s2.0-85032647950
"Arrifano G.P.F., Martín-Doimeadios R.C.R., Jiménez-Moreno M., Ramírez-Mateos V., da Silva N.F.S., Souza-Monteiro J.R., Augusto-Oliveira M., Paraense R.S.O., Macchi B.M., do Nascimento J.L.M., Crespo-Lopez M.E.","Large-scale projects in the amazon and human exposure to mercury: The case-study of the Tucuruí Dam",2018,"Ecotoxicology and Environmental Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028326006&doi=10.1016%2fj.ecoenv.2017.08.048&partnerID=40&md5=97cc11cadda0bb2048a65e268113da66","The Tucuruí Dam is one of the largest dams ever built in the Amazon. The area is not highly influenced by gold mining as a source of mercury contamination. Still, we recently noted that one of the most consumed fishes (Cichla sp.) is possibly contaminated with methylmercury. Therefore, this work evaluated the mercury content in the human population living near the Tucuruí Dam. Strict exclusion/inclusion criteria were applied for the selection of participants avoiding those with altered hepatic and/or renal functions. Methylmercury and total mercury contents were analyzed in hair samples. The median level of total mercury in hair was above the safe limit (10 µg/g) recommended by the World Health Organization, with values up to 75 µg/g (about 90% as methylmercury). A large percentage of the participants (57% and 30%) showed high concentrations of total mercury (≥ 10 µg/g and ≥ 20 µg/g, respectively), with a median value of 12.0 µg/g. These are among the highest concentrations ever detected in populations living near Amazonian dams. Interestingly, the concentrations are relatively higher than those currently shown for human populations highly influenced by gold mining areas. Although additional studies are needed to confirm the possible biomagnification and bioaccumulation of mercury by the dams in the Amazon, our data already support the importance of adequate impact studies and continuous monitoring. More than 400 hydropower dams are operational or under construction in the Amazon, and an additional 334 dams are presently planned/proposed. Continuous monitoring of the populations will assist in the development of prevention strategies and government actions to face the problem of the impacts caused by the dams. © 2017 Elsevier Inc.","Amazon; Dam; Exposure; Human; Mercury; Tucuruí","Cichla sp.; Pisces",2-s2.0-85028326006
"Suvarna Vani K., Praveen Kumar K.","Feature extraction of protein contact maps from protein 3D-coordinates",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031752257&doi=10.1007%2f978-981-10-5508-9_30&partnerID=40&md5=70063d70670549f0d7cafbb24610f2ea","This work mainly proposes an alternate way of solving challenging problems of computational biology like protein secondary structure assignment, protein fold identification/recognition, protein fold signatures, and contact map overlap problem by exploiting the idea that proteins belonging to the same protein fold have similar contact maps. Pattern mining of contact maps is conducted to extract features that pertain to fold information. Using the work in the literature that predicts contact maps from the primary amino acid sequence, we propose that using pattern features from predicted contact maps would lead to an Ab-﻿﻿Initio method. Hence, instead of extracting features from the primary amino acid sequence, we propose to extract pattern features from the protein contact maps. Protein secondary structure assignment is achieved with an accuracy of 76% on RS126 data set, on par with best of algorithms up to 10% of noise, and then the performance falls to 66% by 15% noise. © Springer Nature Singapore Pte Ltd. 2018.","Contact maps; Features; Protein structure; Secondary structure elements","Amino acids; Protein folding; Computational biology; Contact maps; Extracting features; Features; Protein contact maps; Protein secondary structure; Protein structures; Secondary structure elements; Proteins",2-s2.0-85031752257
"Priyatharshini R., Chitrakala S.","Automatic detection and quantification of calcium objects from clinical images for risk level assessment of coronary disease",2018,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032029646&doi=10.1007%2f978-3-319-63754-9_10&partnerID=40&md5=079246eb202d92e59fb7684fb930dec1","Medical diagnosis is often challenging, owing to the diversity of medical information sources. Significant advancements in healthcare technologies, potentially improving the benefits of diagnosis, may also result in data overload while the obtained information is being processed. From the beginning of time, humans have been susceptible to a surplus of diseases. Of the innumerable life-threatening diseases around, heart disease has garnered a great deal of consideration from medical researchers. Coronary Heart Disease is indubitably the commonest manifestation of Cardiovascular Disease (CVD), representing some 50% of the whole range of cardiovascular events. Medical imaging plays a key role in modern-day health care. Automatic detection and quantification of lesions from clinical images is quite an active research area where the challenge to obtain high accuracy rates is an ongoing process. This chapter presents an approach for mining the disease patterns from Cardiac CT (Computed Tomography) to assess the risk level of an individual with suspected coronary disease. © Springer International Publishing AG 2018.","Active contour model; Calcium object detection; Coronary disease diagnosis; Image segmentation; Risk level categorization",,2-s2.0-85032029646
[No author name available],"10th KES International Conference on Intelligent Interactive Multimedia Systems and Services, IIMSS 2017",2018,"Smart Innovation, Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020406138&partnerID=40&md5=7b48e5ef0e03f76353475ebaa62bb4e3","The proceedings contain 57 papers. The special focus in this conference is on Intelligent Interactive Multimedia Systems and Services. The topics include: Images selection and best descriptor combination for multi-shot person re-identification; dimensionality reduction strategies for CNN-based classification of histopathological images; optimizing multiresolution segmentation for extracting plastic greenhouses from worldview-3 imagery; greenhouse detection using aerial orthophoto and digital surface model; comparison of mesh simplification tools in a 3D watermarking framework; a smart-ca architecture for opencast matterhorn; an effective corpus-based question answering pipeline for Italian; towards a cognitive system for the identification of sleep disorders; an ensemble classifiers approach for emotion classification; sign languages recognition based on neural network architecture; medical entity and relation extraction from narrative clinical records in Italian language; detection of indoor actions through probabilistic induction model; a ROS driven platform for radiomap management optimization in fingerprinting based indoor positioning; improving spatial reasoning by interacting with a humanoid robot; an artificial pain model for a humanoid robot; interaction capabilities of a robotic receptionist; artificial pleasure and pain antagonism mechanism in a social robot; using multilayer perceptron in computer security to improve intrusion detection; a composite methodology for supporting early-detection of handwriting dysgraphia via big data analysis techniques; autonomous vehicle design for predator proof fence monitoring; sentiment analysis method for tracking touristics reviews in social media network and mobility based machine learning modeling for event mining in social networks.",,,2-s2.0-85020406138
"Doretto A., Piano E., Bona F., Fenoglio S.","How to assess the impact of fine sediments on the macroinvertebrate communities of alpine streams? A selection of the best metrics",2018,"Ecological Indicators",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028358546&doi=10.1016%2fj.ecolind.2017.08.041&partnerID=40&md5=68ae8a033030a0e10438ca09fbd3d8f3","Excessive fine sediment accumulation (i.e., siltation) in streams and rivers originates from several human activities and globally results in heavy alterations of aquatic habitats and biological communities. In this study the correlation between fine sediment and several benthic invertebrate community metrics was tested through a manipulative approach in alpine streams, where siltation mainly results as a physical alteration (i.e., the clogging of substrate interstices) without the influence of co-occurring confounding factors. We selected 12 candidate metrics, belonging to three different categories: compositional, structural and functional. We first carried out a manipulative experiment where artificial substrates were used to provide standardized conditions of siltation. All candidate metrics were calculated for each artificial substrate and the selection of the best combination of metrics was statistically performed with an information-theoretic approach. All candidate metrics were calculated both at family level and also at a mixed level (family and genus) in order to account for the systematic resolution. Then, data from a field study on alpine streams affected by mining activities were used as independent dataset for testing the performance of the selected metrics. We found that the total taxa richness, the EPT (Ephemeroptera, Plecoptera and Trichoptera) richness and the abundance of benthic invertebrates associated to rheophilous conditions and coarse mineral substrates were the most sensitive metrics. When these metrics were aggregated into a multimetric index in the validation dataset, we observed high and significant correlations between index values and the quantity of fine sediment for both taxonomic levels, especially for the mixed level. The findings of this study provide useful tools for biomonitoring the effects of fine sediment in low order, mountainous streams and contribute to improve our diagnostic ability on stressor-specific alterations. © 2017 Elsevier Ltd","Benthic invertebrates; Ecological assessment; Multimetric index; Rivers; Siltation; Taxonomic resolution","Animals; Aquatic organisms; Information theory; Rivers; Statistical tests; Substrates; Benthic invertebrates; Ecological assessment; Multi-metric indices; Siltation; Taxonomic resolution; Sediments; Ephemeroptera; Invertebrata; Plecoptera; Trichoptera",2-s2.0-85028358546
[No author name available],"4th International Conference on Advanced Computing, Networking and Informatics, ICACNI 2016",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032639968&partnerID=40&md5=8cdb85252f950230f6161c04339d9db5","The proceedings contain 61 papers. The special focus in this conference is on Advanced Computing, Networking and Informatics. The topics include: A browser-based distributed framework for content sharing and student collaboration; seed point selection algorithm in clustering of image data; comparative analysis of AHP and its integrated techniques applied for stock index ranking; a proposed what-why-how (WWH) learning model for students and strengthening learning skills through computational thinking; band power tuning of primary motor cortex EEG for continuous bimanual movements; probabilistically generated ternary quasigroup based stream cipher; dynamic access control in a hierarchy with constant key derivation cost; an efficient LWE-based additively homomorphic encryption with shorter public keys; understanding perception of cache-based side-channel attack on cloud environment; an enhanced remote user authentication scheme for multi-server environment using smartcard; a proposed bucket based feature selection technique (BBFST) for phishing e-mail classification; a novel security mechanism in symmetric cryptography using MRGA; techniques for enhancing the security of fuzzy vault: A review; an efficient vector quantization based watermarking method for image integrity authentication; xSS attack prevention using DOM-based filter; friendship recommendation system using topological structure of social networks; personalized recommendation approach for academic literature using high-utility itemset mining technique; an estimation of user preferences for search engine results and its usage patterns; a comparative analysis of various spam classifications; a semantic approach to classifying Twitter users; review spam detection using opinion mining; review spam detection using semi-supervised technique; dimensionality reduction using decision-based framework for classification: Sky and ground; comparative analysis of adaptive beamforming techniques.",,,2-s2.0-85032639968
"Kunanuntakij K., Varabuntoonvit V., Vorayos N., Panjapornpon C., Mungcharoen T.","Thailand Green GDP assessment based on environmentally extended input-output model",2018,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013498896&doi=10.1016%2fj.jclepro.2017.02.106&partnerID=40&md5=227259639014119e17e096c2831fe79b","Green GDP is an indicator of economic growth with the environmental impact on that growth factored into the traditional GDP. This study aims to develop a Green GDP model for Thailand using the EIO-LCA method. Green GDP is calculated by subtracting environmental costs from the traditional GDP. Environmental cost can be further divided into three components based on the System of Environmental-Economic Accounting (SEEA) which include depletion cost, degradation cost and defensive cost. Life cycle assessment is a tool for evaluating the environmental impact which, in this study, focuses on GHG emissions. The total direct GHG emissions for Thailand are calculated for 1990–2020 based on the 2006 IPCC guidelines and Thailand public statistical data for each economic sector. It was found that the amount of GHG emissions are 242–459 million tonnes CO2eq/yr which come from 10 economic sectors, comprised of agriculture, mining, manufacturing, petroleum refinery, power generation, gas separation, construction, commercial, residential and transportation. More than 80% of the total direct emissions come from four sectors: manufacturing (28%), power generation (26%), transportation (16%) and agriculture (15%). The portion of total GHG emissions (direct and indirect emissions) contributions is different from direct GHG emissions because the proportion of emissions from upstream processes in each sector is different. The portion of total GHG emissions of manufacturing, agriculture, transportation and residential are 60%, 22%, 13% and 5% respectively. The total Thailand Green GDP is consolidated from the results of each sector's Green GDP. The difference between Thailand's GDP and Green GDP is about 2% due to the degradation cost of GHG emissions with variability in the ratio of GDP to Green GDP across different sectors. The forecast of Green GDP by sector for 2015–2020 can be used as business-as-usual (BAU) scenarios to support policy making as well as the NAMA and INDC Action Plan for Thailand. © 2017 Elsevier Ltd","EIO-LCA method; Green GDP; Greenhouse gas emissions; Input-output analysis; Sustainable development; Thailand","Agriculture; Carbon dioxide; Costs; Economic analysis; Economics; Environmental impact; Gas emissions; Housing; Life cycle; Manufacture; Petroleum transportation; Sustainable development; Environmental costs; Environmental economics; Green GDP; Input output analysis; Input output model; LCA methods; Life Cycle Assessment (LCA); Thailand; Greenhouse gases",2-s2.0-85013498896
"Cefalo R., Calderan M., Filippi F., Montefusco C., Piemonte A., Sluga T.","The actual perspectives of GNSS multi-constellation services and receivers for kinematic applications",2018,"Lecture Notes in Geoinformation and Cartography",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028677350&doi=10.1007%2f978-3-319-56218-6_4&partnerID=40&md5=7522c866462d88b49d5b33fefcb332eb","11 years ago, on 28 December 2005, was launched the Europe’s very first navigation satellite, GIOVE-A (Galileo In-Orbit Validation Element-A), thus starting the deployment of Galileo, the EU’s own global satellite navigation system. The deployment phase of Galileo suffered of many difficulties and delays but recently the Programme has been accelerated and, in the last months, the pace of deploying Europe’s own satellite navigation system continued to increase with the launch of the 18th Galileo satellite, on 17 November 2016. It is expected that the system will be fully operational by 2020, with actual implementation costs in the range of 5.23 billion of euros and 7 billion foreseen till 2020 for EGNOS e Galileo Projects. The excellent performances of Galileo satellites, as measured on the ground, “allows Europe to join the club of the worldwide providers of satellite navigation services”. Galileo will be integrated by EGNOS (European Geostationary Navigation Overlay service). Consisting of three geostationary satellites and a network of ground stations, EGNOS achieves its aim by transmitting a signal containing information on the reliability and accuracy of the positioning signals sent out by GPS. It allows users, in Europe and beyond, to determine their position to within 1.5 m (1σ). Since the first signals became available to users, demonstrations have shown the usefulness of EGNOS services in every type of kinematic application, in the aerial, maritime and terrestrial domain. As part of ‘GIANT’ (GNSS Introduction in the AviatioN sector), tests have proved the benefits of EGNOS when landing at airports with fewer aids or when helicopters make emergency landings. The integrity data provided by EGNOS is particularly suited for applications driven by stringent safety constraints during critical navigation phases such as landing aircraft, manoeuvring ships in narrow channels, and tracking the precise locations of trains. Actually many applications are based on EGNOS, and the Open Service (OS), available since October 2009, is widely used in the agricultural world, where it has proved valuable for reducing the use of fertilisers, thus helping to protect the environment. The accuracy of the Open Service has also proved useful to guide blind people in the city via mobile phones—like car drivers using GPS. Towards the certification of the Safety of Life service, many tests have been performed in the aviation, maritime and rail sectors. Mapping of fixed assets, controlling mining machinery and other professional uses are potential applications that could benefit from the EGNOS Commercial Data. Demonstrations showing the potential of EGNOS have been performed in many other sectors and many applications are just waiting to be thought of, such as for rail, road and maritime users. The European Commission (EC) estimates that 6–7% of European GDP (Gross Domestic Product)—around 800 billion by value—is already dependent on satellite navigation. Any GNSS device available on the marked is able to receive GPS, GLONASS and EGNOS signals and globally 40% of GNSS receivers are ready to receive Galileo signals. Multi-constellation services and receivers are used by millions of persons in the world, being part of their daily life, towards a future where geo-localisation of persons and things will become essential for safety and well-being. In this paper the current trend and benefits of multi-constellation services and receivers as well as innovative kinematic research applications, carried out by GeoSNav Lab, Department of Engineering and Architecture, University of Trieste, research team, using multi-constellation receivers, are presented. © Springer International Publishing AG 2018.","Cable cars; Galileo; GNSS; Kinematic; Multi-constellation","Air navigation; Aircraft detection; Cellular telephone systems; Environmental engineering; Fighter aircraft; Geophysics; Geostationary satellites; Kinematics; Navigation systems; Network architecture; Orbits; Safety engineering; Satellite navigation aids; Satellites; Cable cars; European geostationary navigation overlay services; GALILEO; Global satellite navigation systems; GNSS; Multi-constellation; Satellite navigation services; Satellite navigation systems; Global positioning system",2-s2.0-85028677350
"Pourghasemi H.R., Yousefi S., Kornejady A., Cerdà A.","Performance assessment of individual and ensemble data-mining techniques for gully erosion modeling",2017,"Science of the Total Environment",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026378376&doi=10.1016%2fj.scitotenv.2017.07.198&partnerID=40&md5=1326a9393553bb3b61c17a16f07a0af0","Gully erosion is identified as an important sediment source in a range of environments and plays a conclusive role in redistribution of eroded soils on a slope. Hence, addressing spatial occurrence pattern of this phenomenon is very important. Different ensemble models and their single counterparts, mostly data mining methods, have been used for gully erosion susceptibility mapping; however, their calibration and validation procedures need to be thoroughly addressed. The current study presents a series of individual and ensemble data mining methods including artificial neural network (ANN), support vector machine (SVM), maximum entropy (ME), ANN-SVM, ANN-ME, and SVM-ME to map gully erosion susceptibility in Aghemam watershed, Iran. To this aim, a gully inventory map along with sixteen gully conditioning factors was used. A 70:30% randomly partitioned sets were used to assess goodness-of-fit and prediction power of the models. The robustness, as the stability of models' performance in response to changes in the dataset, was assessed through three training/test replicates. As a result, conducted preliminary statistical tests showed that ANN has the highest concordance and spatial differentiation with a chi-square value of 36,656 at 95% confidence level, while the ME appeared to have the lowest concordance (1772). The ME model showed an impractical result where 45% of the study area was introduced as highly susceptible to gullying, in contrast, ANN-SVM indicated a practical result with focusing only on 34% of the study area. Through all three replicates, the ANN-SVM ensemble showed the highest goodness-of-fit and predictive power with a respective values of 0.897 (area under the success rate curve) and 0.879 (area under the prediction rate curve), on average, and correspondingly the highest robustness. This attests the important role of ensemble modeling in congruently building accurate and generalized models which emphasizes the necessity to examine different models integrations. The result of this study can prepare an outline for further biophysical designs on gullies scattered in the study area. © 2017 Elsevier B.V.","ANN; Goodness-of-fit; MaxEnt; Prediction power; Robustness; SVM; Vulnerability","Data mining; Erosion; Forecasting; Maximum entropy methods; Neural networks; Robustness (control systems); Support vector machines; Calibration and validations; Data mining methods; Goodness of fit; MaxEnt; Performance assessment; Spatial differentiation; Susceptibility mapping; Vulnerability; Landforms; artificial neural network; data mining; gully erosion; maximum entropy analysis; numerical model; performance assessment; support vector machine; vulnerability; watershed; Article; artificial neural network; comparative study; controlled study; data mining method; environmental factor; environmental monitoring; environmental planning; geographic distribution; Iran; land use; maximum entropy; methodology; nonhuman; performance; priority journal; support vector machine; validation study; watershed management; Iran",2-s2.0-85026378376
"Lam Pham T., Kino H., Terakura K., Miyake T., Tsuda K., Takigawa I., Chi Dam H.","Machine learning reveals orbital interaction in materials",2017,"Science and Technology of Advanced Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032589753&doi=10.1080%2f14686996.2017.1378060&partnerID=40&md5=5b5486dcfa03bb0b054d078ac305b8b5","We propose a novel representation of materials named an ‘orbital-field matrix (OFM)’, which is based on the distribution of valence shell electrons. We demonstrate that this new representation can be highly useful in mining material data. Experimental investigation shows that the formation energies of crystalline materials, atomization energies of molecular materials, and local magnetic moments of the constituent atoms in bimetal alloys of lanthanide metal and transition-metal can be predicted with high accuracy using the OFM. Knowledge regarding the role of the coordination numbers of the transition-metal and lanthanide elements in determining the local magnetic moments of the transition-metal sites can be acquired directly from decision tree regression analyses using the OFM. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","data mining; machine learning; magnetic materials; Material descriptor; material informatics","Artificial intelligence; Crystalline materials; Data mining; Decision trees; Learning systems; Magnetic materials; Magnetic moments; Magnetism; Metals; Rare earth elements; Regression analysis; Transition metals; Atomization energies; Decision tree regression; Descriptors; Experimental investigations; Local magnetic moments; Material Informatics; Molecular materials; Transition metal site; Transition metal alloys",2-s2.0-85032589753
"Liu B., Ai S., Zhang W., Huang D., Zhang Y.","Assessment of the bioavailability, bioaccessibility and transfer of heavy metals in the soil-grain-human systems near a mining and smelting area in NW China",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026474513&doi=10.1016%2fj.scitotenv.2017.07.215&partnerID=40&md5=fe94f07d76f92726d042beef75a1de07","Elucidating the transfer behaviour of heavy metals from soils to grains and ultimately to humans is of great significance for both human health risk assessment and pollution control. In this study, the bioavailability of heavy metals (Cd, Cu, Pb, Zn, Cr and Ni) in farmland soils and bioaccessibility in grains (spring wheat, maize and rice) were determined to elaborate transfer dynamics in the soil-grain-human systems near a mining and smelting area in the Dongdagou watershed, Baiyin district, Gansu province, NW China. The results showed that Cd, Cu, Pb and Zn concentrations in soils were elevated compared to background levels, while Cr and Ni concentrations were relatively low throughout the region. High levels of bioavailable soil Cd were found using both EDTA and CH3COOH extraction methods. Mean concentrations of Cd, Pb and Zn in spring wheat grains and the Cd and Cr concentrations in maize grains exceeded the relevant maximum levels for pollutants according to the Chinese national standards for food safety. Except for Ni (41.90%) and Pb (31.39%), heavy metal bioaccessibility was relatively low in grains, ranging from 10.80% (Cd) to 17.18% (Zn). CH3COOH-extracted Cd, Pb and Ni, EDTA-extracted Zn, and total Cu in soils were the best indices for evaluation of uptake in grains (R2 = 0.54–0.91, p &lt; 0.001). Internal exposure doses of Cd and Ni in humans from spring wheat grain consumption was predicted by the linear correlations between bioaccessible and total metal concentrations (R2 = 0.61 and 0.67; p &lt; 0.001). The results from this study provide sufficient data and theoretical support for the use of these methods for local pollution prevention and control. © 2017 Elsevier B.V.","Agroecosystem; Bioaccessibility; Bioavailability; Heavy metals; In vitro stimulated digestion; Transfer behaviour","Biochemistry; Cadmium; Copper; Grain (agricultural product); Health risks; Heavy metals; Lead; Lead smelting; Nickel; Pollution control; Risk assessment; Smelting; Soil pollution; Soils; Zinc; Zinc smelting; Agro ecosystems; Bioaccessibility; Bioavailability; In-vitro; Transfer behaviour; Pollution; acetic acid; cadmium; chromium; copper; edetic acid; heavy metal; lead; nickel; zinc; agricultural ecosystem; agricultural land; bioavailability; concentration (composition); heavy metal; mining; pollutant transport; smelting; soil pollution; agricultural land; agroecosystem; Article; bioaccessibility; bioavailability; China; concentration (parameters); controlled study; correlational study; environmental parameters; exposure variable; food analysis; food intake; food safety; grain; heavy metal transfer; human; maize; miner; molecular dynamics; nonhuman; predictor variable; priority journal; rice; smelter; soil analysis; soil grain human system; soil pollution; soil property; spring wheat; China; Gansu; Triticum aestivum; Zea mays",2-s2.0-85026474513
"Cagnin R.C., Quaresma V.S., Chaillou G., Franco T., Bastos A.C.","Arsenic enrichment in sediment on the eastern continental shelf of Brazil",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021973727&doi=10.1016%2fj.scitotenv.2017.06.162&partnerID=40&md5=25513922982efb54a712e4ec0a0a6c08","This study focuses on the vertical distribution of total and reactive As in two contrasted coastal sedimentary environments: the Abrolhos Continental Shelf (ACS), a carbonate and siliciclastic shelf sediment, and the Doce River Continental Shelf (DRCS), a submerged delta system. The Doce River was the location of a massive ore tailings dam collapsed in November 2015. Millions of liters of tailings were dumped into the river and reached the continental shelf, causing the country's biggest environmental disaster. We evaluated the As content in sediment of the DRCS before the dam collapse. At both sites, the total As background measured in bottom sediment revealed relative natural enrichment (above 8 mg/kg). Content of As decrease with depth; reactive As showed surficial peaks which were associated with Fe and Mn oxides. The ACS sediment did not show significant enrichment or contamination of As, with an enrichment factor (EF) of approximately 2 and a geoaccumulation index (Igeo) near 0. In contrast, the DRCS exhibited severe As enrichment (EF = 15) and contamination (Igeo between 3–4). This enrichment is attributed to long-term iron and gold exploitation in the Doce River watershed. The high levels of reactive As, up to 108 mg/kg, alert us to an environmental risk due to potential As bioaccessibility. These data provide an important perspective on the As contamination in continental shelves and encourage the monitoring of the ore mine environmental impacts. © 2017 Elsevier B.V.","Arsenic; Continental shelf; Mining; Sediment","Arsenic; Mining; Ore tailings; Rivers; Sediments; Tailings; Arsenic enrichments; Continental shelves; Enrichment factors; Environmental disasters; Environmental risks; Geo-accumulation index; Sedimentary environment; Vertical distributions; Environmental impact; arsenic; gold; ion; manganese oxide; arsenic; coastal sediment; continental shelf; enrichment; environmental impact; geoaccumulation index; mining; sediment pollution; tailings dam; vertical distribution; Article; biogeochemical cycling; Brazil; concentration (parameters); continental shelf; controlled study; mine tailings; organic matter production; particle size; priority journal; risk assessment; river; sand; sea pollution; sediment; sedimentation rate; surface property; watershed; Brazil; Doce River; Houtman Abrolhos",2-s2.0-85021973727
"Broberg M.C., Uddling J., Mills G., Pleijel H.","Fertilizer efficiency in wheat is reduced by ozone pollution",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023614692&doi=10.1016%2fj.scitotenv.2017.07.069&partnerID=40&md5=01810b6d33849755359a286413a7f1cc","Inefficient use of fertilizers by crops increases the risk of nutrient leaching from agro-ecosystems, resulting in economic loss and environmental contamination. We investigated how ground-level ozone affects the efficiency by which wheat used applied nitrogen (N) fertilizer to produce grain protein (NEP, N efficiency with respect to protein yield) and grain yield (NEY, N efficiency with respect to grain yield) across a large number of open-top chamber field experiments. Our results show significant negative ozone effects on NEP and NEY, both for a larger data set obtained from data mining (21 experiments, 70 treatments), and a subset of data for which stomatal ozone flux estimates were available (7 experiments, 22 treatments). For one experiment, we report new data on N content of different above-ground plant fractions as well as grain K and P content. Our analysis of the combined dataset demonstrates that the grain yield return for a certain investment in N fertilizer is reduced by ozone. Results from the experiment with more detailed data further show that translocation of accumulated N from straw and leaves to grains is significantly and negatively affected by ozone, and that ozone decreases fertilizer efficiency also for K and P. As a result of lower N fertilization efficiency, ozone causes a risk of increased N losses from agroecosystems, e.g. through nitrate leaching and nitrous oxide emissions, a hitherto neglected negative effect of ozone. This impact of ozone on the N cycle implies that society is facing a dilemma where it either (i) accepts increased N pollution and counteracts ozone-induced yield reductions by increasing fertilization or (ii) counteracts N pollution under elevated ozone by reducing fertilization, accepting further yield loss adding to the direct effect of ozone on yield. © 2017 Elsevier B.V.","Nitrogen; Nitrogen translocation; O3; Phosphorus; Potassium; Triticum aestivum","Ecosystems; Efficiency; Fertilizers; Grain (agricultural product); Leaching; Losses; Nitrogen; Nitrogen fertilizers; Nitrogen oxides; Ozone; Phosphorus; Plants (botany); Pollution; Potassium; Proteins; Environmental contamination; Fertilizer efficiency; Ground-level ozone; Nitrous oxide emissions; Nutrient leaching; Open top chambers; Stomatal ozone fluxes; Triticum aestivum; Air pollution; grain protein; nitrogen; phosphorus; plant protein; potassium; unclassified drug; fertilizer; leaching; nitrogen; nutrient; ozone; phosphorus; potassium; wheat; agroecosystem; Article; fertilization; fertilizer application; fertilizer efficiency; fractionation; grain yield; nonhuman; ozone layer; ozone pollution; pollution; priority journal; productivity; risk factor; species translocation; wheat; Triticum aestivum",2-s2.0-85023614692
"Dutta I., Dutta S., Raahemi B.","Detecting financial restatements using data mining techniques",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028089270&doi=10.1016%2fj.eswa.2017.08.030&partnerID=40&md5=2587f1727cd1a28787c54542d72d068b","Financial restatements have been a major concern for the regulators, investors and market participants. Most of the previous studies focus only on fraudulent (or intentional) restatements and the literature has largely ignored unintentional restatements. Earlier studies have shown that large scale unintentional restatements can be equally detrimental and may erode investors’ confidence. Therefore it is important for us to pay a close to the significant unintentional restatements as well. A lack of focus on unintentional restatements could lead to a more relaxed internal control environment and lessen the efforts for curbing managerial oversights and instances of misreporting. In order to address this research gap, we focus on developing predictive models based on both intentional (fraudulent) and unintentional (erroneous) financial restatements using a comprehensive real dataset that includes 3,513 restatement cases over a period of 2001 to 2014. To the best of our knowledge it is the most comprehensive dataset used in the financial restatement predictive models. Our study also makes contributions to the datamining literature by (i) focussing on various datamining techniques and presenting a comparative analysis, (ii) ensuring the robustness of various predictive models over different time periods. We have employed all widely used data mining techniques in this area, namely, Decision Tree (DT), Artificial Neural Network (ANN), Naïve Bayes (NB), Support Vector Machine (SVM), and Bayesian Belief Network (BBN) Classifier while developing the predictive models. We find that ANN outperforms other data mining algorithms in our empirical setup in terms of accuracy and area under the ROC curve. It is worth noting that our models remain consistent over the full sample period (2001-2014), pre-financial-crisis period (2001-2008), and post-financial-crisis period (2009-2014). We believe this study will benefit academics, regulators, policymakers and investors. In particular, regulators and policymakers can pay a close attention to the suspected firms and investors can take actions in advance to reduce their investment risks. The results can also help improving expert and intelligent systems by providing more insights on both intentional and unintentional financial restatements. © 2017 Elsevier Ltd","Artificial neural network (ANN); Data mining; Decision tree (DT); Financial restatements; Naïve Bayes (NB); Support vector machine (SVM)","Bayesian networks; Decision trees; Finance; Intelligent systems; Neural networks; Sodium; Support vector machines; Trees (mathematics); Area under the ROC curve; Comparative analysis; Data mining algorithm; Financial restatements; Internal controls; Market participants; Post financial crisis; Predictive models; Data mining",2-s2.0-85028089270
"Krishnamoorthy S.","HMiner: Efficiently mining high utility itemsets",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027585254&doi=10.1016%2fj.eswa.2017.08.028&partnerID=40&md5=4b20a0ce915f701f47a8255543b85d43","High utility itemset mining problem uses the notion of utilities to discover interesting and actionable patterns. Several data structures and heuristic methods have been proposed in the literature to efficiently mine high utility itemsets. This paper advances the state-of-the-art and presents HMiner, a high utility itemset mining method. HMiner utilizes a few novel ideas and presents a compact utility list and virtual hyperlink data structure for storing itemset information. It also makes use of several pruning strategies for efficiently mining high utility itemsets. The proposed ideas were evaluated on a set of benchmark sparse and dense datasets. The execution time improvements ranged from a modest thirty percent to three orders of magnitude across several benchmark datasets. The memory consumption requirements also showed up to an order of magnitude improvement over the state-of-the-art methods. In general, HMiner was found to work well in the dense regions of both sparse and dense benchmark datasets. © 2017 Elsevier Ltd","Data mining; Frequent itemset mining; High utility mining","Data mining; Data structures; Hypertext systems; Benchmark datasets; Frequent itemset mining; High utility itemset minings; High utility itemsets; Memory consumption; State-of-the-art methods; Three orders of magnitude; Utility mining; Heuristic methods",2-s2.0-85027585254
"Mohamed N.S., Zainudin S., Ali Othman Z.","Metaheuristic approach for an enhanced mRMR filter method for classification using drug response microarray data",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028024950&doi=10.1016%2fj.eswa.2017.08.026&partnerID=40&md5=1a79033a4d6a8e7497f4308265a22bb8","Quality data mining analysis based on microarray gene expression data is a good approach for disease classification and other fields, such as pharmacology, as well as a useful tool for medical innovation. One of the challenges in classification is that microarrays involve high dimensionality and a large number of redundant and irrelevant features. Feature selection is the most popular method for determining the optimal number of features that will be used for classification. Feature selection is important to accelerate learning, which is represented only by the optimal feature subset. The current approach for microarray feature selection for the filter method is to simply select the top-ranked genes, i.e., keeping the 50 or 100 best-ranked genes. However, the current approach is determined by human intuition; it requires trial and error, and thus, is time-consuming. Accordingly, this study aims to propose a metaheuristic approach for selecting the top n relevant genes in drug microarray data to enhance the minimum redundancy–maximum relevance (mRMR) filter method. Three metaheuristics are applied, namely, particle swarm optimization (PSO), cuckoo search (CS), and artificial bee colony (ABC). Subsequently, k-nearest neighbor and support vector machine are used as classifiers to evaluate classification performance. The experiment used a microarray gene dataset of liver xenobiotic and pharmacological responses. Experimental results show that meta-heuristic is more efficient approaches that have reduced the complexity of the classifier. Furthermore, the results show that mRMR-CS exhibits the best performance compared with mRMR-PSO and mRMR-ABC. © 2017 Elsevier Ltd","Classification; Data mining; Feature selection; Filter; Microarray","Bandpass filters; Bioassay; Data mining; Evolutionary algorithms; Feature extraction; Filtration; Gene expression; Genes; Heuristic algorithms; Medical computing; Microarrays; Nearest neighbor search; Optimization; Particle swarm optimization (PSO); Quality control; Support vector machines; Artificial bee colonies (ABC); Classification performance; Disease classification; Filter; K-nearest neighbors; Meta-heuristic approach; Microarray gene expression data; Pharmacological response; Classification (of information)",2-s2.0-85028024950
"Xiao X., Attanasio A., Chiusano S., Cerquitelli T.","Twitter data laid almost bare: An insightful exploratory analyser",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028650209&doi=10.1016%2fj.eswa.2017.08.017&partnerID=40&md5=1b90834ac549ded305ca0e5c4adad925","In today's world, social networks and online communities continuously generate tons of data that reflect users’ habits, personal interests, opinions and emotions. However, little profit can be gained from such huge raw data collections unless we are able to translate them into useful knowledge. Microblogs like Twitter have recently attracted a great body of research works to mine useful insights about users interests and preferences in different geographical areas and time periods. Indeed, the rather heterogeneous dimensions characterizing Twitter data, such as space, time and text content, impose innovative methods in the data mining discovery process. This paper presents TCHARM, a data analytics methodology based on cluster analysis and association rule discovery to gain interesting knowledge from large collections of Twitter data. TCHARM explores tweet collections along the three dimensions characterizing tweets (i.e., text content, posting time and place) to support context-aware topic trend analysis. To discover groups of tweets with a good cohesion on the three tweet features, TCHARM exploits a novel distance measure (TASTE) which allows driving the clustering task by considering in one step the three tweet features. Association rule analysis is then exploited to concisely describe the cluster content with a set of understandable and significant patterns which reveal underlying correlations among frequent topics, tweeting times and places. TCHARM can provide useful information to understand the evolution of people's involvement in different topics, across geographical areas and over time. TCHARM find applications in various domains by providing a valuable support in decision making to domain experts. The experimental evaluation performed on real datasets demonstrates the effectiveness of the proposed approach in discovering cohesive clusters and actionable knowledge from Twitter data. © 2017 Elsevier Ltd","Apache spark; Association rules; Cluster analysis; Social networks; Text-spatio-temporal distance; Tweets","Association rules; Behavioral research; Cluster analysis; Data mining; Decision making; Social networking (online); Association rule analysis; Association rule discovery; Experimental evaluation; Innovative method; On-line communities; Significant patterns; Spatio temporal; Tweets; Data acquisition",2-s2.0-85028650209
"Gorawski M., Gorawska A., Pasterak K.","The TUBE algorithm: Discovering trends in time series for the early detection of fuel leaks from underground storage tanks",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028088353&doi=10.1016%2fj.eswa.2017.08.016&partnerID=40&md5=e7a2ac620cc1482d588c9e3ba9066076","Leaks and spills of hazardous fluids like petroleum endanger the environment, while remediation costs and penalties imposed when petroleum contaminates the ecosystem affect economics heavily. Therefore, it is crucial to detect any possible symptoms of a leak as soon as possible. Most of existing leak detection techniques require specialized equipment to be used, while purely software-based methods rely solely on data analysis and are very desirable since they can be deployed on petrol stations without any changes to the existing infrastructure. Moreover, such techniques can be considered as complementary to the hardware leak detection systems, as they provide additional security level. In this paper we present the TUBE algorithm, which detects fuel leaks from underground storage tanks, using only standard measurements that are normally registered on petrol stations, i.e. the amount of stored, sold, and delivered fuel. The TUBE algorithm is an autonomous solution capable of making decisions independently as well as supporting human-made decisions and thus can be considered as an expert leak detection system. The TUBE algorithm introduces a new data mining technique for trend detection and cleaning data over time series, which can be easily adapted to any other problem domain. A trend detection technique, called tubes, created for the TUBE algorithm is a novel data analysis method that allows to envelop uncertainties and oscillations in data and produce stable trends. Trend interpretation technique described in this paper has been designed especially for fuel leak detection purposes using our industrial experience. This paper includes a step-by-step usage example of the TUBE algorithm and its evaluation according to the United States Environmental Protection Agency requirements for leakage detection systems (the EPA SIR standard). Such an evaluation involves calculating the probability of detection and the probability of false alarm. The TUBE algorithm has obtained 98.84% probability of detection and 0.07% probability of false alarm while rejecting 42.22% of analyzed datasets due to their uncertainty. Rejecting datasets from analysis is compliant with the EPA SIR standard; however, rejection rate higher than 20% is not acceptable. Therefore we have evaluated the two-phase filtering stage of the algorithm in order to find the best combination of filters as means of data cleaning. Moreover, we have discussed the results pointing at the overall data quality problem, since it is the main cause of rejecting some datasets from the analysis. Finally, the TUBE algorithm has obtained 93.11% probability of detection and 0.73% probability of false alarm for the best combination of all parameters with 15.56% rejection rate, which is acceptable by the EPA SIR standard. The value of probability of detection is not fully compliant with the EPA SIR standard where 95% probability of detection with probability of false alarm lower than 5% is required. We have found that the requirements for the aforementioned probabilities have been completely fulfilled for datasets representing manifolded tank systems but not for single tank datasets. Such a situation was unexpected since manifolded tank systems are generally claimed to be more complex for analysis as they are in fact systems of multiple single tanks directly connected. In this paper we have also measured the time and memory complexity of the TUBE algorithm as well as discussed the issues connected to the TUBE algorithm deployment on petrol stations using our industrial experience in the topic. © 2017 The Authors","Anomaly detection; Leak detection; Petrol station; Quality of data; Time series; Trend detection","Alarm systems; Data handling; Data mining; Digital storage; Environmental Protection Agency; Errors; Filtration; Fuel storage; Fuels; Gasoline; Information analysis; Leak detection; Offshore pipelines; Probability; Tanks (containers); Time series; Tubes (components); Uncertainty analysis; Anomaly detection; Leakage detection systems; Petrol stations; Probability of false alarm; Quality of data; Trend detection; Underground storage tanks; United States environmental protection agency; Quality control",2-s2.0-85028088353
"Kamkarhaghighi M., Makrehchi M.","Content Tree Word Embedding for document representation",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028017103&doi=10.1016%2fj.eswa.2017.08.021&partnerID=40&md5=7a633253e22f49d65359fac091308787","Only humans can understand and comprehend the actual meaning that underlies natural written language, whereas machines can form semantic relationships only after humans have provided the parameters that are necessary to model the meaning. To enable computer models to access the underlying meaning in written language, accurate and sufficient document representation is crucial. Recently, word embedding approaches have drawn much attention in text mining research. One of the main benefits of such approaches is the use of global corpuses with the generation of pre-trained word vectors. Although very effective, these approaches have their disadvantages. Relying only on pre-trained word vectors may neglect the local context and increase word ambiguity. In this study, a new approach, Content Tree Word Embedding (CTWE), is introduced to mitigate the risk of word ambiguity and inject a local context into globally pre-trained word vectors. CTWE is basically a framework for document representation while using word embedding feature learning. The CTWE structure is locally learned from training data and ultimately represents the local context. While CTWE is constructed, each word vector is updated based on its location in the content tree. For the task of classification, the results show an improvement in F-score and accuracy measures when using two deep learning-based word embedding approaches, namely GloVe and Word2Vec. © 2017 Elsevier Ltd","Content tree; Deep learning; GloVe; Sentiment analysis; Word embedding; Word2Vec","Data mining; Deep learning; Natural language processing systems; Semantics; Vectors; Content tree; GloVe; Sentiment analysis; Word embedding; Word2Vec; Forestry",2-s2.0-85028017103
"De Bock K.W.","The best of two worlds: Balancing model strength and comprehensibility in business failure prediction using spline-rule ensembles",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026906285&doi=10.1016%2fj.eswa.2017.07.036&partnerID=40&md5=537aabffb80e3c85649bbb57550429ec","Numerous organizations and companies rely upon business failure prediction to assess and minimize the risk of initiating business relationships with partners, clients, debtors or suppliers. Advances in research on business failure prediction have been largely dominated by algorithmic development and comparisons led by a focus on improvements in model accuracy. In this context, ensemble learning has recently emerged as a class of particularly well-performing methods, albeit often at the expense of increased model complexity. However, in practice, model choice is rarely based on predictive performance alone. Models should be comprehensible and justifiable to assess their compliance with common sense and business logic, and guarantee their acceptance throughout the organization. A promising ensemble classification algorithm that has been shown to reconcile performance and comprehensibility are rule ensembles. In this study, an extension entitled spline-rule ensembles is introduced and validated in the domain of business failure prediction. Spline-rule ensemble complement rules and linear terms found in conventional rule ensembles with smooth functions with the aim of better accommodating nonlinear simple effects of individual features on business failure. Experiments on a large selection of 21 datasets of European companies in various sectors and countries (i) demonstrate superior predictive performance of spline-rule ensembles over a set of well-established yet powerful benchmark methods, (ii) show the superiority of spline-rule ensembles over conventional rule ensembles and thus demonstrate the value of the incorporation of smoothing splines, (iii) investigate the impact of alternative term regularization procedures and (iv) illustrate the comprehensibility of the resulting models through a case study. In particular, the ability of the technique to reveal the extent and the way in which predictors impact business failure, and if and how variables interact, are exemplified. © 2017 Elsevier Ltd","Bankruptcy prediction; Business failure prediction; Data mining; Ensemble learning; Model comprehensibility; Penalized cubic regression splines; Risk management; Rule ensembles; Spline-rule ensembles",,2-s2.0-85026906285
"Ding J., Dong Y., Gao T., Zhang Z., Liu Y.","Sentiment Analysis of Chinese Micro-Blog Based on Classification and Rich Features",2017,"Proceedings - 13th Web Information Systems and Applications Conference, WISA 2016 - In conjunction with 1st Symposium on Big Data Processing and Analysis, BDPA 2016 and 1st Workshop on Information System Security, ISS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027224067&doi=10.1109%2fWISA.2016.22&partnerID=40&md5=5ba0c42f5f9f084a22f0ffca015b0677","With the development of the Web2.0, micro-blogs gradually become a common essential part of the public life. The reviews in the micro-blogs have huge hidden value. Many machine learning approaches have been used to solve sentiment analysis. However, the features used in existing researches are still not enough. To improve the accuracy of sentiment analysis, in this paper, we use a classification approach to solve two tasks of sentiment analysis: identifying opinion sentence and judging sentiment polarity of the emotional sentence. And we incorporate five kinds of features: sentiment lexicons-based features, N-POS(part of speech combination)-based features, pattern-based features, special symbols-based features and length-based features to train seven classifiers and compare their performance. Experimental result shows that Random Forest classifier achieves the best performance. © 2016 IEEE.","Chinese micro-blog; classification; multiple features; sentiment analysis","Big data; Blogs; Data handling; Data mining; Decision trees; Information systems; Learning systems; Classification approach; Machine learning approaches; Micro-blog; Multiple features; Random forest classifier; Sentiment analysis; Sentiment lexicons; Special symbols; Classification (of information)",2-s2.0-85027224067
"Wang Y., Wang B., Li X.","A Case Study of Mining and Correlation Analysis of Public Security Events in Heterogeneous and Unstructured Web Messages",2017,"Proceedings - 13th Web Information Systems and Applications Conference, WISA 2016 - In conjunction with 1st Symposium on Big Data Processing and Analysis, BDPA 2016 and 1st Workshop on Information System Security, ISS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027242166&doi=10.1109%2fWISA.2016.32&partnerID=40&md5=9f00c12c3f48591e21118d6c13563ea4","Incidents of public security have an ascendant trend in recent years all over the world, and it is more important to understand the correlation of different kinds of public security incidents. With the popularization of the Internet, numerous web messages can provide resources to do that. However, an important challenge is that the web messages are often heterogeneous and unstructured. In this paper, we propose a methodology to extract events from noisy web messages automatically. Then, based on the essence and the propagation features of event ontologies, we analyze the correlation between three typical kinds of public security incidents in China. With a heterogeneous dataset containing microblog messages and news reports during 2011-2014, the case study shows that our methodology can locate the security event on timeline with an error less than 1 day. Furthermore, the analysis from multiple views not only reveals the correlation between different kinds of public security event, but also reveals the difference between the heterogeneous resources of news report and microblog. © 2016 IEEE.","Event extraction; Heterogeneous and unstructured; Public security; Social media","Data handling; Information systems; Correlation analysis; Event extraction; Event ontology; Heterogeneous and unstructured; Heterogeneous resources; Public security; Security events; Social media; Big data",2-s2.0-85027242166
"Tao L., Jiao M., Dai Y., Gao C.","A Multilayer Collaborative Filtering Recommendation Method in Electricity Market",2017,"Proceedings - 13th Web Information Systems and Applications Conference, WISA 2016 - In conjunction with 1st Symposium on Big Data Processing and Analysis, BDPA 2016 and 1st Workshop on Information System Security, ISS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027224540&doi=10.1109%2fWISA.2016.20&partnerID=40&md5=5be02578d1080420cc07bc510460dafc","Transaction price accurate recommendation is a hot issue for buyer and seller on bidding information services in Chinese electricity market, a novel multilayer collaborative filtering algorithm is proposed to solve the bidding prices accurate mining problem. A three-tier relationship model of user-item-attribute is described to accommodate the real electricity transaction on bidding price service mining and recommendation. The fuzzy evaluation method is presented to improve candidate items sets of similar neighbors, integrating user fuzzy preference by attributes into membership degree evaluation. And then, the similarity function is improved to determine better proportions in items Pearson coefficients. A case study is done to give an application example for electricity market. And the experiment is also implemented to prove that the model and the algorithm are efficient and robust for application value by performance of experiment results. © 2016 IEEE.","electricity market; fuzzy evaluation; information service; multilayer collaborative filtering; recommendation algorithm","Big data; Collaborative filtering; Commerce; Costs; Data handling; Electric industry; Information services; Information systems; Multilayers; Power markets; Application examples; Collaborative filtering algorithms; Collaborative filtering recommendations; Electricity transactions; Fuzzy evaluation; Fuzzy evaluation method; Recommendation algorithms; Similarity functions; Information filtering",2-s2.0-85027224540
"Wei L., Li Y.","Recommendation Based on Bipartite Network of Two-Step Model",2017,"Proceedings - 13th Web Information Systems and Applications Conference, WISA 2016 - In conjunction with 1st Symposium on Big Data Processing and Analysis, BDPA 2016 and 1st Workshop on Information System Security, ISS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027235274&doi=10.1109%2fWISA.2016.21&partnerID=40&md5=8a19fe838341aff611d2ea6dbc85a85c","Traditional recommendation only uses the single model of selection or rating to mining the users' interests. Network-Based Inference (NBI) is typical of selection single model. In order to make full use of information, in this paper, Recommendation based on bipartite network of two-step model(RBNTM) is put forward. The method on the basis of Network-based inference, considering scores also reflects the degree of the user preferences, combining NBI and rating prediction with two-step model to improve the accuracy of the prediction. On MoiveLens datasets experiments show that the proposed, RBNTM is effective. © 2016 IEEE.","bipartite network; recommendation; two-step model","Data handling; Information systems; Bipartite network; Network-based; recommendation; Single models; TWo-step model; Users' interests; Big data",2-s2.0-85027235274
"Zheng W.-S., Gao X., Li Y., Lai J.","Special issue on 2016 International Conference on Intelligence Science and Big Data Engineering",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021072369&doi=10.1016%2fj.neucom.2017.05.079&partnerID=40&md5=2d831da1f2524c4ee2873d8819883e4a",[No abstract available],,"behavioral science; brain computer interface; cluster analysis; conference paper; Conference Paper; connectome; data mining; facial recognition; functional magnetic resonance imaging; image analysis; image processing; intelligence science; learning algorithm; machine learning; priority journal; science; software",2-s2.0-85021072369
"Li Z., Xia Y., Ji Z., Zhang Y.","Brain voxel classification in magnetic resonance images using niche differential evolution based Bayesian inference of variational mixture of Gaussians",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020449579&doi=10.1016%2fj.neucom.2016.08.147&partnerID=40&md5=250bfde1d2bcec98e47df8df0cb66ab5","Classification of brain voxels into gray matter, white matter, and cerebrospinal fluid (CSF) using magnetic resonance imaging (MRI) is pivotal for quantitative brain analyses. In spite of its computational effectiveness, the most commonly used statistical classification models are less capable of handling the intensity non-uniformity (INU) and partial volume effect (PVE), and hence may produce less accurate results. In this paper, we propose a novel approach, namely the VMG-NDE algorithm, to improve brain voxel classification in MRI images by considering all effects simultaneously. There are four planks in this algorithm, including (1) using variational mixture of Gaussians (VMG) model to characterize the variation of voxel values caused by PVE, (2) training a cohort of local VMG models on small data volumes extracted from the image to reduce the impact of INU, (3) employing the niche differential evolution (NDE) to infer each local VMG model, aiming to avoid falling into local optima, and (4) constructing a probabilistic brain atlas for each study and using it to incorporate the anatomy prior into the classification process. After training local VMG models, we classify each brain voxel using a linear combination of the predictions generated by all those models. This algorithm has been evaluated against the variational expectation-maximization based and genetic algorithm based segmentation algorithms and the segmentation routines in the widely used statistical parametric mapping (SPM) package, expectation-maximization segmentation (EMS) package and FSL package on both synthetic and clinical T1-weighted brain MRI studies. Our results suggest that the proposed algorithm can differentiate major brain tissue types more effectively and produce improved brain voxel classification accuracy. © 2017","Differential evolution; Image segmentation; Magnetic resonance imaging (MRI); Variational Bayesian inference; Variational mixture of Gaussians","Bayesian networks; Brain; Brain mapping; Cerebrospinal fluid; Data mining; Evolutionary algorithms; Genetic algorithms; Image classification; Inference engines; Magnetic levitation vehicles; Magnetic resonance imaging; Magnetism; Maximum principle; Mixtures; Optimization; Resonance; Three dimensional computer graphics; Classification process; Differential Evolution; Expectation - maximizations; Segmentation algorithms; Statistical classification; Statistical parametric mapping; Variational Bayesian inferences; Variational mixture of Gaussians; Image segmentation; Article; Bayes theorem; cerebrospinal fluid; classification algorithm; classifier; data extraction; genetic algorithm; gray matter; image segmentation; neuroimaging; nuclear magnetic resonance imaging; prediction; priority journal; statistical distribution; white matter",2-s2.0-85020449579
"Sweeting A.J., Aughey R.J., Cormack S.J., Morgan S.","Discovering frequently recurring movement sequences in team-sport athlete spatiotemporal data",2017,"Journal of Sports Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011900726&doi=10.1080%2f02640414.2016.1273536&partnerID=40&md5=03d16d6d113e547385477a1523180f02","Athlete external load is typically analysed from predetermined movement thresholds. The combination of movement sequences and differences in these movements between playing positions is also currently unknown. This study developed a method to discover the frequently recurring movement sequences across playing position during matches. The external load of 12 international female netball athletes was collected by a local positioning system during four national-level matches. Velocity, acceleration and angular velocity were calculated from positional (X, Y) data, clustered via one-dimensional k-means and assigned a unique alphabetic label. Combinations of velocity, acceleration and angular velocity movement were compared using the Levenshtein distance and similarities computed by the longest common substring problem. The contribution of each movement sequence, according to playing position and relative to the wider data set, was then calculated via the Minkowski distance. A total of 10 frequently recurring combinations of movement were discovered, regardless of playing position. Only the wing attack, goal attack and goal defence playing positions are closely related. We developed a technique to discover the movement sequences, according to playing position, performed by elite netballers. This methodology can be extended to discover the frequently recurring movements within other team sports and across levels of competition. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","activity profile; data mining; Netball; WASP",,2-s2.0-85011900726
"Liu J., Wang J., Li G., Chen H., Shen L., Xing L.","Evaluation of the energy performance of variable refrigerant flow systems using dynamic energy benchmarks based on data mining techniques",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031418128&doi=10.1016%2fj.apenergy.2017.09.116&partnerID=40&md5=a20bf0ef3652961f9dfbf246709d4c9d","The variable refrigerant flow (VRF) system has extremely different energy performance at various operation conditions. Its power consumption is inconsistent even under the steady operation condition. In order to accurately evaluate the VRF system's dynamic energy performance, this study proposed a data-mining-based method to benchmark and assess its energy uses. The correlation analysis is used for key factors selection and the interquartile range rule is employed to remove outliers of the database. In addition, the power consumption patterns are classified using decision tree (DT) method. The classification results are validated by the ANOVA analysis and post hoc test. Nine energy benchmarks are established based on the classified power consumption patterns. Moreover, an energy consumption rating system is established to provide quantitative assessment on the power consumption of the VRF system. A case study is conducted by comparatively analyzing the energy performance of the VRF system at multiple refrigerant charge fault cases. Results show that both the PLR and OT significantly affected the power consumption of the VRF system. However, the degree to which the refrigerant charge fault affects system power consumption varies with the power consumption patterns. For different patterns, the power consumptions of the VRF system were either lower, higher or similar to each other at various RCLs. Results also suggest that the energy benchmarking process provide reasonable classification criteria, and the grading process provide quantitative assessment on the energy consumption. Therefore, the proposed dynamic energy benchmarks are reliable and reasonable to evaluate the dynamic energy performance of VRF systems. © 2017 Elsevier Ltd","Data mining techniques; Energy benchmarking; Energy performance evaluation; Variable refrigerant flow","Benchmarking; Decision trees; Electric power utilization; Energy efficiency; Energy utilization; Fluidized bed combustion; Grading; Hand held computers; Refrigerants; Classification criterion; Classification results; Correlation analysis; Energy benchmarking; Energy performance evaluations; Inter quartile ranges; Quantitative assessments; Refrigerant flow; Data mining",2-s2.0-85031418128
"Bacchelli A., Mocci A., Cleve A., Lanza M.","Mining structured data in natural language artifacts with island parsing",2017,"Science of Computer Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027453302&doi=10.1016%2fj.scico.2017.06.009&partnerID=40&md5=379451768777028390d736928ca518a7","Software repositories typically store data composed of structured and unstructured parts. Researchers mine this data to empirically validate research ideas and to support practitioners' activities. Structured data (e.g., source code) has a formal syntax and is straightforward to analyze; unstructured data (e.g., documentation) is a mix of natural language, noise, and snippets of structured data, and it is harder to analyze. Especially the structured content (e.g., code snippets) in unstructured data contains valuable information. Researchers have proposed several approaches to recognize, extract, and analyze structured data embedded in natural language. We analyze these approaches and investigate their drawbacks. Subsequently, we present two novel methods, based on scannerless generalized LR (SGLR) and Parsing Expression Grammars (PEGs), to address these drawbacks and to mine structured fragments within unstructured data. We validate and compare these approaches on development emails and Stack Overflow posts with JAVA code fragments. Both approaches achieve high precision and recall values, but the PEG-based one achieves better computational performances and simplicity in engineering. © 2017","Island parsing; Mining software repositories; Unstructured data","Codes (symbols); Precision engineering; Computational performance; Formal syntaxes; Island parsing; Mining software repositories; Natural languages; Parsing expression grammars; Software repositories; Unstructured data; Syntactics",2-s2.0-85027453302
"Garciarena U., Santana R.","An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025602749&doi=10.1016%2fj.eswa.2017.07.026&partnerID=40&md5=15deff76ab7e64d1fcc6be9266e20583","When applying data-mining techniques to real-world data, we often find ourselves facing observations that have no value recorded for some attributes. This can be caused by several phenomena, such as a machine's incapability to record certain characteristics or a person refusing to answer a question in a poll. Depending on that motivation, values gone missing may follow one kind of pattern or another, or describe no regularity at all. One approach to palliate the effect of missing data on machine learning tasks is to replace the missing observations. Imputation algorithms attempt to calculate a value for a missing gap, using information associated with it, i.e., the attribute and/or other values in the same observation. While several imputation methods have been proposed in the literature, few works have addressed the question of the relationship between the type of missing data, the choice of the imputation method, and the effectiveness of classification algorithms that used the imputed data. In this paper we address the relationship among these three factors. By constructing a benchmark of hundreds of databases containing different types of missing data, and applying several imputation methods and classification algorithms, we empirically show that an interaction between imputation methods and supervised classification can be deduced. Besides, differences in terms of classification performance for the same imputation method in different missing data patterns have been found. This points to the convenience of considering the combined choice of the imputation method and the classifier algorithm according to the missing data type. © 2017 Elsevier Ltd","Imputation methods; Machine learning; Missing data; Supervised classifiers","Artificial intelligence; Data mining; Education; Learning systems; Supervised learning; Classification algorithm; Classification performance; Classifier algorithms; Imputation algorithm; Imputation methods; Missing data; Supervised classification; Supervised classifiers; Classification (of information)",2-s2.0-85025602749
"Carvalho J.P., Rosa H., Brogueira G., Batista F.","MISNIS: An intelligent platform for twitter topic mining",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026830341&doi=10.1016%2fj.eswa.2017.08.001&partnerID=40&md5=bcc131711e34911bf06492478226a8d0","Twitter has become a major tool for spreading news, for dissemination of positions and ideas, and for the commenting and analysis of current world events. However, with more than 500 million tweets flowing per day, it is necessary to find efficient ways of collecting, storing, managing, mining and visualizing all this information. This is especially relevant if one considers that Twitter has no ways of indexing tweet contents, and that the only available categorization “mechanism” is the #hashtag, which is totally dependent of a user's will to use it. This paper presents an intelligent platform and framework, named MISNIS - Intelligent Mining of Public Social Networks’ Influence in Society - that facilitates these issues and allows a non-technical user to easily mine a given topic from a very large tweet's corpus and obtain relevant contents and indicators such as user influence or sentiment analysis. When compared to other existent similar platforms, MISNIS is an expert system that includes specifically developed intelligent techniques that: (1) Circumvent the Twitter API restrictions that limit access to 1% of all flowing tweets. The platform has been able to collect more than 80% of all flowing portuguese language tweets in Portugal when online; (2) Intelligently retrieve most tweets related to a given topic even when the tweets do not contain the topic #hashtag or user indicated keywords. A 40% increase in the number of retrieved relevant tweets has been reported in real world case studies. The platform is currently focused on Portuguese language tweets posted in Portugal. However, most developed technologies are language independent (e.g. intelligent retrieval, sentiment analysis, etc.), and technically MISNIS can be easily expanded to cover other languages and locations. © 2017 Elsevier Ltd","Fuzzy fingerprints; Intelligent topic mining; Sentiment analysis; Text analytics; Twitter","Expert systems; Social networking (online); Fuzzy fingerprints; Sentiment analysis; Text analytics; Topic minings; Twitter; Data mining",2-s2.0-85026830341
"Zhou F., Jiao J.R., Yang X.J., Lei B.","Augmenting feature model through customer preference mining by hybrid sentiment analysis",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026664625&doi=10.1016%2fj.eswa.2017.07.021&partnerID=40&md5=09f68777492af4b68a265c54032bb97b","A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets. © 2017","Customer preference mining; Feature model; Product line planning; Sentiment analysis","Data mining; Sales; Customer preferences; Feature modeling; Individual customers; Online product reviews; Product feature; Product variants; Product-lines; Sentiment analysis; Customer satisfaction",2-s2.0-85026664625
"Rana T.A., Cheah Y.-N.","A two-fold rule-based model for aspect extraction",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026436878&doi=10.1016%2fj.eswa.2017.07.047&partnerID=40&md5=0b0cfb4da72f112226c0472faa3a8200","Opinion target extraction or aspect extraction is the most important subtask of the aspect-based sentiment analysis. This task focuses on the identification of the targets of user's opinions or sentiments from online reviews. In the recent years, syntactic patterns-based approaches have performed quite well and produced significant improvement in the aspect extraction task. However, these approaches are heavily dependent on the dependency parsers which produced syntactic relations following the grammatical rules and language constraints. In contemporary, users do not give much importance to these rules and constraints while expressing their opinions about particular product and neither reviewer websites restrict users to do so. This makes syntactic patterns-based approaches vulnerable. Therefore, in this paper, we are proposing a two-fold rules-based model (TF-RBM) which uses rules defined on the basis of sequential patterns mined from customer reviews. The first fold extracts aspects associated with domain independent opinions and the second fold extracts aspects associated with domain dependent opinions. We have also applied frequency- and similarity-based approaches to improve the aspect extraction accuracy of the proposed model. Our experimental evaluation has shown better results as compared with the state-of-the-art and most recent approaches. © 2017 Elsevier Ltd","Aspect extraction; Aspect pruning; Aspect-based sentiment analysis; Explicit aspects; Opinion mining; Sequential pattern-based rules","Data mining; Extraction; Aspect pruning; Explicit aspects; Opinion mining; Sentiment analysis; Sequential patterns; Syntactics",2-s2.0-85026436878
"Heng J., Wang J., Xiao L., Lu H.","Research and application of a combined model based on frequent pattern growth algorithm and multi-objective optimization for solar radiation forecasting",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029639631&doi=10.1016%2fj.apenergy.2017.09.063&partnerID=40&md5=841afa61fa50b015e0553fbb3821c703","Solar radiation forecasting plays a significant role in precisely designing solar energy systems and in the efficient management of solar energy plants. Most research only focuses on accuracy improvements; however, for an effective forecasting model, considering only accuracy or stability is inadequate. To solve this problem, a combined model based on nondominated sorting-based multiobjective bat algorithm (NSMOBA) is developed for the optimization of weight coefficients of each model to achieve high accuracy and stability results simultaneously. In addition, a statistical method and data mining-based approach are used to determine the input variables for constructing the combined model. Monthly average solar radiation and meteorological variables from six datasets in the U.S. collected for case studies were used to assess the comprehensive performance (both in accuracy and stability) of the proposed combined model. The simulation in four experiments demonstrated the following: (a) the proposed combined model is suitable for providing accurate and stable solar radiation forecasting; (b) the combined model exhibits a more competitive forecasting performance than the individual models by using the advantage of each model; (c) the NSMOBA is an efficient algorithm for providing accurate forecasting results and improving the stability where the single bat algorithm is insufficient. © 2017 Elsevier Ltd","Combined model; Forecasting accuracy and stability; Frequent pattern growth algorithm; Global solar radiation forecasting; Non-dominated sorting based multi-objective bat algorithm","Data mining; Forecasting; Optimization; Solar energy; Solar radiation; Sorting; Stability; Combined model; Forecasting accuracy; Frequent pattern growth; Global solar radiation; Multi objective; Multiobjective optimization",2-s2.0-85029639631
"Escalante H.J., Villatoro-Tello E., Garza S.E., López-Monroy A.P., Montes-y-Gómez M., Villaseñor-Pineda L.","Early detection of deception and aggressiveness using profile-based representations",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026377974&doi=10.1016%2fj.eswa.2017.07.040&partnerID=40&md5=e2bd5c489fc1164005d32966220ca71f","E-communication represents a major threat to users who are exposed to a number of risks and potential attacks. Detecting these risks with as much anticipation as possible is crucial for prevention. However, much research so far has focused on forensic tools that can be applied only when an attack has been performed. This paper proposes a novel and effective methodology for the early detection of threats in written social media. The goal is to recognize a potential attack before it is consummated, and using a minimum amount of information. The proposed approach considers the use of profile-based representations (PBRs) for this goal. PBRs have multiple benefits, including non-sparsity, low dimensionality, and a proved discriminative power. Moreover, representations for partial documents can be derived naturally with PBRs, which makes them suitable for the addressed problem. Results include empirical evidence on the usefulness of PBRs in the early recognition setting for two tasks in which anticipation is critical: sexual predator detection and aggressive text identification. These results reveal, on the one hand, that PBRs achieve state of the art performance when using full-length documents (i.e., the classical task), and, on the other hand, that the proposed methodology outperforms previous work on early recognition of sexual predators by a considerable margin, while obtaining state of the art performance in aggressive text identification. To the best of our knowledge, these are the best results reported on early recognition for the approached problems. We foresee this work will pave the way for the development of novel methodologies for the problem and will motivate further research from the intelligent systems and text mining communities. © 2017 Elsevier Ltd","Aggressive text identification; Deception detection; Profile-based representations; Sexual predator detection","Data mining; Intelligent systems; Amount of information; Deception detection; Detection of deceptions; Discriminative power; Low dimensionality; Profile-based representations; State-of-the-art performance; Text identification; Character recognition",2-s2.0-85026377974
"Marques P., Marto J., Gonçalves L.M., Pacheco R., Fitas M., Pinto P., Serralheiro M.L.M., Ribeiro H.","Cynara scolymus L.: A promising Mediterranean extract for topical anti-aging prevention",2017,"Industrial Crops and Products",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029705858&doi=10.1016%2fj.indcrop.2017.09.033&partnerID=40&md5=90e2ce5ce01bbf820b511c5d8dd43c1d","Cynara scolymus L. (CS), usually known as globe artichoke, is a Mediterranean plant with antioxidant properties due to its content of polyphenols, including caffeoylquinic and dicaffeoylquinic acids. The use of bioactive ingredients or phytochemicals extracted from plant tissues in cosmetics is increasing, thus artichoke extract due to its promising constituents can be incorporated in topical formulations. The aim of this study was the chemical characterization of different CS extracts in order to investigate the antioxidant and the sun protection potential in topical formulations. Aqueous extracts were analyzed by HPLC to quantify target compounds, such as cynarin, chlorogenic acid and cynaroside. Antioxidant activity by DPPH assay and ROS scavenging activity in HaCaT cells, as well as cytotoxicity assays and sun protection factor were assessed. The results showed that CS extract and CSC fraction, one of the purified fractions, were rich in polyphenols and presented antioxidant and photoprotective activity. Then, both fractions were incorporated in two topical formulations: O/W emulsion and hydrogel. Physicochemical characterization, microbiological control, cytotoxicity assays and ROS scavenging activity in HaCaT cells were performed to ensure the quality, safety and efficacy of the products developed. In vivo studies, Human Repeat Insult Patch Testing and an assay to determine their antioxidant capacity, were also performed. Besides the excellent antioxidant and photoprotective activity, the final formulations proved to be also suitable and safe for topical use. © 2017 Elsevier B.V.","Antioxidant activity; Cynara scolymus L.; Globe artichoke; Plant extract; Sun protection factor; Topical formulation","Agents; Antioxidants; Assays; Emulsification; Emulsions; Plant extracts; Solar radiation; Textiles; Anti-oxidant activities; Cynara scolymus L; Globe artichoke; Sun protection factor; Topical formulations; Data mining; antioxidant; aqueous solution; bioactivity; bioassay; emulsion; factor analysis; herb; phytochemistry; plant extract; testing method; Cynara scolymus",2-s2.0-85029705858
"McKee L.J., Bonnema A., David N., Davis J.A., Franz A., Grace R., Greenfield B.K., Gilbreath A.N., Grosso C., Heim W.A., Hunt J.A., Leatherbarrow J.E., Lowe S., Pearce S.A., Ross J.R.M., Yee D.","Long-term variation in concentrations and mass loads in a semi-arid watershed influenced by historic mercury mining and urban pollutant sources",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021682402&doi=10.1016%2fj.scitotenv.2017.04.203&partnerID=40&md5=fce0684a7daf3bd4477797ee0d8a6ea8","Urban watersheds are significantly anthropogenically-altered landscapes. Most previous studies cover relatively short periods, without addressing concentrations, loads, and yields in relation to annual climate fluctuations, and datasets on Ag, Se, PBDEs, and PCDD/Fs are rare. Intensive storm-focused sampling and continuous turbidity monitoring were employed to quantify pollution at two locations in the Guadalupe River (California, USA). At a downstream location, we determined loads of suspended sediment (SS) for 14 yrs., mercury (HgT), PCBs, and total organic carbon (TOC) (8 yrs), total methylmercury (MeHgT) (6 yrs), nutrients, and trace elements including Ag and Se (3 yrs), DDTs, chlordanes, dieldrin, and PBDEs (2 yrs), and PCDD/Fs (1 yr). At an upstream location, we determined loads of SS for 4 yrs. and HgT, MeHgT, PCBs and PCDD/Fs for 1 yr. These data were compared to previous studies, climatically adjusted, and used to critically assess the use of small datasets for estimating annual average conditions. Concentrations and yields in the Guadalupe River appear to be atypical for total phosphorus, DDTs, dieldrin, HgT, MeHgT, Cr, Ni, and possibly Se due to local conditions. Other pollutants appear to be similar to other urban systems. On average, wet season flow varied by 6.5-fold and flow-weighted mean (FWM) concentrations varied 4.4-fold, with an average 7.1-fold difference between minimum and maximum annual loads. Loads for an average runoff year for each pollutant were usually less than the best estimate of long-term average. The arithmetic average of multiple years of load data or a FWM concentration combined with mean annual flow was also usually below the best estimate of long-term average load. Mean annual loads using sampled years were also less than the best estimate of long-term average by a mean of 2.2-fold. Climatic adjustment techniques are needed for computing estimates of long-term average annual loads. © 2016 Elsevier B.V.","Inter-annual variation; Management; Methylmercury; PCB; Turbidity surrogate; Yield","Carbon; Insecticides; Location; Management; Mercury (metal); Organic carbon; Organic pollutants; Pollution; Polychlorinated biphenyls; Rivers; Silver; Suspended sediments; Trace elements; Turbidity; Water pollution; Climate fluctuations; Interannual variation; Local conditions; Methyl mercury; Semi-arid watershed; Total Organic Carbon; Turbidity monitoring; Yield; River pollution; chlordane; chlorphenotane; chromium; dieldrin; mercury; methylmercury; nickel; organochlorine pesticide; phosphorus; polybrominated diphenyl ether; polychlorinated biphenyl; polychlorinated dibenzodioxin; polychlorinated dibenzofuran; selenium; silver; annual variation; concentration (composition); management; mercury (element); methylmercury; mining; PCB; pollutant source; river pollution; semiarid region; turbidity; urban pollution; watershed; Article; climate change; concentration (parameters); controlled study; environmental monitoring; mining; nutrient; particle resuspension; pollutant mass load; priority journal; runoff; seasonal variation; total organic carbon; United States; urban area; water and water related phenomena; water pollution; watershed; California; Guadalupe River [California]; United States",2-s2.0-85021682402
"Li Z., Chen B., Ning D., Song Z., Guo G., Qiu X.","Partitioning large-scale artificial society on distributed cluster with statistical movement graph",2017,"Journal of Statistical Computation and Simulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028551378&doi=10.1080%2f00949655.2017.1369540&partnerID=40&md5=1be1f85b47c9a0b353b392a3f3611fb9","Distributed agent-based simulation is a popular method to realize computational experiment on large-scale artificial society. Meanwhile, the partitioning strategy of the artificial society models among hosts is playing an essential role for simulation engine to offer high execution efficiency as it has great impact on the communication overheads and computational load-balancing during simulation. Aiming at the problem, we firstly analyze the execution and scheduling process of agents during simulation and model it as wide-sense cyclostationary random process. Then, a static statistical partitioning model is proposed to obtain the optimal partitioning strategy with minimum average communication cost and load imbalance factor. To solve the static statistical partitioning model, this paper turns it into a graph-partitioning problem. A statistical movement graph-based partitioning algorithm is then devised which generates task graph model by mining the statistical movement information from initialization data of simulation model. In the experiments, two other popular partitioning methods are used to evaluate the performance of proposed graph-partitioning algorithm. Furthermore, this paper compares the graph-partitioning performance under different task graph model. The results indicate that our proposed statistical movement graph-based static partitioning method outperforms all other methods in reducing the communication overhead while satisfying the load balance constraint. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Artificial society; cyclostationary random process; distributed agent-based simulation; graph partitioning; workload partitioning",,2-s2.0-85028551378
"Giraldo J.D., Rivas B.L., Elgueta E., Mancisidor A.","Metal ion sorption by chitosan–tripolyphosphate beads",2017,"Journal of Applied Polymer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025443265&doi=10.1002%2fapp.45511&partnerID=40&md5=306cefd33ab42c4ca0ec3c9465aa641a","Heavy metal removal from wastewater is crucial for the proper management of discharged water from mining operations. This residual water is typically unusable for other purposes such as for human/animal, crop, or industrial consumption. Eco-friendly adsorption materials are necessary to ensure the sustainable treatment of this wastewater. Therefore, the sorption of Cu(II), Cd(II), Pb(II), and Zn(II) ions onto chitosan–tripolyphosphate (CTPP) beads was investigated using real mining wastewater and prepared ion metal solutions. The effects of pH, contact time, temperature, selectivity, and maximum sorption capacity in successive batches at different concentrations were studied. The optimum sorption of cations, except for copper (pH 3) was found at pH 5. Equilibrium in the adsorption of all metals was reached at 24 h of contact. Studies of the maximum sorption capacity at different concentrations showed that the CTPP beads could adsorb 158, 55, 47, and 47 mg/g of Pb(II), Cu(II), Cd(II), and Zn(II), respectively. Experimental data for the sorption of Pb(II) were optimally correlated with the Langmuir model. The thermodynamic parameters such as the changes in enthalpy (ΔH0), entropy (ΔS0), and free energy (ΔG0) were determined. © 2017 Wiley Periodicals, Inc. J. Appl. Polym. Sci. 2017, 134, 45511. © 2017 Wiley Periodicals, Inc.","adsorption; biomaterials; resins","Adsorption; Biomaterials; Cadmium; Cadmium compounds; Chitin; Chitosan; Copper; Heavy metals; Lead; Metal ions; Metals; Resins; Sorption; Wastewater treatment; Zinc; Zinc compounds; Adsorption materials; Heavy metal removal; Industrial consumption; Metal ion sorptions; Sorption capacities; Sustainable treatments; Thermodynamic parameter; Tripolyphosphates; Chemicals removal (water treatment)",2-s2.0-85025443265
"Zhong X., Enke D.","A comprehensive cluster and classification mining procedure for daily stock market return forecasting",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020937035&doi=10.1016%2fj.neucom.2017.06.010&partnerID=40&md5=b5aff5d0b063ae9c1167a4b05cf59ac9","Data mining and big data analytic techniques are playing an important role in many application fields, including the financial markets. However, only few studies have focused on predicting daily stock market returns, and among these studies, the data mining procedures utilized are either incomplete or inefficient. This paper presents a comprehensive data mining process to forecast the daily direction of the S&P 500 Index ETF (SPY) return based on 60 financial and economical features. The fuzzy c-means method (FCM) is initially used to cluster the preprocessed data. A principal component analysis (PCA) is applied next to the entire data set and each of seven clusters. The dimension of the entire cleaned data set is then reduced according to the combining results from the entire data set and each cluster. Corresponding to different levels of the dimensionality reduction, twelve new data sets are generated from the entire cleaned data. Artificial neural networks (ANNs) and logistic regression models are then used with the twelve transformed data sets for classification in order to forecast the daily direction of future market returns and indicate the efficiency of dimensionality reduction with PCA. A group of hypothesis tests are performed over the classification and simulation results to show that the ANNs give significantly higher classification accuracy than logistic regression, and that the trading strategies guided by the comprehensive cluster and classification mining procedure based on PCA and ANNs gain higher risk-adjusted profits than the comparison benchmarks, as well as those strategies guided by the forecasts based on PCA and logistic regression models. © 2017 Elsevier Ltd","Artificial neural networks (ANNs); Daily stock return forecasting; Data mining; Fuzzy c-means (FCM); Logistic regression; Principal component analysis (PCA)","Classification (of information); Commerce; Data mining; Economic analysis; Electronic trading; Finance; Financial data processing; Financial markets; Forecasting; Fuzzy neural networks; Fuzzy systems; Investments; Neural networks; Principal component analysis; Reduction; Regression analysis; Classification accuracy; Classification mining; Dimensionality reduction; Fuzzy C mean; Fuzzy c-means methods; Logistic regression models; Logistic regressions; Stock return forecasting; Big data; accuracy; Article; artificial neural network; benchmarking; cluster analysis; data mining; financial information system; financial management; forecasting; fuzzy c means method; fuzzy system; logistic regression analysis; principal component analysis; priority journal; process optimization; simulation; stock market return",2-s2.0-85020937035
"Feng X., Kong X., Xu D., Qin J.","A fast and effective principal singular subspace tracking algorithm",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020807939&doi=10.1016%2fj.neucom.2017.06.006&partnerID=40&md5=8fd1046061e5f398b330c23d60a64217","In this paper, we propose a fast and effective neural network algorithm to perform singular value decomposition (SVD) of a cross-covariance matrix between two high-dimensional data streams. Firstly, we derive a dynamical system from a newly proposed information criterion. This system exhibits a single stable stationary point if and only if the weight matrices of the left and right neural networks span the left and right principal singular subspace of a cross-covariance matrix, respectively, and the other stationary points are (unstable) saddle points. Then, a principal singular subspace (PSS) tracking algorithm is obtained from the dynamical system. Moreover, convergence analysis shows that the proposed algorithm converges to a stationary point that relates to the principal singular values. Thus, compared with traditional algorithms who can only track the PSS, the proposed algorithm can not only track the PSS but also estimate all of the corresponding principal singular values based on the extracted subspace. Finally, numerical simulations and practical application are carried to further demonstrate the efficiency of the proposed algorithm. © 2017 Elsevier B.V.","Cross-correlation neural network; Principal singular subspace; Principal singular value; Singular value decomposition","Clustering algorithms; Covariance matrix; Data mining; Dynamical systems; Tracking (position); Convergence analysis; Cross correlations; Cross covariance matrices; High-dimensional data streams; Information criterion; Neural network algorithm; Singular subspaces; Singular values; Singular value decomposition; algorithm; Article; artificial neural network; data analysis; information processing; intermethod comparison; mathematical analysis; principal singular subspace tracking algorithm; priority journal; simulation",2-s2.0-85020807939
"Kang Z., Peng C., Cheng Q.","Kernel-driven similarity learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020776356&doi=10.1016%2fj.neucom.2017.06.005&partnerID=40&md5=11ac5f93771154c23589f19026180f17","Similarity measure is fundamental to many machine learning and data mining algorithms. Predefined similarity metrics are often data-dependent and sensitive to noise. Recently, data-driven approach which learns similarity information from data has drawn significant attention. The idea is to represent a data point by a linear combination of all (other) data points. However, it is often the case that more complex relationships beyond linear dependencies exist in the data. Based on the well known fact that kernel trick can capture the nonlinear structure information, we extend this idea to kernel spaces. Nevertheless, such an extension brings up another issue: its algorithm performance is largely determined by the choice of kernel, which is often unknown in advance. Therefore, we further propose a multiple kernel-based learning method. By doing so, our model can learn both linear and nonlinear similarity information, and automatically choose the most suitable kernel. As a result, our model is capable of learning complete similarity information hidden in data set. Comprehensive experimental evaluations of our algorithms on clustering and recommender systems demonstrate its superior performance compared to other state-of-the-art methods. This performance also shows the great potential of our proposed algorithm for other possible applications. © 2017 Elsevier B.V.","Clustering; Kernel method; Multiple kernel learning; Nonlinear relation; Recommender systems; Similarity measure; Sparse representation","Data mining; Learning systems; Recommender systems; Clustering; Kernel methods; Multiple Kernel Learning; Nonlinear relations; Similarity measure; Sparse representation; Clustering algorithms; algorithm; Article; cluster analysis; controlled study; kernel method; linear system; machine learning; nonlinear system; priority journal; similarity learning",2-s2.0-85020776356
"Saxena A., Prasad M., Gupta A., Bharill N., Patel O.P., Tiwari A., Er M.J., Ding W., Lin C.-T.","A review of clustering techniques and developments",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021901708&doi=10.1016%2fj.neucom.2017.06.053&partnerID=40&md5=2380c4d11327605fc99fe27d100b80a8","This paper presents a comprehensive study on clustering: exiting methods and developments made at various times. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some similarity inherent among them. There are different methods for clustering the objects such as hierarchical, partitional, grid, density based and model based. The approaches used in these methods are discussed with their respective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are the central components of clustering, are also presented in the paper. The applications of clustering in some fields like image segmentation, object and character recognition and data mining are highlighted. © 2017 Elsevier B.V.","Clustering; Data mining; Pattern recognition; Similarity measures; Unsupervised learning","Character recognition; Data mining; Education; Image segmentation; Unsupervised learning; Central component; Clustering; Clustering techniques; Density-based; Evaluation criteria; Model-based OPC; Similarity measure; Pattern recognition; algorithm; Article; artificial neural network; automated pattern recognition; bioinformatics; cluster analysis; data mining; decision tree; gene expression; image segmentation; information retrieval; nuclear magnetic resonance imaging; priority journal; spatial analysis",2-s2.0-85021901708
"Wang J., Liu W., Xing W., Zhang S.","Two-level superpixel and feedback based visual object tracking",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021854508&doi=10.1016%2fj.neucom.2017.06.031&partnerID=40&md5=32ea88073f8a32dc7063ca666c690579","While numerous superpixel-based tracking algorithms have been proposed and demonstrated successfully, there still remain some challenges, such as determining the number of superpixels, mining and exploiting the structural information of superpixels and handling the drifts. In this paper, we propose a tracking method with two-level superpixels and a novel update strategy based on feedback to deal with the challenges mentioned above. Firstly, Bilateral filter is introduced to filter out outliers and improve the boundary capability of object as well as segmentation of superpixels. Then two-level superpixel is proposed to determine superpixel number automatically through iterating instead of setting superpixel number empirically which affects the robustness of tracking algorithm. Moreover, a novel measuring method which considers color similarity and relative positions of superpixels is proposed to make a better use of structural information of superpixels and improve tracking performance by adding relative position of superpixels into the appearance model. Finally, a feedback based update strategy is presented to handle drifts existing in tracking by calculating the adaptation of appearance model and updating the parameters like superpixel number and relative position of superpixels. Experiments on challenging sequences and comparisons to state-of-the-art methods demonstrate the feasibility and effectiveness of the proposed tracking algorithm. © 2017","Feedback; Superpixel; Two-level superpixel; Visual tracking","Feedback; Filtration; Tracking (position); Appearance modeling; Novel measuring method; State-of-the-art methods; Structural information; Super pixels; Tracking performance; Visual object tracking; Visual Tracking; Pixels; Article; automated pattern recognition; Bayes theorem; data mining; feedback system; illumination; image display; image segmentation; machine learning; online system; priority journal; process optimization; superpixel based tracking algorithm; visual object tracking",2-s2.0-85021854508
"Cai H., Guo Y., Yang W.-A., Lu K.","Mining frequent trajectory patterns of WIP in Internet of Things-based spatial-temporal database",2017,"International Journal of Computer Integrated Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016118105&doi=10.1080%2f0951192X.2017.1307522&partnerID=40&md5=439cffa35b1cca20dea77f5f9b0beb43","The application of Internet of Things technologies has led to a data-rich manufacturing environment by connecting manufacturing objects as a collaborative community. However, advanced analytics approach is comparatively inadequate for work-in-process (WIP) trajectory data. On the other hand, although the topic of mining frequent trajectory patterns has raised a great deal of attention, it mainly focuses on the fields of vehicle traffic management and users’ behaviours. When applied in manufacturing shop floor, the extracted knowledge is physical trajectory patterns and lacks manufacturing significance. This paper manages to obtain logical knowledge with manufacturing significance from WIP trajectory data. In this paper, a data model is introduced to map physical trajectories of WIP into logical space, in order to capture logical features of manufacturing system. Moreover, an algorithm named PMP is proposed to extract logical trajectory patterns. Several experiments are conducted to examine the performance. The results prove the efficiency and feasibility of the proposed method. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","data mining; frequent trajectory; Internet of Things; manufacturing system; work-in-process","Internet of things; Manufacture; Trajectories; Collaborative community; Internet of things technologies; Logical features; Manufacturing environments; Spatial temporals; Trajectory pattern; Vehicle traffic; Work in process; Data mining",2-s2.0-85016118105
"Terziyan V.","Social Distance metric: from coordinates to neighborhoods",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028531521&doi=10.1080%2f13658816.2017.1367796&partnerID=40&md5=aba4e6406349a67032ba39e8555e2eb3","Choice of a distance metric is a key for the success in many machine learning and data processing tasks. The distance between two data samples traditionally depends on the values of their attributes (coordinates) in a data space. Some metrics also take into account the distribution of samples within the space (e.g. local densities) aiming to improve potential classification or clustering performance. In this paper, we suggest the Social Distance metric that can be used on top of any traditional metric. For a pair of samples x and y, it averages the two numbers: the place (rank), which sample y holds in the list of ordered nearest neighbors of x; and vice versa, the rank of x in the list of the nearest neighbors of y. Average is a contraharmonic Lehmer mean, which penalizes the difference between the numbers by giving values greater than the Arithmetic mean for the unequal arguments. We consider normalized average as a distance function and we prove it to be a metric. We present several modifications of such metric and show that their properties are useful for a variety of classification and clustering tasks in data spaces or graphs in a Geographic Information Systems context and beyond. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","classification; clustering; data mining; density; distance function; graphs; Lehmer mean; Metric; social neighborhood",,2-s2.0-85028531521
"Akkas O., Lee C.H., Hu Y.H., Harris Adamson C., Rempel D., Radwin R.G.","Measuring exertion time, duty cycle and hand activity level for industrial tasks using computer vision",2017,"Ergonomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021865858&doi=10.1080%2f00140139.2017.1346208&partnerID=40&md5=81de2095958caabc01bad01ef196853a","Two computer vision algorithms were developed to automatically estimate exertion time, duty cycle (DC) and hand activity level (HAL) from videos of workers performing 50 industrial tasks. The average DC difference between manual frame-by-frame analysis and the computer vision DC was −5.8% for the Decision Tree (DT) algorithm, and 1.4% for the Feature Vector Training (FVT) algorithm. The average HAL difference was 0.5 for the DT algorithm and 0.3 for the FVT algorithm. A sensitivity analysis, conducted to examine the influence that deviations in DC have on HAL, found it remained unaffected when DC error was less than 5%. Thus, a DC error less than 10% will impact HAL less than 0.5 HAL, which is negligible. Automatic computer vision HAL estimates were therefore comparable to manual frame-by-frame estimates. Practitioner Summary: Computer vision was used to automatically estimate exertion time, duty cycle and hand activity level from videos of workers performing industrial tasks. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","automated exposure analysis; Computer vision; exposure assessment; repetitive motion; work related musculoskeletal disorders","Data mining; Decision trees; Sensitivity analysis; Trees (mathematics); Computer vision algorithms; Exposure analysis; Exposure assessment; Feature vectors; Frame-by-frame analysis; Industrial tasks; Repetitive motions; Work-related musculoskeletal disorders; Computer vision",2-s2.0-85021865858
"Hu Y., Ye X., Shaw S.-L.","Extracting and analyzing semantic relatedness between cities using news articles",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028556133&doi=10.1080%2f13658816.2017.1367797&partnerID=40&md5=745211ac4ba975d47bb9c0fd7e8646f2","News articles capture a variety of topics about our society. They reflect not only the socioeconomic activities that happened in our physical world, but also some of the cultures, human interests, and public concerns that exist only in the perceptions of people. Cities are frequently mentioned in news articles, and two or more cities may co-occur in the same article. Such co-occurrence often suggests certain relatedness between the mentioned cities, and the relatedness may be under different topics depending on the contents of the news articles. We consider the relatedness under different topics as semantic relatedness. By reading news articles, one can grasp the general semantic relatedness between cities; yet, given hundreds of thousands of news articles, it is very difficult, if not impossible, for anyone to manually read them. This paper proposes a computational framework which can ‘read’ a large number of news articles and extract the semantic relatedness between cities. This framework is based on a natural language processing model and employs a machine learning process to identify the main topics of news articles. We describe the overall structure of this framework and its individual modules, and then apply it to an experimental dataset with more than 500,000 news articles covering the top 100 US cities spanning a 10-year period. We perform exploratory visualizations of the extracted semantic relatedness under different topics and over multiple years. We also analyze the impact of geographic distance on semantic relatedness and find varied distance decay effects. The proposed framework can be used to support large-scale content analysis in city network research. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","city network; geographic knowledge discovery; geospatial semantics; Place relatedness; semantic analysis; spatial data mining",,2-s2.0-85028556133
"Dempsey P.G., Pollard J., Porter W.L., Mayton A., Heberger J.R., Gallagher S., Reardon L., Drury C.G.","Development of ergonomics audits for bagging, haul truck and maintenance and repair operations in mining",2017,"Ergonomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021138464&doi=10.1080%2f00140139.2017.1335885&partnerID=40&md5=d561bb43016ea9933574f53426a133eb","The development and testing of ergonomics and safety audits for small and bulk bag filling, haul truck and maintenance and repair operations in coal preparation and mineral processing plants found at surface mine sites is described. The content for the audits was derived from diverse sources of information on ergonomics and safety deficiencies including: analysis of injury, illness and fatality data and reports; task analysis; empirical laboratory studies of particular tasks; field studies and observations at mine sites; and maintenance records. These diverse sources of information were utilised to establish construct validity of the modular audits that were developed for use by mine safety personnel. User and interrater reliability testing was carried out prior to finalising the audits. The audits can be implemented using downloadable paper versions or with a free mobile NIOSH-developed Android application called ErgoMine. Practitioner Summary: The methodology used to develop ergonomics audits for three types of mining operations is described. Various sources of audit content are compared and contrasted to serve as a guide for developing ergonomics audits for other occupational contexts. ©, This work was authored as part of the Contributor's official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law.","audit; haulage vehicles; maintenance; Mining; observational method","Coal industry; Job analysis; Maintenance; Mine trucks; Mining; Repair; Safety testing; Trucks; Android applications; audit; Development and testing; Inter-rater reliabilities; Maintenance records; Mineral processing plants; Observational method; Sources of informations; Ergonomics",2-s2.0-85021138464
"Tewo R.K., Maree J.P., Ruto S., Rutto H.L., Koech L.K.","The gypsum reduction process and its validation using the Mintek Pyrosim model",2017,"Chemical Engineering Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032027956&doi=10.1080%2f00986445.2017.1365064&partnerID=40&md5=c6277c69f2eced653dd1ab51205335a8","Waste gypsum is produced as a by-product from the fertilizer, mining industries, and acid mine water neutralization using calcium carbonate and/or lime and desalination processes using reverse osmosis and ion-exchange processes, resulting in environmental and storage problems. The purpose of the study was to establish optimum operating conditions for the recovery of valuable products, e.g., sulfur and precipitated calcium carbonate, from waste gypsum, hence, offer an attractive solution to the gypsum waste problem. The paper presents results on thermal studies of waste gypsum in a tube furnace and its validation using the Mintek Pyrosim model. Gypsum was homogeneously mixed with coal and the reduction experiments conducted. The following findings were made: (i) reduction of waste gypsum is an endothermic reaction since, ΔH values were greater than 0 (ΔH > 0) when the reduction temperature was increased from 25 to 1200°C, (ii) energy requirement is dependent on temperature and gypsum to coal ratio. Gypsum to calcium sulfide conversion of 83.5 and 83.8% was obtained at the optimum temperature range of 1100–1200°C and gypsum to calcium sulfide conversion of 85.4% was obtained at the optimum coal to gypsum mole ratio of 2.1:1, (iii) excess coal gave a lower conversion, and (iv) the predicted data using Mintek Pyrosim were found to be similar to the experimental data. © 2017 Taylor & Francis.","Calcium sulfide; coal; gypsum; Pyrosim Mintek","Calcium; Calcium carbonate; Coal; Desalination; Digital storage; Groundwater; Ion exchange; Produced Water; Sulfur compounds; Attractive solutions; Calcium sulfide; Endothermic reactions; Ion exchange process; Optimum operating conditions; Precipitated calcium carbonate; Pyrosim Mintek; Reduction temperatures; Gypsum",2-s2.0-85032027956
"Pise N., Kulkarni P.","Evolving learners’ behavior in data mining",2017,"Evolving Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032575216&doi=10.1007%2fs12530-016-9167-3&partnerID=40&md5=2cbdfbeb449f9479b99f42ab7fc81b6c","An evaluation and choice of learning algorithms is a current research area in data mining, artificial intelligence and pattern recognition, etc. Supervised learning is one of the tasks most frequently used in data mining. There are several learning algorithms available in machine learning field and new algorithms are being added in machine learning literature. There is a need for selecting the best suitable learning algorithm for a given data. With the information explosion of different learning algorithms and the changing data scenarios, there is a need of smart learning system. The paper shows one approach where past experiences learned are used to suggest the best suitable learner using 3 meta-features namely simple, statistical and information theoretic features. The system tests 38 UCI benchmark datasets from various domains using nine classifiers from various categories. It is observed that for 29 datasets, i.e., 76 % of datasets, both the predicted and actual accuracies directly match. The proposed approach is found to be correct for algorithm selection of these datasets. New proposed equation of finding classifier accuracy based on meta-features is determined and validated. The study compares various supervised learning algorithms by performing tenfold cross-validation paired t test. The work helps in a critical step in data mining for selecting the suitable data mining algorithm. © 2016, Springer-Verlag Berlin Heidelberg.","Classification; Data characteristics; Data mining techniques; Intelligent data analysis; Learning algorithms; Machine learning","Artificial intelligence; Classification (of information); Data mining; Information theory; Learning systems; Pattern recognition; Supervised learning; Algorithm selection; Benchmark datasets; Cross validation; Data characteristics; Data mining algorithm; Information explosion; Intelligent data analysis; Machine learning literature; Learning algorithms",2-s2.0-85032575216
"Hoseini A.A., Steen S.","Multivariate time series data mining in ship monitoring database",2017,"Journal of Offshore Mechanics and Arctic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027555793&doi=10.1115%2f1.4037293&partnerID=40&md5=5444b5db137153e293760733b03fc57c","A framework is presented for data mining in multivariate time series collected over hours of ship operation to extract vessel states from the data. The measurements made by a ship monitoring system lead to a collection of time-organized in-service data. Usually, these time series datasets are big, complicated, and highly dimensional. The purpose of time-series data mining is to bridge the gap between a massive database and meaningful information hidden behind the data. An important aspect of the framework proposed is selecting relevant variables, eliminating unnecessary information or noises, and extracting the essential features of the problem so that the vessel behavior can be identified reliably. Principal component analysis (PCA) is employed to address the issues of multicollinearity in the data and dimensionality reduction. The data mining approach itself is established on unsupervised data clustering using self-organizing map (SOM) and k-means, and k-nearest neighbors search (k-NNS) for searching and recovering specific information from the database. As a case study, the results are based on onboard monitoring data of the Norwegian University of Science and Technology (NTNU) research vessel, ""Gunnerus."" The scope of this work is limited to detecting ship maneuvers. However, it is extendable to a wide range of smart marine applications. As illustrated in the results, this approach is effective in identifying the prior unknown states of the ship with acceptable accuracy. © 2017 by ASME.",,"Clustering algorithms; Conformal mapping; Database systems; Marine applications; Monitoring; Nearest neighbor search; Principal component analysis; Regression analysis; Self organizing maps; Ships; Time series; Dimensionality reduction; K-nearest neighbors; Multivariate time series; Norwegian University; On-board monitoring; Science and Technology; Specific information; Time series data mining; Data mining",2-s2.0-85027555793
"Kebede M., Zegeye D.T., Zeleke B.M.","Predicting CD4 count changes among patients on antiretroviral treatment: Application of data mining techniques",2017,"Computer Methods and Programs in Biomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030129345&doi=10.1016%2fj.cmpb.2017.09.017&partnerID=40&md5=f6c39c1509895f5203cfc1917442830d","Background and objectives To monitor the progress of therapy and disease progression, periodic CD4 counts are required throughout the course of HIV/AIDS care and support. The demand for CD4 count measurement is increasing as ART programs expand over the last decade. This study aimed to predict CD4 count changes and to identify the predictors of CD4 count changes among patients on ART. Methods A cross-sectional study was conducted at the University of Gondar Hospital from 3,104 adult patients on ART with CD4 counts measured at least twice (baseline and most recent). Data were retrieved from the HIV care clinic electronic database and patients` charts. Descriptive data were analyzed by SPSS version 20. Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology was followed to undertake the study. WEKA version 3.8 was used to conduct a predictive data mining. Before building the predictive data mining models, information gain values and correlation-based Feature Selection methods were used for attribute selection. Variables were ranked according to their relevance based on their information gain values. J48, Neural Network, and Random Forest algorithms were experimented to assess model accuracies. Result The median duration of ART was 191.5 weeks. The mean CD4 count change was 243 (SD 191.14) cells per microliter. Overall, 2427 (78.2%) patients had their CD4 counts increased by at least 100 cells per microliter, while 4% had a decline from the baseline CD4 value. Baseline variables including age, educational status, CD8 count, ART regimen, and hemoglobin levels predicted CD4 count changes with predictive accuracies of J48, Neural Network, and Random Forest being 87.1%, 83.5%, and 99.8%, respectively. Random Forest algorithm had a superior performance accuracy level than both J48 and Artificial Neural Network. The precision, sensitivity and recall values of Random Forest were also more than 99%. Conclusions Nearly accurate prediction results were obtained using Random Forest algorithm. This algorithm could be used in a low-resource setting to build a web-based prediction model for CD4 count changes. © 2017","Antiretroviral treatment; CD4 count change; Computational methods; J48, Decision tree; Neural Network; Random Forest","Arts computing; Computational methods; Decision trees; Disease control; Forecasting; Neural networks; Patient treatment; Antiretroviral treatment; CD4 count change; Cross-sectional study; Feature selection methods; Low-resource settings; Predictive data mining; Random forest algorithm; Random forests; Data mining",2-s2.0-85030129345
"Blaylock B.K., Horel J.D., Liston S.T.","Cloud archiving and data mining of High-Resolution Rapid Refresh forecast model output",2017,"Computers and Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027517763&doi=10.1016%2fj.cageo.2017.08.005&partnerID=40&md5=01403bb8bc42103b0600fd23c1314ac1","Weather-related research often requires synthesizing vast amounts of data that need archival solutions that are both economical and viable during and past the lifetime of the project. Public cloud computing services (e.g., from Amazon, Microsoft, or Google) or private clouds managed by research institutions are providing object data storage systems potentially appropriate for long-term archives of such large geophysical data sets. We illustrate the use of a private cloud object store developed by the Center for High Performance Computing (CHPC) at the University of Utah. Since early 2015, we have been archiving thousands of two-dimensional gridded fields (each one containing over 1.9 million values over the contiguous United States) from the High-Resolution Rapid Refresh (HRRR) data assimilation and forecast modeling system. The archive is being used for retrospective analyses of meteorological conditions during high-impact weather events, assessing the accuracy of the HRRR forecasts, and providing initial and boundary conditions for research simulations. The archive is accessible interactively and through automated download procedures for researchers at other institutions that can be tailored by the user to extract individual two-dimensional grids from within the highly compressed files. Characteristics of the CHPC object storage system are summarized relative to network file system storage or tape storage solutions. The CHPC storage system is proving to be a scalable, reliable, extensible, affordable, and usable archive solution for our research. © 2017 Elsevier Ltd","Atmospheric modeling; Cloud computing; Data stewardship; Object data storage","Cloud computing; Data mining; Data storage equipment; Digital storage; Forecasting; Societies and institutions; Atmospheric model; Data stewardship; High performance computing; Initial and boundary conditions; Meteorological condition; Object data; Object storage systems; Retrospective analysis; Distributed computer systems",2-s2.0-85027517763
"Tahmasebi P., Javadpour F., Sahimi M.","Data mining and machine learning for identifying sweet spots in shale reservoirs",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025811616&doi=10.1016%2fj.eswa.2017.07.015&partnerID=40&md5=0d7b0d7131c8874d7204448db331b48a","Due to its complex structure, production form a shale-gas formation requires more drillings than those for the traditional reservoirs. Modeling of such reservoirs and making predictions for their production also require highly extensive datasets. Both are very costly. In-situ measurements, such as well-logging, are one of most indispensable tools for providing considerable amount of information and data for such unconventional reservoirs. Production from shale reservoirs involves the so-called fracking, i.e. injection of water and chemicals into the formation in order to open up flow paths for the hydrocarbons. The measurements and any other types of data are utilized for making critical decisions regarding development of a potential shale reservoir, as it requires hundreds of millions of dollar initial investment. The questions that must be addressed include, does the region under study can be used economically for producing hydrocarbons? If the response to the first question is affirmative, then, where are the best places to carry out hydro-fracking? Through the answers to such questions one identifies the sweet spots of shale reservoirs, which are the regions that contain high total organic carbon (TOC) and brittle rocks that can be fractured. In this paper, two methods from data mining and machine learning techniques are used to aid identifying such regions. The first method is based on a stepwise algorithm that determines the best combination of the variables (well-log data) to predict the target parameters. However, in order to incorporate more training, and efficiently use the available datasets, a hybrid machine-learning algorithm is also presented that models more accurately the complex spatial correlations between the input and target parameters. Then, statistical comparisons between the estimated variables and the available data are made, which indicate very good agreement between the two. The proposed model can be used effectively to estimate the probability of targeting the sweet spots. In the light of an automatic input and parameter selection, the algorithm does not require any further adjustment and can continuously evaluate the target parameters, as more data become available. Furthermore, the method is able to optimally identify the necessary logs that must be run, which significantly reduces data acquisition operations. © 2017 Elsevier Ltd","Big data; Brittleness index; Fracable zones; Genetic algorithm; Neural networks; Shale formation","Artificial intelligence; Carbon; Complex networks; Data mining; Education; Fracture mechanics; Genetic algorithms; Hydraulic fracturing; Hydrocarbons; Learning algorithms; Learning systems; Neural networks; Organic carbon; Parameter estimation; Shale; Water injection; Well logging; Amount of information; Brittleness index; Hybrid machine learning; Machine learning techniques; Shale formation; Spatial correlations; Statistical comparisons; Unconventional reservoirs; Big data",2-s2.0-85025811616
"Finogeev A.G., Parygin D.S., Finogeev A.A.","The convergence computing model for big sensor data mining and knowledge discovery",2017,"Human-centric Computing and Information Sciences",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015690994&doi=10.1186%2fs13673-017-0092-7&partnerID=40&md5=4ef58b935b6e90c820409f2c95786feb","The article considers the model and method of converged computing and storage to create SCADA systems based on wireless networks for the energy industry. Computing power of modern wireless sensor network nodes allow the transfer to them some operations sensor data mining and offload the dispatching data centre servers. This fog computing model is used for the aggregation of primary data, forecast trends controlled variables as well as to warn about abnormal and emergency situations on distributed SCADA systems objects. Large arrays of sensor data, integral indicators and heterogeneous information from other sources (e.g., weather stations, security and fire alarm systems, video surveillance systems, etc.) is more appropriate to process via GRID computing model. GRID computing model has three-tier architecture, which includes the main server at the first level, a cluster of servers at the second level, and a lot of GPU video card with support for Compute Unified Device Architecture at the third level. The model of cloud computing and cloud storage today is the basis for the accumulation of the results of data mining and knowledge discovery. Means of communication and remote access can solve the problem of intellectual processing and visualization of information with elements of augmented reality and geo-information technologies within the framework of mobile computing model. The implementation of these four computing models for the operation of components of SCADA system is the convergent approach to distributed sensor data processing, which is discussed in the article. © 2017, The Author(s).","Cloud computing; Cloud storage; Convergence computing; Data mining; Fog computing; GRID; Knowledge discovery; Mobile computing; Wireless sensor network",,2-s2.0-85015690994
"Sagar K., Saha A.","Qualitative usability feature selection with ranking: a novel approach for ranking the identified usability problematic attributes for academic websites using data-mining techniques",2017,"Human-centric Computing and Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032203112&doi=10.1186%2fs13673-017-0111-8&partnerID=40&md5=dd0dd5b6cc66d33aa0b2b7a4584ec43d","Objective: The aim of this study is to identify common usability problematic patterns that belong to top-50 academic websites as a whole and then ranking of these identified usability problems is also provided. Methods: In this study, a novel approach is proposed that is based upon the integration of conventional usability testing and heuristic evaluation with data-mining knowledge discovery process. An experiment is conducted to evaluate ISO 9241-151 guidelines under 16-different categories by hundred participants who are frequent users of academic websites. After evaluation, the qualitative usability data is collected and different data-mining techniques i.e. association rule and decision tree are applied to recognize fully functional and problematic usability attributes. Identified problematic attributes represent common usability problems patterns related to academic websites from the qualitative viewpoint only. This study further prioritizes these problematic attributes by using the ranking algorithm that represents the order in which usability issues must be resolved. Results: In this study, 16-different categories are considered for usability evaluation of academic websites. The results show that no issues are identified in two-categories i.e. {Headings_Titles_Labels and The Home_Page}. In Scrolling and Paging category, horizontal scrolling is identified as a major issue whereas, in Internationalization category, the users do not identify supported languages on most of the academic websites. Users do not find websites to be highly secured under Security category. Our findings investigate that most of the issues are found in Search and Social Media categories. Furthermore, users easily locate 50.53% guidelines on websites as fully functional whereas, 49.46% of characteristics are considered as problematic usability features that are not functional on the academic website as a whole. Conclusions: Identification of common usability problems at an early stage can lower substantially the development efforts in cost and time. Software developers can restrain from these potential usability problems during the development of novel systems under the same context. Providing appropriate solutions for these problems can become valuable in software development. The proposed approach concludes that conventional usability evaluation methods can go beyond just than testing of systems. The study is a milestone towards identification and prioritizing problematic usability features for academic websites and helps in providing the wholistic approach of usability problematic patterns for web-domain. © 2017, The Author(s).","Association rule; Attribute selection; Data-mining knowledge discovering in databases; Decision tree; Heuristic evaluation; Qualitative usability testing; Usability; Usability engineering",,2-s2.0-85032203112
"Zhuang Z., Ben X., Yan R., Pang J., Li Y.","Accurately predicting heat transfer performance of ground heat exchanger for ground-coupled heat pump systems using data mining methods",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962883656&doi=10.1007%2fs00521-016-2307-7&partnerID=40&md5=32a72df446c0f49386fca4fe7ec096e4","Nowadays, the ground-coupled heat pump (GCHP) systems have been recognized as one of the most energy-efficient systems for heating, cooling and hot water supply in both residential and commercial buildings. However, the heat transfer of ground heat exchanger (GHE) involves in large spatial scales, long time span and complex influential factors. We develop a data mining framework constructed by using 1998 experimental data to study the effects of 12 input variables composed of seven borehole parameters, two U-tube parameters, two ground parameters and one circulating liquid parameter to accurately predict the heat transfer performance of GHE for GCHP systems in 10 years. Hence, selecting a suitable input configuration to improve the energy efficiency has important sustainability benefits. The role of each of independent variable explaining the output variables is analyzed by partial least squares regression. Furthermore, support vector regression and M5 Model Tree are, respectively, used to predict the heat transfer performance. Extensive simulations show that we can predict the average quantity of heat exchanger, temperature of ground around GHE, inlet temperature of heat pump unit with very low level of error. © 2016, The Natural Computing Applications Forum.","Ground heat exchanger (GHE); Ground-coupled heat pump (GCHP); M5 Model Tree; Partial least squares regression (PLSR); Support vector regression (SVR)","Cooling systems; Data mining; Energy efficiency; Forecasting; Forestry; Geothermal heat pumps; Heat pump systems; Heat transfer; Least squares approximations; Office buildings; Pumps; Regression analysis; Water supply; Ground heat exchangers; Ground-coupled heat pump; M5 model tree; Partial least squares regressions (PLSR); Support vector regression (SVR); Heat exchangers",2-s2.0-84962883656
"Pinto P., Theodoro I., Arrais M., Oliveira J.","Data mining and social web semantics: A case study on the use of hashtags and memes in Online Social Networks",2017,"IEEE Latin America Transactions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032620708&doi=10.1109%2fTLA.2017.8071088&partnerID=40&md5=b8b6edca022f8ef360f3988c0444fe27","This research is derived from the works ""Comportamento das Hashtags Durante Grandes Eventos (2016)"" and ""Enriquecimento Semântico de Imagens Divulgadas em Redes Sociais On-line (2016)"", both presented in VII WAIHCWS - Workshop on Aspects of Human-Computer Interaction for The Social Web, which was held at IHC 2016. The study demonstrates the convergence between social, behavioral and trends aspects for communication through Social Networking Online (SNO). The work consolidates the methodology, data analysis and results of the two surveys that worked with social networks Facebook and Twitter respectively. © 2003-2012 IEEE.","Facebook; Hashtag; Indexing; Meme; Semantics; Social Networking; Twitter","Data mining; Human computer interaction; Indexing (of information); Semantic Web; Semantics; Facebook; Hashtag; Hashtags; Meme; On-line social networks; Social webs; Twitter; Social networking (online)",2-s2.0-85032620708
"Javadi S., Hashemy M.","Evaluation of groundwater vulnerability using data mining technique in Hashtgerd plain",2017,"Journal of the Earth and Space Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014099546&partnerID=40&md5=a02ebebfd98b79a258e99d777e622db2","Groundwater vulnerability assessment is an effective informative method to provide basis for determining source of pollution. Vulnerability maps are employed as an important solution in order to handle entrance of pollution into the aquifers. A common way to develop groundwater vulnerability map is DRASTIC index. Meanwhile, application of the method is not easy for any aquifer due to choosing appropriate constant values of weights and ranks. Clustering technique would be an influential method for regionalization of groundwater flow zone to facilitate vulnerability assessment of groundwater aquifers. In this study, a new approach using k-means clustering is applied to make vulnerability maps. Four features of depth to groundwater, hydraulic conductivity, recharge value and vadose zone are considered at the same time as features of clustering. Five regions are recognized out of the Hashtgerd plain. Each zone corresponds to a different level of vulnerability. The results show that clustering provides a more realistic vulnerability map so that, Pearson's correlation coefficients between nitrate concentrations and clustering vulnerability is 72%.","Clustering; Data mining; Groundwater; Vulnerability assessment","aquifer pollution; data mining; groundwater flow; hydraulic conductivity; nitrate; pollutant source; recharge; vadose zone; vulnerability; Alborz; Hashtgerd; Iran",2-s2.0-85014099546
"Majumdar J., Naraseeyappa S., Ankalaki S.","Analysis of agriculture data using data mining techniques: application of big data",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021713833&doi=10.1186%2fs40537-017-0077-4&partnerID=40&md5=337a12718f420a4c5c4fb683f899cd62","In agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the numerous crops involved in the planning. Data mining techniques are necessary approach for accomplishing practical and effective solutions for this problem. Agriculture has been an obvious target for big data. Environmental conditions, variability in soil, input levels, combinations and commodity prices have made it all the more relevant for farmers to use information and get help to make critical farming decisions. This paper focuses on the analysis of the agriculture data and finding optimal parameters to maximize the crop production using data mining techniques like PAM, CLARA, DBSCAN and Multiple Linear Regression. Mining the large amount of existing crop, soil and climatic data, and analysing new, non-experimental data optimizes the production and makes agriculture more resilient to climatic change. © 2017, The Author(s).","Big Data; CLARA and DBSCAN; PAM",,2-s2.0-85021713833
"Monaghan A.A.","Unconventional energy resources in a crowded subsurface: Reducing uncertainty and developing a separation zone concept for resource estimation and deep 3D subsurface planning using legacy mining data",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019546596&doi=10.1016%2fj.scitotenv.2017.05.125&partnerID=40&md5=c56e494881b42a0b18774b51d37d39bc","Over significant areas of the UK and western Europe, anthropogenic alteration of the subsurface by mining of coal has occurred beneath highly populated areas which are now considering a multiplicity of ‘low carbon’ unconventional energy resources including shale gas and oil, coal bed methane, geothermal energy and energy storage. To enable decision making on the 3D planning, licensing and extraction of these resources requires reduced uncertainty around complex geology and hydrogeological and geomechanical processes. An exemplar from the Carboniferous of central Scotland, UK, illustrates how, in areas lacking hydrocarbon well production data and 3D seismic surveys, legacy coal mine plans and associated boreholes provide valuable data that can be used to reduce the uncertainty around geometry and faulting of subsurface energy resources. However, legacy coal mines also limit unconventional resource volumes since mines and associated shafts alter the stress and hydrogeochemical state of the subsurface, commonly forming pathways to the surface. To reduce the risk of subsurface connections between energy resources, an example of an adapted methodology is described for shale gas/oil resource estimation to include a vertical separation or ‘stand-off’ zone between the deepest mine workings, to ensure the hydraulic fracturing required for shale resource production would not intersect legacy coal mines. Whilst the size of such separation zones requires further work, developing the concept of 3D spatial separation and planning is key to utilising the crowded subsurface energy system, whilst mitigating against resource sterilisation and environmental impacts, and could play a role in positively informing public and policy debate. © 2017 British Geological Survey, a component institute of NERC","Carboniferous; Coal mine plan; Geological uncertainty; Shale gas","Carbon; Coal; Coal bed methane; Coal deposits; Coal storage; Decision making; Digital storage; Energy resources; Environmental impact; Geology; Geothermal energy; Hydraulic fracturing; Methane; Mine surveys; Resource valuation; Risk perception; Separation; Shale; Shale gas; Shale oil; Size separation; 3-D seismic survey; Carboniferous; Geological uncertainty; Resource estimation; Spatial separation; Subsurface planning; Unconventional resources; Vertical separation; Coal mines; hydrocarbon; Carboniferous; coal mine; conceptual framework; data mining; energy resource; estimation method; hydrocarbon resource; shale gas; Article; coal mining; decision making; energy resource; environmental impact; geochemical analysis; limit of quantitation; measurement accuracy; priority journal; prospective study; risk reduction; uncertainty; United Kingdom; Scotland; United Kingdom; Western Europe",2-s2.0-85019546596
"Miller C., Meggers F.","Mining electrical meter data to predict principal building use, performance class, and operations strategy for hundreds of non-residential buildings",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030704655&doi=10.1016%2fj.enbuild.2017.09.056&partnerID=40&md5=08d62f7e6ee43c57355751a66518f41a","This study focuses on the inference of characteristic data from a data set of 507 non-residential buildings. A two-step framework is presented that extracts statistical, model-based, and pattern-based behavior. The goal of the framework is to reduce the expert intervention needed to utilize measured raw data in order to infer information such as building use type, performance class, and operational behavior. The first step is temporal feature extraction, which utilizes a library of data mining techniques to filter various phenomenon from the raw data. This step transforms quantitative raw data into qualitative categories that are presented in heat map visualizations for interpretation. In the second step, a random forest classification model is tested for accuracy in predicting primary space use, magnitude of energy consumption, and type of operational strategy using the generated features. The results show that predictions with these methods are 45.6% more accurate for primary building use type, 24.3% more accurate for performance class, and 63.6% more accurate for building operations type as compared to baselines. © 2017 Elsevier B.V.","Building performance; Data mining; Energy efficiency; Performance classification; Smart meters","Buildings; Decision trees; Energy efficiency; Energy utilization; Filtration; Forecasting; Housing; Smart meters; Building operations; Building performance; Operational behavior; Operational strategies; Operations strategies; Random forest classification; Residential building; Step transforms; Data mining",2-s2.0-85030704655
"Wang L., Li M., Cao Y., Han Z., Wang X., Atkinson E.J., Liu H., Amin S.","Proton Pump Inhibitors and the Risk for Fracture at Specific Sites: Data Mining of the FDA Adverse Event Reporting System",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025166236&doi=10.1038%2fs41598-017-05552-1&partnerID=40&md5=6a28b28e488ca1ba380feda7dd09beb4","Proton pump inhibitors (PPIs) are widely used to treat gastric acid-related disorders. Concerns have been raised about potential fracture risk, especially at the hip, spine and wrist. However, fracture risk at other bone sites has not been as well studied. We investigated the association between PPIs and specific fracture sites using an aggregated knowledge-enhanced database, the Food and Drug Administration Adverse Event Reporting System Data Mining Set (AERS-DM). Proportional reporting ratio (PRR) was used to detect statistically significant associations (signals) between PPIs and fractures. We analyzed both high level terms (HLT) and preferred terms (PT) for fracture sites, defined by MedDRA (Medical Dictionary for Regulatory Activities). Of PPI users reporting fractures, the mean age was 65.3 years and the female to male ratio was 3.4:1. Results revealed signals at multiple HLT and PT fracture sites, consistent for both sexes. These included fracture sites with predominant trabecular bone, not previously reported as being associated with PPIs, such as 'rib fractures', where signals were detected for overall PPIs as well as for each of 5 generic ingredients (insufficient data for dexlansoprazole). Based on data mining from AERS-DM, PPI use appears to be associated with an increased risk for fractures at multiple sites. © 2017 The Author(s).",,,2-s2.0-85025166236
"Geilhufe R.M., Borysov S.S., Bouhon A., Balatsky A.V.","Data Mining for Three-Dimensional Organic Dirac Materials: Focus on Space Group",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026770425&doi=10.1038%2fs41598-017-07374-7&partnerID=40&md5=40bc8d93fb293bc8006ee6e2ed3905ad","We combined the group theory and data mining approach within the Organic Materials Database that leads to the prediction of stable Dirac-point nodes within the electronic band structure of three-dimensional organic crystals. We find a particular space group P212121 (#19) that is conducive to the Dirac nodes formation. We prove that nodes are a consequence of the orthorhombic crystal structure. Within the electronic band structure, two different kinds of nodes can be distinguished: 8-fold degenerate Dirac nodes protected by the crystalline symmetry and 4-fold degenerate Dirac nodes protected by band topology. Mining the Organic Materials Database, we present band structure calculations and symmetry analysis for 6 previously synthesized organic materials. In all these materials, the Dirac nodes are well separated within the energy and located near the Fermi surface, which opens up a possibility for their direct experimental observation. © 2017 The Author(s).",,,2-s2.0-85026770425
"Serrano J.I., Romero J.P., Castillo M.D.D., Rocon E., Louis E.D., Benito-León J.","A data mining approach using cortical thickness for diagnosis and characterization of essential tremor /692/53/2421 /692/617/375/346 /129 /141 /123 article",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019849163&doi=10.1038%2fs41598-017-02122-3&partnerID=40&md5=32a165260e081acb53382bdf6bf9016e","Essential tremor (ET) is one of the most prevalent movement disorders. Being that it is a common disorder, its diagnosis is considered routine. However, misdiagnoses may occur regularly. Over the past decade, several studies have identified brain morphometric changes in ET, but these changes remain poorly understood. Here, we tested the informativeness of measuring cortical thickness for the purposes of ET diagnosis, applying feature selection and machine learning methods to a study sample of 18 patients with ET and 18 age- and sex-matched healthy control subjects. We found that cortical thickness features alone distinguished the two, ET from controls, with 81% diagnostic accuracy. More specifically, roughness (i.e., the standard deviation of cortical thickness) of the right inferior parietal and right fusiform areas was shown to play a key role in ET characterization. Moreover, these features allowed us to identify subgroups of ET patients as well as healthy subjects at risk for ET. Since treatment of tremors is disease specific, accurate and early diagnosis plays an important role in tremor management. Supporting the clinical diagnosis with novel computer approaches based on the objective evaluation of neuroimage data, like the one presented here, may represent a significant step in this direction. © 2017 The Author(s).",,,2-s2.0-85019849163
"Omer M.Z., Gao H., Mustafa N.","Privacy-preserving of SVM over vertically partitioned with imputing missing data",2017,"Distributed and Parallel Databases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028998320&doi=10.1007%2fs10619-017-7203-3&partnerID=40&md5=77597bbe9458a90606a566fc8344196b","Most distributed data mining algorithms can efficiently manage and mine complete data from distributed resources. However, for an incomplete data some modifications are required in order to perform distributed data mining techniques and maintaining the privacy of the sensitive information to provide pretty good results of data mining. Classification is important tasks of data mining aimed at discovering knowledge and classify new instances. SVM is classified as one of the most important algorithm used for classification problems in several various spheres. In this paper, we proposed a new distributed privacy-preserving protocol with multiple imputations of missing or incomplete data. More so, a multiple imputations based on multivariate imputation by chained equations is used for missing data and Paillier cryptosystem for maintaining the privacy of the participants. Finally we constructed a global SVM model by introducing a third party (semi-honest approach) over vertical partition data based in Gram matrix without revealing the privacy of the data and used to classify new instances. The performance evolution of the proposed protocol was investigated while using accuracy metric on the distributed and centralized data. Results of our experiments reveal that the accuracy is the same as centralized data and achieve better results with imputed data while compared with omitted data. The performance of distributed data on our protocol achieves better processing time compared with centralized data. © 2017, Springer Science+Business Media, LLC.","Data imputation; Distributed privacy-preserving; Gram matrix; Paillier cryptosystem","Classification (of information); Cryptography; Data mining; Information management; Matrix algebra; Data imputation; Distributed data mining; Distributed data mining algorithms; Gram matrices; Paillier cryptosystem; Performance evolutions; Privacy preserving; Privacy-preserving protocols; Data privacy",2-s2.0-85028998320
"Nwagwu H.C., Okereke G., Nwobodo C.","Mining and visualising contradictory data",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032667180&doi=10.1186%2fs40537-017-0100-9&partnerID=40&md5=6b7f2baa0520bd64ec31e35c676d0452","Big datasets are often stored in flat files and can contain contradictory data. Contradictory data undermines the soundness of the information from a noisy dataset. Traditional tools such as pie chart and bar chart are overwhelmed when used to visually identify contradictory data in multidimensional attribute-values of a big dataset. This work explains the importance of identifying contradictions in a noisy dataset. It also examines how contradictory data in a large and noisy dataset can be mined and visually analysed. The authors developed ‘ConTra’, an open source application which applies mutual exclusion rule in identifying contradictory data, existing in comma separated values (CSV) dataset. ConTra’s capability to enable the identification of contradictory data in different sizes of datasets is examined. The results show that ConTra can process large dataset when hosted in servers with fast processors. It is also shown in this work that ConTra is 100% accurate in identifying contradictory data of objects whose attribute values do not conform to the mutual exclusion rule of a dataset in CSV format. Different approaches through which ConTra can mine and identify contradictory data are also presented. © 2017, The Author(s).","Comma separated values; ConTra; Contradictions; Contradictory data; Dataset; Mutual exclusion values",,2-s2.0-85032667180
"Christensen K., Liland K.H., Kvaal K., Risvik E., Biancolillo A., Scholderer J., Nørskov S., Næs T.","Mining online community data: The nature of ideas in online communities",2017,"Food Quality and Preference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020649599&doi=10.1016%2fj.foodqual.2017.06.001&partnerID=40&md5=8e93edf4abcb6556eb8290d1693f9a7e","Ideas are essential for innovation and for the continuous renewal of a firm's product offerings. Previous research has argued that online communities contain such ideas. Therefore, online communities such as forums, Facebook groups, blogs etc. are potential gold mines for innovative ideas that can be used for boosting the innovation performance of the firm. However, the nature of online community data makes idea detection labor intensive. As an answer to this problem, research has shown that it might be possible to detect ideas from online communities, automatically. Research is however, yet to provide an answer to what is it that makes such automatic idea detection possible? Our study is based on two datasets from dialogue between members of two distinct online communities. The first community is related to beer. The second is related to Lego. We generate machine learning classifiers based on Support Vector Machines and Partial Least Squares that can detect ideas from each respective online community. We use partial least squares to investigate what are the words and expressions that allows for automatic classification of ideas. We conclude that ideas from the two online communities, contains suggestion/solution words and expressions and it is these that make automatic idea detection possible. In addition we conclude that the nature of the ideas in the beer community seems to be related to the brewing process. The nature of the ideas in the Lego community seems to be related to new products that consumers would want. © 2017","Machine learning; Natural language processing; Online communities, ideas; Partial least squares; Support vector machines; Text mining",,2-s2.0-85020649599
"Silva T.H., Vaz de Melo P.O.S., Almeida J.M., Musolesi M., Loureiro A.A.F.","A large-scale study of cultural differences using urban data about eating and drinking preferences",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031802761&doi=10.1016%2fj.is.2017.10.002&partnerID=40&md5=77a53cb9fe7f9cbefa3c9bfaadefb5ff","Traditional ways to study urban social behavior, e.g. surveys, are costly and do not scale. Recently, some studies have been showing new ways of obtaining data through location-based social networks (LBSNs), such as Foursquare, which could revolutionize the study of urban social behavior. We use Foursquare check-ins to represent user preferences regarding eating and drinking habits. Considering datasets differing in terms of volume of data and observation window size, our results indicate that spatio-temporal eating and drinking habits of users voluntarily expressed in LBSNs has the potential to explain cultural habits of the users. From this, we propose a methodology to identify cultural boundaries and similarities across populations at different scales, e.g., countries, cities, or neighborhoods. This methodology is extensively evaluated in several aspects. For instance, by proposing some variations of it disregarding some of the considered dimensions, as well as analyzing the results using datasets from different periods and window of observation. The results indicate that our proposed methodology is a promising approach for automatic cultural habits separation, which could enable new urban services. © 2017 Elsevier Ltd","Cross-cultural study; Foursquare; Large scale assessment; Location-based social network; Urban data mining","Hardware; Information systems; Cross-cultural study; Cultural boundaries; Cultural difference; Foursquare; Large scale assessment; Large-scale studies; Location-based social networks; Observation window; Data mining",2-s2.0-85031802761
"Xu C., Ren J., Zhang Y., Qin Z., Ren K.","DPPro: Differentially Private High-Dimensional Data Release via Random Projection",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028955314&doi=10.1109%2fTIFS.2017.2737966&partnerID=40&md5=cebf5035bf2dad9b56af5f26e5cef964","Releasing representative data sets without compromising the data privacy has attracted increasing attention from the database community in recent years. Differential privacy is an influential privacy framework for data mining and data release without revealing sensitive information. However, existing solutions using differential privacy cannot effectively handle the release of high-dimensional data due to the increasing perturbation errors and computation complexity. To address the deficiency of existing solutions, we propose DPPro, a differentially private algorithm for high-dimensional data release via random projection to maximize utility while guaranteeing privacy. We theoretically prove that DPPro can generate synthetic data set with the similar squared Euclidean distance between high-dimensional vectors while achieving $(\epsilon,\delta)$-differential privacy. Based on the theoretical analysis, we observed that the utility guarantees of released data depend on the projection dimension and the variance of the noise. Extensive experimental results demonstrate that DPPro substantially outperforms several state-of-the-art solutions in terms of perturbation error and privacy budget on high-dimensional data sets. © 2017 IEEE.","Differential privacy; high-dimensional data; privacy guarantees; random projection; utility guarantees","Budget control; Clustering algorithms; Data mining; Database systems; Algorithm design and analysis; Bayes method; Complexity theory; Differential privacies; High dimensional data; Histograms; Random projections; Utility Guarantees; Data privacy",2-s2.0-85028955314
"Afful-Dadzie E., Afful-Dadzie A.","Liberation of public data: Exploring central themes in open government data and freedom of information research",2017,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021670038&doi=10.1016%2fj.ijinfomgt.2017.05.009&partnerID=40&md5=e4b78e65b7f485dcc7fb276cfbf0c509","This paper conducts a comparative literature survey of Open Government Data (OGD) and Freedom of Information (FOI), with a view to tracking the central themes in the two civil society campaigns. With seeming similarities and a growing popularity in research, the major themes framing research on the two movements have not clearly emerged. Topic modelling, text mining and document analysis methods are used to extract the themes as well as key named entities. The topics are subsequently labeled and with expert guidance, their semantic meaning are provided. The results indicate that the major theme in FOI research borders on issues relating to disclosure, publishing, access and cost of requests. On the other hand, themes in OGD research have largely centered on technology and related concepts. The approach also helped in determining key similarities and differences in the two campaigns as reported in research. © 2017 Elsevier Ltd","Bibliometrics; Freedom of information (FOI); Open government data (OGD); Public data; Text mining; Topic modelling","International law; Natural language processing systems; Semantics; Bibliometrics; Freedom of informations; Open government data (OGD); Public data; Text mining; Data mining",2-s2.0-85021670038
"Briggs K.","Making sense of SEND; the Standard for Exchange of Nonclinical Data",2017,"Regulatory Toxicology and Pharmacology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032182175&doi=10.1016%2fj.yrtph.2017.10.012&partnerID=40&md5=28afb4a1c0615611a8cd45803c8eec37","The Standard for Exchange of Nonclinical Data (SEND) is currently the preferred submission format for nonclinical animal data by the US FDA and became a requirement on the 18th December 2016. Application of these data standards is the first step to being able to perform cross-study querying and is expected to open up opportunities for data mining and meta-analysis by the pharmaceutical industry. This paper reports on our experiences in developing a tool to allow recent SEND formatted studies to be explored alongside historical nonclinical data already gathered as part of the eTOX project. Combining SEND data with historical data will positively impact the power of any analysis performed and increase the likelihood of being able to detect rare effects. It describes the use of KNIME in generating dose group averages and incidences from individual animal level data captured in SEND. There are a number of options for opening and reading SEND files but the benefits of using KNIME are that it is a free, open source data mining framework which allows the data to be viewed in a holistic manner rather than one domain at a time. Additionally it incorporates several nodes useful for aggregating and visualising the data to more easily identify patterns and trends. © 2017 Elsevier Inc.","Data mining; eTOX; KNIME; Open access; SEND","access to information; Article; controlled study; data collection method; data mining; drug approval; drug industry; eTOX database; factual database; food and drug administration; information; information processing; priority journal; reading; standard for exchange of nonclinical data; standardization; workflow",2-s2.0-85032182175
"Yu D., Xu Z., Pedrycz W., Wang W.","Information sciences 1968–2016: A retrospective analysis with text mining and bibliometric",2017,"Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027406448&doi=10.1016%2fj.ins.2017.08.031&partnerID=40&md5=ec462e6d6c26556c73140281b0c8e597","This study provides a comprehensive overview of the publications in Information Sciences (INS) from 1968 to 2016 inclusive, which encompasses the history of this journal from its inception. 7721 articles containing 153,606 references, which are the primary data source were downloaded from the Web of Science. It studies the most prolific authors, most cited authors, most representative articles, top influential institutions and the nationalities of the authors whose papers were published in the journal. The key contributors and INS articles that have made profound impact are highlighted on a basis of bibliometric and customized text mining techniques. CiteSpace, a data visualization software, was used to make the comprehensive analysis of the 153,606 citations and construct the co-citation network maps, which can illustrate salient patterns and emerging trends. This paper not only provides the important reference to future studies by exploring the structures and trends of INS publications, which have evolved over time, but also offers a demonstration of an effective analytical method for evaluating journal citation and co-citation data in the future. © 2017 Elsevier Inc.","Bibliometric; Citation and co-citation; CiteSpace; Information sciences; Text mining","Data visualization; Information science; Bibliometric; Citespace; Cocitation; Comprehensive analysis; Retrospective analysis; Text mining; Text mining techniques; Visualization software; Data mining",2-s2.0-85027406448
"Sardari S., Eftekhari M., Afsari F.","Hesitant fuzzy decision tree approach for highly imbalanced data classification",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028996989&doi=10.1016%2fj.asoc.2017.08.052&partnerID=40&md5=b8cc80321b541b88acaa2af10a36fdd0","Fuzzy decision tree algorithms provide one of the most powerful classifiers applied to any kind of data. In this paper, some new Fuzzy Decision Tree (FDT) approaches based on Hesitant Fuzzy Sets (HFSs) have been introduced to classify highly imbalanced data sets. Our proposed classifiers employ k-means clustering algorithm to divide the majority class samples into several clusters. Then, each cluster sample is labeled by a new synthetic class label. After that, five discretization methods (Fayyad, Fusinter, Fixed Frequency, Proportional, and Uniform Frequency) are considered to generate Membership Functions (MFs) of each attribute. Five FDTs are constructed based on five discretization methods Hesitant Fuzzy Information Gain (HFIG) is proposed as a new attribute selection criterion that can be used instead of Fuzzy Information Gain (FIG). HFIG is calculated by aggregating obtained FIGs from different discretization methods by information energy. For predicting the class label of new samples, three aggregation methods are utilized. The combination of splitting criterion (HFIG or FIG), five different discretization methods (for generating MFs) and three aggregation methods (to predict class label of new samples) generate special classifiers for addressing the imbalanced classification. For illustrating the difference between our proposed methods, taxonomy is proposed in the paper that categorizes them in three general categories. The experimental results show that our proposed methods outperform the other fuzzy rule-based approaches over 20 highly imbalanced data sets of KEEL in terms of AUC. © 2017 Elsevier B.V.","Fuzzy decision tree; Hesitant fuzzy set; Imbalanced data classification; K-mean clustering","Clustering algorithms; Data mining; Decision trees; Discrete event simulation; Fuzzy inference; Fuzzy sets; Membership functions; Trees (mathematics); Volume measurement; Discretization method; Fuzzy decision trees; Hesitant fuzzy sets; Imbalanced classification; Imbalanced data; Imbalanced Data-sets; K-mean clustering; K-Means clustering algorithm; Classification (of information)",2-s2.0-85028996989
"Amrit C., Paauw T., Aly R., Lavric M.","Identifying child abuse through text mining and machine learning",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024891722&doi=10.1016%2fj.eswa.2017.06.035&partnerID=40&md5=586459c9e8239e9ebdf04cdff2f299fd","In this paper, we describe how we used text mining and analysis to identify and predict cases of child abuse in a public health institution. Such institutions in the Netherlands try to identify and prevent different kinds of abuse. A significant part of the medical data that the institutions have on children is unstructured, found in the form of free text notes. We explore whether these consultation data contain meaningful patterns to determine abuse. Then we train machine learning models on cases of abuse as determined by over 500 child specialists from a municipality in The Netherlands. The resulting model achieves a high score in classifying cases of possible abuse. We methodologically evaluate and compare the performance of the classifiers. We then describe our implementation of the decision support API at a municipality in the Netherlands. © 2017 Elsevier Ltd","Child abuse; Decision support; Machine learning; Text mining","Artificial intelligence; Data mining; Decision support systems; Learning systems; Child abuse; Decision supports; Free texts; Machine learning models; Medical data; Netherlands; Text mining; Education",2-s2.0-85024891722
"Yang L., Zhang J.","Automatic transfer learning for short text mining",2017,"Eurasip Journal on Wireless Communications and Networking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014608014&doi=10.1186%2fs13638-017-0815-5&partnerID=40&md5=15063c31e2d78295c1c4ad3ca2d1625b","As a new emerging technique, transfer learning enjoys the advantage of integrating the well-learnt knowledge from another related work to facilitate an improved learning result of one task. Most of the existing transfer learning methods are designed for long texts and short texts. However, the latter one distinguishes from the former one in terms of its sparse nature, noise words, syntactical structure, and colloquial terminologies used. A transfer learning algorithm called automatic transfer learning (AutoTL) is proposed for short text mining. By transferring knowledge automatically learnt from the online information, the proposed method enables training data to be selected automatically. Furthermore, it does not make any a priori assumption about probability distribution. Our experimental results on 20Newsgroups, Simulated Real Auto Aviation, and Reuter-21578 validate the higher performance of the proposed AutoTL over several state-of-of-the-art methods. © 2017, The Author(s).","Latent semantic analysis; Short text mining; Transfer learning","Data mining; Probability distributions; Semantics; Latent Semantic Analysis; On-line information; Related works; Short texts; Syntactical structures; Training data; Transfer learning; Transfer learning methods; Learning algorithms",2-s2.0-85014608014
"Bickel M.W.","A new approach to semantic sustainability assessment: text mining via network analysis revealing transition patterns in German municipal climate action plans",2017,"Energy, Sustainability and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024851475&doi=10.1186%2fs13705-017-0125-0&partnerID=40&md5=7bff8bed1aead011b2a3afe1ac937a2c","Background: Various monitoring approaches have shown that urban areas and their energy systems are major contributors to climate change. Corresponding observations have mostly been based on physical data. However, text data is an untapped source of information that can be analyzed by text mining methods. Taking the example of the German Energy Transition, an interpretation network analysis was used to assess local transition patterns in 16 municipal climate action plans of regional centers in the State of Lower Saxony. Based on the holistic concept of social-ecological systems, three analytical perspectives were introduced, the social system, the energy system, and the three principles of strong sustainability, which inspired three questions: What is the “horizon of attention” regarding the stages of the energy system? What potential, in terms of coordination or collaboration, can be identified on basis of network links between societal sub-systems and the energy system? Are strong sustainability principles adequately linked to the energy system to support a transition towards sustainability? Methods: A computer-aided interpretation network analysis was used. The (co-)occurrence of indicative words representing pre-defined categories was checked in the measures proposed in the plans to analyze the importance and connectedness of these categories. For this purpose, three thesauri were created as fixed literature-based categorization schemes. Results: Municipalities had a nuanced understanding of climate protection focusing on energy conversion and end-use. Public administrations were closely connected role models, economic stakeholders seemed only partly interlinked. The plans referred to all three sustainability principles. However, their implementation might not fully acknowledge the ecological carrying capacity, because, e.g., the strategy of setting limits could not be clearly identified. Conclusions: To advance municipal climate protection, current cross-sectoral multilevel governance approaches should be improved with emphasis on capacity of local administrations, electricity grids, or renewables in the sectors heat and mobility. Also, more emphasis on sustainability communication and education based on all three sustainability principles will be crucial for a transition towards sustainability. From a methodological viewpoint, the text mining approach could confirm and complement recent studies. Considering its limitations and prospects, it can be advanced to useful tool sets for semantic sustainability assessments. © 2017, The Author(s).","German energy transition (Energiewende); Municipal climate action plan; Semantic network analysis; Semantic sustainability assessment; Text mining; Urban energy system","Climate change; Computer aided analysis; Ecology; Public administration; Semantic Web; Semantics; Sustainable development; Action plan; Energiewende; Semantic network analysis; Sustainability assessment; Text mining; Urban energy systems; Data mining; action plan; assessment method; data interpretation; data mining; education; electricity; electricity supply; energy resource; monitoring; network analysis; renewable resource; sustainability; Germany; Lower Saxony",2-s2.0-85024851475
"Xuan J., Luo X., Lu J., Zhang G.","Explicitly and implicitly exploiting the hierarchical structure for mining website interests on news events",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028321335&doi=10.1016%2fj.ins.2017.08.056&partnerID=40&md5=f08a94bfa77fee8eca36e4e959313b12","After a news event, many different websites publish coverage of that event, each expressing their own unique commentary, perspectives, and viewpoints. Websites form around a specific set of interests to cater to different audiences, and discovering these interests can help audiences C especially people and organizations that are interested in news C select the most appropriate websites to use as their sources of information. This paper presents three methods for formally defining and mining a websites interests, each of which is explicitly or implicitly based on a hierarchial structure: website-webpage-keyword. The first, and most straightforward, method explicitly uses keyword-layer network communities and the mapping relations between websites and keywords. The second method expands upon the first method with an iterative algorithm that combines both the mapping relations and the network relations from the website-webpage-keyword structure to further refine the keyword-layer network communities. In the third method, a website topic model implicitly captures the mapping relations among the websites, webpages, and keywords. The performance of three proposed methods in website interest mining is compared using a bespoke evaluation metric. The experimental results show that the iterative procedure designed in the second method is able to improve website interest mining performance, and the website topic model in the third method achieves the best performance among the three methods. © 2017 Elsevier Inc.","News event; Text mining; Web mining; Website interest","Data mining; Iterative methods; Mapping; Natural language processing systems; Network layers; Evaluation metrics; Hierarchial structures; Hierarchical structures; Iterative algorithm; News event; Sources of informations; Text mining; Web Mining; Websites",2-s2.0-85028321335
"Osvaldo S.S., Jr., Lopes D., Silva A.C., Abdelouahab Z.","Developing software systems to Big Data platform based on MapReduce model: An approach based on Model Driven Engineering",2017,"Information and Software Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025631316&doi=10.1016%2fj.infsof.2017.07.006&partnerID=40&md5=85844ee21dd60c4ec4c39fb3bf47604a","Context: The need to analyze a large volume and variety of data for the purpose of extracting information has been promoting investments in Big Data, e.g., for storage, analysis and, more recently, methodologies and approaches for software system development for Big Data platforms. The application of software engineering for Big Data is recent and emerging, so in the literature we find a number of challenges and opportunities related to Big Data, but few practical approaches. Objective In this paper, we propose a practical approach based on MDE (Model Driven Engineering) to support the semi-automated development of software systems for Big Data platform that use MapReduce model. Method The proposed approach consists of framework, process, metamodels, visual Alf, transformation definitions written in ATL and Eclipse IDE plug-in. The proposed framework uses concepts of MDE, Weaving and software development based on Y. Our proposed process guides the use of our approach. A graphical notation and extended metamodel for Alf (i.e. visual Alf) assign executable behavior for UML or DSLs. An Eclipse IDE plug-in implements our approach. Results We show the applicability of the proposed approach through an illustrative example. Conclusion Our approach brings a contribution because the development of software systems is assisted by models which preserves the business logic and adds Big Data features throughout the development process. © 2017 Elsevier B.V.","Big Data; Framework; Metamodels; Model Driven Engineering","Application programs; Computer software; Data mining; Digital storage; Integrodifferential equations; Java programming language; Software design; Software engineering; Development process; Extracting information; Framework; Graphical notation; MapReduce models; Meta model; Model-driven Engineering; Software systems; Big data",2-s2.0-85025631316
"Shahrouzi S.N., Perera D.G.","Dynamic partial reconfigurable hardware architecture for principal component analysis on mobile and embedded devices",2017,"Eurasip Journal on Embedded Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013436244&doi=10.1186%2fs13639-017-0074-x&partnerID=40&md5=f8b58ee7fb8958ad8699ce2c35a16aff","With the advancement of mobile and embedded devices, many applications such as data mining have found their way into these devices. These devices consist of various design constraints including stringent area and power limitations, high speed-performance, reduced cost, and time-to-market requirements. Also, applications running on mobile devices are becoming more complex requiring significant processing power. Our previous analysis illustrated that FPGA-based dynamic reconfigurable systems are currently the best avenue to overcome these challenges. In this research work, we introduce efficient reconfigurable hardware architecture for principal component analysis (PCA), a widely used dimensionality reduction technique in data mining. For mobile applications such as signature verification and handwritten analysis, PCA is applied initially to reduce the dimensionality of the data, followed by similarity measure. Experiments are performed, using a handwritten analysis application together with a benchmark dataset, to evaluate and illustrate the feasibility, efficiency, and flexibility of reconfigurable hardware for data mining applications. Our hardware designs are generic, parameterized, and scalable. Furthermore, our partial and dynamic reconfigurable hardware design achieved 79 times speedup compared to its software counterpart, and 71% space saving compared to its static reconfigurable hardware design. © 2017, The Author(s).","Data mining; Embedded systems; FPGAs; Mobile devices; Partial and dynamic reconfiguration; Principal component analysis; Reconfigurable hardware","Benchmarking; Computer hardware; Data mining; Data reduction; Dynamic models; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Memory architecture; Mobile devices; Reconfigurable architectures; Reconfigurable hardware; Structural design; Data mining applications; Dimensionality reduction techniques; Dynamic re-configuration; Dynamic reconfigurable system; High-speed performance; Mobile applications; Signature verification; Similarity measure; Principal component analysis",2-s2.0-85013436244
"Baechle C., Agarwal A., Zhu X.","Big data driven co-occurring evidence discovery in chronic obstructive pulmonary disease patients",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016926720&doi=10.1186%2fs40537-017-0067-6&partnerID=40&md5=fb2389decadaf4c15b41c02004e3bc99","Background: Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that affects airflow to the lungs. Discovering the co-occurrence of COPD with other diseases, symptoms, and medications is invaluable to medical staff. Building co-occurrence indexes and finding causal relationships with COPD can be difficult because often times disease prevalence within a population influences results. A method which can better separate occurrence within COPD patients from population prevalence would be desirable. Large hospital systems may potentially have tens of millions of patient records spanning decades of collection and a big data approach that is scalable is desirable. The presented method, Co-Occurring Evidence Discovery (COED), presents a methodology and framework to address these issues. Methods: Natural Language Processing methods are used to examine 64,371 deidentified clinical notes and discover associations between COPD and medical terms. Apache cTAKES is leveraged to annotate and structure clinical notes. Several extensions to cTAKES have been written to parallelize the annotation of large sets of clinical notes. A co-occurrence score is presented which can penalize scores based on term prevalence, as well as a baseline method traditionally used for finding co-occurrence. These scoring systems are implemented using Apache Spark. Dictionaries of ground truth terms for diseases, medications, and symptoms have been created using clinical domain knowledge. COED and baseline methods are compared using precision, recall, and F1 score. Results: The highest scoring diseases using COED are lung and respiratory diseases. In contrast, baseline methods for co-occurrence rank diseases with high population prevalence highest. Medications and symptoms evaluated with COED share similar results. When evaluated against ground truth dictionaries, the maximum improvements in recall for symptoms, diseases, and medications were 0.212, 0.130, and 0.174. The maximum improvements in precision for symptoms, diseases, and medications were 0.303, 0.333, and 0.180. Median increase in F1 score for symptoms, diseases, and medications were 38.1%, 23.0%, and 17.1%. A paired t-test was performed and F1 score increases were found to be statistically significant, where p < 0.01. Conclusion: Penalizing terms which are highly frequent in the corpus results in better precision and recall performance. Penalizing frequently occurring terms gives a better picture of the diseases, symptoms, and medications co-occurring with COPD. Using a mathematical and computational approach rather than purely expert driven approach, large dictionaries of COPD related terms can be assembled in a short amount of time. © 2017, The Author(s).","Big data; Data mining; Decision support system; Health informatics",,2-s2.0-85016926720
"Sinoara R.A., Antunes J., Rezende S.O.","Text mining and semantics: a systematic mapping study",2017,"Journal of the Brazilian Computer Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021660797&doi=10.1186%2fs13173-017-0058-7&partnerID=40&md5=d10b9d803eb262017cc5cb935f3fccc8","As text semantics has an important role in text meaning, the term semantics has been seen in a vast sort of text mining studies. However, there is a lack of studies that integrate the different research branches and summarize the developed works. This paper reports a systematic mapping about semantics-concerned text mining studies. This systematic mapping study followed a well-defined protocol. Its results were based on 1693 studies, selected among 3984 studies identified in five digital libraries. The produced mapping gives a general summary of the subject, points some areas that lacks the development of primary or secondary studies, and can be a guide for researchers working with semantics-concerned text mining. It demonstrates that, although several studies have been developed, the processing of semantic aspects in text mining remains an open research problem. © 2017, The Author(s).","Systematic review; Text mining; Text semantics","Digital libraries; Mapping; Semantics; Research problems; Systematic mapping; Systematic mapping studies; Systematic Review; Text mining; Data mining",2-s2.0-85021660797
"Karmitsa N., Bagirov A.M., Taheri S.","New diagonal bundle method for clustering problems in large data sets",2017,"European Journal of Operational Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021213133&doi=10.1016%2fj.ejor.2017.06.010&partnerID=40&md5=10a24e6312d41999c3fd98253fba940c","Clustering is one of the most important tasks in data mining. Recent developments in computer hardware allow us to store in random access memory (RAM) and repeatedly read data sets with hundreds of thousands and even millions of data points. This makes it possible to use conventional clustering algorithms in such data sets. However, these algorithms may need prohibitively large computational time and fail to produce accurate solutions. Therefore, it is important to develop clustering algorithms which are accurate and can provide real time clustering in large data sets. This paper introduces one of them. Using nonsmooth optimization formulation of the clustering problem the objective function is represented as a difference of two convex (DC) functions. Then a new diagonal bundle algorithm that explicitly uses this structure is designed and combined with an incremental approach to solve this problem. The method is evaluated using real world data sets with both large number of attributes and large number of data points. The proposed method is compared with two other clustering algorithms using numerical results. © 2017 Elsevier B.V.","Bundle methods; Data mining; DC function; Nonconvex optimization; Nonsmooth optimization","Computer hardware; Data mining; Numerical methods; Optimization; Problem solving; Random access storage; Bundle methods; Clustering problems; Conventional clustering; DC functions; Incremental approach; Nonconvex optimization; Nonsmooth optimization; Random access memory; Clustering algorithms",2-s2.0-85021213133
"Liu L., Wang S., Su G., Hu B., Peng Y., Xiong Q., Wen J.","A framework of mining semantic-based probabilistic event relations for complex activity recognition",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026469024&doi=10.1016%2fj.ins.2017.07.022&partnerID=40&md5=689d425dad5cff5bc6bbccef9915bc0b","Human activity recognition has become a key research topic in a variety of applications. Modeling activity events and their rich relations using high-level human understandable activity models such as semantic-based knowledge base hold promise. However, formulas in current semantic-based approaches are generally manually encoded, which is rather unrealistic in situations where event relations are intricate. Moreover, current approaches for learning event relations often lack the capability to handle uncertainties. To address these issues, we present a framework to learn an event knowledge base (EKB) of probabilistic interval-based event relations and use them to infer varied semantic-level queries about activity occurrences under uncertainty. Specifically, we formalize an activity model to represent eight temporal and hierarchical event relations and four commonly performed queries. We leverage pattern mining techniques to learn an EKB associated with these relations and queries in a unified way. Experimental results show that the proposed framework with the learned EKB involving temporal and hierarchical dependencies leads to a significant performance improvement on activity recognition, particularly in the presence of incomplete or incorrect observations. © 2017 Elsevier Inc.","Activity model; Pattern mining; Probabilistic event relation learning; Semantic-based representation","Data mining; Knowledge based systems; Pattern recognition; Semantics; Activity modeling; Activity recognition; Complex activity; Human activity recognition; Pattern mining; Probabilistic event relation learning; Research topics; Semantic levels; Education",2-s2.0-85026469024
"Yu S., Zhang D., Mabu S., Chen J., Hirasawa K.","Improving the robustness of GNP-PCA using the multiagent system",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029441894&doi=10.1016%2fj.asoc.2017.08.043&partnerID=40&md5=fe9cbed13ab650254cc45d2f3a76f23d","In order to improve the robustness of Genetic Network Programming fuzzy data mining and PCA (GNP-PCA) based face recognition in the Gaussian and Salt&Pepper noisy testing environments, a GNP-based multi-agent system is constructed using GNP-PCA and multi-resolution analysis in this paper. In the proposed approach, the different scales of training images in the Laplacian pyramid are regarded as sub-environments and each GNP-PCA is performed as an agent in its corresponding environment. Face recognition is finally realized by maximizing the weighted average matching degrees of all the persons in the training database. Experimental results indicate that the proposed method has improved the robustness of GNP-PCA in the Gaussian and Salt&Pepper noisy testing environments considerably. © 2017 Elsevier B.V.","Face recognition; Gaussian pyramid; Genetic Network Programming; Laplacian pyramid; Multiagent system; Principal Component Analysis; Robustness","Data mining; Face recognition; Gaussian distribution; Genetic programming; Laplace transforms; Multi agent systems; Robustness (control systems); Fuzzy-data mining; Gaussian pyramids; Genetic network programming; Laplacian Pyramid; Matching degree; Testing environment; Training database; Weighted averages; Principal component analysis",2-s2.0-85029441894
"Song K., Kwon N., Anderson K., Park M., Lee H.-S., Lee S.","Predicting hourly energy consumption in buildings using occupancy-related characteristics of end-user groups",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030452570&doi=10.1016%2fj.enbuild.2017.09.060&partnerID=40&md5=df3ea43ca4d6b3d9ecb44215add121e3","Accurate predictions of energy consumption are essential to optimizing building energy use performance. To date, substantial efforts have been undertaken to improve prediction accuracy, specifically while focusing on occupants’ presence in buildings. Unfortunately, two significant obstacles remain when predicting building energy consumption using occupancy data. First, occupancy diversity among end-user groups is rarely considered during model development. Second, occupancy's correlation with energy consumption may be weak due to variances in occupant behavior. Therefore, this research aims to investigate how occupancy-related characteristics of end-user groups affect prediction performance. In order to achieve this objective, a data mining-based prediction model is constructed to mimic building thermal behaviors. The experimental results using the proposed prediction model make it evident that prediction accuracy is improved when considering diverse occupancy and its correlation with energy use. In addition, significant prediction accuracy is achieved using only a minimal amount of historical data. With the proposed prediction model, it is possible to obtain more detailed information about energy use patterns (e.g., load shape, the amount of energy use) for end-user groups. Thus, facility managers will be able to personalize the operation of energy-consuming equipment depending on end-user group for reducing energy consumption without compromising occupants’ thermal comfort. © 2017","Data mining techniques; Energy saving; Energy use prediction; Occupancy status; Occupants’ energy use behavior","Buildings; Data mining; Energy utilization; Forecasting; Building energy consumption; Building energy use; Energy use; Energy use patterns; Occupancy status; Prediction accuracy; Prediction performance; Reducing energy consumption; Energy conservation",2-s2.0-85030452570
"Vasconcelos I., Vasconcelos R.O., Olivieri B., Roriz M., Endler M., Junior M.C.","Smartphone-based outlier detection: a complex event processing approach for driving behavior detection",2017,"Journal of Internet Services and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029939870&doi=10.1186%2fs13174-017-0065-0&partnerID=40&md5=1dbe7c5cad2f7780feaca3bd774635d5","The majority of fatal car crashes are caused by reckless driving. With the sophistication of vehicle instrumentation, reckless maneuvers, such as abrupt turns, acceleration, and deceleration, can now be accurately detected by analyzing data related to the driver-vehicle interactions. Such analysis usually requires very specific in-vehicle hardware and infrastructure sensors (e.g. loop detectors and radars), which can be costly. Hence, in this paper, we investigated if off-the-shelf smartphones can be used to online detect and classify the driver’s behavior in near real-time. To do so, we first modeled and performed an intrinsic evaluation to assess the performance of three outlier detection algorithms formulated as a data stream processing network which receives as input and processes data streams of smartphone and vehicle sensors. Next, we implemented a novel scoring mechanism based on online outlier detection to quantitatively evaluate drivers’ maneuvers as either cautious or reckless. Thus, we adapted a data mining mechanism which takes into account a sensor’s data rates and power to determine driver behavior in the scoring process. Finally, as the intrinsic evaluation does not necessarily reveal how well an algorithm will perform in a real-world scenario, we evaluated the algorithm that achieved the best result in a real-world case study to assess drivers’ driving behavior. Our results indicate that the algorithm performs quickly and accurately; the algorithm classifies driver behavior with 95.45% accuracy. Moreover, such results are obtained within 100 milliseconds of processing time on average. © 2017, The Author(s).","In-Vehicle sensing; Online driving behavior detection; Online outlier detection; Smartphone","Accidents; Data communication systems; Data handling; Data mining; Smartphones; Statistics; Vehicles; Complex event processing; Data stream processing; Driving behavior; Outlier Detection; Outlier detection algorithm; Real-world scenario; Reckless driving; Vehicle interactions; Behavioral research",2-s2.0-85029939870
"Bouakkaz M., Ouinten Y., Loudcher S., Strekalova Y.","Textual aggregation approaches in OLAP context: A survey",2017,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022002579&doi=10.1016%2fj.ijinfomgt.2017.06.005&partnerID=40&md5=6d7d928dd22d40082cab33a5d5772648","In the last decade, OnLine Analytical Processing (OLAP) has taken an increasingly important role as a research field. Solutions, techniques and tools have been provided for both databases and data warehouses to focus mainly on numerical data. however these solutions are not suitable for textual data. Therefore recently, there has been a huge need for new tools and approaches that treat and manipulate textual data and aggregate it as well. Textual aggregation techniques emerge as a key tool to perform textual data analysis in OLAP for decision support systems. This paper aims at providing a structured and comprehensive overview of the literature in the field of OLAP Textual Aggregation. We provide a new classification framework in which the existing textual aggregation approaches are grouped into two main classes, namely approaches based on cube structure and approaches based on text mining. We discuss and synthesize also the potential of textual similarity metrics, and we provide a recent classification of them. © 2017 Elsevier Ltd","Aggregation; Data mining; Data warehouse; OLAP; Textual data","Agglomeration; Artificial intelligence; Data warehouses; Decision support systems; Text processing; Aggregation techniques; Classification framework; Cube structures; OLAP; On-line analytical processing; Techniques and tools; Textual data; Textual similarities; Data mining",2-s2.0-85022002579
"Murugesan G., Abdulkadhar S., Bhasuran B., Natarajan J.","BCC-NER: bidirectional, contextual clues named entity tagger for gene/protein mention recognition",2017,"Eurasip Journal on Bioinformatics and Systems Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019048413&doi=10.1186%2fs13637-017-0060-6&partnerID=40&md5=f1ce99784d8b5008ff1d84e23d618bd8","Tagging biomedical entities such as gene, protein, cell, and cell-line is the first step and an important pre-requisite in biomedical literature mining. In this paper, we describe our hybrid named entity tagging approach namely BCC-NER (bidirectional, contextual clues named entity tagger for gene/protein mention recognition). BCC-NER is deployed with three modules. The first module is for text processing which includes basic NLP pre-processing, feature extraction, and feature selection. The second module is for training and model building with bidirectional conditional random fields (CRF) to parse the text in both directions (forward and backward) and integrate the backward and forward trained models using margin-infused relaxed algorithm (MIRA). The third and final module is for post-processing to achieve a better performance, which includes surrounding text features, parenthesis mismatching, and two-tier abbreviation algorithm. The evaluation results on BioCreative II GM test corpus of BCC-NER achieve a precision of 89.95, recall of 84.15 and overall F-score of 86.95, which is higher than the other currently available open source taggers. © 2017, The Author(s).","Bidirectional parsing; Biomedical text mining; Conditional random fields; Hybrid NER approaches; Margin-infused relaxed algorithm; Named entity recognition","Bioinformatics; Cell culture; Data mining; Feature extraction; Genes; Industrial plants; Natural language processing systems; Random processes; Text processing; Bidirectional parsing; Biomedical text minings; Conditional random field; Hybrid NER approaches; Named entity recognition; Character recognition; human; mining; model; molecular recognition; recall; word processing",2-s2.0-85019048413
"Uddin M.F., Lee J.","Proposing stochastic probability-based math model and algorithms utilizing social networking and academic data for good fit students prediction",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023753518&doi=10.1007%2fs13278-017-0448-z&partnerID=40&md5=429df37de405be6db236962cf42506ec","The research progress presented in this paper comes under the areas of data science. The authors propose enhanced machine learning (supervised learning) framework for the prediction of the students through stochastic probability-based math constructs/model and an algorithm [Good Fit Student (GFS)], along with the enhanced quantification of target variables and algorithmic metrics. Academia in today’s modern world sees the problem of dropouts, low retention, poor student performances, lack of motivation, and unnecessary change of study majors and re-admissions. The authors consider this challenge as a research problem and attempt to solve it by utilizing social networking-based personality traits, relevant data and features to improve the predictive modeling approach. The authors recognize that admission choices are often governed by family trends, affordability, basic motivation, market trends, and natural instincts. However, natural gifts and talents are minimally used to select such directions in the academics. The authors based on literature review identify this a research gap and improves with a unique blend of algorithms/methods, an improved modeling of performance metrics, built upon cross-validation to improve fitness, and enhance the process of feature engineering and tuning for reduced errors and optimum fitness, at the end. The authors present the latest progress of their research in this paper. The included results show the progress of the work and ongoing improvements. The authors use machine learning techniques, Microsoft SQL Server, Excel data mining, R and Python to develop and test their model. The authors provide related work and conclude with final remarks and future work. © 2017, Springer-Verlag GmbH Austria.","Algorithm tuning and blending; Data analytics and mining; Feature engineering; Good Fit Student Algorithm; Machine learning; Personality features correlation with academic data; Social networking features correlation; Stochastic probability distribution-based modeling",,2-s2.0-85023753518
"Kumar S., Kumar P.","Upper approximation based privacy preserving in online social networks",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024115392&doi=10.1016%2fj.eswa.2017.07.010&partnerID=40&md5=fc8e245d43065c1af9afc3acb4aa5dac","With the advent of the online social network and advancement of technology, people get connected and interact on social network. To better understand the behavior of users on social network, we need to mine the interactions of users and their demographic data. Companies with less or no expertise in mining would need to share this data with the companies of expertise for mining purposes. The major challenge in sharing the social network data is maintaining the individual privacy on social network while retaining the implicit knowledge embedded in the social network. Thus, there is a need of anonymizing the social network data before sharing it to the third-party. The current study proposes to use upper approximation concept of rough sets for developing a solution for privacy preserving social network graph publishing. The proposed algorithm is capable of preserving the privacy of graph structure while simultaneously maintaining the utility or value that can be generated from the graph structure. The proposed algorithm is validated by showing its effectiveness on several graph mining tasks like clustering, classification, and PageRank computation. The set of experiments were conducted on four standard datasets, and the results of the study suggest that the proposed algorithm would maintain the both the privacy of individuals and the accuracy of the graph mining tasks. © 2017 Elsevier Ltd","Graph publishing; Online social network; Privacy preserving; Rough-sets","Approximation algorithms; Clustering algorithms; Data mining; Data privacy; Graphic methods; Online systems; Rough set theory; Social sciences computing; Websites; Demographic data; Graph structures; Implicit knowledge; Individual privacy; On-line social networks; PageRank computations; Privacy preserving; Upper approximation; Social networking (online)",2-s2.0-85024115392
"Doan T., Kalita J.","Predicting run time of classification algorithms using meta-learning",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376525&doi=10.1007%2fs13042-016-0571-6&partnerID=40&md5=681bdff69578d6026244f3fc3f331bbc","Selecting a right classification algorithm is an important step for the success of any data mining project. Run time can be used to assess efficiency of a classification algorithm of interest. Experimenting with several algorithms can increase the cost of a data mining project. Using the idea of meta-learning, we present an approach to estimate the run time of a particular classification algorithm on an arbitrary dataset. Our work provides a way to evaluate the choice of an algorithm without experimental execution. We demonstrate that the use of multivariate adaptive regression splines significantly outperforms other regression models, even a ‘state-of-the-art’ algorithm such as support vector regression. © 2016, Springer-Verlag Berlin Heidelberg.","Classification; Meta-learning; Regression model; Runtime","Classification (of information); Learning algorithms; Regression analysis; Classification algorithm; Metalearning; Mining projects; Multivariate adaptive regression splines; Regression model; Runtimes; State of the art; Support vector regression (SVR); Data mining",2-s2.0-85032376525
"Ooi S.Y., Tan S.C., Cheah W.P.","Temporal sampling forest (TS-F): an ensemble temporal learner",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978153695&doi=10.1007%2fs00500-016-2242-7&partnerID=40&md5=3f4d36e6cdebd480e8b8d6e114c0960e","Ensemble learning is in favour of machine learning community due to its tolerance in handling divergence and biasness issues faced by a single learner. In this work, an ensemble temporal learner, namely temporal sampling forest (TS-F), is proposed. Building on the random forest, we consider its limitations in handling temporal classification tasks. Temporal data classification is an important area of machine learning and data mining, where it fills the gap of ordinary data classification when the observed datasets are temporally related across sequential and time domains. TS-F incorporated the temporal sampling (bagging) and temporal randomization procedures in the classical random forest, hence extending its ability to handle temporal data. TS-F was tested on 11 public sequential and temporal datasets from different domains. Experiments demonstrate that TS-F could provide promising results with average classification accuracy of 98 %, substantiating its ability to escalate the random forest performance in the application of temporal classification. © 2016, Springer-Verlag Berlin Heidelberg.","Ensemble learner; Random forest; Temporal application; Temporal classification","Artificial intelligence; Data mining; Decision trees; Learning systems; Classification accuracy; Data classification; Ensemble learner; Machine learning communities; Random forests; Randomization procedure; Temporal applications; Temporal classification; Classification (of information)",2-s2.0-84978153695
"Virta J., Nordhausen K.","Blind source separation of tensor-valued time series",2017,"Signal Processing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020772309&doi=10.1016%2fj.sigpro.2017.06.008&partnerID=40&md5=0a7e22f742102541fa4b90082defb3eb","The blind source separation model for multivariate time series generally assumes that the observed series is a linear transformation of an unobserved series with temporally uncorrelated or independent components. Given the observations, the objective is to find a linear transformation that recovers the latent series. Several methods for accomplishing this exist and three particular ones are the classic SOBI and the recently proposed generalized FOBI (gFOBI) and generalized JADE (gJADE), each based on the use of joint lagged moments. In this paper we generalize the methodologies behind these algorithms for tensor-valued time series. We assume that our data consists of a tensor observed at each time point and that the observations are linear transformations of latent tensors we wish to estimate. The tensorial generalizations are shown to have particularly elegant forms and we show that each of them is Fisher consistent and orthogonal equivariant. Comparing the new methods with the original ones in various settings shows that the tensorial extensions are superior to both their vector-valued counterparts and to two existing tensorial dimension reduction methods for i.i.d. data. Finally, applications to fMRI-data and video processing show that the methods are capable of extracting relevant information from noisy high-dimensional data. © 2017 Elsevier B.V.","FOBI; JADE; Multilinear algebra; SOBI","Clustering algorithms; Data handling; Data mining; Linear transformations; Mathematical transformations; Silicate minerals; Tensors; Time series; Video signal processing; Dimension reduction method; FOBI; High dimensional data; Independent components; JADE; Multi-linear algebras; Multivariate time series; SOBI; Blind source separation",2-s2.0-85020772309
"Ghadiri N., Ghaffari M., Nikbakht M.A.","BigFCM: Fast, precise and scalable FCM on hadoop",2017,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023611123&doi=10.1016%2fj.future.2017.06.010&partnerID=40&md5=6426668b9c967012274e6fb7e8f18d4b","Clustering plays an important role in mining big data both as a modeling technique and a preprocessing step in many data mining process implementations. Fuzzy clustering provides more flexibility than non-fuzzy methods by allowing each data record to belong to more than one cluster to some degree. However, a serious challenge in fuzzy clustering is the lack of scalability. Massive datasets in emerging fields such as geosciences, biology, and networking do require parallel and distributed computations with high performance to solve real-world problems. Although some clustering methods are already improved to execute on big data platforms, their execution time is highly increased for gigantic datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering method named BigFCM is proposed and designed for the Hadoop distributed data platform. Based on the MapReduce programming model, the proposed algorithm exploits several mechanisms including an efficient caching design to achieve several orders of magnitude reduction in execution time. The BigFCM performance compared with Apache Mahout K-Means and Fuzzy K-Means through an evaluation framework developed in this research. Extensive evaluation using over multi-gigabyte datasets including SUSY and HIGGS shows that BigFCM is scalable while it preserves the quality of clustering. © 2017 Elsevier B.V.","Big data; Clustering; Data mining; MapReduce algorithms; Unsupervised learning and clustering; Vagueness and fuzzy logic","Cluster analysis; Data mining; Distributed computer systems; Fuzzy clustering; Fuzzy logic; Quality control; Clustering; Distributed computations; Evaluation framework; Fuzzy C means clustering; Map-reduce; Map-reduce programming; Orders of magnitude; Quality of clustering; Big data",2-s2.0-85023611123
"Yang X., Li T., Liu D., Chen H., Luo C.","A unified framework of dynamic three-way probabilistic rough sets",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027881437&doi=10.1016%2fj.ins.2017.08.053&partnerID=40&md5=ca26307af0a9a2e9339bd196a295f2f9","The incremental learning technology has been widely applied in efficient and effective data mining with big data based on granular computing, rough sets and three-way approaches. In real-life applications, the information systems will evolve over time with four levels of variational situations, which can be described by the combination of the variations of attributes, objects, condition attributes values and decision attributes values. Considering updating knowledge with multilevel variations of data, this paper proposes a unified dynamic framework of decision-theoretic rough sets for incrementally updating three-way probabilistic regions, namely, positive region, boundary region and negative region. Through improving the representation of three-way regions based on the well-established Bayesian decision procedure, a novel matrix approach is introduced by the construction of Boolean matrix and specific definition of matrix operation. Subsequently, at the variations of level-1, the fundamental updating propositions, which can induce the corresponding propositions with the variations of level-2, level-3, level-4, respectively, are presented by the matrix updating strategies. Finally, experiments with four incremental algorithms developed for the verification of feasibility and efficiency under multilevel variations of data are conducted by comparison with non-incremental algorithm. © 2017 Elsevier Inc.","Decision-theoretic rough sets; Incremental learning; Matrix; Multilevel variations; Three-way decisions","Big data; Data mining; Matrix algebra; Real time systems; Decision-theoretic rough sets; Incremental learning; Incremental learning technology; Incrementally Updating; Multilevel variations; Probabilistic rough sets; Real-life applications; Three-way decisions; Rough set theory",2-s2.0-85027881437
"Chen Y., Zeng Z., Lu J.","Neighborhood rough set reduction with fish swarm algorithm",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990949436&doi=10.1007%2fs00500-016-2393-6&partnerID=40&md5=02cf812ec7bb974734047172d23848d4","Feature reduction refers to the problem of deleting those input features that are less predictive of a given outcome; a problem encountered in many areas such as pattern recognition, machine learning and data mining. In particular, it has been successfully applied in tasks that involve datasets containing huge numbers of features. Rough set theory has been used as such a data set preprocessor with much success, but current methods are inadequate at solving the problem of numerical feature reduction. As the classical rough set model can just be used to evaluate categorical features, we introduce a neighborhood rough set model to deal with numerical datasets by defining a neighborhood relation. However, this method is still not enough to find the optimal subsets regularly. In this paper, we propose a new feature reduction mechanism based on fish swarm algorithm (FSA) in an attempt to polish up this. The method is then applied to the problem of finding optimal feature subsets in the neighborhood rough set reduction process. We define three foraging behaviors of fish to find the optimal subsets and a fitness function to evaluate the best solutions. We construct the neighborhood feature reduction algorithm based on FSA and design some experiments comparing with a heuristic neighborhood feature reduction method. Experimental results show that the FSA-based neighborhood reduction method is suitable to deal with numerical data and more possibility to find an optimal reduct. © 2016, Springer-Verlag Berlin Heidelberg.","Feature reduction; Fish swarm algorithm; Granular computing; Neighborhood system; Rough set theory","Artificial intelligence; Computation theory; Data mining; Data reduction; Granular computing; Heuristic methods; Learning systems; Numerical methods; Optimization; Pattern recognition; Problem solving; Set theory; Categorical features; Feature reduction; Fish-swarm algorithms; Foraging behaviors; Neighborhood relation; Neighborhood rough sets; Neighborhood systems; Numerical datasets; Rough set theory",2-s2.0-84990949436
"Wei X., Gong H., Yu J., Liu P., Wang L., Zhang Y., Zhang X.","SesameFG: An integrated database for the functional genomics of sesame",2017,"Scientific Reports",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019890798&doi=10.1038%2fs41598-017-02586-3&partnerID=40&md5=fdf99285cbba59619e64ca07cc1621ba","Sesame (Sesamum indicum L.) has high oil content, a small diploid genome and a short growth period, making it an attractive species for genetic studies on oilseed crops. With the advancement of next-generation sequencing technology, genomics and functional genomics research of sesame has developed quickly in the last few years, and large amounts of data have been generated. However, these results are distributed in many different publications, and there is a lack of integration. To promote functional genomics research of sesame, we collected genetic information combined with comprehensive phenotypic information and integrated them in the web-based database named SesameFG. The current version of SesameFG contains phenotypic information on agronomic traits of 705 sesame accessions, de novo assembled genomes of three sesame varieties, massive numbers of identified SNPs, gene expression profiles of five tissues, gene families, candidate genes for the important agronomic traits and genomic-SSR markers. All phenotypic and genotypic information in SesameFG is available for online queries and can be downloaded freely. SesameFG provides useful search functions and data mining tools, including Genome Browser and local BLAST services. SesameFG is freely accessible at http://ncgr.ac.cn/SesameFG/. SesameFG provides valuable resources and tools for functional genomics research and the molecular breeding of sesame. © 2017 The Author(s).",,"agronomic trait; breeding; data base; data mining; functional genomics; gene expression profiling; genetic marker; multigene family",2-s2.0-85019890798
"McGlinn K., Yuce B., Wicaksono H., Howell S., Rezgui Y.","Usability evaluation of a web-based tool for supporting holistic building energy management",2017,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028979335&doi=10.1016%2fj.autcon.2017.08.033&partnerID=40&md5=793011a9aada5805ca634446cfcc520d","This paper presents the evaluation of the level of usability of an intelligent monitoring and control interface for energy efficient management of public buildings, called BuildVis, which forms part of a Building Energy Management System (BEMS.) The BEMS ‘intelligence’ is derived from an intelligent algorithm component which brings together ANN-GA rule generation, a fuzzy rule selection engine, and a semantic knowledge base. The knowledge base makes use of linked data and an integrated ontology to uplift heterogeneous data sources relevant to building energy consumption. The developed ontology is based upon the Industry Foundation Classes (IFC), which is a Building Information Modelling (BIM) standard and consists of two different types of rule model to control and manage the buildings adaptively. The populated rules are a mix of an intelligent rule generation approach using Artificial Neural Network (ANN) and Genetic Algorithms (GA), and also data mining rules using Decision Tree techniques on historical data. The resulting rules are triggered by the intelligent controller, which processes available sensor measurements in the building. This generates ‘suggestions’ which are presented to the Facility Manager (FM) on the BuildVis web-based interface. BuildVis uses HTML5 innovations to visualise a 3D interactive model of the building that is accessible over a wide range of desktop and mobile platforms. The suggestions are presented on a zone by zone basis, alerting them to potential energy saving actions. As the usability of the system is seen as a key determinate to success, the paper evaluates the level of usability for both a set of technical users and also the FMs for five European buildings, providing analysis and lessons learned from the approach taken. © 2017","Artificial neural network; BEMS; Fuzzy logic; Genetic algorithm; IFC; Information visualisation; Ontology","Architectural design; Buildings; Data mining; Decision trees; Energy conservation; Energy efficiency; Energy management; Energy utilization; Fuzzy inference; Fuzzy logic; Genetic algorithms; Knowledge based systems; Multimedia systems; Neural networks; Ontology; Potential energy; Semantics; Trees (mathematics); Usability engineering; Websites; BEMS; Building energy consumption; Building energy management systems; Building energy managements; Building Information Modelling; Heterogeneous data sources; Industry Foundation Classes - IFC; Information visualisation; Energy management systems",2-s2.0-85028979335
"Feng X., Yang T., Yu H.","A new multi-colony fairness algorithm for feature selection",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977134568&doi=10.1007%2fs00500-016-2257-0&partnerID=40&md5=e5542ee032a8a027f829d1db339ab5ca","As the world gradually transforms from an information world to a data-driven world, areas of pattern recognition and data mining are facing more and more challenges. The process of feature subset selection becomes a necessary part of big data pattern recognition due to the data with explosive growth. Inspired by the behavior of grabbing resources in animals, this paper adds personal grabbing-resource behavior into the model of resource allocation transformed from the model of feature selection. Multi-colony fairness algorithm (MCFA) is proposed to deal with grabbing-resource behaviors in order to obtain a better distribution scheme (i.e., to obtain a better feature subset). The algorithm effectively fuses strategies of the random search and the heuristic search. In addition, it combines methods of filter and wrapper so as to reduce the amount of calculation while improving classification accuracies. The convergence and the effectiveness of the proposed algorithm are verified both from mathematical and experimental aspects. MCFA is compared with other four classic feature selection algorithms such as sequential forward selection, sequential backward selection, sequential floating forward selection, and sequential floating backward selection and three mainstream feature selection algorithms such as relevance–redundancy feature selection, minimal redundancy–maximal relevance, and ReliefF. The comparison results show that the proposed algorithm can obtain better feature subsets both in the aspects of feature subset length which is defined as the number of features in a feature subset and the classification accuracy. The two aspects indicate the efficiency and the effectiveness of the proposed algorithm. © 2016, Springer-Verlag Berlin Heidelberg.","Feature selection; Grabbing-resource behavior; Multi-colony fairness algorithm (MCFA); Resource allocation","Algorithms; Big data; Data mining; Filtration; Heuristic algorithms; Pattern recognition; Redundancy; Resource allocation; Set theory; Classification accuracy; Distribution scheme; Experimental aspects; Fairness algorithm; Feature selection algorithm; Feature subset selection; Grabbing-resource behavior; Sequential forward selection; Feature extraction",2-s2.0-84977134568
"Liu P., Hoth N., Drebenstedt C., Sun Y., Xu Z.","Hydro-geochemical paths of multi-layer groundwater system in coal mining regions — Using multivariate statistics and geochemical modeling approaches",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019616772&doi=10.1016%2fj.scitotenv.2017.05.146&partnerID=40&md5=778ea962cee4286cb115b2bde78243f7","Groundwater is an important drinking water resource that requires protection in North China. Coal mining industry in the area may influence the water quality evolution. To provide primary characterization of the hydrogeochemical processes and paths that control the water quality evolution, a complex multi-layer groundwater system in a coal mining area is investigated. Multivariate statistical methods involving hierarchical cluster analysis (HCA) and principal component analysis (PCA) are applied, 6 zones and 3 new principal components are classified as major reaction zones and reaction factors. By integrating HCA and PCA with hydrogeochemical correlations analysis, potential phases, reactions and connections between various zones are presented. Carbonates minerals, gypsum, clay minerals as well as atmosphere gases - CO2, H2O and NH3 are recognized as major reactants. Mixtures, evaporation, dissolution/precipitation of minerals and cation exchange are potential reactions. Inverse modeling is finally used, and it verifies the detailed processes and diverse paths. Consequently, 4 major paths are found controlling the variations of groundwater chemical properties. Shallow and deep groundwater is connected primarily by the flow of deep groundwater up through fractures and faults into the shallow aquifers. Mining does not impact the underlying aquifers that represent the most critical groundwater resource. But controls should be taken to block the mixing processes from highly polluted mine water. The paper highlights the complex hydrogeochemical evolution of a multi-layer groundwater system under mining impact, which could be applied to further groundwater quality management in the study area, as well as most of the other coalfields in North China. © 2017 Elsevier B.V.","Coal mining regions; Groundwater environment; Hydro-geochemical processes; Inverse geochemical modeling; Multi-layer evolutions; Multivariate statistics","Aquifers; Carbon dioxide; Chemical analysis; Cluster analysis; Coal; Coal deposits; Coal industry; Coal mines; Geochemistry; Groundwater; Hierarchical systems; Hydrochemistry; Hydrogeology; Inverse problems; Minerals; Multivariant analysis; Potable water; Principal component analysis; Process control; Quality control; Quality management; Statistical methods; Water; Water quality; Water resources; Coal mining; Geochemical process; Groundwater environment; Inverse Geochemical modeling; Multivariate statistics; Groundwater resources; ammonia; bicarbonate; calcium; calcium sulfate; carbon dioxide; carbonic acid derivative; ground water; lake water; magnesium; sulfate; surface water; water; aquifer; coal mining; data inversion; groundwater; hierarchical system; hydrogeochemistry; mixing; multivariate analysis; water quality; aquifer; Article; cation exchange; China; clay; cluster analysis; coal mining; dissolution; evaporation; geochemical analysis; molecular evolution; precipitation; principal component analysis; priority journal; water flow; water pollution; water quality; China",2-s2.0-85019616772
"Nie F., Li J., Li X.","Convex Multiview Semi-Supervised Classification",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028690206&doi=10.1109%2fTIP.2017.2746270&partnerID=40&md5=216a58d3d39ff6f97aa066ba5fc519c8","In many practical applications, there are a great number of unlabeled samples available, while labeling them is a costly and tedious process. Therefore, how to utilize unlabeled samples to assist digging out potential information about the problem is very important. In this paper, we study a multiclass semi-supervised classification task in the context of multiview data. First, an optimization method named Parametric multiview semi-supervised classification (PMSSC) is proposed, where the built classifier for each individual view is explicitly combined with a weight factor. By analyzing the weakness of it, a new adapted weight learning strategy is further formulated, and we come to the convex multiview semi-supervised classification (CMSSC) method. Comparing with the PMSSC, this method has two significant properties. First, without too much loss in performance, the newly used weight learning technique achieves eliminating a hyperparameter, and thus it becomes more compact in form and practical to use. Second, as its name implies, the CMSSC models a convex problem, which avoids the local-minimum problem. Experimental results on several multiview data sets demonstrate that the proposed methods achieve better performances than recent representative methods and the CMSSC is preferred due to its good traits. © 1992-2012 IEEE.","Multiview data; semi-supervised classification; weight learning","Additives; Data mining; Feature extraction; Learning systems; Personnel training; Multi-view datum; Optimization method; Semi- supervised learning; Semi-supervised classification; weight learning; Supervised learning",2-s2.0-85028690206
"García G.M., Nunes B.P., Lopes G.R., Casanova M.A., Paes Leme L.A.P.","Techniques for comparing and recommending conferences",2017,"Journal of the Brazilian Computer Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015633539&doi=10.1186%2fs13173-017-0053-z&partnerID=40&md5=af649ba5cc4c2f53e97e2baaf2a98e46","This article defines, implements, and evaluates techniques to automatically compare and recommend conferences. The techniques for comparing conferences use familiar similarity measures and a new measure based on co-authorship communities, called co-authorship network community similarity index. The experiments reported in the article indicate that the technique based on the new measure performs better than the other techniques for comparing conferences, which is therefore the first contribution of the article. Then, the article focuses on three families of techniques for conference recommendation. The first family adopts collaborative filtering based on the conference similarity measures investigated in the first part of the article. The second family includes two techniques based on the idea of finding, for a given author, the strongest related authors in the co-authorship network and recommending the conferences that his co-authors usually publish in. The first member of this family is based on the Weighted Semantic Connectivity Score—WSCS, which is accurate but quite costly to compute for large co-authorship networks. The second member of this family is based on a new score, called the Modified Weighted Semantic Connectivity Score—MWSCS, which is much faster to compute and as accurate as the WSCS. The third family includes the Cluster-WSCS-based and the Cluster-MWSCS-based conference recommendation techniques, which adopt conference clusters generated using a subgraph of the co-authorship network. The experiments indicate as the best performing conference recommendation technique the Cluster-WSCS-based technique. This is the second contribution of the article. Finally, the article includes experiments that use data extracted from the DBLP repository and a web-based application that enables users to interactively analyze and compare a set of conferences. © 2017, The Author(s).","Co-authorship networks; Conference comparison; Conference recommendation; Linked data; Recommender systems; Social network analysis","Collaborative filtering; Recommender systems; Semantics; Social networking (online); Co-authorship networks; Co-authorships; Conference comparison; Conference recommendation; Linked datum; Recommendation techniques; Similarity measure; Web-based applications; Data mining",2-s2.0-85015633539
"Iam-On N., Boongoen T.","Generating descriptive model for student dropout: a review of clustering approach",2017,"Human-centric Computing and Information Sciences",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007614225&doi=10.1186%2fs13673-016-0083-0&partnerID=40&md5=ad74de69c68776fc61666cb8b000d14d","The implementation of data mining is widely considered as a powerful instrument for acquiring new knowledge from a pile of historical data, which is normally left unstudied. This data driven methodology has proven effective to improve the quality of decision-making in several domains such as business, medical and complex engineering problems. Recently, educational data mining (EDM) has obtained a great deal of attention among educational researchers and computer scientists. In general, publications in the field of EDM focus on understanding student types and targeted marketing, using both descriptive and predictive models to maximize student retention. Inspired by previous attempts, this paper aims to establish the clustering approach as a practical guideline to explore student categories and characteristics, with the working example on a real dataset to illustrate analytical procedures and results. © 2017, The Author(s).","Clustering; Dropout; Educational data mining; Retention; Student performance",,2-s2.0-85007614225
"Abubacker N.F., Azman A., Doraisamy S., Murad M.A.A.","An integrated method of associative classification and neuro-fuzzy approach for effective mammographic classification",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962799344&doi=10.1007%2fs00521-016-2290-z&partnerID=40&md5=260e42dafddb0c66acf821e74e099be8","Computer-aided diagnosis has gained a significant attention in helping radiologists to improve the accuracy of mammographic detection and diagnostic decision. The aim of proposed research work is to build an efficient and accurate classifier for the classification of mammogram images using a hybrid method by incorporating Genetic Association Rule Miner (GARM) with the learning capability of neural network. A set of features is extracted that comprises of 34 features based on the second and third level of the wavelet decomposition with 13 features measured directly from the gray-level co-occurrence matrix. In order to eliminate the inappropriate features and to increase the efficiency mining process, a multivariate filter is used for feature selection. Based on the selected features, an association rule mining based on modified GARM is used to generate association rules. In the classification phase, the newly generated association rules are used as the input for the creation and training of an artificial neural network. Furthermore, an extended associative classifier using fuzzy feed-forward backpropagation neural network (ACFNN) is proposed as an effective classifier in the context of mammography. The proposed ACFNN performance is compared with associative classifier using feed-forward backpropagation neural network (ACNN). Based on the experimental results, the performance of the proposed ACFNN is improved significantly. Furthermore, it can be inferred that the mammogram classification is better by using ACFNN with accuracy of 95.1 % as compared to ACNN with 93.7 %. © 2016, The Natural Computing Applications Forum.","Association rule mining; Fuzzy feed-forward neural network; Genetic algorithm; Mammographic image classification","Association rules; Backpropagation; Computer aided diagnosis; Data mining; Filtration; Fuzzy inference; Genetic algorithms; Mammography; Neural networks; Wavelet decomposition; X ray screens; Associative classification; Associative classifiers; Detection and diagnostics; Feedforward backpropagation; Gray level co-occurrence matrix; Learning capabilities; Mammogram classifications; Neuro-fuzzy approach; Image classification",2-s2.0-84962799344
"Xu X., Wang X., Li Y., Haghighi M.","Business intelligence in online customer textual reviews: Understanding consumer perceptions and influential factors",2017,"International Journal of Information Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021808636&doi=10.1016%2fj.ijinfomgt.2017.06.004&partnerID=40&md5=a9e93eb77b039167ecf5fb8475ef4810","With the rapid development of information technology, customers not only shop online—they also post reviews on social media. This user-generated content (UGC) can be useful to understand customers’ shopping experiences and influence future customers’ purchase intentions. Therefore, business intelligence and analytics are increasingly being advocated as a way to analyze customers’ UGC in social media and support firms’ marketing activities. However, because of its open structure, UGC such as customer reviews can be difficult to analyze, and firms find it challenging to harness UGC. To fill this gap, this study aims to examine customer satisfaction and dissatisfaction toward attributes of hotel products and services based on online customer textual reviews. Using a text mining approach, latent semantic analysis (LSA), we identify the key attributes driving customer satisfaction and dissatisfaction toward hotel products and service attributes. Additionally, using a regression approach, we examine the effects of travel purposes, hotel types, star level, and editor recommendations on customers’ perceptions of attributes of hotel products and services. This study bridges customer online textual reviews with customers’ perceptions to help business managers better understand customers’ needs through UGC. © 2017 Elsevier Ltd","Customer dissatisfaction; Customer satisfaction; Online textual reviews; Regression; Text mining","Data mining; Hotels; Information analysis; Sales; Semantics; Social networking (online); Customer dissatisfaction; Influential factors; Latent Semantic Analysis; Marketing activities; Products and services; Regression; Text mining; User generated content (UGC); Customer satisfaction",2-s2.0-85021808636
"Li L.-S., Gan S.-J., Yin X.-D.","Feedback recurrent neural network-based embedded vector and its application in topic model",2017,"Eurasip Journal on Embedded Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978699229&doi=10.1186%2fs13639-016-0038-6&partnerID=40&md5=ba99389bc315cbc6a347232547daccc6","While mining topics in a document collection, in order to capture the relationships between words and further improve the effectiveness of discovered topics, this paper proposed a feedback recurrent neural network-based topic model. We represented each word as a one-hot vector and embedded each document into a low-dimensional vector space. During the process of document embedding, we applied the long short-term memory method to capture the backward relationships between words and proposed a feedback recurrent neural network to capture the forward relationships between words. In the topic model, we used the original and muted document pairs as positive samples and the original and random document pairs as negative samples to train the model. The experiments show that the proposed model consumes not only lower running time and memory but also has better effectiveness during topic analysis. © 2016, The Author(s).","Aggregation delay; Aggregation tree; Data aggregation; Wireless sensor networks","Data mining; Recurrent neural networks; Trees (mathematics); Vector spaces; Aggregation trees; Data aggregation; Document collection; ITS applications; Long short term memory; Low dimensional; Negative samples; Topic analysis; Wireless sensor networks",2-s2.0-84978699229
"Wang Y.-J., Ying B.-B., Shen W., Zheng R.-C., Zheng Y.-G.","Rational design of Kluyveromyces marxianus ZJB14056 aldo–keto reductase KmAKR to enhance diastereoselectivity and activity",2017,"Enzyme and Microbial Technology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026868439&doi=10.1016%2fj.enzmictec.2017.07.012&partnerID=40&md5=a5021857b78f937b2ca12ace912cb905","t−Butyl 6−cyano−(3R,5R)−dihydroxyhexanoate ((3R,5R)−1b) is a valuable chiral synthon of atorvastatin calcium. A novel NADPH-specific aldo−keto reductase (AKR) was identified from a thermotolerant yeast Kluyveromyces marxianus ZJB14056 by genome database mining, displaying t−butyl 6−cyano−(5R)−hydroxy−3−oxohexanoate ((5R)−1a) reducing activity and moderate diastereoselectivity (dep ∼80.5%). Molecular homology modeling and docking studies demonstrated that the side chain of Trp297 blocks binding of (5R)−1a to KmAKR. The mutation of Trp297 to His led to dramatic conformational changes and significant improvement in both diastereoselectivity and activity. In comparison with KmAKR, KmAKR−W297H displayed strict diastereoselectivity, and 2.8−fold, 3.9−fold improvement in kcat and kcat/Km toward (5R)−1a, which were 10.36 s−1 and 6.56 s−1·mM−1 respectively. Coupling KmAKR−W297H with Exiguobacterium sibiricum glucose dehydrogenase (EsGDH) for coenzyme regeneration, 100 mM (5R)−1a was completely reduced to (3R,5R)−1b within 12 h, in a dep &gt;99.5%. © 2017 Elsevier Inc.","Aldo−keto reductase; Kluyveromyces marxianus; Site−saturation mutagenesis; t−Butyl 6−cyano−(3R,5R)−dihydroxyhexanoate","Biochemistry; Biotechnology; Atorvastatin calciums; Coenzyme regeneration; Conformational change; Diastereo-selectivity; Glucose dehydrogenase; Kluyveromyces marxianus; Saturation mutagenesis; Thermotolerant yeasts; Stereoselectivity; aldo keto reductase; carbonyl derivative; edetic acid; glucose dehydrogenase; hexanoic acid derivative; metal ion; oxidoreductase; unclassified drug; amino acid sequence; Article; catalysis; cloning; coenzyme; comparative study; conformational transition; controlled study; data base; data mining; diastereoisomer; enzyme activity; enzyme binding; enzyme conformation; enzyme kinetics; enzyme specificity; Exiguobacterium; Exiguobacterium sibiricum; Kluyveromyces marxianus; molecular docking; nonhuman; pH; polyacrylamide gel electrophoresis; reduction (chemistry); screening; sequence alignment; sequence analysis; sequence homology; site directed mutagenesis; stereoselectivity",2-s2.0-85026868439
"Nourani V., Molajou A.","Application of a hybrid association rules/decision tree model for drought monitoring",2017,"Global and Planetary Change",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032507167&doi=10.1016%2fj.gloplacha.2017.10.008&partnerID=40&md5=80a4797cd399ed48a3f6f3a05f9db816","The previous researches have shown that the incorporation of the oceanic-atmospheric climate phenomena such as Sea Surface Temperature (SST) into hydro-climatic models could provide important predictive information about hydro-climatic variability. In this paper, the hybrid application of two data mining techniques (decision tree and association rules) was offered to discover affiliation between drought of Tabriz and Kermanshah synoptic stations (located in Iran) and de-trend SSTs of the Black, Mediterranean and Red Seas. Two major steps of the proposed model were the classification of de-trend SST data and selecting the most effective groups and extracting hidden information involved in the data. The techniques of decision tree which can identify the good traits from a data set for the classification purpose were used for classification and selecting the most effective groups and association rules were employed to extract the hidden predictive information from the large observed data. To examine the accuracy of the rules, confidence and Heidke Skill Score (HSS) measures were calculated and compared for different considering lag times. The computed measures confirm reliable performance of the proposed hybrid data mining method to forecast drought and the results show a relative correlation between the Mediterranean, Black and Red Sea de-trend SSTs and drought of Tabriz and Kermanshah synoptic stations so that the confidence between the monthly Standardized Precipitation Index (SPI) values and the de-trend SST of seas is higher than 70 and 80% respectively for Tabriz and Kermanshah synoptic stations. © 2017 Elsevier B.V.","Association rules; Data mining; Decision tree; Drought; Kermanshah; SST; Tabriz","Association rules; Classification (of information); Climate models; Decision trees; Drought; Oceanography; Surface waters; Trees (mathematics); Atmospheric climate; Climatic variability; Kermanshah; Predictive information; Reliable performance; Sea surface temperature (SST); Standardized precipitation index; Tabriz; Data mining",2-s2.0-85032507167
"Thompson J.J., Leung B.H., Blair M.R., Taboada M.","Sentiment analysis of player chat messaging in the video game StarCraft 2: Extending a lexicon-based model",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030559558&doi=10.1016%2fj.knosys.2017.09.022&partnerID=40&md5=709fad65b31766f79d94e342bd6c1dec","There is a growing need for automated tools which make predictions about the positivity or negativity of sentiment conveyed by text. Such tools have a number of important applications in game user research. They are useful for understanding users generally, as they may give Big Data researchers access to a new source of information about player learning environments. Sentiment analysis methods are also applicable to the detection of toxicity, and the identification of players or player messages that are a potential threat to the player experience. A major challenge in sentiment analysis, however, is developing portable models that can be applied to new domains with relatively little effort. In the present study we extend a lexicon-based sentiment extractor, SO-CAL, to the analysis of instant messages across 1000 games of StarCraft 2. We show that, with updates to dictionary entries that are tailored to the classification task at hand, SO-CAL constitutes a respectable classifier of sentiment and toxicity that is robust across differences in player region and league. We verify the performance of our toxicity detector against a sample of 2025 additional games. Our results support the proposal that lexicon-based sentiment extraction is a useful and portable method of sentiment analysis, and that it can be deployed to identify toxicity. © 2017 Elsevier B.V.","Instant messaging; Sentiment analysis; Toxicity; Video game chat","Computer aided instruction; Data mining; Human computer interaction; Toxicity; Classification tasks; Game user researches; Instant messaging; Learning environments; Player experience; Potential threats; Sentiment analysis; Video game; Big data",2-s2.0-85030559558
"Prusa J.D., Khoshgoftaar T.M.","Improving deep neural network design with new text data representations",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017014012&doi=10.1186%2fs40537-017-0065-8&partnerID=40&md5=fbe68c7ea07ed92aeb88d8923d636933","Using traditional machine learning approaches, there is no single feature engineering solution for all text mining and learning tasks. Thus, researchers must determine and implement the best feature engineering approach for each text classification task; however, deep learning allows us to skip this step by extracting and learning high-level features automatically from low-level text representations. Convolutional neural networks, a popular type of neural network for deep learning, have been shown to be effective at performing feature extraction and classification for many domains including text. Recently, it was demonstrated that convolutional neural networks can be used to train classifiers from character-level representations of text. This approach achieved superior performance compared to classifiers trained on word-level text representations, likely due to the use of character-level representations preserving more information from the data. Training neural networks from character level data requires a large volume of instances; however, the large volume of training data and model complexity makes training these networks a slow and computationally expensive task. In this paper, we propose a new method of creating character-level representations of text to reduce the computational costs associated with training a deep convolutional neural network. We demonstrate that our method of character embedding greatly reduces training time and memory use, while significantly improving classification performance. Additionally, we show that our proposed embedding can be used with padded convolutional layers to enable the use of current convolutional network architectures, while still facilitating faster training and higher performance than the previous approach for learning from character-level text. © 2017, The Author(s).","Big data; Character embedding; Convolutional neural networks; Deep learning; GPU computing; Sentiment; Text mining",,2-s2.0-85017014012
"Kulkarni A.R., Tokekar V., Kulkarni P.","Characterizing evolving behavior of context vectors for context based clustering",2017,"Evolving Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032575831&doi=10.1007%2fs12530-016-9171-7&partnerID=40&md5=de5f8b7b0a4c9b056f9352873be49f44","Characterizing evolving behavior of document vectors helps in identifying similarity between text documents. As document vectors contain terms and their importances in documents, discovering association and disassociation between terms is very important. This paper introduces characterization of evolving behavior of document vectors to identify similar and dissimilar segments in document vectors. This approach is particularly suitable where document vectors contain similar patterns of term occurrences but the patterns could be away from each other with regard to distance. The main objective of this paper is to capture evolving structure of context vector, document vector of contextually related terms, for discovering similarity between them. Context vector reduces the size of document vector from 6 to 12.57%. Evaluation is done by clustering the documents using Unweighted Pair Group Method with Arithmetic Mean with standard datasets. This results in formation of clusters with better entropy and purity. Mann–Whitney–Wilcoxon U test demonstrates statistically significant quality enhancement. © 2016, Springer-Verlag Berlin Heidelberg.","Behavioral patterns; Context based clustering; Text mining; UPGMA","Data mining; Natural language processing systems; Arithmetic mean; Behavioral patterns; Context-based; Document vectors; Quality enhancement; Term occurrences; Text mining; UPGMA; Vectors",2-s2.0-85032575831
"Chiba K., Kanazaki M., Shimada T.","Simple control of oxidizer flux for efficient extinction–reignition on a single-stage hybrid rocket",2017,"Aerospace Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030474598&doi=10.1016%2fj.ast.2017.09.017&partnerID=40&md5=447a0ddf93384f4a4aaef201e6eb4a6c","This study aims at revealing the effect of the simple control of oxidizer flux on a hybrid rocket engine for an efficient extinction–reignition sequence to extend the downrange as well as the duration in the lower thermosphere. The data mining result in the prior study achieved a hypothesis that the thrust control between 1st and 2nd combustions spurs improving extinction–reignition performance. Accordingly, this study simply defines two amounts of oxidizer flux for 1st and 2nd combustions; we investigate using the design informatics platform whether this simple control of oxidizer flux is practically effective to improving extinction–reignition performance. Consequently, oxidizer flux control successfully fulfills the downrange extension as well as the duration protraction as we had expected. Furthermore, the evolutionary multiobjective optimization generates non-simple structure of the feasible region in the design space. Trajectory analyses for signature individuals and a data mining have specified detailed design strategies. In addition, they have also explained physical reasons why the non-simple structure is formed. Extinction–reignition with oxidizer flux control is useful for efficient operations of the hybrid rocket system. © 2017 Elsevier Masson SAS","Design informatics; Extinction–reignition; Hybrid rocket engine; Oxidizer flux control; Single-stage sounding launch vehicle","Data mining; Engines; Ionosphere; Multiobjective optimization; Rocket engines; Flux control; Hybrid rocket engines; Informatics; Re-ignition; Single stage; Rockets",2-s2.0-85030474598
"Reagan A.J., Danforth C.M., Tivnan B., Williams J.R., Dodds P.S.","Sentiment analysis methods for understanding large-scale texts: a case for using continuum-scored words and word shift graphs",2017,"EPJ Data Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032565890&doi=10.1140%2fepjds%2fs13688-017-0121-9&partnerID=40&md5=510182caca37e3a50a60afd0b81bf9b2","The emergence and global adoption of social media has rendered possible the real-time estimation of population-scale sentiment, an extraordinary capacity which has profound implications for our understanding of human behavior. Given the growing assortment of sentiment-measuring instruments, it is imperative to understand which aspects of sentiment dictionaries contribute to both their classification accuracy and their ability to provide richer understanding of texts. Here, we perform detailed, quantitative tests and qualitative assessments of 6 dictionary-based methods applied to 4 different corpora, and briefly examine a further 20 methods. We show that while inappropriate for sentences, dictionary-based methods are generally robust in their classification accuracy for longer texts. Most importantly they can aid understanding of texts with reliable and meaningful word shift graphs if (1) the dictionary covers a sufficiently large portion of a given text’s lexicon when weighted by word usage frequency; and (2) words are scored on a continuous scale. © 2017, The Author(s).","data visualization; language; natural language processing; sentiment; sentiment analysis; sentiment dictionaries; text visualization","Behavioral research; Data mining; Data visualization; Scales (weighing instruments); Visualization; language; sentiment; Sentiment analysis; Sentiment dictionaries; Text visualization; Natural language processing systems",2-s2.0-85032565890
"Ma M., Wang P.","On the consistency of event processing: A semantic approach",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029852331&doi=10.1016%2fj.knosys.2017.08.021&partnerID=40&md5=577b9168cefe86feab33a5615318c04f","Event processing is one of the cornerstone technologies in bridging physical world and cyber system together. Although event-based processing system has been widely used in various applications, however, consistency of event processing is still an open issue need further exploration. The inconsistency problem produces inaccurate detection result and corrupts the system correctness. In this paper, we propose a semantic approach for event modeling and detection model transformation with a semantic calculus system. Particularly, we first propose a complex event semantic model, OntoEvent, and define several key operators and properties to describe the logic, temporal and attribute relations in complex events. Second, we propose the concept of event constraint and elaborate the occurrence, temporal, and attribute functions to formalize the semantic implications in OntoEvent model. On that basis, we present the extraction rules and establish a calculus mechanism for constraints based on axioms. With these works, an automata-based detection model, named OntoCEP (Ontology-based Complex Event Detection), and a pipelined procedure for the assembly from constraints to OntoCEP model is proposed. The procedure is composed of several sequential phases and the consistency in each assembly phase is proved. Therefore, we establish a semantic-consistent mapping mechanism from event to detection model in the form of constraints. Experiments and evaluations prove that our approach ensure the consistency with event and detection models. Besides, our detection model consumes less computational resources and outperforms other selected benchmarked models in terms of computational efficiency and processing capability. © 2017 Elsevier B.V.","Consistency; Data stream; Event processing; Internet of things; Ontology; Semantic","Calculations; Computation theory; Computational efficiency; Data mining; Internet of things; Ontology; Attribute functions; Complex event detection; Computational resources; Consistency; Data stream; Event Processing; Processing capability; Processing systems; Semantics",2-s2.0-85029852331
"Li Y., Li T., Liu H.","Recent advances in feature selection and its applications",2017,"Knowledge and Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018783981&doi=10.1007%2fs10115-017-1059-8&partnerID=40&md5=b93a63a04972d57cd3ee24ebad4241cc","Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval. © 2017, Springer-Verlag London.","Data mining; Feature selection; Survey",,2-s2.0-85018783981
"Peña-Araya V., Quezada M., Poblete B., Parra D.","Gaining historical and international relations insights from social media: spatio-temporal real-world news analysis using Twitter",2017,"EPJ Data Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030848172&doi=10.1140%2fepjds%2fs13688-017-0122-8&partnerID=40&md5=1f4449add81d40c2442569a4badcac1b","The immense growth of the social Web, which has made a large amount of user data easily and publicly available, has opened a whole new spectrum for research in social behavioral sciences. However, as the volume of social media content increases at a very fast rate, it becomes extremely difficult to systematically obtain high-level information from this data. As a consequence, tasks related to the analysis of historical news events based on social media data have not been explored, which limits any type of comparative historical research, causality analysis, and discovery of knowledge from patterns extracted from aggregated social media event information. In this work, we target this issue by proposing a compact high-level representation of news events using social media information. This representation explicitly includes temporal information about the event and information about locations, in particular of geopolitical entities. We call this a spatio-temporal context-aware event representation. Our hypothesis is that by including social, temporal, and spatial information in the event representation, we are enabling the analysis of historical world news from a social and geopolitical perspective. This facilitates, new information retrieval tasks related to historical event information extraction and international relations analysis. We support our claims by presenting two applications of this idea: the first, a visual tool, named Galean, for retrieval and exploration of historical news events within their geopolitical and temporal context. The second, a quantitative analysis of a 2-year Twitter dataset of news events reported by U.S. and U.K. media, which we explore using data mining techniques on our event representations. We present two case studies of event exploration using Galean and user evaluation of this tool, as well as details of our data mining empirical results. © 2017, The Author(s).","event modeling; geo-temporal context; historical analysis; visualization","Behavioral research; Flow visualization; Human computer interaction; Social networking (online); Event model; Event representations; geo-temporal context; High-level information; Historical analysis; International relations; Social media informations; Spatial informations; Data mining",2-s2.0-85030848172
"Sriwanna K., Boongoen T., Iam-On N.","Graph clustering-based discretization of splitting and merging methods (GraphS and GraphM)",2017,"Human-centric Computing and Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026905920&doi=10.1186%2fs13673-017-0103-8&partnerID=40&md5=bc35d840bcc12ca24b4ed9fa82637083","Discretization plays a major role as a data preprocessing technique used in machine learning and data mining. Recent studies have focused on multivariate discretization that considers relations among attributes. The general goal of this method is to obtain the discrete data, which preserves most of the semantics exhibited by original continuous data. However, many techniques generate the final discrete data that may be less useful with natural groups of data not being maintained. This paper presents a novel graph clustering-based discretization algorithm that encodes different similarity measures into a graph representation of the examined data. The intuition allows more refined data-wise relations to be obtained and used with the effective graph clustering technique based on normalized association to discover nature graphs accurately. The goodness of this approach is empirically demonstrated over 30 standard datasets and 20 imbalanced datasets, compared with 11 well-known discretization algorithms using 4 classifiers. The results suggest the new approach is able to preserve the natural groups and usually achieve the efficiency in terms of classifier performance, and the desired number of intervals than the comparative methods. © 2017, The Author(s).","Data mining; Graph clustering; Multivariate discretization; Normalized association; Normalized cuts",,2-s2.0-85026905920
"Subeesh V., Singh H., Maheswari E., Beulah E.","Novel adverse events of vortioxetine: A disproportionality analysis in USFDA adverse event reporting system database",2017,"Asian Journal of Psychiatry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029710875&doi=10.1016%2fj.ajp.2017.09.005&partnerID=40&md5=a420cbfd9269027cbbf49ec189744fac","Background Signal detection is one of the most advanced and emerging field in pharmacovigilance. It is a modern method of detecting new reaction (which can be desired or undesired) of a drug. It facilitates early adverse drug reaction detection which enables health professionals to identify adverse events that may not have been identified in pre-marketing clinical trials. Vortioxetine, the first mixed serotonergic antidepressant was initially approved by the US Food and Drug Administration (USFDA) on September 30, 2013 for the treatment of adults with Major Depressive Disorder (MDD). This study was to identify the signal strength for vortioxetine associated ADRs using data mining technique in USFDA Adverse Event Reporting System (AERS) database. Methodology Most commonly used three data mining algorithms, Reporting Odds Ratio (ROR), Proportional Reporting Ratio (PRR) and Information Component (IC) were selected for the study and they were applied retrospectively in USFDA AERS database from 2015Q1 to 2016Q3. A value of ROR-1.96SE > 1, PRR ≥ 2, IC- 2SD > 0 were considered as the positive signal. Result A study population of 61,22,000 were reported all over the world. Among which 3481 reactions were associated with vortioxetine which comprised of 632 unique events encompassed with 27 clinically relevant reactions. ROR, PRR and IC showed positive signal for weight loss, agitation, anger, ketoacidosis, insomnia and abnormal dreams. Conclusion The present study suggests that vortioxetine may result in these adverse events. Further pharmacoepidemiologic studies are necessary to confirm this conclusion and to improve the precision of the prevalence and/or the risk factors of this ADRs. © 2017 Elsevier B.V.","Signal detection; USFDA adverse event reporting system database; Vortioxetine","vortioxetine; abnormal dreaming; agitation; algorithm; anger; Article; data mining; drug safety; drug surveillance program; human; information component algorithm; insomnia; ketoacidosis; priority journal; proportional reporting ratio algorithm; reporting odds ratio algorithm; signal detection; weight reduction",2-s2.0-85029710875
"Banerjee A., Maji P.","Stomped-t: A novel probability distribution for rough-probabilistic clustering",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028731427&doi=10.1016%2fj.ins.2017.08.083&partnerID=40&md5=34e22296a19b20a456bd5ad2f6f8476d","Rough clustering is one of the principal research areas in data mining, machine learning, pattern recognition, and bioinformatics. Among different variants of rough clustering, rough-probabilistic clustering is a new concept introduced recently. In rough-probabilistic clustering, a class is defined as the union of two disjoint regions, namely, a crisp lower approximation region and a probabilistic boundary region. In this regard, stomped normal (SN) distribution provides a statistical modeling of the data set in rough-probabilistic clustering framework. The SN distribution models the central tendency, dispersion, and width of the lower approximation region of each class using its mean, variance, and width parameter, respectively. However, it does not take into consideration the property of kurtosis of the class distribution, which controls the concentration of values around mean and shape of the tail of data distribution. In this background, a novel probability distribution, named stomped-t (St) distribution, is introduced in the paper for rough-probabilistic clustering. The proposed probability distribution incorporates the property of kurtosis into the SN framework. The proposed St probability distribution is then integrated within the rough-probabilistic clustering framework for precise and robust clustering of the data. The efficacy of the proposed clustering algorithm is demonstrated for unsupervised data clustering and image segmentation problems, along with a comparative performance analysis with related algorithms. © 2017 Elsevier Inc.","Expectation-maximization algorithm; Hidden Markov random field model; Image segmentation; Rough-probabilistic clustering; Stomped normal distribution","Cluster analysis; Data mining; Higher order statistics; Image segmentation; Learning systems; Markov processes; Maximum principle; Normal distribution; Pattern recognition; Probability; Probability distributions; Class distributions; Comparative performance analysis; Distribution models; Expectation-maximization algorithms; Hidden Markov random field model; Lower approximation; Probabilistic clustering; Statistical modeling; Clustering algorithms",2-s2.0-85028731427
"Yang X.-H., Zhu Q.-P., Huang Y.-J., Xiao J., Wang L., Tong F.-C.","Parameter-free Laplacian centrality peaks clustering",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032454034&doi=10.1016%2fj.patrec.2017.10.025&partnerID=40&md5=f4a51911c0aaff694ed45ad2b13e68ec","As an important tool of data mining, clustering analysis can measure similarity between different data and classify them. It is widely applied in many fields such as pattern recognition, economics and biology. In this paper, we propose a new clustering algorithm. First, original unclassified dataset is converted into a weighted complete graph in which a node represents a data point and distance between two data points is used as weight of the edge between the corresponding two nodes. Second, local importance of each node in the network is calculated and evaluated by Laplacian centrality. The cluster center has higher Laplacian centrality than surrounding neighbor nodes and relatively large distance from nodes with higher Laplacian centralities. The new algorithm is a true parameter-free clustering method. It can automatically classify the dataset without any priori parameters. In this paper, the new algorithm was compared with 8 well-known clustering algorithms in 7 real datasets. Results show that the proposed algorithm has good clustering effect. © 2017","Laplacian centrality peaks clustering; Parameter-free; Weighted complete graph","Classification (of information); Data mining; Graph theory; Laplace transforms; Pattern recognition; Cluster centers; Clustering analysis; Clustering effect; Clustering methods; Complete graphs; Laplacians; New clustering algorithms; Parameter-free; Clustering algorithms",2-s2.0-85032454034
"Shehab M., Khader A.T., Al-Betar M.A.","A survey on applications and variants of the cuckoo search algorithm",2017,"Applied Soft Computing Journal",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021751588&doi=10.1016%2fj.asoc.2017.02.034&partnerID=40&md5=d27466e2d925fee299ba1915197d6c6b","This paper introduces a comprehensive and exhaustive overview of the cuckoo search algorithm (CSA). CSA is a metaheuristic swarm-based approach established by Yang and Deb [10] to emulate the cuckoo breeding behavior. Owing to the successful application of CSA for a wide variety of optimization problems, since then, researchers have developed several new algorithms in this field. This article displays a comprehensive review of all conducting intensive research survey into the pros and cons, main architecture, and extended versions of this algorithm. It is worth mentioning that the materials of this survey paper are categorized in accordance with the structure of the CSA in which the materials are divided into the CSA versions and modification, publication years, the CSA applications areas, and the hybridization of CSA. The survey paper ends with solid conclusions about the current research on CSA and the possible future directions for the relevant audience and readers. The researchers and practitioners on CSA belong to a wide range of audiences from the domains of optimization, engineering, medical, data mining, clustering, etc., who will benefit from this study. © 2017 Elsevier B.V.","Cuckoo search algorithm; Metaheuristic; Nature-inspired algorithms; Optimization; Swarm-based approach","Data mining; Learning algorithms; Medical computing; Surveys; Cuckoo search algorithms; Extended versions; Intensive research; Metaheuristic; Nature inspired algorithms; Optimization problems; Possible futures; Swarm-based approach; Optimization",2-s2.0-85021751588
"Uddin S., Choudhury N., Farhad S.M., Rahman M.T.","The optimal window size for analysing longitudinal networks",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031776173&doi=10.1038%2fs41598-017-13640-5&partnerID=40&md5=bdd8c347b789eea8f33040e14ae2f8e5","The time interval between two snapshots is referred to as the window size. A given longitudinal network can be analysed from various actor-level perspectives, such as exploring how actors change their degree centrality values or participation statistics over time. Determining the optimal window size for the analysis of a given longitudinal network from different actor-level perspectives is a well-researched network science problem. Many researchers have attempted to develop a solution to this problem by considering different approaches; however, to date, no comprehensive and well-acknowledged solution that can be applied to various longitudinal networks has been found. We propose a novel approach to this problem that involves determining the correct window size when a given longitudinal network is analysed from different actor-level perspectives. The approach is based on the concept of actor-level dynamicity, which captures variability in the structural behaviours of actors in a given longitudinal network. The approach is applied to four real-world, variable-sized longitudinal networks to determine their optimal window sizes. The optimal window length for each network, determined using the approach proposed in this paper, is further evaluated via time series and data mining methods to validate its optimality. Implications of this approach are discussed in this article. © 2017 The Author(s).",,"behavior; data mining; human; scientist; time series analysis",2-s2.0-85031776173
"Brückner A., Hilpert A., Heethoff M.","Biomarker function and nutritional stoichiometry of neutral lipid fatty acids and amino acids in oribatid mites",2017,"Soil Biology and Biochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031808481&doi=10.1016%2fj.soilbio.2017.07.020&partnerID=40&md5=4c99cbf103de5c326a94abe827f40c4a","Biomarkers (e.g. fatty acids, amino acids, stable isotopes, and molecular barcodes) have become increasingly important for investigating food web structure and nutrient flow in soil ecosystems. While the biomarker function of fatty acids has been investigated for some soil animal taxa (e.g. collembolans and nematodes), their role in soil-dwelling oribatid mites remained unknown. Here, we investigate the biomarker function and nutritional stoichiometry of neutral lipid fatty acids (NLFA) and amino acids in oribatid mites. We reared the opportunistic model oribatid mite species Archegozetes longisetosus on ten different resources of animal, bacterial, fungal and herbal origin. We analyzed the neutral lipid fatty acid and amino acid compositions of resources and consumers with gas chromatography/mass spectrometry (GC/MS) and ion-exchange chromatography (IEC), respectively. We found diet-dependent amounts and compositions of NLFA in the oribatid mites, but amino acids were stable and independent of diet. Consumer NLFA composition could be used as a reliable predictor of diet using data mining approaches (i.e., Random Forest), while amino acid profiles reflected diet-independent intrinsic physiological properties and confirm the homeostatic protein stoichiometry hypothesis for oribatid mites. © 2017 Elsevier Ltd","Biochemical ecology; Dietary routing; Lipids; Oribatida; Proteins; Trophic ecology","Amino acids; Animals; Biomarkers; Chemical analysis; Chromatographic analysis; Chromatography; Data mining; Decision trees; Ecology; Gas chromatography; Ion chromatography; Ion exchange; Lipids; Nutrition; Proteins; Soils; Stoichiometry; Amino acid compositions; Biochemical ecologies; Dietary routing; Gas chromatography/Mass spectrometry; Ion-exchange chromatography; Oribatida; Physiological properties; Protein stoichiometry; Fatty acids; amino acid; bacterium; biochemical composition; biomarker; fatty acid; food web; mite; physiological response; protein; soil ecosystem; Acari; Animalia; Archegozetes longisetosus; Bacteria (microorganisms); Nematoda; Oribatida",2-s2.0-85031808481
"Kytö M., McGookin D.","Augmenting Multi-Party Face-to-Face Interactions Amongst Strangers with User Generated Content",2017,"Computer Supported Cooperative Work: CSCW: An International Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019727576&doi=10.1007%2fs10606-017-9281-1&partnerID=40&md5=5ecf25aef61e62041244a13471c330c8","We present the results of an investigation into the role of curated representations of self, which we term Digital Selfs, in augmented multi-party face-to-face interactions. Advancements in wearable technologies (such as Head-Mounted Displays) have renewed interest in augmenting face-to-face interaction with digital content. However, existing work focuses on algorithmic matching between users, based on data-mining shared interests from individuals’ social media accounts, which can cause information that might be inappropriate or irrelevant to be disclosed to others. An alternative approach is to allow users to manually curate the digital augmentation they wish to present to others, allowing users to present those aspects of self that are most important to them and avoid undesired disclosure. Through interviews, video analysis, questionnaires and device logging, of 23 participants in 6 multi-party gatherings where individuals were allowed to freely mix, we identified how users created Digital Selfs from media largely outside existing social media accounts, and how Digital Selfs presented through HMDs were employed in multi-party interactions, playing key roles in facilitating strangers to interact with each other. We present guidance for the design of future multi-party digital augmentations in collaborative scenarios. © 2017, The Author(s).","Digital self; Face-to-face interaction; Familiarisation; Head-mounted display; Strangers","Data mining; Helmet mounted displays; Sensory perception; Social networking (online); Street traffic control; Wearable technology; Digital self; Face-to-face interaction; Familiarisation; Head mounted displays; Strangers; Surveys",2-s2.0-85019727576
"Poehlein A., Solano J.D.M., Flitsch S.K., Krabben P., Winzer K., Reid S.J., Jones D.T., Green E., Minton N.P., Daniel R., Dürre P.","Microbial solvent formation revisited by comparative genome analysis",2017,"Biotechnology for Biofuels",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021092774&doi=10.1186%2fs13068-017-0742-z&partnerID=40&md5=b39ec031187e527201dc0711a6d1dcc5","Background: Microbial formation of acetone, isopropanol, and butanol is largely restricted to bacteria belonging to the genus Clostridium. This ability has been industrially exploited over the last 100 years. The solvents are important feedstocks for the chemical and biofuel industry. However, biological synthesis suffers from high substrate costs and competition from chemical synthesis supported by the low price of crude oil. To render the biotechnological production economically viable again, improvements in microbial and fermentation performance are necessary. However, no comprehensive comparisons of respective species and strains used and their specific abilities exist today. Results: The genomes of a total 30 saccharolytic Clostridium strains, representative of the species Clostridium acetobutylicum, C. aurantibutyricum, C. beijerinckii, C. diolis, C. felsineum, C. pasteurianum, C. puniceum, C. roseum, C. saccharobutylicum, and C. saccharoperbutylacetonicum, have been determined; 10 of them completely, and compared to 14 published genomes of other solvent-forming clostridia. Two major groups could be differentiated and several misclassified species were detected. Conclusions: Our findings represent a comprehensive study of phylogeny and taxonomy of clostridial solvent producers that highlights differences in energy conservation mechanisms and substrate utilization between strains, and allow for the first time a direct comparison of sequentially selected industrial strains at the genetic level. Detailed data mining is now possible, supporting the identification of new engineering targets for improved solvent production. © 2017 The Author(s).","Acetone; Butanol; C. beijerinckii; C. saccharobutylicum; C. saccharoperbutylacetonicum; Clostridium acetobutylicum; Phylogeny; Solvents","Acetone; Biology; Butenes; Clostridium; Crude oil; Data mining; Genes; C. beijerinckii; C. saccharobutylicum; C. saccharoperbutylacetonicum; Clostridium acetobutylicum; Phylogeny; Solvents; acetone; bacterium; genome; microbial activity; phylogeny; solvent; Clostridia; Clostridium; Clostridium acetobutylicum; Clostridium aurantibutyricum; Clostridium beijerinckii; Clostridium diolis; Clostridium felsineum; Clostridium pasteurianum; Clostridium puniceum; Clostridium roseum; Clostridium saccharobutylicum; Clostridium saccharoperbutylacetonicum",2-s2.0-85021092774
"Lee G.R., Griffin A., Halton K., Fitzgibbon M.C.","Generating method-specific Reference Ranges – A harmonious outcome?",2017,"Practical Laboratory Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025481327&doi=10.1016%2fj.plabm.2017.06.001&partnerID=40&md5=635800d1aa602ecdfefd28b956addf8b","Objectives When laboratory Reference Ranges (RR) do not reflect analytical methodology, result interpretation can cause misclassification of patients and inappropriate management. This can be mitigated by determining and implementing method-specific RRs, which was the main objective of this study. Design and methods Serum was obtained from healthy volunteers (Male + Female, n > 120) attending hospital health-check sessions during June and July 2011. Pseudo-anonymised aliquots were stored (at − 70 °C) prior t° analysis on Abbott ARCHITECT c16000 chemistry and i2000SR immunoassay analysers. Data were stratified by gender where appropriate. Outliers were excluded statistically (Tukey method) to generate non-parametric RRs (2.5th + 97.5th percentiles). RRs were compared to those quoted by Abbott and UK Pathology Harmony (PH) where possible. For 7 selected tests, RRs were verified using a data mining approach. Results For chemistry tests (n = 23), Upper or Lower Reference Limits (LRL or URL) were > 20% different from Abbott ranges in 25% of tests (11% from PH ranges) but in 38% for immunoassay tests (n = 13). RRs (mmol/L) for sodium (138−144), potassium (3.8–4.9) and chloride (102−110) were considerably narrower than PH ranges (133–146, 3.5–5.0 and 95–108, respectively). The gender difference for ferritin (M: 29–441, F: 8–193 ng/mL) was more pronounced than reported by Abbott (M: 22–275, F: 5–204 ng/mL). Verification studies showed good agreement for chemistry tests (mean [SD] difference = 0.4% [1.2%]) but less so for immunoassay tests (27% [29%]), particularly for TSH (LRL). Conclusion Where resource permits, we advocate using method-specific RRs in preference to other sources, particularly where method bias and lack of standardisation limits RR transferability and harmonisation. © 2017 The Authors","Harmonisation; Method-specific; Reference Ranges","alanine aminotransferase; albumin; alkaline phosphatase; amylase; aspartate aminotransferase; bilirubin; C reactive protein; calcium; chloride; creatine kinase; creatinine; cyanocobalamin; ferritin; folic acid; follitropin; gamma glutamyltransferase; hydrocortisone; lactate dehydrogenase; magnesium; phosphate; potassium; sodium; thyroglobulin antibody; thyroid peroxidase antibody; thyrotropin; thyroxine; transferrin; unindexed drug; urea; uric acid; alanine aminotransferase blood level; albumin blood level; alkaline phosphatase blood level; amylase blood level; antibody blood level; Article; aspartate aminotransferase blood level; bilirubin blood level; blood sampling; calcium blood level; chloride blood level; creatine kinase blood level; creatinine blood level; cryopreservation; data mining; female; ferritin blood level; folic acid blood level; free liothyronine index; free thyroxine index; gamma glutamyl transferase blood level; good laboratory practice; human; hydrocortisone blood level; immunoassay analyzer; intermethod comparison; iron blood level; lactate dehydrogenase blood level; luteinizing hormone blood level; magnesium blood level; male; medical examination; method specific reference range; methodology; normal human; parathyroid hormone blood level; pH; phosphate blood level; potassium blood level; prolactin blood level; protein blood level; quality control procedures; reference value; sex difference; sodium blood level; thyrotropin blood level; urea blood level; uric acid blood level; vitamin blood level; volunteer",2-s2.0-85025481327
"Biasi R., Brunori E., Ferrara C., Salvati L.","Towards sustainable rural landscapes? a multivariate analysis of the structure of traditional tree cropping systems along a human pressure gradient in a mediterranean region",2017,"Agroforestry Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984804721&doi=10.1007%2fs10457-016-0006-0&partnerID=40&md5=6d8c221908f14e98d755fbdb38494160","The worldwide increase of human pressure in rural areas has resulted in a progressive fragmentation of agro-forest landscapes. Relatively few studies have identified processes of landscape modification and patch fragmentation reflecting long-term human activity from those caused by recent urbanization stimulated by economic development, population growth and improved accessibility of rural areas. In rural districts dominated by high-quality crop mosaics and exposed to increasing levels of human pressure, urbanization-driven changes in the use of land have determined complex and non-linear processes of landscape fragmentation which deserve further investigation. Multidimensional procedures and data mining are appropriate to explore such transformations, basing on a comprehensive assessment of landscape metrics. The present study investigates changes in land-use and landscape structure (2000–2008) in three districts of Latium (central Italy) featuring different cropping systems, physiographic attributes and level of human pressure. A principal component analysis was run to identify a core set of metrics aimed at (i) evaluating the role of traditional cropping systems in the preservation of traditional agro-forest systems and (ii) inferring the relationship between spatial heterogeneity in rural landscapes and socio-ecological processes of change. Our results indicate that crop intensification has contributed to landscape homogeneity and simplification in all areas studied, in contrast with the effects of urban expansion resulting in a more fragmented and diversified landscape with relict tree crop patches. The findings of this study clarify the contribution of traditional tree crop systems to sustainable structures and functions of rural landscapes by preserving the place-specific eco-mosaic complexity. © 2016, Springer Science+Business Media Dordrecht.","Environmental quality; Geographic Information System; Italy; Land-use/land cover change; Landscape metrics","data mining; environmental quality; fragmentation; GIS; human activity; land cover; land use change; landscape structure; Mediterranean environment; multivariate analysis; rural landscape; sustainability; urbanization; Italy",2-s2.0-84984804721
"Jing L., Zhao M., Li P., Xu X.","A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of gearbox",2017,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024383420&doi=10.1016%2fj.measurement.2017.07.017&partnerID=40&md5=9d6fd4e5a11801f7f65dc6dbe81395d2","Feature extraction plays a vital role in intelligent fault diagnosis of mechanical system. Nevertheless, traditional feature extraction methods suffer from three problems, which are (1) the requirements of domain expertise and prior knowledge, (2) the sensitive to the changes of mechanical system and (3) the limitations of mining new features. It is attractive and meaningful to investigate an automatic feature extraction method, which can adaptively learn features from raw data and discover new fault-sensitive features. Deep learning has been widely used in image analysis and speech recognition with great success. The key advantage of this method lies into the ability of mining representative information and sensitive features from raw data. However, the application of deep learning in feature leaning for mechanical diagnosis is still few, and limited studies have been carried out to compare the effectiveness of feature leaning with various data types. This paper will focus on developing a convolutional neural network (CNN) to learn features directly from frequency data of vibration signals and testing the different performance of feature learning from raw data, frequency spectrum and combined time-frequency data. Manual features from time domain, frequency domain and wavelet domain as well as three common intelligent methods are used as comparisons. The effectiveness of the proposed method is validated through PHM 2009 gearbox challenge data and a planetary gearbox test rig. The results demonstrate that the proposed method is able to learn features adaptively from frequency data and achieve higher diagnosis accuracy than other comparative methods. © 2017 Elsevier Ltd","Convolutional neural networks; Fault diagnosis; Feature learning; Gearbox","Condition monitoring; Convolution; Data mining; Education; Extraction; Failure analysis; Fault detection; Feature extraction; Gears; Mechanics; Neural networks; Speech recognition; Time domain analysis; Automatic feature extraction; Convolutional neural network; Fault diagnosis method; Feature extraction methods; Feature learning; Gearbox; Intelligent fault diagnosis; Planetary gearboxes; Deep learning",2-s2.0-85024383420
"Xue H., Zhao Z., Cai D.","Unifying the Video and Question Attentions for Open-Ended Video Question Answering",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028703999&doi=10.1109%2fTIP.2017.2746267&partnerID=40&md5=f8840babdbce0a8cf47fb0a307b8501a","Video question answering is an important task toward scene understanding and visual data retrieval. However, current visual question answering works mainly focus on a single static image, which is distinct from the dynamic and sequential visual data in the real world. Their approaches cannot utilize the temporal information in videos. In this paper, we introduce the task of free-form open-ended video question answering. The open-ended answers enable wider applications compared with the common multiple-choice tasks in Visual-QA. We first propose a data set for open-ended Video-QA with the automatic question generation approaches. Then, we propose our sequential video attention and temporal question attention models. These two models apply the attention mechanism on videos and questions, while preserving the sequential and temporal structures of the guides. The two models are integrated into the model of unified attention. After the video and the question are encoded, the answers are generated wordwisely from our models by a decoder. In the end, we evaluate our models on the proposed data set. The experimental results demonstrate the effectiveness of our proposed model. © 1992-2012 IEEE.","attention model; scene understanding; Video question answering","Coherent light; Data mining; Flow visualization; Mathematical models; Motion pictures; Adaptation models; Attention model; Hair; Natural languages; Question Answering; Scene understanding; Image processing",2-s2.0-85028703999
"Casey K., Azcona D.","Utilizing student activity patterns to predict performance",2017,"International Journal of Educational Technology in Higher Education",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012191679&doi=10.1186%2fs41239-017-0044-3&partnerID=40&md5=277ab974b6873b2cad70a609d8ba9ab0","Apart from being able to support the bulk of student activity in suitable disciplines such as computer programming, Web-based educational systems have the potential to yield valuable insights into student behavior. Through the use of educational analytics, we can dispense with preconceptions of how students consume and reuse course material. In this paper, we examine the speed at which students employ concepts which they are being taught during a semester. To show the wider utility of this data, we present a basic classification system for early detection of poor performers and show how it can be improved by including data on when students use a concept for the first time. Using our improved classifier, we can achieve an accuracy of 85% in predicting poor performers prior to the completion of the course. © 2017, The Author(s).","Data Mining; Early intervention; Learning Analytics; Student behavior; Virtual Learning Environments",,2-s2.0-85012191679
"Zheng C., Zhang J., Chen J., Chen C., Tian Y., Deng A., Song Z., Nawaz M.M., van Groenigen K.J., Zhang W.","Nighttime warming increases winter-sown wheat yield across major Chinese cropping regions",2017,"Field Crops Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760707&doi=10.1016%2fj.fcr.2017.09.014&partnerID=40&md5=407eaf3745db2342d1cf0b10eff8a891","Understanding the actual impacts of climatic warming on winter-sown wheat production will benefit cultivar breeding efforts and agronomic innovations and may help to improve food security. Therefore, we conducted a comprehensive study across the main Chinese winter wheat cropping regions, comprising field warming experiments at four locations and an analysis of 36 years of winter wheat yield data. In the field warming experiments, an increase of 1.0 °C in nighttime temperature enhanced wheat yield by 10.1% on average (P &lt; 0.05). Warming-induced enhancement of 1000-grain weight explained most of these yield increases. Warming shortened the length of pre-flowering phase by 5.4 days, while it prolonged the length of post-flowering phase by 3.8 days. Grain yield increases with warming were similar across experimental sites, even though warming-induced changes in the length of growth periods decreased with increasing ambient temperature. Our analysis of the historical data set was consistent with our field warming experiments; between 1980 and 2015, the major Chinese cropping regions experienced significant warming, especially in daily minimum temperature. Across the historical data set, daily minimum temperature was positively correlated with wheat yield (142.0 kg ha−1 °C −1). Our findings are inconsistent with previous reports of yield decreases with warming and may help to inform policy decisions and agronomic innovations of Chinese wheat production to better cope with future climate warming. © 2017 Elsevier B.V.","Daily minimum temperature; Data mining; Field warming experiment; Global warming; Wheat production","crop production; crop yield; data mining; experimental study; flowering; global warming; innovation; low temperature; wheat; winter; China; Triticum aestivum",2-s2.0-85031760707
"Fojo A.T., Musliner K.L., Zandi P.P., Zeger S.L.","A precision medicine approach for psychiatric disease based on repeated symptom scores",2017,"Journal of Psychiatric Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461968&doi=10.1016%2fj.jpsychires.2017.08.008&partnerID=40&md5=6b4eb9e7250c5d087704f406242c5a5b","For psychiatric diseases, rich information exists in the serial measurement of mental health symptom scores. We present a precision medicine framework for using the trajectories of multiple symptoms to make personalized predictions about future symptoms and related psychiatric events. Our approach fits a Bayesian hierarchical model that estimates a population-average trajectory for all symptoms and individual deviations from the average trajectory, then fits a second model that uses individual symptom trajectories to estimate the risk of experiencing an event. The fitted models are used to make clinically relevant predictions for new individuals. We demonstrate this approach on data from a study of antipsychotic therapy for schizophrenia, predicting future scores for positive, negative, and general symptoms, and the risk of treatment failure in 522 schizophrenic patients with observations over 8 weeks. While precision medicine has focused largely on genetic and molecular data, the complementary approach we present illustrates that innovative analytic methods for existing data can extend its reach more broadly. The systematic use of repeated measurements of psychiatric symptoms offers the promise of precision medicine in the field of mental health. © 2017 The Authors","Data mining; Mental health; Patient reported outcome measures; Precision medicine; Schizophrenia",,2-s2.0-85028461968
"Qin H., Wang C., Pan F., Lin Y., Xi X., Luo S.","Estimation of FPAR and FPAR profile for maize canopies using airborne LiDAR",2017,"Ecological Indicators",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026361248&doi=10.1016%2fj.ecolind.2017.07.044&partnerID=40&md5=0cbcf003210101d958ea5f904f185722","FPAR (fraction of photosynthetically active radiation) and FPAR profile (vertical FPAR distribution) are important parameters for characterizing the vegetation growth status and studying global climate change. Few studies have been carried out to estimate FPAR and FPAR profile using waveform LiDAR data. This research explored the potential of airborne small-footprint full-waveform LiDAR in the estimation of FPAR and FPAR profile of the maize canopy in Huailai County of Hebei Province, China. First, the maize growing area was identified by a simple decision tree model. Second, raw waveform data were processed to extract LiDAR-derived energy ratio and energy ratio profile. Third, FPAR and FPAR profile were estimated from LiDAR-derived metrics. Finally, we analyzed the FPAR and FPAR profile estimation results and assessed the model validity using the leave-one-out cross-validation (LOOCV) method. The comparative analyses found that the LiDAR-derived energy ratio profile and field-measured FPAR profile had the same trend and similar change rate for all maize layers. The accuracy assessments indicated that the FPAR and FPAR profile were estimated well by the LiDAR waveform data, with the high R2 (0.90 for the whole canopy, and 0.95, 0.90, 0.93, 0.92, and 0.97 for layers 1–5) and low RMSEs (0.042 for the whole canopy, and 0.033, 0.035, 0.039, 0.043, and 0.044 for layers 1–5). The spatial distribution map of FPAR was produced to describe the maize growth status of the whole study area, and the map showed that the FPAR distributed relatively uniformly. This study suggested that airborne small-footprint full-waveform LiDAR was useful in accurately measuring FPAR and FPAR profile of the maize canopy and in effectively mapping the maize FPAR spatial distribution. © 2017 Elsevier Ltd","Airborne LiDAR; FPAR; FPAR profile; Full-waveform; Maize","Climate change; Data mining; Decision trees; Spatial distribution; Statistical methods; Waveform analysis; Airborne LiDAR; FPAR; FPAR profile; Full-waveforms; Maize; Optical radar; airborne sensing; canopy; climate change; estimation method; global climate; growth; lidar; maize; model validation; numerical model; photosynthetically active radiation; spatial distribution; vegetation mapping; waveform analysis; China; Hebei; Huailai; Zea mays",2-s2.0-85026361248
"Ma W., Luo X., Xuan J., Xue R., Guo Y.","Discover semantic topics in patents within a specific domain",2017,"Journal of Web Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021679582&partnerID=40&md5=1834eada7249794f514d6b01344a5f8d","Patent topic discovery is critical for innovation-oriented enterprises to hedge the patent application risks and raise the success rate of patent application. Topic models are commonly recognized as an efficient tool for this task by researchers from both academy and industry. However, many existing well-known topic models, e.g., Latent Dirichlet Allocation (LDA), which are particularly designed for the documents represented by word-vectors, exhibit low accuracy and poor interpretability on patent topic discovery task. The reason is that 1) the semantics of documents are still under-explored in a specific domain 2) and the domain background knowledge is not successfully utilized to guide the process of topic discovery. In order to improve the accuracy and the interpretability, we propose a new patent representation and organization with additional inter-word relationships mined from title, abstract, and claim of patents. The representation can endow each patent with more semantics than word-vector. Meanwhile, we build a Backbone Association Link Network (Backbone ALN) to incorporate domain background semantics to further enhance the semantics of patents. With new semantic-rich patent representations, we propose a Semantic LDA model to discover semantic topics from patents within a specific domain. It can discover semantic topics with association relations between words rather than a single word vector. At last, accuracy and interpretability of the proposed model are verified on real-world patents datasets from the United States Patent and Trademark Office. The experimental results show that Semantic LDA model yields better performance than other conventional models (e.g., LDA). Furthermore, our proposed model can be easily generalized to other related text mining corpus. © Rinton Press.","Backbone association link network; Domain knowledge; Latent dirichlet allocation; Patent topic discovery","Abstracting; Data mining; Indexing (of information); Semantics; Statistics; Association link networks; Back-ground knowledge; Conventional models; Domain knowledge; Latent Dirichlet allocation; Latent dirichlet allocations; Topic Discovery; United States Patent and Trademark Office; Patents and inventions",2-s2.0-85021679582
"Qiu X., Zhang L., Nagaratnam Suganthan P., Amaratunga G.A.J.","Oblique random forest ensemble via Least Square Estimation for time series forecasting",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028356347&doi=10.1016%2fj.ins.2017.08.060&partnerID=40&md5=5902c839f36250b7a85cd54acb900c18","Recent studies in Machine Learning indicates that the classifiers most likely to be the bests are the random forests. As an ensemble classifier, random forest combines multiple decision trees to significant decrease the overall variances. Conventional random forest employs orthogonal decision tree which selects one “optimal” feature to split the data instances within a non-leaf node according to impurity criteria such as Gini impurity, information gain and so on. However, orthogonal decision tree may fail to capture the geometrical structure of the data samples. Motivated by this, we make the first attempt to study the oblique random forest in the context of time series forecasting. In each node of the decision tree, instead of the single “optimal” feature based orthogonal classification algorithms used by standard random forest, a least square classifier is employed to perform partition. The proposed method is advantageous with respect to both efficiency and accuracy. We empirically evaluate the proposed method on eight generic time series datasets and five electricity load demand time series datasets from the Australian Energy Market Operator and compare with several other benchmark methods. © 2017 Elsevier Inc.","Ensemble learning; Neural networks; Oblique random forest; Support vector regression; Time series forecasting","Data mining; Decision trees; Learning systems; Neural networks; Time series; Classification algorithm; Ensemble classifiers; Ensemble learning; Geometrical structure; Least square estimation; Random forests; Support vector regression (SVR); Time series forecasting; Forecasting",2-s2.0-85028356347
"Wang W., Jiang Y., Wang D., Zhang M.","Through wall human detection under small samples based on deep learning algorithm",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027512359&doi=10.1016%2fj.patcog.2017.07.033&partnerID=40&md5=562c70172495220c1d72178ddc8fd006","Through wall human detection has important applications in the fields of anti-terrorism, anti-explosion and post-disaster relief. The UWB radar-based technology of through-wall human target recognition has been fairly mature in nowadays research. With the rise of deep learning algorithm in recent years, the classification algorithms based on deep learning have demonstrated a strong ability to learn the essential characteristics of dataset from a few sample sets. This paper focuses on studying the classification and identification of behind-wall human target states under small sample conditions. The autoencoder algorithm in the deep learning network model is chosen herein to classify and identify the human targets behind the walls. The autoencoder algorithm extracts the concise data feature representations through automatic learning of inherent characteristics in the data. On the basis of the autoencoder network, we add the denoising encoder and sparsity constraints to extract more efficient feature representations, thereby improving the classification and identification rates. In this paper, we classify and identify the behind-wall human target states separately under single and multiple sensors under a small sample condition; and compare the results with other classification algorithms. The results show that the autoencoder algorithm in deep learning adopted herein allows more effective classification and identification of behind-wall human targets than other algorithms; and that the identification effect with multiple sensors is better than that with a single sensor. © 2017 Elsevier Ltd","Autoencoder; Deep learning; Small sample; Target identification; Ultra-wideband radar","Classification (of information); Data mining; Deep learning; Disaster prevention; Distributed computer systems; Fisher information matrix; Learning systems; Radar; Radar target recognition; Terrorism; Ultra-wideband (UWB); Auto encoders; Classification algorithm; Classification and identifications; Essential characteristic; Inherent characteristics; Small samples; Target identification; Ultra wideband radars; Learning algorithms",2-s2.0-85027512359
"Wang W., Zhang M., Wang D., Jiang Y.","Kernel PCA feature extraction and the SVM classification algorithm for multiple-status, through-wall, human being detection",2017,"Eurasip Journal on Wireless Communications and Networking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029100077&doi=10.1186%2fs13638-017-0931-2&partnerID=40&md5=c8228f3cd3e7c8cfa51e0c7013fae537","Ultra-wideband (UWB) radar with strong anti-jamming performance and high-range resolution can be used to separate multiple human targets in a complex environment. In recent years, through-wall human being detection with UWB radar has become relatively sophisticated. In this paper, the method of kernel principal component analysis (KPCA) feature extraction and the support vector machine (SVM) classification algorithm are applied to identify and classify the multiple statuses of through-wall human being detection. This method makes full use of the KPCA of powerful, nonlinear feature extraction and SVMs, which can solve the problem of multiple-status detection and nonlinear pattern recognition. The experimental data that come from KPCA feature extraction are used as input to the SVM classification algorithm, some of which are used to train the model and the others to test the model. Experimental results showed that KPCA feature extraction and the SVM classification algorithm effectively distinguished four statuses of through-wall human being detection and achieved the desired results. © 2017, The Author(s).","Classification; Feature extraction; Kernel principal component analysis; Support vector machines","Classification (of information); Data mining; Extraction; Feature extraction; Pattern recognition; Radar; Support vector machines; Tracking radar; Ultra-wideband (UWB); Anti jamming performance; Classification algorithm; Complex environments; High range resolution; Human being detection; Kernel principal component analyses (KPCA); Nonlinear feature extraction; Ultra wideband radars; Principal component analysis",2-s2.0-85029100077
"Ghasemi F., Kalatpour O., Moghimbeigi A., Mohammadfam I.","Selecting strategies to reduce high-risk unsafe work behaviors using the safety behavior sampling technique and bayesian network analysis",2017,"Journal of Research in Health Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018734699&partnerID=40&md5=d25918c90a539381793d64711911885c","Background: High-risk unsafe behaviors (HRUBs) have been known as the main cause of occupational accidents. Considering the financial and societal costs of accidents and the limitations of available resources, there is an urgent need for managing unsafe behaviors at workplaces. The aim of the present study was to find strategies for decreasing the rate of HRUBs using an integrated approach of safety behavior sampling technique and Bayesian networks analysis. Study design: A cross-sectional study. Methods: The Bayesian network was constructed using a focus group approach. The required data was collected using the safety behavior sampling, and the parameters of the network were estimated using Expectation-Maximization algorithm. Using sensitivity analysis and belief updating, it was determined that which factors had the highest influences on unsafe behavior. Results: Based on BN analyses, safety training was the most important factor influencing employees' behavior at the workplace. High quality safety training courses can reduce the rate of HRUBs about 10%. Moreover, the rate of HRUBs increased by decreasing the age of employees. The rate of HRUBs was higher in the afternoon and last days of a week. Conclusions: Among the investigated variables, training was the most important factor affecting safety behavior of employees. By holding high quality safety training courses, companies would be able to reduce the rate of HRUBs significantly. © 2017, Health Hamadan University of Medical Sciences. All rights reserved.","Accident Prevention; Behavior; Construction Industry; Data Mining; Occupational Injuries; Safety Management","adult; algorithm; Article; Bayes theorem; behavior; controlled study; cross-sectional study; employee; high risk patient; high risk unsafe behavior; human; information processing; Iran; occupational accident; occupational safety; risk reduction; sensitivity analysis; social acceptance; workplace; age; Bayes theorem; health education; high risk behavior; occupational exposure; safety; workplace; Adult; Age Factors; Algorithms; Bayes Theorem; Focus Groups; Health Education; Humans; Occupational Exposure; Risk-Taking; Safety; Workplace",2-s2.0-85018734699
"Aziz R., Verma C.K., Srivastava N.","A novel approach for dimension reduction of microarray",2017,"Computational Biology and Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032512589&doi=10.1016%2fj.compbiolchem.2017.10.009&partnerID=40&md5=a607c3d4964f92c1dfdcb54202e52645","This paper proposes a new hybrid search technique for feature (gene) selection (FS) using Independent component analysis (ICA) and Artificial Bee Colony (ABC) called ICA + ABC, to select informative genes based on a Naïve Bayes (NB) algorithm. An important trait of this technique is the optimization of ICA feature vector using ABC. ICA + ABC is a hybrid search algorithm that combines the benefits of extraction approach, to reduce the size of data and wrapper approach, to optimize the reduced feature vectors. This hybrid search technique is facilitated by evaluating the performance of ICA + ABC on six standard gene expression datasets of classification. Extensive experiments were conducted to compare the performance of ICA + ABC with the results obtained from recently published Minimum Redundancy Maximum Relevance (mRMR) +ABC algorithm for NB classifier. Also to check the performance that how ICA + ABC works as feature selection with NB classifier, compared the combination of ICA with popular filter techniques and with other similar bio inspired algorithm such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). The result shows that ICA + ABC has a significant ability to generate small subsets of genes from the ICA feature vector, that significantly improve the classification accuracy of NB classifier compared to other previously suggested methods. © 2017 Elsevier Ltd","Artificial bee colony (ABC); Cancer classification; Feature selection (FS); Independent component analysis (ICA); Naïve bayes (NB)","Barium compounds; Data mining; Evolutionary algorithms; Feature extraction; Gene expression; Genes; Genetic algorithms; Independent component analysis; Information analysis; Optimization; Particle swarm optimization (PSO); Sodium; Sodium compounds; Artificial bee colonies (ABC); Bio-inspired algorithms; Cancer classification; Classification accuracy; Gene expression datasets; Hybrid search algorithms; Independent component analysis(ICA); Minimum redundancy-maximum relevances; Classification (of information)",2-s2.0-85032512589
"Goumatianos N., Christou I.T., Lindgren P., Prasad R.","An algorithmic framework for frequent intraday pattern recognition and exploitation in forex market",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018823471&doi=10.1007%2fs10115-017-1052-2&partnerID=40&md5=ca8d3065ec40ba2d450fb38775bb6187","We present a knowledge discovery-based framework that is capable of discovering, analyzing and exploiting new intraday price patterns in forex markets, beyond the well-known chart formations of technical analysis. We present a novel pattern recognition algorithm for Pattern Matching, that we successfully used to construct more than 16,000 new intraday price patterns. After processing and analysis, we extracted 3518 chart formations that are capable of predicting the short-term direction of prices. In our experiments, we used forex time series from 8 paired-currencies in various time frames. The system computes the probabilities of events such as “within next 5 periods, price will increase more than 20 pips”. Results show that the system is capable of finding patterns whose output signals (tested on unseen data) have predictive accuracy which varies between 60 and 85% depending on the type of pattern. We test the usefulness of the discovered patterns, via implementation of an expert system using a straightforward strategy based on the direction and the accuracy of the pattern predictions. We compare our method against three standard trading techniques plus a “random trader,” and we also test against the results presented in two recently published studies. Our framework performs very well against all systems we directly compare , and also, against all other published results. © 2017, Springer-Verlag London.","Data mining; Forex; Hidden intraday patterns; Pattern recognition; Template grid method",,2-s2.0-85018823471
"Kumar K.M., Reddy A.R.M.","An efficient k-means clustering filtering algorithm using density based initial cluster centers",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027859827&doi=10.1016%2fj.ins.2017.07.036&partnerID=40&md5=4d3cb60b85d439bcd9992fce48b66bea","k-means is a preeminent partitional based clustering method that finds k clusters from the given dataset by computing distances from each point to k cluster centers iteratively. The filtering algorithm improves the performance of k-means by imposing an index structure on the dataset and reduces the number of cluster centers searched while finding the nearest center of a point. The performance of filtering algorithm is influenced by the degree of separation between initial cluster centers. In this paper, we propose an efficient initial seed selection method, RDBI, to improve the performance of k-means filtering method by locating the seed points at dense areas of the dataset and well separated. The dense areas are identified by representing the data points in a kd-tree. A comprehensive experimental analysis is performed to evaluate the performance efficiency of proposed method against state-of-the-art initialization methods and shown that the proposed method is efficient in terms of both running time and clustering accuracy. © 2017 Elsevier Inc.","Initial cluster centers; k-means clustering; kd-tree; Knowledge discovery","Cluster computing; Data mining; Iterative methods; Signal filtering and prediction; Trees (mathematics); Clustering accuracy; Degree of separation; Experimental analysis; Initial cluster centers; Initialization methods; K-d tree; K-means clustering; Performance efficiency; Clustering algorithms",2-s2.0-85027859827
"Masoodi T.A., Banaganapalli B., Vaidyanathan V., Talluri V.R., Shaik N.A.","Computational Analysis of Breast Cancer GWAS Loci Identifies the Putative Deleterious Effect of STXBP4 and ZNF404 Gene Variants",2017,"Journal of Cellular Biochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031818207&doi=10.1002%2fjcb.26080&partnerID=40&md5=d99fe9c5130bdc2bf226e35a28282bc0","The genome-wide association studies (GWAS) have enabled us in identifying different breast cancer (BC) susceptibility loci. However, majority of these are non-coding variants with no annotated biological function. We investigated such 78 noncoding genome wide associated SNPs of BC and further expanded the list to 2,162 variants with strong linkage-disequilibrium (LD, r2 ≥0.8). Using multiple publically available algorithms such as CADD, GWAVA, and FATHAMM, we classified all these variants into deleterious, damaging, or benign categories. Out of total 2,241 variants, 23 (1.02%) variants were extreme deleterious (rank 1), 70 (3.12%) variants were deleterious (rank 2), and 1,937 (86.43%) variants were benign (rank 3). The results show 14% of lead or associated variants are under strong negative selection (GERP++ RS ≥2), and ∼22% are under balancing selection (Tajima's D score &gt;2) in CEU population of 1KGP—the regions being positively selected (GERP++ RS &lt;0) in mammalian evolution. The expression quantitative trait loci of highest deleteriously ranked genes were tested on relevant adipose and breast tissues, the results of which were extended for protein expression on breast tissues. From the concordance analysis of ranking system of GWAVA, CADD, and FATHMM, eQTL and protein expression, we identified the deleterious SNPs localized in STXBP4 and ZNF404 genes which might play a role in BC development by dysregulating its gene expression. This simple approach will be easier to implement and to prioritize large scale GWAS data for variety of diseases and link to the potentially unrecognized functional roles of genes. J. Cell. Biochem. 118: 4296–4307, 2017. © 2017 Wiley Periodicals, Inc. © 2017 Wiley Periodicals, Inc.","BREAST CANCER; CADD; COMPUTATIONAL ANALYSIS; eQTL; FATHMM; GWAS","protein MDMX; ANKLE1 gene; Article; balancing selection; breast cancer; breast tissue; CTSW gene; data mining; gene; gene deletion; gene expression; gene linkage disequilibrium; gene locus; genetic association; genome-wide association study; human; human tissue; MAP1LC3A gene; priority journal; protein expression; quantitative trait locus; single nucleotide polymorphism; SNX32 gene; STXBP4 gene; ZNF404 gene",2-s2.0-85031818207
"Iliou C., Kalpakis G., Tsikrika T., Vrochidis S., Kompatsiaris I.","Hybrid focused crawling on the Surface and the Dark Web",2017,"Eurasip Journal on Information Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021724304&doi=10.1186%2fs13635-017-0064-5&partnerID=40&md5=3b44d0ed344e3b8adc4b79ea689ec38f","Focused crawlers enable the automatic discovery of Web resources about a given topic by automatically navigating through the Web link structure and selecting the hyperlinks to follow by estimating their relevance to the topic of interest. This work proposes a generic focused crawling framework for discovering resources on any given topic that reside on the Surface or the Dark Web. The proposed crawler is able to seamlessly navigate through the Surface Web and several darknets present in the Dark Web (i.e., Tor, I2P, and Freenet) during a single crawl by automatically adapting its crawling behavior and its classifier-guided hyperlink selection strategy based on the destination network type and the strength of the local evidence present in the vicinity of a hyperlink. It investigates 11 hyperlink selection methods, among which a novel strategy proposed based on the dynamic linear combination of a link-based and a parent Web page classifier. This hybrid focused crawler is demonstrated for the discovery of Web resources containing recipes for producing homemade explosives. The evaluation experiments indicate the effectiveness of the proposed focused crawler both for the Surface and the Dark Web. © 2017, The Author(s).","Dark web; Darknets; Dynamic linear combination; Focused crawling; Freenet; I2P; Tor","Data mining; Hypertext systems; Network security; Websites; Dark web; Darknets; Dynamic linear; Focused crawling; Freenets; Web crawler",2-s2.0-85021724304
"Santos L.P.D., Caon T., Battisti M.A., Silva C.H.B.D., Simões C.M.O., Reginatto F.H., de Campos A.M.","Antioxidant polymeric nanoparticles containing standardized extract of Ilex paraguariensis A. St.-Hil. for topical use",2017,"Industrial Crops and Products",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026788050&doi=10.1016%2fj.indcrop.2017.07.035&partnerID=40&md5=45480b049d4397cf964b143c8cb20e25","Although various compounds from Ilex paraguariensis present high antioxidant activity, limitations in terms of chemical stability may be found, impacting on the formulation step. In this context, nanotechnology may be alternatively used to provide an additional protection for these compounds, avoiding degradative reactions. Different formulation parameters (eg. particle size, zeta potential, internal phase fraction) were assayed to maximize a local antioxidant action. Firstly, the extraction procedure for obtaining increased concentrations of antioxidant compounds was optimized. High stirring speed and ethanol as extractor liquid were defined as key parameters. Nanoparticles were prepared by double emulsion solvent evaporation technique. Polycaprolactone-based nanoparticles showed to be more stable than poly(lactic-co-glycolic acid)-based nanoparticles. Instead of poly(vinyl alcohol), Pluronic® was selected as surfactant because it avoids aggregate formation and provides smaller particle size. While higher polymer content contributes positively to maintaining both reduced size particle and distribution, an increased extract concentration impacted negatively on these parameters. Stability studies were also performed using skin extracts and homogenates and confirmed that nanostructures provide an additional protection. Moreover, the incorporation of Ilex extract in nanoparticles significantly reduced the amount of chlorogenic acid permeated through the skin (no ex vivo permeation until 12 h for this system), extending its topical antioxidant effect. © 2017","Chemical stability; Ilex paraguariensis; Polymer nanoparticles; Topical delivery","Antioxidants; Chemical compounds; Chemical stability; Emulsification; Nanoparticles; Nanostructures; Particle size; Polymers; Polyvinyl alcohols; Anti-oxidant activities; Antioxidant compounds; Double emulsion-solvent evaporation; Ilex paraguariensis; Poly lactic-co-glycolic acid; Polymer nanoparticles; Polymeric nanoparticles; Topical delivery; Data mining; antioxidant; ethanol; nanoparticle; nanotechnology; particle size; plant extract; polymer; shrub; solvent; surfactant; Ilex; Ilex paraguariensis",2-s2.0-85026788050
"Gupta B.B., Tewari A., Jain A.K., Agrawal D.P.","Fighting against phishing attacks: state of the art and future challenges",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961211997&doi=10.1007%2fs00521-016-2275-y&partnerID=40&md5=c9be65bfd15ee4b02b0dce84b3df9616","In the last few years, phishing scams have rapidly grown posing huge threat to global Internet security. Today, phishing attack is one of the most common and serious threats over Internet where cyber attackers try to steal user’s personal or financial credentials by using either malwares or social engineering. Detection of phishing attacks with high accuracy has always been an issue of great interest. Recent developments in phishing detection techniques have led to various new techniques, specially designed for phishing detection where accuracy is extremely important. Phishing problem is widely present as there are several ways to carry out such an attack, which implies that one solution is not adequate to address it. Two main issues are addressed in our paper. First, we discuss in detail phishing attacks, history of phishing attacks and motivation of attacker behind performing this attack. In addition, we also provide taxonomy of various types of phishing attacks. Second, we provide taxonomy of various solutions proposed in the literature to detect and defend from phishing attacks. In addition, we also discuss various issues and challenges faced in dealing with phishing attacks and spear phishing and how phishing is now targeting the emerging domain of IoT. We discuss various tools and datasets that are used by the researchers for the evaluation of their approaches. This provides better understanding of the problem, current solution space and future research scope to efficiently deal with such attacks. © 2016, The Natural Computing Applications Forum.","Bag-of-word; Data mining; Key logger; Machine learning; Malware; Phishing; Social engineering; Soft computing; Spam; Visual similarity","Artificial intelligence; Data mining; Internet; Learning systems; Malware; Soft computing; Taxonomies; Bag of words; Phishing; Social engineering; Spam; Visual similarity; Computer crime",2-s2.0-84961211997
"Hale B.J.","“+1 for Imgur”: A content analysis of SIDE theory and common voice effects on a hierarchical bidirectionally-voted commenting system",2017,"Computers in Human Behavior",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029007165&doi=10.1016%2fj.chb.2017.09.003&partnerID=40&md5=5ef633cf0cd8be136f9a7af2c8afdef8","This research is among the first to attempt quantitative analysis of a site-bound digital subculture by identifying commenting regularities, using Imgur as a case study. A theoretical framework using SIDE theory and common voice informs a content analysis of 105 Imgur posts and 4404 comments in which the Imgur subculture is identified. Building on initial qualitative work, quantitative evidence is found for common voice, elucidating the Imgur subculture through identification of strategic commenting strategies. Findings indicate that post content affects commenting strategies, as the prevalence of common voice features varied according to post categorization. Additionally, post categories elicited different levels of common voice, with the community identification and humor categories eliciting the greatest concentration of common voice responses and information/mobilization and social support eliciting the least. Findings also suggest a preference for comments that approve of post content, as approving comments received a higher score and more responses than disapproving comments, while disapproving comments received more reprimanding responses. Results provide evidence that empirical study of site-bound digital subgroups is possible, informing future digital media research by providing a theoretical and methodological framework for examining digital subcultures. © 2017 Elsevier Ltd","Common voice; Content analysis; Imgur; SIDE; Social media; Subculture","Data mining; Digital storage; Content analysis; Imgur; SIDE; Social media; Subculture; Speech recognition; classification; conceptual framework; content analysis; empiricism; human; humor; prevalence; quantitative analysis; social media; social support; theoretical study; voice",2-s2.0-85029007165
"D’Addio R.M., Domingues M.A., Manzato M.G.","Exploiting feature extraction techniques on users’ reviews for movies recommendation",2017,"Journal of the Brazilian Computer Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020289943&doi=10.1186%2fs13173-017-0057-8&partnerID=40&md5=9e4985d342c6884d73a7dcba9a2381fc","Recommender systems help users to deal with the information overload problem by producing personalized content according to their interests. Beyond the traditional recommender strategies, there is a growing effort to incorporate users’ reviews into the recommendation process, since they provide a rich set of information regarding both items’ features and users’ preferences. This article proposes a recommender system that uses users’ reviews to produce items’ representations that are based on the overall sentiment toward the items’ features. We focus on exploiting the impact that different feature extraction techniques, allied with sentiment analysis, cause in an item attribute-aware neighborhood-based recommender algorithm. We compare four techniques of different granularities (terms and aspects) in two recommendation scenarios (rating prediction and item recommendation) and elect the most promising technique. We also compare our techniques with traditional structured metadata constructions, which are used as the baseline in our experimental evaluation. The results show that the techniques based on terms provide better results, since they produce a larger set of features, hence detailing better the items. © 2017, The Author(s).","Feature extraction; Item representation; Recommender systems; Sentiment analysis","Data mining; Extraction; Recommender systems; Different granularities; Experimental evaluation; Feature extraction techniques; Information overloads; Item representation; Personalized content; Recommender algorithms; Sentiment analysis; Feature extraction",2-s2.0-85020289943
"de Souza R.O., Alves G.D.A.D., Forte A.L.S.A., Marquele-Oliveira F., da Silva D.F., Rogez H., Fonseca M.J.V.","Byrsonima crassifolia extract and fraction prevent UVB-induced oxidative stress in keratinocytes culture and increase antioxidant activity on skin",2017,"Industrial Crops and Products",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022327620&doi=10.1016%2fj.indcrop.2017.07.015&partnerID=40&md5=aeda82f5d390414b1f9fa99b101fc2b8","Solar ultraviolet radiation exposure, particularly UVB rays (280–320 nm), can lead to skin lesions, photocarcinogenesis and acceleration of skin photoaging since UVB radiation may reach both the epidermal and dermal layers of the skin. Treatments that can ameliorate UVB-induced skin damage include natural extracts, which can act as skin photochemoprotective agents. Byrsonima crassifolia is widely used in the folk medicine. Previously studies have shown high antioxidant activity of BC leaves extracts. In this study we described the first photochemoprotective potential of Byrsonima crassifolia extract (BCP) and fraction (BCF) against UVB-induced damage in keratinocytes and the ability of topical formulations with BCP or BCF to increase the antioxidant activity in pig ear skin. The results of characterization of BCP and BCF indicate that the phenolic content was increased two-fold after an enrichment process for obtaining BCF. Despite differences in the phenolic content, both BCP and BCF exhibited similar IC50 values for lipid peroxidation and the DPPH[rad] method during the antioxidant activity study. However, for the chemiluminescence assay using the xanthine/luminol/XOD, BCF exhibited higher antioxidant activity than BCP. The different phenolic content in BCP and BCF did not influence their photochemoprotective activity in HaCaT cells, and both samples exhibited similar levels of protection. After treatment with BCP and BCF (1.2–5 μg/mL) and UVB irradiation exposure, the effect of lipid peroxidation in vitro was maintained in cell culture, and both IL-6 and TNF-α secretion and NF-κB activation were suppressed. After the development of the different formulations, BCP and BCF increased the antioxidant activity on skin and the formulation containing BCF showed higher skin retention, especially for (+) catechin, which was able to pass through the stratum corneum. Based on these findings, BCF could be topically applied to prevent/treat the damage induced by UVB radiation in the skin. © 2017 Elsevier B.V.","Catechin; Cytokine; Cytotoxicity; Franz cell; Photochemoprotection; Topical formulation","Antioxidants; Cell culture; Chemiluminescence; Cytotoxicity; Dermatology; Flavonoids; Irradiation; Lipids; Organic compounds; Oxidation; Oxidative stress; Phenols; Catechin; Cytokines; Franz cell; Photochemoprotection; Topical formulations; Data mining; antioxidant; carcinogen; cells and cell components; induced response; irradiation; lesion; phenolic compound; photochemistry; pig; plant extract; shrub; skin; ultraviolet B radiation; Byrsonima crassifolia; Suidae",2-s2.0-85022327620
"Borgwardt S., Brieden A., Gritzmann P.","An LP-based k-means algorithm for balancing weighted point sets",2017,"European Journal of Operational Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019867710&doi=10.1016%2fj.ejor.2017.04.054&partnerID=40&md5=ee223e91101f85b0d68faa8ea628d058","The classical k-means algorithm for partitioning n points in Rd into k clusters is one of the most popular and widely spread clustering methods. The need to respect prescribed lower bounds on the cluster sizes has been observed in many scientific and business applications. In this paper, we present and analyze a generalization of k-means that is capable of handling weighted point sets and prescribed lower and upper bounds on the cluster sizes. We call it weight-balanced k-means. The key difference to existing models lies in the ability to handle the combination of weighted point sets with prescribed bounds on the cluster sizes. This imposes the need to perform partial membership clustering, and leads to significant differences. For example, while finite termination of all k-means variants for unweighted point sets is a simple consequence of the existence of only finitely many partitions of a given set of points, the situation is more involved for weighted point sets, as there are infinitely many partial membership clusterings. Using polyhedral theory, we show that the number of iterations of weight-balanced k-means is bounded above by nO(dk), so in particular it is polynomial for fixed k and d. This is similar to the known worst-case upper bound for classical k-means for unweighted point sets and unrestricted cluster sizes, despite the much more general framework. We conclude with the discussion of some additional favorable properties of our method. © 2017 Elsevier B.V.","Clustering; Data mining; k-Means; Linear programming; Weight-balancing","Cluster analysis; Data mining; Linear programming; 52B12; 62H30; 68Q32; 90C05; 90C90; Clustering; K-means; Geometry",2-s2.0-85019867710
"Cubero-Fernandez A., Rodriguez-Lozano F.J., Villatoro R., Olivares J., Palomares J.M.","Efficient pavement crack detection and classification",2017,"Eurasip Journal on Image and Video Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020758830&doi=10.1186%2fs13640-017-0187-0&partnerID=40&md5=7d7fac66f5e7b91d0da4f7b266894565","Each year, millions of dollars are invested on road maintenance and reparation all over the world. In order to minimize costs, one of the main aspects is the early detection of those flaws. Different types of cracks require different types of repairs; therefore, not only a crack detection is required but a crack type classification. Also, the earlier the crack is detected, the cheaper the reparation is. Once the images are captured, several processes are applied in order to extract the main characteristics for emphasizing the cracks (logarithmic transformation, bilateral filter, Canny algorithm, and a morphological filter). After image preprocessing, a decision tree heuristic algorithm is applied to finally classify the image. This work obtained an average of 88% of success detecting cracks and an 80% of success detecting the type of the crack. It could be implemented in a vehicle traveling as fast as 130 kmh or 81 mph. © 2017, The Author(s).","Automatic detection; Crack detection; Heuristic classifier; Pavement crack; Road maintenance; Road safety","Cracks; Data mining; Decision trees; Heuristic algorithms; Image enhancement; Image processing; Motor transportation; Pavements; Roads and streets; Transportation; Trees (mathematics); Automatic Detection; Logarithmic transformations; Morphological filters; Pavement crack detection; Pavement cracks; Road maintenance; Road safety; Type classifications; Crack detection",2-s2.0-85020758830
"Öztoprak H., Toycan M., Alp Y.K., Arıkan O., Doğutepe E., Karakaş S.","Machine-based classification of ADHD and nonADHD participants using time/frequency features of event-related neuroelectric activity",2017,"Clinical Neurophysiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032391582&doi=10.1016%2fj.clinph.2017.09.105&partnerID=40&md5=a724d8ba953ecc4f172478a805662b6b","Objective Attention-deficit/hyperactivity disorder (ADHD) is the most frequent diagnosis among children who are referred to psychiatry departments. Although ADHD was discovered at the beginning of the 20th century, its diagnosis is still confronted with many problems. Method A novel classification approach that discriminates ADHD and nonADHD groups over the time-frequency domain features of event-related potential (ERP) recordings that are taken during Stroop task is presented. Time-Frequency Hermite-Atomizer (TFHA) technique is used for the extraction of high resolution time-frequency domain features that are highly localized in time-frequency domain. Based on an extensive investigation, Support Vector Machine-Recursive Feature Elimination (SVM-RFE) was used to obtain the best discriminating features. Results When the best three features were used, the classification accuracy for the training dataset reached 98%, and the use of five features further improved the accuracy to 99.5%. The accuracy was 100% for the testing dataset. Based on extensive experiments, the delta band emerged as the most contributing frequency band and statistical parameters emerged as the most contributing feature group. Conclusion The classification performance of this study suggests that TFHA can be employed as an auxiliary component of the diagnostic and prognostic procedures for ADHD. Significance The features obtained in this study can potentially contribute to the neuroelectrical understanding and clinical diagnosis of ADHD. © 2017 International Federation of Clinical Neurophysiology","Attention-deficit/hyperactivity disorder (ADHD); Classification; Feature selection; Machine learning; Support vector machine-recursive feature elimination (SVM-RFE); Time-frequency Hermite atomizer","Article; attention deficit disorder; child; clinical article; clinical effectiveness; controlled study; data mining; diagnostic accuracy; disease classification; DSM-IV; electric activity; event related potential; human; information processing; machine learning; male; methodology; priority journal; sensitivity and specificity; statistical parameters; Stroop test; support vector machine",2-s2.0-85032391582
"Nhem S., Lee Y.J., Phin S.","Sustainable management of forest in view of media attention to REDD + policy, opportunity and impact in Cambodia",2017,"Forest Policy and Economics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028329461&doi=10.1016%2fj.forpol.2017.08.011&partnerID=40&md5=467e81aff02a2c717568e242652a3256","The media plays a vital role in raising public awareness and concerns. This study analyzed media attention to the policy, opportunity and impact of REDD + towards enhancing sustainable forest management and mitigating climate change. Content analysis, case study and e-Delphi methods were employed. We collected 178 news articles from four media outlets in Cambodia and examined the media discourses led by the National REDD + Program Secretariat, TVK and Radio FM102 for the period of 2011–2016. The findings revealed that the media attention led by the Secretariat in collaboration with two media outlets spread the REDD + message nationwide and journalists gained knowledge and jointly set the media agenda with the government institutions. Four other reputable national media outlets published few news articles on REDD +. The e-Delphi survey of experts found that the key challenges limiting media discourses on REDD + were that the responsible institutions did not share information about REDD + events with the general media and that journalists had limited knowledge of REDD + and found the technical issues difficult. The study strongly highlighted that media framing is the most comprehensive choice to gain attention from policy makers and the public on REDD + policy, opportunity and impact. © 2017","Cambodia; e-Delphi; Media agenda-setting; Media attention; Protect events; REDD +","Climate change; Data mining; Decision making; Agenda settings; Cambodia; Media attention; Protect events; REDD; Forestry",2-s2.0-85028329461
"Castaño-Díaz M., Álvarez-Álvarez P., Tobin B., Nieuwenhuis M., Afif-Khouri E., Cámara-Obregón A.","Evaluation of the use of low-density LiDAR data to estimate structural attributes and biomass yield in a short-rotation willow coppice: an example in a field trial",2017,"Annals of Forest Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032300050&doi=10.1007%2fs13595-017-0665-7&partnerID=40&md5=5dd18cf4e347340fe7afdb4b58f1f11e","Key message: LiDAR data (low-density data, 0.5 pulses m−2) represent an excellent management resource as they can be used to estimate forest stand characteristics in short-rotation willow coppice (SRWC) with reasonable accuracy. The technology is also a useful, practical tool for carrying out inventories in these types of stands. Context: This study evaluated the use of very low-density airborne LiDAR (light detection and ranging) data (0.5 pulses m−2), which can be accessed free of charge, in an SRWC established in degraded mining land. Aims: This work aimed to determine the utility of low-density LiDAR data for estimating main forest structural attributes and biomass productivity and for comparing the estimates with field measurements carried out in an SRWC planted in marginal land. Methods: The SRWC was established following a randomized complete block design with three clones, planted at two densities and with three fertilization levels. Use of parametric (multiple regression) and non-parametric (classification and regression trees, CART) fitting techniques yielded models with good predictive power and reliability. Both fitting methods were used for comprehensive analysis of the data and provide complementary information. Results: The results of multiple regression analysis indicated close relationships (Rfit 2 = 0.63–0.97) between LiDAR-derived metrics and the field measured data for the variables studied (H, D20, D130, FW, and DW). High R2 values were obtained for models fitted using the CART technique (R2 = 0.73–0.94). Conclusion: Low-density LiDAR data can be used to model structural attributes and biomass yield in SRWC with reasonable accuracy. The models developed can be used to improve and optimize follow-up decisions about the management of these crops. © 2017, INRA and Springer-Verlag France SAS.","Airborne laser scanning; Energy crops; LiDAR; Mining land; SRC; Willow","airborne survey; biomass; coppice; crop yield; deciduous tree; energy crop; lidar; mining; Salix",2-s2.0-85032300050
"Akpalu W., Normanyo A.K.","Gold Mining Pollution and the Cost of Private Healthcare: The Case of Ghana",2017,"Ecological Economics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020991226&doi=10.1016%2fj.ecolecon.2017.06.025&partnerID=40&md5=6969930c72308e056534ab0b77912d5e","To attract greater levels of foreign direct investment into their gold-mining sectors, many mineral-rich countries in sub-Saharan Africa have been willing to overlook serious instances of mining company non-compliance with environmental standards. These lapses in regulatory oversight and enforcement have led to high levels of pollution in many mining communities. The likelihood is high that the risk of pollution-related sicknesses will necessitate increasingly high healthcare expenditures in affected communities. In this study, we propose and estimate a hedonic-type model that relates healthcare expenditure to the degree of residents' exposure to mining pollution using data obtained on gold mining in Ghana. This has been confirmed by our empirical results, with an elasticity coefficient of 0.12. Furthermore, while healthcare expenditure does not vary between males and females, younger household heads spend more on their health than their older counterparts after controlling for health status, income and access to health insurance. © 2017 The Authors","Ghana; Healthcare expenditure; Hedonic analysis; Mining pollution","cost analysis; ecological economics; foreign direct investment; gold mine; health care; health expenditure; health insurance; hedonic analysis; mining; pollution; Ghana; Sub-Saharan Africa",2-s2.0-85020991226
"Boukhris I., Elouedi Z., Ajabi M.","Toward intrusion detection using belief decision trees for big data",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014082595&doi=10.1007%2fs10115-017-1034-4&partnerID=40&md5=2c433ddfa8dc79174fe34d6c346f4559","Big data refers to datasets that we cannot manage with standard tools and within which lie valuable information previously hidden. New data mining techniques are needed to deal with the increasing size of such data, their complex structure as well as their veracity which is on covering questions of data imperfection and uncertainty. Even though big data veracity is often overlooked, it is very challenging and important for an accurate and reliable mining and knowledge discovery. This paper proposes MapReduce-based belief decision trees for big data as classifiers of uncertain large-scale datasets. The proposed averaging and conjunctive classification approaches are experimented for intrusion detection on KDD’99 massive intrusion dataset. Several granularity attacks’ levels have been considered depending on whether dealing with whole kind of attacks, or grouping them in categories or focusing on distinguishing normal and abnormal connections. © 2017, Springer-Verlag London.","Belief function theory; Big data; Classification under uncertainty; Intrusion detection; Veracity",,2-s2.0-85014082595
"Zhang X., Butts C.T.","Activity correlation spectroscopy: a novel method for inferring social relationships from activity data",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004045043&doi=10.1007%2fs13278-016-0419-9&partnerID=40&md5=8b89680de80363f48a331f9636a9c546","Inferring social relationships among individuals is a difficult problem, typically requiring either intrusive measurements (e.g., surveys) or convenience samples (e.g., ties captured by online social networking systems). Automatically collected activity data from mobile devices or similar sources provide a promising alternative for inexpensive and unobtrusive detection of interpersonal relationships. Here, we introduce a new method—activity correlation spectroscopy—for inferring relationships by exploiting the spectral and distributional structure of activity correlation within dyads. Unlike existing techniques, our approach can be employed with minimal, individual-level (i.e., non-relational), and non-identifying data that are easily collected using commodity hardware. We demonstrate our methodology via an application to detection of friendship and group co-membership using mobile device and survey data from the MIT Reality Mining study (Eagle et al. in Proc Natl Acad Sci 106:15274–15278, 2009). © 2016, Springer-Verlag Wien.","Activity data; Mobile devices; Network measurement; Relationship detection; Social networks; Spectroscopy",,2-s2.0-85004045043
"Banerjee B.P., Raval S., Zhai H., Cullen P.J.","Health condition assessment for vegetation exposed to heavy metal pollution through airborne hyperspectral data",2017,"Environmental Monitoring and Assessment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032817672&doi=10.1007%2fs10661-017-6333-4&partnerID=40&md5=490d5d1dba12956b19869abc7ec366e0","Recent advancements in hyperspectral remote sensing technology now provide improved diagnostic capabilities to assess vegetation health conditions. This paper uses a set of 13 vegetation health indices related to chlorophyll, xanthophyll, blue/green/red ratio and structure from airborne hyperspectral reflectance data collected around a derelict mining area in Yerranderie, New South Wales, Australia. The studied area has ten historic mine shafts with a legacy of heavy metals and acidic contamination in a pristine ecosystem now recognised as Great Blue Mountain World Heritage Area. The forest is predominantly comprised of different species of Eucalyptus trees. In addition to the airborne survey, ground-based spectra of the tree leaves were collected along the two accessible heavy metal contaminated pathways. The stream networks in the area were classified and the geospatial patterns of vegetation health were analysed along the Tonalli River, a major water tributary flowing through the National Park. Despite the inflow of contaminated water from the near-mine streams, the measured vegetation health indices along Tonalli River were found to remain unchanged. The responses of the vegetation health indices between the near-mine and away-mine streams were found similar. Based on the along-stream and inter-stream analysis of the spectral indices of vegetation health, no significant impact of the heavy metal pollution could be noticed. The results indicate the possibility of the vegetation having developed immunity towards the high levels of heavy metal pollution over a century of exposure. © 2017, Springer International Publishing AG, part of Springer Nature.","Abandoned mines; Heavy metal pollution; Hyperspectral remote sensing; Vegetation stress","Abandoned mines; Forestry; Health; Heavy metals; Mine shafts; Pollution; Remote sensing; Rivers; Surveys; Vegetation; Water pollution; Water resources; Airborne hyperspectral data; Heavy metal pollution; Hyperspectral reflectance; Hyperspectral remote sensing; Hyperspectral remote sensing technology; New South Wales , Australia; Vegetation health indices; Vegetation stress; River pollution; Eucalyptus",2-s2.0-85032817672
"Farrahi K., Zia K.","Trust reality-mining: evidencing the role of friendship for trust diffusion",2017,"Human-centric Computing and Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011708020&doi=10.1186%2fs13673-016-0085-y&partnerID=40&md5=8dd2d435cbefea1356e18acf1df456c1","Value sensitive design is driven by the motivation of making social and moral values central to the development of ICT systems. Among the most challenging concerns when imparting shared values like accountability, transparency, liberty, fairness and trust into information technology are reliable and comprehensive formal and computational models of those values. This paper, educated by trust theories and models from cognitive science, social sciences and artificial intelligence, proposes a novel stochastic computational model of trust, encapsulating abstractions of human cognitive capabilities and empirically evidenced social interaction patterns. Qualitative and quantitative features of trust are identified, upon which our formal model is phrased. Reality mining methods are used to validate the model based on a real life community dataset. We analyze the time-varying dynamics of the interaction and communication patterns of the community, consider varying types of relationships as well as their symmetry. Social network data analysis shows that our model better fits the evolved friendships compared to a well designed synthetic trust model, which is used as the baseline. © 2017, The Author(s).","Pervasive trust; Reality mining; Socio-technical systems; Value sensitive design",,2-s2.0-85011708020
"Sarker A.","A customizable pipeline for social media text normalization",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029235304&doi=10.1007%2fs13278-017-0464-z&partnerID=40&md5=92c1566b886ea45f4ae3060bec4c16c0","Social networks are persistently generating text-based data that encapsulate vast amounts of knowledge. However, the presence of non-standard terms and misspellings in texts originating from social networks poses a crucial challenge for natural language processing and machine learning systems that attempt to mine this knowledge. To address this problem, we propose a sequential, modular, and hybrid pipeline for social media text normalization. In the first phase, text preprocessing techniques and social media-specific vocabularies gathered from publicly available sources are used to transform, with high precision, out-of-vocabulary terms into in-vocabulary terms. A sequential language model, generated using the partially normalized texts from the first phase, is then utilized to normalize short, high-frequency, ambiguous terms. A supervised learning module is employed to normalize terms based on a manually annotated training corpus. Finally, a tunable, distributed language model-based backoff module at the end of the pipeline enables further customization of the system to specific domains of text. We performed intrinsic evaluations of the system on a publicly available domain-independent dataset from Twitter, and our system obtained an F-score of 0.836, outperforming other benchmark systems for the task. We further performed brief, task-oriented evaluations of the system to illustrate the customizability of the system to domain-specific tasks and the effects of normalization on downstream applications. The modular design enables the easy customization of the system to distinct types domain-specific social media text, in addition to its off-the-shelf application to generic social media text. © 2017, Springer-Verlag GmbH Austria.","Lexical normalization; Natural language processing; Social media data preparation; Social media text normalization; Social network mining; Text mining",,2-s2.0-85029235304
"De Maio C., Fenza G., Loia V., Orciuoli F.","Distributed online Temporal Fuzzy Concept Analysis for stream processing in smart cities",2017,"Journal of Parallel and Distributed Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015679534&doi=10.1016%2fj.jpdc.2017.02.002&partnerID=40&md5=98f2baa54a4c822298a1d68a6e471bdd","Nowadays one of the main challenges in the smart cities is mining high-level semantics from low-level activities. In this context real-time data streams are continuously produced and analysed by efficient and effective algorithms, which are able to handle complexities related to big data, in order to enable the core functions of Decision Support Systems in the smart city. These algorithms should receive input data coming from different city domains (or pillars) and process, aggregate and reason over them in a way that it is possible to find hidden correlations among different and heterogeneous elements (e.g., traffic, weather, cultural events) along space and time dimensions. This paper proposes the online implementation and deployment of Temporal Fuzzy Concept Analysis on a distributed real-time computation system, based on Apache Storm, to face with big data stream analysis in the smart city context. Such online distributed algorithm is able to incrementally generate the timed fuzzy lattice that organizes the knowledge on several and cross-domain aspects of the city. Temporal patterns, of how situations evolve in the city, can be elicited by both exploring the lattice and observing its growth in order to obtain actionable knowledge to support smart city decision-making processes. © 2017 Elsevier Inc.","Distributed architecture; Online data streams; Parallel computation; Smart city; Temporal Fuzzy Concept Analysis","Artificial intelligence; Big data; Data communication systems; Decision making; Decision support systems; Distributed computer systems; Fuzzy inference; Real time systems; Semantics; Decision making process; Distributed architecture; Fuzzy concept; Online data; Online implementation; Parallel Computation; Real-time computations; Real-time data streams; Smart city",2-s2.0-85015679534
"Joseph T.G., Curley M., Anand A.","Operational Methodologies for Rolling Resistance Evaluation",2017,"Geotechnical and Geological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022032887&doi=10.1007%2fs10706-017-0292-y&partnerID=40&md5=faa1baceacf3619b9c4eb7f91c14ecd2","The rolling resistance of materials used for haul road construction and to cap in-pit running surfaces is poorly understood and rarely evaluated in practice. Mine waste materials are frequently used to build and cap haul roads, only supplemented by surface capping where better quality gravel or crush materials are available within local geologic and economic limitations. Rolling resistance is most often estimated from limited geotechnical publications, made available through mining equipment manufacturer supplemental information whose origin is difficult at best to verify. This paper outlines two methodologies which may be easily employed to evaluate rolling resistance for (a) selection of materials prior to road and running surface planning and construction and (b) an ongoing evaluation of haul road performance, such that critical areas may be identified for more frequent road maintenance or re-build. To select appropriate road building materials, a scale laboratory test is outlined which highlights the fact that rolling resistance, expressed as a percentage equivalent slope, is independent of haul truck size and hence a material property. It may permit not only the selection of individual materials, but also be used to trial the performance of a composite set of haul road layers’ overall rolling resistance performance as subgrade layers become deteriorated. An ongoing field evaluation of rolling resistance uses (a) the haul truck as the measurement tool, where each tire is a measurement device, reflected by the load response at the suspension, and (b) a cyclic plate load test to establish the resilient pressure stiffness of the ground surface conditions. The latter indicates the ground deformation, which, when evaluated in parallel to the tire deformation establishing the changing tire–ground contact area, permits the rolling resistance to be evaluated. Plotting the rolling resistance magnitudes by GPS truck location as a truck moves around the mine site identifies the critical zones requiring maintenance attention on an ongoing real-time basis. © 2017, Springer International Publishing AG.","Deformation; Resilient pressure stiffness; Rolling resistance; Scale test; Suspension data; Tires","Automobile testing; Deformation; Mine trucks; Mining equipment; Road construction; Roads and streets; Stiffness; Tires; Transportation; Trucks; Equipment manufacturers; Ground deformations; Ground surface condition; Measurement device; Measurement tools; Plate load tests; Selection of materials; Suspension datum; Rolling resistance",2-s2.0-85022032887
"Pal J., Gonawela A.","Studying political communication on Twitter: the case for small data",2017,"Current Opinion in Behavioral Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031115629&doi=10.1016%2fj.cobeha.2017.09.009&partnerID=40&md5=24034cf4966118a32612b56a22a7a5d8","Big data has dramatically changed the study of political communications online as researchers access massive feeds of data on social media behavior, networks, and language. However, the nature political communication remains inherently message-driven, where the composition, timing, and metaphor are necessary components of the overall message. This article surveys research on political communication on Twitter and classifies it into seven subjective domains of research. The methodological approaches that have been applied toward these domains include quantitative technique studying the size, shape, profile of the networks and their nodes; large-scale data mining techniques applied to study the contents of Twitter messaging; and qualitative methods for in-depth study of messages. Showing that qualitative research methods have extended our understanding of political communications domains, we propose that small data approaches, through interpretive analysis and commentary by human readers, can be coupled with large-scale data analysis for deeper, contextual understanding of political messaging. © 2017 Elsevier Ltd",,,2-s2.0-85031115629
"Ma L., Song D., Liao L., Wang J.","PSVM: a preference-enhanced SVM model using preference data for classification",2017,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024098161&doi=10.1007%2fs11432-016-9020-4&partnerID=40&md5=f1ab138ae465fec97d08e921f19ed021","Classification is an essential task in data mining, machine learning and pattern recognition areas. Conventional classification models focus on distinctive samples from different categories. There are fine-grained differences between data instances within a particular category. These differences form the preference information that is essential for human learning, and, in our view, could also be helpful for classification models. In this paper, we propose a preference-enhanced support vector machine (PSVM), that incorporates preference-pair data as a specific type of supplementary information into SVM. Additionally, we propose a two-layer heuristic sampling method to obtain effective preference-pairs, and an extended sequential minimal optimization (SMO) algorithm to fit PSVM. To evaluate our model, we use the task of knowledge base acceleration-cumulative citation recommendation (KBA-CCR) on the TREC-KBA-2012 dataset and seven other datasets from UCI, StatLib and mldata.org. The experimental results show that our proposed PSVM exhibits high performance with official evaluation metrics. © 2017, Science China Press and Springer-Verlag GmbH Germany.","classification; preference; sampling; sequential minimal optimization (SMO); SVM","Education; Heuristic methods; Human form models; Knowledge based systems; Optimization; Pattern recognition; Sampling; Support vector machines; Classification models; Evaluation metrics; Heuristic sampling; preference; Preference information; Sequential minimal optimization; Sequential minimal optimization algorithms; Supplementary information; Classification (of information)",2-s2.0-85024098161
"Munaiah N., Kroh S., Cabrey C., Nagappan M.","Curating GitHub for engineered software projects",2017,"Empirical Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017654597&doi=10.1007%2fs10664-017-9512-6&partnerID=40&md5=6a70435a3c6130922b69d04fac369f3f","Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an engineered software project. We identify software engineering practices (called dimensions) and propose means for validating their existence in a GitHub repository. We used reaper to measure the dimensions of 1,857,423 GitHub repositories. We then used manually classified data sets of repositories to train classifiers capable of predicting if a given GitHub repository contains an engineered software project. The performance of the classifiers was evaluated using a set of 200 repositories with known ground truth classification. We also compared the performance of the classifiers to other approaches to classification (e.g. number of GitHub Stargazers) and found our classifiers to outperform existing approaches. We found stargazers-based classifier (with 10 as the threshold for number of stargazers) to exhibit high precision (97%) but an inversely proportional recall (32%). On the other hand, our best classifier exhibited a high precision (82%) and a high recall (86%). The stargazer-based criteria offers precision but fails to recall a significant portion of the population. © 2017, Springer Science+Business Media New York.","Curation tools; Data curation; GitHub; Mining software repositories","Software engineering; Curation; Data curation; GitHub; High-precision; Mining software repositories; Reference implementation; Software engineering practices; Software project; Classification (of information)",2-s2.0-85017654597
"Pirhaji L., Milani P., Dalin S., Wassie B.T., Dunn D.E., Fenster R.J., Avila-Pacheco J., Greengard P., Clish C.B., Heiman M., Lo D.C., Fraenkel E.","Identifying therapeutic targets by combining transcriptional data with ordinal clinical measurements",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029637229&doi=10.1038%2fs41467-017-00353-6&partnerID=40&md5=61ba5f3f0cadbd469ddc3be37894fd7b","The immense and growing repositories of transcriptional data may contain critical insights for developing new therapies. Current approaches to mining these data largely rely on binary classifications of disease vs. control, and are not able to incorporate measures of disease severity. We report an analytical approach to integrate ordinal clinical information with transcriptomics. We apply this method to public data for a large cohort of Huntington's disease patients and controls, identifying and prioritizing phenotype-associated genes. We verify the role of a high-ranked gene in dysregulation of sphingolipid metabolism in the disease and demonstrate that inhibiting the enzyme, sphingosine-1-phosphate lyase 1 (SPL), has neuroprotective effects in Huntington's disease models. Finally, we show that one consequence of inhibiting SPL is intracellular inhibition of histone deacetylases, thus linking our observations in sphingolipid metabolism to a well-characterized Huntington's disease pathway. Our approach is easily applied to any data with ordinal clinical measurements, and may deepen our understanding of disease processes. © 2017 The Author(s).",,"histone deacetylase; sphingolipid; sphingosine 1 phosphate; sphingosine 1 phosphate lyase 1; unclassified drug; analytical method; enzyme; enzyme activity; genetic analysis; inhibition; metabolism; nervous system disorder; neurology; animal cell; Article; cohort analysis; controlled study; enzyme inhibition; gene expression; human; human cell; Huntington chorea; mouse; neuroprotection; nonhuman; phenotype; sphingolipid metabolism; transcriptomics",2-s2.0-85029637229
"Najafov J., Najafov A.","CrossCheck: An open-source web tool for high-throughput screen data analysis",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025680232&doi=10.1038%2fs41598-017-05960-3&partnerID=40&md5=a63009220afb222acbbef1ef68136fc3","Modern high-throughput screening methods allow researchers to generate large datasets that potentially contain important biological information. However, oftentimes, picking relevant hits from such screens and generating testable hypotheses requires training in bioinformatics and the skills to efficiently perform database mining. There are currently no tools available to general public that allow users to cross-reference their screen datasets with published screen datasets. To this end, we developed CrossCheck, an online platform for high-throughput screen data analysis. CrossCheck is a centralized database that allows effortless comparison of the user-entered list of gene symbols with 16,231 published datasets. These datasets include published data from genome-wide RNAi and CRISPR screens, interactome proteomics and phosphoproteomics screens, cancer mutation databases, low-throughput studies of major cell signaling mediators, such as kinases, E3 ubiquitin ligases and phosphatases, and gene ontological information. Moreover, CrossCheck includes a novel database of predicted protein kinase substrates, which was developed using proteome-wide consensus motif searches. CrossCheck dramatically simplifies high-throughput screen data analysis and enables researchers to dig deep into the published literature and streamline data-driven hypothesis generation. CrossCheck is freely accessible as a web-based application at http://proteinguru.com/crosscheck. © 2017 The Author(s).",,,2-s2.0-85025680232
"Shi P., Zhang Y., Hu Z., Ma K., Wang H., Chai T.","The response of soil bacterial communities to mining subsidence in the west China aeolian sand area",2017,"Applied Soil Ecology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029712549&doi=10.1016%2fj.apsoil.2017.09.020&partnerID=40&md5=01a5a5a459a546e2f940a01bae53715f","Soil bacteria play a vital role in terrestrial ecosystems and are very sensitive to changes in the environment. Land subsidence due to underground coal mining could affect soil properties, but the extent of this effect on the soil bacterial community remains unclear. Here, we investigated the effect of land subsidence on soil bacterial communities and their response to changes in the soil environment in a control area and a land subsidence area in the West China Aeolian Sand Area. The results showed that electrical conductivity (EC), total carbon, (TC), total nitrogen (TN), available potassium (AK) and soil organic matter (SOM) at a soil depth of 20 cm were significantly decreased in the land subsidence area compared to the unexploited area, and Illumina MiSeq sequencing data revealed that the bacterial community at a soil depth of 0–180 cm was dominated by Pseudomonas, Gp4, Gp6, Sphingomonas, Gemmatimonas, Arthrobacter, Aciditerrimonas and Gaiella. Land subsidence decreased soil microbial richness and diversity. In addition, there was a significant decrease in the relative abundance of some core genera in the topsoil, such as Sphingomonas, Nocardioides and Saccharibacteria_genera_incertae_sedis, indicating that the dominant bacteria had strong anti-interference abilities and played important roles in the nutrient-poor soils of the mining area. Redundancy analysis (RDA) showed that the main factors driving the changes in the bacterial community structure were EC, water content (WC) and soil depth. The vertical leakage of water and nutrients was caused by subsidence and cracks in the ground, leading to decreased soil microbial richness and diversity. These results suggested that the soil nutrients and soil microbial community has yet to recover by self-healing after two years of land subsidence; thus, artificial restoration might be required. © 2017","Bacterial diversity; Illumina MiSeq sequencing; Land　subsidence; Soil bacterial community; Soil properties",,2-s2.0-85029712549
"Charro E., Moyano A.","Soil and vegetation influence in plants natural radionuclides uptake at a uranium mining site",2017,"Radiation Physics and Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025077393&doi=10.1016%2fj.radphyschem.2017.07.014&partnerID=40&md5=c277e3d503ead1aa63a79833b09ea83f","The main objective of this work is to investigate the uptake of several radionuclides by the vegetation characteristic of a dehesa ecosystem in uranium mining-impacted soils in Central-West of Spain. The activity concentration for 238U, 226Ra, 210Pb, 232Th, and 224Ra was measured in soil and vegetation samples using a Canberra n-type HPGe gamma-ray spectrometer. Transfer factors of natural radionuclides in different tissues (leaves, branches, twigs, and others) of native plants were evaluated. From these data, the influence of the mine, the physicochemical parameters of the soils and the type of vegetation were analyzed in order to explain the accumulation of radionuclides in the vegetation. A preferential uptake of 210Pb and 226Ra by plants, particularly by trees of the Quercus species (Quercus pyrenaica and Quercus ilex rotundifolia), has been observed, being the transfer factors for 226Ra and 210Pb in these tree species higher than those for other plants (like Pinus pinaster, Rubur ulmifolius and Populus sp.). The analysis of radionuclide contents and transfer factors in the vegetation showed no evidence of influence of the radionuclide concentration in soils, although it could be explained in terms of the type of plants and, in particular, of the tree's species, with special attention to the tree's rate of growth, being higher in slow growing species. © 2017 Elsevier Ltd","Natural radionuclides; Soil-to-plant transfer factors; Uranium mine","Forestry; Gamma rays; Lead; Plants (botany); Radioisotopes; Soil pollution; Soils; Uranium; Uranium mines; Activity concentration; Natural radionuclides; Physicochemical parameters; Quercus pyrenaica; Quercus species; Radionuclide concentration; Radionuclide content; Soil-to-plant transfer factors; Vegetation; lead 210; radioisotope; radium 224; radium 226; thorium 232; uranium 238; Article; bioaccumulation; blackberry; controlled study; gamma spectrometry; nonhuman; oak; pine; Pinus pinaster; plant leaf; plant stem; plant tissue; pollution transport; Populus; Quercus ilex rotundifolia; Quercus pyrenaica; radioactive contamination; radioisotope distribution; Rubur ulmifolius; soil pollution; soil property; Spain; uranium mine; vegetation",2-s2.0-85025077393
"Akbari B., Wee P., Yaqubi M., Mohammadnia A.","Comprehensive transcriptome mining of the direct conversion of mesodermal cells",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028848813&doi=10.1038%2fs41598-017-10903-z&partnerID=40&md5=2bc65e073bdcf1d9cf5ef896ffb0f44d","The direct reprogramming of somatic cells is a promising approach for regenerative medicine, especially in the production of mesoderm layer-derived cells. Meta-analysis studies provide precise insight into the undergoing processes and help increase the efficiency of reprogramming. Here, using 27 high-throughput expression data sets, we analyzed the direct reprogramming of mesodermal cells in humans and mice. Fibroblast-derived cells showed a common expression pattern of up- and down-regulated genes that were mainly involved in the suppression of the fibroblast-specific gene expression program, and may be used as markers of the initiation of reprogramming. Furthermore, we found a specific gene expression profile for each fibroblast-derived cell studied, and each gene set appeared to play specific functional roles in its cell type, suggesting their use as markers for their mature state. Furthermore, using data from protein-DNA interactions, we identified the main transcription factors (TFs) involved in the conversion process and ranked them based on their importance in their gene regulatory networks. In summary, our meta-analysis approach provides new insights on the direct conversion of mesodermal somatic cells, introduces a list of genes as markers for initiation and maturation, and identifies TFs for which manipulating their expression may increase the efficiency of direct conversion. © 2017 The Author(s).",,,2-s2.0-85028848813
"Guo Y., Gao H., Wu Q.","A meteorological information mining-based wind speed model for adequacy assessment of power systems with wind power",2017,"International Journal of Electrical Power and Energy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021161562&doi=10.1016%2fj.ijepes.2017.05.031&partnerID=40&md5=12126d6f6406bf4fe848bd1daed3163a","Accurate wind speed simulation is an essential prerequisite to analyze the power systems with wind power. A wind speed model considering meteorological conditions and seasonal variations is proposed in this paper. Firstly, using the path analysis method, the influence weights of meteorological factors are calculated. Secondly, the meteorological data are classified into several states using an improved Fuzzy C-means (FCM) algorithm. Then the Markov chain is used to model the chronological characteristics of meteorological states and wind speed. The proposed model was proved to be more accurate in capturing the characteristics of probability distribution, auto-correlation and seasonal variations of wind speed compared with the traditional Markov chain Monte Carlo (MCMC) and autoregressive moving average (ARMA) model. Furthermore, the proposed model was applied to adequacy assessment of generation systems with wind power. The assessment results of the modified IEEE-RTS79 and IEEE-RTS96 demonstrated the effectiveness and accuracy of the proposed model. © 2017 Elsevier Ltd","Adequacy assessment; Clustering analysis; Markov chain; Meteorological factors; Wind speed model","Chains; Factor analysis; Markov processes; Meteorology; Probability distributions; Regression analysis; Speed; Speed control; Wind power; Adequacy assessment; Autoregressive moving average model; Clustering analysis; Markov chain Monte Carlo; Meteorological condition; Meteorological factors; Meteorological information; Wind speed models; Wind",2-s2.0-85021161562
"Kibanov M., Stumme G., Amin I., Lee J.G.","Mining social media to inform peatland fire and haze disaster management",2017,"Social Network Analysis and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024392071&doi=10.1007%2fs13278-017-0446-1&partnerID=40&md5=bfb1b319301626a526bfaac34c2860e7","Peatland fires and haze events are disasters with national, regional, and international implications. The phenomena lead to direct damage to local assets, as well as broader economic and environmental losses. Satellite imagery is still the main and often the only available source of information for disaster management. In this article, we test the potential of social media to assist disaster management. To this end, we compare insights from two datasets: fire hotspots detected via NASA satellite imagery and almost all GPS-stamped tweets from Sumatra Island, Indonesia, posted during 2014. Sumatra Island is chosen as it regularly experiences a significant number of haze events, which affect citizens in Indonesia as well as in nearby countries including Malaysia and Singapore. We analyze temporal correlations between the datasets and their geo-spatial interdependence. Furthermore, we show how Twitter data reveal changes in users’ behavior during severe haze events. Overall, we demonstrate that social media are a valuable source of complementary and supplementary information for haze disaster management. Based on our methodology and findings, an analytics tool to improve peatland fire and haze disaster management by the Indonesian authorities is under development. © 2017, Springer-Verlag GmbH Austria.",,,2-s2.0-85024392071
"Shang R., Liu H., Jiao L., Esfahani A.M.G.","Community mining using three closely joint techniques based on community mutual membership and refinement strategy",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029836559&doi=10.1016%2fj.asoc.2017.08.050&partnerID=40&md5=fcca771e9fe3dcaded2e9df3cd5aeb20","Community structure has become one of the central studies of the topological structure of complex networks in the past decades. Although many advanced approaches have been proposed to identify community structure, those state-of-the-art methods still lack efficiency in terms of a balance between stability, accuracy and computation time. Here, we propose an algorithm with different stages, called TJA-net, to efficiently identify communities in a large network with a good balance between accuracy, stability and computation time. First, we propose an initial labeling algorithm, called ILPA, combining K-nearest neighbor (KNN) and label propagation algorithm (LPA). To produce a number of sub-communities automatically, ILPA iteratively labels a node in a network using the labels of its adjacent nodes and their index of closeness. Next, we merge sub-communities using the mutual membership of two communities. Finally, a refinement strategy is designed for modifying the label of the wrongly clustered nodes at boundaries. In our approach, we propose and use modularity density as the objective function rather than the commonly used modularity. This can deal with the issue of the resolution limit for different network structures enhancing the result precision. We present a series of experiments with artificial and real data set and compare the results obtained by our proposed algorithm with the ones obtained by the state-of-the-art algorithms, which shows the effectiveness of our proposed approach. The experimental results on large-scale artificial networks and real networks illustrate the superiority of our algorithm. © 2017 Elsevier B.V.","Community detection; Community mutual membership; K-nearest neighbor; Large-scale complex networks; Refinement strategy","Arthroplasty; Computational efficiency; Iterative methods; Motion compensation; Nearest neighbor search; Social sciences; Text processing; Community detection; Community mutual membership; K nearest neighbor (KNN); K-nearest neighbors; Refinement strategy; State-of-the-art algorithms; State-of-the-art methods; Topological structure; Complex networks",2-s2.0-85029836559
"Lin F.P.-Y., Pokorny A., Teng C., Epstein R.J.","TEPAPA: A novel in silico feature learning pipeline for mining prognostic and associative factors from text-based electronic medical records",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026632360&doi=10.1038%2fs41598-017-07111-0&partnerID=40&md5=b02efbfd2f8ea995afffdb56103dcbdc","Vast amounts of clinically relevant text-based variables lie undiscovered and unexploited in electronic medical records (EMR). To exploit this untapped resource, and thus facilitate the discovery of informative covariates from unstructured clinical narratives, we have built a novel computational pipeline termed Text-based Exploratory Pattern Analyser for Prognosticator and Associator discovery (TEPAPA). This pipeline combines semantic-free natural language processing (NLP), regular expression induction, and statistical association testing to identify conserved text patterns associated with outcome variables of clinical interest. When we applied TEPAPA to a cohort of head and neck squamous cell carcinoma patients, plausible concepts known to be correlated with human papilloma virus (HPV) status were identified from the EMR text, including site of primary disease, tumour stage, pathologic characteristics, and treatment modalities. Similarly, correlates of other variables (including gender, nodal status, recurrent disease, smoking and alcohol status) were also reliably recovered. Using highly-associated patterns as covariates, a patient's HPV status was classifiable using a bootstrap analysis with a mean area under the ROC curve of 0.861, suggesting its predictive utility in supporting EMR-based phenotyping tasks. These data support using this integrative approach to efficiently identify disease-associated factors from unstructured EMR narratives, and thus to efficiently generate testable hypotheses. © 2017 The Author(s).",,,2-s2.0-85026632360
"Gangoiti J., Van Leeuwen S.S., Meng X., Duboux S., Vafiadi C., Pijning T., Dijkhuizen L.","Mining novel starch-converting Glycoside Hydrolase 70 enzymes from the Nestlé Culture Collection genome database: The Lactobacillus reuteri NCC 2613 GtfB",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028618340&doi=10.1038%2fs41598-017-07190-z&partnerID=40&md5=f6ce47f364801b9083b6aa9367193f52","The Glycoside hydrolase (GH) family 70 originally was established for glucansucrases of lactic acid bacteria (LAB) converting sucrose into α-glucan polymers. In recent years we have identified 3 subfamilies of GH70 enzymes (designated GtfB, GtfC and GtfD) as 4,6-α-glucanotransferases, cleaving (α1 → 4)-linkages in maltodextrins/starch and synthesizing new (α1 → 6)-linkages. In this work, 106 putative GtfBs were identified in the Nestlé Culture Collection genome database with ∼2700 genomes, and the L. reuteri NCC 2613 one was selected for further characterization based on variations in its conserved motifs. Using amylose the L. reuteri NCC 2613 GtfB synthesizes a low-molecular-mass reuteran-like polymer consisting of linear (α1 → 4) sequences interspersed with (α1 → 6) linkages, and (α1 → 4,6) branching points. This product specificity is novel within the GtfB subfamily, mostly comprising 4,6-α-glucanotransferases synthesizing consecutive (α1 → 6)-linkages. Instead, its activity resembles that of the GtfD 4,6-α-glucanotransferases identified in non-LAB strains. This study demonstrates the potential of large-scale genome sequence data for the discovery of enzymes of interest for the food industry. The L. reuteri NCC 2613 GtfB is a valuable addition to the starch-converting GH70 enzyme toolbox. It represents a new evolutionary intermediate between families GH13 and GH70, and provides further insights into the structure-function relationships of the GtfB subfamily enzymes. © 2017 The Author(s).",,,2-s2.0-85028618340
"Zhang H., Kiranyaz S., Gabbouj M.","Outlier edge detection using random graph generation models and applications",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018865430&doi=10.1186%2fs40537-017-0073-8&partnerID=40&md5=a95f183285b9d1d0e26003f9ce3c5f36","Outliers are samples that are generated by different mechanisms from other normal data samples. Graphs, in particular social network graphs, may contain nodes and edges that are made by scammers, malicious programs or mistakenly by normal users. Detecting outlier nodes and edges is important for data mining and graph analytics. However, previous research in the field has merely focused on detecting outlier nodes. In this article, we study the properties of edges and propose effective outlier edge detection algorithm. The proposed algorithms are inspired by community structures that are very common in social networks. We found that the graph structure around an edge holds critical information for determining the authenticity of the edge. We evaluated the proposed algorithms by injecting outlier edges into some real-world graph data. Experiment results show that the proposed algorithms can effectively detect outlier edges. In particular, the algorithm based on the Preferential Attachment Random Graph Generation model consistently gives good performance regardless of the test graph data. More important, by analyzing the authenticity of the edges in a graph, we are able to reveal underlying structure and properties of a graph. Thus, the proposed algorithms are not limited in the area of outlier edge detection. We demonstrate three different applications that benefit from the proposed algorithms: (1) a preprocessing tool that improves the performance of graph clustering algorithms; (2) an outlier node detection algorithm; and (3) a novel noisy data clustering algorithm. These applications show the great potential of the proposed outlier edge detection techniques. They also address the importance of analyzing the edges in graph mining—a topic that has been mostly neglected by researchers. © 2017, The Author(s).","Graph mining; Outlier detection; Outlier edge",,2-s2.0-85018865430
"Roberts B.M., Blewitt G., Dailey C., Murphy M., Pospelov M., Rollings A., Sherman J., Williams W., Derevianko A.","Search for domain wall dark matter with atomic clocks on board global positioning system satellites",2017,"Nature Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032496763&doi=10.1038%2fs41467-017-01440-4&partnerID=40&md5=1731293b9867b20b0894ea8460bf71af","Cosmological observations indicate that dark matter makes up 85% of all matter in the universe yet its microscopic composition remains a mystery. Dark matter could arise from ultralight quantum fields that form macroscopic objects. Here we use the global positioning system as a ~ 50,000 km aperture dark matter detector to search for such objects in the form of domain walls. Global positioning system navigation relies on precision timing signals furnished by atomic clocks. As the Earth moves through the galactic dark matter halo, interactions with domain walls could cause a sequence of atomic clock perturbations that propagate through the satellite constellation at galactic velocities ~ 300 km s-1. Mining 16 years of archival data, we find no evidence for domain walls at our current sensitivity level. This improves the limits on certain quadratic scalar couplings of domain wall dark matter to standard model particles by several orders of magnitude. © 2017 The Author(s).",,"global positioning system; mining; velocity",2-s2.0-85032496763
"Do C.-T., Douzal-Chouakria A., Marié S., Rombaut M., Varasteh S.","Multi-modal and multi-scale temporal metric learning for a robust time series nearest neighbors classification",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027708029&doi=10.1016%2fj.ins.2017.08.020&partnerID=40&md5=fb76c1ca78ad08960c2c11f7f3c1a81a","The definition of a metric between time series is inherent to several data analysis and mining tasks, including clustering, classification or forecasting. Time series data present naturally several modalities covering their amplitude, behavior or frequential spectrum, that may be expressed with varying delays and at multiple temporal scales—exhibited globally or locally. Combining several modalities at multiple temporal scales to learn a holistic metric is a key challenge for many real temporal data applications. This paper proposes a Multi-modal and Multi-scale Temporal Metric Learning (M2TML) approach for a robust time series nearest neighbors classification. The solution lies in embedding time series into a dissimilarity space where a pairwise SVM is used to learn both linear and non linear combined metric. A sparse and interpretable variant of the solution shows the ability of the learned temporal metric to localize accurately discriminative modalities as well as their temporal scales. A wide range of 30 public and challenging datasets, encompassing images, traces and ECG data, are used to show the efficiency and the potential of M2TML for an effective time series nearest neighbors classification. © 2017 Elsevier Inc.","Classification; Dissimilarity space; kNN; Metric learning; Multi-modal metric; Multi-scale metric; Svm; Time series","Classification (of information); Nearest neighbor search; Time series; Dissimilarity space; Effective time; Metric learning; Multi-modal; Multi-scale metric; Nearest neighbors; Temporal scale; Time-series data; Time series analysis",2-s2.0-85027708029
"Schleicher J., Peres C.A., Amano T., Llactayo W., Leader-Williams N.","Conservation performance of different conservation governance regimes in the Peruvian Amazon",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029279232&doi=10.1038%2fs41598-017-10736-w&partnerID=40&md5=0ce63eeb42dee3316ea8c4b0e629715e","State-controlled protected areas (PAs) have dominated conservation strategies globally, yet their performance relative to other governance regimes is rarely assessed comprehensively. Furthermore, performance indicators of forest PAs are typically restricted to deforestation, although the extent of forest degradation is greater. We address these shortfalls through an empirical impact evaluation of state PAs, Indigenous Territories (ITs), and civil society and private Conservation Concessions (CCs) on deforestation and degradation throughout the Peruvian Amazon. We integrated remote-sensing data with environmental and socio-economic datasets, and used propensity-score matching to assess: (i) how deforestation and degradation varied across governance regimes between 2006-2011; (ii) their proximate drivers; and (iii) whether state PAs, CCs and ITs avoided deforestation and degradation compared with logging and mining concessions, and the unprotected landscape. CCs, state PAs, and ITs all avoided deforestation and degradation compared to analogous areas in the unprotected landscape. CCs and ITs were on average more effective in this respect than state PAs, showing that local governance can be equally or more effective than centralized state regimes. However, there were no consistent differences between conservation governance regimes when matched to logging and mining concessions. Future impact assessments would therefore benefit from further disentangling governance regimes across unprotected land. © 2017 The Author(s).",,"deforestation; degradation; driver; forest; human; landscape; logging; mining; propensity score; remote sensing",2-s2.0-85029279232
"Li C., Fu L., Stafford J., Belosevic M., Gamal El-Din M.","The toxicity of oil sands process-affected water (OSPW): A critical review",2017,"Science of the Total Environment",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020481459&doi=10.1016%2fj.scitotenv.2017.06.024&partnerID=40&md5=fb93ca084136cd345e1e3286efdf50f2","Large volumes of oil sands process-affected water (OSPW) are produced by the surface-mining oil sands industry in Alberta. Both laboratory and field studies have demonstrated that the exposure to OSPW leads to many physiological changes in a variety of organisms. Adverse effects include compromised immunological function, developmental delays, impaired reproduction, disrupted endocrine system, and higher prevalence of tissue-specific pathological manifestations. The composition of OSPW varies with several factors such as ore sources, mining process, and tailings management practices. Differences in water characteristics have confounded interpretation or comparison of OSPW toxicity across studies. Research on individual fractions extracted from OSPW has helped identify some target pollutants. Naphthenic acids (NAs) are considered as the major toxic components in OSPW, exhibiting toxic effects through multiple modes of action including narcosis and endocrine disruption. Other pollutants, like polycyclic aromatic hydrocarbons (PAHs), metals, and ions may also contribute to the overall OSPW toxicity. Studies have been conducted on OSPW as a whole complex effluent mixture, with consideration of the presence of unidentified components, and the interactions (potential synergistic or antagonistic reactions) among chemicals. This review summarizes the toxicological data derived from in vitro and in vivo exposure studies using different OSPW types, and different taxa of organisms. In general, toxicity of OSPW was found to be dependent on the OSPW type and concentration, duration of exposures (acute versus sub chronic), and organism studied. © 2017 Elsevier B.V.","Alberta oil sands; Composition; Naphthenic acids; OSPW; Toxicity; Water type","Aromatic hydrocarbons; Biology; Chemical analysis; Effluents; Oil sands; Organic acids; Pollution; Polycyclic aromatic hydrocarbons; Sand; Toxicity; Alberta oil sands; Immunological functions; Laboratory and field studies; Naphthenic acid; Oil Sands Process-affected Waters; OSPW; Polycyclic aromatic hydrocarbons (PAHS); Water types; Produced Water; chemical compound; naphthenic acid; oil sands process affected water; polycyclic aromatic hydrocarbon; unclassified drug; water; carboxylic acid; chemical composition; oil sand; physiological response; pollution effect; toxicity; toxicology; water; Amphibia; anesthesia; bird; Canada; carcinogenicity; chemical composition; chemical interaction; chemistry; concentration (parameters); controlled study; effluent; environmental exposure; environmental impact; experimental animal; fish; genotoxicity; immunotoxicity; in vitro study; in vivo study; industry; invertebrate; LC50; mammal; nonhuman; oxidative stress; physical chemistry; pollution monitoring; pond; priority journal; Review; surface mining oil sands industry; tailing pond; toxicity; toxicity testing; water chemistry; water quality; Alberta; Canada",2-s2.0-85020481459
"McJannet D., Hawdon A., Van Niel T., Boadle D., Baker B., Trefry M., Rea I.","Measurements of evaporation from a mine void lake and testing of modelling approaches",2017,"Journal of Hydrology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032818163&doi=10.1016%2fj.jhydrol.2017.10.064&partnerID=40&md5=ab19adf311ff4cf8be4baa1bd95d2a77","Pit lakes often form in the void that remains after open cut mining operations cease. As pit lakes fill, hydrological and geochemical processes interact and these need to be understood for appropriate management actions to be implemented. Evaporation is important in the evolution of pit lakes as it acts to concentrate various constituents, controls water level and changes the thermal characteristics of the water body. Despite its importance, evaporation from pit lakes is poorly understood. To address this, we used an automated floating evaporation pan and undertook measurements at a pit lake over a 12 month period. We also developed a new procedure for correcting floating pan evaporation estimates to lake evaporation estimates based on surface temperature differences. Total annual evaporation was 2690 mm and reflected the strong radiation inputs, high temperatures and low humidity experienced in this region. Measurements were used to test the performance of evaporation estimates derived using both pan coefficient and aerodynamic modelling techniques. Daily and monthly evaporation estimates were poorly reproduced using pan coefficient techniques and their use is not recommended for such environments. Aerodynamic modelling was undertaken using a range of input datasets that may be available to those who manage pit lake systems. Excellent model performance was achieved using over-water or local over-land meteorological observations, particularly when the sheltering effects of the pit were considered. Model performance was reduced when off-site data were utilised and differences between local and off-site vapor pressure and wind speed were found to be the major cause. © 2017","Evaporation; Evaporation pan; Measurements; Mine void; Model; Pit lake","Aerodynamics; Lakes; Measurements; Models; Open pit mining; Water levels; Wind; Aerodynamic modelling; Evaporation pans; Geochemical process; Meteorological observation; Mine voids; Pit lakes; Surface temperature difference; Thermal characteristics; Evaporation",2-s2.0-85032818163
"Quintela-Sabarís C., Marchand L., Kidd P.S., Friesl-Hanl W., Puschenreiter M., Kumpiene J., Müller I., Neu S., Janssen J., Vangronsveld J., Dimitriou I., Siebielec G., Gałązka R., Bert V., Herzig R., Cundy A.B., Oustrière N., Kolbas A., Galland W., Mench M.","Assessing phytotoxicity of trace element-contaminated soils phytomanaged with gentle remediation options at ten European field trials",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019170393&doi=10.1016%2fj.scitotenv.2017.04.187&partnerID=40&md5=1c406112c8bce812c405c51d5586786f","Gentle remediation options (GRO), i.e. in situ stabilisation, (aided) phytoextraction and (aided) phytostabilisation, were implemented at ten European sites contaminated with trace elements (TE) from various anthropogenic sources: mining, atmospheric fallout, landfill leachates, wood preservatives, dredged-sediments, and dumped wastes. To assess the performance of the GRO options, topsoil was collected from each field trial, potted, and cultivated with lettuce (Lactuca sativa L.) for 48 days. Shoot dry weight (DW) yield, photosynthesis efficiency and major element and TE concentrations in the soil pore water and lettuce shoots were measured. GRO implementation had a limited effect on TE concentrations in the soil pore water, although use of multivariate Co-inertia Analysis revealed a clear amelioration effect in phytomanaged soils. Phytomanagement increased shoot DW yield at all industrial and mine sites, whereas in agricultural soils improvements were produced in one out of five sites. Photosynthesis efficiency was less sensitive than changes in shoot biomass and did not discriminate changes in soil conditions. Based on lettuce shoot DW yield, compost amendment followed by phytoextraction yielded better results than phytostabilisation; moreover shoot ionome data proved that, depending on initial soil conditions, recurrent compost application may be required to maintain crop production with common shoot nutrient concentrations. © 2017 Elsevier B.V.","Biomass; Chlorophyll fluorescence; GREENLAND project; Lettuce; Phytoextraction; Phytostabilisation","Biomass; Composting; Cultivation; Dredging; Efficiency; Leachate treatment; Photosynthesis; Plants (botany); Protective coatings; Remediation; Soil pollution control; Soils; Trace elements; Water; Wood; Chlorophyll fluorescence; Greenland; Lettuce; Phyto-stabilisation; Phytoextraction; Soil pollution; trace element; wood protecting agent; biomass; chlorophyll; fluorescence; phytoremediation; phytotoxicity; soil pollution; soil remediation; trace element; Article; crop production; dredging; dumping; Europe; fallout; landfill leachate; lettuce; mining; nonhuman; photosynthesis; phytoremediation; phytotoxicity; plant nutrient; priority journal; sediment; shoot; soil analysis; soil management; soil pollution; Biomass; Chlorophylls; Fluorescence; Europe; Lactuca; Lactuca sativa",2-s2.0-85019170393
"Parviainen A., Roman-Alpiste M.J., Marchesi C., Suárez-Grau J.M., Pérez-López R.","New insights into the metal partitioning in different microphases of human gallstones",2017,"Journal of Trace Elements in Medicine and Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029602322&doi=10.1016%2fj.jtemb.2017.09.013&partnerID=40&md5=5dc0d3256e78d072eb8fc2136bbf241b","Chronic metal exposure, e.g. from metal mining, may cause accumulation of metals in soft and hard tissues, and in developing biomineralizations in the human body. Gallstones are biomineralizations formed in the gallbladder which are able to trap trace elements from the bile. Laser Ablation-Inductively Coupled Plasma-Mass Spectrometry (LA-ICP-MS) was used to analyze gallstone cross-sections to trace the elemental abundances and correlate them with the principal phases constituting gallstones, namely cholesterol, Ca bilirubinate salts, Ca carbonate, and Ca phosphate. Five different types of gallstones (pure, mixed, and composite cholesterol stones, pigment stone, and carbonate stone) were chosen according to a previous classification based on phase characterization by different spectroscopic and microscopic techniques. These data were combined with bulk solution ICP-MS/OES analyses for total elemental concentrations. The results indicated that cholesterol has a zero capacity to retain elements except for Ca. Hence, pure cholesterol stones contained the lowest bulk metal concentrations, and the metals were found in the scarce carbonate and phosphate phases in these calculi. Calcium and trace element concentrations increased in other types of gallstones along with increasing amount of bilirubinate, carbonates and phosphates; pigment stones being the most enriched in metals. Phosphates were the principal carriers of Ca, P, Na, Mg, Mn, Fe, Pb, and Cd, whereas carbonate phases were enriched in Ca, Mg, Na, and Mn in order of decreasing abundance. Bilirubinate on the other hand was enriched in Ca, Cu, Ag, and Ni. The higher trace metal affinities of bilirubinate and phosphate explain the elevated metal concentrations observed in the pigment stones. These results give new insight to the trace metal behavior in the gallstone formation and the metal accumulation in the human body, validating the possible use of these biomineralizations as a proxy for exposure to metal pollution. © 2017 Elsevier GmbH","Bilirubinate; Biomineralization; Ca carbonate; Ca phosphate; Cholesterol; Pigment stone","bilirubin; cadmium; calcium; calcium carbonate; cholesterol; iron; lead; magnesium; manganese; nickel; silver; sodium; trace metal; Article; binding affinity; bioaccumulation; biomineralization; black pigment stone; carbonate stone; cholesterol stone; composite cholesterol stone; gallstone; human; human tissue; long term exposure; mass spectrometry; mining; mixed cholesterol stone; pollution; priority journal; pure cholesterol stone; Spain",2-s2.0-85029602322
"Muromachi S., Miyamoto H., Ohmura R.","Solubility of Nitrogen Gas in Aqueous Solution of Tetra-n-Butylammonium Bromide",2017,"International Journal of Thermophysics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031778599&doi=10.1007%2fs10765-017-2310-y&partnerID=40&md5=1fa3cea003d5593a0e35bf46cddc54af","Semiclathrate hydrates are water-based host-guest compounds formed from aqueous solutions of ionic guest substances. These materials can greatly moderate formation pressures and temperatures from canonical gas hydrates. This is a significant advantage for industrial applications such as gas separation and storage. N 2 gas is a major component contained in various flue gases and is usually mixed with CO 2. Semiclathrate hydrates can separate these gases under moderate thermodynamic conditions. Tetra-n-butylammonium bromide (TBAB) is a widely used ionic guest substance. To develop the application technologies and their theoretical models, solubility data of N 2 gas in TBAB aqueous solutions are required. In this study, we report N 2 gas solubility measured by an absolute gravimetric method for the semiclathrate hydrate formation system of TBAB + H 2O + N 2. The measurement pressures, temperatures and TBAB mass fractions were 3 MPa, 5 MPa and 7 MPa, 292.15 K, 302.15 K and 307.15 K, and 0 (pure water), 0.10, 0.20, 0.32 and 0.40, respectively. The uncertainties were 0.056 MPa, 0.44 K and 0.00012 in mole fraction. Although the technical difficulty lays on measurements of small N 2 gas solubility by the absolute gravimetric method, our data implied the unique gas dissolution property of aqueous TBAB solution depending on the TBAB concentration. The aqueous TBAB solutions with mass fractions of 0.10 and 0.20 had similar N 2 gas solubility as that in pure water. With higher mass fractions, 0.32 and 0.40, the N 2 gas solubility slightly increased from that in pure water, which implies the salting-in effect of TBAB. © 2017, Springer Science+Business Media, LLC.","Aqueous solution; Nitrogen; Semiclathrate hydrate; Solubility; Tetra-n-butylammonium bromide","Digital storage; Gas hydrates; Gases; Hydration; Solubility; Solution mining; Solutions; Application technologies; Formation pressure; Gravimetric methods; Host-guest compound; Semi-clathrate hydrates; Technical difficulties; Tetra-n-butylammonium bromide; Thermodynamic conditions; Nitrogen",2-s2.0-85031778599
"Rojas C., Vargas I.T., Bruns M.A., Regan J.M.","Electrochemically active microorganisms from an acid mine drainage-affected site promote cathode oxidation in microbial fuel cells",2017,"Bioelectrochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029035015&doi=10.1016%2fj.bioelechem.2017.07.013&partnerID=40&md5=506cebcbb7a7027e9af8d42fdf481c74","The limited database of acidophilic or acidotolerant electrochemically active microorganisms prevents advancements on microbial fuel cells (MFCs) operated under low pH. In this study, three MFCs were used to enrich cathodic biofilms using acid mine drainage (AMD) sediments as inoculum. Linear sweep voltammetry showed cathodic current plateaus of 5.5 (± 0.7) mA at about − 170 mV vs Ag/AgCl and 8.5 (± 0.9) mA between − 500 mV to − 450 mV vs Ag/AgCl for biofilms developed on small graphite fiber brushes. After gamma irradiation, biocathodes exhibited a decrease in current density approaching that of abiotic controls. Electrochemical impedance spectroscopy showed six-fold lower charge transfer resistance with viable biofilm. Pyrosequencing data showed that Proteobacteria and Firmicutes dominated the biofilms. Acidithiobacillus representatives were enriched in some biocathodes, supporting the potential importance of these known iron and sulfur oxidizers as cathodic biocatalysts. Other acidophilic chemolithoautotrophs identified included Sulfobacillus and Leptospirillum species. The presence of chemoautotrophs was consistent with functional capabilities predicted by PICRUSt related to carbon fixation pathways in prokaryotic microorganisms. Acidophilic or acidotolerant heterotrophs were also abundant; however, their contribution to cathodic performance is unknown. This study directs subsequent research efforts to particular groups of AMD-associated bacteria that are electrochemically active on cathodes. © 2017 Elsevier B.V.","BESs; Biocathodes; Exoelectrotrophs; Iron oxidizers; Lithotrophs","Biofilms; Carbon; Cathodes; Charge transfer; Drainage; Electrochemical impedance spectroscopy; Electrodes; Fuel cells; Irradiation; Microorganisms; Silver; BESs; Biocathodes; Exoelectrotrophs; Iron-oxidizer; Lithotrophs; Microbial fuel cells; acid mine drainage; Acidithiobacillus; Article; biocatalyst; biocathode; biofilm; chemolithoautotroph; controlled study; current density; electricity; electrochemical impedance spectroscopy; electrochemistry; electrode; electron transport; epifluorescence microscopy; exoelectrotrophic microorganism; Firmicutes; gamma irradiation; Leptospirillum; microbial community; microbial fuel cell; nonhuman; organism forms; oxidation; potentiometry; Proteobacteria; pyrosequencing; Sulfobacillus; sulfur oxidizer; bacterium; bioenergy; biofilm; electrode; industrial waste; metabolism; microbiology; mining; oxidation reduction reaction; pH; industrial waste; Bacteria; Bioelectric Energy Sources; Biofilms; Electrochemistry; Electrodes; Hydrogen-Ion Concentration; Industrial Waste; Mining; Oxidation-Reduction",2-s2.0-85029035015
"Liu Y., Lu H., Poletto M., Guo X., Gong X.","Bulk flow properties of pulverized coal systems and the relationship between inter-particle forces and particle contacts",2017,"Powder Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029385138&doi=10.1016%2fj.powtec.2017.07.057&partnerID=40&md5=3f98337f077d69d1e45e9054157fba8a","Two groups of lignite powders characterised by different particle size distributions were prepared herein to investigate the effect of particle size distribution (PSD) on the flow properties of fine powders. The first group was composed of samples with a narrow PSD and prepared using an air classifier. The second group was made of two different samples of the same lignite material characterised by industrial-grade particle size distributions, which were much wider in range than the samples of the first group. The experimentally determined flow properties were used to understand the effect of the PSD. The packing properties and the flow behaviour of all the coal powder samples were characterised in terms of compressibility and flow properties using an FT4 powder flow rheometer. Furthermore, the Brunauer, Emmett and Teller surface areas and the dispersive surface energies were determined using a surface energy analyzer. The samples with similar mean particle sizes, but different particle size distributions, provided significantly different results. Accordingly, a micro-scale approach inspired by the Rumpf and Molerus approach was proposed to obtain a theoretical insight of the effect of the particle size distribution on the bulk flow properties of these powders. This approach was modified to account for wide particle size distributions. A procedure to estimate the tensile strength of the industrial lignite powders starting from the data relative to the narrow-size samples was also developed. The approach validity was demonstrated by the acceptable agreement between the values of the isostatic tensile strength for industrial pulverized coals estimated with the model application and the values directly calculated from the measured flow properties. The bulk flow properties of the industrial grade pulverized lignite were investigated based on the model results to highlight the most significant phenomena affecting the powder flow properties. © 2017 Elsevier B.V.","Adhesion force model; Industrial powder; Isostatic tensile strength; Particle size distribution; Pulverized coal","Coal; Interfacial energy; Light transmission; Lignite; Particle size analysis; Powders; Pulverized fuel; Size distribution; Tensile strength; Adhesion forces; Interparticle force; Mean particle size; Model application; Packing properties; Particle contacts; Pulverized coal systems; Pulverized coals; Particle size; coal; lignite; adhesion; Article; coal mining; compression; controlled study; dispersion; energy; flowmeter; force; particle size; powder flow; quantitative analysis; surface area; tensile strength",2-s2.0-85029385138
"Miguéis V.L., Camanho A.S., Borges J.","Predicting direct marketing response in banking: comparison of class imbalance methods",2017,"Service Business",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007610447&doi=10.1007%2fs11628-016-0332-3&partnerID=40&md5=ab3ed13007c962cdf60b11982691d74e","Customers’ response is an important topic in direct marketing. This study proposes a data mining response model supported by random forests to support the definition of target customers for banking campaigns. Class imbalance is a typical problem in telemarketing that can affect the performance of the data mining techniques. This study also contributes to the literature by exploring the use of class imbalance methods in the banking context. The performance of an undersampling method (the EasyEnsemble algorithm) is compared with that of an oversampling method (the Synthetic Minority Oversampling Technique) in order to determine the most appropriate specification. The importance of the attribute features included in the response model is also explored. In particular, discriminative performance was enhanced by the inclusion of demographic information, contact details and socio-economic features. Random forests, supported by an undersampling algorithm, presented very high prediction performance, outperforming the other techniques explored. © 2017, Springer-Verlag Berlin Heidelberg.","Class imbalance; Customer targeting; Direct marketing; Random forests; Response modelling",,2-s2.0-85007610447
"Parodi S., Dosi C., Zambon A., Ferrari E., Muselli M.","Identifying Environmental and Social Factors Predisposing to Pathological Gambling Combining Standard Logistic Regression and Logic Learning Machine",2017,"Journal of Gambling Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032567253&doi=10.1007%2fs10899-017-9679-1&partnerID=40&md5=4c999c8342eded444b07dc6deefd0b31","Identifying potential risk factors for problem gambling (PG) is of primary importance for planning preventive and therapeutic interventions. We illustrate a new approach based on the combination of standard logistic regression and an innovative method of supervised data mining (Logic Learning Machine or LLM). Data were taken from a pilot cross-sectional study to identify subjects with PG behaviour, assessed by two internationally validated scales (SOGS and Lie/Bet). Information was obtained from 251 gamblers recruited in six betting establishments. Data on socio-demographic characteristics, lifestyle and cognitive-related factors, and type, place and frequency of preferred gambling were obtained by a self-administered questionnaire. The following variables associated with PG were identified: instant gratification games, alcohol abuse, cognitive distortion, illegal behaviours and having started gambling with a relative or a friend. Furthermore, the combination of LLM and LR indicated the presence of two different types of PG, namely: (a) daily gamblers, more prone to illegal behaviour, with poor money management skills and who started gambling at an early age, and (b) non-daily gamblers, characterised by superstitious beliefs and a higher preference for immediate reward games. Finally, instant gratification games were strongly associated with the number of games usually played. Studies on gamblers habitually frequently betting shops are rare. The finding of different types of PG by habitual gamblers deserves further analysis in larger studies. Advanced data mining algorithms, like LLM, are powerful tools and potentially useful in identifying risk factors for PG. © 2017, Springer Science+Business Media New York.","Logic Learning Machine; Logistic regression; Problem gambling; ROC analysis",,2-s2.0-85032567253
"Jayanthi N., Babu B.V., Rao N.S.","Survey on clinical prediction models for diabetes prediction",2017,"Journal of Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028314925&doi=10.1186%2fs40537-017-0082-7&partnerID=40&md5=4ef312725e979355da1dc2a3137ffbb4","Predictive analytics has gained a lot of reputation in the emerging technology Big data. Predictive analytics is an advanced form of analytics. Predictive analytics goes beyond data mining. A huge amount of medical data is available today regarding the disease, their symptoms, reasons for illness, and their effects on health. But this data is not analysed properly to predict or to study a disease. The aim of this paper is to give a detailed version of predictive models from base to state-of-art, describing various types of predictive models, steps to develop a predictive model, their applications in health care in a broader way and particularly in diabetes. © 2017, The Author(s).","Clinical prediction models; Diabetes; Hybrid model; Machine learning; Predictive analytics; Traditional model",,2-s2.0-85028314925
"Zhang R.F., Kong X.F., Wang H.T., Zhang S.H., Legut D., Sheng S.H., Srinivasan S., Rajan K., Germann T.C.","An informatics guided classification of miscible and immiscible binary alloy systems",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028453232&doi=10.1038%2fs41598-017-09704-1&partnerID=40&md5=afa536a9873cf8bf5c94a597d8317b19","The classification of miscible and immiscible systems of binary alloys plays a critical role in the design of multicomponent alloys. By mining data from hundreds of experimental phase diagrams, and thousands of thermodynamic data sets from experiments and high-throughput first-principles (HTFP) calculations, we have obtained a comprehensive classification of alloying behavior for 813 binary alloy systems consisting of transition and lanthanide metals. Among several physics-based descriptors, the slightly modified Pettifor chemical scale provides a unique two-dimensional map that divides the miscible and immiscible systems into distinctly clustered regions. Based on an artificial neural network algorithm and elemental similarity, the miscibility of the unknown systems is further predicted and a complete miscibility map is thus obtained. Impressively, the classification by the miscibility map yields a robust validation on the capability of the well-known Miedema's theory (95% agreement) and shows good agreement with the HTFP method (90% agreement). Our results demonstrate that a state-of-the-art physics-guided data mining can provide an efficient pathway for knowledge discovery in the next generation of materials design. © 2017 The Author(s).",,,2-s2.0-85028453232
"Yu G., Yu X., Wang J.","Network-aided Bi-clustering for discovering cancer subtypes",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018187588&doi=10.1038%2fs41598-017-01064-0&partnerID=40&md5=b78b7bd0d7f762186d43dfcc0e3ebe0d","Bi-clustering is a widely used data mining technique for analyzing gene expression data. It simultaneously groups genes and samples of an input gene expression data matrix to discover bi-clusters that relevant samples exhibit similar gene expression profiles over a subset of genes. The discovered bi-clusters bring insights for categorization of cancer subtypes, gene treatments and others. Most existing bi-clustering approaches can only enumerate bi-clusters with constant values. Gene interaction networks can help to understand the pattern of cancer subtypes, but they are rarely integrated with gene expression data for exploring cancer subtypes. In this paper, we propose a novel method called Network-aided Bi-Clustering (NetBC). NetBC assigns weights to genes based on the structure of gene interaction network, and it iteratively optimizes sum-squared residue to obtain the row and column indicative matrices of bi-clusters by matrix factorization. NetBC can not only efficiently discover bi-clusters with constant values, but also bi-clusters with coherent trends. Empirical study on large-scale cancer gene expression datasets demonstrates that NetBC can more accurately discover cancer subtypes than other related algorithms. © 2017 The Author(s).",,"empiricism; gene expression; gene interaction; human; tumor gene",2-s2.0-85018187588
"Maity M., Dhane D., Mungle T., Maiti A.K., Chakraborty C.","Web-Enabled Distributed Health-Care Framework for Automated Malaria Parasite Classification: an E-Health Approach",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032464001&doi=10.1007%2fs10916-017-0834-0&partnerID=40&md5=f47b947a9ca2a12984c1d0f0da7f36b4","Web-enabled e-healthcare system or computer assisted disease diagnosis has a potential to improve the quality and service of conventional healthcare delivery approach. The article describes the design and development of a web-based distributed healthcare management system for medical information and quantitative evaluation of microscopic images using machine learning approach for malaria. In the proposed study, all the health-care centres are connected in a distributed computer network. Each peripheral centre manages its’ own health-care service independently and communicates with the central server for remote assistance. The proposed methodology for automated evaluation of parasites includes pre-processing of blood smear microscopic images followed by erythrocytes segmentation. To differentiate between different parasites; a total of 138 quantitative features characterising colour, morphology, and texture are extracted from segmented erythrocytes. An integrated pattern classification framework is designed where four feature selection methods viz. Correlation-based Feature Selection (CFS), Chi-square, Information Gain, and RELIEF are employed with three different classifiers i.e. Naive Bayes’, C4.5, and Instance-Based Learning (IB1) individually. Optimal features subset with the best classifier is selected for achieving maximum diagnostic precision. It is seen that the proposed method achieved with 99.2% sensitivity and 99.6% specificity by combining CFS and C4.5 in comparison with other methods. Moreover, the web-based tool is entirely designed using open standards like Java for a web application, ImageJ for image processing, and WEKA for data mining considering its feasibility in rural places with minimal health care facilities. © 2017, Springer Science+Business Media, LLC.","Computer-aided diagnosis; Electronic healthcare system; Feature extraction; Feature selection; Malaria screening; Supervised classification","accuracy; algorithm; Article; blood smear; classifier; computer security; erythrocyte; health care management; health service; human; image quality; learning algorithm; light exposure; malaria; medical information; microscopy; noise; Plasmodium; quantitative analysis; sensitivity and specificity; signal noise ratio; telehealth",2-s2.0-85032464001
"Garstone R.A., Gill C., Moliere D., Yang D., Bende-Michl U., Fiddes P.","Accounting for water in the minerals industry: Capitalising on regulatory reporting",2017,"Water Resources and Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027710870&doi=10.1016%2fj.wri.2017.07.002&partnerID=40&md5=fe9fd8b4762b4dbd963740908e138af0","Australia has been rapidly advancing the field of water accounting as a tool to improve water management across the country. Water accounting is the application of a consistent and structured approach to identify, measure and report water resource information. The Bureau of Meteorology (the Bureau) has developed Australian Water Accounting Standards for General Purpose Water Accounting Reports.Following collaboration between the Bureau and the Newmarket Gold Mining Company, this paper investigates how General Purpose Water Account Reporting can be applied and used in the minerals industry to simplify and improve aspects of regulatory reporting. This case study demonstrates how General Purpose Water Accounting Reports and the lessons learned from the ongoing development of the Australian National Water Account can be practically applied to regulatory reporting and corporate data management for a mining operation in the Australian Northern Territory. This paper also demonstrates the benefits of aligning a standardised water account with data that is already routinely collected as part of mining operations environmental compliance. © 2017 The Authors.","General purpose water accounting; Mine site regulatory reporting; National Water Account; National Water Initiative; Sustainability reporting; Water resource accounting","Economic geology; Information management; Sustainable development; Water management; Mine site; National Water Account; National Water Initiative; Resource accountings; Sustainability reporting; Water accountings; Water resources",2-s2.0-85027710870
"Campbell M.J., Trump D.L.","Vitamin D Receptor Signaling and Cancer",2017,"Endocrinology and Metabolism Clinics of North America",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030331193&doi=10.1016%2fj.ecl.2017.07.007&partnerID=40&md5=10bea08d506c175e49c21a4aba3a904d","The vitamin D receptor (VDR) binds the secosteroid hormone 1,25(OH)2D3 with high affinity and regulates gene programs that control a serum calcium levels, as well as cell proliferation and differentiation. A significant focus has been to exploit the VDR in cancer settings. Although preclinical studies have been strongly encouraging, to date clinical trials have delivered equivocal findings that have paused the clinical translation of these compounds. However, it is entirely possible that mining of genomic data will help to refine precisely what are the key anticancer actions of vitamin D compounds and where these can be used most effectively. © 2017 The Author(s)","Chemoprevention; Chemotherapy; Genomics; TCGA; Vitamin D","colecalciferol; transcriptome; vitamin D receptor; apoptosis; biological activity; cancer epidemiology; cancer incidence; cell kinetics; cell proliferation; clinical trial (topic); disease association; human; malignant neoplasm; metastasis; nonhuman; preclinical study; priority journal; protein analysis; protein expression; protein function; Review; signal transduction; tumor escape; tumor invasion; upregulation; vitamin blood level",2-s2.0-85030331193
"Kumaresan D., Cross A.T., Moreira-Grez B., Kariman K., Nevill P., Stevens J., Allcock R.J.N., O'Donnell A.G., DIxon K.W., Whiteley A.S.","Microbial Functional Capacity Is Preserved Within Engineered Soil Formulations Used in Mine Site Restoration",2017,"Scientific Reports",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017141062&doi=10.1038%2fs41598-017-00650-6&partnerID=40&md5=2447e0ea61570063392c02391b7eeed8","Mining of mineral resources produces substantial volumes of crushed rock based wastes that are characterised by poor physical structure and hydrology, unstable geochemistry and potentially toxic chemical conditions. Recycling of these substrates is desirable and can be achieved by blending waste with native soil to form a 'novel substrate' which may be used in future landscape restoration. However, these post-mining substrate based 'soils' are likely to contain significant abiotic constraints for both plant and microbial growth. Effective use of these novel substrates for ecosystem restoration will depend on the efficacy of stored topsoil as a potential microbial inoculum as well as the subsequent generation of key microbial soil functions originally apparent in local pristine sites. Here, using both marker gene and shotgun metagenome sequencing, we show that topsoil storage and the blending of soil and waste substrates to form planting substrates gives rise to variable bacterial and archaeal phylogenetic composition but a high degree of metabolic conservation at the community metagenome level. Our data indicates that whilst low phylogenetic conservation is apparent across substrate blends we observe high functional redundancy in relation to key soil microbial pathways, allowing the potential for functional recovery of key belowground pathways under targeted management. © 2017 The Author(s).",,"ecosystem restoration; functional status; inoculation; marker gene; metagenome; soil; storage",2-s2.0-85017141062
"Schoening T., Jones D.O.B., Greinert J.","Compact-Morphology-based poly-metallic Nodule Delineation",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031769823&doi=10.1038%2fs41598-017-13335-x&partnerID=40&md5=22b55025d30a4043a8f7ff157ba53c36","Poly-metallic nodules are a marine resource considered for deep sea mining. Assessing nodule abundance is of interest for mining companies and to monitor potential environmental impact. Optical seafloor imaging allows quantifying poly-metallic nodule abundance at spatial scales from centimetres to square kilometres. Towed cameras and diving robots acquire high-resolution imagery that allow detecting individual nodules and measure their sizes. Spatial abundance statistics can be computed from these size measurements, providing e.g. seafloor coverage in percent and the nodule size distribution. Detecting nodules requires segmentation of nodule pixels from pixels showing sediment background. Semi-supervised pattern recognition has been proposed to automate this task. Existing nodule segmentation algorithms employ machine learning that trains a classifier to segment the nodules in a high-dimensional feature space. Here, a rapid nodule segmentation algorithm is presented. It omits computation-intense feature-based classification and employs image processing only. It exploits a nodule compactness heuristic to delineate individual nodules. Complex machine learning methods are avoided to keep the algorithm simple and fast. The algorithm has successfully been applied to different image datasets. These data sets were acquired by different cameras, camera platforms and in varying illumination conditions. Their successful analysis shows the broad applicability of the proposed method. © 2017 The Author(s).",,"classification; classifier; diving; human; human experiment; illumination; image processing; imagery; morphology; pattern recognition; robotics; sediment; statistics",2-s2.0-85031769823
"Jia Z.-Y., Shen T.-Y., Jiang W., Qin H.-L.","Identification of molecular mechanisms of glutamine in pancreatic cancer",2017,"Oncology Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032708473&doi=10.3892%2fol.2017.7068&partnerID=40&md5=a39e2da0f864cb0e153a1ebf291f1cdd","The aim of the present study was to explore the critical genes and molecular mechanisms in pancreatic cancer (PC) cells with glutamine. By analyzing microarray data GSE17632 from the Gene Expression Omnibus database, the DEGs between PC cells treated with glutamine and without glutamine were evaluated. Additionally, function enrichment analyses and protein-protein interaction (PPI) network construction of DEGs were performed. Network module and literature mining analyses were performed to analyze the critical DEGs in PC cells. In total, 495 genes were selected as DEGs between control and glutamine cells in PC. These DEGs were mainly enriched in several Gene Ontology (GO) terms in biological process, cellular components and molecular function. Additionally, they were also enriched in certain pathways, including metabolic pathways and the Janus kinase-signal transducer and activator of transcription (JAK-STAT) signaling pathway. MYC, heat shock 70kDa protein 5 (HSPA5), interleukin 8 (IL8), and chemokine (C-X-C motif) receptor 4 (CXCR4) were hub genes in the PPI network. Furthermore, two sub-network modules of PPI network and two co-occurrence networks were obtained. The DEGs of MYC, HSPA5, IL18 and CXCR4 may exert important roles in molecular mechanisms of PC cells with glutamine. © 2017, Spandidos Publications. All rights reserved.","Differentially-expressed gene; Enrichment analysis; Glutamine; Pancreatic cancer; Protein-protein interaction network",,2-s2.0-85032708473
"Krochmal M., Cisek K., Filip S., Markoska K., Orange C., Zoidakis J., Gakiopoulou C., Spasovski G., Mischak H., Delles C., Vlahou A., Jankowski J.","Identification of novel molecular signatures of IgA nephropathy through an integrative -omics analysis",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027871454&doi=10.1038%2fs41598-017-09393-w&partnerID=40&md5=fb3879ad647167493380fd9fb15472e7","IgA nephropathy (IgAN) is the most prevalent among primary glomerular diseases worldwide. Although our understanding of IgAN has advanced significantly, its underlying biology and potential drug targets are still unexplored. We investigated a combinatorial approach for the analysis of IgAN-relevant -omics data, aiming at identification of novel molecular signatures of the disease. Nine published urinary proteomics datasets were collected and the reported differentially expressed proteins in IgAN vs. healthy controls were integrated into known biological pathways. Proteins participating in these pathways were subjected to multi-step assessment, including investigation of IgAN transcriptomics datasets (Nephroseq database), their reported protein-protein interactions (STRING database), kidney tissue expression (Human Protein Atlas) and literature mining. Through this process, from an initial dataset of 232 proteins significantly associated with IgAN, 20 pathways were predicted, yielding 657 proteins for further analysis. Step-wise evaluation highlighted 20 proteins of possibly high relevance to IgAN and/or kidney disease. Experimental validation of 3 predicted relevant proteins, adenylyl cyclase-associated protein 1 (CAP1), SHC-transforming protein 1 (SHC1) and prolylcarboxypeptidase (PRCP) was performed by immunostaining of human kidney sections. Collectively, this study presents an integrative procedure for -omics data exploitation, giving rise to biologically relevant results. © 2017 The Author(s).",,,2-s2.0-85027871454
"Gomes S.S., Araújo M.F., Monge Soares A.M., Correia V.H.","Provenance evidence for Roman lead artefacts of distinct chronology from Portuguese archaeological sites",2017,"Journal of Archaeological Science: Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030834894&doi=10.1016%2fj.jasrep.2017.10.002&partnerID=40&md5=1377d5839947caba00754ff9a4c28661","In the present study, a set of 24 glandes plumbeae found at Alto dos Cacos, a Roman Republican military camp located in the Tagus valley, Portugal, was analysed by a quadrupole based ICP-MS to determine the tin (Sn) content and lead (Pb) isotope ratios. Results were compared with similar data previously obtained for fistulae plumbeae aquariae from Conimbriga, an important Lusitanian Roman centre during the Empire. Low Sn contents (≤ 0.01 wt%) were observed in 25% of glandes plumbeae indicating that were probably made with non-recycled lead. A similar situation was perceived for the set of fistulae aquariae, although most of the remaining fistulae present systematically higher Sn concentrations than those of glandes suggesting that lead recycling increased during the Empire. Pb isotope ratios distribution differentiated the analysed samples into two distinct groups: one composed by most of glandes plumbeae (15) and the other by the remaining glandes plumbeae (9) and all fistulae aquariae. The comparison with Pb isotope ratios of the published data for several lead ore deposits, exploited by the Roman in Iberian Peninsula, suggests that lead used in the manufacture of most of the glandes plumbeae would come from Linares-La Carolina, Alcudia Valley and Ossa Morena Zone. Also, some glandes could have been made using these ores, probably mixed with lead ores from Gallia Narbonensis (Southern France) or from Sardinia in the Mediterranean region. On the other hand, lead used in most fistulae aquariae came from Iberian mines, namely from Sierra Morena (Alcudia Valley and Linares-La Carolina mines) and Ossa Morena mining district, although in some cases, probably mixed with lead from the Iberian Pyrite Belt. © 2017 Elsevier Ltd","Fistulae aquariae; Glandes plumbeae; Hispania Ulterior; Lusitania; Pb isotope ratios; Sn content",,2-s2.0-85030834894
"Baldim J.L., Da Silva B.L., Chagas-Paula D.A., Lago J.H.G., Soares M.G.","A strategy for the identification of patterns in the biosynthesis of nonribosomal peptides by Betaproteobacteria species",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028844115&doi=10.1038%2fs41598-017-11314-w&partnerID=40&md5=5b08b13f148705cd6dbb47be75ef4e0d","Nonribosomal peptides have an important pharmacological role due to their extensive biological properties. The singularities in the biosynthesis of these natural products allowed the development of genome-mining strategies which associate them to their original biosynthetic gene clusters. Generally, these compounds present complex architectures that make their identification difficult. Based on these evidences, genomes from species of the class Betaproteobacteria were studied with the purpose of finding biosynthetic similarities ï""¿among themï""¿. These organisms were applied as templates due to their large number of biosynthetic gene clusters and the natural products isolated from them. The strategy for Rapid Identification of Nonribosomal Peptides Portions (RINPEP) proposed in this work was built by reorganizing the data obtained from antiSMASH and NCBI with a product-centered way. The verification steps of RINPEP comprehended the fragments of existent compounds and predictions obtained in silico with the purpose of finding common subunits expressed by different genomic sequences. The results of this strategy revealed patterns in a global overview of the biosynthesis of nonribosomal peptides by Betaproteobacteria. © 2017 The Author(s).",,,2-s2.0-85028844115
"Zheng L., Chen Y., Ye L., Jiao W., Song H., Mei H., Li D., Yang F., Li H., Huang K., Tong Q.","MiRNA-584-3p inhibits gastric cancer progression by repressing Yin Yang 1- facilitated MMP-14 expression",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029378578&doi=10.1038%2fs41598-017-09271-5&partnerID=40&md5=5124f93af3171f7865167a80379011f7","Recent evidence shows the emerging roles of promoter-targeting endogenous microRNAs (miRNAs) in regulating gene transcription. However, miRNAs affecting the transcription of matrix metalloproteinase 14 (MMP-14) in gastric cancer remain unknown. Herein, through integrative mining of public datasets, we identified the adjacent targeting sites of Yin Yang 1 (YY1) and miRNA-584-3p (miR-584-3p) within MMP-14 promoter. We demonstrated that YY1 directly targeted the MMP-14 promoter to facilitate its expression in gastric cancer cells. In contrast, miR-584-3p recognized its complementary site within MMP-14 promoter to suppress its expression. Mechanistically, miR-584-3p interacted with Argonaute 2 to recruit enhancer of zeste homolog 2 and euchromatic histone lysine methyltransferase 2, resulting in enrichment of repressive epigenetic markers and decreased binding of YY1 to MMP-14 promoter. miR-584-3p inhibited the in vitro and in vivo tumorigenesis and aggressiveness of gastric cancer cells through repressing YY1-facilitated MMP-14 expression. In clinical gastric cancer tissues, the expression of YY1 and miR-584-3p was positively or negatively correlated with MMP-14 levels. In addition, miR-584-3p and YY1 were independent prognostic factors associated with favorable and unfavorable outcome of gastric cancer patients, respectively. These data demonstrate that miR-584-3p directly targets the MMP-14 promoter to repress YY1-facilitated MMP-14 expression and inhibits the progression of gastric cancer. © 2017 The Author(s).",,,2-s2.0-85029378578
"López-Blanco C., Kenney W.F., Varas A.","Recent flood management efforts obscure the climate signal in a sediment record from a tropical lake",2017,"Journal of Paleolimnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031128130&doi=10.1007%2fs10933-017-0004-x&partnerID=40&md5=898a6ab8e97b53a220b977c1e9f2eca7","Lake sediment cores contain useful archives of severe past climate events (e.g. droughts, hurricanes, dust storms) and patterns of global climate change. Most such records come from higher latitudes in the Northern Hemisphere. Here we present sediment core data from shallow, equatorial Lake La Tembladera on the southern coast of Ecuador, which contains a &gt; 100-year record of flooding events. We used geochemical properties of a 210Pb-dated sediment core to identify two distinct sedimentary facies. Facies one represents lake-derived deposits and is characterized by sediments with relatively higher organic matter and nitrogen content. Facies two represents flood events and is characterized by sediments with greater concentrations of inorganic carbon and terrigenous elements Al and Ti. Pollen from introduced Pinus sp. and Hg associated with recent gold mining provided independent time-markers in the sediment to validate the 210Pb chronology. The comparisons of the geochemical signal with the ENSO reconstructions revealed that the sediments contain a poor signal of ENSO from AD 1930–1965. The weak ENSO signal may result from minimal climate sensitivity in the lake or early water control efforts (e.g. rudimentary damming). However, from AD 1965, the climate signal is totally masked by recent and documented anthropogenic works such as dams and canals. This record shows how anthropogenic efforts to minimize societal impacts of ENSO floods have masked the natural climate signal and nicely mirrored recent trends between climate and anthropogenic activities in tropical latitudes. © 2017, Springer Science+Business Media B.V.","210Pb; Geochemistry; Human impact; Hydrologic alterations; Sedimentary facies; Shallow lakes",,2-s2.0-85031128130
"Peng L., Zhu W., Liao B., Duan Y., Chen M., Chen Y., Yang J.","Screening drug-target interactions with positive-unlabeled learning",2017,"Scientific Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027437083&doi=10.1038%2fs41598-017-08079-7&partnerID=40&md5=34d34ec55ad913140f33abcbdaa3bd61","Identifying drug-target interaction (DTI) candidates is crucial for drug repositioning. However, usually only positive DTIs are deposited in known databases, which challenges computational methods to predict novel DTIs due to the lack of negative samples. To overcome this dilemma, researchers usually randomly select negative samples from unlabeled drug-target pairs, which introduces a lot of false-positives. In this study, a negative sample extraction method named NDTISE is first developed to screen strong negative DTI examples based on positive-unlabeled learning. A novel DTI screening framework, PUDTI, is then designed to infer new drug repositioning candidates by integrating NDTISE, probabilities that remaining ambiguous samples belong to the positive and negative classes, and an SVM-based optimization model. We investigated the effectiveness of NDTISE on a DTI data provided by NCPIS. NDTISE is much better than random selection and slightly outperforms NCPIS. We then compared PUDTI with 6 state-of-the-art methods on 4 classes of DTI datasets from human enzymes, ion channels, GPCRs and nuclear receptors. PUDTI achieved the highest AUC among the 7 methods on all 4 datasets. Finally, we validated a few top predicted DTIs through mining independent drug databases and literatures. In conclusion, PUDTI provides an effective pre-filtering method for new drug design. © 2017 The Author(s).",,,2-s2.0-85027437083
"Hao Y., Xie Y., Ma J., Zhang W.","The critical role of local policy effects in arid watershed groundwater resources sustainability: A case study in the Minqin oasis, China",2017,"Science of the Total Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020193515&doi=10.1016%2fj.scitotenv.2017.04.177&partnerID=40&md5=bbe5137ea5586f20fe36beef48378807","Designed as a watershed groundwater restoration policy (WGRP), the Comprehensive Treatment Program of the Shiyang River Basin (CTSRB) was launched in 2006 to restore the groundwater resources in the Minqin oasis, northwestern China. This study sought to verify the recovery effects of CTSRB implementation from the perspective of groundwater depth. We reconstructed the spatio-temporal distribution of groundwater depth at interannual and pixel scales by using digital groundwater depth models (DGDMs), based on the ordinary kriging interpolation method. Using DGDMs data, various measures of the groundwater table (e.g., regional depths, surface areas, depletion cones, and conditions in irrigated regions including Ba, Quanshan, and Hu) were quantitatively analyzed and compared for the pre-CTSRB (2001–2006), CTSRB I (2006–2010), and CTSRB II (2010–2015) periods, for which spatial trends in the annual amplitudes of groundwater depth were compared. Finally, strategies that impacted the groundwater behavior before and during the CTSRB periods, possible indirect and adverse effects, and long-term strategies and prospects were discussed. The results showed that groundwater depth first declined sharply, before increasing slowly and stabilizing after implementation of the CTSRB. Areas of greater groundwater depth (<− 20 m) and four groundwater depletion cones expanded during the pre-CTSRB period, whereas variable shrinking trends were detected during the CTSRB period. Spatial analysis showed that groundwater recovery mainly occurred along the periphery of the three irrigated regions, among which recovery effects in Hu were more obvious than those in Quanshan and Ba, with pumping-well densities the main reason for the difference. Therefore, various strategies (increasing the surface water supply, reducing groundwater mining, and some other auxiliary measures) of CTSRB together supported groundwater recovery in the Minqin oasis. Overall, this research demonstrates an innovative perspective to verify the effects of WGRPs in arid and semi-arid areas. © 2017","Comprehensive treatment program of the Shiyang River basin (CTSRB); Ecological restoration; Groundwater; Minqin oasis; Spatio-temporal dynamics","Climate change; Groundwater; Interpolation; Recovery; Restoration; Surface waters; Water supply; Watersheds; Arid and semi-arid areas; Ecological restoration; Ground water depths; Groundwater depletion cone; Minqin oasis; Shiyang river basins; Spatio-temporal dynamics; Spatiotemporal distributions; Groundwater resources; ground water; surface water; groundwater resource; kriging; restoration ecology; spatial distribution; sustainability; temporal distribution; water depth; watershed; Article; case study; China; controlled study; environmental policy; environmental sustainability; irrigation (agriculture); priority journal; quantitative analysis; semiarid climate; water supply; water table; watershed management; China; Gansu; Minqin Oasis; Shiyang Basin",2-s2.0-85020193515
"Márquez-Chamorro A.E., Resinas M., Ruiz-Cortés A., Toro M.","Run-time prediction of business process indicators using evolutionary decision rules",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020486298&doi=10.1016%2fj.eswa.2017.05.069&partnerID=40&md5=f384f532844961ec42723f6900dc178f","Predictive monitoring of business processes is a challenging topic of process mining which is concerned with the prediction of process indicators of running process instances. The main value of predictive monitoring is to provide information in order to take proactive and corrective actions to improve process performance and mitigate risks in real time. In this paper, we present an approach for predictive monitoring based on the use of evolutionary algorithms. Our method provides a novel event window-based encoding and generates a set of decision rules for the run-time prediction of process indicators according to event log properties. These rules can be interpreted by users to extract further insight of the business processes while keeping a high level of accuracy. Furthermore, a full software stack consisting of a tool to support the training phase and a framework that enables the integration of run-time predictions with business process management systems, has been developed. Obtained results show the validity of our proposal for two large real-life datasets: BPI Challenge 2013 and IT Department of Andalusian Health Service (SAS). © 2017 Elsevier Ltd","Business process indicator; Business process management; Evolutionary algorithm; Predictive monitoring; Process mining","Administrative data processing; Data mining; Enterprise resource management; Forecasting; Business Process; Business process management; Business process management systems; Corrective actions; Predictive monitoring; Process indicators; Process mining; Process performance; Evolutionary algorithms",2-s2.0-85020486298
"Shang Z., Cai W., Cao Y., Wang F., Wang Z., Lu J., Zhang J.","An integrated strategy for rapid discovery and identification of the sequential piperine metabolites in rats using ultra high-performance liquid chromatography/high resolution mass spectrometery",2017,"Journal of Pharmaceutical and Biomedical Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029423902&doi=10.1016%2fj.jpba.2017.09.012&partnerID=40&md5=37d3b3fede24c74095025055229fcd7b","Piperine, one of the major bioactive constituents isolated from natural flavorings and medicinal-culinary herbs, possesses various biological activities. In the present study, an integrated strategy based on ultra high-performance liquid chromatography/high resolution mass spectrometry was established to reveal piperine metabolism in rats. First of all, post-acquisition data-mining methods, including high resolution extracted ion chromatograms (HREICs) and multiple mass defect filtering (MMDF), were used to screen piperine metabolite candidates in a full-scan HRMS1 level. Then, parent ion list-dynamic exclusion coupled with data-dependent data-acquisition method was utilized to acquire MSn datasets. In addition, the established reverse molecular assembly (RMA) approach based on paired diagnostic product ions (pDPIs) coupled with neutral loss fragments (NLFs) was used to ascertain and identify the major-to-trace piperine metabolites efficiently. And then, the calculated ClogP values were utilized to distinguish the positional isomers. As a result, a total of 148 piperine metabolites were detected and characterized tentatively. The results demonstrated that piperine mainly underwent hydrogenation, dehydrogenation, hydroxylation, glucuronide conjugation, sulfate conjugation, ring-cleavage, and their composite reactions. Our results not only provided novel and useful data to better understand the safety, toxicity and efficacy of this potential therapeutic agent, but also indicated that the proposed strategy was reliable for a rapid discovery and identification drug-related constituents in vivo. © 2017","Metabolic profile; Multiple data processing methods; Piperine; Reverse molecular assembly strategy; Ultra high-performance liquid chromatography/high resolution mass spectrometry","glucuronide; piperine; animal experiment; Article; controlled study; dehydrogenation; demethylation; drug identification; drug metabolism; electrospray mass spectrometry; high performance liquid chromatography; hydrogenation; hydroxylation; isomer; liquid chromatography-mass spectrometry; male; nonhuman; priority journal; rat; ultra performance liquid chromatography",2-s2.0-85029423902
"Kim H.K., Kim H., Cho S.","Bag-of-concepts: Comprehending document representation through clustering words in distributed representation",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019749668&doi=10.1016%2fj.neucom.2017.05.046&partnerID=40&md5=52915122594b486a2ba6e0fc8877a6f2","Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate weighting scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others. © 2017 Elsevier B.V.","Bag-of-concepts; Interpretable document representation; Word2vec clustering","Data mining; Decision trees; Natural language processing systems; Text processing; Vectors; Bag-of-concepts; Curse of dimensionality; Distributed representation; Document Representation; Interpretability; Inverse Document Frequency; Neural network model; Word2vec clustering; Inverse problems; Article; artificial neural network; classifier; controlled study; data mining; decision tree; measurement accuracy; priority journal; probability; qualitative analysis; quantitative analysis",2-s2.0-85019749668
"Hayashi N., Watanabe S.","Upper bound of Bayesian generalization error in non-negative matrix factorization",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019636753&doi=10.1016%2fj.neucom.2017.04.068&partnerID=40&md5=9948cee7e65ad199b32eb08b07d01afa","Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used for text mining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied. © 2017 Elsevier B.V.","Bayesian learning; Non-negative matrix factorization (NMF); Real log canonical threshold (RLCT)","Data mining; Errors; Factorization; Learning systems; Signal processing; Bayesian learning; Generalization Error; Knowledge discovery method; Matrix factorizations; Nonnegative matrix factorization; Optimization method; Real log canonical threshold (RLCT); Statistical modeling; Matrix algebra; analytical error; Article; Bayesian learning; bioinformatics; data mining; factor analysis; machine learning; mathematical analysis; priority journal; signal processing; statistical model",2-s2.0-85019636753
"de Ville N., Le H.M., Schmidt L., Verbanck M.A.","Data-mining analysis of in-sewer infiltration patterns: seasonal characteristics of clear water seepage into Brussels main sewers",2017,"Urban Water Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028765380&doi=10.1080%2f1573062X.2017.1363252&partnerID=40&md5=478f3779e79b1659123812d59507df5b","Parasitic clear water infiltration is known to increase the waste water volumes in most sewerage systems. Amongst others, a problem arising from that is a significant variation of waste water pollutant concentration over time, which complicates the purification process and increases its cost. Capitalizing on new extensive databases, we propose a new method for the identification of clear water infiltration rates based on data-mining and data consolidation of long time data series. Based on a straightforward anthropogenic tracer, together with a simple but rigorous water budget, the infiltrated volumes are quantified day-by-day for the entire zone treated by a major waste water treatment plant. Brussels city is used as an example of the applicability of the method over several years, demonstrating the significant seasonal changes in sewer infiltration rates in the area and the progress achieved so far by structural sewer repair. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","anthropogenic tracer; clear water seepage; combined sewers; Infiltration/Inflow; water balance","anthropogenic source; data mining; infiltration; inflow; seepage; sewer network; wastewater treatment; water budget; Belgium; Brussels [Belgium]",2-s2.0-85028765380
"Le T., Stahl F., Gaber M.M., Gomes J.B., Fatta G.D.","On expressiveness and uncertainty awareness in rule-based classification for data streams",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021256217&doi=10.1016%2fj.neucom.2017.05.081&partnerID=40&md5=f099558d00fd869e52b4216fc39e28a8","Mining data streams is a core element of Big Data Analytics. It represents the velocity of large datasets, which is one of the four aspects of Big Data, the other three being volume, variety and veracity. As data streams in, models are constructed using data mining techniques tailored towards continuous and fast model update. The Hoeffding Inequality has been among the most successful approaches in learning theory for data streams. In this context, it is typically used to provide a statistical bound for the number of examples needed in each step of an incremental learning process. It has been applied to both classification and clustering problems. Despite the success of the Hoeffding Tree classifier and other data stream mining methods, such models fall short of explaining how their results (i.e., classifications) are reached (black boxing). The expressiveness of decision models in data streams is an area of research that has attracted less attention, despite its paramount of practical importance. In this paper, we address this issue, adopting Hoeffding Inequality as an upper bound to build decision rules which can help decision makers with informed predictions (white boxing). We termed our novel method Hoeffding Rules with respect to the use of the Hoeffding Inequality in the method, for estimating whether an induced rule from a smaller sample would be of the same quality as a rule induced from a larger sample. The new method brings in a number of novel contributions including handling uncertainty through abstaining, dealing with continuous data through Gaussian statistical modelling, and an experimentally proven fast algorithm. We conducted a thorough experimental study using benchmark datasets, showing the efficiency and expressiveness of the proposed technique when compared with the state-of-the-art. © 2017 The Authors","Abstaining; Big Data Analytics; Classification; Data Stream Mining; Expressiveness; Modular classification rule induction","Classification (of information); Data communication systems; Data mining; Education; Knowledge engineering; Trees (mathematics); Uncertainty analysis; Abstaining; Data analytics; Data stream mining; Expressiveness; Modular classification rule inductions; Big data; algorithm; Article; awareness; classification; classifier; conation; data mining; experimental study; expressiveness; human; information processing; learning theory; prediction; priority journal; sample size; statistical model; uncertainty",2-s2.0-85021256217
"Beyaz A., Özkaya M.T., İçen D.","Identification of some spanish olive cultivars using image processing techniques",2017,"Scientia Horticulturae",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024492882&doi=10.1016%2fj.scienta.2017.06.041&partnerID=40&md5=b87cf041036ae935bc5c6ad3af0e8f29","The aim of this research was to identify some Spanish olive cultivars using image processing techniques. For this purpose, Lechin De Granada, Arbequina, Picual, Verdial De V-M, Picudo, Hojiblanca and Empeltre Olive cultivars were identified utilizing the image processing and analysis techniques. Therefore, images of olives taken as 300 dpi with the 2896 × 1944 pixels, were captured using a DSLR camera, and evaluations of pixels were used for considering the pixel distribution and dimension measurements. LabVIEW Vision Assistant v2013 (NI) and Image j (NIH) software were used for image analysis procedures. Artificial Neural Network analysis were used to assess information of the length, width and color data results obtained from the fruits and stones (olive stones). All cultivars were identified. In addition, different classification techniques were applied to the olive stone and fruit data with the help of SPSS v22. Clementine v12 was used as a data mining software package from SPSS. The cultivars were identified 90% from dimensions with Artificial Neural Networks. © 2017 Elsevier B.V.","Artificial vision; Image analysis; Olive fruit; Olive stone; Varietal identification","agricultural technology; analytical method; artificial neural network; color; computer vision; cultivar; data mining; evergreen tree; fruit; identification method; image; image analysis; image classification; image processing; pixel; software; Citrus reticulata; Oleaceae",2-s2.0-85024492882
"Cho M.-H., Yoo J.S.","Exploring online students’ self-regulated learning with self-reported surveys and log files: a data mining approach",2017,"Interactive Learning Environments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990247985&doi=10.1080%2f10494820.2016.1232278&partnerID=40&md5=7edb937e2ced4020c27c9f4410143816","Many researchers who are interested in studying students’ online self-regulated learning (SRL) have heavily relied on self-reported surveys. Data mining is an alternative technique that can be used to discover students’ SRL patterns from large data logs saved on a course management system. The purpose of this study was to identify students’ online SRL patterns with the use of data mining techniques. We examined both self-reported self-regulation surveys and log files to predict online students’ achievements and found using log files was more powerful in predicting students’ achievements in an online course than self-reported survey data. Discussions to enhance teaching and learning practices with the use of data mining are provided. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","Data mining; log files; online learning; self-regulated learning; self-reported surveys",,2-s2.0-84990247985
"Fuentes-Cobas L.E., Chateigner D., Fuentes-Montero M.E., Pepponi G., Grazulis S.","The representation of coupling interactions in the Material Properties Open Database (MPOD)",2017,"Advances in Applied Ceramics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027852993&doi=10.1080%2f17436753.2017.1343782&partnerID=40&md5=135092075b8ad9deec0afb95b2a69b08","The Material Properties Open Database (MPOD, http://mpod.cimav.edu.mx) is a functional element of the web-based open databases system linked with crystallography. MPOD delivers single-crystal tensor properties in several representations, ranging from numerical matrices to 3D printing. Longitudinal moduli surfaces can be displayed in computers as well as in smart cell phones. Properties are stored as ‘.mpod’ files. IUCr formatting standards (CIF) are followed. The original published paper containing the data is cited. Structural and experimental information is also registered and linked. ‘Coupling properties’, say piezo-effects and magnetoelectricity, represent interactions linking different subsystems in a material. Currently, piezoelectricity occupies a significant fraction of cases in MPOD. The implications of crystal symmetry in piezoelectricity are systematically taken into account. Matrices’ elements and longitudinal moduli surfaces are checked for consistency with the Neumann principle. The inclusion of magnetoelectric axial tensors introduces exciting features into MPOD. © 2017 Institute of Materials, Minerals and Mining. Published by Taylor & Francis on behalf of the Institute.","anisotropy; Crystal properties; Neumann principle; open database","3D printers; Anisotropy; Crystallography; Database systems; Mobile phones; Piezoelectric devices; Piezoelectricity; Single crystals; Tensors; Coupling interaction; Coupling properties; Crystal properties; Formatting standards; Functional elements; Longitudinal modulus; Neumann; Numerical matrices; Crystal symmetry",2-s2.0-85027852993
"Umetani S.","Exploiting variable associations to configure efficient local search algorithms in large-scale binary integer programs",2017,"European Journal of Operational Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020041852&doi=10.1016%2fj.ejor.2017.05.025&partnerID=40&md5=c5ec7127b8403ace79f0798989db1fe7","We present a data mining approach for reducing the search space of local search algorithms in a class of binary integer programs including the set covering and partitioning problems. The quality of locally optimal solutions typically improves if a larger neighborhood is used, while the computation time of searching the neighborhood increases exponentially. To overcome this, we extract variable associations from the instance to be solved in order to identify promising pairs of flipping variables in the neighborhood search. Based on this, we develop a 4-flip neighborhood local search algorithm that incorporates an efficient incremental evaluation of solutions and an adaptive control of penalty weights. Computational results show that the proposed method improves the performance of the local search algorithm for large-scale set covering and partitioning problems. © 2017 The Author(s)","Combinatorial optimization; Heuristics; Local search; Set covering problem; Set partitioning problem","Bins; Combinatorial optimization; Data mining; Integer programming; Learning algorithms; Local search (optimization); Computational results; Heuristics; Incremental evaluation; Local search; Local search algorithm; Partitioning problem; Set covering problem; Set partitioning problem; Optimization",2-s2.0-85020041852
"Musto C., Lops P., de Gemmis M., Semeraro G.","Semantics-aware Recommender Systems exploiting Linked Open Data and graph-based features",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028328141&doi=10.1016%2fj.knosys.2017.08.015&partnerID=40&md5=02b7c647c1441f8e06381e39a52a1842","The recent spread of Linked Open Data (LOD) fueled the research in the area of Recommender Systems, since the (semantic) data points available in the LOD cloud can be exploited to improve the performance of recommendation algorithms by enriching item representations with new and relevant features. In this article we investigate the impact of the features gathered from the LOD cloud on a hybrid recommendation framework based on three classification algorithms, Random Forests, Naïve Bayes and Logistic Regression. Specifically, we extend the representation of the items by introducing two new types of features: LOD-based features, structured data extracted from the LOD cloud, as the genre of a movie or the writer of a book, and graph-based features, computed on the ground of the topological characteristics of both the bipartite graph-based representation connecting users and items, and the tripartite representation connecting users, items and properties in the LOD cloud. In the experimental session we assess the effectiveness of these novel features; results show that the use of information coming from the LOD cloud could improve the overall accuracy of our recommendation framework. Finally, our approach outperform several state-of-the-art recommendation techniques, thus confirming the insights behind this research. © 2017","Classifiers; Linked Open Data; Machine learning; Recommender Systems; Semantics","Classifiers; Data mining; Decision trees; Graphic methods; Learning systems; Semantics; Topology; Classification algorithm; Graph-based features; Hybrid recommendation; Linked open data (LOD); Linked open datum; Recommendation algorithms; Recommendation techniques; Topological characteristics; Recommender systems",2-s2.0-85028328141
"Ristić A., Bugarinović Ž., Vrtunski M., Govedarica M., Petrovački D.","Integration of modern remote sensing technologies for faster utility mapping and data extraction",2017,"Construction and Building Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023614009&doi=10.1016%2fj.conbuildmat.2017.07.030&partnerID=40&md5=3b0bd8bbb6d1dc25ba5bb0da2651df74","The aim of the research presented in this paper is to analyze the benefits of integrating a mobile system capable of very fast, reliable and relatively inexpensive detection, identification and status examination of district heating network. Thermal imaging using unmanned aerial vehicle is used for pipeline route detection, inspection of validity of cadastral data and for locating possible leakages. Ground Penetrating Radar – GPR technology is used for control sampling of radargrams on specific locations of routes in order to achieve following: identification of the geometric characteristics of district heating pipelines and structure, prevention and registration of damage, as well as automated data extraction. The main part of the paper is dedicated to the algorithm for automated data extraction, based on artificial neural networks and pattern recognition. Radargrams of district heating pipeline were used as input data for the extraction algorithm, while the results are geometric characteristics such as pipe depth, distance between pipes and diameter. © 2017 Elsevier Ltd","Aerial thermography; Automated data extraction; District heating network; Edge detection; GPR; Neural networks","Automation; Damage detection; Data mining; Edge detection; Extraction; Ground penetrating radar systems; Heating; Heating equipment; Infrared imaging; Neural networks; Pattern recognition; Pipelines; Thermography (imaging); Aerial thermography; Automated data; District heating networks; Extraction algorithms; Geometric characteristics; Ground Penetrating Radar; Remote sensing technology; Specific location; District heating",2-s2.0-85023614009
"Zhou P., Hu X., Li P., Wu X.","Online feature selection for high-dimensional class-imbalanced data",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029209977&doi=10.1016%2fj.knosys.2017.09.006&partnerID=40&md5=2eb35b070ef7b22712f3fb4980a9f4b6","When tackling high dimensionality in data mining, online feature selection which deals with features flowing in one by one over time, presents more advantages than traditional feature selection methods. However, in real-world applications, such as fraud detection and medical diagnosis, the data is high-dimensional and highly class imbalanced, namely there are many more instances of some classes than others. In such cases of class imbalance, existing online feature selection algorithms usually ignore the small classes which can be important in these applications. It is hence a challenge to learn from high-dimensional and class imbalanced data in an online manner. Motivated by this, we first formalize the problem of online streaming feature selection for class imbalanced data, and then present an efficient online feature selection framework regarding the dependency between condition features and decision classes. Meanwhile, we propose a new algorithm of Online Feature Selection based on the Dependency in K nearest neighbors, called K-OFSD. In terms of Neighborhood Rough Set theory, K-OFSD uses the information of nearest neighbors to select relevant features which can get higher separability between the majority class and the minority class. Finally, experimental studies on seven high-dimensional and class imbalanced data sets show that our algorithm can achieve better performance than traditional feature selection methods with the same numbers of features and state-of-the-art online streaming feature selection algorithms in an online manner. © 2017 Elsevier B.V.","Class imbalance; High dimensional; Neighborhood rough set; Online feature selection","Data mining; Diagnosis; Nearest neighbor search; Rough set theory; Class imbalance; Class imbalanced data sets; Feature selection algorithm; Feature selection methods; High dimensionality; High-dimensional; Neighborhood rough sets; Online feature selection; Feature extraction",2-s2.0-85029209977
"Baralis E., Cagliero L., Garza P.","Planning stock portfolios by means of weighted frequent itemsets",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019897449&doi=10.1016%2fj.eswa.2017.05.051&partnerID=40&md5=f0565185973537d34cb09aa748f10df4","Planning stock portfolios is a challenging task, because investors have to forecast stock market trends. To limit losses due to wrong forecasts a common strategy is diversification, which consists in buying stocks belonging to different sectors/markets to spread bets across different assets. Since the amount of stock market data is continuously growing, an appealing research strategy is to first apply data mining algorithms to discover significant patterns from potentially large stock datasets and then exploit them to support investor decision-making. This article presents an itemset-based approach to supporting buy-and-hold investors in technical analyses by automatically identifying promising sets of high-yield yet diversified stocks to buy. Specifically, it investigates the use of itemsets to generate stock portfolios from historical stock data and recommend them for buy-and-hold investments. To achieve this goal, it analyzes stock market datasets, which contain for each stock the closing prices on different trading days. Datasets are enriched with (analyst-provided) taxonomies, which are used to classify stocks as the corresponding sectors. Unlike previous approaches, it generates a model composed of a subset of potentially interesting itemsets, which are then used to support investors in decision-making. The selected itemsets represent promptly usable stock portfolios satisfying expert's requirements on minimal average return and minimal level of diversification across sectors. The experiments performed on real stock datasets acquired under different market conditions demonstrate the effectiveness of the proposed approach compared to real stock funds. © 2017 Elsevier Ltd","Frequent itemset mining; Stock data analysis","Classification (of information); Commerce; Data mining; Decision making; Finance; Financial markets; Investments; Data mining algorithm; Diversified stocks; Frequent itemset mining; Market condition; Research strategy; Significant patterns; Stock portfolio; Technical analysis; Electronic trading",2-s2.0-85019897449
"Chen W., Pourghasemi H.R., Panahi M., Kornejady A., Wang J., Xie X., Cao S.","Spatial prediction of landslide susceptibility using an adaptive neuro-fuzzy inference system combined with frequency ratio, generalized additive model, and support vector machine techniques",2017,"Geomorphology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030723536&doi=10.1016%2fj.geomorph.2017.09.007&partnerID=40&md5=fb4f46b168ac9b98cab23f7c92a96465","The spatial prediction of landslide susceptibility is an important prerequisite for the analysis of landslide hazards and risks in any area. This research uses three data mining techniques, such as an adaptive neuro-fuzzy inference system combined with frequency ratio (ANFIS-FR), a generalized additive model (GAM), and a support vector machine (SVM), for landslide susceptibility mapping in Hanyuan County, China. In the first step, in accordance with a review of the previous literature, twelve conditioning factors, including slope aspect, altitude, slope angle, topographic wetness index (TWI), plan curvature, profile curvature, distance to rivers, distance to faults, distance to roads, land use, normalized difference vegetation index (NDVI), and lithology, were selected. In the second step, a collinearity test and correlation analysis between the conditioning factors and landslides were applied. In the third step, we used three advanced methods, namely, ANFIS-FR, GAM, and SVM, for landslide susceptibility modeling. Subsequently, the results of their accuracy were validated using a receiver operating characteristic curve. The results showed that all three models have good prediction capabilities, while the SVM model has the highest prediction rate of 0.875, followed by the ANFIS-FR and GAM models with prediction rates of 0.851 and 0.846, respectively. Thus, the landslide susceptibility maps produced in the study area can be applied for management of hazards and risks in landslide-prone Hanyuan County. © 2017 Elsevier B.V.","ANFIS-FR; China; GAM; Landslide spatial modeling; SVM","artificial neural network; data mining; data set; frequency analysis; landslide; numerical model; prediction; spatial variation; support vector machine; China; Hanyuan; Sichuan",2-s2.0-85030723536
"Rönnqvist S., Sarlin P.","Bank distress in the news: Describing events through deep learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021743187&doi=10.1016%2fj.neucom.2016.12.110&partnerID=40&md5=dc79cd375dc21b49fdac0a32d67142bf","While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics. © 2017 Elsevier B.V.","Bank distress; Distributional semantics; Event detection; Financial risk; Neural networks; Text mining","Data mining; Education; Finance; Natural language processing systems; Neural networks; Semantics; Bank distress; Distributional semantics; Event detection; Financial risks; Text mining; Deep learning; Article; artificial neural network; descriptive research; European; financial management; government; language; machine learning; model; prediction; priority journal; risk assessment; semantics",2-s2.0-85021743187
"Sa'adi Z., Shahid S., Chung E.-S., Ismail T.B.","Projection of spatial and temporal changes of rainfall in Sarawak of Borneo Island using statistical downscaling of CMIP5 models",2017,"Atmospheric Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026832180&doi=10.1016%2fj.atmosres.2017.08.002&partnerID=40&md5=936567b9c60e9e4f97bec808118cba98","This study assesses the possible changes in rainfall patterns of Sarawak in Borneo Island due to climate change through statistical downscaling of General Circulation Models (GCM) projections. Available in-situ observed rainfall data were used to downscale the future rainfall from ensembles of 20 GCMs of Coupled Model Intercomparison Project phase 5 (CMIP5) for four Representative Concentration Pathways (RCP) scenarios, namely, RCP2.6, RCP4.5, RCP6.0 and RCP8.5. Model Output Statistics (MOS) based downscaling models were developed using two data mining approaches known as Random Forest (RF) and Support Vector Machine (SVM). The SVM was found to downscale all GCMs with normalized mean square error (NMSE) of 48.2–75.2 and skill score (SS) of 0.94–0.98 during validation. The results show that the future projection of the annual rainfalls is increasing and decreasing on the region-based and catchment-based basis due to the influence of the monsoon season affecting the coast of Sarawak. The ensemble mean of GCMs projections reveals the increased and decreased mean of annual precipitations at 33 stations with the rate of 0.1% to 19.6% and one station with the rate of − 7.9% to − 3.1%, respectively under all RCP scenarios. The remaining 15 stations showed inconsistency neither increasing nor decreasing at the rate of − 5.6% to 5.2%, but mainly showing a trend of decreasing rainfall during the first period (2010–2039) followed by increasing rainfall for the period of 2070–2099. © 2017 Elsevier B.V.","Climate change projection; General circulation model; Representative concentration pathways; Sarawak; Statistical downscaling","Catchments; Climate models; Data mining; Decision trees; Mean square error; Rain; Support vector machines; Climate change projections; General circulation model; Representative concentration pathways; Sarawak; Statistical downscaling; Climate change; atmospheric general circulation model; climate change; climate modeling; climate prediction; CMIP; downscaling; rainfall; spatial variation; statistical analysis; temporal variation; Borneo; East Malaysia; Malaysia; Sarawak",2-s2.0-85026832180
"Abbot J., Marohasy J.","Skilful rainfall forecasts from artificial neural networks with long duration series and single-month optimization",2017,"Atmospheric Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024830494&doi=10.1016%2fj.atmosres.2017.07.015&partnerID=40&md5=7e4b67aa9c43c602b7fa96afd6bfb1a2","General circulation models, which forecast by first modelling actual conditions in the atmosphere and ocean, are used extensively for monthly rainfall forecasting. We show how more skilful monthly and seasonal rainfall forecasts can be achieved through the mining of historical climate data using artificial neural networks (ANNs). This technique is demonstrated for two agricultural regions of Australia: the wheat belt of Western Australia and the sugar growing region of coastal Queensland. The most skilful monthly rainfall forecasts measured in terms of Ideal Point Error (IPE), and a score relative to climatology, are consistently achieved through the use of ANNs optimized for each month individually, and also by choosing to input longer historical series of climate indices. Using the longer series restricts the number of climate indices that can be used. © 2017",,"Neural networks; Rain; Weather forecasting; Actual conditions; Climate index; General circulation model; Growing regions; Monthly rainfalls; Rainfall forecasts; Seasonal rainfall; Western Australia; Forecasting; artificial neural network; climate modeling; climatology; data mining; growing season; optimization; rainfall; temporal variation; weather forecasting; Australia; Triticum aestivum",2-s2.0-85024830494
"Martinez-del-Rincon J., Santofimia M.J., del Toro X., Barba J., Romero F., Navas P., Lopez J.C.","Non-linear classifiers applied to EEG analysis for epilepsy seizure detection",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019972258&doi=10.1016%2fj.eswa.2017.05.052&partnerID=40&md5=09c20c431a364dc57942d970f0a0955b","This work presents a novel approach for automatic epilepsy seizure detection based on EEG analysis that exploits the underlying non-linear nature of EEG data. In this paper, two main contributions are presented and validated: the use of non-linear classifiers through the so-called kernel trick and the proposal of a Bag-of-Words model for extracting a non-linear feature representation of the input data in an unsupervised manner. The performance of the resulting system is validated with public datasets, previously processed to remove artifacts or external disturbances, but also with private datasets recorded under realistic and non-ideal operating conditions. The use of public datasets caters for comparison purposes whereas the private one shows the performance of the system under realistic circumstances of noise, artifacts, and signals of different amplitudes. Moreover, the proposed solution has been compared to state-of-the-art works not only for pre-processed and public datasets but also with the private datasets. The mean F1-measure shows a 10% improvement over the second-best ranked method including cross-dataset experiments. The obtained results prove the robustness of the proposed solution to more realistic and variable conditions. © 2017 Elsevier Ltd","Bag of words; Classification algorithms; Epilepsy; Non-linear classifiers; SVM; Wavelet","Information retrieval; Neurology; Bag of words; Classification algorithm; Epilepsy; Nonlinear classifiers; Wavelet; Data mining",2-s2.0-85019972258
"Bequé A., Lessmann S.","Extreme learning machines for credit scoring: An empirical evaluation",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019633628&doi=10.1016%2fj.eswa.2017.05.050&partnerID=40&md5=2ebe192e17e106a4fcc5dc9cf4d1ff60","Classification algorithms are used in many domains to extract information from data, predict the entry probability of events of interest, and, eventually, support decision making. This paper explores the potential of extreme learning machines (ELM), a recently proposed type of artificial neural network, for consumer credit risk management. ELM possess some interesting properties, which might enable them to improve the quality of model-based decision support. To test this, we empirically compare ELM to established scoring techniques according to three performance criteria: ease of use, resource consumption, and predictive accuracy. The mathematical roots of ELM suggest that they are especially suitable as a base model within ensemble classifiers. Therefore, to obtain a holistic picture of their potential, we assess ELM in isolation and in conjunction with different ensemble frameworks. The empirical results confirm the conceptual advantages of ELM and indicate that they are a valuable alternative to other credit risk modelling methods. © 2017","Artificial neural networks; Classifier ensembles; Credit scoring; Extreme learning machines","Classification (of information); Data mining; Decision making; Decision support systems; Knowledge acquisition; Neural networks; Risk assessment; Risk management; Classification algorithm; Classifier ensembles; Consumer credit risks; Credit scoring; Empirical evaluations; Ensemble classifiers; Extreme learning machine; Performance criterion; Learning systems",2-s2.0-85019633628
"Dokuz A.S., Celik M.","Discovering socially important locations of social media users",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020049640&doi=10.1016%2fj.eswa.2017.05.068&partnerID=40&md5=db4fd9181ba63cff5763d97ae6ccafdd","Socially important locations are places that are frequently visited by social media users in their social media life. Discovering socially interesting, popular or important locations from a location based social network has recently become important for recommender systems, targeted advertisement applications, and urban planning, etc. However, discovering socially important locations from a social network is challenging due to the data size and variety, spatial and temporal dimensions of the datasets, the need for developing computationally efficient approaches, and the difficulty of modeling human behavior. In the literature, several studies are conducted for discovering socially important locations. However, majority of these studies focused on discovering locations without considering historical data of social media users. They focused on analysis of data of social groups without considering each user's preferences in these groups. In this study, we proposed a method and interest measures to discover socially important locations that consider historical user data and each user's (individual's) preferences. The proposed algorithm was compared with a naïve alternative using real-life Twitter dataset. The results showed that the proposed algorithm outperforms the naïve alternative. © 2017 Elsevier Ltd","Historical social media data analysis; Social media networking sites; Socially important locations mining; Spatial social media mining; Twitter","Behavioral research; Information analysis; Location; Computationally efficient; Location-based social networks; Social media; Social media datum; Social media minings; Targeted advertisements; Temporal dimensions; Twitter; Social networking (online)",2-s2.0-85020049640
"Chakraborty S., Man T., Paulette L., Deb S., Li B., Weindorf D.C., Frazier M.","Rapid assessment of smelter/mining soil contamination via portable X-ray fluorescence spectrometry and indicator kriging",2017,"Geoderma",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025114234&doi=10.1016%2fj.geoderma.2017.07.003&partnerID=40&md5=9f9504a4074ab9492ac22ab73ab0d75c","Industrial pollution is a worldwide problem, especially near mining/smelter sites where toxic metals tend to accumulate in soils, sediments, and water. These elements pose a risk both for humans and other organisms' health. In Eastern Europe, assessment of toxic elements such as Pb, Cu, Zn, and others remain challenging because traditional methods are costly and time consuming due to sample collection, chemical digestion, and quantification in laboratories. To reduce these limitations, new assessment methods are needed for deployment in impacted areas. The study conducted herein is the first of its kind to combine portable X-ray fluorescence (PXRF) spectrometry with non-parametric indicator kriging for rapid soil pollution hotspot mapping in Eastern Europe. PXRF was used to assess As, Cu, Cr, Mn, Pb, Zn, and V at 131 georeferenced points (121 impacted; 10 control) in and around the city of Baia Mare, Romania. For spatial variability analysis, ordinary kriging interpolation was used to predict elemental levels in unsampled locations. Pb exceeded the action limit in 91.09% of the area, followed by As (81.20%), Cu (41.52%), Zn (26.69%), and Cr (5.58%). Indicator kriging was then used to estimate the probabilities of data exceeding certain threshold levels. As a result, the pollution hotspots were quickly identified. The highest estimated probabilities of surpassing the Romanian action limits were found around the smelting plant and dispersal stack. Results indicated a likelihood of exceeding action limits of 75% for Cu and between 50 and 75% for Zn. A major portion of the study area showed high probabilities for As and Pb surpassing the Romanian action limits by 75%. Summarily, the PXRF/indicator kriging approach proved effective at rapidly assessing the potential of metal-laden soils to exceed government mandated limits. Using this approach, other cities impacted by similar operations can quickly and cost effectively map areas of concern. © 2017 Elsevier B.V.","Indicator kriging; Pollution; Romania; Smelting; Toxic metals","Fluorescence; Fluorescence spectroscopy; Health risks; Interpolation; Lead; Pollution; Probability; Sediments; Smelting; Soils; Spectrometry; Water pollution; X ray spectrometers; Zinc; Chemical digestion; Indicator kriging; Industrial pollution; Portable x-ray fluorescence; Portable x-ray fluorescence spectrometries; Romania; Spatial variability; Toxic metals; Soil pollution; assessment method; environmental indicator; human activity; industrial emission; kriging; mining; smelting; soil pollution; spectrometry; toxic substance; X-ray fluorescence; Baia Mare; Eastern Europe; Maramures; Romania",2-s2.0-85025114234
"Hassan F.A., Hamdan M.A., Flower R.J., Shallaly N.A., Ebrahem E.","Holocene alluvial history and archaeological significance of the Nile floodplain in the Saqqara-Memphis region, Egypt",2017,"Quaternary Science Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031719053&doi=10.1016%2fj.quascirev.2017.09.016&partnerID=40&md5=9b5463ea5deed738e7ee13f0a73ea712","A suite of drill cores undertaken on the Saqqara-Memphisfloodplain revealed an array of Late Pleistocene-Holocene sediment facies that show a complex of spatio-temporal changes in sediment related to migration of the River Nile, Nile flood variations, settlement sites and climate change. The recovered data enhance our understanding of the history of the modern River Nile and its relationship to the emergence and continuity of Egyptian civilization. The floodplain of the Saqqara-Memphis area reveals a sequence of aggradation and degradation events comprising six clearly marked sedimentary units (I-VI), overlying Late Pleistocene fluvial sand and gravel (unit I). Deposition of unit II resumed during a period of high Nile flow, rapid sea level rise and locally wet climatic conditions. As a result, the floodplain was occupied by swamps and anastomosing channels. Subsequently, the Nile changed to a more stable meandering channel system with well-developed levees and flood basins (unit III). This aggradation unit was subsequently eroded by the end of Old Kingdom (ca. 4.2 kyr cal BP). The degradation hiatus was followed by a widespread layer of alluvial silt and sand indicating very high Nile floods that coincide with historical records of very high floods during the Middle Kingdom and frequently high floods during the New Kingdom (unit IV). During the last two thousand years (units VI-VII) floods generally diminished except for several notable lows and highs. Our calculations of the long-term rate of siltation during the Middle and Late Holocene suggest an average rate of 0.235 m/century rather than the commonly cited 0.09–0.12 m per century. In addition, our study of satellite imagery of the Memphite region in the context of archaeological data combined with our own geological studies reveal that the main Nile in Neolithic and Predynastic times (ca.7.0–5.0 kyr cal BP) ran along the eastern edge of the current floodplain. A lateral branch of the Nile ran along the western edge of the floodplain. It is on the bank of this branch that the first capital of a unified Egypt was established. Our cores also reveal during the Dynastic period, the western branch shifted eastwards, while the main Nile shifted westwards. © 2017 Elsevier Ltd","Alluvial history; Geoarchaeology; Holocene; Nile floodplain; Palaeoclimate; Saqqara-Memphis","Banks (bodies of water); Climate change; Mining laws and regulations; Rivers; Satellite imagery; Sea level; Alluvial history; Flood plains; Geo archaeologies; Holocenes; Memphis; Palaeoclimate; Floods; aggradation; alluvial plain; archaeology; core analysis; degradation; facies; floodplain; fluvial deposit; historical record; history of geography; Holocene; paleoclimate; sand and gravel; satellite imagery; sedimentary sequence; spatiotemporal analysis; Egypt; Nile River",2-s2.0-85031719053
"Mitra S.K., Chattopadhyay M.","The nexus between food price inflation and monsoon rainfall in India: exploring through comparative data mining models",2017,"Climate and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971378938&doi=10.1080%2f17565529.2016.1174662&partnerID=40&md5=fdea2a52eb22a0f0e8b8caa5ce32b6e5","In the paper we analysed the impact on food inflation of monsoon rainfall using different data mining tools, namely: lda (Linear Discriminant Analysis), qda (Quadratic Discriminant Analysis), lr (logistic regression), rpart (Recursive Partitioning and Regression Trees), knn (k-Nearest Neighbors Network), and formulated the models in such a way that food inflation at the end of the financial year can be predicted from the rainfall received during the monsoon month of the year, and a few other known variables. The study is expected to be useful as it can predict the chances of high food inflation with 65% and 63% of accuracy by rpart and lr models, respectively. This information on the chances of high food inflation just after monsoon months can be very useful for policy-makers. While prediction of high food inflation will not in itself solve the problem, it would help decision-makers to take precautionary measures to minimize its adverse impacts on the population. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","data mining; food inflation; performance measures; predictive modelling; rainfall","commodity price; data mining; food market; inflation; monsoon; performance assessment; policy making; precipitation intensity; prediction; rainfall; statistical analysis; India",2-s2.0-84971378938
"Pfeuffer J., Sachsenberg T., Alka O., Walzer M., Fillbrunn A., Nilse L., Schilling O., Reinert K., Kohlbacher O.","OpenMS – A platform for reproducible analysis of mass spectrometry data",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020715888&doi=10.1016%2fj.jbiotec.2017.05.016&partnerID=40&md5=ebc5f6e097ac34a0257fddcdddb59840","Background In recent years, several mass spectrometry-based omics technologies emerged to investigate qualitative and quantitative changes within thousands of biologically active components such as proteins, lipids and metabolites. The research enabled through these methods potentially contributes to the diagnosis and pathophysiology of human diseases as well as to the clarification of structures and interactions between biomolecules. Simultaneously, technological advances in the field of mass spectrometry leading to an ever increasing amount of data, demand high standards in efficiency, accuracy and reproducibility of potential analysis software. Results This article presents the current state and ongoing developments in OpenMS, a versatile open-source framework aimed at enabling reproducible analyses of high-throughput mass spectrometry data. It provides implementations of frequently occurring processing operations on MS data through a clean application programming interface in C++ and Python. A collection of 185 tools and ready-made workflows for typical MS-based experiments enable convenient analyses for non-developers and facilitate reproducible research without losing flexibility. Conclusions OpenMS will continue to increase its ease of use for developers as well as users with improved continuous integration/deployment strategies, regular trainings with updated training materials and multiple sources of support. The active developer community ensures the incorporation of new features to support state of the art research. © 2017 The Authors","Analysis workflows; Mass spectrometry; Reproducible research; Software libraries; Tool collection","Application programming interfaces (API); C++ (programming language); Computer programming; Diagnosis; Mass spectrometry; Open source software; Continuous integrations; Mass spectrometry data; Open source frameworks; Processing operations; Reproducible research; Software libraries; Technological advances; Work-flows; Spectrometry; Article; bioinformatics; computer analysis; computer interface; data analysis software; data mining; documentation; machine learning; mass spectrometry; priority journal; workflow",2-s2.0-85020715888
"Avros R., Dudka V., Křena B., Letko Z., Pluháčková H., Ur S., Vojnar T., Volkovich Z.","Boosted decision trees for behaviour mining of concurrent programmes",2017,"Concurrency Computation ",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028664932&doi=10.1002%2fcpe.4268&partnerID=40&md5=f9d88c73f8aecfe084624f6d356408bf","Testing of concurrent programmes is difficult since the scheduling nondeterminism requires one to test a huge number of different thread interleavings. Moreover, repeated test executions that are performed in the same environment will typically examine similar interleavings only. One possible way how to deal with this problem is to use the noise injection approach, which influences the scheduling by injecting various kinds of noise (delays, context switches, etc) into the common thread behaviour. However, for noise injection to be efficient, one has to choose suitable noise injection heuristics from among the many existing ones as well as to suitably choose values of their various parameters, which is not easy. In this paper, we propose a novel way how to deal with the problem of choosing suitable noise injection heuristics and suitable values of their parameters (as well as suitable values of parameters of the programmes being tested themselves). Here, by suitable, we mean such settings that maximize chances of meeting a given testing goal (such as, eg, maximizing coverage of rare behaviours and thus maximizing chances to find rarely occurring concurrency-related bugs). Our approach is, in particular, based on using data mining in the context of noise-based testing to get more insight about the importance of the different heuristics in a particular testing context as well as to improve fully automated noise-based testing (in combination with both random as well as genetically optimized noise setting). Copyright © 2017 John Wiley & Sons, Ltd.","AdaBoost; automated testing; concurrent programmes; data mining; genetic algorithms; noise injection","Adaptive boosting; Data mining; Genetic algorithms; Scheduling; Automated testing; Boosted decision trees; Concurrent programmes; Context switch; Fully automated; Noise based testing; Noise injection; Non-determinism; Decision trees",2-s2.0-85028664932
"Schomburg I., Jeske L., Ulbrich M., Placzek S., Chang A., Schomburg D.","The BRENDA enzyme information system–From a database to an expert system",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018415213&doi=10.1016%2fj.jbiotec.2017.04.020&partnerID=40&md5=11be88bf9196d039dd216e11269da906","Enzymes, representing the largest and by far most complex group of proteins, play an essential role in all processes of life, including metabolism, gene expression, cell division, the immune system, and others. Their function, also connected to most diseases or stress control makes them interesting targets for research and applications in biotechnology, medical treatments, or diagnosis. Their functional parameters and other properties are collected, integrated, and made available to the scientific community in the BRaunschweig ENzyme DAtabase (BRENDA). In the last 30 years BRENDA has developed into one of the most highly used biological databases worldwide. The data contents, the process of data acquisition, data integration and control, the ways to access the data, and visualizations provided by the website are described and discussed. © 2017 The Authors","Enzyme database; Enzyme kinetics; Enzyme-catalysed reactions; Enzyme-ligand interaction; Enzymes and diseases; Metabolic pathways","Cell proliferation; Data acquisition; Data integration; Database systems; Diagnosis; Disease control; Enzyme kinetics; Expert systems; Gene expression; Intelligent databases; Metabolism; Biological database; Enzyme-ligand interactions; Functional parameters; Medical treatment; Metabolic pathways; Other properties; Research and application; Scientific community; Enzymes; enzyme; ligand; membrane protein; access to information; Article; biochemical analysis; Braunschweig enzyme database; calculation; cellular distribution; control system; data analysis software; data base; data mining; diseases; enzyme activity; enzyme kinetics; enzyme structure; expert system; feedback system; gene function; gene ontology; gene product; genome; history; human; information processing; information system; kinetics; molecular genetics; nonhuman; online system; organisms; prediction; priority journal; protein structure; quality control; search engine; sequence analysis; statistics; structure analysis; systems biology; tissue distribution",2-s2.0-85018415213
"Reimer L.C., Söhngen C., Vetcininova A., Overmann J.","Mobilization and integration of bacterial phenotypic data—Enabling next generation biodiversity analysis through the BacDive metadatabase",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019110829&doi=10.1016%2fj.jbiotec.2017.05.004&partnerID=40&md5=431941ea551006490647f292cfed5b8d","Microbial data and metadata are scattered throughout the scientific literature, databases and unpublished lab notes and thereby often are difficult to access. Hot spots of (meta)data are internal descriptions of culture collections and initial descriptions of novel taxa in primary literature. Here we describe three exemplary mobilization projects which yielded metadata published through the prokaryotic metadatabase BacDive. The Reichenbach collection of myxobacteria includes information on 12,535 typewritten index cards which were digitized. A total of 37,156 data points were extracted by text mining. In the second mobilization project, Analytical Profile Index (API) tests on paper forms were targeted. Overall 6820 API tests were digitized, which provide physiological data of 4524 microbial strains. Thirdly, the extraction of metadata from 523 new species descriptions of the International Journal of Systematic and Evolutionary Microbiology, yielding 35,651 data points, is described. All data sets were integrated and published in BacDive. Thereby these metadata not only became accessible and searchable but were also linked to strain taxonomy, isolation source, cultivation condition, and molecular biology data. © 2017 The Authors","Bacterial biodiversity; Bacterial phenotype; Data mobilization; Database; Metadata","Bacteria; Biodiversity; Database systems; Metadata; Molecular biology; Bacterial biodiversity; Bacterial phenotype; Biodiversity analysis; Cultivation conditions; Culture collection; Data mobilization; International journals; Scientific literature; Data mining; Article; bacterial strain; bacterium isolation; data extraction; factual database; metadata; microbial diversity; molecular biology; new species; nonhuman; phenotype; priority journal; species cultivation",2-s2.0-85019110829
"Moldavska A., Welo T.","The concept of sustainable manufacturing and its definitions: A content-analysis based literature review",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029166981&doi=10.1016%2fj.jclepro.2017.08.006&partnerID=40&md5=329ddbd0397a683765dedcc2ac48b778","The concept of sustainable manufacturing (SM) is becoming increasingly mature due to the focus on many of its research topics for a long time. This research has undoubtedly extended the body of knowledge, yet the numerous definitions of SM in prior art still indicate a lack of consensus on the true meaning of the concept. It is thus to be expected that these discrepancies will constrain further development and use of the SM concept in industrial practice. The goal of this paper is to analyze the different definitions of SM and identify the current understanding of what researchers mean by the concept. We use an inductive content analysis of definitions published from 1990 to 2016 in a variety of academic journals. A total of 189 articles including a manifest definition of SM and 89 original definitions were identified. Our analysis revealed that the most commonly used definition is the one proposed by U.S. Department of Commerce in 2008; 63% of the analyzed articles cite or slightly rephrase this definition, while 86% of the identified definitions are used in less than three articles. Although the majority of researchers seems to agree upon eleven sub-categories of SM, a wide range of issues (67 sub-categories) associated with SM indicates inconsistency in the general understanding of the concept. It is proposed that the findings in this study can serve as a foundation for the development of a common language for SM in both research field and industrial practice. © 2017 The Authors","Content analysis; Literature review; Manufacturing; Sustainability; Sustainable manufacturing definitions","Data mining; Industrial management; Manufacture; Sustainable development; Academic journal; Body of knowledge; Common languages; Content analysis; Department of Commerce; Industrial practices; Literature reviews; Sustainable manufacturing; Industrial research",2-s2.0-85029166981
"Silvius G.","Sustainability as a new school of thought in project management",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029176756&doi=10.1016%2fj.jclepro.2017.08.121&partnerID=40&md5=c0da0b53c17d6e2f077b868f424c1e05","Sustainability is one of the most important challenges of our time. It is recognized that projects play a pivotal role in the realization of more sustainable business practices and a developing theme in project management research is the relationship between projects and sustainability. As the literature on this topic is evolving, this paper discusses the question whether the growing attention for sustainability in project management research represents a new ‘school of thought’ in project management? The study builds upon earlier work on schools of project management research, in which nine schools were identified. The question whether sustainability should be considered a new school of project management is answered by deriving the criteria for recognition as a school and performing a structured literature review on a sample of 71 articles on sustainability in project management, taken from the leading academic journals on this topic. As criteria for recognition as a school of project management, the criteria content, community and impact were found. After a content analysis of the articles in the sample, the conclusion is reached that sustainability qualifies a new, distinct and emerging school of thinking in project management. The defining characteristics of this sustainability school are: considering Projects in a societal perspective, having a Management for stakeholders approach, applying Triple bottom line criteria, and taking a Values based approach to projects and project management. © 2017 Elsevier Ltd","Project management; Project management theory; Sustainability; Triple constraint","Data mining; School buildings; Sustainable development; Academic journal; Content analysis; Literature reviews; Project management research; Project management theory; Sustainable business; Triple Bottom Line; Triple constraint; Project management",2-s2.0-85029176756
"Rauscher B., Valentini E., Hardeland U., Boutros M.","Phenotype databases for genetic screens in human cells",2017,"Journal of Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023740558&doi=10.1016%2fj.jbiotec.2017.06.008&partnerID=40&md5=c67082494a57bdc1a47d3f84835c9d0e","Genetic screens are powerful tools to identify components that make up biological systems. Perturbations introduced by methods such as RNA interference (RNAi) or CRISPR/Cas9-mediated genome editing lead to biological phenotypes that can be examined to understand the molecular function of genes in the cell. Over the years, many of such experiments have been conducted providing a wealth of knowledge about genotype-to-phenotype relationships. These data are a rich source of information and it is in a common interest to make them available in a simplified and integrated format. Thus, an important challenge is that genetic screening data can be stored in databases in standardized ways, allowing users to gain new biological insights through data mining and integrated analyses. Here, we provide an overview of available phenotype databases for human cells. We review in detail two databases for high-throughput screens, GenomeRNAi and GenomeCRISPR, and describe how these resources are integrated into the German Network for Bioinformatics Infrastructure de.NBI as part of the European infrastructure for life-science information ELIXIR. © 2017 The Author(s)","Database; de.NBI; ELIXIR; Functional genomics; High-throughput biology; Phenotype","Biotechnology; De.NBI; ELIXIR; Functional genomics; High throughput; Phenotype; Database systems; Article; bioinformatics; biomedicine; CRISPR-CAS9 system; gene ontology; genetic database; genetic screening; high throughput screening; human; human cell; phenotype; priority journal; RNA interference",2-s2.0-85023740558
"Gómez S.E., Martínez B.C., Sánchez-Esguevillas A.J., Hernández Callejo L.","Ensemble network traffic classification: Algorithm comparison and novel ensemble scheme proposal",2017,"Computer Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030660345&doi=10.1016%2fj.comnet.2017.07.018&partnerID=40&md5=49611bf7ee2321b3a41cc8046799ee29","Network Traffic Classification (NTC) is a key piece for network monitoring, Quality-of-Service management and network security. Machine Learning algorithms have drawn the attention of many researchers during the last few years as a promising solution for network traffic classification. In Machine Learning, ensemble algorithms are classifiers formed by a set of base estimators that cooperate to build more complex models according to given training and classification strategies. Resulting models normally exhibit significant accuracy improvements compared to single estimators, but also extra time cost, which may obstruct the application of these methods to online NTC. This paper studies and compares the performance of seven popular ensemble algorithms based on Decision Trees, focusing on model accuracy, byte accuracy, and latency to determine whether ensemble learning can be properly applied to this modeling task. We show that some of the studied algorithms overcome single Decision Tree in terms of model accuracy and byte accuracy. However, the notable latency increase hinders the application of these methods in real time contexts. Additionally, we introduce a novel ensemble classifier that exploits the imbalanced populations presented in traffic networks datasets to achieve faster classifications. The experimental results show that our scheme retains the accuracy improvements of ensemble methods but with low latency punishment, enhancing the prospect of ensembles methods for online network traffic classification. © 2017 Elsevier B.V.",,"Artificial intelligence; Classification (of information); Data mining; Decision trees; Learning systems; Network security; Quality of service; Telecommunication traffic; Accuracy Improvement; Algorithm comparison; Ensemble algorithms; Ensemble classifiers; Ensemble learning; Ensemble networks; Network Monitoring; Network traffic classification; Learning algorithms",2-s2.0-85030660345
"Xu Q., Li H., He Y., Liu F., Peng D.","Comparison of data-driven models of loess landslide runout distance estimation",2017,"Bulletin of Engineering Geology and the Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032814337&doi=10.1007%2fs10064-017-1176-3&partnerID=40&md5=903e64556f5dceba05f62ac336dea615","Irrigation-induced landslides with long runout distances endanger local communities. Estimating runout distance of landslides may contribute to the mitigation of potential hazards. Conventional mechanism-related methods require a series of experiments and/or numerical simulations that are commonly time-consuming and expensive, yet data-driven models reduce the experimental workload and require less prior knowledge in the geological history as well as mechanical behavior of the material. A data-driven model is proposed to forecast landslide runout distance using geometrical characteristics of the landslide. The geometrical dataset of the shallow loess landslides and loess-bedrock landslides occurred in Heifangtai terrace, China, was employed to develop the model. All geometrical datasets were obtained from field investigation and monitoring. Seven data-mining techniques were used and compared for runout estimation, among which the most optimal technique was integrated in the estimation model for loess slope failures. The multi-layer perceptron method outperforms other algorithms, and thus it was selected for the runout distance estimation model. Parametric models are constructed to fit runout distance based on the estimation. Hazard analysis measurements, including value-at-risk (VaR) and tail-value-at-risk (TVaR), are computed for the parametric distributions, which shows the potential area of impact and number of residential clusters at risk. © 2017 Springer-Verlag GmbH Germany","Data-mining; Distribution fitting; Field investigation; Predictive modeling; Tail-value-at-risk; Value-at-risk","Consumer behavior; Data mining; Geometry; Hazards; Landslides; Numerical methods; Risk assessment; Sediments; Value engineering; Distribution fitting; Field investigation; Geometrical characteristics; Mechanical behavior; Multi layer perceptron; Parametric distributions; Predictive modeling; Value at Risk; Parameter estimation",2-s2.0-85032814337
"Rezaei M.","Feasibility of novel techniques to predict the elastic modulus of rocks based on the laboratory data",2017,"International Journal of Geotechnical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032822580&doi=10.1080%2f19386362.2017.1397873&partnerID=40&md5=35409463f8467856f98e98aebf5eb540","Elastic modulus of intact rock (E) plays a critical role in designing of civil, mining and rock engineering projects. Generally, this parameter is directly measured in laboratory in which complicated test tools and core specimens with high quality and fitting dimensions are needed. As an alternative, back propagation neural network (BPNN), radial basis function neural network (RBFNN) and multiple linear regression (MLR) techniques are used to estimate E in this study. For this purpose, measured data from the Azad and Bakhtiary dam sites in Iran are considered for models construction and evaluation. On the basis of minimum error index, neural networks with architecture 8-5-7-1 and 8-12-1 are found as the optimum networks for BPNN and RBFNN models, respectively. Comparing the performance of developed models is conducted using the five statistical indices. Accordingly, it is found that the performance of RBFNN model is somewhat better than BPNN model and both more realistic than the MLR mode. Also, the results of RBFNN and BPNN models gave higher conformity with the actual dada compared to the MLR model. Finally, parametric study shows that inputs P-wave velocity and unconfined compressive strength are the most effective variables and depth of coring and porosity are the least effective ones on the elastic modulus in the BPNN and RBFNN models, respectively. The main conclusion of this study is developing the new models with high accuracy and satisfied confidence in which all of the possible effective parameters were considered in estimation of the elastic modulus. © 2017 Informa UK Limited, trading as Taylor & Francis Group","back propagation; Elastic modulus; multiple linear regression; neural network; radial basis function",,2-s2.0-85032822580
"Martínez-Arzate S.G., Tenorio-Borroto E., Barbabosa Pliego A., Díaz-Albiter H.M., Vázquez-Chagoyán J.C., González-Díaz H.","PTML Model for Proteome Mining of B-Cell Epitopes and Theoretical-Experimental Study of Bm86 Protein Sequences from Colima, Mexico",2017,"Journal of Proteome Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032788062&doi=10.1021%2facs.jproteome.7b00477&partnerID=40&md5=8790ef5b28832ae5b8a661b2e88fae7c","In this work, we developed a general perturbation theory and machine learning method for data mining of proteomes to discover new B-cell epitopes useful for vaccine design. The method predicts the epitope activity ϵq(cqj) of one query peptide (q-peptide) under a set of experimental query conditions (cqj). The method uses as input the sequence of the q-peptide. The method also uses as input information about the sequence and epitope activity ϵr(crj) of a peptide of reference (r-peptide) assayed under similar experimental conditions (crj). The model proposed here is able to classify 1raquo;048raquo;190 pairs of query and reference peptide sequences from the proteome of many organisms reported on IEDB database. These pairs have variations (perturbations) under sequence or assay conditions. The model has accuracy, sensitivity, and specificity between 71 and 80% for training and external validation series. The retrieved information contains structural changes in 83raquo;683 peptides sequences (Seq) determined in experimental assays with boundary conditions involving 1448 epitope organisms (Org), 323 host organisms (Host), 15 types of in vivo process (Proc), 28 experimental techniques (Tech), and 505 adjuvant additives (Adj). Afterward, we reported the experimental sampling, isolation, and sequencing of 15 complete sequences of Bm86 gene from state of Colima, Mexico. Last, we used the model to predict the epitope immunogenic scores under different experimental conditions for the 26raquo;112 peptides obtained from these sequences. The model may become a useful tool for epitope selection toward vaccine design. The theoretical-experimental results on Bm86 protein may help the future design of a new vaccine based on this protein. © 2017 American Chemical Society.","B-cell epitope; Bm86 protein; epitope prediction; machine learning; PCR; perturbation theory; proteome mining",,2-s2.0-85032788062
"Dvořák J., Wittlingerová Z., Vochozka M., Stehel V., Maroušková A.","Updated energy policy of the Czech Republic may result in instability of the electricity grid in Central Europe",2017,"Clean Technologies and Environmental Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032805766&doi=10.1007%2fs10098-017-1451-9&partnerID=40&md5=cf52be680ba0d2a5dd899c16347340ee","Brown coal is the resource used for generating half of the electricity and most of the heat that is distributed over the public network in the Czech Republic, which is one of the largest exporters of electricity in Europe. As a result of public call for cleaner energy sources, the energy policy of the Czech Republic has been updated recently. The government act calls for significant decrease in brown coal mining. Prediction of material flow analysis for the entire energy sector of the Czech Republic till 2040 was carried out. The data revealed mounting evidence indicating that the novel energy policy proposed was too ambitious which may affect the stability of the power grid in the surrounding countries. Worse still, it appears that after 2025, the Czech energy sector will be in short supply of limestone, which is likely to result in lower levels of flue gas desulphurization or limestone mining in protected landscape areas. It is concluded that further diversification of local energy resources is advisable to avoid unwanted negative environmental impacts. © 2017 Springer-Verlag GmbH Germany","Brown coal; Energy policy; Lignite; Material flow analysis","Coal; Coal deposits; Energy policy; Energy resources; Environmental impact; Lignite; Limestone; Brown coal mining; Central Europe; Cleaner energies; Electricity grids; Flue gas desulphurization; Material flow analysis; Protected landscape area; Public networks; Electric power transmission networks",2-s2.0-85032805766
"Chen Z., Soh Y.C.","Comparing occupancy models and data mining approaches for regular occupancy prediction in commercial buildings",2017,"Journal of Building Performance Simulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976524126&doi=10.1080%2f19401493.2016.1199735&partnerID=40&md5=2046305b61a1f4761d90941ecbb85b29","Occupancy information can help us to achieve high energy-efficient buildings. Previous works mainly focus on predicting the presence and absence of occupants in homes or single person offices. We attempt to predict regular occupancy level in a commercial building deployment scenario. The occupancy prediction models can be divided into two categories of occupancy models and data mining approaches. For the occupancy models, we shall investigate the efficiencies of two widely used multi-occupant models, that is, inhomogeneous Markov chain and multivariate Gaussian. For the data mining approaches, we propose the application of autoregressive integrated moving average, artificial neural network and support vector regression. Experiments have been conducted using actual occupancy data under four different prediction horizons, that is, 15 min, 30 min, 1 and 2 h. The results demonstrated a guideline in how to choose a proper method for the prediction of occupancy in commercial buildings under different prediction horizons. © 2016 International Building Performance Simulation Association (IBPSA).","building occupancy prediction; data mining approaches; occupancy models; time horizons","Buildings; Energy efficiency; Forecasting; Markov processes; Neural networks; Office buildings; Auto-regressive integrated moving average; Building occupancy; Commercial building; Deployment scenarios; Energy efficient building; Occupancy predictions; Support vector regression (SVR); Time horizons; Data mining",2-s2.0-84976524126
"Zhang Y., Xin Y., Li Q., Ma J., Li S., Lv X., Lv W.","Empirical study of seven data mining algorithms on different characteristics of datasets for biomedical classification applications",2017,"BioMedical Engineering Online",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032747113&doi=10.1186%2fs12938-017-0416-x&partnerID=40&md5=e8b9b585aa4e409f4ce8606e9f670778","Background: Various kinds of data mining algorithms are continuously raised with the development of related disciplines. The applicable scopes and their performances of these algorithms are different. Hence, finding a suitable algorithm for a dataset is becoming an important emphasis for biomedical researchers to solve practical problems promptly. Methods: In this paper, seven kinds of sophisticated active algorithms, namely, C4.5, support vector machine, AdaBoost, k-nearest neighbor, naïve Bayes, random forest, and logistic regression, were selected as the research objects. The seven algorithms were applied to the 12 top-click UCI public datasets with the task of classification, and their performances were compared through induction and analysis. The sample size, number of attributes, number of missing values, and the sample size of each class, correlation coefficients between variables, class entropy of task variable, and the ratio of the sample size of the largest class to the least class were calculated to character the 12 research datasets. Results: The two ensemble algorithms reach high accuracy of classification on most datasets. Moreover, random forest performs better than AdaBoost on the unbalanced dataset of the multi-class task. Simple algorithms, such as the naïve Bayes and logistic regression model are suitable for a small dataset with high correlation between the task and other non-task attribute variables. K-nearest neighbor and C4.5 decision tree algorithms perform well on binary- and multi-class task datasets. Support vector machine is more adept on the balanced small dataset of the binary-class task. Conclusions: No algorithm can maintain the best performance in all datasets. The applicability of the seven data mining algorithms on the datasets with different characteristics was summarized to provide a reference for biomedical researchers or beginners in different fields. © 2017 The Author(s).","Applicability of algorithm; Characters of datasets; Classification task; Data mining","Adaptive boosting; Binary trees; Bins; Bioinformatics; Classification (of information); Decision trees; Motion compensation; Nearest neighbor search; Regression analysis; Support vector machines; C4.5 decision tree algorithm; Characters of datasets; Classification tasks; Correlation coefficient; Data mining algorithm; K-nearest neighbors; Logistic Regression modeling; Logistic regressions; Data mining",2-s2.0-85032747113
"Lin J.C.-W., Hong T.-P., Fournier-Viger P., Liu Q., Wong J.-W., Zhan J.","Efficient hiding of confidential high-utility itemsets with minimal side effects",2017,"Journal of Experimental and Theoretical Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019170832&doi=10.1080%2f0952813X.2017.1328462&partnerID=40&md5=b2e1739061d71197f05f5a66543360cd","Privacy preserving data mining (PPDM) is an emerging research problem that has become critical in the last decades. PPDM consists of hiding sensitive information to ensure that it cannot be discovered by data mining algorithms. Several PPDM algorithms have been developed. Most of them are designed for hiding sensitive frequent itemsets or association rules. Hiding sensitive information in a database can have several side effects such as hiding other non-sensitive information and introducing redundant information. Finding the set of itemsets or transactions to be sanitised that minimises side effects is an NP-hard problem. In this paper, a genetic algorithm (GA) using transaction deletion is designed to hide sensitive high-utility itemsets for PPUM. A flexible fitness function with three adjustable weights is used to evaluate the goodness of each chromosome for hiding sensitive high-utility itemsets. To speed up the evolution process, the pre-large concept is adopted in the designed algorithm. It reduces the number of database scans required for verifying the goodness of an evaluated chromosome. Substantial experiments are conducted to compare the performance of the designed GA approach (with/without the pre-large concept), with a GA-based approach relying on transaction insertion and a non-evolutionary algorithm, in terms of execution time, side effects, database integrity and utility integrity. Results demonstrate that the proposed algorithm hides sensitive high-utility itemsets with fewer side effects than previous studies, while preserving high database and utility integrity. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Genetic algorithm; high-utility mining; privacy preserving data mining; transaction deletion","Chromosomes; Computational complexity; Data privacy; Database systems; Genetic algorithms; Data mining algorithm; Database integrity; High utility itemsets; Privacy preserving data mining; Sensitive informations; transaction deletion; Transaction insertions; Utility mining; Data mining",2-s2.0-85019170832
"Jiang Y., Li Y., Yang C., Liu K., Armstrong E.M., Huang T., Moroni D.F., Finch C.J.","A comprehensive methodology for discovering semantic relationships among geospatial vocabularies using oceanographic data discovery as an example",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026542390&doi=10.1080%2f13658816.2017.1357819&partnerID=40&md5=13f3f9ac575fd028fcdab383c3381f82","It is challenging to find relevant data for research and development purposes in the geospatial big data era. One long-standing problem in data discovery is locating, assimilating and utilizing the semantic context for a given query. Most research in the geospatial domain has approached this problem in one of two ways: building a domain-specific ontology manually or discovering automatically, semantic relationships using metadata and machine learning techniques. The former relies on rich expert knowledge but is static, costly and labor intensive, whereas the second is automatic and prone to noise. An emerging trend in information science takes advantage of large-scale user search histories, which are dynamic but subject to user- and crawler-generated noise. Leveraging the benefits of these three approaches and avoiding their weaknesses, a novel methodology is proposed to (1) discover vocabulary-based semantic relationships from user search histories and clickstreams, (2) refine the similarity calculation methods from existing ontologies and (3) integrate the results of ontology, metadata, user search history and clickstream analysis to better determine their semantic relationships. An accuracy assessment by domain experts for the similarity values indicates an 83% overall accuracy for the top 10 related terms over randomly selected sample queries. This research functions as an example for building vocabulary-based semantic relationships for different geographical domains to improve various aspects of data discovery, including the accuracy of the vocabulary relationships of commonly used search terms. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","big data; click behavior; Query augmentation; search history; semantic search; web mining","data mining; Internet; machine learning; metadata; methodology; oceanography; spatial data; World Wide Web",2-s2.0-85026542390
"Zhang Q., Wang P., Chen H., Huang Q., Jiang H., Zhang Z., Zhang Y., Luo X., Sun S.","A novel method for urban area extraction from VIIRS DNB and MODIS NDVI data: a case study of Chinese cities",2017,"International Journal of Remote Sensing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020746757&doi=10.1080%2f01431161.2017.1339927&partnerID=40&md5=94db5e2c996075f7a0923f0151d5eafc","Mapping urban areas at the regional and global scales has been used in ecology, environment, sociology, and other subjects. Recently, it has become increasingly popular to extract urban areas from night-time light remote-sensing data. In this article, we reported an alternative method to extract information of urban areas from VIIRS Day/Night Band (DNB) and MODIS normalized differential vegetation index (NDVI) data based on the adaptive mutation particle swarm optimization (AMPSO) algorithm and the Support Vector Machine (SVM) classification algorithm. This method was validated using the urban areas of nine Chinese cities classified from Landsat Enhanced Thematic Mapper (ETM+) images by object-oriented image classification technology. We demonstrated that this new method for urban area extraction had a good classification coherency with the Landsat8 OLI result. In addition, it is more robust than other classification methods, and can be used to characterize the inter-urban texture as well. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"Classification (of information); Extraction; Forestry; Optimization; Particle swarm optimization (PSO); Radiometers; Remote sensing; Support vector machines; Classification algorithm; Classification methods; Classification technology; Extract informations; Landsat enhanced thematic mapper (ETM+); Night time lights; Normalized differential vegetation indices; Remote sensing data; Data mining; algorithm; image classification; Landsat thematic mapper; MODIS; NDVI; remote sensing; support vector machine; urban area; VIIRS; China",2-s2.0-85020746757
"Datta S., Dev V.A., Eden M.R.","Hybrid genetic algorithm-decision tree approach for rate constant prediction using structures of reactants and solvent for Diels-Alder reaction",2017,"Computers and Chemical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013910141&doi=10.1016%2fj.compchemeng.2017.02.022&partnerID=40&md5=f598b3499ab5f0a0b62e18bc7e5d4513","In recent years, Computer-Aided Molecular Design (CAMD) has been extensively used for defining and designing reactions at their maximal potential. In all of these contributions, either the structures of reactants/products have been considered to be unchanging or the solvent structure. Developing a QSPR model which not only captures the influence of reactant structures but also the solvent effect on reaction rate, is essential. Since the structures of reactants and products are related, such QSPR models will serve as a prerequisite for the simultaneous CAMD of reactants, products and solvents. They will also provide a useful tool for predicting the rate constant without relying on experiments. To develop such a QSPR, in our work, the Diels-Alder reaction with different sets of reactants and solvents was investigated. Connectivity indices were used to represent the structures of the members of each set. Principal Component Analysis (PCA) was applied to identify principal components (PCs) corresponding to the structures of reactants and solvent of each set. Linear models expressed in terms of PCs were then generated using a Decision Tree (DT) algorithm such that the R2 value was maximized. These models formed the initial population on which the GA performed operations such as crossover and mutation to obtain model(s) with best rate constant prediction. Thus, the novelty of our approach is that after feature extraction using PCA, a DT algorithm generates an ensemble of linear models, which through the GA is transformed into a model with best fit. Our approach required much lesser generations to provide a model with highest R2 ext value as compared to the case where the DT did not initialize the population of models. © 2017 Elsevier Ltd","Divide and conquer; Genetic algorithm-decision tree; Hybrid algorithm; QSPR; Reactants; Solvents; Structure-rate constant relationships","Chemical reactions; Data mining; Decision trees; Feature extraction; Forecasting; Genetic algorithms; Rate constants; Solvents; Trees (mathematics); Computer aided molecular design; Crossover and mutation; Diels-Alder reaction; Divide and conquer; Hybrid algorithms; Hybrid genetic algorithms; QSPR; Reactants; Principal component analysis",2-s2.0-85013910141
"Twerefou D.K., Ayimpusah E.A., Owusu-Afriyie J., Adjei-Mantey K., Bokpin G.A.","The contest for mineral wealth: an economic analysis of conflicts in Ghanaian mining communities",2017,"International Review of Applied Economics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020715818&doi=10.1080%2f02692171.2017.1332016&partnerID=40&md5=c8221fb227f90394baaeca99a5484bb4","The paper uses survey data from 1458 households in 60 communities from 24 districts in 5 regions of Ghana and logistic regression to examine conflicts as a contest for mineral wealth in mining communities, estimates the determinants of conflicts in these mining communities and examines how these contests could erode and/or enhance Ghana’s gains from mining. The paper finds that the likelihood of a conflict occurring in a mining area is about 56.7%. Village effect was found to be a significant positive predictor of mining conflict. Also, improvement in primary education, employment opportunities to community members of ages 25–50, the strength of institutions and the absence of small-scale miners in a mining community reduces the probability of conflicts occurring by 12.8, 35.8, 6.57 and 17.7%, respectively. While an increase in pollution levels increases the likelihood of conflicts occurring by 7.1%, primary occupation in manufacturing and services, and increase in household monthly expenditure significantly increases the likelihood of conflicts within the mining communities as the cost of living increases. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","conflicts; economic analysis; Ghana; Minerals wealth; mining communities","conflict management; economic analysis; employment; household expenditure; mineral; primary education; small scale mining; Ghana",2-s2.0-85020715818
"Sarwat S.G.","Materials science and engineering of phase change random access memory",2017,"Materials Science and Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029520801&doi=10.1080%2f02670836.2017.1341723&partnerID=40&md5=1704d23ba569c2eac6344bec916647d9","Having monopolised the optical data storage industry since the very beginning, phase change materials are now being intensively explored for next-generation electronic data storage, referred to as phase change random access memory (PCRAM). Because phase change materials are electrically programmable; capable of reversibly switching between two stable structural phases of contrasting electrical properties, besides data storage they also enable data computation. For these reasons, PCRAM envisages to overcome both miniaturisation and data flow bottlenecks, challenges which current silicon charge-based technology is failing to cope with. This review, while reasoning the need for a switch to a newer data storage technology, and comparing PCRAM with other data storage and computation platforms, comprehensively takes stock of the benefits and challenges associated with PCRAM. This review also critically investigates and associates the materials science and physics, such as the atomic structure and bonding, thermodynamics and kinetics of the phase transformation, with the PCRAM device characteristics and performance. Various device design-concepts and requirements are reviewed. Recent advances, and evolution of newer platforms, including those relating to neuromorphic computing and photonic memory are also described. This is the winning review of the 2017 Materials Literature Review Prize of the Institute of Materials, Minerals and Mining, run by the Editorial Board of MST. Sponsorship of the prize by TWI Ltd is gratefully acknowledged. © 2017 Institute of Materials, Minerals and Mining.","Phase change materials; phase change memory; thermodynamics; transistors","Atomic physics; Crystal atomic structure; Data storage equipment; Digital storage; Optical data storage; Phase change memory; Phase transitions; Random access storage; Storage (materials); Thermodynamics; Transistors; Data storage technology; Device characteristics; Electronic data storage; Materials science and engineering; Neuromorphic computing; Phase change random access memory; Phase change random access memory (PCRAM); Thermodynamics and kinetics; Phase change materials",2-s2.0-85029520801
"Lim S.-C., Wiklund H., Glover A.G., Dahlgren T.G., Tan K.-S.","A new genus and species of abyssal sponge commonly encrusting polymetallic nodules in the Clarion-Clipperton Zone, East Pacific Ocean",2017,"Systematics and Biodiversity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328195&doi=10.1080%2f14772000.2017.1358218&partnerID=40&md5=7b1387276a0b92d02737efc2177e70b5","The Clarion-Clipperton Zone (CCZ) in the East Pacific is a vast region targeted for deep-sea mineral exploration, for which there are almost no published taxonomic data. Here we describe Plenaster craigi gen. nov. sp. nov. from depths of ∼4000 m in the eastern CCZ polymetallic nodule province. Despite over 40 years of intense exploration in the area, we reveal that P. craigi sp. nov. is the most abundant sponge and the most common metazoan encrusting on nodules in our study area at the eastern CCZ. It has a mean abundance of 15.3 ± 8.9 individuals per m2 across 11 stations in a 30 × 30 km study site nested within the Singapore exploration area. The white encrusting sponge is filled with spheroxyasters with occasional styles protruding the surface. Plenaster craigi sp. nov. is morphologically similar to genera from three different families in two orders: Timea (Timeidae; Tethyida); Hemiasterella and Leptosastra (Hemiasterellidae; Tethyida); and Paratimea (Stelligeridae; Axinellida). However, based on the molecular (COI and 28S) phylogenetic trees generated in this study, P. craigi sp. nov. was located in the Order Axinellida and appeared to be distant to Timea, Hemiasterella, Leptosastra, and Paratimea. We propose a new genus for our material to be placed provisionally in the family Stelligeridae, as it is the only family in the order Axinellida whose members possess euasters. This provisional placement may change when sequences of the type specimens of these genera and advanced phylogenetic reconstruction methods become available in the future. However, we have shown clearly that Plenaster gen. nov. is unique and distinct from all currently known taxa. Plenaster craigi sp. nov. being an abundant metazoan encrusting on nodule and easily identified filter-feeding animal is a potentially indicator species for future mining impacts in the eastern CCZ, and possibly across the entire CCZ. LSID: urn:lsid:zoobank.org:act:99D2DDBA-4643-4752-8C8A-AFF314E6B6E3. © 2017, © The Trustees of the Natural History Museum, London 2017. All Rights Reserved.","28S; abyss; deep-sea mining; manganese nodule; new species; Plenastergen. nov; Porifera",,2-s2.0-85030328195
"Baek J., Choi Y.","A new algorithm to find raster-based least-cost paths using cut and fill operations",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025175579&doi=10.1080%2f13658816.2017.1356463&partnerID=40&md5=b6b264202de91bf9957c5c6646a536ee","We developed a least-cost path analysis algorithm that satisfies a slope threshold condition in hilly terrain. The new algorithm uses an expanding moving-window to explore a combination of cells that satisfy an elevation threshold condition and then supplements this by executing cut and fill operations when there are obstacle cells between source and destination cells. Cut and fill factors regarding the difference in the actual elevation and revised elevation are considered and a least-cost path is analyzed after calculating the accumulated travel cost to the destination point. After applying the developed algorithm to synthetic and real-world data, the least accumulated travel cost from the source point can then be calculated for all cells on the raster surface by considering various slope thresholds, moving-window sizes and raster data resolutions. This algorithm can be implemented as a useful tool in GIS software as well as engineering design software utilized in the construction and mining industries. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","cut and fill operations; digital elevation model; earthwork costs; Geographic Information Systems; least-cost path analysis","algorithm; cutting (excavation); digital elevation model; earthworks; GIS; path analysis; raster",2-s2.0-85025175579
"Mansourinejad M., Ketabchi M.","Modification of Olson–Cohen model for predicting stress-state dependency of martensitic transformation",2017,"Materials Science and Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021060788&doi=10.1080%2f02670836.2017.1342016&partnerID=40&md5=f715bbf343ec55cde1a9c56a66129449","The well-known Olson–Cohen model is modified by incorporating the effect of stress state on the transformation kinetics in metastable austenitic stainless steels. By assuming isothermal condition, the relationships between the parameters of Olson–Cohen model, stress triaxiality and absolute value of Lode angle parameter have been established. The proposed model predicts that the saturation level and slope of transformation curve increase with increasing the stress triaxiality in addition to the absolute value of Lode angle parameter. Uniaxial and plane strain tension tests have been conducted on the two types of austenitic stainless steels AISI 304 and AISI 201 to evaluate the validity of the model. The results show that the predicted transformation curves are in good agreement with the experimental data. © 2017 Institute of Materials, Minerals and Mining.","lode angle parameter; Olson–Cohen model; stress state; transformation kinetics; transformation-induced plasticity","Austenite; Austenitic stainless steel; Austenitic transformations; Metadata; Strain; Tensile testing; Effect of stress state; Isothermal conditions; Lode angle; Plane strain tension; Stress state; Stress-state dependency; Transformation induced plasticity; Transformation kinetics; Martensitic transformations",2-s2.0-85021060788
"Gan W., Lin J.C.-W., Chao H.-C., Zhan J.","Data mining in distributed environment: a survey",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024478319&doi=10.1002%2fwidm.1216&partnerID=40&md5=87fda68c3503fe14314bbd61acdb81d8","Due to the rapid growth of resource sharing, distributed systems are developed, which can be used to utilize the computations. Data mining (DM) provides powerful techniques for finding meaningful and useful information from a very large amount of data, and has a wide range of real-world applications. However, traditional DM algorithms assume that the data is centrally collected, memory-resident, and static. It is challenging to manage the large-scale data and process them with very limited resources. For example, large amounts of data are quickly produced and stored at multiple locations. It becomes increasingly expensive to centralize them in a single place. Moreover, traditional DM algorithms generally have some problems and challenges, such as memory limits, low processing ability, and inadequate hard disk, and so on. To solve the above problems, DM on distributed computing environment [also called distributed data mining (DDM)] has been emerging as a valuable alternative in many applications. In this study, a survey of state-of-the-art DDM techniques is provided, including distributed frequent itemset mining, distributed frequent sequence mining, distributed frequent graph mining, distributed clustering, and privacy preserving of distributed data mining. We finally summarize the opportunities of data mining tasks in distributed environment. WIREs Data Mining Knowl Discov 2017, 7:e1216. doi: 10.1002/widm.1216. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.",,"Distributed computer systems; Information management; Surveys; Distributed clustering; Distributed computing environment; Distributed data mining; Distributed environments; Distributed systems; Frequent itemset mining; Large amounts of data; Problems and challenges; Data mining",2-s2.0-85024478319
"Kavakiotis I., Samaras P., Triantafyllidis A., Vlahavas I.","FIFS: A data mining method for informative marker selection in high dimensional population genomic data",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030696585&doi=10.1016%2fj.compbiomed.2017.09.020&partnerID=40&md5=c32cf88d4315867fc05c00cb67cacc11","Background and objective Single Nucleotide Polymorphism (SNPs) are, nowadays, becoming the marker of choice for biological analyses involving a wide range of applications with great medical, biological, economic and environmental interest. Classification tasks i.e. the assignment of individuals to groups of origin based on their (multi-locus) genotypes, are performed in many fields such as forensic investigations, discrimination between wild and/or farmed populations and others. Τhese tasks, should be performed with a small number of loci, for computational as well as biological reasons. Thus, feature selection should precede classification tasks, especially for Single Nucleotide Polymorphism (SNP) datasets, where the number of features can amount to hundreds of thousands or millions. Methods In this paper, we present a novel data mining approach, called FIFS – Frequent Item Feature Selection, based on the use of frequent items for selection of the most informative markers from population genomic data. It is a modular method, consisting of two main components. The first one identifies the most frequent and unique genotypes for each sampled population. The second one selects the most appropriate among them, in order to create the informative SNP subsets to be returned. Results The proposed method (FIFS) was tested on a real dataset, which comprised of a comprehensive coverage of pig breed types present in Britain. This dataset consisted of 446 individuals divided in 14 sub-populations, genotyped at 59,436 SNPs. Our method outperforms the state-of-the-art and baseline methods in every case. More specifically, our method surpassed the assignment accuracy threshold of 95% needing only half the number of SNPs selected by other methods (FIFS: 28 SNPs, Delta: 70 SNPs Pairwise FST: 70 SNPs, In: 100 SNPs.) Conclusion Our approach successfully deals with the problem of informative marker selection in high dimensional genomic datasets. It offers better results compared to existing approaches and can aid biologists in selecting the most informative markers with maximum discrimination power for optimization of cost-effective panels with applications related to e.g. species identification, wildlife management, and forensics. © 2017 Elsevier Ltd","Ancestry informative marker; Big data; Bioinformatics; Data mining; Feature selection; Frequent pattern mining; Machine learning; Population genomics; Single nucleotide polymorphism","Big data; Bioinformatics; Classification (of information); Cost effectiveness; Feature extraction; Genes; Learning systems; Nucleotides; Population statistics; Ancestry informative marker; Classification tasks; Data mining methods; Forensic investigation; Frequent pattern mining; Genomics; Single nucleotide polymorphisms; Species identification; Data mining; Article; bioinformatics; breed difference; data mining; frequent item feature selection; gene frequency; genotype; Great Britain; heterozygosity; nonhuman; pig breed; population; population genetics; priority journal; single nucleotide polymorphism",2-s2.0-85030696585
"Lostado-Lorza R., Escribano-Garcia R., Fernandez-Martinez R., Illera-cueva M., Mac Donald B.J.","Using the finite element method and data mining techniques as an alternative method to determine the maximum load capacity in tapered roller bearings",2017,"Journal of Applied Logic",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006989209&doi=10.1016%2fj.jal.2016.11.009&partnerID=40&md5=0bd243a4779b8097058442ddc40c2c76","Double-row tapered roller bearings (TRBs) are mechanical devices designed to support a combination of preload, radial load, axial load and torque. They are widely used in vehicles for high load and moderate rotation speeds. This combination of loads produces high contact stresses on the bearing raceways that are difficult to calculate, and can cause undesirable defects like fatigue spalling and pitting. In recent decades, the Finite Element Method (FEM) has been used to obtain the distribution of the contact stresses on each of the raceways, although this method has the disadvantage of a high computational cost. The myriad of possible combinations of input loads on the TRB (preload, radial load, axial load and torque) makes it much harder to calculate the distribution of these contact stresses. This paper proposes a methodology that combines the FEM and data mining techniques to determine the maximum load capacity in TRBs. First, a three-dimensional finite element (FE) model was generated according to the real materials' properties, geometry and coefficients of friction of all parts that make up the double-row TRB. Subsequently, a Design of Experiment (DoE) was completed that considered a combination of the mentioned input loads, which were simulated in the FE model. Based on the contact stresses obtained from the FE simulations, a group of regression models – linear regression (LR), Gaussian processes (GP), artificial neural networks (ANN), support vector machines (SVM) and regression trees (RT) – were built to predict the contact stresses ratios that act on each of the row of rollers in the outer raceway of the TRB. Finally, the best combination of input loads was achieved by applying evolutionary optimization techniques based on genetic algorithms (GA) to the best regression models previously obtained. The maximum load capacity of the TRB was achieved when the radial load obtained was a maximum, while the stresses ratios of the two contacts in the outer raceway of the TRB were close to 25%. © 2016 Elsevier B.V.","Data mining techniques; Design of experiments; Double-row tapered roller bearing; Finite elements method; Optimization","Axial loads; Bearings (machine parts); Data mining; Design of experiments; Friction; Genetic algorithms; Neural networks; Optimization; Regression analysis; Roller bearings; Rollers (machine components); Support vector machines; Tapered roller bearings; Coefficients of friction; Computational costs; Evolutionary Optimization Techniques; Gaussian process; Maximum load capacity; Mechanical device; Regression model; Three dimensional finite elements; Finite element method",2-s2.0-85006989209
"Peral J., Maté A., Marco M.","Application of Data Mining techniques to identify relevant Key Performance Indicators",2017,"Computer Standards and Interfaces",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008235223&doi=10.1016%2fj.csi.2016.11.006&partnerID=40&md5=8db2402fbf53eed422c8f86d9d77e0cd","Currently dashboards are the preferred tool across organizations to monitor business performance. Dashboards are often composed of different data visualization techniques, amongst which are Key Performance Indicators (KPIs) which play a crucial role in quickly providing accurate information by comparing current performance against a target required to fulfil business objectives. However, KPIs are not always well known and sometimes it is difficult to find an appropriate KPI to associate with each business objective. In addition, Data Mining techniques are often used when forecasting trends and visualizing data correlations. In this paper we present a new approach to combining these two aspects in order to drive Data Mining techniques to obtain specific KPIs for business objectives in a semi-automated way. The main benefit of our approach is that organizations do not need to rely on existing KPI lists or test KPIs over a cycle as they can analyze their behavior using existing data. In order to show the applicability of our approach, we apply our proposal to the fields of Massive Open Online Courses (MOOCs) and Open Data extracted from the University of Alicante in order to identify the KPIs. © 2016 Elsevier B.V.","Big data; Data Mining; Decision trees; KPIs; Open Data","Benchmarking; Big data; Data visualization; Decision trees; Digital storage; Trees (mathematics); Business objectives; Business performance; Current performance; Key performance indicators; KPIs; Massive open online course; Open datum; Visualization technique; Data mining",2-s2.0-85008235223
"Xue P., Zhou Z., Fang X., Chen X., Liu L., Liu Y., Liu J.","Fault detection and operation optimization in district heating substations based on data mining techniques",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027583686&doi=10.1016%2fj.apenergy.2017.08.035&partnerID=40&md5=9368d59a17fe73ff18e302e303fb1c42","The present generation of district heating (DH) technologies will have to be further developed into the 4th generation to fulfil the important role in future smart energy systems. At present, automatic meter reading systems have been installed in DH systems. These systems make hourly or even minutely meter readings available at low cost. However, the sheer quantity and complex of the data poses a challenge at various levels for traditional data analysis approaches. Data mining is a promising technology and is used to automatically extract valuable knowledge hidden in large amounts of data. To investigate the potential application of descriptive data mining techniques in DH systems, this study proposes a method based on descriptive data mining to improve the energy performance of DH substations. The proposed method consists of five steps: data cleaning, data transformation, cluster analysis, association analysis, and interpretation/evaluation. Data cleaning and transformation are implemented to improve data quality and transform data into forms that are appropriate for mining. Cluster analysis is performed to identify distinct operating patterns of substations. Based on each pattern, association analysis is then adopted to discover the unsuspected knowledge in the form of rules. Interpretation/evaluation is performed to select and interpret potentially useful rules. To demonstrate its applicability, the proposed method is used to analyze the datasets obtained from an automatic meter reading system at two substations in the DH system in Changchun, China. This application reveals that the method can effectively extract potentially useful knowledge and thereby provide essential guidance for the fault detection and operation optimization of DH substations. © 2017","Automatic meter reading system; Data mining; District heating substation; Fault detection; Operation optimization",,2-s2.0-85027583686
"Nariya M.K., Kim J.H., Xiong J., Kleindl P.A., Hewarathna A., Fisher A.C., Joshi S.B., Schöneich C., Forrest M.L., Middaugh C.R., Volkin D.B., Deeds E.J.","Comparative Characterization of Crofelemer Samples Using Data Mining and Machine Learning Approaches With Analytical Stability Data Sets",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593356&doi=10.1016%2fj.xphs.2017.07.013&partnerID=40&md5=b4e108e740352c6219254a3f3aade778","There is growing interest in generating physicochemical and biological analytical data sets to compare complex mixture drugs, for example, products from different manufacturers. In this work, we compare various crofelemer samples prepared from a single lot by filtration with varying molecular weight cutoffs combined with incubation for different times at different temperatures. The 2 preceding articles describe experimental data sets generated from analytical characterization of fractionated and degraded crofelemer samples. In this work, we use data mining techniques such as principal component analysis and mutual information scores to help visualize the data and determine discriminatory regions within these large data sets. The mutual information score identifies chemical signatures that differentiate crofelemer samples. These signatures, in many cases, would likely be missed by traditional data analysis tools. We also found that supervised learning classifiers robustly discriminate samples with around 99% classification accuracy, indicating that mathematical models of these physicochemical data sets are capable of identifying even subtle differences in crofelemer samples. Data mining and machine learning techniques can thus identify fingerprint-type attributes of complex mixture drugs that may be used for comparative characterization of products. © 2017 American Pharmacists Association®","comparative characterization; crofelemer; data mining; supervised learning","crofelemer; accuracy; analytic method; Article; data mining; discriminant analysis; filtration; fractionation; incubation time; machine learning; mathematical model; molecular weight; physical chemistry; principal component analysis; temperature",2-s2.0-85028593356
"Batmaz İ., Danışoğlu S., Yazıcı C., Kartal-Koç E.","A data mining application to deposit pricing: Main determinants and prediction models",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027218367&doi=10.1016%2fj.asoc.2017.07.047&partnerID=40&md5=cfbb5fa08aba75deabea2fd29788e08b","This study provides unique empirical evidence regarding the determinants of deposit pricing by employing data mining methods and making use of proprietary data provided by a commercial bank. Results highlight the importance of taking into account customer- and account-specific characteristics in the determination of deposit rates. Contrary to existing evidence obtained from macro-level bank data, the customer-level data used in this study suggest that depositors with a multi-faceted and long-term relationship with the same bank seem to benefit from higher deposit rates as a reward for being a core depositor. The location of the customer is also shown to have a limited effect on the deposit rates. © 2017 Elsevier B.V.","Artificial neural networks; Classification and regression trees; Core deposits; Deposit pricing; Deposit rates; Generalized linear models; Multivariate adaptive regression splines; Random forest; Support vector regression","Costs; Data mining; Decision trees; Neural networks; Regression analysis; Sales; Classification and regression tree; Deposit rate; Generalized linear model; Multivariate adaptive regression splines; Random forests; Support vector regression (SVR); Deposits",2-s2.0-85027218367
"Shouval R., Hadanny A., Shlomo N., Iakobishvili Z., Unger R., Zahger D., Alcalai R., Atar S., Gottlieb S., Matetzky S., Goldenberg I., Beigel R.","Machine learning for prediction of 30-day mortality after ST elevation myocardial infraction: An Acute Coronary Syndrome Israeli Survey data mining study",2017,"International Journal of Cardiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028571788&doi=10.1016%2fj.ijcard.2017.05.067&partnerID=40&md5=70263613ff04d109eeb8bb8e8d4fe002","Background Risk scores for prediction of mortality 30-days following a ST-segment elevation myocardial infarction (STEMI) have been developed using a conventional statistical approach. Objective To evaluate an array of machine learning (ML) algorithms for prediction of mortality at 30-days in STEMI patients and to compare these to the conventional validated risk scores. Methods This was a retrospective, supervised learning, data mining study. Out of a cohort of 13,422 patients from the Acute Coronary Syndrome Israeli Survey (ACSIS) registry, 2782 patients fulfilled inclusion criteria and 54 variables were considered. Prediction models for overall mortality 30 days after STEMI were developed using 6 ML algorithms. Models were compared to each other and to the Global Registry of Acute Coronary Events (GRACE) and Thrombolysis In Myocardial Infarction (TIMI) scores. Results Depending on the algorithm, using all available variables, prediction models' performance measured in an area under the receiver operating characteristic curve (AUC) ranged from 0.64 to 0.91. The best models performed similarly to the Global Registry of Acute Coronary Events (GRACE) score (0.87 SD 0.06) and outperformed the Thrombolysis In Myocardial Infarction (TIMI) score (0.82 SD 0.06, p < 0.05). Performance of most algorithms plateaued when introduced with 15 variables. Among the top predictors were creatinine, Killip class on admission, blood pressure, glucose level, and age. Conclusions We present a data mining approach for prediction of mortality post-ST-segment elevation myocardial infarction. The algorithms selected showed competence in prediction across an increasing number of variables. ML may be used for outcome prediction in complex cardiology settings. © 2017 Elsevier Ireland Ltd","Data mining; Machine learning; Mortality; Outcome; STEMI",,2-s2.0-85028571788
"Alfian G., Rhee J., Ahn H., Lee J., Farooq U., Ijaz M.F., Syaekhoni M.A.","Integration of RFID, wireless sensor networks, and data mining in an e-pedigree food traceability system",2017,"Journal of Food Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019689648&doi=10.1016%2fj.jfoodeng.2017.05.008&partnerID=40&md5=786a241e03f662dd2785165a98e2c6a2","Due to the growing customer health awareness, food quality and safety has gained considerable attention. Therefore, consumer demand for complete visibility of food quality and history along the supply chain has significantly increased. This study proposes an e-pedigree food traceability system, utilizing radio frequency identification technology to track and trace product location and wireless sensor network to collect temperature and humidity during storage and transportation. Missing sensor data may occur in real cases, as sensor data are lost or corrupted due to many reasons. The proposed system utilizes data mining techniques to predict missing sensor data. The proposed system was tested for kimchi supply chain in Korea, and showed significant benefit to managers as well as customers by providing real-time location as well as complete temperature and humidity history. The multilayer perceptron model provided the best prediction accuracy for missing sensor data compared to other models. The proposed e-pedigree food traceability system will help managers optimize food distribution while also increasing customer satisfaction, as it can monitor product freshness. © 2017 Elsevier Ltd","Data mining; e-pedigree; RFID; Traceability; WSN","Customer satisfaction; Data mining; Digital storage; Food safety; Food storage; Managers; Radio frequency identification (RFID); Sales; Supply chains; Food quality and safeties; Food traceability systems; Prediction accuracy; Radio frequency identification technology; Real-time location; Storage and transportations; Temperature and humidities; Traceability; Wireless sensor networks",2-s2.0-85019689648
"Ghasri M., Hossein Rashidi T., Waller S.T.","Developing a disaggregate travel demand system of models using data mining techniques",2017,"Transportation Research Part A: Policy and Practice",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028939550&doi=10.1016%2fj.tra.2017.08.020&partnerID=40&md5=0566a8189511c8d1a3bb83ac0e711d4c","The travel demand modelling has experienced a paradigm shift from aggregate to disaggregate models, leading to an increase in computational time and simulation cost. Meanwhile, transferability models have emerged to reduce the associated cost and computational burden, but haven't discounted the disaggregation level. This research proposes the proof of the concept of an innovative transferability modelling framework to estimate total number of trips and trip attributes in a tour of trips at a disaggregate level. In contrast to tour-based or activity-based models, the focus of transferability models is on replicating trip patterns rather than reflecting travellers’ behaviour. Similar to previous transferability models, classifying decision tree is utilized as one of the modelling techniques in this study. Moreover, the merits of a modified version of decision tree and the random forest methods are examined. Victorian Integrated Survey of Travel and Activity (VISTA) in 2007 and 2009 are utilized to calibrate and validate the proposed framework, respectively. According to the results, the random forest method shows highest individual-level accuracy while matching the system-level observed distributions. © 2017 Elsevier Ltd","Disaggregate modelling structure; Random forest; Transferability modelling; Travel demand modelling","Decision trees; Transportation; Activity-based models; Computational burden; Computational time; Modelling framework; Modelling techniques; Random forest methods; Random forests; Travel demand modelling; Data mining; algorithm; data mining; numerical model; paradigm shift; travel demand",2-s2.0-85028939550
"Shi F., Chen L., Han J., Childs P.","A Data-Driven Text Mining and Semantic Network Analysis for Design Information Retrieval",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030458529&doi=10.1115%2f1.4037649&partnerID=40&md5=56e526b728ede0c2b89e9414f0bd7353","With the advent of the big-data era, massive information stored in electronic and digital forms on the internet become valuable resources for knowledge discovery in engineering design. Traditional document retrieval method based on document indexing focuses on retrieving individual documents related to the query, but is incapable of discovering the various associations between individual knowledge concepts. Ontology-based technologies, which can extract the inherent relationships between concepts by using advanced text mining tools, can be applied to improve design information retrieval in the largescale unstructured textual data environment. However, few of the public available ontology database stands on a design and engineering perspective to establish the relations between knowledge concepts. This paper develops a WordNet focusing on design and engineering associations by integrating the text mining approaches to construct an unsupervised learning ontology network. Subsequent probability and velocity network analysis are applied with different statistical behaviors to evaluate the correlation degree between concepts for design information retrieval. The validation results show that the probability and velocity analysis on our constructed ontology network can help recognize the high related complex design and engineering associations between elements. Finally, an engineering design case study demonstrates the use of our constructed semantic network in real-world project for design relations retrieval.",,"Big data; Design; Information retrieval; Ontology; Query processing; Semantic Web; Semantics; Design information; Document Retrieval; Engineering design; Engineering perspective; Real world projects; Semantic network analysis; Statistical behavior; Validation results; Data mining",2-s2.0-85030458529
"McComb C., Cagan J., Kotovsky K.","Mining Process Heuristics from Designer Action Data Via Hidden Markov Models",2017,"Journal of Mechanical Design, Transactions of the ASME",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029154277&doi=10.1115%2f1.4037308&partnerID=40&md5=ab33da111d38b264978f05f9b248497c","Configuration design problems, characterized by the assembly of components into a final desired solution, are common in engineering design. Various theoretical approaches have been offered for solving configuration type problems, but few studies have examined the approach that humans naturally use to solve such problems. This work applies data-mining techniques to quantitatively study the processes that designers use to solve configuration design problems. The guiding goal is to extract beneficial design process heuristics that are generalizable to the entire class of problems. The extraction of these human problem-solving heuristics is automated through the application of hidden Markov models to the data from two behavioral studies. Results show that designers proceed through four procedural states in solving configuration design problems, roughly transitioning from topology design to shape and parameter design. High-performing designers are distinguished by their opportunistic tuning of parameters early in the process, enabling a more effective and nuanced search for solutions. Copyright © 2017 by ASME.",,"Data mining; Design; Hidden Markov models; Markov processes; Product design; Behavioral studies; Configuration designs; Engineering design; Human problem solving; Parameter designs; Theoretical approach; Topology design; Tuning of parameters; Problem solving",2-s2.0-85029154277
"Simovici D.A., Vetro R., Hua K.","Ultrametricity of dissimilarity spaces and its significance for data mining",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994701973&doi=10.1007%2f978-3-319-45763-5_8&partnerID=40&md5=774480b61dfe7a69da020f5b7173c2c4","We introduce a measure of ultrametricity for dissimilarity spaces and examine transformations of dissimilarities that impact this measure. Then, we study the influence of ultrametricity on the behavior of two classes of data mining algorithms (kNN classification and PAM clustering) applied on dissimilarity spaces.We show that there is an inverse variation between ultrametricity and performance of classifiers. For clustering, increased ultrametricity generate clusterings with better separation. Lowering ultrametricity produces more compact clusters. © Springer International Publishing Switzerland 2017.",,,2-s2.0-84994701973
"Semanjski I., Gautama S., Ahas R., Witlox F.","Spatial context mining approach for transport mode recognition from mobile sensed big data",2017,"Computers, Environment and Urban Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026912795&doi=10.1016%2fj.compenvurbsys.2017.07.004&partnerID=40&md5=80fe07fa319e300e411d668e7625d74a","Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms. © 2017 Elsevier Ltd","Context mining; Geographic information systems; Mobile sensed big data; Smart city; Spatial awareness; Support vector machines; Transport mode recognition; Urban data",,2-s2.0-85026912795
"Zhao C., Johnsson M., He M.","Erratum to: Data mining with clustering algorithms to reduce packaging costs: A case study (Packaging Technology and Science, (2017), 30, 5, (173-193), 10.1002/pts.2286)",2017,"Packaging Technology and Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020707205&doi=10.1002%2fpts.2325&partnerID=40&md5=2516c8de7077e72143af6ecf2ed226eb","Subsequent to publication of this article, the following Acknowledgement of funding has been added: ACKNOWLEDGEMENTS This work was supported by the Research Foundation for Youth Scholars of Beijing Technology and Business University, Grant Number: QNJJ2017 24. Copyright © 2017 John Wiley & Sons, Ltd.",,,2-s2.0-85020707205
"Chiochetta C.G., Toumi H., Böhm R.F.S., Engel F., Poyer-Radetski G., Rörig L.R., Adani F., Radetski C.M.","Use of phytoproductivity data in the choice of native plant species to restore a degraded coal mining site amended with a stabilized industrial organic sludge",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029529634&doi=10.1007%2fs11356-017-0128-6&partnerID=40&md5=91dbf8d5e7b9844f191bebcd44b01b7b","Coal mining-related activities result in a degraded landscape and sites associated with large amounts of dumped waste material. The arid soil resulting from acid mine drainage affects terrestrial and aquatic ecosystems, and thus, site remediation programs must be implemented to mitigate this sequential deleterious processes. A low-cost alternative material to counterbalance the affected physico-chemical-microbiological aspects of the degraded soil is the amendment with low contaminated and stabilized industrial organic sludge. The content of nutrients P and N, together with stabilized organic matter, makes this material an excellent fertilizer and soil conditioner, fostering biota colonization and succession in the degraded site. However, choice of native plant species to restore a degraded site must be guided by some minimal criteria, such as plant survival/adaptation and plant biomass productivity. Thus, in this 3-month study under environmental conditions, phytoproductivity tests with five native plant species (Surinam cherry Eugenia uniflora L., C. myrianthum–Citharexylum myrianthum, Inga–Inga spp., Brazilian peppertree Schinus terebinthifolius, and Sour cherry Prunus cerasus) were performed to assess these criteria, and additional biochemical parameters were measured in plant tissues (i.e., protein content and peroxidase activity) exposed to different soil/sludge mixture proportions. The results show that three native plants were more adequate to restore vegetation on degraded sites: Surinam cherry, C. myrianthum, and Brazilian peppertree. Thus, this study demonstrates that phytoproductivity tests associated with biochemical endpoint measurements can help in the choice of native plant species, as well as aiding in the choice of the most appropriate soil/stabilized sludge proportion in order to optimize biomass production. © 2017, Springer-Verlag GmbH Germany.","Degraded soil; Native plants; Phytotoxicity; Sludge amendment; Soil restoration; Stabilized industrial sludge","acid mine drainage; activated sludge; aquatic ecosystem; biomass; coal mining; data set; fertilizer; native species; organic matter; physicochemical property; phytotoxicity; plant; plant community; soil amendment; soil degradation; soil remediation; vegetation cover; Citharexylum myrianthum; Eugenia uniflora; Inga; Prunus cerasus; Schinus terebinthifolius",2-s2.0-85029529634
"Huang C., Lu R., Choo K.-K.R.","Secure and flexible cloud-assisted association rule mining over horizontally partitioned databases",2017,"Journal of Computer and System Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008517603&doi=10.1016%2fj.jcss.2016.12.005&partnerID=40&md5=692a8b8da4db4b026989bc90d9ea44e6","With recent trends in big data and cloud computing, data mining has also attracted considerable interest due to its potential to deal with distributed data in the cloud. However, existing data mining technologies may not be directly deployed as we need to avoid accidental privacy disclosure when data from different sources are mined. In this paper, we propose a secure and flexible cloud-assisted association rule mining over horizontally partitioned databases. Using the proposed scheme, data owners can provide their data and mine the association rules in the cloud flexibly, while being assured of minimal risks of privacy leakage. We then show that our proposed scheme achieves privacy-preserving mining of association rules, and provides resilience against collusion attacks. A comparative summary demonstrates that the proposed scheme is more efficient, in terms of computational costs, relative to several existing homomorphic-encryption-based schemes. © 2016 Elsevier Inc.","Association rule mining; Big data mining; Cloud privacy-preserving; Resilience against collusion attacks","Association rules; Big data; Cryptography; Data privacy; Collusion attack; Computational costs; Data mining technology; Ho-momorphic encryptions; Mining of association rules; Partitioned database; Privacy disclosures; Privacy preserving; Data mining",2-s2.0-85008517603
"Lim Y., Kang U.","Time-weighted counting for recently frequent pattern mining in data streams",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015828269&doi=10.1007%2fs10115-017-1045-1&partnerID=40&md5=ea2276da0c48c4ae6c4b3289d09d0eeb","How can we discover interesting patterns from time-evolving high-speed data streams? How to analyze the data streams quickly and accurately, with little space overhead? How to guarantee the found patterns to be self-consistent? High-speed data stream has been receiving increasing attention due to its wide applications such as sensors, network traffic, social networks, etc. The most fundamental task on the data stream is frequent pattern mining; especially, focusing on recentness is important in real applications. In this paper, we develop two algorithms for discovering recently frequent patterns in data streams. First, we propose TwMinSwap to find top-k recently frequent items in data streams, which is a deterministic version of our motivating algorithm TwSample providing theoretical guarantees based on item sampling. TwMinSwap improves TwSample in terms of speed, accuracy, and memory usage. Both require only O(k) memory spaces and do not require any prior knowledge on the stream such as its length and the number of distinct items in the stream. Second, we propose TwMinSwap-Is to find top-k recently frequent itemsets in data streams. We especially focus on keeping self-consistency of the discovered itemsets, which is the most important property for reliable results, while using O(k) memory space with the assumption of a constant itemset size. Through extensive experiments, we demonstrate that TwMinSwap outperforms all competitors in terms of accuracy and memory usage, with fast running time. We also show that TwMinSwap-Is is more accurate than the competitor and discovers recently frequent itemsets with reasonably large sizes (at most 5–7) depending on datasets. Thanks to TwMinSwap and TwMinSwap-Is, we report interesting discoveries in real world data streams, including the difference of trends between the winner and the loser of U.S. presidential candidates, and temporal human contact patterns. © 2017, Springer-Verlag London.","Data stream; Frequent items; Frequent itemsets; Hot items; Sampling; Time-weighted counting; Top-k items",,2-s2.0-85015828269
"Song B., Luo J.","Mining Patent Precedents for Data-Driven Design: The Case of Spherical Rolling Robots",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030474081&doi=10.1115%2f1.4037613&partnerID=40&md5=96e09f0a2fdfca90b2cf6010e70f757a","Data-driven engineering designers often search for design precedents in patent databases to learn about relevant prior arts, seek design inspiration, or assess the novelty of their own new inventions. However, patent retrieval relevant to the design of a specific product or technology is often unstructured and unguided, and the resultant patents do not sufficiently or accurately capture the prior design knowledge base. This paper proposes an iterative and heuristic methodology to comprehensively search for patents as precedents of the design of a specific technology or product for data-driven design. The patent retrieval methodology integrates the mining of patent texts, citation relationships, and inventor information to identify relevant patents; particularly, the search keyword set, citation network, and inventor set are expanded through the designer's heuristic learning from the patents identified in prior iterations. The method relaxes the requirement for initial search keywords while improving patent retrieval completeness and accuracy. We apply the method to identify self-propelled spherical rolling robot (SPSRRs) patents. Furthermore, we present two approaches to further integrate, systemize, visualize, and make sense of the design information in the retrieved patent data for exploring new design opportunities. Our research contributes to patent data-driven design.",,"Heuristic methods; Iterative methods; Knowledge based systems; Machine design; Patents and inventions; Product design; Citation networks; Data-driven design; Design information; Design knowledge; Design precedents; Engineering designer; Heuristic learning; Patent retrieval; Search engines",2-s2.0-85030474081
"Soriano-Vargas A., Vani B.C., Shimabukuro M.H., G. Monico J.F., F. Oliveira M.C., Hamann B.","Visual analytics of time-varying multivariate ionospheric scintillation data",2017,"Computers and Graphics (Pergamon)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028943956&doi=10.1016%2fj.cag.2017.08.013&partnerID=40&md5=e1ca68e914efb0d6fc4da94a6d333192","We present a clustering-based interactive approach to multivariate data analysis, motivated by the specific needs of scintillation data. Ionospheric scintillation is a rapid variation in the amplitude and/or phase of radio signals traveling through the ionosphere. This spatial and time-varying phenomenon is of great interest since it affects the reception quality of satellite signals. Specialized receivers at strategic regions can track multiple variables related to this phenomenon, generating a database of observations of regional ionospheric scintillation. We introduce a visual analytics solution to support analysis of such data, keeping in mind the general applicability of our approach to similar multivariate data analysis situations. Taking into account typical user questions, we combine visualization and data mining algorithms that satisfy these goals: (i) derive a representation of the variables monitored that conveys their behavior in detail, at multiple user-defined aggregation levels; (ii) provide overviews of multiple variables regarding their behavioral similarity over selected time periods; (iii) support users when identifying representative variables for characterizing scintillation behavior. We illustrate the capabilities of our proposed framework by presenting case studies driven directly by questions formulated by collaborating domain experts. © 2017 Elsevier Ltd","Data visualization; Ionospheric scintillation; Time-varying multivariate data; Visual analytics; Visual feature selection","Data handling; Data mining; Information analysis; Ionosphere; Ionospheric measurement; Multivariant analysis; Scintillation; Visualization; Behavioral similarities; Data mining algorithm; Interactive approach; Ionospheric scintillation; Multivariate data; Multivariate data analysis; Satellite signals; Visual analytics; Data visualization",2-s2.0-85028943956
"Yuan X., Liu Y., Huang Y., Tian F.","An approach to quality validation of large-scale data from the Chinese Flash Flood Survey and Evaluation (CFFSE)",2017,"Natural Hazards",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023746522&doi=10.1007%2fs11069-017-2986-0&partnerID=40&md5=a49eb26fd724708402d8b390a2570940","Quality control of large-scale flash flood survey and evaluation data is vital and refers to various social and natural factors. In this study, we present a quality validation approach that uses a data model, Anselin Local Moran’s I (DM-Moran), which is based on a model of the flash flood data and a spatial data mining algorithm. The approach of the DM-Moran model involves examining logical relationships and detecting anomalous survey units, which effectively integrates the advantages of certainty rules and checking for reasonableness. It resolves the inconsistencies in massive amounts of flash flood survey data that result from inconsistencies. We used the DM-Moran model to validate the quality of the data of the Chinese Flash Flood Survey and Evaluation (CFFSE) project. The kappa coefficients of the two steps of this approach were 0.95 and 0.99, which meet the requirements of the CFFSE project. We consider the DM-Moran model an effective approach to checking the quality of various other large-scale disaster datasets. © 2017, Springer Science+Business Media B.V.","Data quality validation; DM-Moran model; Flash flood; Spatial data mining; Survey and evaluation","algorithm; data mining; data quality; data set; flash flood; numerical model; spatial data; survey",2-s2.0-85023746522
"Huang Q., Cervone G., Zhang G.","A cloud-enabled automatic disaster analysis system of multi-sourced data streams: An example synthesizing social media, remote sensing and Wikipedia data",2017,"Computers, Environment and Urban Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026778223&doi=10.1016%2fj.compenvurbsys.2017.06.004&partnerID=40&md5=c92e7d74021441c2ed45a76f29566640","Social media streams and remote sensing data have emerged as new sources for tracking disaster events, and assessing their damages. Previous studies focus on a case-by-case approach, where a specific event was first chosen and filtering criteria (e.g., keywords, spatiotemporal information) are manually designed and used to retrieve relevant data for disaster analysis. This paper presents a framework that synthesizes multi-sourced data (e.g., social media, remote sensing, Wikipedia, and Web), spatial data mining and text mining technologies to build an architecturally resilient and elastic solution to support disaster analysis of historical and future events. Within the proposed framework, Wikipedia is used as a primary source of different historical disaster events, which are extracted to build an event database. Such a database characterizes the salient spatiotemporal patterns and characteristics of each type of disaster. Additionally, it can provide basic semantics, such as event name (e.g., Hurricane Sandy) and type (e.g., flooding) and spatiotemporal scopes, which are then tuned by the proposed procedures to extract additional information (e.g., hashtags for searching tweets), to query and retrieve relevant social media and remote sensing data for a specific disaster. Besides historical event analysis and pattern mining, the cloud-based framework can also support real-time event tracking and monitoring by providing on-demand and elastic computing power and storage capabilities. A prototype is implemented and tested with data relative to the 2011 Hurricane Sandy and the 2013 Colorado flooding. © 2017 Elsevier Ltd","Disaster coordination and relief; Disaster management","Digital storage; Disaster prevention; Disasters; Floods; Hurricanes; Information filtering; Query processing; Remote sensing; Semantics; Social networking (online); Disaster management; Filtering criteria; Historical disasters; Remote sensing data; Spatial data mining; Spatiotemporal information; Spatiotemporal patterns; Storage capability; Data mining; data mining; database; disaster management; disaster relief; historical record; remote sensing; social media; spatial data; World Wide Web",2-s2.0-85026778223
"Shknevsky A., Shahar Y., Moskovitch R.","Consistent discovery of frequent interval-based temporal patterns in chronic patients’ data",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030872549&doi=10.1016%2fj.jbi.2017.10.002&partnerID=40&md5=e738d8b719cc47c5a0335a7bb00aadd4","Increasingly, frequent temporal patterns discovered in longitudinal patient records are proposed as features for classification and prediction, and as means to cluster patient clinical trajectories. However, to justify that, we must demonstrate that most frequent temporal patterns are indeed consistently discoverable within the records of different patient subsets within similar patient populations. We have developed several measures for the consistency of the discovery of temporal patterns. We focus on time-interval relations patterns (TIRPs) that can be discovered within different subsets of the same patient population. We expect the discovered TIRPs (1) to be frequent in each subset, (2) preserve their “local” metrics - the absolute frequency of each pattern, measured by a Proportion Test, and (3) preserve their “global” characteristics - their overall distribution, measured by a Kolmogorov-Smirnov test. We also wanted to examine the effect on consistency, over a variety of settings, of varying the minimal frequency threshold for TIRP discovery, and of using a TIRP-filtering criterion that we previously introduced, the Semantic Adjacency Criterion (SAC). We applied our methodology to three medical domains (oncology, infectious hepatitis, and diabetes). We found that, within the minimal frequency ranges we had examined, 70–95% of the discovered TIRPs were consistently discoverable; 40–48% of them maintained their local frequency. TIRP global distribution similarity varied widely, from 0% to 65%. Increasing the threshold usually increased the percentage of TIRPs that were repeatedly discovered across different patient subsets within the same domain, and the probability of a similar TIRP distribution. Using the SAC principle, enhanced, for most minimal support levels, the percentage of repeating TIRPs, their local consistency and their global consistency. The effect of using the SAC was further strengthened as the minimal frequency threshold was raised. © 2017 Elsevier Inc.","Classification; Clustering; Frequent pattern mining; Pattern consistency; Pattern repetition; Prediction; Temporal abstraction; Temporal data mining; Temporal knowledge discovery; Time intervals mining","Classification (of information); Data mining; Forecasting; Large scale systems; Semantics; Set theory; Clustering; Frequent pattern mining; Pattern consistency; Pattern repetition; Temporal abstraction; Temporal data mining; Time interval; Probability distributions; algorithm; Article; chronic patient; criterion variable; data mining; diabetes mellitus; frequency; human; infectious hepatitis; medical record; methodology; oncology; patient coding; priority journal; probability; semantic adjacency criterion; symbolic time interval; time factor; time interval relations pattern; time perception; validation study",2-s2.0-85030872549
"Li T., Liu Z., Li J., Jia C., Li K.-C.","CDPS: A cryptographic data publishing system",2017,"Journal of Computer and System Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008419768&doi=10.1016%2fj.jcss.2016.12.004&partnerID=40&md5=c8cdd7fccd9c3a13904210620e307a97","The traditional data publishing methods will remove the sensitive attributes and generate the abundant records to achieve the goal of privacy protection. In the big data environment, the requirement of utilizing data (e.g., data mining) become more and more various, which is beyond the scope of the traditional method. This paper provides a cryptographic data publishing system that preserves the data integrity (i.e., the original data structure is preserved) and achieves anonymity without deletion of any attribute or utilization of redundancy. The security analysis shows that our system is secure under our proposed security model. © 2017 Elsevier Inc.","Big data; Data privacy; Data publishing; Format-preserving encryption","Cryptography; Data mining; Data privacy; Data environment; Data integrity; Data publishing; Format-preserving encryptions; Privacy protection; Security analysis; Security model; Sensitive attribute; Big data",2-s2.0-85008419768
"Subramaniyaswamy V., Logesh R.","Adaptive KNN based Recommender System through Mining of User Preferences",2017,"Wireless Personal Communications",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020548595&doi=10.1007%2fs11277-017-4605-5&partnerID=40&md5=e9a88e5aa7ba4f7f3d67f73aef27a39a","Research for the generation of reliable recommendations has been the main goal focused by many researchers in recent years. Though many recommendation approaches have been developed to assist users in the selection of their interesting items in the online world, still the personalization problem exists. In this paper, we present a new recommendation approach to address the problems such as scalability, sparsity, and cold-start in a collective way. We have developed a knowledge-based domain specific ontology for the generation of personalized recommendations. We have also introduced two different ontology-based predictive models as minion representation model and prominent representation model for the effective generation of recommendations to all types of users. The prediction models are induced by data mining algorithms by correlating the user preferences and features of items for user modeling. We have proposed a new variant of KNN algorithm as Adaptive KNN for the collaborative filtering based recommender system. The proposed recommendation approach is validated with standard MovieLens dataset and obtained results are evaluated with Precision, Recall, F-Measure, and Accuracy. The experimental results had proved the better performance of our proposed AKNN algorithm over other algorithms with the highly sparse data taken for the recommendation generation. © 2017, Springer Science+Business Media, LLC.","Adaptive KNN; Clustering; Ontology; Personalization; Recommender system; Web mining","Collaborative filtering; Data mining; Knowledge based systems; Ontology; Adaptive KNN; Clustering; Data mining algorithm; Domain-specific ontologies; Personalizations; Personalized recommendation; Representation model; Web Mining; Recommender systems",2-s2.0-85020548595
"Jeong J., Park E., Han W.S., Kim K.-Y., Jun S.-C., Choung S., Yun S.-T., Oh J., Kim H.-J.","A predictive estimation method for carbon dioxide transport by data-driven modeling with a physically-based data model",2017,"Journal of Contaminant Hydrology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030475847&doi=10.1016%2fj.jconhyd.2017.09.011&partnerID=40&md5=0fb05dfac917a1da290c2c3004d86a98","In this study, a data-driven method for predicting CO2 leaks and associated concentrations from geological CO2 sequestration is developed. Several candidate models are compared based on their reproducibility and predictive capability for CO2 concentration measurements from the Environment Impact Evaluation Test (EIT) site in Korea. Based on the data mining results, a one-dimensional solution of the advective–dispersive equation for steady flow (i.e., Ogata-Banks solution) is found to be most representative for the test data, and this model is adopted as the data model for the developed method. In the validation step, the method is applied to estimate future CO2 concentrations with the reference estimation by the Ogata-Banks solution, where a part of earlier data is used as the training dataset. From the analysis, it is found that the ensemble mean of multiple estimations based on the developed method shows high prediction accuracy relative to the reference estimation. In addition, the majority of the data to be predicted are included in the proposed quantile interval, which suggests adequate representation of the uncertainty by the developed method. Therefore, the incorporation of a reasonable physically-based data model enhances the prediction capability of the data-driven model. The proposed method is not confined to estimations of CO2 concentration and may be applied to various real-time monitoring data from subsurface sites to develop automated control, management or decision-making systems. © 2017 Elsevier B.V.","CO2 concentration; Cyber-physical system (CPS); Data model; Data-driven model; Early warning system (EWS); Process-based model","Carbon; Carbon dioxide; Data mining; Data structures; Decision making; Embedded systems; Estimation; Forecasting; Real time systems; CO2 concentration; Cyber-physical systems (CPS); Data-driven model; Early warning systems; Process-based modeling; Information management; carbon dioxide; carbon sequestration; concentration (composition); data set; early warning system; estimation method; leakage; steady flow; transport process; Article; carbon dioxide transport; carbon sequestration; concentration (parameters); controlled study; data analysis; data mining; flow measurement; mathematical analysis; molecular model; prediction; priority journal; process development; Korea",2-s2.0-85030475847
"Lughofer E.","On-line active learning: A new paradigm to improve practical useability of data stream modeling methods",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021725695&doi=10.1016%2fj.ins.2017.06.038&partnerID=40&md5=5b9a29b56986674fba83487e2154dc5e","The central purpose of this survey is to provide readers an insight into the recent advances and challenges in on-line active learning. Active learning has attracted the data mining and machine learning community since around 20 years. This is because it served for important purposes to increase practical applicability of machine learning techniques, such as (i) to reduce annotation and measurement costs for operators and measurement equipments, (ii) to reduce manual labeling effort for experts and (iii) to reduce computation time for model training. Almost all of the current techniques focus on the classical pool-based approach, which is off-line by nature as iterating over a pool of (unlabeled) reference samples a multiple times to choose the most promising ones for improving the performance of the classifiers. This is achieved by (time-intensive) re-training cycles on all labeled samples available so far. For the on-line, stream mining case, the challenge is that the sample selection strategy has to operate in a fast, ideally single-pass manner. Some first approaches have been proposed during the last decade (starting from around 2005) with the usage of machine learning (ML) oriented incremental classifiers, which are able to update their parameters based on selected samples, but not their structures. Since 2012, on-line active learning concepts have been proposed in connection with the paradigm of evolving models, which are able to expand their knowledge into feature space regions so far unexplored. This opened the possibility to address a particular type of uncertainty, namely that one which stems from a significant novelty content in streams, as, e.g., caused by drifts, new operation modes, changing system behaviors or non-stationary environments. We will provide an overview about the concepts and techniques for sample selection and active learning within these two principal major research lines (incremental ML models versus evolving systems), a comparison of their essential characteristics and properties (raising some advantages and disadvantages), and a study on possible evaluation techniques for them. We conclude with an overview of real-world application examples where various on-line AL approaches have been already successfully applied in order to significantly reduce user's interaction efforts and costs for model updates. © 2017 Elsevier Inc.","Data stream mining; Evolving models; Incremental ML and DM methods; Interaction effort and cost reduction; On-line active learning; Single-pass sample selection; Uncertainty and novelty in streams","Artificial intelligence; Cost reduction; Costs; Data communication systems; Data mining; E-learning; Learning systems; Personnel training; Active Learning; Data stream mining; Evolving models; Incremental ML and DM methods; Sample selection; Uncertainty and novelty in streams; Education",2-s2.0-85021725695
"Nawapornanan C., Intakosum S., Boonjing V.","High candidates generation: A new efficient method for mining share-frequent patterns",2017,"Jurnal Teknologi",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032255745&doi=10.11113%2fjt.v79.10292&partnerID=40&md5=0bb3e8b31a44e34e43b53774ec1cf72e","The share frequent patterns mining is more practical than the traditional frequent patternset mining because it can reflect useful knowledge such as total costs and profits of patterns. Mining share-frequent patterns becomes one of the most important research issue in the data mining. However, previous algorithms extract a large number of candidate and spend a lot of time to generate and test a large number of useless candidate in the mining process. This paper proposes a new efficient method for discovering share-frequent patterns. The new method reduces a number of candidates by generating candidates from only high transaction-measure-value patterns. The downward closure property of transaction-measure-value patterns assures correctness of the proposed method. Experimental results on dense and sparse datasets show that the proposed method is very efficient in terms of execution time. Also, it decreases the number of generated useless candidates in the mining process by at least 70%. © 2017 Penerbit UTM Press. All rights reserved.","Association rule mining; Data mining; Frequent itemsets mining; Frequent patterns mining; Knowledge discovering; Share-frequent patterns mining",,2-s2.0-85032255745
"Kieu T., Vo B., Le T., Deng Z.-H., Le B.","Mining top-k co-occurrence items with sequential pattern",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019574472&doi=10.1016%2fj.eswa.2017.05.021&partnerID=40&md5=0ba014bb7495d2f7598af38b8ac88114","Frequent sequential pattern mining has become one of the most important tasks in data mining. It has many applications, such as sequential analysis, classification, and prediction. How to generate candidates and how to control the combinatorically explosive number of intermediate subsequences are the most difficult problems. Intelligent systems such as recommender systems, expert systems, and business intelligence systems use only a few patterns, namely those that satisfy a number of defined conditions. Challenges include the mining of top-k patterns, top-rank-k patterns, closed patterns, and maximal patterns. In many cases, end users need to find itemsets that occur with a sequential pattern. Therefore, this paper proposes approaches for mining top-k co-occurrence items usually found with a sequential pattern. The Naive Approach Mining (NAM) algorithm discovers top-k co-occurrence items by directly scanning the sequence database to determine the frequency of items. The Vertical Approach Mining (VAM) algorithm is based on vertical database scanning. The Vertical with Index Approach Mining (VIAM) algorithm is based on a vertical database with index scanning. VAM and VIAM use pruning strategies to reduce the search space, thus improving performance. VAM and VIAM are especially effective in mining the co-occurrence items of a long input pattern. The three algorithms were evaluated using real-world databases. The experimental results show that these algorithms perform well, especially VAM and VIAM. © 2017 Elsevier Ltd","Co-occurrence sequential mining; Sequential pattern mining; Top-k mining","Database systems; Expert systems; Human computer interaction; Intelligent systems; Scanning; Business intelligence systems; Co-occurrence; Frequent sequential patterns; Improving performance; Real-world database; Sequential analysis; Sequential patterns; Sequential-pattern mining; Data mining",2-s2.0-85019574472
"A.G. R., Abdulla M.S., Asharaf A.S.","Lightly trained support vector data description for novelty detection",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019880831&doi=10.1016%2fj.eswa.2017.05.007&partnerID=40&md5=401bafc0bf8dc36cb7ae381730ee89a2","Anomaly (or outlier) detection is well researched objective in data mining due to its importance and inherent challenges. An outlier could be the key discovery to be made from large datasets and the insights gathered from them could be of significance in a wide variety of domains like information security, business intelligence, clinical decision support, financial monitoring etc. Recently, Support Vector Data Description (SVDD) driven approaches are shown as having good predictive accuracy. This paper proposes a novel low-complexity anomaly detection algorithm based on Support Vector Data Description (SVDD). The proposed algorithm reduces the complexity by avoiding the calculation of Lagrange multipliers of an objective function, instead locates an approximate pre-image of the SVDD sphere's center, within the input space itself. The crux of the training algorithm is a gradient descent of the primal objective function using Simultaneous Perturbation Stochastic Approximation (SPSA). Experiments using datasets obtained from UCI machine learning repository have demonstrated that the accuracies of the proposed approach are comparable while the training time is much lesser than Classical SVDD. © 2017 Elsevier Ltd","One-class classification; Outlier detection; Scaling; SVDD","Approximation algorithms; Computational complexity; Data mining; Decision support systems; Lagrange multipliers; Learning algorithms; Learning systems; Optimization; Security of data; Statistics; Stochastic systems; Anomaly-detection algorithms; One-class Classification; Outlier Detection; Scaling; Simultaneous perturbation stochastic approximation; Support vector data description; SVDD; UCI machine learning repository; Data description",2-s2.0-85019880831
"Fuller D., Buote R., Stanley K.","A glossary for big data in population and public health: Discussion and commentary on terminology and research methods",2017,"Journal of Epidemiology and Community Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031786632&doi=10.1136%2fjech-2017-209608&partnerID=40&md5=da036bc9e142236745e04d435f32b806","The volume and velocity of data are growing rapidly and big data analytics are being applied to these data in many fields. Population and public health researchers may be unfamiliar with the terminology and statistical methods used in big data. This creates a barrier to the application of big data analytics. The purpose of this glossary is to define terms used in big data and big data analytics and to contextualise these terms. We define the five Vs of big data and provide definitions and distinctions for data mining, machine learning and deep learning, among other terms. We provide key distinctions between big data and statistical analysis methods applied to big data. We contextualise the glossary by providing examples where big data analysis methods have been applied to population and public health research problems and provide brief guidance on how to learn big data analysis methods. © Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017.",,"data interpretation; data mining; machine learning; public health; research method; statistical analysis; terminology; data analysis; data mining; human; machine learning; nomenclature; public health; statistical analysis",2-s2.0-85031786632
"Liu Y., Liu L., Liu H., Wang X., Yang H.","Mining domain knowledge from app descriptions",2017,"Journal of Systems and Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027980934&doi=10.1016%2fj.jss.2017.08.024&partnerID=40&md5=cb27f59a3ef6165d6151f2e30a74c244","Domain analysis aims at gaining knowledge to a particular domain in the early stage of software development. A key challenge in domain analysis is to extract features automatically from related product artifacts. Compared with other kinds of artifacts, high volume of descriptions can be collected from App marketplaces (such as Google Play and Apple Store) easily when developing a new mobile application (App), so it is essential for the success of domain analysis to gain features and relationships from them using data analysis techniques. In this paper, we propose an approach to mine domain knowledge from App descriptions automatically, where the information of features in a single App description is firstly extracted and formally described by a Concern-based Description Model (CDM), which is based on predefined rules of feature extraction and a modified topic modeling method; then the overall knowledge in the domain is identified by classifying, clustering and merging the knowledge in the set of CDMs and topics, and the results are formalized by a Data-based Raw Domain Model (DRDM). Furthermore, we propose a quantified evaluation method for prioritizing the knowledge in DRDM. The proposed approach is validated by a series of experiments. © 2017","App descriptions; Data analysis; Domain analysis; Feature extraction","Classification (of information); Data handling; Data reduction; Extraction; Feature extraction; Information analysis; Software design; Data analysis techniques; Description model; Domain analysis; Domain knowledge; Mobile applications; Quantified evaluations; Related products; Topic Modeling; Data mining",2-s2.0-85027980934
"Mobasheri A.","A rule-based spatial reasoning approach for openstreetmap data quality enrichment; case study of routing and navigation",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032723752&doi=10.3390%2fs17112498&partnerID=40&md5=472b7a9e7df1a10a8f58183000f4a71d","Finding relevant geospatial information is increasingly critical because of the growing volume of geospatial data available within the emerging “Big Data” era. Users are expecting that the availability of massive datasets will create more opportunities to uncover hidden information and answer more complex queries. This is especially the case with routing and navigation services where the ability to retrieve points of interest and landmarks make the routing service personalized, precise, and relevant. In this paper, we propose a new geospatial information approach that enables the retrieval of implicit information, i.e., geospatial entities that do not exist explicitly in the available source. We present an information broker that uses a rule-based spatial reasoning algorithm to detect topological relations. The information broker is embedded into a framework where annotations and mappings between OpenStreetMap data attributes and external resources, such as taxonomies, support the enrichment of queries to improve the ability of the system to retrieve information. Our method is tested with two case studies that leads to enriching the completeness of OpenStreetMap data with footway crossing points-of-interests as well as building entrances for routing and navigation purposes. It is concluded that the proposed approach can uncover implicit entities and contribute to extract required information from the existing datasets. © 2017 by the author. Licensee MDPI, Basel, Switzerland.","Crowdsourced geographic information; Data mining; Data quality enrichment; OpenStreetMap; Routing; VGI","Air navigation; Big data; Case based reasoning; Navigation; Query processing; Data quality; Geo-spatial informations; Geographic information; Implicit informations; Information broker; OpenStreetMap; Routing; Topological relations; Data mining",2-s2.0-85032723752
"Szalkai B., Grolmusz V.K., Grolmusz V.I.","Identifying combinatorial biomarkers by association rule mining in the CAMD Alzheimer's database",2017,"Archives of Gerontology and Geriatrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029393074&doi=10.1016%2fj.archger.2017.08.006&partnerID=40&md5=53ab40f4f0c3017af6b6bfa8241823d9","The concept of combinatorial biomarkers was conceived when it was noticed that simple biomarkers are often inadequate for recognizing and characterizing complex diseases. Here we present an algorithmic search method for complex biomarkers which may predict or indicate Alzheimer's disease (AD) and other kinds of dementia. We show that our method is universal since it can describe any Boolean function for biomarker discovery. We applied data mining techniques that are capable to uncover implication-like logical schemes with detailed quality scoring. The new SCARF program was applied for the Tucson, Arizona based Critical Path Institute's CAMD database, containing laboratory and cognitive test data for 5821 patients from the placebo arm of clinical trials of large pharmaceutical companies, and consequently, the data is much more reliable than numerous other databases for dementia. The results of our study on this larger than 5800-patient cohort suggest beneficial effects of high B12 vitamin level, negative effects of high sodium levels or high AST (aspartate aminotransferase) liver enzyme levels to cognition. As an example for a more complex and quite surprising rule: Low or normal blood glucose level with either low cholesterol or high serum sodium would also increase the probability of bad cognition with a 3.7 multiplier. The source code of the new SCARF program is publicly available at http://pitgroup.org/static/scarf.zip. © 2017 Elsevier B.V.","Alzheimer's disease; Association rule mining; Combinatorial biomarkers; SCARF","aspartate aminotransferase; biological marker; cholesterol; cyanocobalamin; donepezil; glucose; sodium; aged; algorithm; Alzheimer disease; Article; Coalition Against Major Diseases database; cognition; combinatorial library; controlled study; data base; data mining; dementia; drug industry; female; glucose blood level; human; major clinical study; male; priority journal; sodium blood level; software",2-s2.0-85029393074
"Chen C.-C., Shuai H.-H., Chen M.-S.","Distributed and scalable sequential pattern mining through stream processing",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015613838&doi=10.1007%2fs10115-017-1037-1&partnerID=40&md5=a50764e38dcd8232d2cfa9cdf3c3c260","Scalability is a primary issue in existing sequential pattern mining algorithms for dealing with a large amount of data. Previous work, namely sequential pattern mining on the cloud (SPAMC), has already addressed the scalability problem. It supports the MapReduce cloud computing architecture for mining frequent sequential patterns on large datasets. However, this existing algorithm does not address the iterative mining problem, which is the problem that reloading data incur additional costs. Furthermore, it did not study the load balancing problem. To remedy these problems, we devised a powerful sequential pattern mining algorithm, the sequential pattern mining in the cloud-uniform distributed lexical sequence tree algorithm (SPAMC-UDLT), exploiting MapReduce and streaming processes. SPAMC-UDLT dramatically improves overall performance without launching multiple MapReduce rounds and provides perfect load balancing across machines in the cloud. The results show that SPAMC-UDLT can significantly reduce execution time, achieves extremely high scalability, and provides much better load balancing than existing algorithms in the cloud. © 2017, Springer-Verlag London.","Big data; Cloud computing; Data mining; MapReduce; Sequential pattern mining; Streaming MapReduce",,2-s2.0-85015613838
"Capozzoli A., Piscitelli M.S., Gorrino A., Ballarini I., Corrado V.","Data analytics for occupancy pattern learning to reduce the energy consumption of HVAC systems in office buildings",2017,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027495049&doi=10.1016%2fj.scs.2017.07.016&partnerID=40&md5=e6504019f7a31468c7e34e0b817d96c4","In the last few years, the collecting and processing of occupancy data have become emerging issues since they can affect, either directly or indirectly, several energy operations in buildings. The application of data analytics-based methods makes it possible to exploit the potentialities of occupancy related knowledge to enhance the energy management in buildings. A methodology, aimed at implementing an occupancy-based HVAC system operation schedule, is presented in this article. The process is based on the convenience of displacing groups of occupants with similar occupancy patterns to the same thermal zone. An optimisation of the stop schedule of an HVAC system has been investigated, considering a typical week's occupancy patterns. The methodology was used to analyse the Zaanstad Town Hall (The Netherlands), considering anonymous occupancy data for a monitoring period of four months. The resulting optimised schedule was tested, through an energy simulation approach, considering a model calibrated with real energy consumption data. The savings related to the energy consumption of the HVAC system, as a result of the implementation of the strategy, in comparison to an occupancy-independent operation schedule amounted to 14%. The proposed process can be generalized and drive energy managers in evaluating optimised occupancy-based HVAC system operation schedules. © 2017 Elsevier Ltd","Building simulation; Data mining; Energy management; Occupancy profiles; Office building; Pattern recognition","Buildings; Climate control; Data handling; Data mining; Digital storage; Energy management; Office buildings; Pattern recognition; Building simulation; Data analytics; Energy consumption datum; Energy managers; Energy simulation; Monitoring periods; Occupancy profiles; Pattern Learning; Energy utilization",2-s2.0-85027495049
"Damiani M.L., Hachem F.","Segmentation techniques for the summarization of individual mobility data",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022224433&doi=10.1002%2fwidm.1214&partnerID=40&md5=72613e7525d048846da25b829a615e95","Segmentation techniques partition a sequence of data points into a series of disjoint subsequences—segments—based on some criteria. Depending on the context and the nature of data themselves, segments return an approximate representation. The final result is a summarized representation of the sequence. This intuitive mechanism has been extensively studied, for example, for the summarization of time series in order to preserve the ‘shape’ of the sequence while omitting irrelevant details. This survey focuses on the use of segmentation methods for extracting behavioral information from individual mobility data, in particular from spatial trajectories. Such information can then be given a compact representation in the form of summarized trajectories, e.g., semantic trajectories and symbolic trajectories. Two major streams of research are discussed, spanning computational geometry and data mining respectively, that are emblematic of the multiplicity of views. WIREs Data Mining Knowl Discov 2017, 7:e1214. doi: 10.1002/widm.1214. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.",,"Computational geometry; Semantics; Trajectories; Compact representation; Data points; Individual mobility; Segmentation methods; Segmentation techniques; Semantic trajectories; Spatial trajectory; Data mining",2-s2.0-85022224433
"Suárez-Cetrulo A.L., Cervantes A.","An online classification algorithm for large scale data streams: iGNGSVM",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020486793&doi=10.1016%2fj.neucom.2016.12.093&partnerID=40&md5=464f2be0f9dbe24d178e6743b455398c","Stream Processing has recently become one of the current commercial trends to face huge amounts of data. However, normally these techniques need specific infrastructures and high resources in terms of memory and computing nodes. This paper shows how mini-batch techniques and topology extraction methods can help making gigabytes of data to be manageable for just one server using computationally costly Machine Learning techniques as Support Vector Machines. The algorithm iGNGSVM is proposed to improve the performance of Support Vector Machines in datasets where the data is continuously arriving. It is benchmarked against a mini-batch version of LibSVM, achieving good accuracy rates and performing faster than this. © 2017 Elsevier B.V.","Data classification; Growing Neural Gas; Large datasets; Online learning; Support Vector Machines; Topology extraction","Classification (of information); Extraction; Learning systems; Support vector machines; Topology; Data classification; Extraction method; Growing neural gas; Large datasets; Machine learning techniques; On-line classification; Online learning; Stream processing; Data mining; accuracy; algorithm; Article; calculation; classification algorithm; cost benefit analysis; data analysis; extraction; linear system; mathematical computing; nonlinear system; online system; priority journal; support vector machine",2-s2.0-85020486793
"Tang L., Zhang Y., Dai F., Yoon Y., Song Y., Sharma R.S.","Social Media Data Analytics for the U.S. Construction Industry: Preliminary Study on Twitter",2017,"Journal of Management in Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029504535&doi=10.1061%2f%28ASCE%29ME.1943-5479.0000554&partnerID=40&md5=1a08573739b2cd645f27d33378460931","The increasing use of the Internet for many purposes is creating big data, many of which are generated from social media. These big data potentially could assist in obtaining valuable administrative information and even explore new social phenomena. Traditional ways of collecting data, such as questionnaire surveys, are time-consuming and costly. Therefore, the use of social media affords the opportunity to extract information that might be of benefit to the construction industry in a responsive and inexpensive manner. To this end, this paper explores whether information and knowledge that would be valuable in the construction domain can be generated by analyzing social media data. Twitter was selected for an initial trial analysis because of its wide usage in the United States. Because they represent a majority of the construction users in Twitter, the following four user clusters were selected and analyzed: construction workers, construction companies, construction unions, and construction media. For each user identified in the four clusters, the 3,200 most recent Twitter messages were collected, which were analyzed from the following aspects: sentiment analysis, topic modeling, link analysis, geolocation analysis, and timeline analysis. Different data-analysis methods were used for the specific themes, such as Stanford Natural Language Processing (StanfordNLP) for sentiment analysis. The detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, as well as future research directions, are discussed in this paper. For example, the sentiment analysis results indicated that construction workers tend to have a higher proportion of negative messages compared to the other clusters, which may prompt more attention to emotional guidance and understanding by construction companies and the public. This paper benefits academia by testing an alternative way of studying the construction population, which could help decision makers gain a better understanding of real-world situations in the construction industry. © 2017 American Society of Civil Engineers.","Big data; Construction industry; Social media; Twitter; United States","Construction industry; Data mining; Decision making; Natural language processing systems; Social networking (online); Surveys; Construction companies; Data analysis methods; Future research directions; Questionnaire surveys; Real world situations; Social media; Twitter; United States; Big data",2-s2.0-85029504535
"Raphaeli O., Goldstein A., Fink L.","Analyzing online consumer behavior in mobile and PC devices: A novel web usage mining approach",2017,"Electronic Commerce Research and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029576387&doi=10.1016%2fj.elerap.2017.09.003&partnerID=40&md5=2aac73c4fef09389e2b98ae9e4d113d4","We investigate and compare online consumer behavior on an e-retailer website in mobile versus PC devices, through the application of a web usage mining approach on clickstream data recorded in server-side log files. Online consumer behavior is characterized through both engagement measures and the discovery of common sequences of navigation patterns, using an innovative approach that combines footstep graph visualization with sequential association rule mining. We find that sessions conducted through mobile devices are more likely to consist of task-oriented behavior whereas sessions conducted through PC devices are characterized by a more exploration-oriented browsing behavior. Moreover, we find that certain sequence rules are associated with an increased likelihood of purchase in both mobile and PC sessions. The results demonstrate the value of our approach in analyzing online browsing behavior, across platforms, in the context of electronic retailing. © 2017 Elsevier B.V.","E-commerce; Footstep graph; M-commerce; Navigation patterns; Online browsing behavior; Sequential association rule mining; Web usage mining","Air navigation; Association rules; Commerce; Data mining; Electronic commerce; Browsing behavior; Footstep graph; M-commerce; Navigation patterns; Sequential association rules; Web usage mining; Consumer behavior",2-s2.0-85029576387
"Li H., Wang Y., Wang H., Zhou B.","Multi-window based ensemble learning for classification of imbalanced streaming data",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014518180&doi=10.1007%2fs11280-017-0449-x&partnerID=40&md5=5a8514ca7a43f966454dea3f26fcf898","Imbalanced streaming data is commonly encountered in real-world data mining and machine learning applications, and has attracted much attention in recent years. Both imbalanced data and streaming data in practice are normally encountered together; however, little research work has been studied on the two types of data together. In this paper, we propose a multi-window based ensemble learning method for the classification of imbalanced streaming data. Three types of windows are defined to store the current batch of instances, the latest minority instances, and the ensemble classifier. The ensemble classifier consists of a set of latest sub-classifiers, and the instances employed to train each sub-classifier. All sub-classifiers are weighted prior to predicting the class labels of newly arriving instances, and new sub-classifiers are trained only when the precision is below a predefined threshold. Extensive experiments on synthetic datasets and real-world datasets demonstrate that the new approach can efficiently and effectively classify imbalanced streaming data, and generally outperforms existing approaches. © 2017, Springer Science+Business Media New York.","Class imbalance; Ensemble learning; Multi-window; Streaming data","Data mining; Learning systems; Class imbalance; Ensemble classifiers; Ensemble learning; Machine learning applications; Multi-Windows; Real-world datasets; Streaming data; Synthetic datasets; Classification (of information)",2-s2.0-85014518180
"Hayes T., McArdle J.J.","Should we impute or should we weight? Examining the performance of two CART-based techniques for addressing missing data in small sample research with nonnormal variables",2017,"Computational Statistics and Data Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020795314&doi=10.1016%2fj.csda.2017.05.006&partnerID=40&md5=447a2fb92660ac63b6ed2aa61b0baf53","Recently, researchers have proposed a variety of new methods for employing exploratory data mining algorithms to address missing data. Two promising classes of missing data methods take advantage of classification and regression trees and random forests. A first method uses the predicted probabilities of response (vs. non-response) generated by a CART analysis to create inverse probability weights. This method has been shown to perform well in prior simulations when nonresponse was generated by tree-based structures, even under low sample sizes. A second method uses the values falling in terminal nodes of CART trees to generate multiple imputations. In prior studies, these methods performed well at estimating main effects and interactions in regression models when sample sizes were large (N=1000), but their performance was not evaluated under small sample conditions. In the present research, we assess the performance of CART-based weights and CART-based imputations under low sample sizes (N=125 or 250) and nonnormality when missing data are generated by smooth functions (linear, quadratic, cubic, interactive). Results suggest that random forest weights excel under low sample sizes, regardless of nonnormality, whereas CART multiple imputation is more efficient with larger samples (N=500 or 1000). © 2017 Elsevier B.V.","CART; Classification and regression trees; Missing data; Nonnormality; Random forests; Small samples","Classification (of information); Decision trees; Fisher information matrix; Inverse problems; Regression analysis; CART; Classification and regression tree; Missing data; Non-normality; Random forests; Small samples; Data mining",2-s2.0-85020795314
"Sciascia S., Radin M.","What can Google and Wikipedia can tell us about a disease? Big Data trends analysis in Systemic Lupus Erythematosus",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029609953&doi=10.1016%2fj.ijmedinf.2017.09.002&partnerID=40&md5=4ca30d135f07ec306706c41f72a3a104","Objective To investigate trends of Internet search volumes linked to Systemic Lupus Erythematosus (SLE), on-going clinical trials and research developments associated to the disease, using Big Data monitoring and data mining. Methods We performed a longitudinal analysis based on the large amount of data generated by Google Trends, scientific search tools (SCOPUS, Medline/Pubmed/ClinicalTrails.gov) considering ‘SLE’, and ‘lupus’ in a 5-year web-based research. Wikipedia page views were also analysed using WikiTrends and the results were compared with the search volumes generated by Google Trends. Results We observed an overall higher distribution of search volumes from Google Trends in United States, South America, Canada, South Africa, Australia and Europe (mainly Italy, United Kingdom, Spain, France, Germany), showing a geographically heterogeneity in insight into health-related behaviour of the different populations towards SLE. By comparing the search volumes analysing the Wikipedia page views of both SLE and belimumab, we found a close peak trend, reflecting the knowledge translation after the approval of belimumab for the treatment of SLE. When focusing on search volumes of Google Trends, we noticed that the highest peaks were related to news headlines that involved celebrities affected by SLE, also when comparing to the peak generated by the approval of belimumab. Conclusion This new approach, able to investigate health information seeking, might give an estimate of the health-related demand and even of the health-related behaviour of SLE, bringing new light to unanswered questions. © 2017 Elsevier B.V.","Big Data; Clinical trials; Geoepidemiology; Infodemiology; Infoveillance; Systemic Lupus Erythematosus","Data mining; Health; Monitoring; Search engines; Websites; Clinical trial; Geoepidemiology; Infodemiology; Infoveillance; Systemic lupus erythematosus; Big data; belimumab; Article; Australia; Canada; Europe; health behavior; human; Medline; patient attitude; priority journal; Scopus; search engine; South Africa; South America; systematic review; systemic lupus erythematosus; United States",2-s2.0-85029609953
"Ding I.-J., Chang Y.-J.","HMM with improved feature extraction-based feature parameters for identity recognition of gesture command operators by using a sensed Kinect-data stream",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021085814&doi=10.1016%2fj.neucom.2016.11.089&partnerID=40&md5=1e20602490b021989383ecf5382ef552","Artificial internet of things technology encourages the development of robots and sensors. Kinect sensors with excellent human gesture recognition and robots with smart interactions with people are expected to have numerous innovative applications. In robotic sport instructor systems for rehabilitation and exercise training, identity recognition of the gesture operator is a crucial problem. With operator identity recognition, the gesture classification model owned by the operator can be further adjusted using the operator's active gestures, and a user-adaptive sport instructor robot system can then be achieved. This study developed an identity recognition method to classify gesture operators in which an improved feature extraction scheme that considered the practical height of a person was introduced for effective identification. According to the improved feature extraction design, a 40-dimension feature vector with the physical characteristics of the human skeleton was further developed as the feature parameter for enhancing identity recognition. A hidden Markov model (HMM) with fine considerations of continuous-time gesture variations was adopted as the recognition model for identifying ten gesture operators in the sport instructor robot system. Experimental results demonstrated the superiority of the presented approach because the constructed corresponding ten HMM active user models with the improved feature extraction-based feature parameter exhibited an excellent average identity recognition accuracy of 87.67% among all ten players, with each of them making six specified sport rehabilitation actions. © 2017 Elsevier B.V.","Feature parameter; Gesture command operator; HMM; Identity recognition; Improved feature extraction; Kinect camera","Data mining; Extraction; Feature extraction; Hidden Markov models; Human robot interaction; Markov processes; Personnel training; Robots; Sports; Feature parameters; Gesture classifications; Gesture commands; Human gesture recognition; Identity recognition; Internet of things technologies; Kinect cameras; Physical characteristics; Gesture recognition; Article; athletic rehabilitation; biometry; body height; body regions; data analysis software; data extraction; gesture; gesture command operator; hidden Markov model; human; identity recognition; mathematical model; measurement accuracy; motion analysis system; operator; priority journal; robotics; skeleton",2-s2.0-85021085814
"Liu H., He G., Jiao W., Wang G., Peng Y., Cheng B.","Sequential pattern mining of land cover dynamics based on time-series remote sensing images",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978911330&doi=10.1007%2fs11042-016-3730-6&partnerID=40&md5=6fa0a9eb812fb7aa055ae0dbfe232f65","Remote sensing images constitute a new type of multimedia data well suited to land cover change detection tasks, as they can repetitively provide information about the land surface and its changes over large and inaccessible areas. With plans for more missions and higher resolution earth observation systems, the challenge is increasingly going to be the efficient usability of the millions of collected images, especially the decades of remote sensing image time series, to describe land cover and/or scene evolution and dynamics. In contrast to traditional land cover change measures using pair-wise comparisons that emphasize the compositional or configurational changes between dates, this research focuses on the analysis of the temporal sequence of land cover dynamics, which refers to the succession of land cover types for a given area over more than two observational periods. The expected novel significance of this study is the generalization of the application of the sequential pattern mining method for capturing the spatial variability of landscape patterns and their trajectories of change to reveal information regarding process regularities with satellite imagery. Experimental results showed that this approach not only quantifies land cover changes in terms of the percentage area affected and maps the spatial distribution of these land cover changes but also reveals possibly interesting or useful information regarding the trajectories of change. This method is a valuable complement to existing bi-temporal change detection methods. © 2016, Springer Science+Business Media New York.","Data mining; Image time series; Land cover change; Remote sensing satellite; Sequential pattern mining","Dynamics; Image reconstruction; Remote sensing; Satellite imagery; Signal detection; Time series; Earth observation systems; Image time-series; Land-cover change; Pair-wise comparison; Process regularities; Remote sensing images; Remote sensing satellites; Sequential-pattern mining; Data mining",2-s2.0-84978911330
"Shen L., Yan H., Fan H., Wu Y., Zhang Y.","An integrated system of text mining technique and case-based reasoning (TM-CBR) for supporting green building design",2017,"Building and Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027838671&doi=10.1016%2fj.buildenv.2017.08.026&partnerID=40&md5=f631110ff79c99364b27be0f99409c36","Green building has been commonly accepted as an important strategy adopted by governments around the world for mitigating climate change and energy shortage problem. However, the selection and application of green building technologies under different situations usually puzzles designers, although various advanced technologies for green building are available. This study therefore introduces an integrated system of text mining and case-based reasoning (TM-CBR) to help designers retrieve the most similar green building cases for references when producing design for new green buildings. It is the first attempt in this study to integrate text mining technique into a CBR system to improve the efficiency of decision making in green building design. There are two major components of TM-CBR, case representation and case retrieval. Two kinds of case features, namely, identified features and textual features are used collectively to represent a green building case. Four value formats are considered to measure local similarity in the process of case retrieval. Seven cases are chosen randomly from 71 LEED collected cases as the target cases to test the effectiveness of the TM-CBR system. This study provides a new approach to retrieve the successful experience from similar previous cases to improve the effectiveness and adequacy of green building design. © 2017 Elsevier Ltd","Case representation; Case retrieval; Case-based reasoning; Green building design; Text mining","Architectural design; Buildings; Climate change; Data mining; Decision making; Integrated control; Advanced technology; Case representation; Case retrieval; Green building design; Integrated systems; Text mining; Text mining techniques; Textual features; Case based reasoning",2-s2.0-85027838671
"Goh Y.M., Ubeynarayana C.U.","Construction accident narrative classification: An evaluation of text mining techniques",2017,"Accident Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028564740&doi=10.1016%2fj.aap.2017.08.026&partnerID=40&md5=b29a7695083edc5aca0e7bc5e4c9d753","Learning from past accidents is fundamental to accident prevention. Thus, accident and near miss reporting are encouraged by organizations and regulators. However, for organizations managing large safety databases, the time taken to accurately classify accident and near miss narratives will be very significant. This study aims to evaluate the utility of various text mining classification techniques in classifying 1000 publicly available construction accident narratives obtained from the US OSHA website. The study evaluated six machine learning algorithms, including support vector machine (SVM), linear regression (LR), random forest (RF), k-nearest neighbor (KNN), decision tree (DT) and Naive Bayes (NB), and found that SVM produced the best performance in classifying the test set of 251 cases. Further experimentation with tokenization of the processed text and non-linear SVM were also conducted. In addition, a grid search was conducted on the hyperparameters of the SVM models. It was found that the best performing classifiers were linear SVM with unigram tokenization and radial basis function (RBF) SVM with uni-gram tokenization. In view of its relative simplicity, the linear SVM is recommended. Across the 11 labels of accident causes or types, the precision of the linear SVM ranged from 0.5 to 1, recall ranged from 0.36 to 0.9 and F1 score was between 0.45 and 0.92. The reasons for misclassification were discussed and suggestions on ways to improve the performance were provided. © 2017 Elsevier Ltd","Accident classification; Construction safety; Data mining; Support vector machine; Text mining","Accidents; Classification (of information); Decision trees; Learning algorithms; Learning systems; Nearest neighbor search; Radial basis function networks; Support vector machines; Text processing; Construction accidents; Construction safety; Hyperparameters; K nearest neighbor (KNN); Misclassifications; Radial Basis Function(RBF); Text mining; Text mining techniques; Data mining",2-s2.0-85028564740
"Vinué G., Epifanio I.","Archetypoid analysis for sports analytics",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020110235&doi=10.1007%2fs10618-017-0514-1&partnerID=40&md5=1f557373523827cb4907e63d30d5850f","We intend to understand the growing amount of sports performance data by finding extreme data points, which makes human interpretation easier. In archetypoid analysis each datum is expressed as a mixture of actual observations (archetypoids). Therefore, it allows us to identify not only extreme athletes and teams, but also the composition of other athletes (or teams) according to the archetypoid athletes, and to establish a ranking. The utility of archetypoids in sports is illustrated with basketball and soccer data in three scenarios. Firstly, with multivariate data, where they are compared with other alternatives, showing their best results. Secondly, despite the fact that functional data are common in sports (time series or trajectories), functional data analysis has not been exploited until now, due to the sparseness of functions. In the second scenario, we extend archetypoid analysis for sparse functional data, furthermore showing the potential of functional data analysis in sports analytics. Finally, in the third scenario, features are not available, so we use proximities. We extend archetypoid analysis when asymmetric relations are present in data. This study provides information that will provide valuable knowledge about player/team/league performance so that we can analyze athlete’s careers. © 2017, The Author(s).","Archetype analysis; Extreme point; Functional data analysis; Multidimensional scaling; Performance analysis; Sports data mining","Data handling; Data mining; Information analysis; Sports; Archetype analysis; Extreme points; Functional data analysis; Multi-dimensional scaling; Performance analysis; Sports data; Time series analysis",2-s2.0-85020110235
"Dong C., Loukides G.","Approximating Private Set Union/Intersection Cardinality with Logarithmic Complexity",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023767094&doi=10.1109%2fTIFS.2017.2721360&partnerID=40&md5=e24011dea2fb90e6240df5dfbba1649e","The computation of private set union/intersection cardinality (PSU-CA/PSI-CA) is one of the most intensively studied problems in privacy preserving data mining (PPDM). However, the existing protocols are computationally too expensive to be employed in real-world PPDM applications. In response, we propose efficient approximate protocols, whose accuracy can be tuned according to application requirements. We first propose a two-party PSU-CA protocol based on Flajolet-Martin sketches. The protocol has logarithmic computational/communication complexity and relies mostly on symmetric key operations. Thus, it is much more efficient and scalable than existing protocols. In addition, our protocol can hide its output. This feature is necessary in PPDM applications, since the union cardinality is often an intermediate result that must not be disclosed. We then propose a two-party PSI-CA protocol, which is derived from the PSU-CA protocol with virtually no cost. Both our two-party protocols can be easily extended to the multiparty setting. We also design an efficient masking scheme for (1n)-OT. The scheme is used in optimizing the two-party protocols and is of independent interest, since it can speed up (1n)-OT significantly when n is large. Finally, we show through experiments the effectiveness and efficiency of our protocols. © 2005-2012 IEEE.","cryptographic protocols; data mining; Data privacy; data security","Computer networks; Safety engineering; Application requirements; Effectiveness and efficiencies; Intermediate results; Logarithmic complexity; Masking schemes; Privacy-preserving data mining; Symmetric keys; Two-party protocols; Data privacy",2-s2.0-85023767094
"Ceron J.D., Lopez D.M., Ramirez G.A.","A mobile system for sedentary behaviors classification based on accelerometer and location data",2017,"Computers in Industry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021065666&doi=10.1016%2fj.compind.2017.06.005&partnerID=40&md5=4610a8c0c0ac57a46e47d24b419483e3","Background Sedentary behaviors are associated to the development of noncommunicable diseases (NCD) such as cardiovascular diseases (CVD), type 2 diabetes, and cancer. Accelerometers and inclinometers have been used to estimate sedentary behaviors, however a major limitation is that these devices do not provide enough contextual information in order to recognize the specific sedentary behavior performed, e.g., sitting or lying watching TV, using the PC, sitting at work, driving, etc. Objective Propose and evaluate the precision of a mobile system for objectively measuring six sedentary behaviors using accelerometer and location data. Results The system is implemented as an Android Mobile App, which identifies individual's sedentary behaviors based on accelerometer data taken from the smartphone or a smartwatch, and symbolic location data obtained from Bluetooth Low Energy (BLE) beacons. The system infers sedentary behaviors by means of a supervised Machine Learning Classifier. The precision of the classification of five of the six studied sedentary behaviors exceeded 95% using accelerometer data from a smartwatch attached to the wrist and 98% using accelerometer data from a smartphone put into the pocket. Statistically significant improvement in the average precision of the classification due to the use of BLE beacons was found by comparing the precision of the classification using accelerometer data only, and BLE beacons localization technology. Conclusions The proposed system provides contextual information of specific sedentary behaviors by inferring with very high precision the physical location where the sedentary event occurs. Moreover, it was found that, when accelerometers are put in the user's pocket, instead of the wrist and, when symbolic location is inferred using BLE beacons; the precision in the classification is improved. In practice, the proposed system has the potential to contribute to the understanding of the context and determinants of sedentary behaviors, necessary for the implementation and monitoring of personalized noncommunicable diseases prevention programs, for instance, sending sedentary behavior alerts, or providing personalized recommendations on physical activity. The system could be used at work to promote active breaks and healthy habits. © 2017","Accelerometer; BLE beacons; Data mining; Sedentary behaviors classification; Sensor mining","Accelerometers; Data mining; Diseases; Learning systems; Location; Signal encoding; Smartphones; Wearable computers; BLE beacons; Bluetooth low energies (BLE); Cardiovascular disease; Contextual information; Localization technologies; Non-communicable disease; Personalized recommendation; Supervised machine learning; Classification (of information)",2-s2.0-85021065666
"Alonso S.G., de la Torre Díez I., Rodrigues J.J.P.C., Hamrioui S., López-Coronado M.","A Systematic Review of Techniques and Sources of Big Data in the Healthcare Sector",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031914025&doi=10.1007%2fs10916-017-0832-2&partnerID=40&md5=e2188602a17b63373574d6ea232bc596","The main objective of this paper is to present a review of existing researches in the literature, referring to Big Data sources and techniques in health sector and to identify which of these techniques are the most used in the prediction of chronic diseases. Academic databases and systems such as IEEE Xplore, Scopus, PubMed and Science Direct were searched, considering the date of publication from 2006 until the present time. Several search criteria were established as ‘techniques’ OR ‘sources’ AND ‘Big Data’ AND ‘medicine’ OR ‘health’, ‘techniques’ AND ‘Big Data’ AND ‘chronic diseases’, etc. Selecting the paper considered of interest regarding the description of the techniques and sources of Big Data in healthcare. It found a total of 110 articles on techniques and sources of Big Data on health from which only 32 have been identified as relevant work. Many of the articles show the platforms of Big Data, sources, databases used and identify the techniques most used in the prediction of chronic diseases. From the review of the analyzed research articles, it can be noticed that the sources and techniques of Big Data used in the health sector represent a relevant factor in terms of effectiveness, since it allows the application of predictive analysis techniques in tasks such as: identification of patients at risk of reentry or prevention of hospital or chronic diseases infections, obtaining predictive models of quality. © 2017, Springer Science+Business Media, LLC.","Big data; Chronic diseases; Data mining; Health sector; Sources; Techniques","Article; bibliographic database; chronic disease; data mining; drug industry; electronic health record; electronic medical record; health care cost; hospital infection; human; intensive care unit; Internet; medical history; meta analysis; national health organization; personalized medicine; prediction; remote sensing; social network; systematic review",2-s2.0-85031914025
"Chaklader R., Parkinson M.B.","Data-Driven Sizing Specification Utilizing Consumer Text Reviews",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030475095&doi=10.1115%2f1.4037476&partnerID=40&md5=b73fc5d8e69d00a131e28f59ba710fe0","The objective of this work is to introduce a new method for determining preliminary design specifications related to human-artifact interaction. This new method uses data mining of large numbers of consumer reviews. User opinion on specific product features can be time-consuming or expensive to obtain through traditional methods including surveys, experiments, and observational studies. Data mining review text of already released products may be a potentially less time consuming and costly method. Previously established methods of determining design for human variability information from consumer reviews, such as the frequency and accuracy summation (FAS) number and subsequent manual analysis, are explored. The weighted phrase rating (WPR), a new metric which can be an automated tool to quickly analyze consumer reviews, is also introduced. It does not require manual parsing of the reviews, which extends its applicability to larger review pools. This new method is shown to quickly and economically provide information useful to the establishment of design specifications. © 2017 by ASME.",,"Specifications; Automated tools; Consumer reviews; Design specification; Human variability; Manual analysis; Observational study; Preliminary design; Product feature; Data mining",2-s2.0-85030475095
"Lee P.Y., Loh W.P., Chin J.F.","Feature selection in multimedia: The state-of-the-art review",2017,"Image and Vision Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030164271&doi=10.1016%2fj.imavis.2017.09.004&partnerID=40&md5=cacf01f4bcd4273137073ba1b29a6026","Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining. © 2017","Data mining; Feature selection; Multimedia; Search strategies","Data handling; Data mining; Heuristic algorithms; Classification and clustering; Classification and recognition; Heuristic search strategy; Multimedia; Multimedia data mining; Performance measure; Search strategies; State-of-the art reviews; Feature extraction",2-s2.0-85030164271
"Li T., De la Prieta Pintado F., Corchado J.M., Bajo J.","Multi-source homogeneous data clustering for multi-target detection from cluttered background with misdetection",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025177645&doi=10.1016%2fj.asoc.2017.07.012&partnerID=40&md5=d4d0be491a639d8ec67afdb1913dd5b0","This paper investigates a particular data mining problem which is to ‘identify’ an unknown number of targets based on homogeneous observations that are collected via multiple independent sources. This particular clustering problem corresponds to a significant problem of multi-target detection in the multi-sensor/scan context. No prior information is given about either the level of clutter (namely noisy data) or the number of targets/clusters, both of which have to be learned online from the data. In addition, the data-points from the same source cannot be grouped into the same cluster (namely the cannot link, CL, constraint) and the sizes of the generated clusters need to be bounded by the number of data sources. In the proposed approach, a density-based clustering mechanism is proposed firstly to identify dense regions as clusters and to remove clutter at the coarser level; the CL constraint is then applied for finer data mining and to distinguish overlapping clusters. Illustrative datasets are employed to demonstrate the validity of the present clustering approach for multi-target detection and estimation in cluttered environments which are affected by both misdetection and clutter. © 2017 Elsevier B.V.","Constrained clustering; Multi-target detection; Object identification; Sensor fusion","Clustering algorithms; Clutter (information theory); Data mining; Radar clutter; Cluttered backgrounds; Cluttered environments; Constrained clustering; Density-based Clustering; Multi-target detection; Object identification; Overlapping clusters; Sensor fusion; Cluster analysis",2-s2.0-85025177645
"Moskovitch R., Polubriaginof F., Weiss A., Ryan P., Tatonetti N.","Procedure prediction from symbolic Electronic Health Records via time intervals analytics",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030775906&doi=10.1016%2fj.jbi.2017.07.018&partnerID=40&md5=2de721386aff9ae2b1c4acf24d332850","Prediction of medical events, such as clinical procedures, is essential for preventing disease, understanding disease mechanism, and increasing patient quality of care. Although longitudinal clinical data from Electronic Health Records provides opportunities to develop predictive models, the use of these data faces significant challenges. Primarily, while the data are longitudinal and represent thousands of conceptual events having duration, they are also sparse, complicating the application of traditional analysis approaches. Furthermore, the framework presented here takes advantage of the events duration and gaps. International standards for electronic healthcare data represent data elements, such as procedures, conditions, and drug exposures, using eras, or time intervals. Such eras contain both an event and a duration and enable the application of time intervals mining – a relatively new subfield of data mining. In this study, we present Maitreya, a framework for time intervals analytics in longitudinal clinical data. Maitreya discovers frequent time intervals related patterns (TIRPs), which we use as prognostic markers for modelling clinical events. We introduce three novel TIRP metrics that are normalized versions of the horizontal-support, that represents the number of TIRP instances per patient. We evaluate Maitreya on 28 frequent and clinically important procedures, using the three novel TIRP representation metrics in comparison to no temporal representation and previous TIRPs metrics. We also evaluate the epsilon value that makes Allen's relations more flexible with several settings of 30, 60, 90 and 180 days in comparison to the default zero. For twenty-two of these procedures, the use of temporal patterns as predictors was superior to non-temporal features, and the use of the vertically normalized horizontal support metric to represent TIRPs as features was most effective. The use of the epsilon value with thirty days was slightly better than the zero. © 2017","Electronic Health Records; Prediction; Temporal data mining; Time intervals mining","Data mining; Forecasting; Records management; Clinical procedure; Electronic health record; Electronic healthcare; International standards; Prognostic markers; Temporal data mining; Temporal representations; Time interval; Health; analytic method; Article; conceptual framework; data mining; drug exposure; electronic health record; longitudinal study; mathematical analysis; mathematical computing; mathematical parameters; medical procedures; prediction; priority journal; prognosis; standardization; statistical analysis; statistical parameters; symbolism; time factor; time intervals related pattern",2-s2.0-85030775906
"Wang J., Zhu C., Zhou Y., Zhang W.","Vessel Spatio-temporal Knowledge Discovery with AIS Trajectories Using Co-clustering",2017,"Journal of Navigation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021742165&doi=10.1017%2fS0373463317000406&partnerID=40&md5=b6a3d968e5db87c94fedaeff6258d908","Large volumes of data collected by the Automatic Identification System (AIS) provide opportunities for studying both single vessel motion behaviours and collective mobility patterns on the sea. Understanding these behaviours or patterns is of great importance to maritime situational awareness applications. In this paper, we leveraged AIS trajectories to discover vessel spatio-temporal co-occurrence patterns, which distinguish vessel behaviours simultaneously in terms of space, time and other dimensions (such as ship type, speed, width etc.). To this end, available AIS data were processed to generate spatio-temporal matrices and spatio-temporal tensors (i.e., multidimensional arrays). We then imposed a sparse bilinear decomposition on the matrices and a sparse multi-linear decomposition on the tensors. Experimental results on a real-world dataset demonstrated the effectiveness of this methodology, with which we show the existence of connection among regions, time, and vessel attributes. © Copyright The Royal Institute of Navigation 2017.","AIS Data; Co-clustering; Spatio-temporal Data Mining; Trajectories","data mining; data processing; spatiotemporal analysis; trajectory; vessel",2-s2.0-85021742165
"Ravi K., Ravi V., Prasad P.S.R.K.","Fuzzy formal concept analysis based opinion mining for CRM in financial services",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019743048&doi=10.1016%2fj.asoc.2017.05.028&partnerID=40&md5=e37486b72898926429b96b01f3904976","Owing to the easy access to social media, consumers or customers are increasingly turning to social media to express their grievances and feedback on various products and services offered by the Banking, Financial, Services and Insurance industry. Because non-redressal of complaints eventually leads to customer churn, there is an urgent need to analyze the complaints. In this regard, we propose a novel descriptive analytics model that performs complaints/grievances analytics and summarizes the lengthy and verbose complaints concisely in a form that resembles association rules. The proposed hybrid model comprises fuzzy formal concept analysis and concept-level sentiment analysis (FFCA + SA) in tandem, which in turn is compared against formal concept analysis and concept-level sentiment analysis (FCA + SA). Because of the immediate fallout of the negative sentiments, a financial company is interested in studying them in more detail than the positive ones. Therefore, the model generates a list of ‘association rules’ the corresponding negative sentiment score along with the list of associated documents. Association rules are rank ordered according to the negative sentiment score, which in turn reflects severity affected services/products. The proposed model also provides interactive visualization that enables business analysts and managers to access a specific set of complaints without having to go through the entire set thoroughly. This saves a lot of time that would have otherwise been spent on cumbersome manual operations. Moreover, partial evaluation of the proposed methodology by human annotators yielded 64.06% matching score in terms of the opinions determination of aspects. © 2017 Elsevier B.V.","Association rules; Customer complaints; Descriptive analytics; Fuzzy formal concept analysis; Sentiment analysis; Text mining","Association rules; Data mining; Finance; Information analysis; Natural language processing systems; Sales; Social networking (online); Visualization; Customer complaints; Descriptive analytics; Financial companies; Interactive visualizations; Negative sentiments; Products and services; Sentiment analysis; Text mining; Formal concept analysis",2-s2.0-85019743048
"Buddhika T., Malensek M., Pallickara S.L., Pallickara S.","Synopsis: A Distributed Sketch over Voluminous Spatiotemporal Observational Streams",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029006283&doi=10.1109%2fTKDE.2017.2734661&partnerID=40&md5=2761a2b1992139b00d5da5476000ba37","Networked observational devices have proliferated in recent years, contributing to voluminous data streams from a variety of sources and problem domains. These streams often have a spatiotemporal component and include multidimensional features of interest. Processing such data in an offline fashion using batch systems or data warehouses is costly from both a storage and computational standpoint, and in many situations the insights derived from the data streams are useful only if they are timely. In this study, we propose Synopsis, an online, distributed sketch that is constructed from voluminous spatiotemporal data streams. The sketch summarizes feature values and inter-feature relationships in memory to facilitate real-time query evaluations and to serve as input to computations expressed using analytical engines. As the data streams evolve, Synopsis performs targeted dynamic scaling to ensure high accuracy and effective resource utilization. We evaluate our system in the context of two real-world spatiotemporal datasets and demonstrate its efficacy in both scalability and query evaluations. © 2017 IEEE.","Data sketches; query evaluations; spatiotemporal data; streaming systems","Batch data processing; Data communication systems; Data handling; Data mining; Data warehouses; Digital storage; Distributed database systems; Feature extraction; Interactive computer systems; Query languages; Query processing; Real time systems; Vegetation; data sketches; Distributed database; Geo-spatial analysis; Query evaluation; Spatio-temporal data; Spatiotemporal phenomena; Streaming systems; Search engines",2-s2.0-85029006283
"Oulad-Naoui S., Cherroun H., Ziadi D.","A formal series-based unification of the frequent itemset mining approaches",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017028386&doi=10.1007%2fs10115-017-1048-y&partnerID=40&md5=a91b8c721413edf904d366a930a4a95a","Over the last two decades, a great deal of work has been devoted to the algorithmic aspects of the frequent itemset (FI) mining problem, leading to a phenomenal number of algorithms and associated implementations, each of which claims supremacy. Meanwhile, it is generally well agreed that developing a unifying theory is one of the most important issues in data mining research. Hence, our primary motivation for this work is to introduce a high-level formalism for this basic problem, which induces a unified vision of the algorithmic approaches presented so far. The key distinctive feature of the introduced model is that it combines, in one fashion, both the qualitative and the quantitative aspects of this basic problem. In this paper, we propose a new model for the FI-mining task based on formal series. In fact, we encode the itemsets as words over a sorted alphabet and express this problem by a formal series over the counting semiring (N, + , × , 0 , 1) , whose range represents the itemsets, and the coefficients are their supports. The aim is threefold: First, to define a clear, unified and extensible theoretical framework through which we can state the main FI-approaches. Second, to prove a convenient connection between the determinization of the acyclic weighted automaton that represents a transaction dataset and the computation of the associated collection of FI. Finally, to devise a first algorithmic transcription, baptized Wafi, of our model by means of weighted automata, which we evaluate against representative leading algorithms. The obtained results show the suitability of our formalism. © 2017, Springer-Verlag London.","Algorithms; Data mining; Formal series; Frequent itemsets; Unification; Weighted automata",,2-s2.0-85017028386
"Rowe E., Asbell-Clarke J., Baker R.S., Eagle M., Hicks A.G., Barnes T.M., Brown R.A., Edwards T.","Assessing implicit science learning in digital games",2017,"Computers in Human Behavior",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018450823&doi=10.1016%2fj.chb.2017.03.043&partnerID=40&md5=4ddb167fa1de34dde5db7c1d20f27ff2","Building on the promise shown in game-based learning research, this paper explores methods for Game-Based Learning Assessments (GBLA) using a variety of educational data mining techniques (EDM). GBLA research examines patterns of behaviors evident in game data logs for the measurement of implicit learning—the development of unarticulated knowledge that is not yet expressible on a test or formal assessment. This paper reports on the study of two digital games showing how the combination of human coding with EDM has enabled researchers to measure implicit learning of Physics. In the game Impulse, researchers combined human coding of video with educational data mining to create a set of automated detectors of students' implicit understanding of Newtonian mechanics. For Quantum Spectre, an optics puzzle game, human coding of Interaction Networks was used to identify common student errors. Findings show that several of our measures of student implicit learning within these games were significantly correlated with improvements in external postassessments. Methods and detailed findings were different for each type of game. These results suggest GBLA shows promise for future work such as adaptive games and in-class, data-driven formative assessments, but design of the assessment mechanics must be carefully crafted for each game. © 2017 Elsevier Ltd","Computer-based assessment; Educational data mining; Game-based learning; Implicit science learning; Learning analytics","Codes (symbols); Coding errors; Data mining; E-learning; Education; Students; Computer-based assessments; Educational data mining; Game-based Learning; Learning analytics; Science learning; Computer games; behavior; data mining; error; human; human experiment; learning; mechanics; optics; physics; scientist; student; videorecording",2-s2.0-85018450823
"Plastino A., Gonçalves E.C., da Silva P.N., Carneiro G., Azeredo R.B.D.V.","Combining classification and regression for improving permeability estimations from 1H NMR relaxation data",2017,"Journal of Applied Geophysics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032737244&doi=10.1016%2fj.jappgeo.2017.09.003&partnerID=40&md5=cb6bfeabdcc1565c4f927a2b2a081514","Permeability is a key property to state if producing fluids from a hydrocarbon reservoir will be feasible or not. Nevertheless, the accurate estimation of permeability in carbonate rock reservoirs is considered a challenging task. This is due to two main reasons: (i) the extreme heterogeneity typically contained in these kinds of rocks; (ii) the fact that the permeability of carbonate rocks can vary enormously (from 0.01 mD to more than 1000 mD). In this paper, we explore a methodology to estimate permeability, from nuclear magnetic resonance (NMR) data, based on the combination of classification and regression techniques. This proposal works in two steps. First, data preprocessing and classification algorithms are applied to the original target dataset in order to build a classification model that is able to identify the permeability class zone – among four possible ranges – of a new rock sample. In the second step, according to the predicted permeability class zone, a specialized regression model is selected and will be responsible for determining the final permeability continuous value of the new rock sample. We evaluated this approach over a dataset formed by 1H NMR relaxation responses of 78 rock samples and their respective laboratory-measured permeability values. The obtained results revealed that the proposed approach, which combines classification and regression, led to more uniform and accurate predictions (RMSE = 0.413 and r2 = 0.888) compared with the use of regression in a stand-alone manner (RMSE = 0.641 and r2 = 0.735). © 2017 Elsevier B.V.","Classification; Data mining; Nuclear magnetic resonance; Permeability; Regression","Carbonates; Classification (of information); Data mining; Mechanical permeability; Nuclear magnetic resonance; Regression analysis; Rocks; Sedimentary rocks; Carbonate rock reservoir; Classification algorithm; Classification models; Hydrocarbon reservoir; Improving permeabilities; Nuclear magnetic resonance(NMR); Regression; Regression techniques; Petroleum reservoir engineering",2-s2.0-85032737244
"Hashemi M.","A testbed for evaluating network construction algorithms from GPS traces",2017,"Computers, Environment and Urban Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027576113&doi=10.1016%2fj.compenvurbsys.2017.08.003&partnerID=40&md5=68ed2241d81e18fd46bad3a9376af256","Developing algorithms which construct street/pedestrian networks from crowd-sourced GPS traces has been an ongoing research since the outbreak of inexpensive GPS receivers on mobile devices. Although, the proposed algorithms are evaluated by their developers, the evaluation results cannot be used to compare their accuracy because: (a) different algorithms target different types of networks, some designed for complicated networks while others for simple ones, (b) GPS traces, used in different studies, are not the same, some of them are more accurate and denser than others, and (c) the constructed networks are evaluated either qualitatively or with different quantitative metrics. Lack of a comprehensive testbed for evaluating network construction algorithms has made it difficult for authors, reviewers, and readers to monitor the effectiveness of such algorithms. This study establishes a testbed for evaluating network construction algorithms containing three components: (a) street and pedestrian networks with different densities and complexities as the baseline, (b) collections of GPS traces with different accuracies and sampling rates to be used by algorithms to construct those networks, and (c) three quantitative metrics to indicate the completeness, precision, and topology correctness of the constructed network, in addition to the algorithm's time complexity, conventionally used to indicate its time performance. This testbed not only paves the way for comparing network construction algorithms but also allows researchers to focus on their algorithms rather than collecting data for testing it or looking for ways to describe its accuracy. © 2017 Elsevier Ltd","Artificial intelligence; GPS data; Network construction; Pedestrian network; Spatial data mining; Street network","Artificial intelligence; Data mining; Global positioning system; Testbeds; GPS data; Network construction; Pedestrian network; Spatial data mining; Street network; Computational complexity",2-s2.0-85027576113
"Sethi T., Shah N.H.","Pharmacovigilance Using Textual Data: The Need to Go Deeper and Wider into the Con(text)",2017,"Drug Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026922848&doi=10.1007%2fs40264-017-0585-3&partnerID=40&md5=2c41ce2d678109befa18b1c7a282a5e8",[No abstract available],,"clinical practice; drug safety; drug surveillance program; electronic health record; human; information technology; medical ontology; natural language processing; Note; priority journal; voluntary reporting; data mining; Adverse Drug Reaction Reporting Systems; Data Mining; Pharmacovigilance",2-s2.0-85026922848
"Kholghi M., De Vine L., Sitbon L., Zuccon G., Nguyen A.","Clinical information extraction using small data: An active learning approach based on sequence representations and word embeddings",2017,"Journal of the Association for Information Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030108381&doi=10.1002%2fasi.23936&partnerID=40&md5=4c408144d65a831ca31238cc5217dba2","This article demonstrates the benefits of using sequence representations based on word embeddings to inform the seed selection and sample selection processes in an active learning pipeline for clinical information extraction. Seed selection refers to choosing an initial sample set to label to form an initial learning model. Sample selection refers to selecting informative samples to update the model at each iteration of the active learning process. Compared to supervised machine learning approaches, active learning offers the opportunity to build statistical classifiers with a reduced amount of training samples that require manual annotation. Reducing the manual annotation effort can support automating the clinical information extraction process. This is particularly beneficial in the clinical domain, where manual annotation is a time-consuming and costly task, as it requires extensive labor from clinical experts. Our empirical findings demonstrate that (a) using sequence representations along with the length of sequence for seed selection shows potential towards more effective initial models, and (b) using sequence representations for sample selection leads to significantly lower manual annotation efforts, with up to 3% and 6% fewer tokens and concepts requiring annotation, respectively, compared to state-of-the-art query strategies. © 2017 ASIS&T",,"Classification (of information); Data mining; Information analysis; Information retrieval; Learning systems; Supervised learning; Active-learning process; Empirical findings; Manual annotation; Query strategies; Sample selection; State of the art; Statistical classifier; Supervised machine learning; Artificial intelligence; classifier; embedding; extraction; human; pipeline; plant seed; sampling; supervised machine learning",2-s2.0-85030108381
"Hadi W., Issa G., Ishtaiwi A.","ACPRISM: Associative classification based on PRISM algorithm",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024827742&doi=10.1016%2fj.ins.2017.07.025&partnerID=40&md5=f5ddddfed8ced8ecc984f2ed6efef863","Associative classification (AC) is an integration between association rules and classification tasks that aim to predict unseen samples. Several studies indicate that the AC algorithms produce more accurate results than classical data mining algorithms. However, current AC algorithms inherit from association rules two major drawbacks resulting in a massive set of generated rules, in addition to a very large number of models (classifiers). In response to these two drawbacks, a new AC algorithm based on PRISM algorithm (ACPRISM) is proposed which employs the power of the PRISM algorithm to decrease the number of generated rules. To investigate the efficiency and the performance of the proposed algorithm, five different algorithms were tested, namely FACA, CBA, MAC, PRISM and RIPPER. Two experiments were conducted on groundwater and 16 different well-known datasets using predictive accuracy (%), number of generated rules and time taken to build the model (learning times). Our experimental results show that the ACPRISM produced the lowest number of rules, and is much more efficient and more scalable than all considered algorithms with regard to learning times. Finally, the ACPRISM outperformed the CBA, MCAR, PRISM and RIPPER algorithms in terms of predictive accuracy, and produced comparable results to the FACA algorithm. © 2017 Elsevier Inc.","Association classification; Classification; Data mining; PRISM algorithm","Association rules; Classification (of information); Education; Groundwater; Prisms; Association classification; Associative classification; Classification tasks; Data mining algorithm; Predictive accuracy; Prism algorithms; RIPPER algorithm; To a very large; Data mining",2-s2.0-85024827742
"Siegel J.E., Bhattacharyya R., Kumar S., Sarma S.E.","Air filter particulate loading detection using smartphone audio and optimized ensemble classification",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030470709&doi=10.1016%2fj.engappai.2017.09.015&partnerID=40&md5=c29594968780a301786e0036b02e7697","Automotive engine intake filters ensure clean air delivery to the engine, though over time these filters load with contaminants hindering free airflow. Today's open-loop approach to air filter maintenance has drivers replace elements at predetermined service intervals, causing costly and potentially harmful over- and under-replacement. The result is that many vehicles consistently operate with reduced power, increased fuel consumption, or excessive particulate-related wear which may harm the catalyst or damage machined engine surfaces. We present a method of detecting filter contaminant loading from audio data collected by a smartphone and a stand microphone. Our machine learning approach to filter supervision uses Mel-Cepstrum, Fourier and Wavelet features as input into a classification model and applies feature ranking to select the best-differentiating features. We demonstrate the robustness of our technique by showing its efficacy for two vehicle types and different microphones, finding a best result of 79.7% accuracy when classifying a filter into three loading states. Refinements to this technique will help drivers supervise their filters and aid in optimally timing their replacement. This will result in an improvement in vehicle performance, efficiency, and reliability, while reducing the cost of maintenance to vehicle owners. © 2017 Elsevier Ltd","Ambient intelligence; Data mining and knowledge discovery; Emerging applications and technology; Intelligent vehicles; Machine learning","Air filters; Air intakes; Ambient intelligence; Artificial intelligence; Bandpass filters; Classification (of information); Data mining; Engines; Filtration; Impurities; Intelligent vehicle highway systems; Learning algorithms; Learning systems; Microphones; Smartphones; Vehicle performance; Vehicles; Classification models; Contaminant loadings; Cost of maintenance; Data mining and knowledge discovery; Emerging applications; Ensemble classification; Machine learning approaches; Particulate loading; Loading",2-s2.0-85030470709
"Gkatziaki V., Giatsoglou M., Chatzakou D., Vakali A.","DYNAMICITY: Revealing city dynamics from citizens social media broadcasts",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026753970&doi=10.1016%2fj.is.2017.07.007&partnerID=40&md5=57fb80483e2ba42767759baf92928478","Social media and mobile devices have revolutionized the way people communicate and share information in various contexts, such as in cities. In today's “smart” cities, massive amounts of multiple forms of geolocated content is generated daily in social media, out of which knowledge for social interactions and urban dynamics can be derived. This work addresses the problem of detecting urban social activity patterns and interactions, by modeling cities into “dynamic areas”, i.e., coherent geographic areas shaped through social activities. Social media users provide the information on such social activities and interactions in cases when they are on the move around the city neighborhoods. The proposed approach models city places as feature vectors which represent users visiting patterns (social activity), the time of observed visits (temporal activity), and the context of functionality of visited places category. To uncover the dynamics of city areas, a clustering approach is proposed which considers the derived feature vectors to group people's activities with respect to location, time, and context. The proposed methodology has been implemented on the DYNAMICITY platform which demonstrates neighborhood analytics via a Web interface that allows end-users to explore neighborhoods dynamics and gain insights for city cross-neighborhood patterns and inter-relationships. © 2017 Elsevier Ltd","Crowdsourcing; Data mining; Smart city applications; Social data mining; Urban dynamics","Crowdsourcing; Data mining; Human computer interaction; Social networking (online); Clustering approach; Geographic areas; Inter-relationships; Neighborhood pattern; Social activities; Social data mining; Social interactions; Urban dynamics; Smart city",2-s2.0-85026753970
"Slanzi G., Pizarro G., Velásquez J.D.","Biometric information fusion for web user navigation and preferences analysis: An overview",2017,"Information Fusion",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019897318&doi=10.1016%2fj.inffus.2017.02.006&partnerID=40&md5=1654666619dc450c1155407d740d9c53","Throughout the years having knowledge of Web users’ interests, navigational actions and preferences has gained importance due to the objectives of organizations and companies. Traditionally this field has been studied from the Web Mining perspective, particularly through the Web Usage Mining (WUM) concept, which consists of the application of machine learning techniques over data originated in the Web (Web data) for automatic extraction of behavioral patterns from Web users. WUM makes use of data sources that approximate users’ behavior, such as weblogs or clickstreams among others; however these sources imply a considerable degree of subjectivity to interpret. For that reason, the application of biometric tools with the possibility of measuring actual responses to the stimuli presented via websites has become of interest in this field. Instead of doing separate analyses, information fusion (IF) tries to improve results by developing efficient methods for transforming information from different sources into a single representation, which then could be used to guide biometric data fusion to complement the traditional WUM studies and obtain better results. This paper presents a survey of Biometric IF applied to the WUM field, by first defining WUM and its main applications, later explaining how the Biometric IF could be applied and finally reviewing several studies that apply this concept to WUM. © 2017 Elsevier B.V.","Biometric data; Information fusion; Web usage mining","Biometrics; Data fusion; Information fusion; Learning systems; Metadata; Websites; Automatic extraction; Behavioral patterns; Biometric data; Biometric informations; Clickstreams; Machine learning techniques; Separate analysis; Web usage mining; Data mining",2-s2.0-85019897318
"Abid A., Masmoudi A., Kachouri A., Mahfoudhi A.","Outlier Detection in Wireless Sensor Networks Based on OPTICS Method for Events and Errors Identification",2017,"Wireless Personal Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020532302&doi=10.1007%2fs11277-017-4583-7&partnerID=40&md5=1730275bc3635de4e05800a879f7e247","Wireless Sensor Network is composed of small, low cost, low energy, and multifunctional sensors. In addition, this network could have scalability, topology, synchronization, radio-coverage, safety and security constraints. Therefore, our challenge is to classify data into normal and abnormal measurements using outlier detection methods. This paper explore the density-based method Ordering Points to Identify the Clustering Structure. Proposed detector applies an auto- configuration of parameters without previous known environmental conditions. It also extracts hierarchical clusters that serve in a post-processing treatment for classification of data into errors and events. Performance is examined within a real and synthetic databases from Intel Berkeley Research lab. Results demonstrate that our proposed process analyzes data of this network with an average equal to 81% of outlier detection rate, 74% of precision rate and only 2% of false alarms rate that it is very low compared to other methods. © 2017, Springer Science+Business Media, LLC.","Data analysis; Density clustering; OPTICS; Outlier detection; WSN","Classification (of information); Data handling; Data mining; Data reduction; Hierarchical systems; Low power electronics; Optics; Statistics; Classification of data; Density clustering; Density-based method; Environmental conditions; Hierarchical clusters; Multifunctional sensors; Outlier Detection; Safety and securities; Wireless sensor networks",2-s2.0-85020532302
"He C., Fu H., Guo C., Luk W., Yang G.","A Fully-Pipelined Hardware Design for Gaussian Mixture Models",2017,"IEEE Transactions on Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032439772&doi=10.1109%2fTC.2017.2712152&partnerID=40&md5=b948a9fc850e01ed8c03c08a2f103ec1","Gaussian Mixture Models (GMMs) are widely used in many applications such as data mining, signal processing and computer vision, for probability density modeling and soft clustering. However, the parameters of a GMM need to be estimated from data by, for example, the Expectation-Maximization algorithm for Gaussian Mixture Models (EM-GMM), which is computationally demanding. This paper presents a novel design for the EM-GMM algorithm targeting reconfigurable platforms, with five main contributions. First, a pipeline-friendly EM-GMM with diagonal covariance matrices that can easily be mapped to hardware architectures. Second, a function evaluation unit for Gaussian probability density based on fixed-point arithmetic. Third, our approach is extended to support a wide range of dimensions or/and components by fitting multiple pieces of smaller dimensions onto an FPGA chip. Fourth, we derive a cost and performance model that estimates logic resources. Fifth, our dataflow design targeting the Maxeler MPC-X2000 with a Stratix-5SGSD8 FPGA can run over 200 times faster than a 6-core Xeon E5645 processor, and over 39 times faster than a Pascal TITAN-X GPU. Our design provides a practical solution to applications for training and explores better parameters for GMMs with hundreds of millions of high dimensional input instances, for low-latency and high-performance applications. © 1968-2012 IEEE.","algorithms implemented in hardware; data flow engine; expectation maximization; Gaussian mixture model; high performance computing; reconfigurable hardware","Communication channels (information theory); Computation theory; Computer hardware; Covariance matrix; Data flow analysis; Data mining; Field programmable gate arrays (FPGA); Fixed point arithmetic; Gaussian distribution; Hardware; Image segmentation; Integrated circuit design; Maximum principle; Object recognition; Probability density function; Signal processing; Algorithms implemented-in-hardware; Data flow; Expectation - maximizations; Gaussian Mixture Model; High performance computing; Reconfigurable hardware",2-s2.0-85032439772
"Jiang N., Zhou C., Lu S., Zhang Z.","Propagation and prediction of blasting vibration on slope in an open pit during underground mining",2017,"Tunnelling and Underground Space Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032281415&doi=10.1016%2fj.tust.2017.09.005&partnerID=40&md5=d220ae98292427907c9cb4307304812d","In order to assess the influence of underground mining blasting on the stability of open pit slopes and control the potential risk, it is crucial to investigate the propagation and attenuation of blasting vibration in rock slopes. In this paper, we aimed to investigate the attenuation of blasting vibration on open pit slopes subjected to underground mining activities using Daye Iron Mine as an example. To this end, we first analyzed the characteristics of blast loadings using the dynamic finite element method. We then established a three dimensional (3D) numerical model for open pit subjected to underground mining and proved its reliability using the field monitoring data. Next, we calculated and analyzed the distribution characteristics of peak particle velocity (PPV) by inputting the obtained blast loadings into the numerical model and discussed the impacts of blasting vibration on open pit slopes subjected to underground mining. To better and more conveniently assess and predict the impacts of blasting vibration, we further established a mathematical model to describe the attenuation of PPV on open pit slopes subjected to underground mining blasting through theoretical analysis and proposed a PPV predicting model for slopes in Daye Iron Mine based on the numerical simulations of underground mining blasting at different elevations. © 2017 Elsevier Ltd","Peak particle velocity; Propagation; Rock slope; Underground mining","Blasting; Cerenkov counters; Finite element method; Forecasting; Iron mines; Numerical models; Risk assessment; Slope stability; Velocity control; Vibration analysis; Wave propagation; Distribution characteristics; Dynamic finite element method; Field monitoring data; Peak particle velocities; Rock slope; Three dimensional (3D) numerical models; Underground mining; Underground mining activities; Open pit mining; blasting; computer simulation; numerical model; open pit mine; reliability analysis; slope stability; stability analysis; theoretical study; three-dimensional modeling; vibration; China",2-s2.0-85032281415
"Hua J.-C., Noorian F., Moss D., Leong P.H.W., Gunaratne G.H.","High-dimensional time series prediction using kernel-based Koopman mode regression",2017,"Nonlinear Dynamics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029473070&doi=10.1007%2fs11071-017-3764-y&partnerID=40&md5=19524836db348e46959dcd5759f18980","We propose a novel methodology for high-dimensional time series prediction based on the kernel method extension of data-driven Koopman spectral analysis, via the following methodological advances: (a) a new numerical regularization method, (b) a natural ordering of Koopman modes which provides a fast alternative to the sparsity-promoting procedure, (c) a predictable Koopman modes selection technique which is equivalent to cross-validation in machine learning, (d) an optimization method for selected Koopman modes to improve prediction accuracy, (e) prediction model generation and selection based on historical error measures. The prediction accuracy of this methodology is excellent: for example, when it is used to predict clients’ order flow time series of foreign exchange, which is almost random, it can achieve more than 10% improvement on root-mean-square error over auto-regressive moving average. This methodology also opens up new possibilities for data-driven modeling and forecasting complex systems that generate the high-dimensional time series. We believe that this methodology will be of interest to the community of scientists and engineers working on quantitative finance, econometrics, system biology, neurosciences, meteorology, oceanography, system identification and control, data mining, machine learning, and many other fields involving high-dimensional time series and spatio-temporal data. © 2017, Springer Science+Business Media B.V.","Complex systems; Data-driven Koopman operator; Dynamic mode decomposition; High-dimensional time series; Kernel methods; Spatio-temporal dynamics","Artificial intelligence; Data mining; Economics; Forecasting; Large scale systems; Learning systems; Mean square error; Numerical methods; Spectrum analysis; Statistics; Time series; Data driven; Dynamic mode decompositions; High-dimensional; Kernel methods; Spatio-temporal dynamics; Time series analysis",2-s2.0-85029473070
"Trovati M., Hayes J., Palmieri F., Bessis N.","Automated extraction of fragments of Bayesian networks from textual sources",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026241045&doi=10.1016%2fj.asoc.2017.07.009&partnerID=40&md5=eada9fe59a129e298698ef2f8fc20462","Mining large amounts of unstructured data for extracting meaningful, accurate, and actionable information, is at the core of a variety of research disciplines including computer science, mathematical and statistical modelling, as well as knowledge engineering. In particular, the ability to model complex scenarios based on unstructured datasets is an important step towards an integrated and accurate knowledge extraction approach. This would provide a significant insight in any decision making process driven by Big Data analysis activities. However, there are multiple challenges that need to be fully addressed in order to achieve this, especially when large and unstructured data sets are considered. In this article we propose and analyse a novel method to extract and build fragments of Bayesian networks (BNs) from unstructured large data sources. The results of our analysis show the potential of our approach, and highlight its accuracy and efficiency. More specifically, when compared with existing approaches, our method addresses specific challenges posed by the automated extraction of BNs with extensive applications to unstructured and highly dynamic data sources. The aim of this work is to advance the current state-of-the-art approaches to the automated extraction of BNs from unstructured datasets, which provide a versatile and powerful modelling framework to facilitate knowledge discovery in complex decision scenarios. © 2017 Elsevier B.V.","Bayesian networks; Network theory; Text mining","Automation; Bayesian networks; Big data; Circuit theory; Complex networks; Extraction; Automated extraction; Bayesian Networks (bns); Decision making process; Knowledge extraction; Modelling framework; State-of-the-art approach; Statistical modelling; Text mining; Data mining",2-s2.0-85026241045
"Cao X., Li H., Dang L., Lin Y.","A two-party privacy preserving set intersection protocol against malicious users in cloud computing",2017,"Computer Standards and Interfaces",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994141312&doi=10.1016%2fj.csi.2016.08.004&partnerID=40&md5=8dbc83329d69cfe6f12a9b1df744a496","Set intersection is a fundamental operation in data mining and cloud computing. In this paper, a protocol is proposed to determine the intersection between two datasets while preserving the privacy of each set. Malicious participants are considered and the misbehavior of participants is prevented. Our method is based on a combination of commutative encryption and hash-based commitments. Performance evaluation demonstrates the effectiveness of the protocol. Security discussion is given showing that the protocol provides data privacy, secure set intersection and tolerance to participants cheating. © 2016 Elsevier B.V.","Commutative encryption; Data mining; Malicious participants; Set intersection","Cloud computing; Cryptography; Data mining; Commutative encryption; Fundamental operations; Malicious participant; Privacy preserving set intersections; Secure set intersections; Set intersection; Data privacy",2-s2.0-84994141312
"ElRafey A., Wojtusiak J.","Recent advances in scaling-down sampling methods in machine learning",2017,"Wiley Interdisciplinary Reviews: Computational Statistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029230224&doi=10.1002%2fwics.1414&partnerID=40&md5=5ebbe8d0cfcb3fc6665c22a987651f80","Data sampling methods have been investigated for decades in the context of machine learning and statistical algorithms, with significant progress made in the past few years driven by strong interest in big data and distributed computing. Most recently, progress has been made in methods that can be broadly categorized into random sampling including density-biased and nonuniform sampling methods; active learning methods, which are a type of semi-supervised learning and an area of intense research; and progressive sampling methods which can be viewed as a combination of the above two approaches. A unified view of scaling-down sampling methods is presented in this article and complemented with descriptions of relevant published literature. WIREs Comput Stat 2017, 9:e1414. doi: 10.1002/wics.1414. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.","data mining; evolutionary computation; health infomratics; machine learning","Artificial intelligence; Big data; Data mining; Distributed computer systems; Evolutionary algorithms; Learning algorithms; Sampling; Signal sampling; Supervised learning; Active learning methods; Data sampling; Nonuniform sampling; Progressive sampling; Random sampling; Scaling down; Semi- supervised learning; Statistical algorithm; Learning systems",2-s2.0-85029230224
"Zhao N., Zhang L., Du B., Zhang Q., You J., Tao D.","Robust Dual Clustering with Adaptive Manifold Regularization",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028918917&doi=10.1109%2fTKDE.2017.2732986&partnerID=40&md5=1cd70a92aa00bf8b3152896a53d9a385","In recent years, various data clustering algorithms have been proposed in the data mining and engineering communities. However, there are still drawbacks in traditional clustering methods which are worth to be further investigated, such as clustering for the high dimensional data, learning an ideal affinity matrix which optimally reveals the global data structure, discovering the intrinsic geometrical and discriminative properties of the data space, and reducing the noises influence brings by the complex data input. In this paper, we propose a novel clustering algorithm called robust dual clustering with adaptive manifold regularization (RDC), which simultaneously performs dual matrix factorization tasks with the target of an identical cluster indicator in both of the original and projected feature spaces, respectively. Among which, the l2,1 -norm is used instead of the conventional l2-norm to measure the loss, which helps to improve the model robustness by relieving the influences by the noises and outliers. In order to better consider the intrinsic geometrical and discriminative data structure, we incorporate the manifold regularization term on the cluster indicator by using a particularly learned affinity matrix which is more suitable for the clustering task. Moreover, a novel augmented lagrangian method (ALM) based procedure is designed to effectively and efficiently seek the optimal solution of the proposed RDC optimization. Numerous experiments on the representative data sets demonstrate the superior performance of the proposed method compares to the existing clustering algorithms. © 2017 IEEE.","Clustering; dimension reduction; manifold regularization; matrix factorization","Automobile engine manifolds; Cluster analysis; Constrained optimization; Data mining; Data structures; Factorization; Lagrange multipliers; Matrix algebra; Optimization; Robustness (control systems); Clustering; Clustering methods; Dimension reduction; Manifold regularizations; Matrix factorizations; Clustering algorithms",2-s2.0-85028918917
"Yu B., Choi W., Liu L.","Exploring correlation for fast skyline computation",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019044934&doi=10.1007%2fs11227-017-2064-0&partnerID=40&md5=304d1cd2b966f42a1b7ba0fc5feadecf","Scaling skyline queries over high-dimensional datasets remains to be challenging due to the fact that most existing algorithms assume dimensional independence when establishing the worst-case complexity by discarding correlation distribution. In this paper, we present HashSkyline, a systematic and correlation-aware approach for scaling skyline queries over high-dimensional datasets with three novel features: First, it offers a fast hash-based method to prune non-skyline points by utilizing data correlation characteristics and speed up the overall skyline evaluation for correlated datasets. Second, we develop HashSkylineG P U, which can dramatically reduce the response time for anti-correlated and independent datasets by capitalizing on the parallel processing power of GPUs. Third, the HashSkyline approach uses the pivot cell-based mechanism combined with the correlation threshold to determine the correlation distribution characteristics for a given dataset, enabling adaptive configuration of HashSkyline for skyline query evaluation by auto-switching of HashSkylineC P U and HashSkylineG P U. We evaluate the validity of HashSkyline using both synthetic datasets and real datasets. Our experiments show that HashSkyline consumes significantly less pre-processing cost and achieves significantly higher overall query performance, compared to existing state-of-the-art algorithms. © 2017, Springer Science+Business Media New York.","Data analysis; Information extraction; Parallel computing; Skyline","Data reduction; Indexing (of information); Information analysis; Information retrieval; Parallel processing systems; Program processors; Query processing; Correlation distribution; Correlation threshold; High dimensional datasets; Parallel processing; Skyline; Skyline computations; State-of-the-art algorithms; Worst-case complexity; Data mining",2-s2.0-85019044934
"Hirth G.A., Johansen M.P., Carpenter J.G., Bollhöfer A., Beresford N.A.","Whole-organism concentration ratios in wildlife inhabiting Australian uranium mining environments",2017,"Journal of Environmental Radioactivity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018167153&doi=10.1016%2fj.jenvrad.2017.04.007&partnerID=40&md5=767114ae506475d66f5d4051dc4cd20c","Wildlife concentration ratios for 226Ra, 210Pb, 210Po and isotopes of Th and U from soil, water, and sediments were evaluated for a range of Australian uranium mining environments. Whole-organism concentration ratios (CRwo-media) were developed for 271 radionuclide-organism pairs within the terrestrial and freshwater wildlife groups. Australian wildlife often has distinct physiological attributes, such as the lower metabolic rates of macropod marsupials as compared with placental mammals. In addition, the Australian CRswo-media originate from tropical and semi-arid climates, rather than from the temperate-dominated climates of Europe and North America from which most (&gt;90%) of internationally available CRwo-media values originate. When compared, the Australian and non-Australian CRs are significantly different for some wildlife categories (e.g. grasses, mammals) but not others (e.g. shrubs). Where differences exist, the Australian values were higher, suggesting that site-, or region-specific CRswo-media should be used in detailed Australian assessments. However, in screening studies, use of the international mean values in the Wildlife Transfer Database (WTD) appears to be appropriate, as long as the values used encompass the Australian 95th percentile values. Gaps in the Australian datasets include a lack of marine parameters, and no CR data are available for freshwater phytoplankton, zooplankton, insects, insect larvae or amphibians; for terrestrial environments, there are no data for amphibians, annelids, ferns, fungi or lichens &amp; bryophytes. The new Australian specific parameters will aide in evaluating remediation plans and ongoing operations at mining and waste sites within Australia. They have also substantially bolstered the body of U- and Th-series CRwo-media data for use internationally. © 2017 The Authors","238U decay series; Australia; Concentration ratios; Uranium mining; Wildlife","Biology; Fungi; Mammals; Phytoplankton; Plankton; Radioactive wastes; Uranium; Water; <sup>238</sup>U decay series; Australia; Concentration ratio; Uranium mining; Wildlife; Animals; lead 210; polonium; radium 226; thorium; uranium; concentration (composition); data set; marsupial; metabolism; mining; parameterization; physiological response; radioactive decay; uranium isotope; wildlife management; Amphibia; annelid; Article; Australia; buffalo; ecosystem restoration; environmental impact assessment; environmental radioactivity; fern; freshwater species; fungus; insect; kangaroo; lichen (organism); marsupial; metabolic rate; mining; moss; nonhuman; phytoplankton; placental mammal; Poaceae; reptile; sediment; semiarid climate; shrub; soil pollution; terrestrial species; tree; water pollution; wildlife; zooplankton; Australia; Europe; North America; Amphibia; Annelida; bryophytes; Eutheria; Filicophyta; Fungi; Hexapoda; Mammalia; Metatheria; Poaceae",2-s2.0-85018167153
"Shin S.K., Hur H., Cheon E.K., Oh O.H., Lee J.S., Ko W.J., Kim B.S., Kwon Y.","A personalized and learning approach for identifying drugs with adverse events",2017,"Yonsei Medical Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032001506&doi=10.3349%2fymj.2017.58.6.1229&partnerID=40&md5=f89724c556690d927a4a589e6f365b18","Purpose: Adverse drug events (ADEs) are associated with high health and financial costs and have increased as more elderly patients treated with multiple medications emerge in an aging society. It has thus become challenging for physicians to identify drugs causing adverse events. This study proposes a novel approach that can improve clinical decision making with recommendations on ADE causative drugs based on patient information, drug information, and previous ADE cases. Materials and Methods: We introduce a personalized and learning approach for detecting drugs with a specific adverse event, where recommendations tailored to each patient are generated using data mining techniques. Recommendations could be improved by learning the associations of patients and ADEs as more ADE cases are accumulated through iterations. After consulting the system-generated recommendations, a physician can alter prescriptions accordingly and report feedback, enabling the system to evolve with actual causal relationships. Results: A prototype system is developed using ADE cases reported over 1.5 years and recommendations obtained from decision tree analysis are validated by physicians. Two representative cases demonstrate that the personalized recommendations could contribute to more prompt and accurate responses to ADEs. Conclusion: The current system where the information of individual drugs exists but is not organized in such a way that facilitates the extraction of relevant information together can be complemented with the proposed approach to enhance the treatment of patients with ADEs. Our illustrative results show the promise of the proposed system and further studies are expected to validate its performance with quantitative measures. © Yonsei University College of Medicine 2017.","Adverse drug event; Clinical decision making; Data mining; Learning","antidepressant agent; iomeprol; nefopam; paramacet; tramadol; unclassified drug; adult; aged; Article; decision making; dizziness; drug information; female; hospital personnel; human; learning; major clinical study; male; mining; nausea and vomiting; personalized medicine; physician; pruritus; rash; urticaria",2-s2.0-85032001506
"Yu C., Wang N., Yang L.T., Yao D., Hsu C.-H., Jin H.","A semi-supervised social relationships inferred model based on mobile phone data",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007415956&doi=10.1016%2fj.future.2016.11.027&partnerID=40&md5=8801b0fa97c514ef0702b98478e2e90e","Exploring the relationships of humans is an important study in the mobile communication network. But the relationship prediction accuracy is not good enough when the number of known relationship labels (e.g., “friend” and “colleague”) is small, especially when the number of different relation classes are imbalanced in the mobile communication network. To deal with issues, we present a semi-supervised social relationships inferred model. This model can infer the relationships based on a large amount of unlabeled data or a small amount of labeled data. The model is a co-training style semi-supervised model which is combined with the support vector machine and naive Bayes. The final relationship labels are decided by the two classifiers. The proposed model is evaluated by a real mobile communication network dataset and the experiment results show that the model is effective in relationship mining, especially when the relationship network is in a stable state. © 2016 Elsevier B.V.","Graphic structure; Mobile phone data; Semi-supervised learning; Social relationship mining","Cellular telephones; Mobile phones; Social aspects; Supervised learning; Telephone sets; Graphic structures; Mobile communication networks; Mobile phone datum; Prediction accuracy; Relationship networks; Semi- supervised learning; Semi-supervised; Social relationships; Mobile telecommunication systems",2-s2.0-85007415956
"Jiang H., Kwong C.K., Yung K.L.","Predicting Future Importance of Product Features Based on Online Customer Reviews",2017,"Journal of Mechanical Design, Transactions of the ASME",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030462796&doi=10.1115%2f1.4037348&partnerID=40&md5=e1aaaafd26b185df9738101082418345","Previous studies conducted customer surveys based on questionnaires and interviews, and the survey data were then utilized to analyze product features. In recent years, online customer reviews on products became extremely popular, which contain rich information on customer opinions and expectations. However, previous studies failed to properly address the determination of the importance of product features and prediction of their future importance based on online reviews. Accordingly, a methodology for predicting future importance weights of product features based on online customer reviews is proposed in this paper which mainly involves opinion mining, a fuzzy inference method, and a fuzzy time series method. Opinion mining is adopted to analyze the online reviews and extract product features. A fuzzy inference method is used to determine the importance weights of product features using both frequencies and sentiment scores obtained from opinion mining. A fuzzy time series method is adopted to predict the future importance of product features. A case study on electric irons was conducted to illustrate the proposed methodology. To evaluate the effectiveness of the fuzzy time series method in predicting the future importance, the results obtained by the fuzzy time series method are compared with those obtained by the three common forecasting methods. The results of the comparison show that the prediction results based on fuzzy time series method are better than those based on exponential smoothing, simple moving average, and fuzzy moving average methods. Copyright © 2017 by ASME.","Fuzzy inference method; Fuzzy time series method; New product development; Opinion mining; Product features","Data mining; Forecasting; Product development; Sales; Surveys; Time series; Fuzzy inference method; Fuzzy time series; New product development; Opinion mining; Product feature; Fuzzy inference",2-s2.0-85030462796
"Piao S., Dallachy F., Baron A., Demmen J., Wattam S., Durkin P., McCracken J., Rayson P., Alexander M.","A time-sensitive historical thesaurus-based semantic tagger for deep semantic annotation",2017,"Computer Speech and Language",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019659019&doi=10.1016%2fj.csl.2017.04.010&partnerID=40&md5=4dabf1ea93ff8423d7eee205ad4592b7","Automatic extraction and analysis of meaning-related information from natural language data has been an important issue in a number of research areas, such as natural language processing (NLP), text mining, corpus linguistics, and data science. An important aspect of such information extraction and analysis is the semantic annotation of language data using a semantic tagger. In practice, various semantic annotation tools have been designed to carry out different levels of semantic annotation, such as topics of documents, semantic role labeling, named entities or events. Currently, the majority of existing semantic annotation tools identify and tag partial core semantic information in language data, but they tend to be applicable only for modern language corpora. While such semantic analyzers have proven useful for various purposes, a semantic annotation tool that is capable of annotating deep semantic senses of all lexical units, or all-words tagging, is still desirable for a deep, comprehensive semantic analysis of language data. With large-scale digitization efforts underway, delivering historical corpora with texts dating from the last 400 years, a particularly challenging aspect is the need to adapt the annotation in the face of significant word meaning change over time. In this paper, we report on the development of a new semantic tagger (the Historical Thesaurus Semantic Tagger), and discuss challenging issues we faced in this work. This new semantic tagger is built on existing NLP tools and incorporates a large-scale historical English thesaurus linked to the Oxford English Dictionary. Employing contextual disambiguation algorithms, this tool is capable of annotating lexical units with a historically-valid highly fine-grained semantic categorization scheme that contains about 225,000 semantic concepts and 4,033 thematic semantic categories. In terms of novelty, it is adapted for processing historical English data, with rich information about historical usage of words and a spelling variant normalizer for historical forms of English. Furthermore, it is able to make use of knowledge about the publication date of a text to adapt its output. In our evaluation, the system achieved encouraging accuracies ranging from 77.12% to 91.08% on individual test texts. Applying time-sensitive methods improved results by as much as 3.54% and by 1.72% on average. © 2017","Corpus annotation; Historical thesaurus; Language technology; Natural language processing; Semantic annotation; Semantic lexicon","Computational linguistics; Data handling; Data mining; Linguistics; Natural language processing systems; System program documentation; Thesauri; Corpus annotations; Language technology; Semantic annotation tools; Semantic annotations; Semantic categorization; Semantic information; Semantic lexicon; Semantic role labeling; Semantics",2-s2.0-85019659019
"Prabhakar T.V.N., Geetha P.","Two-dimensional empirical wavelet transform based supervised hyperspectral image classification",2017,"ISPRS Journal of Photogrammetry and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029724929&doi=10.1016%2fj.isprsjprs.2017.09.003&partnerID=40&md5=c3b557afd1d92511184121f48537e0ca","Hyperspectral image classification is one of the major field of application for hyperspectral imaging systems. Though hyperspectral data gives accurate results than their multispectral counterparts, they are computationally more complex due to their high dimensionality. One of the classical problem while dealing with supervised hyperspectral classification is the class imbalance problem that arises due to the limited availability of samples for training. In order to deal with high dimensionality, many feature mining techniques has been proposed in literature for hyperspectral images. In this paper, we propose a hyperspectral image classification method based on two-dimensional Empirical Wavelet Transform (2D-EWT) feature extraction and compare it with that of Image Empirical Mode Decomposition (IEMD) based extracted features and raw features. Here, the focus is upon the fact that the number of features trained should be less than what is to be tested. Since the computational time for classification is also of prime importance, only some of the fast and best of the classifiers are selected. Sparse-based classifiers are one of the fast and efficient method for supervised classification of hyperspectral images. Subspace Pursuit (SP) and Orthogonal Matching Pursuit (OMP) algorithms are used in our experiments for sparse-based classification. Other classifiers used are Support Vector Machine (SVM) and Hybrid Support Vector Selection and Adaptation (HSVSA). The proposed methodology gives improved performance in terms of classification evaluation measures for hyperspectral image classification task. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Empirical wavelet transform; Feature extraction; Hybrid support vector selection and adaptation; Hyperspectral image classification; Image empirical mode decomposition; Orthogonal matching pursuit; Subspace pursuit; Support vector machines","Extraction; Feature extraction; Hyperspectral imaging; Image compression; Image processing; Independent component analysis; Spectroscopy; Support vector machines; Vectors; Wavelet decomposition; Wavelet transforms; Classification evaluation; Empirical Mode Decomposition; Feature mining technique; Hybrid support; Hyper-spectral classification; Orthogonal matching pursuit; Subspace pursuits; Supervised classification; Image classification; accuracy assessment; algorithm; data mining; empirical orthogonal function analysis; image classification; support vector machine; two-dimensional modeling; wavelet analysis",2-s2.0-85029724929
"Raichelson L., Soffer P., Verbeek E.","Merging event logs: Combining granularity levels for process flow analysis",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028997810&doi=10.1016%2fj.is.2017.08.010&partnerID=40&md5=59e1d435e5fa818ff8e2544c36a0ac32","Process mining techniques enable the discovery and analysis of business processes and the identification of opportunities for improvement. Processes often comprise separately managed procedures documented in separate log files, which are impossible to mine in an integrative manner as the complete end-to-end process flow is obscure. These procedures can have simple (one-to-one) or complex (many-to-one or many-to-many) relationships among them. When complex relationships exist, typically different granularity levels are involved. In this paper, we present a merging algorithm that results in a comprehensive merged log that can handle all kinds of relationships between the procedures. Addressing differences in the granularity levels, it offers two views of the end-to-end process: a case view and an instance view. This enables the identification of process flow problems that could not be detected by previous techniques. The unified log can be used by process mining techniques to identify flow problems, particularly at the point of integration between the processes under consideration. The procedure proposed in this paper has been implemented and evaluated using both synthetic and real-life logs. © 2017 Elsevier Ltd","Abstraction level; End-to-end process flow; Merging logs; Multiple instances; Process mining","Data mining; Abstraction level; Complex relationships; Different granularities; End-to-end process; Granularity levels; Merging algorithms; Multiple instances; Process mining; Merging",2-s2.0-85028997810
"Murtagh F., Contreras P.","Algorithms for hierarchical clustering: an overview, II",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028764537&doi=10.1002%2fwidm.1219&partnerID=40&md5=f425612e27565fa95cd9cd053f0bf0f8","We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86–97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.",,"Conformal mapping; Data mining; Self organizing maps; Agglomerative hierarchical clustering; Density-based approaches; Efficient implementation; Grid-based clustering; Hier-archical clustering; Hierarchical clustering algorithms; Hierarchical Grid; Software environments; Clustering algorithms",2-s2.0-85028764537
"Golay J., Kanevski M.","Unsupervised feature selection based on the Morisita estimator of intrinsic dimension",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029774447&doi=10.1016%2fj.knosys.2017.08.009&partnerID=40&md5=d32594f39501922886be34ef278b7778","This paper deals with a new filter algorithm for selecting the smallest subset of features carrying all the information content of a dataset (i.e. for removing redundant features). It is an advanced version of the fractal dimension reduction technique, and it relies on the recently introduced Morisita estimator of Intrinsic Dimension (ID). Here, the ID is used to quantify dependencies between subsets of features, which allows the effective processing of highly non-linear data. The proposed algorithm is successfully tested on simulated and real world case studies. Different levels of sample size and noise are examined along with the variability of the results. In addition, a comprehensive procedure based on random forests shows that the data dimensionality is significantly reduced by the algorithm without loss of relevant information. And finally, comparisons with benchmark feature selection techniques demonstrate the promising performance of this new filter. © 2017 Elsevier B.V.","Data mining; Intrinsic dimension; Morisita index; Redundancy minimization; Unsupervised feature selection","Benchmarking; Data mining; Decision trees; Filtration; Fractal dimension; Data dimensionality; Dimension reduction techniques; Information contents; Intrinsic dimensions; Morisita index; Redundancy minimization; Selection techniques; Unsupervised feature selection; Feature extraction",2-s2.0-85029774447
"Salman M., Qaisar S., Qamar A.M.","Classification and legality analysis of bowling action in the game of cricket",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019682191&doi=10.1007%2fs10618-017-0511-4&partnerID=40&md5=9017c8495eb1dd013238019c67127781","One of the hot topics in modern era of cricket is to decide whether the bowling action of a bowler is legal or not. Because of the complex bio-mechanical movement of the bowling arm, it is not possible for the on-field umpire to declare a bowling action as legal or illegal. Inertial sensors are currently being used for activity recognition in cricket for the coaching of bowlers and detecting the legality of their moves, since a well trained and legal bowling action is highly significant for the career of a cricket player. After extensive analysis and research, we present a system to detect the legality of the bowling action based on real time multidimensional physiological data obtained from the inertial sensors mounted on the bowlers arm. We propose a method to examine the movement of the bowling arm in the correct rotation order with a precise angle. The system evaluates the bowling action using various action profiles. The action profiles are used so as to simplify the complex bio-mechanical movement of the bowling arm along with minimizing the size of the data provided to the classifier. The events of interest are identified and tagged. Algorithms such as support vector machines, k-nearest neighbor, Naïve Bayes, random forest, and artificial neural network are trained over statistical features extracted from the tagged data. To accomplish the reliability of outcome measures, the technical error of measurement was adopted. The proposed method achieves very high accuracy in the correct classification of bowling action. © 2017, The Author(s).","Activity recognition; Chucking; Classification; Cricket; Feature extraction; Inertial sensors; Machine learning","Biomechanics; Classification (of information); Complex networks; Decision trees; Feature extraction; Inertial navigation systems; Kinematics; Learning systems; Mechanics; Nearest neighbor search; Neural networks; Pattern recognition; Activity recognition; Chucking; Cricket; Inertial sensor; K-nearest neighbors; Mechanical movements; Physiological data; Statistical features; Data mining",2-s2.0-85019682191
"VanDam C., Kanthawala S., Pratt W., Chai J., Huh J.","Detecting clinically related content in online patient posts",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031277894&doi=10.1016%2fj.jbi.2017.09.015&partnerID=40&md5=b5e8d6acfc7bb7ed6e80a6ef42666d22","Patients with chronic health conditions use online health communities to seek support and information to help manage their condition. For clinically related topics, patients can benefit from getting opinions from clinical experts, and many are concerned about misinformation and biased information being spread online. However, a large volume of community posts makes it challenging for moderators and clinical experts, if there are any, to provide necessary information. Automatically identifying forum posts that need validated clinical resources can help online health communities efficiently manage content exchange. This automation can also assist patients in need of clinical expertise by getting proper help. We present our results on testing text classification models that efficiently and accurately identify community posts containing clinical topics. We annotated 1817 posts comprised of 4966 sentences of an existing online diabetes community. We found that our classifier performed the best (F-measure: 0.83, Precision: 0.79, Recall:0.86) when using Naïve Bayes algorithm, unigrams, bigrams, trigrams, and MetaMap Symantic Types. Training took 5 s. The classification process took a fraction of 1 s. We applied our classifier to another online diabetes community, and the results were: F-measure: 0.63, Precision: 0.57, Recall: 0.71. Our results show our model is feasible to scale to other forums on identifying posts containing clinical topic with common errors properly addressed. © 2017","Classification; Clinical topic; Diabetes; Health information seeking; Human-computer interaction; Online health communities; Patient; Text mining","Classification (of information); Health; Human computer interaction; Medical computing; Medical problems; Text processing; Clinical topic; Health informations; Online health communities; Patient; Text mining; Data mining; Article; Bayesian learning; classifier; diabetes mellitus; information retrieval; information seeking; measurement accuracy; measurement error; measurement precision; medical information; online system; priority journal",2-s2.0-85031277894
"Razmjoo A., Xanthopoulos P., Zheng Q.P.","Online feature importance ranking based on sensitivity analysis",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020189146&doi=10.1016%2fj.eswa.2017.05.016&partnerID=40&md5=e193b675a76f73a59ea10a18125a4a12","Online learning is a growing branch of data mining which allows all traditional data mining techniques to be applied on a online stream of data in real time. In this paper, we present a fast and efficient online sensitivity based feature ranking method (SFR) which is updated incrementally. We take advantage of the concept of global sensitivity and rank features based on their impact on the outcome of the classification model. In the feature selection part, we use a two-stage filtering method in order to first eliminate highly correlated and redundant features and then eliminate irrelevant features in the second stage. One important advantage of our algorithm is its generality, which means the method works for correlated feature spaces without preprocessing. It can be implemented along with any single-pass online classification method with separating hyperplane such as SVMs. The proposed method is primarily developed for online tasks, however, we achieve very significant experimental results in comparison with popular batch feature ranking/selection methods. We also perform experiments to compare the method with available online feature ranking methods. Empirical results suggest that our method can be successfully implemented in batch learning or online mode. © 2017 Elsevier Ltd","Feature ranking; Online learning; Sensitivity; Stochastic gradient descent","Data mining; Sensitivity analysis; Stochastic systems; Classification models; Feature ranking; On-line classification; Online learning; Redundant features; Sensitivity; Separating hyperplane; Stochastic gradient descent; E-learning",2-s2.0-85020189146
"Demaine E.D., Ganesan V., Kontsevoi V., Liu Q., Liu Q., Ma F., Nachum O., Sidford A., Waingarten E., Ziegler D.","Arboral satisfaction: Recognition and LP approximation",2017,"Information Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021146474&doi=10.1016%2fj.ipl.2017.04.012&partnerID=40&md5=a8701c8daa152cb867d401fe853ba7cc","A point set P is arborally satisfied if, for any pair of points with no shared coordinates, the box they span contains another point in P. At SODA 2009, Demaine, Harmon, Iacono, Kane, and Pǎtrascu proved a connection between the longstanding dynamic optimality conjecture about binary search trees and the problem of finding the minimum-size arborally satisfied superset of a given 2D point set [1]. We study two basic problems about arboral satisfaction. First, we develop two nontrivial algorithms to test whether a given point set is arborally satisfied. In 2D, both of our algorithms run in O(nlog⁡n) time, and one of them achieves O(n) runtime if the points are presorted; we also show a matching Ω(nlog⁡n) lower bound in the algebraic decision tree model. In d dimensions, our algorithm runs in O(dnlog⁡n+nlogd−1⁡n) time. Second, we study a natural integer linear programming formulation of finding the minimum-size arborally satisfied superset of a given 2D point set, which is equivalent to finding offline dynamically optimal binary search trees. Unfortunately, we conclude that the linear programming relaxation has large integrality gap, making it unlikely to find an approximation algorithm via this approach. © 2017","Algorithms; Analysis of algorithms; Computational geometry; Data structures","Algorithms; Binary trees; Bins; Computational geometry; Data mining; Data structures; Decision trees; Geometry; Integer programming; Analysis of algorithms; Binary search trees; Decision tree modeling; Dynamic optimality; Integrality gaps; Linear programming relaxation; Non-trivial algorithms; Optimal binary search trees; Approximation algorithms",2-s2.0-85021146474
"Shafee T., Masukume G., Kipersztok L., Das D., Häggström M., Heilman J.","Evolution of Wikipedia's medical content: Past, present and future",2017,"Journal of Epidemiology and Community Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031805103&doi=10.1136%2fjech-2016-208601&partnerID=40&md5=e7ed6fd7c3037dc8c6649916c74a4562","As one of the most commonly read online sources of medical information, Wikipedia is an influential public health platform. Its medical content, community, collaborations and challenges have been evolving since its creation in 2001, and engagement by the medical community is vital for ensuring its accuracy and completeness. Both the encyclopaedia's internal metrics as well as external assessments of its quality indicate that its articles are highly variable, but improving. Although content can be edited by anyone, medical articles are primarily written by a core group of medical professionals. Diverse collaborative ventures have enhanced medical article quality and reach, and opportunities for partnerships are more available than ever. Nevertheless, Wikipedia's medical content and community still face significant challenges, and a socioecological model is used to structure specific recommendations. We propose that the medical community should prioritise the accuracy of biomedical information in the world's most consulted encyclopaedia. © The author(s) 2017.",,"accuracy assessment; data interpretation; data mining; medical geography; public health; human",2-s2.0-85031805103
"Hewarathna A., Mozziconacci O., Nariya M.K., Kleindl P.A., Xiong J., Fisher A.C., Joshi S.B., Middaugh C.R., Forrest M.L., Volkin D.B., Deeds E.J., Schöneich C.","Chemical Stability of the Botanical Drug Substance Crofelemer: A Model System for Comparative Characterization of Complex Mixture Drugs",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028352488&doi=10.1016%2fj.xphs.2017.06.022&partnerID=40&md5=af7ccbf3d4544934d07a2f9237991ecf","As the second of a 3-part series of articles in this issue concerning the development of a mathematical model for comparative characterization of complex mixture drugs using crofelemer (CF) as a model compound, this work focuses on the evaluation of the chemical stability profile of CF. CF is a biopolymer containing a mixture of proanthocyanidin oligomers which are primarily composed of gallocatechin with a small contribution from catechin. CF extracted from drug product was subjected to molecular weight–based fractionation and thiolysis. Temperature stress and metal-catalyzed oxidation were selected for accelerated and forced degradation studies. Stressed CF samples were size fractionated, thiolyzed, and analyzed with a combination of negative-ion electrospray ionization mass spectrometry (ESI-MS) and reversed-phase-HPLC with UV absorption and fluorescence detection. We further analyzed the chemical stability data sets for various CF samples generated from reversed-phase-HPLC-UV and ESI-MS using data-mining and machine learning approaches. In particular, calculations based on mutual information of over 800,000 data points in the ESI-MS analytical data set revealed specific CF cleavage and degradation products that were differentially generated under specific storage/degradation conditions, which were not initially identified using traditional analysis of the ESI-MS results. © 2017 American Pharmacists Association®","chemical stability; complex mixture; crofelemer; HPLC; machine learning; mass spectrometry; mutual information scores; oxidation","crofelemer; Article; calculation; data mining; degradation; drug analysis; drug mixture; electrospray mass spectrometry; fluorescence imaging; fractionation; light absorption; machine learning; mathematical model; negative ion electrospray; reversed phase high performance liquid chromatography; temperature stress; thermostability",2-s2.0-85028352488
"Kleindl P.A., Xiong J., Hewarathna A., Mozziconacci O., Nariya M.K., Fisher A.C., Deeds E.J., Joshi S.B., Middaugh C.R., Schöneich C., Volkin D.B., Forrest M.L.","The Botanical Drug Substance Crofelemer as a Model System for Comparative Characterization of Complex Mixture Drugs",2017,"Journal of Pharmaceutical Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027546669&doi=10.1016%2fj.xphs.2017.07.012&partnerID=40&md5=10b25ba8da2f5c495bd8d28d02f6f775","Crofelemer is a botanical polymeric proanthocyanidin that inhibits chloride channel activity and is used clinically for treating HIV-associated secretory diarrhea. Crofelemer lots may exhibit significant physicochemical variation due to the natural source of the raw material. A variety of physical, chemical, and biological assays were used to identify potential critical quality attributes (CQAs) of crofelemer, which may be useful in characterizing differently sourced and processed drug products. Crofelemer drug substance was extracted from tablets of one commercial drug product lot, fractionated, and subjected to accelerated thermal degradation studies to produce derivative lots with variations in chemical and physical composition potentially representative of manufacturing and raw material variation. Liquid chromatography, UV absorbance spectroscopy, mass spectrometry, and nuclear magnetic resonance analysis revealed substantial changes in the composition of derivative lots. A chloride channel inhibition cell-based bioassay suggested that substantial changes in crofelemer composition did not necessarily result in major changes to bioactivity. In 2 companion papers, machine learning and data mining approaches were applied to the analytical and biological data sets presented herein, along with chemical stability data sets derived from forced degradation studies, to develop an integrated mathematical model that can identify CQAs which are most relevant in distinguishing between different populations of crofelemer. © 2017 American Pharmacists Association®","analytical characterization; biopharmaceuticals; circular dichroism; complex mixture; mass spectrometry; NMR spectroscopy; polymer chemical degradation; polymeric drugs; stability; UV-Visible spectroscopy","chloride channel; crofelemer; mytesi; Article; bioassay; biological activity; chemical composition; comparative study; controlled study; data mining; degradation; derivatization; drug manufacture; drug mixture; drug structure; fractionation; liquid chromatography; machine learning; mass spectrometry; mathematical model; nuclear magnetic resonance spectroscopy; physical chemistry; tablet formulation; ultraviolet spectroscopy",2-s2.0-85027546669
"Fernández-Ares A., Mora A.M., Arenas M.G., García-Sanchez P., Romero G., Rivas V., Castillo P.A., Merelo J.J.","Studying real traffic and mobility scenarios for a Smart City using a new monitoring and tracking system",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007463588&doi=10.1016%2fj.future.2016.11.021&partnerID=40&md5=fbe2a74014ba67bc9b90c0c2c957da14","This paper presents a novel mobility monitoring system and some of its applications to address problems that would be solved in a Smart City, such as the optimization of traffic flows in terms of trip-time and security (Smart Traffic), and the improvement of security or energetic issues inside buildings. The system tracks the movement of people and vehicles monitoring the radioelectric space, catching the WiFi and Bluetooth signals emitted by personal (smartphones) or on-board (hands-free) devices. A study has been conducted in four different real scenarios, i.e. with real data gathered by the system: two related with people's mobility (a public building and a discotheque); and two focused in traffic tracking (urban and intercity roads). The analysis has consisted on the application of different data mining techniques to extract useful knowledge, traffic forecasting methods to perform accurate predictions, and statistical analyses to model and validate the system reliability (comparing to other real data sources). The obtained results show the viability and utility of the system in all the cases, along with some of its multiple applications for solving different issues in a city. © 2016 Elsevier B.V.","Internet of Things; Mobility analysis; Smart City; Smart Traffic; Traffic forecast; Transit indicators","Data mining; Forecasting; Internet of things; Mobile security; Mobile telecommunication systems; Reliability analysis; Target tracking; Mobility analysis; Mobility monitoring systems; Monitoring and tracking; Multiple applications; Smart cities; Smart traffic; Traffic Forecasting; Traffic forecasts; Monitoring",2-s2.0-85007463588
"Doostan M., Chowdhury B.H.","Power distribution system fault cause analysis by using association rule mining",2017,"Electric Power Systems Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024865776&doi=10.1016%2fj.epsr.2017.07.005&partnerID=40&md5=ffed7ea0d5c27daf753fcbe7e538c534","In recent years, with the increasing requirements on power distribution utilities to ensure system reliability and to improve customers and regulators satisfaction, utilities seek to find practical solutions that enable them to restrict specific faults or to better manage their responses to unavoidable power outages. For achieving either, it is crucial to acquire a profound understanding of different faults by exploring their underlying causes and identifying key variables related to those causes. Currently, statistical models as well as advanced data analytics techniques are common tools to gain such understanding. Although basic statistical analysis provides a general knowledge of the primary causes of faults; nevertheless, it falls short of describing nuanced conditions that lead to a fault. On the other hand, applying sophisticated algorithms can produce deeper insight into the main causes; however, it would be computationally burdensome and might require a tremendous amount of running time. In order to overcome these problems, this paper proposes a novel approach for fault cause analysis by using association rule mining. The primary goals are to characterize faults according to their underlying causes and to identify important variables that strongly impact fault frequency. This paper proposes a step-by-step procedure, which deals with data preparation, practical issues associated with fault data sets, and implementation of association rule mining. The procedure is followed by a comprehensive case study to demonstrate how the proposed approach can be used to mine for causal structures and identify frequent patterns for vegetation, animal, equipment failure, public accident, and lightning-related faults. © 2017 Elsevier B.V.","Apriori algorithm; Association rule mining; Fault cause analysis; Machine learning; Pattern recognition; RandomForestSRC; ROSE; SMOTE; Synthetic data","Association rules; Customer satisfaction; Learning systems; Pattern recognition; Apriori algorithms; RandomForestSRC; ROSE; SMOTE; Synthetic data; Outages",2-s2.0-85024865776
"Alizadeh T., Grubesic T.H., Helderop E.","Urban governance and big corporations in the digital economy: An investigation of socio-spatial implications of Google Fiber in Kansas City",2017,"Telematics and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018975164&doi=10.1016%2fj.tele.2017.04.007&partnerID=40&md5=c769373548864533816878ccf0b726cf","In February 2010, Google challenged US cities to compete for being the site of its first attempt at building ultra-high-speed fiber-to-the-premises (FTTP) network, promising speeds up to one hundred times faster than pre-existing broadband services. More than 1100 cities applied. Kansas City, however, was announced as the winner of the competition. This paper explores the rollout of Google Fiber in Kansas City from three different perspectives. First, we provide a close examination of urban governance and the Fiber project – highlighting numerous regulatory concessions and incentives provided to Fiber during the construction phase. Second, we explore the ways in which pre-existing digital divides and socio-economic inequalities impacted the Fiber plan for Kansas City. Finally, in an effort to better understand the geographic intricacies of Fiber service, this paper uses a novel data mining technique and exploratory spatial data analysis to highlight the provision footprints for two counties in the Kansas City metropolitan area. We conclude with a discussion of the salient policy implications of projects like Fiber for urban governance, highlighting both the promises and stark realities of such ventures. © 2017 Elsevier Ltd","Broadband; Digital divide; Google Fiber; Kansas City; Spatial equity; Urban governance","Data mining; Economics; Fiber to the x; Public policy; Broadband; Digital divide; Kansas City; Spatial equity; Urban governance; Fibers",2-s2.0-85018975164
"Martinez A., Choi J.-H.","Exploring the potential use of building facade information to estimate energy performance",2017,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029009042&doi=10.1016%2fj.scs.2017.07.022&partnerID=40&md5=c53af51280a4e04c17092502491e6be5","In spite of prolific research on the energy performance of buildings in the last decades, and the growing focus on reducing their operational energy, buildings still prevail as the main end users of energy in the U.S. The goal of this research is to investigate the potential use of building facade information to estimate its energy performance, and to find significant facade attributes depending on different climate conditions in the U.S. This study adopted Energy Use Intensity (EUI) for total consumption and described building information, including window-wall ratio, orientation, aspect ratio, and other building components. Concentration was given to achieve a balanced data collection from best practices and green certified non-residential projects located in different climate conditions in the U.S. Data mining techniques, such as classification tree and statistical tools that included Analysis of Variance (ANOVA), a 2-sample t-test, and regression, were adopted for analysis of this group of buildings. It was found that there were common functional and technical features, as well as similar performances of existing buildings linked to these buildings’ energy consumption. These findings could not only inform the new design of facades, but facade retrofits could be strategically established based on lessons learned from real practice. © 2017 Elsevier Ltd","Best-practice buildings; Building energy performance; Data mining; Evidence-based design","Aspect ratio; Buildings; Data mining; Energy efficiency; Energy utilization; Facades; Statistical mechanics; Trees (mathematics); Best practices; Building energy performance; Classification trees; Energy performance of buildings; Energy use intensities; Evidence-based designs; Operational energy; Residential projects; Analysis of variance (ANOVA)",2-s2.0-85029009042
"Kabra G., Ramesh A., Akhtar P., Dash M.K.","Understanding behavioural intention to use information technology: Insights from humanitarian practitioners",2017,"Telematics and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020840838&doi=10.1016%2fj.tele.2017.05.010&partnerID=40&md5=6a5c08838b72e3947984fb0ed495d3bc","The contemporary research in the area of individual technology adoption mainly focuses on commercial supply chains. However, limited research focuses on the context of humanitarian supply chains. This calls to develop structural models that can scrutinize the technology adoption behaviour of the users in the humanitarian context. Therefore, this study is an attempt to empirically examine the technology adoption behaviour of humanitarian organizations. It extends the unified theory of the acceptance and use of technology (UTAUT) model by integrating personal innovativeness and trust in technology with the behavioural intention to adopt technology in the humanitarian context. Data from 192 humanitarian practitioners, who have experienced a large number of disasters, is utilized to empirically validate the conceptual model. The structural equation modelling results show that - out of four constructs namely performance expectancy, effort expectancy, social influence and facilitating conditions under UTAUT - performance expectancy and effort expectancy significantly affect the IT adoption. Contrary to expectations, trust and personal innovation do not affect the behavioural intention. Also, personal innovation does not moderate the relationship between performance expectancy and effort expectancy. This underlines the need to foster a learning culture within these organizations. The efforts made by involved humanitarian organizations may be directed towards improving the level of education, skills and facilitating them with other resources such as appropriate IT and data mining training, so that the technology adoption becomes an integral part of their daily activities. Finally, detailed implications for humanitarian organizations are discussed. © 2017","Humanitarian logistics; Humanitarian supply chain management; Information technology; Supply chain management; Technology adoption; Unified theory of acceptance and use of technology","Data mining; Information technology; Personnel training; Societies and institutions; Supply chain management; Behavioural intentions; Facilitating conditions; Humanitarian logistics; Personal innovativeness; Structural equation modelling; Technology adoption; Trust in technologies; Unified theory of acceptance and use of technology; Engineering education",2-s2.0-85020840838
"You H., Yang X.","Urban expansion in 30 megacities of China: categorizing the driving force profiles to inform the urbanization policy",2017,"Land Use Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028071394&doi=10.1016%2fj.landusepol.2017.06.020&partnerID=40&md5=6b8802b4c4ccbd6ae42cf82e49f250ec","Currently, land use policy makers in developing countries face the challenge to control urban expansion in a rational and sustainable manner. In this context, understanding the urban expansion process and its determinants is essential for designing land use planning and governance strategies. This paper presents a methodological framework based on data mining techniques for exploring the determinants of urban expansion. We employ the Random Forest regression to apportion the relative importance for 33 determinants of urban expansion across 30 megacities in China. Analysis is facilitated by a dataset of urban land information in period 1993–2012 (derived from time-series nighttime light imageries) as the independent variable, and an equivalent dataset of 33 determinants of four categories (economic, demographic, social and natural) as exploratory variables. Results show that the relative importance of determinants varies with cities but shows some similarities. Greatest contribution is observed for the economic determinants, followed by social determinants. Demographic and natural determinants have comparative relative importance. In general, economic, social, and demographic determinants have increasing contribution to urban expansion, but the natural determinants show declining influence on urban expansion with time. Four clusters are identified by a Self-Organizing Map among the 30 megacities, with respect to the driving forces profiles. Urban expansion in these four clusters is respectively dominated by demographic, social, natural, and economic determinants. Based on these profiles, the land use planners can revised the original ‘top-down’ land-driven development model and formulate more locality-oriented measures to control urban expansion in an orderly manner. © 2017 Elsevier Ltd","Data mining; Driving forces profiles; Self-organizing map; Spatial determinants; Urban expansion; Urbanization policy",,2-s2.0-85028071394
"de l'Etoile S., Behura S., Zopluoglu C.","Acoustic parameters of infant-directed singing in mothers of infants with down syndrome",2017,"Infant Behavior and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029535411&doi=10.1016%2fj.infbeh.2017.09.001&partnerID=40&md5=8ac6828bfe7c0a3de2b1f9d6efdda990","This study compared the acoustic parameters and degree of perceived warmth in two types of infant-directed (ID) songs − the lullaby and the playsong − between mothers of infants with Down syndrome (DS) and mothers of typically-developing (TD) infants. Participants included mothers of 15 DS infants and 15 TD infants between 3 and 9 months of age. Each mother's singing voice was digitally recorded while singing to her infant and subjected to feature extraction and data mining. Mothers of DS infants and TD infants sang both lullabies and playsongs with similar frequency. In comparison with mothers of TD infants, mothers of DS infants used a higher maximum pitch and more key changes during playsong. Mothers of DS infants also took more time to establish a rhythmic structure in their singing. These differences suggest mothers are sensitive to the attentional and arousal needs of their DS infants. Mothers of TD infants sang with a higher degree of perceived warmth which does not agree with previous observations of “forceful warmth” in mothers of DS infants. In comparison with lullaby, all mothers sang playsong with higher overall pitch and slower tempo. Playsongs were also distinguished by higher levels of spectral centroid properties related to emotional expressivity, as well as higher degrees of perceived warmth. These similarities help to define specific song types, and suggest that all mothers sing in an expressive manner that can modulate infant arousal, including mothers of DS infants. © 2017 Elsevier Inc.","Infant self-regulation; Infant-directed singing; Infants with down syndrome","acoustics; arousal; Article; attention; auditory discrimination; autoregulation; child behavior; child development; clinical article; controlled study; data mining; Down syndrome; emotionality; female; human; infant; male; mother child relation; outcome assessment; priority journal; singing; tactile stimulation; voice change",2-s2.0-85029535411
"Gu H., Ren S., Si F., Xu Z.","Evolved FCM framework for working condition classification in furnace system",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969794860&doi=10.1007%2fs00500-016-2184-0&partnerID=40&md5=63d97b7db5f2073a0c6e908cac14fcc0","In this paper, an evolved FCM-based clustering method combined with entropy theory is proposed to develop a working condition classification model for the furnace system in coal-fired power plants. To overcome the disadvantage in beforehand determination of clustering number in basic FCM method, Silhouette index is selected as a parameter to evaluate clustering number adaptively in the process. Each time the FCM runs, the selected Silhouette index evaluates the clustering results considering both close and separation degree. Six datasets from UCI machine learning repository are used to certify the effectiveness of the evolved FCM method. Furthermore, pressure sequences from a 300-MW boiler are then discussed as the industrial case study. Three kinds of entropy values, featured from pressure sequence in time–frequency domain, are obtained for further clustering analysis. The clustering results show the strong relationship between boiler’s load and pressure sequences in furnace system. This method can be considered a reference method for data mining in other fluctuating and time-varying sequences. © 2016, Springer-Verlag Berlin Heidelberg.","Entropy; Evolved FCM; Pressure sequence; Silhouette index","Artificial intelligence; Boilers; Cluster analysis; Clustering algorithms; Coal; Data mining; Entropy; Fossil fuel power plants; Furnaces; Learning systems; Classification models; Clustering analysis; Clustering results; Coal-fired power plant; Evolved FCM; Industrial case study; Silhouette indices; UCI machine learning repository; Frequency domain analysis",2-s2.0-84969794860
"Kanazaki M., Yoda H., Chiba K., Kitagawa K., Shimada T.","Design Methodology of a Hybrid Rocket-Powered Launch Vehicle for Suborbital Flight",2017,"Journal of Aerospace Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027881923&doi=10.1061%2f%28ASCE%29AS.1943-5525.0000778&partnerID=40&md5=452b93506b2fb27ba9d9f68f461a6263","A multidisciplinary design methodology for a launch vehicle (LV) with a hybrid rocket was proposed in order to obtain qualitative design knowledge. In this study, a propulsion system performance and aerodynamic performance were empirically evaluated using a theoretical approach, and flight evaluation was performed by solving equations of motion to evaluate the motion along the horizontal/vertical directions and rotation around the body axis. To demonstrate the applicability of the proposed method, two types of multiobjective design problems were solved using multiobjective evolutionary algorithms, and the results were visualized using the parallel coordinate plot, which is a data mining technique. The first design problem has two objective functions: the maximization of the highest altitude and the minimization of the total mass. In this case, the effect of the constraint related to the body length and diameter ratio was evaluated. The second design problem also had two objective functions and two constraints for a realistic LV design: the maximization of the downrange and the minimization of the total mass. The target altitude was different for these problems although the objective functions were identical. As a result, the trade-off information was successfully acquired for each design problem. It was also found that the body length and diameter ratio were key factors in deciding the maximum altitude. Furthermore, during the downrange maximization, the characteristic geometries observed in the obtained nondominated solutions revealed the design required to fulfill the constraint related to the target altitude. © 2017 American Society of Civil Engineers.","Design optimization; Hybrid rocket; Multiobjective evolutionary algorithm","Automobile bodies; Data mining; Economic and social effects; Equations of motion; Evolutionary algorithms; Function evaluation; Hybrid vehicles; Launch vehicles; Optimization; Propulsion; Rockets; Aero-dynamic performance; Design optimization; Hybrid rockets; Multi objective evolutionary algorithms; Multidisciplinary design methodology; Multiobjective design problems; Nondominated solutions; Parallel coordinate plots; Design",2-s2.0-85027881923
"Moura R., Beer M., Patelli E., Lewis J., Knoll F.","Learning from accidents: Interactions between human factors, technology and organisations as a central element to validate risk studies",2017,"Safety Science",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028743991&doi=10.1016%2fj.ssci.2017.05.001&partnerID=40&md5=e679faa4cea0656f3a94eca12838d1e5","Many industries are subjected to major hazards, which are of great concern to stakeholders groups. Accordingly, efforts to control these hazards and manage risks are increasingly made, supported by improved computational capabilities and the application of sophisticated safety and reliability models. Recent events, however, have revealed that apparently rare or seemingly unforeseen scenarios, involving complex interactions between human factors, technologies and organisations, are capable of triggering major catastrophes. The purpose of this work is to enhance stakeholders’ trust in risk management by developing a framework to verify if tendencies and patterns observed in major accidents were appropriately contemplated by risk studies. This paper first discusses the main accident theories underpinning major catastrophes. Then, an accident dataset containing contributing factors from major events occurred in high-technology industrial domains serves as basis for the application of a clustering and data mining technique (self-organising maps – SOM), allowing the exploration of accident information gathered from in-depth investigations. Results enabled the disclosure of common patterns in major accidents, leading to the development of an attribute list to validate risk assessment studies to ensure that the influence of human factors, technological issues and organisational aspects was properly taken into account. © 2017 Elsevier Ltd","Human factors; Learning from accidents; MATA-D; Organisations; Risk studies validation; Self-organising maps","Accidents; Computation theory; Data mining; Disasters; Hazards; Human engineering; Risk management; Self organizing maps; Societies and institutions; Computational capability; Contributing factor; High technology; Learning from accidents; Major accidents; Organisational aspects; Organisations; Reliability model; Risk assessment; accident; algorithm; Article; artificial neural network; biotechnology; controlled study; cost effectiveness analysis; data mining; human; priority journal; reliability; risk assessment; risk management; safety; self organising maps; validation study",2-s2.0-85028743991
"Yu Z.J., Hu B., Sun Y., Li A., Li J., Zhang G.","Standby energy use and saving potentials associated with occupant behavior of chinese rural homes",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028722881&doi=10.1016%2fj.enbuild.2017.08.070&partnerID=40&md5=8d485c5ed4546a3f2f091b3c033a261a","A deep understanding of household appliance standby energy use (SEU) and associated saving potentials in Chinese rural areas is vitally important due to the huge amounts of households and appliances. However, to date, few study has been conducted and very little is known about the SEU. Meanwhile, existing methods of estimating such saving potentials have two limitations. Firstly, they primarily evaluated it through upgrading appliances while a more practical way of improving behavior should also be considered. Secondly, they do not take the impact of its influential factors into account simultaneously, and tend to significantly decrease the estimation accuracy. To address the above issues, an investigation on the appliance SEU and its influential factors in Chinese rural homes was first conducted. Then, a data mining-based method for estimating the saving potentials was proposed, and improving occupant behavior pertaining to SEU was also considered in the estimation. The method was further applied under three typical scenarios: appliance upgrades, behavior change and the combination of both. In addition, barriers to promote standby energy reductions have been analyzed and corresponding recommendations have been made. Main contributions of this study include: (1) providing SEU of Chinese rural homes and estimating saving potentials caused by various ways including behavior change and, (2) a new method of estimating energy-saving potentials with improved accuracy. © 2017 Elsevier B.V.","Data mining; Energy-saving potential; Household appliance; Occupant behavior; Standby energy use","Data mining; Domestic appliances; Energy conservation; Rural areas; Behavior change; Energy reduction; Energy saving potential; Energy use; Influential factors; Occupant behavior; Saving potentials; Equipment",2-s2.0-85028722881
"Kumar M., Bhatia R., Rattan D.","A survey of Web crawlers for information retrieval",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031399465&doi=10.1002%2fwidm.1218&partnerID=40&md5=a79545a78f7eacb714775dbcdf7c432f","Performance of any search engine relies heavily on its Web crawler. Web crawlers are the programs that get webpages from the Web by following hyperlinks. These webpages are indexed by a search engine and can be retrieved by a user query. In the area of Web crawling, we still lack an exhaustive study that covers all crawling techniques. This study follows the guidelines of systematic literature review and applies it to the field of Web crawling. We used the standard procedure of carrying out a systematic literature review on 248 studies from a total of 1488 articles published in 12 leading journals and other premier conferences and workshops. Existing literature about the Web crawler is classified into different key subareas. Each subarea is further divided according to the techniques being used. We analyzed the distribution of various articles using multiple criteria and depicted conclusions. Various studies that use open source Web crawlers are also reported. We have highlighted future areas of research. We call for an increased awareness in various fields of the Web crawler and identify how techniques from other domains can be used for crawling the Web. Limitations and recommendations for future are also discussed. WIREs Data Mining Knowl Discov 2017, 7:e1218. doi: 10.1002/widm.1218. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.",,"Data mining; Hypertext systems; Search engines; Websites; Hyperlinks; Multiple criteria; Open sources; Standard procedures; Sub-areas; Systematic literature review; User query; Web Crawling; Web crawler",2-s2.0-85031399465
"Yu D., Fang C.","The dynamics of public safety in cities: A case study of Shanghai from 2010 to 2025",2017,"Habitat International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030127917&doi=10.1016%2fj.habitatint.2017.09.007&partnerID=40&md5=49447e5f329ffe9cd2d2970af4d440e6","Cities in China are facing increasing challenges. Urban public safety concerns including urban crime, urban livability and urban disasters start to attract governmental, academic as well as public attention. Applying a system dynamics modeling scheme, this research investigates and attempts to simulate the public safety dynamics of Shanghai with a set of collected indicators that describes Shanghai's infrastructure and development, population, crime, livability and disaster during the past decade (2000–2009). The feedback loops are constructed based on exploratory data mining through regular statistical analyses and grey system simulation. The analytical results suggest Shanghai's public safety is increasing due to a high level of urban socioeconomic development, which provides a foundation for urban public safety. In the meantime, factors that ‘expend’ such foundation (crimes and disasters) increased at a relatively lower level. Dynamic simulation on Shanghai's public safety suggests that the city could still enjoy its continuous improvement of public safety providing the city continues to develop like in the past decade, which might not be the case in the long run. A few scenarios are presented by altering a few critical variables to demonstrate potential public safety dynamics of Shanghai in the next 15 years. © 2017 Elsevier Ltd","Grey system simulation; Prediction; Public safety; Shanghai; System dynamics","crime; data mining; disaster management; safety; socioeconomic conditions; urban economy; urban planning; urban society; China; Shanghai",2-s2.0-85030127917
"Antanasijević D., Pocajt V., Ristić M., Perić-Grujić A.","A differential multi-criteria analysis for the assessment of sustainability performance of European countries: Beyond country ranking",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028047176&doi=10.1016%2fj.jclepro.2017.07.131&partnerID=40&md5=04429555ee637804d2acd4d226c7e5d6","Sustainable development as a concept that aims to enhance the quality of life without affecting environment has been in focus in European policies since the last decade of 20th century. The objectives of EU Sustainable Development Strategy (EU SDS) are grouped into ten thematic areas, where almost each theme has at least one headline and several operational sustainable development indicators (SDIs). A large number of SDIs is needed to evaluate all EU SDS goals, which imposes the use of multivariate data mining techniques. This paper presents an empirical study carried out to assess the sustainability performance of European countries using the differential multi criteria analysis (DMCA) technique. The PROMETHEE (Preference Ranking Organization MeTHod for Enrichment Evaluations) was applied on 38 headline and operational SDIs defined under the EU SDS. This DMCA was applied to 30 European countries over a 10-year period (2004–2014) with the aim to determine the theme specific, as well as, overall sustainability progress. The DMCA reveals that the majority of European countries have made progress in sustainability in the studied period, where Czech Republic, Germany, Hungary and Sweden, have enhanced their sustainability performance concerning all themes. There are only two countries, namely Greece and Ireland, which have not made overall progress in this period. Also, above the average progress has been made concerning social inclusion, sustainable transport, and climate change and energy, while in all other themes additional efforts should be made in order to ensure sustainable performance progress in future years. Although the progress in reducing uneven development between EU members has been made, after 2009 a positive trend can be observed only in a limited number of SDS themes. © 2017 Elsevier Ltd","Multi-criteria analysis; PROMETHEE; Sustainability performance measuring; Sustainable development indicators","Climate change; Data mining; Planning; Enrichment evaluation; European Countries; Multi Criteria Analysis; PROMETHEE; Sustainability performance; Sustainable development indicators; Sustainable performance; Sustainable transport; Sustainable development",2-s2.0-85028047176
"Jadooki S., Mohamad D., Saba T., Almazyad A.S., Rehman A.","Fused features mining for depth-based hand gesture recognition to classify blind human communication",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960113763&doi=10.1007%2fs00521-016-2244-5&partnerID=40&md5=6fc82f4f1895e6aa4ef25122271764fe","Gesture recognition and hand pose tracking are applicable techniques in human–computer interaction fields. Depth data obtained by depth cameras present a very informative explanation of the body or in particular hand pose that it can be used for more accurate gesture recognition systems. The hand detection and feature extraction process are very challenging task in the RGB images that they can be effectively dissolved with simple ways with depth data. However, depth data could be combined with the color information for more reliable recognition. A common hand gesture recognition system requires identifying the hand and its position or direction, extracting some useful features and applying a suitable machine-learning method to detect the performed gesture. This paper presents the novel fusion of the enhanced features for the classification of static signs of the sign language. It begins by explaining how the hand can be separated from the scene by depth data. Then, a combination feature extraction method is introduced for extracting some appropriate features of the images. Finally, an artificial neural network classifier is trained with these fused features and applied to critically analyze various descriptors performance. © 2016, The Natural Computing Applications Forum.","DCT; Depth data; Fused features mining; Hand gesture recognition; Moment invariant","Artificial intelligence; Classification (of information); Extraction; Feature extraction; Human computer interaction; Image processing; Learning systems; Neural networks; Palmprint recognition; Artificial neural network classifiers; Computer interaction; Depth data; Feature extraction methods; Gesture recognition system; Hand-gesture recognition; Machine learning methods; Moment invariant; Gesture recognition",2-s2.0-84960113763
"Hurley R.R., Rothwell J.J., Woodward J.C.","Metal contamination of bed sediments in the Irwell and Upper Mersey catchments, northwest England: exploring the legacy of industry and urban growth",2017,"Journal of Soils and Sediments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012930254&doi=10.1007%2fs11368-017-1668-6&partnerID=40&md5=98db79d2b9f927565eb078b8004a2fcb","Purpose: Manchester is often heralded as the first industrial city. Large volumes of physical and liquid contaminants were released into its river network throughout the industrial period up to the latter part of the twentieth century. Water quality has improved dramatically in recent decades, but, given their environmental significance, it is important to ascertain the extent to which a legacy of contamination persists in the modern bed sediments. Materials and methods: Fine-grained bed sediments were sampled at 40 sites in the Mersey and Irwell catchments. Sediments were wet sieved to isolate the &lt;63-μm grain size fraction. Metal concentrations were determined using XRF. Particle size characteristics were also measured. Sediments were subjected to a five-step sequential extraction procedure to ascertain the environmental significance of metal concentrations. Alongside archival research of past industry, enrichment factors, multivariate statistical techniques and conditional inferences trees were used to identify sources of heavy metals. Results and discussion: Bed sediment-associated heavy metal(loid) concentrations were as follows: As (9.89–110 mg kg−1), Cr (76.5–413 mg kg−1), Cu (53.1–383 mg kg−1), Pb (80.4–442 mg kg−1) and Zn (282–1020 mg kg−1). Enrichment factors ranged from moderate to extremely severe, with Pb showing the greatest enrichment across the catchments. Chemical mobility was generally low, but metal(loid) partitioning identified the influence of anthropogenic sources. Statistical analysis highlighted a number of point sources associated with former industrial sites that operated during the industrial period. Conditional inference trees highlighted the role of the textile industry on Cu concentrations in addition to indicating the complexity of sources, fluxes and stores of sediment-associated contamination throughout the system. Conclusions: Fine-grained sediment-associated metal(loid)s in the Mersey and Irwell catchments are anthropogenically enriched. Concentrations also exceed sediment quality guidelines. A lack of distinct spatial patterning points to a complex network of contaminant inputs across the catchments, even in the headwaters. Whilst potential modern urban sources are likely to be important, spatial patterns and multivariate/data mining techniques also highlighted the importance of releases from former industrial sites as well as the reworking of historically contaminated floodplains and soils. © 2017, The Author(s).","Bed sediment; Enrichment factor; Heavy metals; Historical contamination; Sequential extraction","anthropogenic source; data mining; grain size; heavy metal; pollutant transport; sediment pollution; sequential extraction; textile industry; urban growth; England; Manchester [England]; United Kingdom",2-s2.0-85012930254
"Yan W., Zhang B., Ma S., Yang Z.","A novel regularized concept factorization for document clustering",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027585963&doi=10.1016%2fj.knosys.2017.08.010&partnerID=40&md5=6ffd747f890cf63f87d851194719444c","Document clustering is an important tool for text mining with its goal in grouping similar documents into a single cluster. As typical clustering methods, Concept Factorization (CF) and its variants have gained attention in recent studies. To improve the clustering performance, most of the CF methods use additional supervisory information to guide the clustering process. When the amount of supervisory information is scarce, the improved performance of CF methods will be limited. To overcome this limitation, this paper proposes a novel regularized concept factorization (RCF) algorithm with dual connected constraints, which focuses on whether two documents belong to the same class (must-connected constraint) or different classes (cannot-connected constraint). RCF propagates the limited constraint information from constrained samples to unconstrained samples, allowing the collection of constraint information from the entire data set. This information is used to construct a new data similarity matrix that concentrates on the local discriminative structure of data. The similarity matrix is incorporated as a regularization term in the CF objective function. By doing so, RCF is able to make full use of the supervisory information to preserve the local structure of the data set. Thus, the clustering performance will be improved significantly. Our experiments on standard document databases demonstrate the effectiveness of the proposed method. © 2017","Concept factorization; Document clustering; Manifold regularization","Cluster analysis; Clustering algorithms; Data mining; Factorization; Information retrieval; Clustering methods; Clustering process; Constraint information; Document Clustering; Manifold regularizations; Objective functions; Regularization terms; Standard documents; Matrix algebra",2-s2.0-85027585963
"Gubiani D., Fabbretti E., Cestnik B., Lavrač N., Urbančič T.","Outlier based literature exploration for cross-domain linking of Alzheimer's disease and gut microbiota",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019648050&doi=10.1016%2fj.eswa.2017.05.026&partnerID=40&md5=6797a38a0967c2b33a0e209edae95ff6","In knowledge discovery, experts frequently need to combine knowledge from different domains to get new insights and derive new conclusions. Intelligent systems should support the experts in the search for relationships between concepts from different domains, where huge amounts of possible combinations require the systems to be efficient but also sufficiently general, open and interactive to enable the experts to creatively guide the discovery process. The paper proposes a cross-domain literature mining methodology that achieves this functionality by combining the functionality of two complementary text mining tools: clustering and topic ontology creation tool OntoGen and cross-domain bridging terms exploration tool CrossBee. Focusing on outlier documents identified by OntoGen contributes to the efficiency, while CrossBee allows for flexible and user-friendly bridging concepts exploration and identification. The proposed approach, which is domain independent and can support cross-domain knowledge discovery in any field of science, is illustrated on a biomedical case study dealing with Alzheimer's disease, one of the most threatening age-related diseases, deteriorating lives of numerous individuals and challenging the ageing society as a whole. By applying the proposed methodology to Alzheimer's disease and gut microbiota PubMed articles, we have identified Nitric oxide synthase (NOS) as a potentially valuable link between these two domains. The results support the hypothesis of neuroinflammatory nature of Alzheimer's disease, and is indicative for the quest for identifying strategies to control nitric oxide-associated pathways in the periphery and in the brain. By addressing common mediators of inflammation using literature-based discovery, we have succeeded to uncover previously unidentified molecular links between Alzheimer's disease and gut microbiota with a multi-target therapeutic potential. © 2017 Elsevier Ltd","Alzheimer's disease; Gut microbiome; Literature-based discovery; Outlier detection","Data mining; Disease control; Intelligent systems; Nitric oxide; Statistics; Age-related disease; Alzheimer's disease; Domain independents; Literature-based discoveries; Microbiome; Nitric-oxide synthase; Outlier Detection; Therapeutic potentials; Neurodegenerative diseases",2-s2.0-85019648050
"Moura R., Beer M., Patelli E., Lewis J.","Learning from major accidents: Graphical representation and analysis of multi-attribute events to enhance risk communication",2017,"Safety Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015390337&doi=10.1016%2fj.ssci.2017.03.005&partnerID=40&md5=9504d560357f7e54c1f1dd95d1f8b65a","Major accidents are complex, multi-attribute events, originated from the interactions between intricate systems, cutting-edge technologies and human factors. Usually, these interactions trigger very particular accident sequences, which are hard to predict but capable of producing exacerbated societal reactions and impair communication channels among stakeholders. Thus, the purpose of this work is to convert high-dimensional accident data into a convenient graphical alternative, in order to overcome barriers to communicate risk and enable stakeholders to fully understand and learn from major accidents. This paper first discusses contemporary views and biases related to human errors in major accidents. The second part applies an artificial neural network approach to a major accident dataset, to disclose common patterns and significant features. The complex data will be then translated into 2-D maps, generating graphical interfaces which will produce further insight into the conditions leading to accidents and support a novel and comprehensive “learning from accidents” experience. © 2017 Elsevier Ltd","Accident analysis; Human factors; Learning from accidents; MATA-D; Self-organising maps","Complex networks; Deep neural networks; Human engineering; Neural networks; Risk assessment; Self organizing maps; Accident analysis; Artificial neural network approach; Cutting edge technology; Graphical interface; Graphical representations; Intricate systems; Learning from accidents; Risk communication; Accidents; accident prevention; Article; artificial neural network; cluster analysis; computer interface; data mining; information processing; interpersonal communication; priority journal; risk assessment",2-s2.0-85015390337
"Vista A., Care E., Awwal N.","Visualising and examining sequential actions as behavioural paths that can be interpreted as markers of complex behaviours",2017,"Computers in Human Behavior",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009725327&doi=10.1016%2fj.chb.2017.01.027&partnerID=40&md5=1e90a79f5e019ee1ad03d555c72d6f20","Visualisation of pathways taken by students through problem solving environments provides the potential to identify patterns of exploratory behaviour that may relate to different levels of proficiency or different cognitive approaches. This is particularly the case when problems are situated in complex online environments and are programmed to generate large scale data for analysis. Using process data from an online collaborative problem solving task, we visualised behavioural paths of 607 pairs of students as directed graphs. The empirical paths were then examined using exploratory network analysis based on four main aspects of exploration (prominence, branches, clusters, and shortest paths). The primary purpose of such analysis is to detect sequences that are potentially relevant for establishing particular paths as meaningful markers of complex behaviours. This visualisation approach enabled us to capture all actual (as opposed to possible) pathways from the data. Although there may be an optimal pathway through a complex task according to criteria such as efficiency or correct solution, the added human factors bring a dynamic to the activity that provides a richer environment for understanding students’ collaborative processes, both cognitive and social. The method adopted in this study provides a prototype for exploration of other complex skillsets. © 2017 Elsevier Ltd","Action sequence; Behavioural path; Complex task; Data mining; Network vizualisation; Problem solving","Complex networks; Data mining; Directed graphs; Distributed computer systems; Graph theory; Students; Visualization; Action sequences; Behavioural path; Collaborative problem solving; Collaborative process; Complex task; Exploratory behaviours; Problem solving environments; Vizualisation; Problem solving; behavior; data mining; executive function test; exploratory research; genetic marker; human; major clinical study; student",2-s2.0-85009725327
"Higueras P., Esbrí J.M., García-Ordiales E., González-Corrochano B., López-Berdonces M.A., García-Noguero E.M., Alonso-Azcárate J., Martínez-Coronado A.","Potentially harmful elements in soils and holm-oak trees (Quercus ilex L.) growing in mining sites at the Valle de Alcudia Pb-Zn district (Spain)–Some clues on plant metal uptake",2017,"Journal of Geochemical Exploration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997771340&doi=10.1016%2fj.gexplo.2016.07.017&partnerID=40&md5=91d82ab97f52f561b1eada5683532a1d","This work is aimed to the establishment of some clues concerning metal uptake by the holm oak (Quercus ilex L.) in abandoned base metals (Pb-Zn) mining sites from central Spain. The study is based on total contents of potentially harmful elements (PHEs), including Pb, Zn, Cu, As, Sb, Cd and Hg, in leaves from holm-oak trees as well as in the corresponding soils, in three different sites included in the Alcudia Valley Mining District. Besides, we present other analytical data for the corresponding soils: pH; electrical conductivity; organic matter; and selective extractions: i) Modified Geological Survey Field Leach Test (using deionized water); extraction with ammonium acetate solution at two different pH values; EPA Method 1312 (using a H2SO4:HNO3 solution); and extraction with an EDTA solution. This is aimed to search for relationships between PHE concentrations in the leaves and the different parameters measured in the corresponding soil, in order to determine bioavailability of PHE in this type of environments. The results show areas impacted by different degrees of PHE; in the most polluted area Pb concentrations reach 300 times the regional baseline and background values indicated by World Health Organization. Also, detected concentrations of the elements in leaves show that the holm-oak tree is a species with a very low bioaccumulation capacity, as shown by a selective uptake of PHE, favouring Zn, Cd and Cu with respect to Pb and As, and with Hg as a peculiar case, influenced by foliar uptake. However, metal concentrations in leaves reach levels above published toxicity levels thresholds, in particular for Pb and Zn. After this study, the soil to plant transfer capacity depends on different parameters for each element, being extremely difficult to generalise a common scenario favouring or preventing this process. Instead, we have found a complex case, conditioned by the soil proprieties (reactivity, salt contents and presence and concentration of organic matter) and to lesser extent, by PHE speciation in the soils. © 2016 Elsevier B.V.","Alcudia valley mining district; Base metals; Holm-oak; Mining; Potentially harmful elements; Soil quality; Soil-plant transfer; Spain","Biochemistry; Biogeochemistry; Biological materials; Chemicals removal (water treatment); Deionized water; Extraction; Forestry; Metals; Mining; Organic compounds; Plants (botany); Soil pollution; Soils; Zinc; Base metals; Harmful elements; Holm-oak; Mining district; Soil quality; Soil-plant transfer; Spain; Pollution; base metal; biological uptake; evergreen tree; lead; mine; mining; soil quality; valley; zinc; Alcudia Valley; Castilla-La Mancha; Ciudad Real [Castilla-La Mancha]; Spain; Quercus ilex",2-s2.0-84997771340
"Chen Y., Liu Y., Zhang M., Ma S.","User Satisfaction Prediction with Mouse Movement Information in Heterogeneous Search Environment",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028467983&doi=10.1109%2fTKDE.2017.2739151&partnerID=40&md5=5daf7dca8c50e4174c468beeb33637fd","Satisfaction prediction is one of the prime concerns in search performance evaluation. It is a non-trivial task for three major reasons: (1) The definition of satisfaction is subjective and different users may have different opinions in the process of satisfaction judgment. (2) Most existing studies on satisfaction prediction mainly rely on users' click-through or query reformulation behaviors but there are many sessions without such interactions. (3) Most existing works primarily rely on the hypothesis that all results on search result pages (SERPs) are homogeneous, but a variety of heterogeneous search results have been aggregated into SERPs to improve the diversity and quality of search results recently. To shed light on these research questions, we construct an experimental search engine that could collect users' satisfaction feedback as well as mouse click-through/movement data. Inspired by recent studies in predicting search result relevance based on mouse movement patterns (namely, motifs), we propose to estimate search satisfaction with motifs extracted from mouse movement data on SERPs. Besides the existing frequency-based motif selection method, two novel selection strategies (distance-based and distribution-based) are also adopted to extract high-quality motifs for satisfaction prediction. Experimental results show that the proposed strategies outperform existing methods and have promising generalization capability for unseen users and queries in both a homogeneous and heterogeneous search environment. © 2017 IEEE.","federated search; mouse movement; prediction; Search satisfaction; user behavior","Behavioral research; Data mining; Feature extraction; Forecasting; Mammals; Federated Search; Metasearch; Mice; Mouse movements; Performance evaluation; Search problem; Search Satisfaction; User behaviors; Search engines",2-s2.0-85028467983
"van Bommel M., Bornn L.","Adjusting for scorekeeper bias in NBA box scores",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014000031&doi=10.1007%2fs10618-017-0497-y&partnerID=40&md5=fe220fdea5b6eccd9143896c109137d3","Box score statistics in the National Basketball Association are used to measure and evaluate player performance. Some of these statistics are subjective in nature and since box score statistics are recorded by scorekeepers hired by the home team for each game, there exists potential for inconsistency and bias. These inconsistencies can have far reaching consequences, particularly with the rise in popularity of daily fantasy sports. Using box score data, we estimate models able to quantify both the bias and the generosity of each scorekeeper for two of the most subjective statistics: assists and blocks. We then use optical player tracking data for the 2015–2016 season to improve the assist model by including other contextual spatio-temporal variables such as time of possession, player locations, and distance traveled. From this model, we present results measuring the impact of the scorekeeper and of the other contextual variables on the probability of a pass being recorded as an assist. Results for adjusting season assist totals to remove scorekeeper influence are also presented. © 2017, The Author(s).","Adjusted box score; Basketball; Fantasy sports; Optical tracking; Scorekeeper bias","Computer applications; Data mining; Adjusted box score; Basketball; Contextual variables; Estimate model; National basketball associations; Optical tracking; Scorekeeper bias; Spatio-temporal variables; Sports",2-s2.0-85014000031
"Tavalaei J., Habibuddin M.H., Khairuddin A., Mohd Zin A.A.","Fault location and classification of combined transmission system: Economical and accurate statistic programming framework",2017,"Journal of Electrical Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032879631&doi=10.5370%2fJEET.2017.12.6.2106&partnerID=40&md5=8d35e5c52478c40ec8cb2ca24072c6e3","An effective statistical feature extraction approach of data sampling of fault in the combined transmission system is presented in this paper. The proposed algorithm leads to high accuracy at minimum cost to predict fault location and fault type classification. This algorithm requires impedance measurement data from one end of the transmission line. Modal decomposition is used to extract positive sequence impedance. Then, the fault signal is decomposed by using discrete wavelet transform. Statistical sampling is used to extract appropriate fault features as benchmark of decomposed signal to train classifier. Support Vector Machine (SVM) is used to illustrate the performance of statistical sampling performance. The overall time of sampling is not exceeding 1¼ cycles, taking into account the interval time. The proposed method takes two steps of sampling. The first step takes ¾ cycle of during-fault and the second step takes ¼ cycle of post fault impedance. The interval time between the two steps is assumed to be ¼ cycle. Extensive studies using MATLAB software show accurate fault location estimation and fault type classification of the proposed method. The classifier result is presented and compared with well-established travelling wave methods and the performance of the algorithms are analyzed and discussed. © The Korean Institute of Electrical Engineers.","Combined transmission line; Fault location; Feature extraction; SVM; Wavelet transform","Discrete wavelet transforms; Electric fault location; Electric lines; Extraction; Feature extraction; Location; MATLAB; Sampling; Signal processing; Support vector machines; Wavelet transforms; Fault location estimation; Impedance measurement; Modal decomposition; Programming framework; Statistical feature extractions; Statistical sampling; Transmission systems; Travelling wave methods; Data mining",2-s2.0-85032879631
"Xie X., Ge S., Hu F., Xie M., Jiang N.","An improved algorithm for sentiment analysis based on maximum entropy",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032819593&doi=10.1007%2fs00500-017-2904-0&partnerID=40&md5=bda37d034a4c2b6be062f1b5fa91d65f","Sentiment analysis is an important field of study in natural language processing. In the massive data and irregular data, sentiment classification with high accuracy is a major challenge in sentiment analysis. To address this problem, a novel maximum entropy-PLSA model is proposed. In this model, we first use the probabilistic latent semantic analysis to extract the seed emotion words from the Wikipedia and the training corpus. Then features are extracted from these seed emotion words, which are the input of the maximum entropy model for training the maximum entropy model. The test set is processed similarly into the maximum entropy model for emotional classification. Meanwhile, the training set and the test set are divided by the K-fold method. The maximum entropy classification based on probabilistic latent semantic analysis uses important emotional classification features to classify words, such as the relevance of words and parts of speech in the context, the relevance with degree adverbs, the similarity with the benchmark emotional words and so on. The experiments prove that the classification method proposed by this paper outperforms the compared methods. © 2017 Springer-Verlag GmbH Germany","Maximum entropy; Probabilistic latent semantic analysis; Semantic analysis","Classification (of information); Data mining; Entropy; Natural language processing systems; Semantics; Syntactics; Websites; Classification methods; Emotional classification; Maximum entropy modeling; Parts of speech; Probabilistic latent semantic analysis; Semantic analysis; Sentiment analysis; Sentiment classification; Maximum entropy methods",2-s2.0-85032819593
"Palmer M., Tolosa B., Grau A.M., Gil M.D.M., Obregón C., Morales-Nin B.","Combining sale records of landings and fishers knowledge for predicting métiers in a small-scale, multi-gear, multispecies fishery",2017,"Fisheries Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022052722&doi=10.1016%2fj.fishres.2017.07.001&partnerID=40&md5=2d90e4c4a72b7b07987836ea32c8208b","Stock management should be guided by assessment models that, among others, need to be fed by reliable data of catch and effort. However, precise data are difficult to obtain in heterogeneous fisheries. Specifically, small-scale, multi-gear, multispecies fisheries are dynamic systems where fishers may lively change fishing strategy (i.e., métier) conditioned by multiple drivers. Provided that some stocks can be shared by several métiers, a precise categorization of métiers should be the first step toward métier-specific estimates of catch and effort, which in turn would allow a better understanding of the system dynamics. Here we propose an approach for predicting the métier of any given fishing trip from its landing records. This approach combines the knowledge of expert fishers with the existing sales register of landings in Mallorca (Western Mediterranean). It successfully predicts métiers for all the 162,815 small-scale fishery fishing trips from Mallorca between 2004 and 2015. The largest effort is invested in the métiers Cuttlefish/Fish and Spiny lobster, landings peak for Cuttlefish/Fish and Dolphinfish and revenues for Spiny lobster and Dolphinfish. Métier predictions also allowed us to describe the temporal (seasonal and between-year) trends experienced by each métier and to characterize the species (commercial categories) that are specific to each métier. Seasonal variability is by far more relevant than between-year variability, which confirms that at least some fishers are adopting a rotation cycle of métiers along the year. Effort (fishing trips), landings and gross revenues decreased in the last 12 years (2004–2015). The approach proposed is also applicable to any other fishery for which the métier for a fishing trip sample is known (e.g., on-board observers or logbooks), but relying on fishers expertise points more directly to fishers’ intention. Thus, métier predictions produced with the proposed approach are closer to the actual uses of fishers, providing better grounds for an improved management. © 2017","Classification algorithms; Data-mining; Mallorca (Western Mediterranean); Métier; Small-scale fishery",,2-s2.0-85022052722
"Datta D., Singh S.K., Chowdary C.R.","Bridging the gap: effect of text query reformulation in multimodal retrieval",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008208878&doi=10.1007%2fs11042-016-4262-9&partnerID=40&md5=388d0e79cbf9b9f544830850b62619d0","Multimodal Retrieval provides new paradigms and methods aimed at effectively searching through the enormous volume of data. Multimodal retrieval is a well studied problem often used in image retrieval. Most of the existing works in image retrieval under the pretext of multimodality stress on bridging the semantic gap by using both textual and visual features. In this paper, we use relevance feedback from the user-generated documents associated with the images for expanding textual query and study its effect on both image and text retrieval. We employ a topic decomposition based keyphrase extraction technique to expand the textual queries. Our results articulate the fact that an insightful textual query expansion always improves retrieval performance for both textual or image retrieval. Also, we adopt optimum weight learning scheme to combine the modalities in a privileged way. We perform a comparative study with two well established keyphrase extraction techniques which are used for textual query expansion. A detailed set of experiments on a standard real world dataset is also carried out for the same. © 2016, Springer Science+Business Media New York.","Fisher-LDA; Keyphrase extraction; Multimodal retrieval; Query reformulation; Topic model; Weight learning","Data mining; Extraction; Image retrieval; Query processing; Semantics; Fisher-LDA; Keyphrase extraction; Multi-modal; Query reformulation; Topic Modeling; Weight learning; Information retrieval",2-s2.0-85008208878
"Tayal D.K., Yadav S.K.","Sentiment analysis on social campaign “Swachh Bharat Abhiyan” using unigram method",2017,"AI and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982182197&doi=10.1007%2fs00146-016-0672-5&partnerID=40&md5=a04b83d136777dc8ae2d9505b8abe1ee","Sentiment analysis is the field of natural language processing to analyze opinionated data, for the purpose of decision making. An opinion is a statement about a subject which expresses the sentiments as well as the emotions of the opinion makers on the topic. In this paper, we develop a sentiment analysis tool namely SENTI-METER. This tool estimates the success rate of social campaigns based on the algorithms we developed that analyze the sentiment of word as well as blog. Social campaigns have a huge impact on the mindset of people. One such campaign was launched in India on October 2, 2014, named Swachh Bharat Abhiyan (SBA). Our tool computes an elaborated analysis of Swachh Bharat Abhiyan, which examines the success rate of this social campaign. Here, we performed the location-wise analysis of the campaign and predict the degree of polarity of tweets along with the monthly and weekly analysis of the tweets. The experiments were conducted in five phases namely extraction and preprocessing of tweets, tokenization, sentiment evaluation of a line, sentiment evaluation of a blog (document) and analysis. Our tool is also capable of handling transliterated words. Unbiased tweets were extracted from Twitter related to this specific campaign, and on comparing with manual tagging we were able to achieve 84.47 % accuracy using unigram machine learning approach. This approach helps the government to implement the social campaigns effectively for the betterment of the society. © 2016, Springer-Verlag London.","Lexical analysis; SENTI-METER; Sentiment analysis; Social campaign; Swachh Bharat Abhiyan","Artificial intelligence; Behavioral research; Computational linguistics; Data mining; Decision making; Internet; Learning algorithms; Learning systems; Social networking (online); Lexical analysis; Machine learning approaches; NAtural language processing; Sentiment analysis; Social campaign; Swachh Bharat Abhiyan; Tokenization; Natural language processing systems",2-s2.0-84982182197
"Qu T., Zhang J.H., Chan F.T.S., Srivastava R.S., Tiwari M.K., Park W.-Y.","Demand prediction and price optimization for semi-luxury supermarket segment",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029546623&doi=10.1016%2fj.cie.2017.09.004&partnerID=40&md5=07d65bd0bca478f3f7e38ae57d853ecb","Offline retail stores face day-to-day challenges in clearing out expensive and high-end luxury products. In case of high-end priced products, the demand is seasonal and sensitive. The investment involves high risk and revenues vary beyond fathomable bounds. The primary objective of this research is to present a decision-support system for retail pricing and revenue optimization of these retail products. The sales data of past 2.5 years from prominent retail stores across 45 different regions has been used to develop this study. A regression tree/random forest-based machine learning algorithm is used to predict weekly demand. It incorporates price, holidays, discounts, inventory and other regional factors in decision making. Following this, the demand-price interdependencies are quantified and integrated into an integer linear programming model for optimal price allocation. This methodology has been implemented on offline retailing of expensive products which generally follow high variation in demand. The expected revenue has been optimized by branch & bound and branch & cut method, followed by root node analysis. The solution is further optimized by heuristic methods. © 2017 Elsevier Ltd","Analytics; Assignment; Branch and bound; Data mining; Integer programming; Retailing","Artificial intelligence; Costs; Data mining; Decision making; Decision support systems; Economics; Heuristic methods; Integer programming; Learning algorithms; Learning systems; Optimization; Retail stores; Sales; Analytics; Assignment; Demand prediction; Expensive products; Integer linear programming models; Primary objective; Retailing; Revenue optimization; Branch and bound method",2-s2.0-85029546623
"Affonso C., Rossi A.L.D., Vieira F.H.A., de Carvalho A.C.P.D.L.F.","Deep learning for biological image classification",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019578783&doi=10.1016%2fj.eswa.2017.05.039&partnerID=40&md5=e8424aa7f2b615a3ac2cf26cec4fcca9","A number of industries use human inspection to visually classify the quality of their products and the raw materials used in the production process, this process could be done automatically through digital image processing. The industries are not always interested in the most accurate technique for a given problem, but most appropriate for the expected results, there must be a balance between accuracy and computational cost. This paper investigates the classification of the quality of wood boards based on their images. For such, it compares the use of deep learning, particularly Convolutional Neural Networks, with the combination of texture-based feature extraction techniques and traditional techniques: Decision tree induction algorithms, Neural Networks, Nearest neighbors and Support vector machines. Reported studies show that Deep Learning techniques applied to image processing tasks have achieved predictive performance superior to traditional classification techniques, mainly in high complex scenarios. One of the reasons pointed out is their embedded feature extraction mechanism. Deep Learning techniques directly identify and extract features, considered by them to be relevant, in a given image dataset. However, empirical results for the image data set have shown that the texture descriptor method proposed, regardless of the strategy employed is very competitive when compared with Convolutional Neural Network for all the performed experiments. The best performance of the texture descriptor method could be caused by the nature of the image dataset. Finally are pointed out some perspectives of futures developments with the application of Active learning and Semi supervised methods. © 2017","Deep learning; Image classification; Machine learning; Wood classification","Artificial intelligence; Convolution; Data mining; Decision trees; Deep learning; Extraction; Feature extraction; Image processing; Image retrieval; Image texture; Learning algorithms; Learning systems; Neural networks; Supervised learning; Classification technique; Convolutional neural network; Decision tree induction; Extraction mechanisms; Feature extraction techniques; Predictive performance; Semi-supervised method; Traditional techniques; Image classification",2-s2.0-85019578783
"Sadhu A., Prakash G., Narasimhan S.","A hybrid hidden Markov model towards fault detection of rotating components",2017,"JVC/Journal of Vibration and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032384572&doi=10.1177%2f1077546315627934&partnerID=40&md5=1809c3dc6e2d8d86088b23d9003ac496","A robust hybrid hidden Markov model-based fault detection method is proposed to perform multi-state fault classification of rotating components. The approach presented in this paper enhances the performance of the standard hidden Markov model (HMM) for fault detection by performing a series of pre-processing steps. First, the de-noised time-scale signatures are extracted using wavelet packet decomposition of the vibration data. Subsequently, the Teager Kaiser energy operator is employed to demodulate the time-scale components of the raw vibration signatures, following which the condition indicators are calculated. Out of several possible condition indicators, only relevant features are selected using a decision tree. This pre-processing improves the sensitivity of condition indicators under multiple faults. A Gaussian mixing model-based hidden Markov model (HMM) is then employed for fault detection. The proposed hybrid HMM is an improvement over traditional HMM in that it achieves better separation of the feature space leading to more robust state estimation under multiple fault states and measurement noise scenarios. A simulation employing modulated signals and two experimental validation studies are presented to demonstrate the performance of the proposed method. © SAGE Publications.","Condition-based monitoring; decision tree; fault detection; Gaussian mixing model; hidden Markov model; Teager Kaiser operator; wavelet packet transform","Data mining; Decision trees; Hidden Markov models; Markov processes; Mixing; Trellis codes; Wavelet analysis; Wavelet decomposition; Condition-based monitoring; Experimental validations; Fault classification; Mixing models; Robust state estimation; Teager-Kaiser operator; Wavelet Packet Decomposition; Wavelet packet transforms; Fault detection",2-s2.0-85032384572
"Velásquez L., Cruz-Tirado J.P., Siche R., Quevedo R.","An application based on the decision tree to classify the marbling of beef by hyperspectral imaging",2017,"Meat Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020243772&doi=10.1016%2fj.meatsci.2017.06.002&partnerID=40&md5=250429ecd96206d8aedffc06c8db2f60","The aim of this study was to develop a system to classify the marbling of beef using the hyperspectral imaging technology. The Japanese standard classification of the degree of marbling of beef was used as reference and twelve standards were digitized to obtain the parameters of shape and spatial distribution of marbling of each class. A total of 35 samples M. longissmus dorsi muscle were scanned by the hyperspectral imaging system of 400–1000 nm in reflectance mode. The wavelength of 528 nm was selected to segment the sample and the background, and 440 nm was used for classified the samples. Processing algorithms on image, based on decision tree method, were used in the region of interest obtaining a classification error of 0.08% in the building stage. The results showed that the proposed technique has a great potential, as a non-destructive and fast technique, that can be used to classify beef with respect to the degree of marbling. © 2017 Elsevier Ltd","Binarization; Data mining; Decision tree; Hyperspectral images; Marbled meat; Spatial distribution parameters","Beef; Data mining; Decision trees; Image segmentation; Imaging techniques; Meats; Spatial distribution; Spectroscopy; Trees (mathematics); Binarizations; Classification errors; Decision tree method; Distribution parameters; Hyperspectral imaging technologies; Processing algorithms; Reflectance modes; Region of interest; Hyperspectral imaging",2-s2.0-85020243772
"Saettler A., Laber E., de A. Mello Pereira F.","Decision tree classification with bounded number of errors",2017,"Information Processing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021679210&doi=10.1016%2fj.ipl.2017.06.011&partnerID=40&md5=e8e57dd24a5dc2431b3166287fd2f745","Oblivious decision trees are decision trees where every node in the same level is associated with the same attribute. These trees have been studied in the context of feature selection. In this paper, we study the problem of constructing an oblivious decision tree that incurs at most k classification errors, where k is a given integer. We present a randomized rounding algorithm that, given a parameter 0&lt;ϵ&lt;1/2, builds an oblivious decision tree with cost at most (3/(1−2ϵ))ln⁡(n)OPT(I) and produces at most (k/ϵ) errors, where OPT(I) is the optimal cost and n is the number of objects. The probability of failure of this algorithm is at most (n−1)/2n2. The logarithmic factor in the cost of the tree is the best possible attainable, even for k=0, unless P=NP. © 2017 Elsevier B.V.","Approximation algorithms; Decision trees; Feature selection; Randomized algorithms","Approximation algorithms; Costs; Decision trees; Errors; Feature extraction; Trees (mathematics); Classification errors; Decision tree classification; Optimal costs; Probability of failure; Randomized Algorithms; Randomized rounding; Data mining",2-s2.0-85021679210
"Zhao Y., Qin B., Liu T.","Encoding syntactic representations with a neural network for sentiment collocation extraction",2017,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031897021&doi=10.1007%2fs11432-016-9229-y&partnerID=40&md5=9dbf6b46efb6d81f524dec4677f37538","Sentiment collocation refers to the collocation of a target word and a polarity word. Sentiment collocation extraction aims to extract the targets and their modifying polarity words by analyzing the relationships between them. This can be regarded as a basic sentiment analysis task and is relevant in many practical applications. Previous studies relied mainly on the syntactic path, which is used to connect the target word and the polarity word. To deeply exploit the semantic information of the syntactic path, we propose two types of syntactic representation, namely, relation embedding and subtree embedding, to capture the latent semantic features. Relation embedding is used to represent the latent semantics between targets and their corresponding polarity words, and subtree embedding is used to explore the rich syntactic information for each word on the path. To combine the two types of syntactic representations, a neural network is constructed. We use a recursive neural network (RNN) to model the subtree embeddings, and then the subtree embedding and the word embedding are combined as the enhanced word representation for each word in the syntactic path. Finally, a convolutional neural network (CNN) is adopted to integrate the two types of syntactic representations to extract the sentiment collocations from reviews. Our experiments were conducted on six types of reviews, which included product domains (such as cameras and phones) and service domains (such as hotels and restaurants). The experimental results show that our proposed method can accurately capture the latent semantic features hidden behind the syntactic paths that neither the common feature-based methods nor the syntactic-path-based method can handle, and, further, that it significantly outperforms numerous baselines and previous methods. © 2017, Science China Press and Springer-Verlag GmbH Germany.","convolutional neural network (CNN); neural network; recursive neural network (RNN); sentiment analysis; sentiment collocation extraction; syntactic representation","Convolution; Data mining; Extraction; Natural language processing systems; Network coding; Neural networks; Semantics; Signal encoding; Collocation extraction; Convolutional neural network; Recursive neural networks; Sentiment analysis; Syntactic representation; Syntactics",2-s2.0-85031897021
"Tariq A., Badir Y.F., Tariq W., Bhutta U.S.","Drivers and consequences of green product and process innovation: A systematic review, conceptual framework, and future outlook",2017,"Technology in Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024876848&doi=10.1016%2fj.techsoc.2017.06.002&partnerID=40&md5=f51b1a98b6c64592b7ab16372a514933","Drawing clear conclusions from the literature on green innovation is challenging, due to its sheer breadth and the inconsistent insights it offers. This study systematically addresses this issue by conducting a systematic literature review of articles on innovation in green products and processes, with the aim of enhancing conceptual clarity and consistency, thus, advancing theory and research. This study reviewed 195 articles relating to both green product and process innovation published during 1991–2016. The articles were analyzed in terms of key attributes of green product and process innovation using content analysis. Based on the analysis, this study identifies the key drivers and consequences, mediators and moderators and develops a conceptual framework of green product and process innovation. It also discusses the limitations and main theories of green innovation literature, and articulates potential paths for future research. © 2017 Elsevier Ltd","Consequences; Drivers; Green process innovation; Green product innovation; Systematic literature review","Data mining; Truck drivers; Conceptual frameworks; Consequences; Green innovations; Green process; Green products; Process Innovation; Systematic literature review; Systematic Review; Computer supported cooperative work; conceptual framework; environmental economics; green economy; innovation; literature review; sustainability",2-s2.0-85024876848
"Abbas A.T., Pimenov D.Y., Erdakov I.N., Mikolajczyk T., El Danaf E.A., Taha M.A.","Minimization of turning time for high-strength steel with a given surface roughness using the Edgeworth–Pareto optimization method",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021752538&doi=10.1007%2fs00170-017-0678-2&partnerID=40&md5=dd41e44cec967a9b293fe162091f42de","High-strength steels are used in various civilian and military products. The initial cost of the raw materials for these products is very high. The surface roughness of these products is extremely important during the finishing pass to be accepted during the final inspection. The surface roughness should conform to the required values stated on the design drawing. The paper presents the results of experiments in turning of high-strength steel featuring three factors—cutting speed V, feed rate f, and depth of cut t—on five levels (125 specimens). These were divided into 25 groups. Each of the five groups was subjected to one common machining speed. Each group was machined using five levels of cutting depth. Each depth was processed using five levels of feed rate. Tessa was used for examination of surface roughness. There is little modern research on machining high-strength steel. The high cost of this material compels us to look for the optimum turning conditions to provide for the specified roughness of surface Ra and the minimum machining time of unit volume Tm. As a result of our study, an artificial neural network was designed in Matlab on the basis of the MLP 3-10-1 multilayer perceptron that allows us to predict Ra of the workpiece with ±2.14% accuracy within the range of the experimental cutting speed, depth of cut, and feed rate values. For the first time, a Pareto frontier was obtained for Ra and Tm of the finished workpiece from high-strength steel using the artificial neural network model that was later used to determine the optimum cutting conditions. It is possible to integrate the suggested optimization algorithms into computer-aided manufacturing using Matlab. © 2017, The Author(s).","Artificial neural network; Data mining; Edgeworth–Pareto method; High-strength steel; Optimization; Surface roughness; Turning operation","Computer aided manufacturing; Cutting; Data mining; Multiobjective optimization; Neural networks; Optimization; Pareto principle; Surface roughness; Turning; Artificial neural network modeling; Cutting conditions; Military products; Optimization algorithms; Pareto methods; Pareto optimization; Roughness of surface; Turning operations; High strength steel",2-s2.0-85021752538
"Zhan Q., Zhang J., Yu P., Xie J.","Community detection for emerging social networks",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011824616&doi=10.1007%2fs11280-017-0441-5&partnerID=40&md5=a8cb174c858f3dae11e65ec48ec0885e","Many famous online social networks, e.g., Facebook and Twitter, have achieved great success in the last several years. Users in these online social networks can establish various connections via both social links and shared attribute information. Discovering groups of users who are strongly connected internally is defined as the community detection problem. Community detection problem is very important for online social networks and has extensive applications in various social services. Meanwhile, besides these popular social networks, a large number of new social networks offering specific services also spring up in recent years. Community detection can be even more important for new networks as high quality community detection results enable new networks to provide better services, which can help attract more users effectively. In this paper, we will study the community detection problem for new networks, which is formally defined as the “New Network Community Detection” problem. New network community detection problem is very challenging to solve for the reason that information in new networks can be too sparse to calculate effective similarity scores among users, which is crucial in community detection. However, we notice that, nowadays, users usually join multiple social networks simultaneously and those who are involved in a new network may have been using other well-developed social networks for a long time. With full considerations of network difference issues, we propose to propagate useful information from other well-established networks to the new network with efficient information propagation models to overcome the shortage of information problem. An effective and efficient method, Cat (Cold stArT community detector), is proposed in this paper to detect communities for new networks using information from multiple heterogeneous social networks simultaneously. Extensive experiments conducted on real-world heterogeneous online social networks demonstrate that Cat can address the new network community detection problem effectively. © 2017, Springer Science+Business Media New York.","Cold start problem; Community detection; Data mining; Transfer learning","Data mining; Information dissemination; Population dynamics; Attribute information; Cold start problems; Community detection; Established networks; Information propagation model; Network communities; On-line social networks; Transfer learning; Social networking (online)",2-s2.0-85011824616
"Grubert E., Cook M.","Communication science for science communication: Water management for oil and natural gas extraction",2017,"Journal of Water Resources Planning and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028710509&doi=10.1061%2f%28ASCE%29+WR.1943-5452.0000842&partnerID=40&md5=6d5fa3c46006270c9298a9987ec0e1b3","Water management for oil and natural gas extraction in the United States has become a topic of public interest and concern. This societal relevance simultaneously heightens the need for rigorous performance and dissemination of scientific work and invites caution from experts who are communicating within what is likely a politicized public conversation. This paper uses interviews to investigate experts' current practices and comfort with communicating about water use for oil and natural gas. Participants cite face-to-face interactions and trust-based relationships as important in their interactions, which is consistent with research about effective communication. However, few participants highlight techniques specific to communicating about water as it relates to oil and gas or about controversial issues generally. Participants also rarely use communication science related to objective setting, framing, and measuring success for improvement, likely in part because of a lack of evidence-based training. In many cases, interviewees expressed attitudes consistent with the deficit model of scientific communication, which holds that presentation of scientific facts will change public opinion. This model has been shown to be relatively ineffective. This paper highlights the need for careful communication and evidence-based opportunities for improvement, including a suggestion that professional societies host communication training and coaching sessions. © 2017 American Society of Civil Engineers.","Best practices; Gas; Hydraulic fracturing; Oil; Science communication; Water","Data mining; Gallium; Gases; Hydraulic fracturing; Natural gas; Social aspects; Water; Water management; Best practices; Communication training; Effective communication; Face-to-face interaction; Objective setting; Oil and natural gas; Science communications; Scientific communication; Extraction; communication; extraction method; gas production; hydraulic fracturing; natural gas; oil production; water management; water use; United States",2-s2.0-85028710509
"Cicalese F., Laber E., Saettler A.","Decision Trees for Function Evaluation: Simultaneous Optimization of Worst and Expected Cost",2017,"Algorithmica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992027500&doi=10.1007%2fs00453-016-0225-9&partnerID=40&md5=d17bf9f8ef609a77a9d361d3f68261d0","In several applications of automatic diagnosis and active learning, a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function. In general, the process of reading the value of a variable might involve some cost. This cost should be taken into account when deciding the next variable to read. The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables’ assignments). Our algorithm builds a strategy (decision tree) which attains a logarithmic approximation simultaneously for the expected and worst cost spent. This is best possible under the assumption that P≠ NP. © 2016, Springer Science+Business Media New York.","Approximation algorithms; Decision tress; Function evaluation; Hardness of approximation","Approximation algorithms; Artificial intelligence; Costs; Data mining; Decision trees; Optimization; Trees (mathematics); Automatic diagnosis; Central problems; Decision tress; Discrete functions; Hardness of approximation; Logarithmic approximation; Prior distribution; Simultaneous optimization; Function evaluation",2-s2.0-84992027500
"Sandborn P.","Forecasting technology and part obsolescence",2017,"Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032452730&doi=10.1177%2f0954405415598923&partnerID=40&md5=ab64294121181ff780eb27c412be4045","Technology-centric products often contain parts, software, and materials that have procurement lives that end before the product they are in reaches the end of its life cycle. Life-cycle mismatches between parts and products, which is referred to as obsolescence, can result in large life-cycle costs for mission, safety, and infrastructure critical products, such as aircraft, medical, and military systems. Diminishing Manufacturing Sources and Materials Shortages is a type of obsolescence that describes the loss of the ability to purchase (or procure) a part (or its associated technology) from its original manufacturer. A key enabler for performing pro-active and strategic management of the life cycle of mission, safety, and infrastructure critical products is the ability to forecast when technologies and parts will become unavailable for purchase, that is, obsolete. This article reviews methods that are used to forecast obsolescence, focusing on long-term forecasting used to predict the obsolescence dates for technologies and electronic parts. © Institution of Mechanical Engineers 2016.","data mining; Diminishing Manufacturing Sources and Materials Shortages; electronic parts; Obsolescence; obsolescence management","Data mining; Forecasting; Manufacture; Obsolescence; Diminishing Manufacturing Sources and Materials Shortages; Electronic parts; Lifecycle costs; Long-term forecasting; Military systems; Obsolescence management; Strategic management; Life cycle",2-s2.0-85032452730
"Worachartcheewan A., Nantasenamat C., Prachayasittikul S., Aiemsaard A., Prachayasittikul V.","Towards the design of 3-aminopyrazole pharmacophore of pyrazolopyridine derivatives as novel antioxidants",2017,"Medicinal Chemistry Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021754398&doi=10.1007%2fs00044-017-1967-x&partnerID=40&md5=3b33de0496f2e71d2ed8d6f48b749613","Free radicals and oxidants can cause oxidative damage to physiologically important biomolecules that subsequently leads to the development of a wide range of chronic and degenerative diseases such as aging, cancer, cardiovascular and neurodegenerative diseases. Antioxidants have been shown to be instrumental in counteracting the deleterious effects of these reactive oxygen species. Herein, a series of 20 pyrazolopyridine derivatives with antioxidant activity were utilized for constructing a quantitative structure–activity relationship model as to unravel the origins of the antioxidant activity. Quantum chemical and molecular descriptors were used to quantitate the physicochemical properties of investigated compounds. Significant descriptors as identified by stepwise regression analysis consisted of Mor11m, Mor25v, JGI5, H8p, GATS5p, and GVWAI-50. Statistical parameters suggested that the constructed quantitative structure–activity relationship models were robust with Q2 = 0.9370 and root mean square error = 4.7414 as evaluated via leave-one-out cross-validation. The mechanistic basis of the antioxidant activity as deduced from significant descriptors was rationalized. Particularly, compounds with the highest antioxidant activity required compounds to have the highest mean topological charge index of order 5 (JGI5) and Geary autocorrelation–lag 5/weighted by atomic polarizabilities (GATS5p) but necessitated low 3D-MoRSE-signal 25/weighted by atomic van der Waals volumes (Mor25v). Such properties are well corroborated by the 3-aminopyrazole pharmacophore from investigated compounds. Molecular insights unraveled herein is anticipated to be useful as guidelines for further rational design of novel pyrazole analogs with potent antioxidant activity. © 2017, Springer Science+Business Media, LLC.","Antioxidant activity; Data mining; Multiple linear regression; Oxidative stress; Pyrazolopyridine; QSAR","3 aminopyrazole; antioxidant; pyrazole derivative; pyridine; quinone derivative; unclassified drug; antioxidant activity; Article; chemical structure; pharmacophore; physical chemistry; quantitative structure activity relation; quantum chemistry",2-s2.0-85021754398
"Tong D., Qu Y.R., Prasanna V.K.","Accelerating Decision Tree Based Traffic Classification on FPGA and Multicore Platforms",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021808413&doi=10.1109%2fTPDS.2017.2714661&partnerID=40&md5=e572cae8016f9fe093af81685af60e3e","Machine learning (ML) algorithms have been shown to be effective in classifying a broad range of applications in the Internet traffic. In this paper, we propose algorithms and architectures to realize online traffic classification using flow level features. First, we develop a traffic classifier based on C4.5 decision tree algorithm and Entropy-MDL (Minimum Description Length) discretization algorithm. It achieves an overall accuracy of 97.92 percent for classifying eight major applications. Next we propose approaches to accelerate the classifier on FPGA (Field Programmable Gate Array) and multicore platforms. We optimize the original classifier by merging it with discretization. Our implementation of this optimized decision tree achieves 7500+ Million Classifications Per Second (MCPS) on a state-of-the-art FPGA platform and 75-150 MCPS on two state-of-the-art multicore platforms. We also propose a divide and conquer approach to handle imbalanced decision trees. Our implementation of the divide-and-conquer approach achieves 10,000+ MCPS on a state-of-the-art FPGA platform and 130-340 MCPS on two state-of-the-art multicore platforms. We conduct extensive experiments on both platforms for various application scenarios to compare the two approaches. © 1990-2012 IEEE.","decision tree; FPGA; high throughput; machine learning; multicore; Traffic classification","Acceleration; Artificial intelligence; Decision trees; Education; Field programmable gate arrays (FPGA); Learning algorithms; Learning systems; Logic gates; Optimization; Signal receivers; Telecommunication traffic; Throughput; Trees (mathematics); Classification algorithm; High throughput; Multi core; Multi-core processing; Traffic classification; Data mining",2-s2.0-85021808413
"Xie Z., Zeng Z., Zhou G., Wang W.","Topic enhanced deep structured semantic models for knowledge base question answering",2017,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029915725&doi=10.1007%2fs11432-017-9136-x&partnerID=40&md5=0ebfff1af3534940c24eb2ac64938ca4","Knowledge Base Question Answering (KBQA) is a hot research topic in natural language processing (NLP). The most challenging problem in KBQA is how to understand the semantic information of natural language questions and how to bridge the semantic gap between the natural language questions and the structured fact triples in knowledge base. This paper focuses on simple questions which can be answered by a single fact triple in knowledge base. We propose a topic enhanced deep structured semantic model for KBQA. The proposed method considers the task of KBQA as a matching problem between questions and the subjects and predicates in knowledge base. And the proposed model consists of two stages to match the subjects and predicates, respectively. In the first stage, we propose a Convolutional based Topic Entity Extraction Model (CTEEM) to extract topic entities mentioned in questions. With the extracted entities, we can retrieve the relevant candidate fact triples from knowledge base and obviously decrease the amount of noising candidates. In the second stage, we employ Deep Structured Semantic Models (DSSMs) to compute the semantic relevant score between questions and predicates in the candidates. And we combine the semantic level and the lexical level scores to rank the candidates. We evaluate the proposed method on KBQA dataset released by NLPCC-ICCPOL 2016. The experimental results show that our proposed method achieves the third place among the 21 submitted systems. Furthermore, we also extend the DSSM by using BiLSTM and integrate a convolutional structure on the top of BiLSTM layers. Our experimental results show that the extension models can further improve the performance. © 2017, Science China Press and Springer-Verlag GmbH Germany.","deep learning; knowledge base; question answering; semantic matching; topic entity","Convolution; Deep learning; Knowledge based systems; Natural language processing systems; Semantics; Entity extractions; Hot research topics; Knowledge base; Natural language questions; Question Answering; Semantic information; Semantic matching; topic entity; Data mining",2-s2.0-85029915725
"Cui T.-J., Li S.-S.","Study on the relationship between system reliability and influencing factors under big data and multi-factors",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032823007&doi=10.1007%2fs10586-017-1278-5&partnerID=40&md5=b92779843716a66337914c1bc999762c","The system reliability is related to the factors which affect the system reliability in a multi-factor influencing environment. At the same time, the cumulative fault data increases rapidly as the system operates. These big data magnitude fault data also contains system reliability characteristics. Therefore, it is urgent to solve the problem of system reliability analysis which is suitable for big data and multi factors. The research adopts the method of space fault tree (SFT) and factor space (FS), which are proposed by the author to solve these problems. SFT can solve the system reliability analysis under the influence of many factors. FS has the ability of large data processing and logic analysis. Fault probability is used to represent reliability, FS is introduced into SFT. The logical relationship between reliability and influencing factors is studied, and we also put forward the corresponding analysis method. On the basis of the existing research, the SFT theory is improved, which can be used to infer the causal relationship, reduce factor dimension and compress fault data. This research is the intersection of safety system science and information science intelligent technology. It can provide the basic theory and application method for the reliability analysis of various industrial and mining enterprises, military and other fields. © 2017 Springer Science+Business Media, LLC","Big data; Factor space; Information science; Multi-factors; Reliability; Safety science; Space fault tree","Big data; Data handling; Factor analysis; Information science; Reliability; Reliability theory; Safety factor; Trees (mathematics); Causal relationships; Factor space; Fault probabilities; Fault-trees; Intelligent technology; Logical relationships; Multi factors; Safety science; Reliability analysis",2-s2.0-85032823007
"Persson S., Harnesk D., Islar M.","What local people? Examining the Gállok mining conflict and the rights of the Sámi population in terms of justice and power",2017,"Geoforum",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029166438&doi=10.1016%2fj.geoforum.2017.08.009&partnerID=40&md5=cc3927a490eb6a5376303037f88b4c9f","The global extraction of minerals is commonly located in areas populated by indigenous people; and while conflicts between multinational corporations and local activists and indigenous people are widespread today, the understanding of their dynamics are lacking. The Swedish government's encouragement to an expanding mining industry has caused resistance due to environmental and social implications, particularly its effect on Sámi reindeer husbandry. The resistance to a mine in Gállok is based on the belief that the right to decide about land use historically falls on the Sámi people, and the right to affect land use is detrimental for the survival of Sámi culture and reindeer husbandry. Although the conflict may be perceived as concerning access to natural resources, we argue that the perceived environmental conflict can be viewed as part of a larger struggle over social status and recognition. Data have been collected using qualitative methods such as observations, interviews and documents. The subsequent analysis relies on a meta-theoretical framework of justice as recognition using a typology of relations of power. Our findings suggest that relations of power constitute different categories of social actors. Stakeholders like the Sámi population are subordinated to more dominant stakeholders such as the government, the company and media, who have ‘more’ power or ‘different’ kinds of power ‘over’ others. Through these asymmetric power relations, historical state-Sámi relations are continuously reproduced within prevailing institutions, and also in this mining conflict. Interviewees from business and the municipality testified to the discourses driven by a neoliberal and profit-focused worldview. Challenging the neoliberal discourse, other stakeholders, namely civil society and Sámi, expressed an alternative discourse based on a local, traditional, cultural, environmental and anti-neoliberal worldview. © 2017 The Author(s)","Indigenous; Justice; Mining; Neoliberal; Power; Sweden; Sámi","civil society; human rights; indigenous population; mining industry; neoliberalism; power relations; social conflict; social justice; stakeholder; Sweden; Rangifer tarandus",2-s2.0-85029166438
"Nie L., Wang H., Xu Y.","Application of the arctangent function model in the prediction of ground mining subsidence deformation: a case study from Fushun City, Liaoning Province, China",2017,"Bulletin of Engineering Geology and the Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978174486&doi=10.1007%2fs10064-016-0913-3&partnerID=40&md5=7a9cd855cdc4abb73337dcb27b0279e0","Ground subsidence disasters are characterized by wide distribution, long duration, and high-intensity damage, which can cause serious damages to surface buildings, underground pipelines, aquifers, and so on. Therefore, research on the stability evaluation of ground subsidence and subsidence deformation prediction is of great significance. This paper takes ground subsidence in Fushun City, Liaoning Province, China, as a case study. Combined with data from 60 monitoring points in the subsidence areas, the final settlement deformation values of all monitoring points were obtained through an arctangent function model using non-linear curve fitting with monitoring data. The proposed model could enable the prediction of settlement deformation trends of the monitoring points. Correlation coefficients are all above 0.937, indicating the strong reliability of the prediction model. By processing the final settlement deformation predictive values of the 60 monitoring points, a final settlement contour map was drawn with the help of the Kriging interpolation method. This map could forecast the whole distribution characteristics of ground settlement deformation in the research area. Then, risk zoning can be obtained by combining the settlement rate and residual settlement deformation in the study area. The research results could provide a basis for future city construction and regional planning in Fushun City. © 2016, Springer-Verlag Berlin Heidelberg.","Arctangent function model; Deformation prediction; Ground subsidence; Kriging interpolation method; Non-linear curve fitting; Risk zoning","Aquifers; Curve fitting; Cutting machines (mining); Forecasting; Hydrogeology; Interpolation; Monitoring; Pipelines; Regional planning; Subsidence; Zoning; Arc tangent functions; Deformation prediction; Kriging interpolation methods; Non linear; Risk zoning; Deformation; correlation; damage mechanics; deformation; future prospect; ground settlement; kriging; mining; nonlinearity; regional planning; subsidence; urban planning; China; Fushun; Liaoning",2-s2.0-84978174486
"Mohabbati-Kalejahi N., Yazdi M.A.A., Megahed F.M., Schaefer S.Y., Boyd L.A., Lang C.E., Lohse K.R.","Streamlining science with structured data archives: insights from stroke rehabilitation",2017,"Scientometrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027987830&doi=10.1007%2fs11192-017-2482-z&partnerID=40&md5=88ff2de75e39d2bc4157093ce381eb6b","Recent advances in bibliometrics have focused on text-mining to organize scientific disciplines based on author networks, keywords, and citations. These approaches provide insights, but fail to capture important experimental data that exist in many scientific disciplines. The objective of our paper is to show how such data can be used to organize the literature within a discipline, and identify knowledge gaps. Our approach is especially important for disciplines relying on randomized control trials. Using stroke rehabilitation as an informative example, we construct an interactive graphing platform to address domain general scientific questions relating to bias, common data elements, and relationships between key constructs in a field. Our platform allows researchers to ask their own questions and systematically search the literature from the data up. © 2017, Akadémiai Kiadó, Budapest, Hungary.","Bibliometrics; Data visualization; Meta-science; Randomized controlled trials",,2-s2.0-85027987830
"Sedighi Maman Z., Alamdar Yazdi M.A., Cavuoto L.A., Megahed F.M.","A data-driven approach to modeling physical fatigue in the workplace using wearable sensors",2017,"Applied Ergonomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014121867&doi=10.1016%2fj.apergo.2017.02.001&partnerID=40&md5=17d1f6f7db4fa3bc4c2abbe94a328b81","Wearable sensors are currently being used to manage fatigue in professional athletics, transportation and mining industries. In manufacturing, physical fatigue is a challenging ergonomic/safety “issue” since it lowers productivity and increases the incidence of accidents. Therefore, physical fatigue must be managed. There are two main goals for this study. First, we examine the use of wearable sensors to detect physical fatigue occurrence in simulated manufacturing tasks. The second goal is to estimate the physical fatigue level over time. In order to achieve these goals, sensory data were recorded for eight healthy participants. Penalized logistic and multiple linear regression models were used for physical fatigue detection and level estimation, respectively. Important features from the five sensors locations were selected using Least Absolute Shrinkage and Selection Operator (LASSO), a popular variable selection methodology. The results show that the LASSO model performed well for both physical fatigue detection and modeling. The modeling approach is not participant and/or workload regime specific and thus can be adopted for other applications. © 2017 Elsevier Ltd","Analytics; Feature selection; Penalized regression; Physical fatigue","Feature extraction; Linear regression; Manufacture; Regression analysis; Wearable technology; Analytics; Data-driven approach; Important features; Least absolute shrinkage and selection operators; Manufacturing tasks; Multiple linear regression models; Penalized regression; Physical fatigues; Wearable sensors; adult; Article; biological monitoring; biosensor; controlled study; equipment design; ergonomics; fatigue; female; human; human experiment; job performance; male; middle aged; normal human; occupational health; physical fatigue; portable equipment; shift worker; task performance; work environment; workload; workplace; young adult",2-s2.0-85014121867
"Prasad T.D., Rao D.S.","Noise mapping of a bauxite mine using measured noise and GPS data",2017,"Disaster Advances",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032503708&partnerID=40&md5=af6f4c008a9f1a1381c6b51b0605b014","This study aimed to produce a noise map by determining the noise levels in the different parts of the bauxite mine and to compare them with the Directorate General of Mines Safety (DGMS) prescribed permissible limits. A total of 654 points whose positions are measured by Global Positioning System (GPS), were collected from the mine at varying intervals according to the International Organization for Standardization (ISO) and DGMS guidelines to measure sound pressure levels. The GPS receiver interfaced with the real-time octave band analyzer allowed simultaneous measurement of position and noise level data. Measured points are exported to ArcGIS software for generating the noise map. Results showed that noise levels were higher than the permissible limits in most parts of the mine, the maximum and minimum average noise levels were 104 dB(A) and 50 dB(A), logarithmic mean equivalent A-weighted sound pressure level (LAeq) was 67 ± 12 and LAeq ranging between 81-90 dB (A) at workplaces of dozer and pay loader, 71-80 dB (A) at shovel working locations and 50-60 dB (A) at different haul roads respectively. The results suggest that integrating noise data with GPS can enhance visualization of the noise map and is likely to be more effective at informing policy decision-making, and necessary steps to be taken against high noise levels.","ArcGIS; Bauxite mine; Equivalent sound levels; GPS; Noise mapping; Noise pollution","bauxite; geological mapping; GIS; GPS; mine; mining; noise pollution; real time; safety; signal-to-noise ratio; software",2-s2.0-85032503708
"Bui T.-H., Park S.-B.","Point of interest mining with proper semantic annotation",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995747088&doi=10.1007%2fs11042-016-4114-7&partnerID=40&md5=23794d8f0dc3d70e969b3f8f6029ccf6","Mining geo-tagged social photo media has received large amounts of attention from researchers recently. Points of interest (POI) mining from a collection of geo-tagged photos is one of these problems. POI mining refers to the processes of pattern recognition (namely clustering), extraction and semantic annotation. However, based on unsupervised clustering methods, many POIs might not be mined. Additionally, there is a great challenge for the proper semantic annotation to data clusters after clustering. In practice, there are many applications which require the accuracy of semantic annotation and high quality of pattern recognition such as POI recommendation. In this paper, we study POI mining from a collection of geo-tagged photos in combination with proper semantic annotation by using additional POI information from high coverage external POI databases. We propose a novel POI mining framework by using two-level clustering, random walk and constrained clustering. In random walk clustering step, we separate a large-scale collection of geo-tagged photos into many clusters. In the constrained clustering step, we continue to divide the clusters that include many POIs into many sub-clusters, where the geo-tagged photos in a sub-cluster associate with a particular POI. Experimental results on two datasets of geo-tagged Flickr photos of two cities in California, USA have shown that the proposed method substantially outperforms existing approaches that are adapted to handle the problem. © 2016, Springer Science+Business Media New York.","Clustering; Geo-tagged photo; POI mining; Semantic annotation","Pattern recognition; Random processes; Clustering; Constrained clustering; Geo-tagged photo; Point of interest; Points of Interest(POI); Random walk clustering; Semantic annotations; Unsupervised clustering methods; Semantics",2-s2.0-84995747088
"Liu S., Liu W., Shen J.","Stress evolution law and failure characteristics of mining floor rock mass above confined water",2017,"KSCE Journal of Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013041493&doi=10.1007%2fs12205-017-1578-6&partnerID=40&md5=bfd42a43e28411a65019af54f79bc88a","The discussion of stress evolution law and failure characteristics of rock mass in mining floor above confined water is the key to control the floor water inrush in deep mining. Based on analyzing the strike support pressure of working face, the mechanical model of coal floor above the confined water was established. Then, vertical, horizontal and shear stress distribution maps and the failure range morphological map of mining floor were calculated based on data processing software Origin. Then, with the similar simulation test bed for water inrush from coal seam floor, the fluid solid coupling simulation material was selected, and the stress evolution law and failure characteristics of coal seam mining floor were improved. The results showed that the vertical stress contour is a “semi-elliptical” shape distribution and the greater the floor depth, the smaller the affected degree, but the influenced range increased. The higher shear stress zone occurred in the vicinity of working face coal wall. The failure range morphological map of mining floor was approximately a spoon shape and maximum failure depth was 14.0 m based on the mechanical theory. The maximum failure depth of the floor was 13.4 m based on the similar simulation experiment. So, the experimental results are in agreement with the theoretical results. The results in this paper can provide theory basis for safety mining above confined water. © 2017, Korean Society of Civil Engineers and Springer-Verlag Berlin Heidelberg.","failure characteristics; floor water inrush; mechanical theory; similar simulation test; stress evolution","Coal; Coal deposits; Confined flow; Data handling; Failure (mechanical); Rock mechanics; Rocks; Shear stress; Failure characteristics; Floor water inrush; Mechanical theory; Simulation tests; Stress evolution; Floors",2-s2.0-85013041493
"Hovardas T.","“Battlefields” of blue flags and seahorses: Acts of “fencing” and “de-fencing” place in a gold mining controversy",2017,"Journal of Environmental Psychology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023613141&doi=10.1016%2fj.jenvp.2017.07.005&partnerID=40&md5=89be78b8885cc1dc01239d9b960fedb7","The objective of the present study was to investigate how meanings of place were constructed to shape contrasting positions in a gold mining conflict in Greece, and the implications of these dynamics for place-attachment. A social divide has gradually deepened in the area, separating employees of the mining company, on the one side, from residents involved in agriculture, forestry, fishing, and tourism, on the other. Local people engaged in the conflict were interviewed and interview data were analysed in the frame of discursive positioning of respondents, in order to follow how place-based referents were voiced. Acts of positioning might be approached as acts of “fencing”, namely acts of designating specific areas and assigning specific significations or place-based projects to them. These acts were followed by counter-acts of “de-fencing”, namely, acts of challenging significations attempted by the opposing side. Implications of the study for future research are discussed. © 2017 Elsevier Ltd","Discursive positioning; Mining; Place attachment; Stigma; “Fencing” place",,2-s2.0-85023613141
"Mahdevari S., Shahriar K., Sharifzadeh M., Tannant D.D.","Stability prediction of gate roadways in longwall mining using artificial neural networks",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961113477&doi=10.1007%2fs00521-016-2263-2&partnerID=40&md5=54556b173dbf2e53529e742365544e88","Roadways stability in longwall coal mining is critical to mine productivity and safety of the personnel. In this regard, a typical challenge in longwall mining is to predict roadways stability equipped with a reliable support system in order to ensure their serviceability during mining life. Artificial neural networks (ANNs) were employed to predict the stability conditions of longwall roadways based on roof displacements. In this respect, datasets of the roof displacements monitored in different sections of a 1.2-km-long roadway in Tabas coal mine, Iran, were set up to develop an ANN model. On the other hand, geomechanical parameters obtained through site investigations and laboratory tests were introduced to the ANN model as independent variables. In order to predict the roadway stability, these data were introduced to a multilayer perceptron (MLP) network to estimate the unknown nonlinear relationship between the rock parameters and roof displacements in the gate roadways. A four-layer feed-forward backpropagation neural network with topology 9-7-6-1 was found to be optimum. As a result, the MLP proposed model predicted values close enough to the measured ones with an acceptable range of correlation. A high conformity (R2 = 0.911) was observed between predicted and measured roof displacement values. Concluding remark is the proposed model appears to be a suitable tool for prediction of gate roadways stability in longwall mining. © 2016, The Natural Computing Applications Forum.","Artificial neural networks; Displacement monitoring; Longwall roadways; Roof stability; Tabas coal mine","Coal; Coal mines; Forecasting; Longwall mining; Neural networks; Reconfigurable hardware; Reinforcement; Roofs; Stability; Displacement monitoring; Feedforward backpropagation; Independent variables; Longwall roadway; Multi layer perceptron; Non-linear relationships; Roof stability; Tabas coal; Mine roof control",2-s2.0-84961113477
"Imami D., Lami E., Uberti L.J.","Election cycles in mining licensing: theory and evidence from Albania",2017,"Post-Communist Economies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032654775&doi=10.1080%2f14631377.2017.1361692&partnerID=40&md5=f35b4690b5b3038e1e91792763e1f0c9","Most of the literature on political business and budgetary cycles (PBBC) has focused on fiscal and monetary policy variables in advanced-country contexts. We extend this literature by investigating political cycle effects in a non-monetary, non-fiscal policy regime (the allocation of mining licences) in a transition country context. We propose a model of mining licensing that allows for corruption and for both supply and demand effects to determine the outcome. We then estimate this model using time-series data from post-communist Albania. Relying on a dynamic Poisson model, we find evidence of both opportunistic and partisan effects. Based on our theory, we suggest a corruption interpretation of political cycles in non-fiscal/non-monetary variables. This interpretation, we suggest, may be more applicable to the context of developing and transition countries. Our study raises important questions about the unintended (and often pernicious) effects of transition politics on economic regulation and economic performance in post-socialist economies. © 2017 Informa UK Limited, trading as Taylor & Francis Group","corruption; licensing; mining; Poisson; Political business cycles; transition",,2-s2.0-85032654775
"Dutta M., Saikia J., Taffarel S.R., Waanders F.B., de Medeiros D., Cutruneo C.M.N.L., Silva L.F.O., Saikia B.K.","Environmental assessment and nano-mineralogical characterization of coal, overburden and sediment from Indian coal mining acid drainage",2017,"Geoscience Frontiers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009830140&doi=10.1016%2fj.gsf.2016.11.014&partnerID=40&md5=35b2bb90053ca786c13ee287775948c7","The deterioration of environmental conditions is the major contributory factor to poor health and quality of life that hinders sustainable development in any region. Coal mining is one of the major industries that contribute to the economy of a country but it also impacts the environment. The chemical parameters of the coal, overburden, soil and sediments along with the coal mine drainage (CMD) were investigated in order to understand the overall environmental impact from high sulphur coal mining at northeastern coalfield (India). It was found that the total sulphur content of the coal is noticeably high compared to the overburden (OB) and soil. The volatile matter of the coal is sufficiently high against the high ash content of the soil and overburden. The water samples have a High Electrical Conductivity (EC) and high Total Dissolve Solid (TDS). Lower values of pH, indicate the dissolution of minerals present in the coal as well as other minerals in the mine rejects/overburden. The chemical and nano-mineralogical composition of coal, soil and overburden samples was studied using a High Resolution-Transmission Electron Microscopy (HR-TEM), Energy Dispersive Spectroscopy (EDS), Selected-Area Diffraction (SAED), Field Emission-Scanning Electron Microscopy (FE-SEM)/EDS, X-ray diffraction (XRD), Fourier Transform Infrared Spectroscopy (FTIR), Raman and Ion-Chromatographic analysis, and Mössbauer spectroscopy. From different geochemical analysis it has been found that the mine water sample from Ledo colliery has the lowest pH value of 3.30, Tirap colliery samples have the highest electrical conductivity value of 5.40 ms cm−1. Both Ledo and Tirap coals have total sulphur contents within the range 3–3.50%. The coal mine water from Tirap colliery (TW-15B) has high values of Mg2+ (450 ppm), and Br− (227.17 ppm). XRD analysis revealed the presence of minerals including quartz and hematite in the coals. Mineral analysis of coal mine overburden (OB) indicates the presence both of pyrite and marcasite which was also confirmed in XRD and Mossbauer spectral analysis. The presented data of the minerals and ultra/nano-particles present shows their ability to control the mobility of hazardous elements, suggesting possible use in environmental management technology, including restoration of the delicate Indian coal mine areas. © 2017 China University of Geosciences (Beijing) and Peking University","Advance characterization; Chemical analysis; Coal mine drainage; Environmental assessment; Indian coal; Nano-mineralogy","chemical analysis; coal mine; coal mining; environmental assessment; environmental conditions; environmental impact; environmental management; mineralogy; sulfur; sustainable development; India",2-s2.0-85009830140
"Bhatia S., Sharma M., Bhatia K.K.","Opinion score mining: An algorithmic approach",2017,"International Journal of Intelligent Systems and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032295005&doi=10.5815%2fijisa.2017.11.05&partnerID=40&md5=fbe5e176c1511d9d7c5a61c0853c5f33","Opinions are used to express views and reviews are used to provide information about how a product is perceived. People contributions lie in posting text messages in the form their opinions and emotions which may be based on different topics such as movie, book, product, and politics and so on. The reviews available online can be available in thousands, so making the right decision to select a product becomes a very tedious task. Several research works has been proposed in the past but they were limited to certain issues discussed in this paper. The reviews are collected which periodically updates itself using crawler discussed in our previous work. Further after applying certain pre-processing tasks in order to filter reviews and remove unwanted tokens, the sentiments are classified according to the novel unsupervised algorithm proposed. Our algorithm does not require annotated training data and is adequate to sufficiently classify the raw text into each domain and it is applicable enough to categorize complex cases of reviews as well. Therefore, we propose a novel unsupervised algorithm for categorizing sentiments into positive, negative and neutral category. The accuracy of the designed algorithm is evaluated using the standard datasets like IRIS, MTCARS, and HAR. © 2017 MECS.","Crawler; Mining; Opinion; Sentiment analysis; Unsupervised learning",,2-s2.0-85032295005
"Aryal S., Ting K.M., Washio T., Haffari G.","Data-dependent dissimilarity measure: an effective alternative to geometric distance measures",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017145554&doi=10.1007%2fs10115-017-1046-0&partnerID=40&md5=f4931c0ac71fd968bcbb610d644ad371","Nearest neighbor search is a core process in many data mining algorithms. Finding reliable closest matches of a test instance is still a challenging task as the effectiveness of many general-purpose distance measures such as ℓp-norm decreases as the number of dimensions increases. Their performances vary significantly in different data distributions. This is mainly because they compute the distance between two instances solely based on their geometric positions in the feature space, and data distribution has no influence on the distance measure. This paper presents a simple data-dependent general-purpose dissimilarity measure called ‘mp-dissimilarity’. Rather than relying on geometric distance, it measures the dissimilarity between two instances as a probability mass in a region that encloses the two instances in every dimension. It deems two instances in a sparse region to be more similar than two instances of equal inter-point geometric distance in a dense region. Our empirical results in k-NN classification and content-based multimedia information retrieval tasks show that the proposed mp-dissimilarity measure produces better task-specific performance than existing widely used general-purpose distance measures such as ℓp-norm and cosine distance across a wide range of moderate- to high-dimensional data sets with continuous only, discrete only, and mixed attributes. © 2017, Springer-Verlag London.","Cosine distance; Distance measure; mp-dissimilarity; ℓp-norm",,2-s2.0-85017145554
"Cho Y.-J., Fu P.-W., Wu C.-C.","Popular Research Topics in Marketing Journals, 1995–2014",2017,"Journal of Interactive Marketing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030455391&doi=10.1016%2fj.intmar.2017.06.003&partnerID=40&md5=1d282e50a15e1954e6976ccc3b3a71ec","During the past two decades, the focus of marketing has moved from the tactics of persuasion to the strategies of value cocreation. After moving toward cognitive science and corporate strategies in the early 2000s, marketing research returned to its traditional domains of consumer psychologies and customer management. While conscientious consumers are gradually restraining themselves from selfish indulgence, marketers have refocused on a new set of values that encompass mental, experiential, and societal well-being. In this regard, we adopt an unprecedented approach by incorporating topic modeling with social network analysis. The results show that, in terms of topic heterogeneity, the most impactful journals are the most diverse, whereas each runner-up has a unique focus. Among the journals, we detect two major co-authorship communities, and among the topics, we detect three. Further, we find that the communities of the most cited papers are composed of heterogeneous clusters of similar topics. The pivots within, and the bridges between, these communities are also reported. In the spirit of collaborative research, our topic model and network analysis are shared via online collaboration and visualization platforms that readers can use to explore our models interactively and to download the dataset for further studies. © 2017 Direct Marketing Educational Foundation, Inc. dba Marketing EDGE","Data visualization; Reproducible and collaborative research; Text mining; Topic model",,2-s2.0-85030455391
"Sudha M.","Evolutionary and Neural Computing Based Decision Support System for Disease Diagnosis from Clinical Data Sets in Medical Practice",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029931757&doi=10.1007%2fs10916-017-0823-3&partnerID=40&md5=0c406e5de2ef221753c68ef9cc56cf1b","As a recent trend, various computational intelligence and machine learning approaches have been used for mining inferences hidden in the large clinical databases to assist the clinician in strategic decision making. In any target data the irrelevant information may be detrimental, causing confusion for the mining algorithm and degrades the prediction outcome. To address this issue, this study attempts to identify an intelligent approach to assist disease diagnostic procedure using an optimal set of attributes instead of all attributes present in the clinical data set. In this proposed Application Specific Intelligent Computing (ASIC) decision support system, a rough set based genetic algorithm is employed in pre-processing phase and a back propagation neural network is applied in training and testing phase. ASIC has two phases, the first phase handles outliers, noisy data, and missing values to obtain a qualitative target data to generate appropriate attribute reduct sets from the input data using rough computing based genetic algorithm centred on a relative fitness function measure. The succeeding phase of this system involves both training and testing of back propagation neural network classifier on the selected reducts. The model performance is evaluated with widely adopted existing classifiers. The proposed ASIC system for clinical decision support has been tested with breast cancer, fertility diagnosis and heart disease data set from the University of California at Irvine (UCI) machine learning repository. The proposed system outperformed the existing approaches attaining the accuracy rate of 95.33%, 97.61%, and 93.04% for breast cancer, fertility issue and heart disease diagnosis. © 2017, Springer Science+Business Media, LLC.","Clinical decision support; Disease prediction; Feature reduction and hybrid computing; Genetic algorithm; Neural network; Rough set",,2-s2.0-85029931757
"Tayebi M., Naderi M., Mohammadi J., Tayebi M.H.","Comparing different statistical models for assessing Fe-contaminated soils based on VNIR/SWIR spectral data",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032508497&doi=10.1007%2fs12665-017-7058-y&partnerID=40&md5=6eddc22aa68d64b01dced9dc53c51723","Two statistical models including partial least squares regression (PLSR) and principal component regression were comparatively utilized to determine the predictive accuracy of visible–near-infrared and short-wave infrared reflectance spectroscopy in quantifying the Fe concentration in contaminated soils. Two scenarios were applied to select the best model: Scenario I included all wavelengths (400–2450 nm) and Scenario II encompassed characteristic bands of Fe. Pre-processing techniques used to select the best model included: first and second derivatives (FD and SD), multiplicative scatter correction (MSC) and standard normal variate. The abilities of the predictive models were evaluated by splitting soil samples into two random groups (80 and 20%). The first group (80%) was used to evaluate calibration and validation sets by employing the cross‐validation method, and the second group (20%) was applied to test the models. The coefficient of determination (R2), root mean square error and residual prediction deviation were calculated to evaluate the models. Applying Scenario I indicated that the PLSR model with SD pre-processing was a more accurate technique for predicting the Fe concentration, whereas in the Scenario II, the PLSR model with MSC pre-processing had a better performance. Comparing Scenarios I and II indicated that the more reliable models for predicting the soil Fe content could be constructed by the PLSR model with the SD pre-processing techniques and all wavelengths. The modeling results produced by the PLSR model with the SD pre-processing could be used to detect, map and monitor Fe-contaminated soils by proximal and remote sensing in the mining areas. © 2017, Springer-Verlag GmbH Germany.","Fe-contaminated soil; Partial least squares regression (PLSR); Principal component regression (PCR); VNIR/SWIR spectroscopy","Contamination; Forecasting; Infrared devices; Infrared radiation; Iron compounds; Least squares approximations; Mean square error; Principal component analysis; Regression analysis; Remote sensing; Soils; Calibration and validations; Coefficient of determination; Contaminated soils; Multiplicative scatter correction; Partial least squares regressions (PLSR); Principal component regression; Standard normal variates; VNIR/SWIR spectroscopy; Soil pollution",2-s2.0-85032508497
"Manivannan P., Kanimozhiselvi C.S.","Pointwise mutual information based integral classifier for sentiment analysis in cross domain opinion mining",2017,"Journal of Computational and Theoretical Nanoscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032623149&doi=10.1166%2fjctn.2017.6967&partnerID=40&md5=5a5cc335210d57d163be7a66ec040e1d","Sentiment classification is essential for a lot of applications like opinion mining, opinion summarization, contextual advertising and market analysis. The existing sentiment classification used binary classifier for classifying positive or negative sentiment reviews. However, the classification performance was poor since words that present in the source domain may not exist in the target domain. In order to overcome such limitation, the Pointwise Mutual Information Based Integral Classifier (PMI-IC) model is proposed. The PMI-IC model classifies customer reviews on a different domain for efficient sentimental analysis with improved classification accuracy. The PMI-IC model initially performs preprocessing to remove the unwanted or unnecessary and redundant data in review with aiming at reducing the classification time. Next, PMI-IC model constructs a sentiment sensitive thesaurus using reviews of multiple source domains to group words that express similar sentiments in different domains. After that, PMI-IC model expands sentiment features with the aid of additional interrelated features chosen from the sentiment-sensitive thesaurus. Finally, PMI-IC model applies Pointwise Mutual Information based Integral classifier to efficiently classify the customer reviews into positive and negative reviews for sentiment analysis. The PMI-IC model conducts experimental works on parameters such as true positive rate, classification accuracy and classification time. The experimental results showthat thePMI-ICmodel is able to improve the classification accuracy and also reduces the classification time for sentimental analysis when compared to state-of-the-artworks. © 2017 American Scientific Publishers.","Classification; Pointwise Mutual Information Based Integral Classifier.; Preprocessing; Reviews; Sentiment Sensitive Thesaurus; Sentimental Analysis",,2-s2.0-85032623149
"Solak K.C., Tuncay E., Ulusay R.","An investigation on the mechanisms of instabilities and safe design of the south slope at a lignite pit (SW Turkey) based on a sensitivity approach",2017,"Bulletin of Engineering Geology and the Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014018198&doi=10.1007%2fs10064-017-1025-4&partnerID=40&md5=9ceb9dd3269ec1db6a5fcf9cbc5c00e8","Turkey has significant lignite reserves which are generally being extracted using open pit mining methods. The Hüsamlar pit is one of the operated lignite pits in the well-known Mugla lignite province in SW Turkey. Some local failures and one large failure, which caused the evacuation of the Hüsamlar village located next to the slope crest and interruption in coal production, occurred along the south slope of this pit. This paper outlines the results of the field and laboratory geotechnical investigations associated with the causes and mechanisms of the instabilities, and assessments on the possible modifications in the current and planned final slope geometries to improve the stability of the south slope. Since no sufficient data on groundwater conditions in the pit were available, in order to reduce the uncertainty associated with groundwater, different pore pressure ratios (ru) were considered and a sensitivity approach was used in the stability assessments. The back-analyses of the observed instabilities including one or more benches in the overburden indicated that the most critical modes of failure for the south slope are circular and composite sliding surfaces. Although kinematical analyses suggested that structurally controlled failures would not be expected, one local planar failure that occurred in the south slope emphasizes that the possibility of local planar sliding should be considered when the dip of bedding planes locally exceed 20° and pore pressure becomes high. In addition, the back-analyses revealed that ru was probably between 0.3 and 0.4 and the residual shear strength along the bedding planes was critical when slope instabilities occurred along the south slope. The stability assessments for the current and the final south slope, which was planned by the mining organization operating the pit, indicated that some modifications in bench and slope geometries are necessary to achieve a factor of safety of 1.3, which is a commonly used value in open pit practice. In addition, these assessments also suggested that the most critical zone in the overburden was the thinly bedded marl in terms of stability, and at the thickest part of this material (30 m), the overall slope angles satisfying F = 1.3 at ru values of 0.2, 0.3 and 0.4 should be 18°, 17° and 15°, respectively. Except those in the thinly bedded marl, bench widths in the overburden units and coal seam are reduced and steeper slopes with F ≥ 1.3 were achieved. © 2017, Springer-Verlag Berlin Heidelberg.","Back-analysis; Hüsamlar pit; Pore pressure ratio; Sensitivity approach; Slope stability","Coal deposits; Fertilizers; Groundwater; Lignite; Mining; Open pit mining; Pore pressure; Safety factor; Stability; System stability; Back analysis; Geotechnical investigations; Groundwater conditions; Kinematical analysis; Pore pressure ratios; Residual shear strength; Sensitivity approach; Stability assessment; Slope stability; back analysis; design; geotechnical engineering; lignite; open pit mine; overburden; shear strength; slope stability; stability analysis; Turkey",2-s2.0-85014018198
"Logesh R., Subramaniyaswamy V.","A Reliable Point of Interest Recommendation based on Trust Relevancy between Users",2017,"Wireless Personal Communications",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021766245&doi=10.1007%2fs11277-017-4633-1&partnerID=40&md5=a9400ceb14ff84589929d224d0cc7186","Recent development of internet and its applications has become the source for research in service recommendation. Among them, point of interest (POI) recommendation based on user behaviour has enticed a decent attention. Along with quality of recommendations, the utilization of user feedback has grown to be a key part in the POI recommendations. While implementing similarity-based methods in conventional recommender systems, it faces various issues such as trustworthiness, sparsity and cold-start. The commonness and popularity of social network facilitate people to interact with different users and generate massive data such as user relationships, ratings and interactions. Thus, integration of trust relationship of user in location based social network along with their feedback for POI recommendation is the motivation of this work. In this article, we present a POI recommendation method based on trust enhancement in social networks known as social pertinent trust walker (SPTW). Initially, the level of trust between users in social networks is calculated through matrix factorization technique. Then, SPTW with high probability location category algorithm helps to generate POIs as list of recommendations. Experiments on real-world datasets are conducted to evaluate proposed algorithm for accuracy. Results reveal the effectiveness of approach and quality of recommendations is better, when compared to existing algorithms. © 2017, Springer Science+Business Media, LLC.","Computational intelligence; Location based social network; Point-of-interest recommendation; Recommender systems; Social pertinent trust walker; Trust enhancement; User preference mining","Artificial intelligence; Factorization; Location; Recommender systems; Social networking (online); Location-based social networks; Matrix factorizations; Point of interest; Preference mining; Quality of recommendations; Recommendation methods; Service recommendations; Similarity-Based Methods; Behavioral research",2-s2.0-85021766245
"Helal N.A., Ismail R.M., Badr N.L., Mostafa M.G.M.","Leader-based community detection algorithm for social networks",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026743955&doi=10.1002%2fwidm.1213&partnerID=40&md5=09d2c3d30fe689afab82cff86873efc2","Community detection has become a crucial task in social network mining. Detecting communities summarizes interactions between members for gaining deep understanding of interesting characteristics shared between members of the same community. In this research, we propose a novel community detection algorithm for the purpose of revealing and analyzing hidden similar behavior of online users. The proposed algorithm is based mainly on similar members’ actions rather than the structure similarity only for the aim of detecting communities that are closely mapped to the underlying behavioral communities in real social networks. First, leaders of the social network are discovered, then, communities are detected based on those leaders. The idea is grounded on the assumption that communities could be formed around people with great influence. Extensive experiments and analysis show the ability of the proposed algorithm to successfully detect real-world communities with improved accuracy. WIREs Data Mining Knowl Discov 2017, 7:e1213. doi: 10.1002/widm.1213. For further resources related to this article, please visit the WIREs website. © 2017 Wiley Periodicals, Inc.",,"Signal detection; Social networking (online); Community detection; Community detection algorithms; Online users; Real-world; Social network minings; Structure similarity; Population dynamics",2-s2.0-85026743955
"Apitz S.E., Agius S.","Anatomy of a decision II: Potential effects of changes to Tier I chemical approaches in Canadian Disposal at Sea program sediment assessment protocols",2017,"Integrated Environmental Assessment and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026399427&doi=10.1002%2fieam.1955&partnerID=40&md5=ec9418ba8892971d1d35540ef2c3a291","The effects of possible changes to the Canadian 2-tiered assessment framework for dredged material based on outcomes of the 2006 Contaminated Dredged Material Management Decisions Workshop (CDMMD) are evaluated. Expanding on the “data mining” approach described in a previous paper, which focused solely on chemical lines of evidence, the efficacy of Tier 1 approaches (increases to the number of chemical analytes, use of mean hazard quotients, and the use of a screening bioassay) in predicting toxicity are evaluated. Results suggest value in additional work to evaluate the following areas: 1) further expanding minimum chemical requirements, 2) using more advanced approaches for chemical interpretation, and 3) using a screening-level bioassay (e.g., Canadian solid-phase photoluminescent bacteria test) to determine whether it would complement Tier 1 chemistry as well as or better than the solvent-based Microtox™ test method evaluated in the present study. Integr Environ Assess Manag 2017;13:1072–1085. © 2017 The Authors. Integrated Environmental Assessment and Management published by Wiley Periodicals, Inc. on behalf of Society of Environmental Toxicology & Chemistry (SETAC). © 2017 The Authors. Integrated Environmental Assessment and Management published by Wiley Periodicals, Inc. on behalf of Society of Environmental Toxicology & Chemistry (SETAC)","Biological test method; Canadian Disposal at Sea Program; Chemical action lists; Dredged material management; Sediment toxicity","hazardous waste; pollutant; analysis; Canada; ecotoxicology; hazardous waste; pollutant; procedures; risk assessment; sediment; standards; statistics and numerical data; toxicity; waste management; Canada; Ecotoxicology; Environmental Pollutants; Geologic Sediments; Hazardous Waste; Risk Assessment; Waste Management",2-s2.0-85026399427
"Dawed A.Y., Ali A., Zhou K., Pearson E.R., Franks P.W.","Evidence-based prioritisation and enrichment of genes interacting with metformin in type 2 diabetes",2017,"Diabetologia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028329415&doi=10.1007%2fs00125-017-4404-2&partnerID=40&md5=f51a9bec3d326cd7409861415beb146f","Aims/hypothesis: There is an extensive body of literature suggesting the involvement of multiple loci in regulating the action of metformin; most findings lack replication, without which distinguishing true-positive from false-positive findings is difficult. To address this, we undertook evidence-based, multiple data integration to determine the validity of published evidence. Methods: We (1) built a database of published data on gene–metformin interactions using an automated text-mining approach (n = 5963 publications), (2) generated evidence scores for each reported locus, (3) from which a rank-ordered gene set was generated, and (4) determined the extent to which this gene set was enriched for glycaemic response through replication analyses in a well-powered independent genome-wide association study (GWAS) dataset from the Genetics of Diabetes and Audit Research Tayside Study (GoDARTS). Results: From the literature search, seven genes were identified that are related to the clinical outcomes of metformin. Fifteen genes were linked with either metformin pharmacokinetics or pharmacodynamics, and the expression profiles of a further 51 genes were found to be responsive to metformin. Gene-set enrichment analysis consisting of the three sets and two more composite sets derived from the above three showed no significant enrichment in four of the gene sets. However, we detected significant enrichment of genes in the least prioritised category (a gene set in which their expression is affected by metformin) with glycaemic response to metformin (p = 0.03). This gene set includes novel candidate genes such as SLC2A4 (p = 3.24 × 10−04) and G6PC (p = 4.77 × 10−04). Conclusions/interpretation: We have described a semi-automated text-mining and evidence-scoring algorithm that facilitates the organisation and extraction of useful information about gene–drug interactions. We further validated the output of this algorithm in a drug-response GWAS dataset, providing novel candidate loci for gene–metformin interactions. © 2017, The Author(s).","G6PC; Gene-set enrichment; Metformin; SLC2A4; Text-mining; Type 2 diabetes","ampk alpha2 subunit; ATM protein; g6pc protein; glucose 6 phosphatase; glucose transporter 4; mapk1 protein; metformin; mitogen activated protein kinase 1; multidrug and toxin extrusion protein 1; multidrug and toxin extrusion protein 2; organic cation transporter 1; organic cation transporter 2; organic cation transporter 3; plasma membrane monoamine transporter; protein; protein kinase LKB1; sex hormone binding protein; unclassified drug; algorithm; Article; clinical outcome; DNA damage response; drug interaction; evidence based practice; gene interaction; gene set enrichment analysis; genetic analysis; genome-wide association study; human; non insulin dependent diabetes mellitus; pharmacodynamics; priority journal",2-s2.0-85028329415
"Kostakis O., Tatti N., Gionis A.","Discovering recurring activity in temporal networks",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020523856&doi=10.1007%2fs10618-017-0515-0&partnerID=40&md5=f1f2229f667f349599f2f09ebd99074c","Recent advances in data-acquisition technologies have equipped team coaches and sports analysts with the capability of collecting and analyzing detailed data of team activity in the field. It is now possible to monitor a sports event and record information regarding the position of the players in the field, passing the ball, coordinated moves, and so on. In this paper we propose a new method to analyze such team activity data. Our goal is to segment the overall activity stream into a sequence of potentially recurrent modes, which reflect different strategies adopted by a team, and thus, help to analyze and understand team tactics. We model team activity data as a temporal network, that is, a sequence of time-stamped edges that capture interactions between players. We then formulate the problem of identifying a small number of team modes and segmenting the overall timespan so that each segment can be mapped to one of the team modes; hence the set of modes summarizes the overall team activity. We prove that the resulting optimization problem is NP -hard, and we discuss its properties. We then present a number of different algorithms for solving the problem, including an approximation algorithm that is practical only for one mode, as well as heuristic methods based on iterative and greedy approaches. We benchmark the performance of our algorithms on real and synthetic datasets. Of all methods, the iterative algorithm provides the best combination of performance and running time. We demonstrate practical examples of the insights provided by our algorithms when mining real sports-activity data. In addition, we show the applicability of our algorithms on other types of data, such as social networks. © 2017, The Author(s).","Basketball; Dynamic graphs; Football; Handball; Segmentation; Social networks; Sports analytics; Summarising; Temporal networks","Approximation algorithms; Benchmarking; Data acquisition; Heuristic algorithms; Heuristic methods; Image segmentation; Optimization; Problem solving; Social networking (online); Sports; Basketball; Dynamic graph; Football; Handball; Summarising; Temporal networks; Iterative methods",2-s2.0-85020523856
"Goh A.T.C., Zhang Y., Zhang R., Zhang W., Xiao Y.","Evaluating stability of underground entry-type excavations using multivariate adaptive regression splines and logistic regression",2017,"Tunnelling and Underground Space Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026675165&doi=10.1016%2fj.tust.2017.07.013&partnerID=40&md5=4ffe542d22de2bdfaa04468fee899988","The mining industry relies heavily on the use of empirical methods and charts for the design and assessment of entry-type excavations. The commonly adopted empirical design method, commonly referred to as the critical span graph, which was specifically developed for the assessment of rock stability in entry-type excavations, was based on an extensive database of cut and fill mining operations and case histories in Canada. It plots the critical span versus the rock mass rating for the observed case histories and has been widely accepted for an initial span design of cut and fill stopes. Different approaches, either based on classical regression and classification statistical techniques or even the supervised machine learning methods, have been proposed to classify the observed cases into stable, potentially unstable and unstable groups. This paper presents a new assessment approach which combines the use of a multivariate adaptive regression splines (MARS) approach and the logistic regression (LR) method. The proposed MARS_LR model can capture and describe the intrinsic, complex relationship between input descriptors and the dependent response without having to make any assumptions about the underlying relationship. Considering its simplicity in interpretation, predictive accuracy, its data-driven and adaptive nature plus the ability to map the interaction between variables, the use of MARS_LR model in evaluating stability of underground entry-type excavations is promising. © 2017 Elsevier Ltd","Basis function; Entry-type excavations; Limit state function; Logistic regression; Multivariate adaptive regression splines; Stability","Convergence of numerical methods; Excavation; Function evaluation; Learning systems; Splines; Stability; Supervised learning; Assessment approaches; Basis functions; Complex relationships; Limit state functions; Logistic regressions; Multivariate adaptive regression splines; Statistical techniques; Supervised machine learning; Regression analysis; accuracy assessment; design method; excavation; machine learning; mining industry; multivariate analysis; regression analysis; stability analysis; underground construction; Canada",2-s2.0-85026675165
"Price E.J., Bhattacharjee R., Lopez-Montes A., Fraser P.D.","Metabolite profiling of yam (Dioscorea spp.) accessions for use in crop improvement programmes",2017,"Metabolomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031726018&doi=10.1007%2fs11306-017-1279-7&partnerID=40&md5=49eab6c7ae6cc88f431d527c7cf73ebb","Introduction: Ninety-seven percent of yam (Dioscorea spp.) production takes place in low income food deficit countries (LIFDCs) and the crop provides 200 calories a day to approximately 300 million people. Therefore, yams are vital for food security. Yams have high-yield potential and high market value potential yet current breeding of yam is hindered by a lack of genomic information and genetic resources. New tools are needed to modernise breeding strategies and unlock the potential of yam to improve livelihood in LIFDCs. Objectives: Metabolomic screening has been undertaken on a diverse panel of Dioscorea accessions to assess the utility of the approach for advancing breeding strategies in this understudied crop. Methods: Polar and lipophilic extracts from tubers of accessions from the global yam breeding program have been comprehensively profiled via gas chromatography-mass spectrometry. Results: A visual pathway representation of the measured yam tuber metabolome has been delivered as a resource for biochemical evaluation of yam germplasm. Over 200 compounds were routinely measured in tubers, providing a major advance for the chemo-typing of this crop. Core biochemical redundancy concealed trends that were only elucidated following detailed mining of global metabolomics data. Combined analysis on leaf and tuber material identified a subset of metabolites which allow accurate species classification and highlighted the potential of predicting tuber composition from leaf profiles. Metabolic variation was accession-specific and often localised to compound classes, which will aid trait-targeting for metabolite markers. Conclusions: Metabolomics provides a standalone platform with potential to deliver near-future crop gains for yam. The approach compliments the genetic advancements currently underway and integration with other ‘–omics’ studies will deliver a significant advancement to yam breeding strategies. © 2017, The Author(s).","Crop breeding; Dioscorea; Metabolomics; Natural variation; Yam","breeding; crop improvement; germplasm; human; lipophilicity; lowest income group; mass fragmentography; metabolite; metabolome; metabolomics; mining; nonhuman; plant leaf; visual system; yam",2-s2.0-85031726018
"Knobbe A., Orie J., Hofman N., van der Burgh B., Cachucho R.","Sports analytics for professional speed skating",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019727068&doi=10.1007%2fs10618-017-0512-3&partnerID=40&md5=f910eaa9b3acc55418981ab76b3a888d","In elite sports, training schedules are becoming increasingly complex, and a large number of parameters of such schedules need to be tuned to the specific physique of a given athlete. In this paper, we describe how extensive analysis of historical data can help optimise these parameters, and how possible pitfalls of under- and overtraining in the past can be avoided in future schedules. We treat the series of exercises an athlete undergoes as a discrete sequence of attributed events, that can be aggregated in various ways, to capture the many ways in which an athlete can prepare for an important test event. We report on a cooperation with the elite speed skating team LottoNL-Jumbo, who have recorded detailed training data over the last 15 years. The aim of the project was to analyse this potential source of knowledge, and extract actionable and interpretable patterns that can provide input to future improvements in training. We present two alternative techniques to aggregate sequences of exercises into a combined, long-term training effect, one of which based on a sliding window, and one based on a physiological model of how the body responds to exercise. Next, we use both linear modelling and Subgroup Discovery to extract meaningful models of the data. © 2017, The Author(s).","Physiological modelling; Sequence mining; Speed skating; Sports analytics; Subgroup discovery","Physiology; Recreational facilities; Sports; Discrete sequences; Future improvements; Potential sources; Sequence mining; Speed skating; Subgroup discovery; Training effects; Training schedules; Physiological models",2-s2.0-85019727068
"Kolev G.I., Karapandza R.","Out-of-sample equity premium predictability and sample split–invariant inference",2017,"Journal of Banking and Finance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028601557&doi=10.1016%2fj.jbankfin.2016.07.017&partnerID=40&md5=f3ea6a260fdc00a9ea68ac3386ac5770","For a comprehensive set of 21 equity premium predictors we find extreme variation in out-of-sample predictability results depending on the choice of the sample split date. To resolve this issue we propose reporting in graphical form the out-of-sample predictability criteria for every possible sample split, and two out-of-sample tests that are invariant to the sample split choice. We provide Monte Carlo evidence that our bootstrap-based inference is valid. The in-sample, and the sample split invariant out-of-sample mean and maximum tests that we propose, are in broad agreement. Finally we demonstrate how one can construct sample split invariant out-of-sample predictability tests that simultaneously control for data mining across many variables. © 2016 The Author(s)","Bootstrap; Equity premium predictability; Out-of-sample inference; Sample split choice",,2-s2.0-85028601557
"Arana J., Mba-Jonas A., Jankosky C., Lewis P., Moro P.L., Shimabukuro T.T., Cano M.","Reports of Postural Orthostatic Tachycardia Syndrome After Human Papillomavirus Vaccination in the Vaccine Adverse Event Reporting System",2017,"Journal of Adolescent Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031928583&doi=10.1016%2fj.jadohealth.2017.08.004&partnerID=40&md5=9cc2207871c8e6e5602e486dd5d04d0b","Purpose Human papillomavirus (HPV) vaccination prevents infections with HPV strains that cause certain cancers. Reports of postural orthostatic tachycardia syndrome (POTS) following HPV vaccination have raised safety concerns. We reviewed POTS reports submitted to the Vaccine Adverse Event Reporting System (VAERS). Methods We searched the VAERS database for reports of POTS following any type of HPV vaccination (bivalent, quadrivalent, or nonavalent) from June 2006 to August 2015. We reviewed reports and applied established POTS diagnostic criteria. We calculated unadjusted POTS case reporting rates based on HPV vaccine doses distributed and conducted empirical Bayesian data mining to screen for disproportional reporting of POTS following HPV vaccination. Results Among 40,735 VAERS reports following HPV vaccination, we identified 29 POTS reports that fully met diagnostic criteria. Of these, 27 (93.1%) were in females and mean age was 14 years (range 12–32). Median time from vaccination to start of symptoms was 43 days (range 0–407); most (18, 75.0%) had onset between 0 and 90 days. Symptoms frequently reported concomitantly included headache (22, 75.9%) and dizziness (21, 72.4%). Twenty (68.9%) reports documented a history of pre-existing medical conditions, of which chronic fatigue (5, 17.2%), asthma (4, 13.8%), and chronic headache (3, 10.3%) were most common. Approximately one POTS case is reported for every 6.5 million HPV vaccine doses distributed in the United States. No empirical Bayesian data mining safety signals for POTS and HPV vaccination were detected. Conclusions POTS is rarely reported following HPV vaccination. Our review did not detect any unusual or unexpected reporting patterns that would suggest a safety problem. © 2017","Human papillomavirus (HPV); Human papillomavirus (HPV) vaccine; Postural orthostatic tachycardia syndrome (POTS); Vaccine adverse event; Vaccine Adverse Event Reporting System (VAERS); Vaccine safety",,2-s2.0-85031928583
"Petryshyn L., Pełech-Pilichowski T.","On a property of phase correlation and possibilities to reduce thewalsh function system",2017,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994559169&doi=10.1007%2f978-3-319-47208-9_8&partnerID=40&md5=7119851b4cf437819f63f0ce8ca2929d","Data processing algorithms are used for business, industry and public sector to filter input data, calculate values, detect abrupt changes, acquire information from data or to ensure signal consistency. It is an important research area for Big Data processing and processing of data received from Internet of Things. Typically, classical algorithms are exploited, i.e. statistical procedures, data mining techniques and computational intelligence algorithms. Referring to the area of signal processing, applications of mathematical transformation (e.g. Fourier Transform,Walsh-Fourier Transform) of input signals from either domain to the other are promising. They enable to perform complementary analyses and to consider additional signal components, in particular cyclic (periodic) ones (sin- and cos-components). The Walsh function system is a multiplicative group of Rademacher and Gray functions. In its structure, it contains discrete-harmonic, sin-components of the Rademacher functions, and cos-components of the Gray function, as well as discrete-irregular components of theWalsh function. In the paper, the phase interdependence property has been defined, in pairs of a complete Walsh function system. Odd (sin-components) and even (cos-components) Walsh function subsystems were extracted as theoretical and numerical processing databases. A perspective concerning the processing efficiency and digital signal processing is outlined. © Springer International Publishing AG 2017.",,,2-s2.0-84994559169
"Tang B., He H.","GIR-based ensemble sampling approaches for imbalanced learning",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022345045&doi=10.1016%2fj.patcog.2017.06.019&partnerID=40&md5=2a0211ceb5c1d3a2817d9175a39a67ae","This paper presents two adaptive ensemble sampling approaches for imbalanced learning: one is the undersampling-based approach, and the other one is the oversampling-based approach, with the objectives of bias reduction and adaptive learning. Both of these two approaches are based on a novel class imbalance metric, termed generalized imbalance ratio (GIR), instead of the conventional sample size ratio. Specifically, these two sampling-based approaches adaptively split the imbalanced learning problem into multiple balanced learning subproblems in a probabilistic way, which forces the classifiers trained in the subproblems focus on those difficult to learn samples. In each subproblem, several weak classifiers are trained in a boosting manner. A final stronger classifier is further built by combining all these weak classifiers in a bagging manner. Extensive experiments are conducted on real-life UCI imbalanced data sets to evaluate the performance of the proposed methods. The superior performance demonstrates the effectiveness of the proposed methods and indicates wide potential applications in data mining. © 2017 Elsevier Ltd","Adaptive learning; Boosting and bagging; Generalized imbalance ratio; Imbalanced learning; Undersampling and oversampling","Pattern recognition; Software engineering; Adaptive learning; Boosting and bagging; Generalized imbalance ratio; Imbalanced Learning; Over sampling; Education",2-s2.0-85022345045
"Kuska M.T., Brugger A., Thomas S., Wahabzada M., Kersting K., Oerke E.-C., Steiner U., Mahlein A.-K.","Spectral patterns reveal early resistance reactions of barley against Blumeria graminis f. sp. hordei",2017,"Phytopathology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032501541&doi=10.1094%2fPHYTO-04-17-0128-R&partnerID=40&md5=10dfc1a03fada3d1593ab6dd8e621350","Differences in early plant–pathogen interactions are mainly characterized by using destructive methods. Optical sensors are advanced techniques for phenotyping host–pathogen interactions on different scales and for detecting subtle plant resistance responses against pathogens. A microscope with a hyperspectral camera was used to study interactions between Blumeria graminis f. sp. hordei and barley (Hordeum vulgare) genotypes with high susceptibility or resistance due to hypersensitive response (HR) and papilla formation. Qualitative and quantitative assessment of pathogen development was used to explain changes in hyperspectral signatures. Within 48 h after inoculation, genotype-specific changes in the green and red range (500 to 690 nm) and a blue shift of the red-edge inflection point were observed. Manual analysis indicated resistance-specific reflectance patterns from 1 to 3 days after inoculation. These changes could be linked to host plant modifications depending on individual host–pathogen interactions. Retrospective analysis of hyperspectral images revealed spectral characteristics of HR against B. graminis f. sp. hordei. For early HR detection, an advanced data mining approach localized HR spots before they became visible on the RGB images derived from hyperspectral imaging. The link among processes during pathogenesis and host resistance to changes in hyperspectral signatures provide evidence that sensor-based phenotyping is suitable to advance time-consuming and cost-expensive visual rating of plant disease resistances. © 2017 The American Phytopathological Society.",,,2-s2.0-85032501541
"Gastelum Chavira D.A., Leyva Lopez J.C., Solano Noriega J.J., Ahumada Valenzuela O., Alvarez Carrillo P.A.","A credit ranking model for a parafinancial company based on the ELECTRE-III method and a multiobjective evolutionary algorithm",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022020315&doi=10.1016%2fj.asoc.2017.06.021&partnerID=40&md5=de3e1ded52598d4d1285baf73fbd89d5","Credit rating is an assessment performed by lenders or financial institutions to determine a person's creditworthiness based on the proposed terms of the loan. Frequently, these institutions use rating models to obtain estimates for the probabilities of default for their clients (companies, organizations, government, and individuals) and to assess the risk of credit portfolios. Numerous statistical and data mining methods are used to develop such models. In this paper, the potential of a multicriteria decision-aiding approach is studied. As a first step, the proposed methodology models the problem as a multicriteria evaluation process with multiple and in some cases, conflicting dimensions, which are integrated to derive sound recommendation for DMs. The second step of the methodology involves building a multicriteria outranking model based on ELECTRE III method. An evolutionary algorithm is used to exploit the outranking model. The methodology is applied to a small-scale financial institution operating in the agricultural sector. We compare loan applications based on their attributes and the credit profile of the customer or credit applicant. Our methodology offers the flexibility of combining heterogeneous information together with the preferences of decision makers (DMs), generating both relative and fixed rules for selecting the best loan applications among new and existing customers, which is an improvement over traditional methods The results reveal that outranking models are well suited to credit rating, providing good ranking results and suitable understanding on the relative importance of the evaluation criteria. © 2017","Credit ranking model; Evolutionary multiobjective optimization; Multicriteria decision analysis; Rural credit application","Decision making; Decision theory; Multiobjective optimization; Optimization; Rating; Risk assessment; Societies and institutions; Credit application; Evolutionary multiobjective optimization; Heterogeneous information; Multi objective evolutionary algorithms; Multi-criteria decision aiding; Multi-criteria decision analysis; Multi-criteria evaluation; Ranking model; Evolutionary algorithms",2-s2.0-85022020315
"Narożna D., Książkiewicz M., Przysiecka Ł., Króliczak J., Wolko B., Naganowska B., Mądrzak C.J.","Legume isoflavone synthase genes have evolved by whole-genome and local duplications yielding transcriptionally active paralogs",2017,"Plant Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029807851&doi=10.1016%2fj.plantsci.2017.09.007&partnerID=40&md5=cff41bf6525de7126784eb8e56988fb6","Isoflavone synthase (IFS) is the key enzyme of isoflavonoid biosynthesis. IFS genes were identified in numerous species, although their evolutionary patterns have not yet been reconstructed. To address this issue, we performed structural and functional genomic analysis. Narrow leafed lupin, Lupinus angustifolius L., was used as a reference species for the genus, because it has the most developed molecular tools available. Nuclear genome BAC library clones carrying IFS homologs were localized by linkage mapping and fluorescence in situ hybridization in three chromosome pairs. Annotation of BAC, scaffold and transcriptome sequences confirmed the presence of three full-length IFS genes in the genome. Microsynteny analysis and Bayesian inference provided clear evidence that IFS genes in legumes have evolved by lineage-specific whole-genome and tandem duplications. Gene expression profiling and RNA-seq data mining showed that the vast majority of legume IFS copies have maintained their transcriptional activity. L. angustifolius IFS homologs exhibited organ-specific expression patterns similar to those observed in other Papilionoideae. Duplicated lupin IFS homologs retained non-negligible levels of substitutions in conserved motifs, putatively due to positive selection acting during early evolution of the genus, before the whole-genome duplication. Strong purifying selection preserved newly arisen IFS duplicates from further nonsynonymous changes. © 2017 Elsevier B.V.","Chromosomal localization; Gene expression; Genome duplication; Isoflavone synthase; Lupinus angustifolius; Multigene family",,2-s2.0-85029807851
"Byeon H., Jin H., Cho S.","Development of Parkinson’s disease dementia prediction model based on verbal memory, visuospatial memory, and executive function",2017,"Journal of Medical Imaging and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030100396&doi=10.1166%2fjmihi.2017.2196&partnerID=40&md5=556274f3babda5f4c3d237997276b2c1","Despite cognitive impairment in Parkinson’s disease, little is known about specific cognitive functions in Parkinson‘s Disease Dementia (PDD) such as verbal memory and response inhibition. The objectives of this study were to compare the delayed recall and executive functions of language and visual in normal elderly people and people with PDD, develop a PDD prediction model by using a random forest technique based on the results, and provide basic data for detecting PDD in the early stage and intervening PDD. The subjects of this study were 26 PDD patients who received rehabilitation treatment and 35 healthy elderly. Verbal memory, visuospatial memory, inhibition, attention ability, and verbal fluency were measured by Seoul verbal learning test (SLVT), Rey complex figure test (RCFT), Korean color word Stroop test (K-CWST), trail making test, and controlled oral word association test (COWAT), respectively. A prediction model was developed by using a random forest technique and the results were compared with data mining techniques (i.e., a logistic regression and Naïve Bayes). The healthy elderly group had significantly higher performance ability of SLVT-delayed recall, word reading-correct response, color reading-correct response, Trail Making Test and COWAT than PDD group (p < 005). On the contrary, there was no significant difference in RCFT-delayed recall between the two groups. As a PDD prediction model was developed by the random forest technique, major predictors were SVLT, K-CWST-word reading, trail making test, COWAT-semantic, and COWAT-phonetic. The accuracy of the model was 76.1%. The results showed that the random forests model was had a better predictive power than the logistic regression analysis model and the Naïve Bayes model. In the prediction model, the executive functions of the frontal lobe including verbal memory and generative naming ability were significant predictors of PDD. It is necessary to establish optimal diagnostic criteria for diagnosing PDD in the early stage. Copyright © 2017 American Scientific Publishers All rights reserved.","Delayed verbal memory; Delayed visuospatial memory; Parkinson’s disease dementia",,2-s2.0-85030100396
"Betz M.R., Snyder A.","Coal and family through the boom and bust: A look at the coal Industry's impact on marriage and divorce",2017,"Journal of Rural Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030973613&doi=10.1016%2fj.jrurstud.2017.09.011&partnerID=40&md5=ee9b5ca0816e6740b08bee80257c32e7","Rural America has a long relationship with the coal industry. Long-term shifts toward less labor intensive practices in the industry, coupled with policies aimed at reducing carbon emissions have resulted in substantial employment losses in coal communities. While the economic impacts of declining industries in rural America have been documented, less work has been done to investigate the impact of coal employment losses on social outcomes. We address this gap in the literature by assessing the association between county-level measures of coal employment and marriage, divorce, and cohabitation in nonmetro America. We use a novel proprietary data set to isolate the relationship between marital outcome and coal mining from all other types of mining that aggregated in publicly available data sets. Additionally, we compare these relationships across boom and bust periods. We find that after controlling for total employment growth, the presence of coal mining in a county is significantly associated with marital outcomes and these relationships differ across nonmetro and metro areas. We find some evidence that rural areas that typically have more experience with extraction industries display greater resilience to both positive and negative coal industry shocks compared to metropolitan counties. © 2017 Elsevier Ltd",,,2-s2.0-85030973613
"Belachew M.T., Del Buono N.","Robust embedded projective nonnegative matrix factorization for image analysis and feature extraction",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964227427&doi=10.1007%2fs10044-016-0545-z&partnerID=40&md5=7e8e08777d813642708f48cbfc112f5b","Nonnegative matrix factorization (NMF) is an unsupervised learning method for decomposing high-dimensional nonnegative data matrices and extracting basic and intrinsic features. Since image data are described and stored as nonnegative matrices, the mining and analysis process usually involves the use of various NMF strategies. NMF methods have well-known applications in face recognition, image reconstruction, handwritten digit recognition, image denoising and feature extraction. Recently, several projective NMF (P-NMF) methods based on positively constrained projections have been proposed and were found to perform better than the standard NMF approach in some aspects. However, some drawbacks still affect the existing NMF and P-NMF algorithms; these include dense factors, slow convergence, learning poor local features, and low reconstruction accuracy. The aim of this paper is to design algorithms that address the aforementioned issues. In particular, we propose two embedded P-NMF algorithms: the first method combines the alternating least squares (ALS) algorithm with the P-NMF update rules of the Frobenius norm and the second one embeds ALS with the P-NMF update rule of the Kullback–Leibler divergence. To assess the performances of the proposed methods, we conducted various experiments on four well-known data sets of faces. The experimental results reveal that the proposed algorithms outperform other related methods by providing very sparse factors and extracting better localized features. In addition, the empirical studies show that the new methods provide highly orthogonal factors that possess small entropy values. © 2016, Springer-Verlag London.","Embedded projective nonnegative matrix factorization; Entropy; Feature extraction; Orthogonality; Reconstruction accuracy; Sparsity",,2-s2.0-84964227427
"Kruczek P., Obuchowski J., Wylomanska A., Zimroz R.","Cyclic sources extraction from complex multiple-component vibration signal via periodically time varying filter",2017,"Applied Acoustics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020014718&doi=10.1016%2fj.apacoust.2017.05.013&partnerID=40&md5=7cc40e7ad26133a91ab48d42fc852bdd","The problem of local damage detection is widely discussed in the literature. There are many methods which can be applied, however there is still a need for new techniques addressing specific diagnostic issues. In particular, the case of complex multiple-component vibration signal is a challenging problem. In this paper we focus on such a problem related to a gearbox operating in industrial conditions. Our method consists of several stages. First we transform signal to time-frequency domain using spectrogram. Then for each frequency bin we apply a novel procedure which indicates location of cyclic impulses in given time series. This algorithm is based on the periodically distributed local maxima detection and quantification of their significance. Such procedure requires a priori known fault frequency. If the machine might reveal multiple fault, the procedure has to be calculated separately for each fault frequency. A time-varying filter is designed using the indicated local maxima comprised in the score matrix. Then, signals representing each fault frequency are obtained using inverse short-time Fourier transform algorithm. The method is illustrated with application to simulated and real data from complex mining machine - heavy duty gearbox. © 2017 Elsevier Ltd","Local maxima; Rotating machinery; Semi-blind source extraction; Time-frequency adaptive filter","Acoustic surface wave filters; Adaptive filtering; Adaptive filters; Bandpass filters; Blind source separation; Damage detection; Earthmoving machinery; Extraction; Filtration; Frequency domain analysis; Gears; Machinery; Rotating machinery; Detection and quantifications; Industrial conditions; Local maximum; Periodically time-varying; Semi-blind; Short time Fourier transforms; Time frequency; Time frequency domain; Inverse problems",2-s2.0-85020014718
"Matys Grygar T., Elznicová J., Lelková T., Kiss T., Balogh M., Strnad L., Navrátil T.","Sedimentary archive of contamination in the confined channel of the Ohře River, Czech Republic",2017,"Journal of Soils and Sediments",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010733435&doi=10.1007%2fs11368-017-1664-x&partnerID=40&md5=07946149744e80f842681d36df4b9428","Purpose: The Ohře River has received contamination from metal mining and metallurgy (mainly Cu, Pb, Sn, U, Zn) over the previous five centuries. This contamination history has been poorly documented. Contamination has entered the river system in its middle reach, where the channel is incised and bedrock confined, which impedes overbank deposition. Our objective was to locate and describe a sedimentary record in this unfavourable depositional setting. Materials and methods: Three former channel bars that have coalesced with the riverbank were revealed by examination of historical and current maps and a digital terrain model. Manual coring in the bar and in situ (handheld) X-ray fluorescence (XRF) spectroscopy provided data for developing a contamination chemostratigraphy, which was correlated with the mining history in the region. Detailed topographic examination of the bar and valley edge was important to understanding the evolution of one of the bars. Optically stimulated luminescence (OSL) dating was used to verify the timing of deposition. Results and discussion: Handheld XRF for in situ analysis of element composition is efficient for studying contaminated sediment bodies with complex stratigraphy, which require extensive coring and stratigraphic correlation. Despite the unfavourable settings, the channel bars trapped sufficient sediment to produce a record that correlates with the history of contamination in the drainage basin. In the bar studied in greatest detail, we observed a surprising amount of contamination passing through the Ohře River channel (up to 300 mg kg−1 of Cu, 340 mg kg−1 of Pb and 630 mg kg−1 of Sn in fine sand and silt deposits) associated with a pollution climax in the sixteenth and seventeenth centuries. Modern contamination (Hg and U deposited in the nineteenth and twentieth centuries) was entrapped with low efficiency based on comparable concentrations of Hg and U located 90 km downstream. Conclusions: The efficacy of the use of historical maps and detailed fieldwork was demonstrated by identification of unique depositional meso-environments, which are rare in bedrock-confined fluvial systems. The contamination chemostratigraphy of the bar deposits was correlated with the local mining and pollution history and contributed to an understanding of the bar evolution. The approach used in our study may be applicable to other montane rivers with historic ore mining and processing in their basins. © 2017, Springer-Verlag Berlin Heidelberg.","Confined valley; Contamination chemostratigraphy; Historic pollution; In situ XRF analysis; Sediment transfer zone; Stratigraphic correlation","alluvial deposit; bedrock; chemical composition; drainage basin; heavy metal; in situ measurement; river channel; sediment pollution; stratigraphic correlation; Ohre River",2-s2.0-85010733435
"He Z., Zhang S., Teng J., Xiong Y.","A thermo-elastoplastic model for soft rocks considering structure",2017,"Comptes Rendus - Mecanique",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027494965&doi=10.1016%2fj.crme.2017.07.002&partnerID=40&md5=48e2a66e9f4cd8b95fee1f16a4003f39","In the fields of nuclear waste geological deposit, geothermy and deep mining, the effects of temperature on the mechanical behaviors of soft rocks cannot be neglected. Experimental data in the literature also showed that the structure of soft rocks cannot be ignored. Based on the superloading yield surface and the concept of temperature-deduced equivalent stress, a thermo-elastoplastic model for soft rocks is proposed considering the structure. Compared to the superloading yield surface, only one parameter is added, i.e. the linear thermal expansion coefficient. The predicted results and the comparisons with experimental data in the literature show that the proposed model is capable of simultaneously describing heat increase and heat decrease of soft rocks. A stronger initial structure leads to a greater strength of the soft rocks. Heat increase and heat decrease can be converted between each other due to the change of the initial structure of soft rocks. Furthermore, regardless of the heat increase or heat decrease, a larger linear thermal expansion coefficient or a greater temperature always leads to a much rapider degradation of the structure. The degradation trend will be more obvious for the coupled greater values of linear thermal expansion coefficient and temperature. Lastly, compared to heat decrease, the structure will degrade more easily in the case of heat increase. © 2017 Académie des sciences","Elasto-plastic; Linear thermal expansion coefficient; Soft rocks; Structure; Temperature","degradation; elastoplasticity; literature review; radioactive waste; soft rock; temperature; thermal expansion",2-s2.0-85027494965
"Gissi F., Stauber J., Reichelt-Brushett A., Harrison P.L., Jolley D.F.","Inhibition in fertilisation of coral gametes following exposure to nickel and copper",2017,"Ecotoxicology and Environmental Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022086837&doi=10.1016%2fj.ecoenv.2017.07.009&partnerID=40&md5=61f2106b71f0d718d543284c9453caf4","The mining and production of nickel in tropical regions have the potential to impact on ecologically valuable tropical marine ecosystems. Currently, few data exist to assess the risks of nickel exposure to tropical ecosystems and to derive ecologically relevant water quality guidelines. In particular, data are lacking for keystone species such as scleractinian corals, which create the complex structural reef habitats that support many other marine species. As part of a larger study developing risk assessment tools for nickel in the tropical Asia-Pacific region, we investigated the toxicity of nickel on fertilisation success in three species of scleractinian corals: Acropora aspera, Acropora digitifera and Platygyra daedalea. In the literature, more data are available on the effects of copper on coral fertilisation, so to allow for comparisons with past studies, the toxicity of copper to A. aspera and P. daedalea was also determined. Overall, copper was more toxic than nickel to the fertilisation success of the species tested. Acropora aspera was the most sensitive species to nickel (NOEC < 280 µg Ni/L), followed by A. digitifera with an EC10 of 2000 µg Ni/L and P. daedalea (EC10 > 4610 µg Ni/L). Acropora aspera was also the more sensitive species to copper with an EC10 of 5.8 µg Cu/L. The EC10 for P. daedalea was 16 µg Cu/L, similar to previous studies. This is the first time that the toxicity of nickel on fertilisation success in Acropora species has been reported, and thus provides valuable data that can contribute to the development of reliable water quality guidelines for nickel in tropical marine waters. © 2017","Coral reefs; Indo-Pacific; Metals; Risk assessment; Tropical marine ecotoxicology","copper; nickel; copper; coral reef; ecotoxicology; fertilization (reproduction); gamete; inhibition; nickel; risk assessment; toxicity; Acropora; Acropora aspera; Acropora digitifera; Article; chemical analysis; Cnidaria; controlled study; coral reef; environmental exposure; fertilization; gamete; nonhuman; Platygyra daedalea; quality control; reproductive toxicity; risk assessment; toxicity testing; Pacific Ocean; Pacific Rim; Acropora; Acropora aspera; Acropora digitifera; Anthozoa; Platygyra daedalea; Scleractinia",2-s2.0-85022086837
"Shafaghati L., Razaghi-Moghadam Z., Mohammadnejad J.","A Systems Biology Approach to Understanding Alcoholic Liver Disease Molecular Mechanism: The Development of Static and Dynamic Models",2017,"Bulletin of Mathematical Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028555314&doi=10.1007%2fs11538-017-0336-8&partnerID=40&md5=4c56787eab40e133ddca2d1073ea8c86","Alcoholic liver disease (ALD) is a complex disease characterized by damages to the liver and is the consequence of excessive alcohol consumption over years. Since this disease is associated with several pathway failures, pathway reconstruction and network analysis are likely to explicit the molecular basis of the disease. To this aim, in this paper, a network medicine approach was employed to integrate interactome (protein–protein interaction and signaling pathways) and transcriptome data to reconstruct both a static network of ALD and a dynamic model for it. Several data sources were exploited to assemble a set of ALD-associated genes which further was used for network reconstruction. Moreover, a comprehensive literature mining reveals that there are four signaling pathways with crosstalk (TLR4, NF- κ B, MAPK and Apoptosis) which play a major role in ALD. These four pathways were exploited to reconstruct a dynamic model of ALD. The results assure that these two models are consistent with a number of experimental observations. The static network of ALD and its dynamic model are the first models provided for ALD which offer potentially valuable information for researchers in this field. © 2017, Society for Mathematical Biology.","Alcoholic hepatitis; Alcoholic liver disease; Biological networks; Mathematical modeling",,2-s2.0-85028555314
"Luo Y., Thompson W.K., Herr T.M., Zeng Z., Berendsen M.A., Jonnalagadda S.R., Carson M.B., Starren J.","Natural Language Processing for EHR-Based Pharmacovigilance: A Structured Review",2017,"Drug Safety",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021117594&doi=10.1007%2fs40264-017-0558-6&partnerID=40&md5=1c9e112cd49dc5054ee9c25a3ecb5230","The goal of pharmacovigilance is to detect, monitor, characterize and prevent adverse drug events (ADEs) with pharmaceutical products. This article is a comprehensive structured review of recent advances in applying natural language processing (NLP) to electronic health record (EHR) narratives for pharmacovigilance. We review methods of varying complexity and problem focus, summarize the current state-of-the-art in methodology advancement, discuss limitations and point out several promising future directions. The ability to accurately capture both semantic and syntactic structures in clinical narratives becomes increasingly critical to enable efficient and accurate ADE detection. Significant progress has been made in algorithm development and resource construction since 2000. Since 2012, statistical analysis and machine learning methods have gained traction in automation of ADE mining from EHR narratives. Current state-of-the-art methods for NLP-based ADE detection from EHRs show promise regarding their integration into production pharmacovigilance systems. In addition, integrating multifaceted, heterogeneous data sources has shown promise in improving ADE detection and has become increasingly adopted. On the other hand, challenges and opportunities remain across the frontier of NLP application to EHR-based pharmacovigilance, including proper characterization of ADE context, differentiation between off- and on-label drug-use ADEs, recognition of the importance of polypharmacy-induced ADEs, better integration of heterogeneous data sources, creation of shared corpora, and organization of shared-task challenges to advance the state-of-the-art. © 2017, Springer International Publishing AG.",,"drug; adverse drug reaction; drug surveillance program; electronic health record; human; information; machine learning; methodology; natural language processing; off label drug use; polypharmacy; priority journal; Review; statistical analysis; symbolism",2-s2.0-85021117594
"Katz G., Rokach L.","Wikiometrics: a Wikipedia based ranking system",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009929187&doi=10.1007%2fs11280-016-0427-8&partnerID=40&md5=d9c3070598f0146875e79eb9d9ed2b77","We present a new concept—Wikiometrics—the derivation of metrics and indicators from Wikipedia. Wikipedia provides an accurate representation of the real world due to its size, structure, editing policy and popularity. We demonstrate an innovative “mining” methodology, where different elements of Wikipedia – content, structure, editorial actions and reader reviews – are used to rank items in a manner which is by no means inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world ranking problems: top world universities and academic journals. Our proposed ranking methods were compared to leading and widely accepted benchmarks, and were found to be extremely correlative but with the advantage of the data being publically available. © 2017, Springer Science+Business Media New York.","Ranking; Wikipedia","Hardware; Academic journal; nocv1; Ranking; Ranking methods; Ranking problems; Ranking system; Real-world; Wikipedia; World Wide Web",2-s2.0-85009929187
"Willscher S., Schaum M., Goldammer J., Franke M., Kuehn D., Ihling H., Schaarschmidt T.","Environmental biogeochemical characterization of a lignite coal spoil and overburden site in Central Germany",2017,"Hydrometallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028003413&doi=10.1016%2fj.hydromet.2017.08.008&partnerID=40&md5=24dcfc03f6e58ea5ec74ba2e73af3e89","During the 20th century lignite coal was an important resource for energy and chemical production in Germany. The legacy of the extended open pit lignite coal mining consists of spacious coal spoil areas and the occurrence of large and numerous pit lakes. Various approaches for the remediation of lignite spoil and overburden substrates have been carried out for the last 50 years. The results of former remediation can currently be evaluated by the long-term effects. It is important to learn from the success or problems of the former remediation activities, and to improve these methods for a successful current or future application in mining processes. Seepage waters from coal spoil sites can impact billions of m3 of groundwater in affected areas over a very long time horizon (eternity impacts). A coal spoil site in Central Germany with cohesive soil substrates and layers of carbonaceous drift clays was studied, and biogeochemical parameters of this spoil substrate were characterized. The investigated site partially had a remediated topsoil layer for 40 years, and over this time agricultural cultivation of different plants (grains, legumes, Pocaceae) has been performed. The counts of different microbial populations at several locations and in spatial distribution in the coal spoil substrate were investigated, and geochemical data of the coal spoil substrate were measured. The impact of microbial and geochemical processes on ground- and surface water was evaluated. The measurement results were compared with previous data of microbial communities in sandy substrates and their impact to the environment. Finally, the success of long-term remediation of such sites, the influence onto microbial life and impact to the environment were compared and discussed. © 2017 Elsevier B.V.","Acid mine drainage formation; Biodegradation of coal humic matter; Impact on ground and surface water; Lignite coal spoil area; Microbial counts; Microbial sulfate reduction; Remediation",,2-s2.0-85028003413
"Wei Y.-M., Kang J.-N., Yu B.-Y., Liao H., Du Y.-F.","A dynamic forward-citation full path model for technology monitoring: An empirical study from shale gas industry",2017,"Applied Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027560341&doi=10.1016%2fj.apenergy.2017.08.121&partnerID=40&md5=7ba954c5a3170eaa24ce47f72181b0c4","The utilization of shale gas has become one of the important options to transit into low-carbon economy in the world and its vigorous development relies on successful technology revolution to a great extent. Based on patent data, this paper analyzes the development trends and the status quo of technical innovation of shale gas quantitatively by means of patent maps. A new dynamic model named Forward-Citation Full Path (FCFP) is investigated to identify the key development paths in technology clusters and monitor potential breakthrough technologies on those key paths. Then we employ topic modeling and text mining for patent abstracts to explore the potential promising topics with high innovation activeness in aid of providing specific references for development and foresight of the shale gas technology. The results show that: (1) The patent center of shale gas has been transferring from North American to the Asia-Pacific region and the technological innovation is mainly driven by preferential tax policy and loose environmental regimes. (2) Current hotspots of shale gas technology are production technique including stimulation treatments, environmental protection technology of fracturing fluid and geological prospecting technology. (3) There are five potential topics with high innovation activeness identified by topic modeling and text mining which are synthetic carbon oxide, hydraulic fracturing, fracturing propping agents, horizontal well, and technologies of reservoir exploration and modeling. (4) By means of visualization of technology clusters, it is found that promising technologies are refined simulation technology for shale gas exploration, multi-interval fracturing techniques in horizontal wells with deep pay zones, water treatment and environmental protection technology in shale gas production. (5) The suggested dynamic FCFP model can effectively identify the key development paths and monitor potential breakthrough technology of shale gas. © 2017 Elsevier Ltd","Patent map; Path identification; Shale gas; Technology foresight; Technology monitoring",,2-s2.0-85027560341
"Verdon-Kidd D.C., Hancock G.R., Lowry J.B.","A 507-year rainfall and runoff reconstruction for the Monsoonal North West, Australia derived from remote paleoclimate archives",2017,"Global and Planetary Change",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029804250&doi=10.1016%2fj.gloplacha.2017.09.003&partnerID=40&md5=21b109d38ccd1a8b544c8140dff82e48","The Monsoonal North West (MNW) region of Australia faces a number of challenges adapting to anthropogenic climate change. These have the potential to impact on a range of industries, including agricultural, pastoral, mining and tourism. However future changes to rainfall regimes remain uncertain due to the inability of Global Climate Models to adequately capture the tropical weather/climate processes that are known to be important for this region. Compounding this is the brevity of the instrumental rainfall record for the MNW, which is unlikely to represent the full range of climatic variability. One avenue for addressing this issue (the focus of this paper) is to identify sources of paleoclimate information that can be used to reconstruct a plausible pre-instrumental rainfall history for the MNW. Adopting this approach we find that, even in the absence of local sources of paleoclimate data at a suitable temporal resolution, remote paleoclimate records can resolve 25% of the annual variability observed in the instrumental rainfall record. Importantly, the 507-year rainfall reconstruction developed using the remote proxies displays longer and more intense wet and dry periods than observed during the most recent ~ 100 years. For example, the maximum number of consecutive years of below (above) average rainfall is 90% (40%) higher in the rainfall reconstruction than during the instrumental period. Further, implications for flood and drought risk are studied via a simple GR1A rainfall runoff model, which again highlights the likelihood of extremes greater than that observed in the limited instrumental record, consistent with previous paleoclimate studies elsewhere in Australia. Importantly, this research can assist in informing climate related risks to infrastructure, agriculture and mining, and the method can readily be applied to other regions in the MNW and beyond. © 2017 Elsevier B.V.","Droughts; Floods; MNW; Northern Territory; Paleoclimate; Rainfall; Runoff","Agriculture; Climate models; Drought; Floods; Rain; Runoff; Anthropogenic climate changes; Climate related risks; Global climate model; Northern territories; Paleoclimate studies; Paleoclimates; Rainfall and runoffs; Rainfall-runoff modeling; Climate change; anthropogenic effect; climate modeling; drought; flood; global climate; paleoclimate; rainfall; rainfall-runoff modeling; reconstruction; territory; Australia",2-s2.0-85029804250
"Vasev N.","Governing energy while neglecting health – The case of Poland",2017,"Health Policy",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029791199&doi=10.1016%2fj.healthpol.2017.09.008&partnerID=40&md5=aceecec14688c731f8f73fb97d99969d","The present article discusses Poland's continued reliance on coal power and the consequent impacts on public health. Concrete aspects of the energy infrastructure and political priorities are shown to compromise as compromising public governance and leading to deteriorated health standards among the general population. To make this case, this study juxtaposes the most recent developments in the Polish energy sector with current measures in EU energy policy and reforms in other EU Member States. Special attention is paid to developments in Poland following the political shift in October 2015, when a new government came to power. The ruling conservative party's direct involvement in the management of the mining and utility companies and its strong political ties to miners’ unions are particularly discussed. Theoretically, the article relies on the TAPIC framework for governance. The framework rests on five integral principles of good governance: Transparency, Accountability, Participation, Integrity and Capacity; TAPIC allows scholars to study the impact of governance on public health in any policy area. Methodologically, this study relies on secondary sources, including academic publications, national and international reports, and statistical data on a range of energy and health factors in Poland and Europe. © 2017 The Author","Economic and health costs; Health impacts; Intersectoral governance; Over-reliance on coal; Polish energy sector","attention; government; human; miner; Poland; public health",2-s2.0-85029791199
"Hernández-Vargas J.J., Martiny B.M., Morán-Zenteno D.J., Pérez-Gutiérrez R., López-Martínez M.","40Ar/39Ar geochronology and revised stratigraphy of the late Eocene Taxco volcanic field, southern Mexico",2017,"Journal of South American Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026681791&doi=10.1016%2fj.jsames.2017.07.005&partnerID=40&md5=9ca7d9da201f58492d9ebedf8147610e","The late Eocene Taxco volcanic succession constitutes an important record of explosive silicic volcanism in the north-central Sierra Madre del Sur of southern Mexico. Detailed stratigraphic studies with age dating of individual units in this area were lacking in spite of it being an important mining district. Re-examination of the stratigraphy together with geochronologic studies were used to define the eruptive style and constrain the age of the main episode of silicic activity. Major element data show a rhyolitic composition for all the units analyzed. The volcanic succession records silicic explosive eruptions during which ash-flows, surge and ash-fall deposits, lava flows and domes were emplaced. Earliest activity was contemporary with the end of sediment accumulation of the continental Balsas Group. The first main episode of volcanic activity produced massive, moderately welded, crystal-poor lithic ignimbrites and ash-fall deposits, which evolved into crystal-rich, densely welded ignimbrites with flattened pumice clasts (San Gregorio ignimbrite) thought to represent erupted mush related to caldera collapse during emptying of the magma chamber. A second episode of non-welded, vapor-phase, indurated ignimbrites and surge deposits (Sombrerito ignimbrite), which are rich in pumice but crystal-poor, suggest volatile-rich magmatism and replenishment of the magma chamber by evolved magmas. Rhyolite emplaced in volcanic units of both episodes point to volatile-poor magmas during which rhyolite flows and domes formed. 40Ar/39Ar dating of the units, from the crystal-rich San Gregorio ignimbrite near the base of the succession to rhyolites at the top, has constrained the age of the Taxco volcanic field to a relatively short period between ∼36 and 34.5 Ma. Most of the sanidine ages obtained are indistinguishable within error: the densely welded San Gregorio ignimbrite (34.75 ± 0.26, 35.22 ± 0.26 Ma) and the overlying Peral ignimbrite (35.29 ± 0.41, 35.68 ± 0.20). Plagioclase of the Tenería Formation rhyolites gave the same age within error (34.45 ± 1.10, 34.94 ± 0.34 Ma), documenting the end of the main volcanic episodes. A younger plagioclase age of 30.89 ± 1.45 Ma of a dome near Tetipac gives evidence of later silicic magmatism in the area. The source of the ignimbrites of the Taxco region is inferred to be located in the Tenería area, where abundant rhyolite dome structures seem to obliterate the main caldera structure. Differences in age, mineralogy and geochemistry between the Taxco succession and nearby volcanic zones where caldera structures have been identified preclude sources in these areas. © 2017 Elsevier Ltd","Caldera; Ignimbrite; Late Eocene; Sierra Madre del Sur; Southern Mexico","argon-argon dating; caldera; Eocene; explosive volcanism; geochronology; ignimbrite; lava flow; stratigraphy; volcanic eruption; Mexico [North America]; Sierra Madre del Sur; Sierra Madre [Mexico]",2-s2.0-85026681791
"Fabbri P., Pola M., Piccinini L., Zampieri D., Roghel A., Libera N.D.","Monitoring, utilization and sustainable development of a low-temperature geothermal resource: A case study of the Euganean Geothermal Field (NE, Italy)",2017,"Geothermics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030627653&doi=10.1016%2fj.geothermics.2017.07.002&partnerID=40&md5=b2dd7ae69c2f574d92723f7c189dd771","The Euganean Geothermal Field (EGF) and its thermal water (temperature from 63 °C to 87 °C) represent an important environmental and economic resource for the Veneto Region (NE Italy). Approximately 14.7 × 106 m3 of hot water were exploited in 2015 through 142 boreholes from rocky aquifers located at different depths. The water is mainly used for balneotherapy feeding approximately 240 pools. Hundreds of thousands of tourists visit the spa facilities of the EGF every year, producing a huge income for the regional economy. The Euganean thermal resource suffered a significant anthropic impact during the 20th century related to the development of the local tourism industry. Hydrogeological data and information about the utilization of the resource spanning the century are analyzed to evaluate this impact with the aim of assessing the sustainable utilization of the thermal resource. In particular, the potentiometric levels of the thermal aquifers are affected by seasonal variations (i.e., decreases during the spring and autumn, recoveries during the winter and summer) induced by the different flow rates related to the tourist seasons. Similarly, the historical reconstruction of the level shows a decrease during the initial and middle parts of the 20th century followed by a gradual recovery up to the present. The reduction of the level was related to the growth of the tourism industry attested by the increase in exploitation, wells, mining claims and tourists. The limitation of the flow rate and its continuous monitoring have produced the observed recovery since the 1990s. The performed reconstruction suggests that the present flow rate (approximately 14 × 106 m3/y) produces an acceptable drawdown preserving the Euganean thermal resource for future generations and maintaining a constant income for the regional economy. This work attests that thermal resources for balneotherapeutic purposes could be affected by overexploitation and depletion. Therefore, their sustainable utilization has to be achieved through specific management policies, preserving their important environmental and socio-economic values. © 2017 Elsevier Ltd","Anthropic impact; Geothermal resource monitoring; Overexploitation; Po plain foredeep; Potentiometric level decline; Sustainable management","Aquifers; Economics; Geothermal fields; Hydrogeology; Potentiometers (electric measuring instruments); Recovery; Regional planning; Sustainable development; Temperature; Water levels; Anthropic impact; Geothermal resources; Overexploitation; Po plain; Potentiometric; Sustainable management; Geothermal water resources; alternative energy; exploitation; geothermal system; low temperature; monitoring; resource development; sustainable development; Italy; Po Plain",2-s2.0-85030627653
"Yao H., Wang Q., Wang L., Zhang P., Li M., Liu Y.","An Intrusion Detection Framework Based on Hybrid Multi-Level Data Mining",2017,"International Journal of Parallel Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032691418&doi=10.1007%2fs10766-017-0537-7&partnerID=40&md5=672f4810772a2707eaa1471150fc338d","With the dramatic opening-up of network, network security becomes a severe social problem with the rapid development of network technology. Intrusion Detection System (IDS) is an innovative and proactive network security technology, which becomes a hot topic in both industry and academia in recent years. There are four main characteristics of intrusion data that affect the performance of IDS including multicomponent, data imbalance, time-varying and unknown attacks. We propose a novel IDS framework called HMLD to address these issues, which is an exquisite designed framework based on Hybrid Multi-Level Data Mining. In this paper, we use KDDCUP99 dataset to evaluate the performance of HMLD. The experimental results show that HMLD can reach 96.70% accuracy which is nearly 1% higher than the recent proposed optimal algorithm SVM+ELM+Modified K-Means. In details, HMLD greatly increased the detection accuracy of DoS attacks and R2L attacks. © 2017 Springer Science+Business Media, LLC","Data engineering; Intrusion detection system; KDDCUP99; Machine learning; Multi-level","Computer crime; Data mining; Denial-of-service attack; Intrusion detection; Learning systems; Mercury (metal); Data engineering; Detection accuracy; Intrusion Detection Systems; KDDCUP99; Multilevels; Network technologies; Optimal algorithm; Proactive networks; Network security",2-s2.0-85032691418
"Bakhshipour A., Sanaeifar A., Payman S.H., de la Guardia M.","Evaluation of Data Mining Strategies for Classification of Black Tea Based on Image-Based Features",2017,"Food Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032710316&doi=10.1007%2fs12161-017-1075-z&partnerID=40&md5=d637dd235f5a22cdb9620d5b732ce32f","In this study, a new procedure based on computer vision was developed for qualitative classification of black tea. Images of 240 samples from four different classes of black tea, including Orange Pekoe (OP), Flowery Orange Pekoe (FOP), Flowery Broken Orange Pekoe (FBOP), and Pekoe Dust One (PD-ONE), were acquired and processed using a computer vision system. Eighteen color features, 13 gray-image texture features, and 52 wavelet texture features were extracted and assessed. Two common heuristic feature selection methods: correlation-based feature selection (CFS) and principal component analysis (PCA), were used for selecting the most significant features. Seven of the primary features were selected by CFS as the most relevant ones, while PCA converted the original variables into 11 independent components. These final discriminatory vectors were evaluated by using four different classification methods including decision tree (DT), support vector machine (SVM), Bayesian network (BN), and artificial neural networks (ANN) to predict the qualitative category of tea samples. Among the studied classifiers, the ANN with 7–10–4 topology developed by CFS-selected features provided the best classifier with a classification rate of 96.25%. The other methods assayed provided slightly lower accuracies than ANN from 86.25% for BN till 87.50% for SVM and 88.75% for DT. In all the cases, the accuracy of the classifiers increased when using the CFS-selected features as input variables in front of PCA obtained ones. It can be concluded that image-based features are strong characterizing factors which can be effectively applied for tea quality evaluation. © 2017 Springer Science+Business Media, LLC","ANN; Data mining; Image-based features; Qualitative classification; Wavelet","Bayesian networks; Citrus fruits; Computer vision; Correlation methods; Data mining; Decision trees; Heuristic methods; Image classification; Image processing; Image texture; Neural networks; Principal component analysis; Quality control; Support vector machines; Classification methods; Computer vision system; Correlation based feature selections (CFS); Feature selection methods; Image-based features; Independent components; Qualitative classification; Wavelet; Classification (of information)",2-s2.0-85032710316
"Angelov P.P., Gu X., Principe J.","Autonomous Learning Multi-Model Systems from Data Streams",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032800380&doi=10.1109%2fTFUZZ.2017.2769039&partnerID=40&md5=a1468fc2f41bd659804d610ddacde420","In this paper, an approach to autonomous learning of a multi-model system from streaming data, named ALMMo, is proposed. The proposed approach is generic and can easily be applied also to probabilistic or other types of local models forming multi-model systems. It is fully data-driven and its structure is decided by the nonparametric data clouds extracted from the empirically observed data without making any prior assumptions concerning data distribution and other data properties. All meta-parameters of the proposed system are obtained directly from the data and can be updated recursively, which improves memory- and calculation-efficiency of the proposed algorithm. The structural evolution mechanism and online data cloud quality monitoring mechanism of the ALMMo system largely enhance the ability of handling shifts and/or drifts in the streaming data pattern. Numerical examples of the use of ALMMo system for streaming data analytics, classification and prediction are presented as a proof of the proposed concept. IEEE","AnYa type fuzzy rule-based system; autonomous learning systems; classification; Clouds; Computational modeling; Data analysis; data clouds; Data models; Empirical Data Analytics; Learning systems; Meteorology; nonparametric; prediction; Wavelet transforms","Classification (of information); Clouds; Data reduction; Data structures; Forecasting; Fuzzy inference; Learning systems; Meteorology; Wavelet transforms; Autonomous learning; Computational model; Data clouds; Empirical data; Non-parametric; Data mining",2-s2.0-85032800380
"Aguilar J., Garcia G.","An Adaptive Intelligent Management System of Advertising for Social Networks: A Case Study of Facebook",2017,"IEEE Transactions on Computational Social Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032821637&doi=10.1109%2fTCSS.2017.2759188&partnerID=40&md5=2012540445292509e434859e7acbf2f3","At present, social networks are strongly used for personal or professional purposes. The social networks have accumulated a large amount of information, which can be used in different ways. One of such possible purposes is advertising. In particular, when a company pays for advertising in a social network, it wants to get an ideal deployment of the ads on the network. The ideal behavior aims at reducing the high costs to pay, the number of hours to design advertising, the difficulty in the creation of ads by lots, among other things. In particular, the optimization of the production of advertising on social networks, based on the performance of the ads, is not an easy task. If we make an incorrect design, this can result in losses for a company. In this sense, this paper presents an intelligent system of management of social network advertising, based on data mining techniques, to automatically produce ads. In addition, we test our adaptive mechanism of automated production (generation) of online advertising on Facebook. Our system carries out automatic modifications and improvements of ads. Facebook is one of the more popular social networks, being widely used by companies for advertising, in order to create contents attracting the attention of users. IEEE","Advertising; Companies; Data mining; Decision support systems; Facebook; Facebook; Intelligent systems; intelligent systems; online advertising; Production; social networks","Artificial intelligence; Data mining; Decision support systems; Industry; Information management; Intelligent systems; Marketing; Online systems; Production; Adaptive mechanism; Automated productions; Facebook; High costs; Ideal behavior; Intelligent management systems; Large amounts; Online advertising; Social networking (online)",2-s2.0-85032821637
"He P., Nakano T., Mao Y., Pietro L., Liu Q., Yang K.","Stochastic Channel Switching of Frequency-encoded Signals in Molecular Communication Networks",2017,"IEEE Communications Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032736632&doi=10.1109%2fLCOMM.2017.2768537&partnerID=40&md5=40e77dc72f6ed99ff7ba54284141d99a","This paper investigates the impact of noise on the functionality of channel switches in molecular communication networks. The channel switches considered in this paper are designed based on a set of biological cells that propagate frequency-encoded Ca2&#x002B; signals through gap junction channels. First, we develop a stochastic and generalized model of Ca2&#x002B; signaling considering three types of noise, internal noise, external noise and gating noise of gap junction channels. Second, we develop a method based on spectral analysis to determine whether cells extract information from frequency-encoded Ca2&#x002B; signals. Third, we examine numerically whether channel switches propagate Ca2&#x002B; signals to selected cells in the presence of noise, and show how noise degrades the performance of channel switches. IEEE","Calcium; Data mining; Junctions; Mathematical model; Permeability; Spectral analysis; Stochastic processes","Calcium; Data mining; Mathematical models; Mechanical permeability; Network coding; Random processes; Spectrum analysis; Stochastic models; Waveguide junctions; Biological cells; Channel switches; Channel switching; Extract informations; Gap junction channels; Generalized models; Impact of noise; Molecular communication; Stochastic systems",2-s2.0-85032736632
"Bao Y.P., Li X., Wang M.","A novel method for endpoint temperature prediction in RH",2017,"Ironmaking and Steelmaking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032462722&doi=10.1080%2f03019233.2017.1392104&partnerID=40&md5=314bb34533c454f37102c3c491da95f3","Based on Cloud Model, a novel method was proposed to predict the endpoint temperature in Ruhrstahl Heraeus (RH) for Interstitial-free (IF) steel production, considering the starting temperature, scrap and refining cycle. 300 sets of RH production data was collected, mined and reasoned by Cloud Model. The prediction results of the Cloud Model are compared with BP neural network methods. The final results show that in the error scope from −10 to 10°C, Cloud Model acquired the 93.32% hit rate, BP neural network acquired the 89.33% hit rate. Compared with the BP neural network, the Cloud Model has higher accuracy and stronger generalisation ability. © 2017 Institute of Materials, Minerals and Mining","cloud model; endpoint temperature; RH","Forecasting; Neural networks; Rhodium; Steel scrap; Steelmaking; BP neural networks; Cloud modeling; End-point temperature; Generalisation; Hit rate; Interstitial free steel; Production data; Cloud computing",2-s2.0-85032462722
"Eidsvik J., Martinelli G., Bhattacharjya D.","Sequential information gathering schemes for spatial risk and decision analysis applications",2017,"Stochastic Environmental Research and Risk Assessment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032688048&doi=10.1007%2fs00477-017-1476-y&partnerID=40&md5=41f3891806a679b3081401808d80c25a","Several risk and decision analysis applications are characterized by spatial elements: there are spatially dependent uncertain variables of interest, decisions are made at spatial locations, and there are opportunities for spatial data acquisition. Spatial dependence implies that the data gathered at one coordinate could inform and assist a decision maker at other locations as well, and one should account for this learning effect when analyzing and comparing information gathering schemes. In this paper, we present concepts and methods for evaluating sequential information gathering schemes in spatial decision situations. Static and sequential information gathering schemes are outlined using the decision theoretic notion of value of information, and we use heuristics for approximating the value of sequential information in large-size spatial applications. We illustrate the concepts using a Bayesian network example motivated from risks associated with CO2 sequestration. We present a case study from mining where there are risks of rock hazard in the tunnels, and information about the spatial distribution of joints in the rocks may lead to a better allocation of resources for choosing rock reinforcement locations. In this application, the spatial variables are modeled by a Gaussian process. In both examples there can be large values associated with adaptive information gathering. © 2017 Springer-Verlag GmbH Germany","Adaptive testing; Bayesian networks; Gaussian processes; Sequential information; Spatial risk analysis; Spatial statistics; Value of information","Barium compounds; Bayesian networks; Data acquisition; Decision making; Gaussian distribution; Gaussian noise (electronic); Location; Risk analysis; Risk assessment; Statistical tests; Adaptive testing; Gaussian Processes; Sequential information; Spatial risk analysis; Spatial statistics; Value of information; Spatial variables measurement",2-s2.0-85032688048
"Li J., Zakharov Y.V., Henson B.","Multibranch Autocorrelation Method for Doppler Estimation in Underwater Acoustic Channels",2017,"IEEE Journal of Oceanic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032745022&doi=10.1109%2fJOE.2017.2761478&partnerID=40&md5=b42ac4d1ea969aa9cf755747cef52fdb","In underwater acoustic (UWA) communications, Doppler estimation is one of the major stages in a receiver. Two Doppler estimation methods are often used: the cross-ambiguity function (CAF) method and the single-branch autocorrelation (SBA) method. The former results in accurate estimation but with a high complexity, whereas the latter is less complicated but also less accurate. In this paper, we propose and investigate a multibranch autocorrelation (MBA) Doppler estimation method. The proposed method can be used in communication systems with periodically transmitted pilot signals or repetitive data transmission. For comparison of the Doppler estimation methods, we investigate an orthogonal frequency-division multiplexing (OFDM) communication system in multiple dynamic scenarios using the Waymark simulator, allowing virtual UWA signal transmission between moving transmitter and receiver. For the comparison, we also use the OFDM signals recorded in a sea trial. The comparison shows that the receiver with the proposed MBA Doppler estimation method outperforms the receiver with the SBA method and its detection performance is close to that of the receiver with the CAF method, but with a significantly lower complexity. IEEE","Ambiguity function; autocorrelation; Complexity theory; Correlation; Delays; Doppler effect; Doppler estimation; Estimation; OFDM; orthogonal frequency-division multiplexing (OFDM); Receivers; underwater acoustic (UWA) communications","Autocorrelation; Correlation methods; Doppler effect; Estimation; Frequency estimation; Orthogonal frequency division multiplexing; Orthogonal functions; Receivers (containers); Room and pillar mining; Signal receivers; Ambiguity function; Autocorrelation methods; Complexity theory; Cross Ambiguity Function (CAF); Delays; Doppler estimations; Transmitter and receiver; Underwater acoustic channels; Underwater acoustics",2-s2.0-85032745022
"Motoyama T., Nakano S., Yamamoto Y., Tokiwa H., Asano Y., Ito S.","Product Release Mechanism Associated with Structural Changes in Monomeric l -Threonine 3-Dehydrogenase",2017,"Biochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032632099&doi=10.1021%2facs.biochem.7b00832&partnerID=40&md5=2630bae3e81c66d932c8dd00b0702199","A short chain dehydrogenase like l-threonine 3-dehydrogenase (SDR-TDH) from metagenome data (mtTDH) was identified by database mining. Its enzymatic properties suggested that mtTDH has unique characteristics relative to other SDR-TDHs, including two mesophilic and thermophilic SDR-TDHs identified in this study. The activation energy of mtTDH was the lowest (29.6 kJ/mol) of those of the SDR-TDHs, indicating that it is a psychrophilic enzyme. Size-exclusion chromatography analysis revealed mtTDH is a monomer. Crystal structures of mtTDH in apo, binary, and two ternary complexes (l-Ser- and l-Thr-soaked forms) were determined at resolutions of 1.25-1.9 Å. Structural and computational analysis revealed the molecular mechanism of switching between the open and closed states induced by substrate binding and product release. Furthermore, six residues and two water molecules at the active site contributing to product release were assigned. The residues could be categorized into two groups on the basis of the enzymatic properties of their variants: S111, Y136, and T177 and S74, T178, and D179. The former group appeared to affect l-Thr dehydrogenation directly, because the kcat value of their variants was &gt;80-fold lower than that of wild-type mtTDH. On the other hand, the latter group contributes to switching between the open and closed states, which is important for the high substrate specificity of SDR-TDH for l-Thr: the kcat and Km toward l-Thr values of variants in these residues could not be determined because the initial velocity was unsaturated at high concentrations of l-Thr. On the basis of these findings, we proposed a product release mechanism for SDR-TDH associated with specific structural changes. © 2017 American Chemical Society.",,"Activation energy; Amino acids; Bins; Dehydrogenation; Molecules; Computational analysis; Enzymatic properties; Initial velocities; Molecular mechanism; Product release; Substrate binding; Substrate specificity; Ternary complex; Size exclusion chromatography; alcohol dehydrogenase; L-threonine 3-dehydrogenase; chemistry; genetics; metagenome; molecular model; protein domain; X ray crystallography; Alcohol Oxidoreductases; Crystallography, X-Ray; Metagenome; Models, Molecular; Protein Domains",2-s2.0-85032632099
"Choi J., Kim B., Hahn H., Park H., Jeong Y., Yoo J., Jeong M.K.","Data mining-based variable assessment methodology for evaluating the contribution of knowledge services of a public research institute to business performance of firms",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018422332&doi=10.1016%2fj.eswa.2017.04.057&partnerID=40&md5=0761ba2277f03077140ae99c7e965e3c","This study proposes a methodology for assessing the contribution of knowledge services (KSs) provided by a Korean public research institute to the business performance of firms. A new methodology based on a data mining-based variable assessment method in a regression model is proposed for the service-level assessment. The contribution of the KSs to firms’ business performance is analyzed using their attributes and specific business performance indicators through the conditional variable permutation method in the random forest regression. This reduces the ambiguity in variable importance caused by the correlations among input variables. The proposed methodology is applied to the survey dataset collected from firms. The survey dataset is examined 1) for the whole data and 2) for a subset of the data, namely, small- and medium-sized enterprises (SMEs). The empirical results show behavioral properties of firms with regard to the given KSs in general and SMEs in particular. Practical and user-friendly service product types increase the firms’ expectation on business performance. Also, flexibility in the service products helps firms acquire much-needed knowledge and boosts their expectation on business performance. In particular, SMEs expect better business performance from the KSs that help them create business plans and strategies. © 2017","Data mining; Knowledge service assessment; Public research institute; Relative contribution score; Variable importance","Commerce; Decision trees; Regression analysis; Surveys; Assessment methodologies; Behavioral properties; Knowledge service; Public research institute; Relative contribution; Small and medium-sized enterprise; Variable importances; Variable permutation; Data mining",2-s2.0-85018422332
"Zhao X., Zhang J., Qin X.","LOMA: A local outlier mining algorithm based on attribute relevance analysis",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019249909&doi=10.1016%2fj.eswa.2017.05.009&partnerID=40&md5=4c1d39dd4596b9b17db52648780b9fe3","In this study, we propose a novel local outlier detection approach - called LOMA - to mining local outliers in high-dimensional data sets. To improve the efficiency of outlier detection, LOMA prunes irrelevance attributes and objects in the data set by analyzing attribute relevance with a sparse factor threshold. Such a pruning technique substantially reduce the size of data sets. The core of LOMA is searching sparse subspace, which implements the particle swarm optimization method in reduced data sets. In the process of searching sparse subspace, we introduce the sparse coefficient threshold to represent sparse degrees of data objects in a subspace, where the data objects are considered as local outliers. The attribute relevance analysis provides a guidance for experts and users to identify useless attributes for detecting outliers. In addition, our sparse-subspace-based outlier algorithm is a novel technique for local-outlier detection in a wide variety of applications. Experimental results driven by both synthetic and UCI data sets validate the effectiveness and accuracy of our LOMA. In particular, LOMA achieves high mining efficiency and accuracy when the sparse factor threshold is set to a small value. © 2017 Elsevier Ltd","Attribute relevance analysis; Local outlier; Particle swarm optimization; Sparse coefficient; Sparse factor","Clustering algorithms; Data handling; Data mining; Efficiency; Optimization; Particle swarm optimization (PSO); High dimensional data; Local outliers; Outlier algorithms; Particle swarm optimization method; Pruning techniques; Relevance analysis; Sparse coefficient; Sparse factor; Statistics",2-s2.0-85019249909
"Abualigah L.M., Khader A.T., Al-Betar M.A., Alomari O.A.","Text feature selection with a robust weight scheme and dynamic dimension reduction to text document clustering",2017,"Expert Systems with Applications",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018382941&doi=10.1016%2fj.eswa.2017.05.002&partnerID=40&md5=3840bb52896947ca851baaf5bc7cd492","This paper proposes three feature selection algorithms with feature weight scheme and dynamic dimension reduction for the text document clustering problem. Text document clustering is a new trend in text mining; in this process, text documents are separated into several coherent clusters according to carefully selected informative features by using proper evaluation function, which usually depends on term frequency. Informative features in each document are selected using feature selection methods. Genetic algorithm (GA), harmony search (HS) algorithm, and particle swarm optimization (PSO) algorithm are the most successful feature selection methods established using a novel weighting scheme, namely, length feature weight (LFW), which depends on term frequency and appearance of features in other documents. A new dynamic dimension reduction (DDR) method is also provided to reduce the number of features used in clustering and thus improve the performance of the algorithms. Finally, k-mean, which is a popular clustering method, is used to cluster the set of text documents based on the terms (or features) obtained by dynamic reduction. Seven text mining benchmark text datasets of different sizes and complexities are evaluated. Analysis with k-mean shows that particle swarm optimization with length feature weight and dynamic reduction produces the optimal outcomes for almost all datasets tested. This paper provides new alternatives for text mining community to cluster text documents by using cohesive and informative features. © 2017 Elsevier Ltd","Dynamic dimension reduction; Feature selection; Metaheuristics; Text document clustering; Weight score","Cluster analysis; Clustering algorithms; Data mining; Genetic algorithms; Information retrieval; Optimization; Particle swarm optimization (PSO); Reduction; Text processing; Dimension reduction; Feature selection algorithm; Feature selection methods; Meta heuristics; Particle swarm optimization algorithm; Text Document Clustering; Text feature selections; Weight scores; Feature extraction",2-s2.0-85018382941
"Wu Z., Lei L., Li G., Huang H., Zheng C., Chen E., Xu G.","A topic modeling based approach to novel document automatic summarization",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019046858&doi=10.1016%2fj.eswa.2017.04.054&partnerID=40&md5=bfc9e6f76a357f24728a0b7ef957d79f","Most of existing text automatic summarization algorithms are targeted for multi-documents of relatively short length, thus difficult to be applied immediately to novel documents of structure freedom and long length. In this paper, aiming at novel documents, we propose a topic modeling based approach to extractive automatic summarization, so as to achieve a good balance among compression ratio, summarization quality and machine readability. First, based on topic modeling, we extract the candidate sentences associated with topic words from a preprocessed novel document. Second, with the goals of compression ratio and topic diversity, we design an importance evaluation function to select the most important sentences from the candidate sentences and thus generate an initial novel summary. Finally, we smooth the initial summary to overcome the semantic confusion caused by ambiguous or synonymous words, so as to improve the summary readability. We evaluate experimentally our proposed approach on a real novel dataset. The experiment results show that compared to those from other candidate algorithms, each automatic summary generated by our approach has not only a higher compression ratio, but also better summarization quality. © 2017 Elsevier Ltd","Compression ratio; Novel summarization; Readability; Topic diversity; Topic modeling","Compression ratio (machinery); Information management; Semantics; Automatic summarization; Evaluation function; Extractive automatic summarization; Higher compression ratios; Novel summarization; Readability; Topic diversity; Topic Modeling; Data mining",2-s2.0-85019046858
"Roots E., Calvert A.J., Craven J.","Interferometric seismic imaging around the active Lalor mine in the Flin Flon greenstone belt, Canada",2017,"Tectonophysics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018241469&doi=10.1016%2fj.tecto.2017.04.024&partnerID=40&md5=41d804e6794685f68c6afd6543987930","Seismic interferometry, which recovers the impulse response of the Earth by cross-correlation of ambient noise recorded at sets of two receivers, has found several applications, including the generation of virtual shot gathers for use in seismic reflection processing. To evaluate the effectiveness of this passive recording technique in mineral exploration in a hard-rock environment, 336 receivers recorded 300 h of ambient noise over the volcanogenic massive sulphide deposit of the recently discovered Lalor mine in the Canadian Flin Flon greenstone belt. A novel time-domain beamforming algorithm was developed to search for individual source locations, demonstrating that the vast majority of noise originated from the mine and ventilation shafts of the Lalor mine. The results of the beamforming were utilized in conjunction with frequency-wavenumber filtering to remove undesirable, mostly monochromatic surface wave noise originating from nearby sources. Virtual shot gathers were generated along three receiver lines, each of which was processed as a separate 2-D reflection line. Two of the resulting unmigrated reflection profiles are compared against coincident dipmoveout-stacked data from a larger, coincident 3-D dynamite seismic survey that was also acquired over the Lalor mine in 2013. Using knowledge of the local geology derived from numerous boreholes, coherent events recovered in the passive reflection profiles are inferred to be either spurious arrivals or real reflections, some of which can be interpreted in terms of geological contacts, indicating the future potential of passive recording surveys in hard rock settings. © 2017 Elsevier B.V.","Flin Flon greenstone belt; Lalor mining camp; Seismic interferometry; Seismic reflection imaging; Volcanogenic massive sulphide","Acoustic noise; Beamforming; Geology; Impulse response; Interferometry; Mineral exploration; Minerals; Seismic waves; Seismology; Surface waves; Surveys; Volcanoes; Greenstone belts; Lalor mining camp; Seismic interferometries; Seismic reflection imaging; Volcanogenic massive sulphides; Mine shafts",2-s2.0-85018241469
"Ordozgoiti B., Canaval S.G., Mozo A.","Iterative column subset selection",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032379057&doi=10.1007%2fs10115-017-1115-4&partnerID=40&md5=1ce90d11d5afff9ab47fe066967078cb","Dimensionality reduction is often a crucial step for the successful application of machine learning and data mining methods. One way to achieve said reduction is feature selection. Due to the impossibility of labelling many data sets, unsupervised approaches are frequently the only option. The column subset selection problem translates naturally to this purpose and has received considerable attention over the last few years, as it provides simple linear models for low-rank data reconstruction. Recently, it was empirically shown that an iterative algorithm, which can be implemented efficiently, provides better subsets than other state-of-the-art methods. In this paper, we describe this algorithm and provide a more in-depth analysis. We carry out numerous experiments to gain insights on its behaviour and derive a simple bound for the norm recovered by the resulting matrix. To the best of our knowledge, this is the first theoretical result of this kind for this algorithm. © 2017 Springer-Verlag London Ltd.","Column subset selection; Data mining; Dimensionality reduction; Machine learning; Unsupervised feature selection",,2-s2.0-85032379057
"Pirnia A., Hu J., Peterson S.D., Erath B.D.","Vortex dynamics and flow-induced vibrations arising from a vortex ring passing tangentially over a flexible plate",2017,"Journal of Applied Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032742932&doi=10.1063%2f1.5009068&partnerID=40&md5=2d0bb3b8bb42120d17ff9cad58e65bf5","The extraction of energy from vortical structures advecting through an ambient environment is a topic of interest due to the potential to power miniature in situ sensors and monitors. This work investigates the vortex dynamics and flow-induced vibrations of a flexible plate arising from a vortex ring passing tangentially over it. Experimental measurements of the flow field and plate dynamics are performed in tandem with a coupled potential flow/Kirchhoff-Love plate model in order to (i) elucidate the physics of the vortex-plate interactions in the specified orientation and relate the energy exchange between the ring and the plate to the attendant vortex dynamics; (ii) validate the potential flow model and provide any needed corrections to account for the simplifying assumptions; and (iii) provide empirical data for estimating energy harvesting capabilities in the specified orientation. The plate loading arises as a result of an initial down-wash, followed quickly by a region of reduced pressure as the vortex core passes over the plate. The fundamental physics of the interaction is discussed, identifying three regimes. When the centerline of the vortex ring is positioned greater than approximately 2 vortex ring radii away from the plate it can be considered to be in the far-field, and the resulting vibrations are well predicted through potential flow, once the plate dynamics are corrected for edge effects arising from a finite plate width. As the offset distance of the vortex ring is decreased, diffusion of induced vorticity on the plate into the flow field significantly alters the fluid dynamics, pressure loading, and the resultant plate dynamics, and dramatically increases the strain energy in comparison with the potential flow model predictions. A first-order correction to the potential flow model is proposed to account for the finite plate width, while empirical correlations are presented for the plate strain energy in cases where ring/induced vorticity interactions are significant. © 2017 Author(s).",,"Data mining; Dynamics; Energy harvesting; Flow fields; Plates (structural components); Potential flow; Strain energy; Vortex flow; Vorticity; Ambient environment; Empirical correlations; Energy exchanges; Flow induced vibrations; Fundamental physics; Potential flow model; Simplifying assumptions; Vortical structures; Vibrations (mechanical)",2-s2.0-85032742932
"Mueen A., Chavoshi N., Abu-El-Rub N., Hamooni H., Minnich A., MacCarthy J.","Speeding up dynamic time warping distance for sparse time series data",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032336695&doi=10.1007%2fs10115-017-1119-0&partnerID=40&md5=ed3e02670ec9a0a0139d4c26515a5547","Dynamic time warping (DTW) distance has been effectively used in mining time series data in a multitude of domains. However, in its original formulation DTW is extremely inefficient in comparing long sparse time series, containing mostly zeros and some unevenly spaced nonzero observations. Original DTW distance does not take advantage of this sparsity, leading to redundant calculations and a prohibitively large computational cost for long time series. We derive a new time warping similarity measure (AWarp) for sparse time series that works on the run-length encoded representation of sparse time series. The complexity of AWarp is quadratic on the number of observations as opposed to the range of time of the time series. Therefore, AWarp can be several orders of magnitude faster than DTW on sparse time series. AWarp is exact for binary-valued time series and a close approximation of the original DTW distance for any-valued series. We discuss useful variants of AWarp: bounded (both upper and lower), constrained, and multidimensional. We show applications of AWarp to three data mining tasks including clustering, classification, and outlier detection, which are otherwise not feasible using classic DTW, while producing equivalent results. Potential areas of application include bot detection, human activity classification, search trend analysis, seismic analysis, and unusual review pattern mining. © 2017 Springer-Verlag London Ltd.","Dynamic time warping; Run-length encoding; Sparse time series",,2-s2.0-85032336695
"Edstrom J., Gong Y., Chen D., Wang J., Gong N.","Data-Driven Intelligent Efficient Synaptic Storage for Deep Learning",2017,"IEEE Transactions on Circuits and Systems II: Express Briefs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032725801&doi=10.1109%2fTCSII.2017.2767900&partnerID=40&md5=22fa7e321196e79a4649f4d0beb8f46d","With the availability of big data and advanced hardware technologies, deep learning has been applied in different applications, such as self-driving cars and face recognition. While considering various sources of uncertainty and balancing multiple objectives of a system, the hardware implementation of deep learning needs continuous model updating and intensive synaptic weight storage, and SRAM is critical for the overall performance and energy efficiency. In this brief, we introduce offline data mining to the hardware design process, and the discovered data knowledge combined with a data-driven hardware design technique enables a more intelligent memory with better trade-off between energy efficiency, cost, and classification accuracy, thereby helping relieve the huge burden of data storage in deep learning systems. A 45nm 64 kbits (256 words W 256 bits) synaptic SRAM is presented that enables 45.6% active power saving and 83.2% leakage power saving, with low implementation cost (3.17%) and less than 1% degradation in classification accuracy. IEEE","Artificial neural networks; Biological neural networks; data mining; data-driven; Deep learning; Machine learning; Memory management; Neurons; power efficient; Random access memory; self-correction.; SRAM; Switches; synaptic","Data mining; Deep learning; Digital storage; Economic and social effects; Energy efficiency; Face recognition; Hardware; Integrated circuit design; Learning systems; Neural networks; Neurons; Random access storage; Static random access storage; Switches; Biological neural networks; Data driven; Memory management; Power efficient; Random access memory; Self-correction; synaptic; Big data",2-s2.0-85032725801
"Xing F.Z., Cambria E., Welsch R.E.","Natural language based financial forecasting: a survey",2017,"Artificial Intelligence Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032466713&doi=10.1007%2fs10462-017-9588-9&partnerID=40&md5=804a3970c8530b4c69eaad7cabc2a9ea","Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict financial markets are fast accumulating, gradually establishing the research field of natural language based financial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines. © 2017 Springer Science+Business Media B.V.","Computational finance; Financial forecasting; Knowledge engineering; Natural language processing; Predictive analytics; Text mining","Commerce; Data mining; Engineering research; Finance; Financial markets; Forecasting; Knowledge engineering; Predictive analytics; Semantics; Surveys; Computational finance; Data availability; Financial forecasting; Natural languages; Nlp techniques; Research fields; Stock market prediction; Text mining; Natural language processing systems",2-s2.0-85032466713
"Li N., Hu J., Sun R., Wang S., Luo Z.","A High-capacity 3D Steganography Algorithm with Adjustable Distortion",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032737381&doi=10.1109%2fACCESS.2017.2767072&partnerID=40&md5=37a4e8301570ed8c186b931b87d3b242","In this paper, we devise a novel steganography algorithm that has a high capacity, while still retaining the ability of adjusting the embedding distortion. A shifting strategy is explored to embed the secret data into a given 3D model effectively. In order to reduce the embedding distortion, we propose a truncated space of data instead of directly working on the critical geometric information from vertices of the cover model. The truncated space confines the distortion of each component of the stego-model within the space, that means the embedding distortion could be controlled within a very low threshold. In theory, we can set the length of data truncation to adjust the embedding distortion below a specified level, at the cost of losing certain embedding capacity. Moreover, the embedding capacity is irrelevant to the shape of models, and the quality of the stego-model is mostly dependent on the length of truncation rather than the quantity of embedded secret data. The proposed 3D steganography method has the capability to control the level of embedding distortion, and at the same time, has a high embedding capacity. Various experiments have demonstrated the flexibility and high performance of our new approach. OAPA","3D steganography; adjustable distortion; Aerospace electronics; Data mining; Data models; Distortion; high capacity; Robustness; Solid modeling; Three-dimensional displays","Data mining; Data structures; Distortion (waves); Robustness (control systems); 3D steganography; Aerospace electronics; High capacity; Solid model; Three-dimensional display; Steganography",2-s2.0-85032737381
"Kannan R., Ballard G., Park H.","MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032748054&doi=10.1109%2fTKDE.2017.2767592&partnerID=40&md5=446aefe2a97b9238565d5cbf6aa6cc27","Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that A<formula><tex>$\approx$</tex></formula>WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans from few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online. IEEE","2D; Algorithm design and analysis; Analytical models; Approximation algorithms; Collaboration; Computational modeling; HPC; MPI; NMF; Program processors; Sparse matrices","Analytical models; Approximation algorithms; Big data; Data mining; Factorization; Iterative methods; Parallel processing systems; Program processors; Algorithm design and analysis; Alternating least squares; Collaboration; Computational framework; Computational model; Inter processor communication; Nonnegative matrix factorization; Sparse matrices; Matrix algebra",2-s2.0-85032748054
"Li X., Li D., Yang Z., Chen W.","A Patch-based Saliency Detection Method for Assessing the Visual Privacy Levels of Objects in Photos",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032726972&doi=10.1109%2fACCESS.2017.2767622&partnerID=40&md5=f24a8be1fd64edfcb7e102a4aa7a08e4","Photo privacy protection has recently received increasing attention from the public. However, the overprotection of photo privacy by hiding too much visual information can make photos meaningless. To avoid this, visual information with different degrees of privacy sensitivity can be filtered out using various image-processing techniques. Objects in a photo usually contain visual information that can potentially reveal private information; this potential depends on both the visual saliency of the objects and on the specific categories to which the objects belong. In this paper, we aim to quantitatively evaluate the influence of visual saliency information on privacy and objectively evaluate the levels of visual privacy that objects contain. Meeting this objective faces two challenges: (1) determining a method of effectively detecting generic objects in a photo for the extraction of saliency information; and (2) determining a scientific method for assessing the visual private information contained in objects. To cope with these challenges, we first propose a hierarchical saliency detection method that combines a patch-based saliency detection strategy with an objectness estimation strategy to effectively locate salient objects and obtain the saliency information of each object. The proposed method results in a small set of class-independent locations with high quality and a Mean Average Best Overlap (MABO) score of 0.627 at 1150 locations, which is superior to the score of other saliency detection methods. Second, we build a computational privacy assessment system to scientifically calculate and rank the privacy risks of objects in a photo by creating an improved risk matrix and using the Borda count method. The proposed computational privacy assessment method matches human evaluations to a relatively high degree. OAPA","Borda method; Data mining; Detection algorithms; Estimation; objectness; Privacy; privacy assessment; privacy protection; risk matrix; saliency detection; Search problems; Sensitivity; Visualization","Data mining; Data privacy; Data visualization; Estimation; Flow visualization; Image enhancement; Image processing; Object detection; Risk assessment; Risk perception; Visualization; Borda method; Detection algorithm; objectness; Privacy protection; Risk matrix; Saliency detection; Search problem; Sensitivity; Information filtering",2-s2.0-85032726972
"Kato S., Shinkuma R.","Priority Control in Communication Networks for Accuracy-Freshness Tradeoff in Realtime Road-Traffic Information Delivery",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731882&doi=10.1109%2fACCESS.2017.2767058&partnerID=40&md5=2042e77de2de6f4c9223e4288ef7fcd4","Delivering real-time road-traffic information to the driver is a straightforward solution to the problem of road traffic congestion. The information is more effective as it is fresh and more accurate. However, real-time road-traffic information delivery has a fundamental problem: an accuracyfreshness tradeoff. Unfortunately, real-time road-traffic information delivery has difficulty satisfying both requirements. To guarantee the freshness, the information needs to be delivered on the basis of the data received by a cloud or edge server before a predetermined deadline. However, only a limited amount of data is received due to bandwidth limitation and processing overhead in communication networks, which results in the poor accuracy of the delivered information. The only way to improve the accuracy is to make the deadline less strict, which results in deteriorating the freshness of information. The proposed system solves this trade-off. The key idea is that data more &#x2018;important&#x2019; for the accuracy of information is more prioritized when the data is transferred in communication networks. In the proposed system, &#x2018;importance&#x2019; is determined by how helpful the data is when the system needs to estimate missing spatial information from a limited amount of received data by using the machine learning technique. In the paper, simulation results verify the proposed system ensures the accuracy of road-traffic information while satisfying the freshness requirement. OAPA","active learning; Communication networks; Data mining; edge computing; Image edge detection; Internet of Things; priority control; Probes; realtime information delivery; road-traffic information; Roads; Servers; Vehicles","Artificial intelligence; Bandwidth; Data mining; Economic and social effects; Edge detection; Internet of things; Learning systems; Probes; Roads and streets; Servers; Street traffic control; Telecommunication networks; Traffic congestion; Transportation; Vehicles; Active Learning; Edge computing; Image edge detection; Priority control; Real-time information; Road traffic informations; Roads; Data communication systems",2-s2.0-85032731882
"Dai J., Hu H., Wu W., Qian Y., Huang D.","Maximal Discernibility Pairs based Approach to Attribute Reduction in Fuzzy Rough Sets",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032745701&doi=10.1109%2fTFUZZ.2017.2768044&partnerID=40&md5=1ea7596027be18bc2191216d30721d6b","Attribute reduction is one of the biggest challenges encountered in computational intelligence, data mining, pattern recognition and machine learning. Effective in feature selection as the rough set theory is, it can only handle symbolic attributes. In order to overcome this drawback, proposed is the fuzzy rough sets, an extended model of rough sets, which is able to deal with imprecision and uncertainty in both symbolic and numerical attributes. Existing attribute selection algorithms based on fuzzy rough set model mainly take the angle of &#x201C;attribute set&#x201D;, which means they define the object function representing the predictive ability for attributes subset with regard to the domain of discourse, rather than follow the view of &#x201C;object pair&#x201D;. Algorithms based on the latter can ignore the object pairs that are already discerned by the selected attributes subsets and thus need only to deal with part of object pairs instead of the whole object pairs from the discourse, which makes such algorithms more efficient in attribute selection. In this paper, we propose the concept of Reduced Maximal Discernibility Pair, which directly adopts the perspective of object pair in the framework of fuzzy rough set model. Then we develop two attribute selection algorithms, named as Reduced Maximal Discernibility Pair Selection (RMDPS) and Weighted Reduced Maximal Discernibility Pair Selection (WRMDPS), based on the reduced maximal discernibility pair. Experiment results show that the proposed algorithms are effective and efficient in attribute selection. IEEE","attribute reduction; Computational modeling; Computer science; Feature extraction; fuzzy discernibility matrix; Fuzzy rough sets; maximal discernibility pairs; Numerical models; Prediction algorithms; Rough sets; Symmetric matrices","Artificial intelligence; Computation theory; Computer science; Data mining; Feature extraction; Learning systems; Numerical models; Pattern recognition; Attribute reduction; Computational model; Discernibility; Discernibility matrix; Fuzzy-rough sets; Prediction algorithms; Symmetric matrices; Rough set theory",2-s2.0-85032745701
"Zhang Y., Fu Y., Wang Z., Feng L.","Fault detection Based on modified Kernel Semi-supervised Locally Linear Embedding",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032737506&doi=10.1109%2fACCESS.2017.2767698&partnerID=40&md5=947dbcb9ca736a5c455bd81277f1ab39","In this paper, a novel approach to fault detection for nonlinear processes is proposed. It is based on a manifold learning called modified kernel semi-supervised local linear embedding (MK-SSLLE). Local linear embedding (LLE) is widely applied to fault detection of complex industrial process. However, the LLE only preserves the local structure information of the sample, which ignores the global characteristics of the original data. . The main contributions of the presented approach are as follows: 1) in order to utilize labeled data, the semisupervised learning is introduced into LLE; 2) the regularization term is added to the calculation of local reconstruction weights matrix to strengthen the anti-noise ability in nonlinear processing; 3) in order to extract the global and local characteristic of the observation data, the kernel principal component analysis (KPCA) objective function is integrated with the objective function of LLE. Experimental results on the production process of fused magnesia verify the performance of the proposed method. OAPA","Data mining; Fault detection; Fault detection; Fault diagnosis; Kernel; KPCA; Linear programming; LLE; Manifolds; Principal component analysis; semi-supervised learning","Automobile engine manifolds; Data mining; Failure analysis; Learning algorithms; Linear programming; Principal component analysis; Program diagnostics; Supervised learning; Complex industrial process; Kernel; Kernel principal component analyses (KPCA); KPCA; Local characteristics; Local Linear Embedding; Regularization terms; Semi- supervised learning; Fault detection",2-s2.0-85032737506
"Yang Z., Li Z., Zhu J., Feng G., Wang Q., Hu J., Wang C.","Deriving time-series three-dimensional displacements of mining areas from a single-geometry InSAR dataset",2017,"Journal of Geodesy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032478124&doi=10.1007%2fs00190-017-1079-x&partnerID=40&md5=65fb36a6952a6f3843e83bd7f0c51885","This paper presents a method for deriving time-series three-dimensional (3-D) displacements of mining areas from a single-geometry interferometric synthetic aperture radar (InSAR) dataset (hereafter referred to as the SGI-based method). This is mainly aimed at overcoming the limitation of the traditional multi-temporal InSAR techniques that require SAR data from at least three significantly different imaging geometries to fully retrieve time-series 3-D displacements of mining areas. The SGI-based method first generates the multi-temporal observations of the mining-induced vertical subsidence from the single-geometry InSAR data, using a previously developed method for retrieving 3-D mining-related displacements from a single InSAR pair (SIP). The weighted least-squares solutions of the time series of vertical subsidence are estimated from these generated multi-temporal observations of vertical subsidence. Finally, the time series of horizontal motions in the east and north directions are estimated using the proportional relationship between the horizontal motion and the subsidence gradient of the mining area, on the basis of the SGI-derived time series of vertical subsidence. Seven ascending ALOS PALSAR images from the Datong mining area of China were used to test the proposed SGI-based method. The results suggest that the SGI-based method is effective. The SGI-based method not only extends the SIP-based method to time-series 3-D displacement retrieval from a single-geometry InSAR dataset, but also limits the uncertainty propagation from InSAR-derived deformation to the estimated 3-D displacements. © 2017 Springer-Verlag GmbH Germany","3-D displacements; InSAR; Mining subsidence; SAR; Time series",,2-s2.0-85032478124
"Frégnac Y.","Big data and the industrialization of neuroscience: A safe roadmap for understanding the brain?",2017,"Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032481870&doi=10.1126%2fscience.aan8866&partnerID=40&md5=c5d12f33565cd82e10dada452b43a215","New technologies in neuroscience generate reams of data at an exponentially increasing rate, spurring the design of very-large-scale data-mining initiatives. Several supranational ventures are contemplating the possibility of achieving, within the next decade(s), full simulation of the human brain.",,,2-s2.0-85032481870
"Kistemann T., Schweikart J.","Spatial turn: Opportunity, challenge and methodological momentum for geographical health research [„Spatial turn“: Chance, Herausforderung und Methodenimpuls für die geographische Gesundheitsforschung]",2017,"Bundesgesundheitsblatt - Gesundheitsforschung - Gesundheitsschutz",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032508031&doi=10.1007%2fs00103-017-2647-1&partnerID=40&md5=9de6d8f7a4c9b18c01a824b7c4168d9a","The (re)-discovery of the spatial dimension in many sciences has been guided for some time under the designation “spatial turn”. Immense progress in geographic information sciences (GIS), global positioning systems (GPS), remote sensing and computer-aided cartography, in addition to geostatistical methods such as spatial distribution analysis and trend analysis, multi-level analysis, spatial data-mining and agent-based modelling, has created entirely new opportunities for spatial analysis and the modelling of spatial, health-relevant processes. These methods are increasingly being employed in epidemiology, public health and healthcare research. In the fields of cultural and social sciences, “spatial turn” refers to a paradigm shift that recognizes that geographical space also has a social and cultural meaning. This spatial conception considers space not only as an empty container, but also as a result of social processes. The Euclidean space is extended by socially and culturally shaped spatial perceptions and constructions. The “spatial turn” as a paradigm shift is not limited to the fact that space itself becomes an object of advanced investigation methods. It is instead about approaching objects of research with spatial categories. In light of the “spatial turn”, geographical health research is currently facing great opportunities, but also a double challenge: on the one hand, recognizing, mediating and making meaningful use of the new methodological possibilities. On the other hand, and in line with its self-conception as a part of the medical humanities, it is challenged to implement the “spatial turn” in its social and cultural–scientific dimension, to go beyond stereotypical reception and to meet the paradigmatic significance of “spatial turn”. © 2017 Springer-Verlag GmbH Deutschland","Geographic information sciences; Geographical health research; Space concepts; Therapeutic landscape; “Spatial turn”",,2-s2.0-85032508031
"Chen X., Fang Y., Yang M., Nie F., Zhao Z., Huang J.Z.","PurTreeClust: A Clustering Algorithm for Customer Segmentation from Massive Customer Transaction Data",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032743244&doi=10.1109%2fTKDE.2017.2763620&partnerID=40&md5=437ff83d78d5c35be033f26b8341f57f","Clustering of customer transaction data is an important procedure to analyze customer behaviors in retail and e-commerce companies. Note that products from companies are often organized as a product tree, in which the leaf nodes are goods to sell, and the internal nodes (except root node) could be multiple product categories. Based on this tree, we present a &#x201C;personalized product tree&#x201C;, named purchase tree, to represent a customer&#x0027;s transaction records. So the customers&#x0027; transaction data set can be compressed into a set of purchase trees. We propose a partitional clustering algorithm, named PurTreeClust, for fast clustering of purchase trees. A new distance metric is proposed to effectively compute the distance between two purchase trees. To cluster the purchase tree data, we first rank the purchase trees as candidate representative trees with a novel separate density, and then select the top k customers as the representatives of k customer groups. Finally, the clustering results are obtained by assigning each customer to the nearest representative. We also propose a gap statistic based method to evaluate the number of clusters. A series of experiments were conducted on ten real-life transaction data sets, and experimental results show the superior performance of the proposed method. IEEE","Clustering algorithms; Clustering methods; clustering transaction data; clustering trees; Companies; Computational complexity; Customer segmentation; Data engineering; Knowledge discovery; Measurement; purchase tree","Cluster analysis; Computational complexity; Data mining; Industry; Measurements; Sales; Trees (mathematics); Clustering methods; Clustering trees; Customer segmentation; Data engineering; Transaction data; Clustering algorithms",2-s2.0-85032743244
"Yu Z., Zhang Y., You J., Chen C.L.P., Wong H., Han G., Zhang J.","Adaptive Semi-Supervised Classifier Ensemble for High Dimensional Data Classification",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032739321&doi=10.1109%2fTCYB.2017.2761908&partnerID=40&md5=c025fe77ccfdefb06e137d0c89e14609","High dimensional data classification with very limited labeled training data is a challenging task in the area of data mining. In order to tackle this task, we first propose a feature selection-based semi-supervised classifier ensemble framework (FSCE) to perform high dimensional data classification. Then, we design an adaptive semi-supervised classifier ensemble framework (ASCE) to improve the performance of FSCE. When compared with FSCE, ASCE is characterized by an adaptive feature selection process, an adaptive weighting process (AWP), and an auxiliary training set generation process (ATSGP). The adaptive feature selection process generates a set of compact subspaces based on the selected attributes obtained by the feature selection algorithms, while the AWP associates each basic semi-supervised classifier in the ensemble with a weight value. The ATSGP enlarges the training set with unlabeled samples. In addition, a set of nonparametric tests are adopted to compare multiple semi-supervised classifier ensemble (SSCE)approaches over different datasets. The experiments on 20 high dimensional real-world datasets show that: 1) the two adaptive processes in ASCE are useful for improving the performance of the SSCE approach and 2) ASCE works well on high dimensional datasets with very limited labeled training data, and outperforms most state-of-the-art SSCE approaches. IEEE","Classification; ensemble learning; Feature extraction; feature selection; high dimensional data; Laplace equations; optimization; Power capacitors; Robustness; semi-supervised learning; Semisupervised learning; Training","Clustering algorithms; Data mining; Feature extraction; Laplace equation; Learning algorithms; Optimization; Personnel training; Robustness (control systems); Supervised learning; Adaptive feature selection; Ensemble learning; Feature selection algorithm; High dimensional data; High dimensional datasets; Labeled training data; Power capacitor; Semi- supervised learning; Classification (of information)",2-s2.0-85032739321
"Lin C., Lu J., Wei Z., Wang J., Xiao X.","Optimal algorithms for selecting top-k combinations of attributes: theory and applications",2017,"VLDB Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032388214&doi=10.1007%2fs00778-017-0485-2&partnerID=40&md5=6beadf725790408c3bb7d0a7339784c4","Traditional top-k algorithms, e.g., TA and NRA, have been successfully applied in many areas such as information retrieval, data mining and databases. They are designed to discover k objects, e.g., top-k restaurants, with highest overall scores aggregated from different attributes, e.g., price and location. However, new emerging applications like query recommendation require providing the best combinations of attributes, instead of objects. The straightforward extension based on the existing top-k algorithms is prohibitively expensive to answer top-k combinations because they need to enumerate all the possible combinations, which is exponential to the number of attributes. In this article, we formalize a novel type of top-k query, called top-k, m, which aims to find top-k combinations of attributes based on the overall scores of the top-m objects within each combination, where m is the number of objects forming a combination. We propose a family of efficient top-k, m algorithms with different data access methods, i.e., sorted accesses and random accesses and different query certainties, i.e., exact query processing and approximate query processing. Theoretically, we prove that our algorithms are instance optimal and analyze the bound of the depth of accesses. We further develop optimizations for efficient query evaluation to reduce the computational and the memory costs and the number of accesses. We provide a case study on the real applications of top-k, m queries for an online biomedical search engine. Finally, we perform comprehensive experiments to demonstrate the scalability and efficiency of top-k, m algorithms on multiple real-life datasets. © 2017 Springer-Verlag GmbH Germany","Instance optimal algorithm; Top-k query; Top-k, m query","Computation theory; Data mining; Information retrieval; Query processing; Search engines; Approximate query processing; Biomedical search engines; Efficient query evaluation; Emerging applications; Optimal algorithm; Real life datasets; Top-k query; Top-k, m query; Optimization",2-s2.0-85032388214
"Shickel B., Tighe P.J., Bihorac A., Rashidi P.","Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis",2017,"IEEE Journal of Biomedical and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032736236&doi=10.1109%2fJBHI.2017.2767063&partnerID=40&md5=e44a2d8fba171b1a7a4dcc2ba0f161ce","The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHR). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and de-identification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research. IEEE","clinical informatics; deep learning; electronic health records; Electronic medical records; Hospitals; Informatics; Machine learning; machine learning; Medical diagnostic imaging; survey","Artificial intelligence; Data mining; Deep learning; Diagnosis; Health; Hospitals; Learning algorithms; Learning systems; Medical computing; Medical imaging; Records management; Surveying; Surveys; Clinical informatics; Electronic health record; Electronic medical record; Informatics; Medical diagnostic imaging; E-learning; diagnostic imaging; electronic health record; electronic medical record; extraction; human; information science; machine learning; phenotype; prediction",2-s2.0-85032736236
"Zhang J., Deng Z., Choi K., Wang S.","Data-Driven Elastic Fuzzy Logic System Modeling: Constructing a Concise System with Human-like Inference Mechanism",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032455330&doi=10.1109%2fTFUZZ.2017.2767025&partnerID=40&md5=2114bebb6c4f7f0540cbb542ae8e8e11","The construction of fuzzy logic systems (FLSs) using data-driven techniques has become the most popular modeling approach. However, this approach still faces critical challenges, including the difficulty in obtaining concise models for high-dimensional data and generating accurate fuzzy rules to simulate human inference mechanism. To tackle these issues, a new FLS modeling framework called data-driven elastic FLS (DD-EFLS) is proposed in this paper. The DD-EFLS has two key characteristics. First, the fuzzy rules in the rule base can use different feature subspaces that are extracted from the original high-dimensional space to yield simple and accurate rules in feature spaces of lower dimensionality. Second, fuzzy inferences from various views are implemented by embedding different rules in the corresponding subspaces to imitate human inference mechanism. Based on the DD-EFLS framework, an elastic Takagi-Sugeno-Kang (TSK) FLS modeling method (ETSK-FLS) is proposed to train the elastic TSK FLS using the concise rules and a more human-like inference mechanism for modeling tasks based on high-dimensional datasets. The characteristics and advantages of the proposed framework and the ETSK-FLS method are validated experimentally using both synthetic and real-world datasets. IEEE","concise and interpretable model; Elastic fuzzy logic systems; high-dimensional data; TSK fuzzy logic system","Clustering algorithms; Computer circuits; Data mining; Fuzzy logic; Fuzzy rules; Critical challenges; Data driven technique; Fuzzy logic system; High dimensional data; High dimensional datasets; High dimensional spaces; Key characteristics; TSK fuzzy logic; Fuzzy inference",2-s2.0-85032455330
"Hussain T., Siniscalchi S.M., Lee C., Wang S., Tsao Y., Liao W.","Experimental Study on Extreme Learning Machine Applications for Speech Enhancement",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032456314&doi=10.1109%2fACCESS.2017.2766675&partnerID=40&md5=6966c8ec765ef1e5b601f6d3e45eb672","In wireless telephony and audio data mining applications, it is desirable that noise suppression can be made robust against changing noise conditions and operate in real time (or faster). The learning effectiveness and speed of artificial neural networks are therefore critical factors in applications for speech enhancement tasks. To address these issues, we present an extreme learning machine (ELM) framework, aimed at the effective and fast removal of background noise from a single-channel speech signal, based on a set of randomly chosen hidden units and analytically determined output weights. Because feature learning with shallow ELM may not be effective for natural signals, such as speech, even with a large number of hidden nodes, hierarchical ELM (H-ELM) architectures are deployed by leveraging sparse auto-encoders. In this manner, we not only keep all the advantages of deep models in approximating complicated functions and maintaining strong regression capabilities, but we also overcome the cumbersome and time-consuming features of both greedy layer-wise pre-training and back-propagation (BP) based fine tuning schemes, which are typically adopted for training deep neural architectures. The proposed ELM framework was evaluated on the Aurora&#x2013;4 speech databases. The Aurora4 task provides relatively limited training data, and test speech data corrupted with both additive noise and convolutive distortions for matched and mismatched channels and signal-to-noise ratio (SNR) conditions. In addition, the task includes a subset of testing data involving noise types and SNR levels that are not seen in the training data. The experimental results indicate that when the amount of training data is limited, both ELM and H-ELM based speech enhancement techniques consistently outperform the conventional BP-based shallow and deep learning algorithms, in terms of standardized objective evaluations, under various testing conditions. OAPA","Artificial Neural Networks; Extreme Learning Machine; Hierarchical Extreme Learning Machines; Speech Enhancement","Additive noise; Audio acoustics; Backpropagation; Data mining; Knowledge acquisition; Learning algorithms; Learning systems; Network architecture; Neural networks; Speech; Speech communication; Speech enhancement; Complicated functions; Extreme learning machine; Learning effectiveness; Limited training data; Neural architectures; Objective evaluation; Testing conditions; Wireless telephony; Signal to noise ratio",2-s2.0-85032456314
"Wang Z., Zhao Y., Yuan Y., Wang G., Chen L.","Extreme Learning Machine for large-scale graph classification based on MapReduce",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012882041&doi=10.1016%2fj.neucom.2016.04.071&partnerID=40&md5=cf7e88c620ddf4ad0e491550266f98f4","Discriminative subgraph mining from a large collection of graph objects is a crucial problem for graph classification. Several main memory-based approaches have been proposed to mine discriminative subgraphs, but they always lack scalability and are not suitable for large-scale graph databases. Extreme Learning Machine (ELM) is a simple and efficient Single-hidden Layer Feedforward neural Networks (SLFNs) algorithm with extremely fast learning capacity. In this paper, we propose a discriminative subgraph mining approach based on ELM-Filter strategy within the scalable MapReduce computing model. We randomly partition the collection of graphs among worker nodes, and each worker applies a fast pattern evolutionary method to mine a set of discriminative subgraphs with the help of ELM-Filter strategy in its partition. And, the set of discriminative subgraphs must produce higher ELM training accuracy. The union of all such discriminative subgraphs is the mining result for the input large-scale graphs. Also, based on the proposed Support Graph Vector Model (SGVM) and ELM algorithm, we construct the graph classification model using the mined discriminative subgraphs. Extensive experimental results on both real and synthetic datasets show that our method obviously outperforms the other approaches in terms of both classification accuracy and runtime efficiency. © 2017 Elsevier B.V.","Discriminative subgraph pattern; Extreme Learning Machine; Graph classification; MapReduce","Classification (of information); Distributed computer systems; Feedforward neural networks; Filtration; Knowledge acquisition; Network layers; Classification accuracy; Evolutionary method; Extreme learning machine; Graph classification; Map-reduce; Run-time efficiency; Single-hidden layer feedforward neural networks; Subgraphs; Learning systems; Article; artificial neural network; classification algorithm; computer model; conceptual framework; data base; data mining; Extreme Learning Machine; learning algorithm; mathematical computing; measurement accuracy; priority journal; support vector machine",2-s2.0-85012882041
"Poria S., Peng H., Hussain A., Howard N., Cambria E.","Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis",2017,"Neurocomputing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012926540&doi=10.1016%2fj.neucom.2016.09.117&partnerID=40&md5=5695bf1810a54ea540c1497a1383a1de","The advent of the Social Web has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. In pace with a global deluge of videos from billions of computers, smartphones, tablets, university projectors and security cameras, the amount of multimodal content on the Web has been growing exponentially, and with that comes the need for decoding such information into useful knowledge. In this paper, a multimodal affective data analysis framework is proposed to extract user opinion and emotions from video content. In particular, multiple kernel learning is used to combine visual, audio and textual modalities. The proposed framework outperforms the state-of-the-art model in multimodal sentiment analysis research with a margin of 10–13% and 3–5% accuracy on polarity detection and emotion recognition, respectively. The paper also proposes an extensive study on decision-level fusion. © 2017 Elsevier B.V.","Classification; Convolutional neural network; Deep learning; ELM; Emotion; MKL; Multimodal sentiment analysis; Sentiment; SVM","Classification (of information); Convolution; Data mining; Deep learning; Deep neural networks; Neural networks; Convolutional neural network; Decision level fusion; Emotion; Emotion recognition; Ensemble applications; Multiple Kernel Learning; Sentiment; Sentiment analysis; Modal analysis; accuracy; Article; artificial neural network; camera; cognition; computer; data analysis; emotion; human; Internet; kernel method; priority journal; recognition; smartphone",2-s2.0-85012926540
"Adler P., Falk C., Friedler S.A., Nix T., Rybeck G., Scheidegger C., Smith B., Venkatasubramanian S.","Auditing black-box models for indirect influence",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032187305&doi=10.1007%2fs10115-017-1116-3&partnerID=40&md5=5dedc1b9bfb8d45facd684650071d00c","Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models or asserting that certain problematic attributes (such as race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the data set, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if, for example, the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence such as feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available data sets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. To further demonstrate the effectiveness of this technique, we use it to audit a black-box recidivism prediction algorithm. © 2017 Springer-Verlag London Ltd.","Algorithmic accountability; ANOVA; Black-box auditing; Deep learning; Discrimination-aware data mining; Feature influence; Interpretable machine learning",,2-s2.0-85032187305
"Zheng H., Zhou Y., Liang N., Xiao X., Sangaiah A.K., Zhao. C.","Exploiting User Mobility for Time-aware POI Recommendation in Social Networks",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032447298&doi=10.1109%2fACCESS.2017.2764074&partnerID=40&md5=31b7ba0fd1b3e2afa52f966424584160","Location-Based Social Networks (LBSNs) have recently gained increasing popularity. Timeaware Point-of-Interest(POI) recommendation is one of the most important location-aware services, which can recommend locations to a target user at the specific time based on check-in history. However, current techniques ignore user&#x2019;s mobility within a region, which plays a vital role in the POI chosen decision. Most of existing algorithms fail to capture the latent relations behind the temporal factors and geographical influence. In this paper, we propose a novel hybrid Time-aware POI Recommendation model based on User Mobility: TPR-UM, which improves the POI recommendation accuracy greatly. More specifically, by introducing the implicit region factor, we capture users&#x2019; mobility through collective user actions and geographical properties of locations. We generate the check-in patterns based on different time intervals and different regions to exploit the latent relations between temporal factors and geographical influence. Finally, we conduct comprehensive experiments on two real-world datasets, Gowalla and Foursquare. The experimental results demonstrate that our hybrid method is effective and outperforms other state-of-the-art algorithms in terms of precision, recall and F&#x03B2; measurement. OAPA","Data mining; Recommender systems; Social network services","Data mining; Geographical regions; Location; Recommender systems; Geographical property; Location aware services; Location-based social networks; Point of interest; Real-world datasets; Recommendation accuracy; Social network services; State-of-the-art algorithms; Location based services",2-s2.0-85032447298
"Zhang C., Wei Y., Li Z., Zhao Y.","Hazard-Based Design of the Bow-Tie Method to Prevent and Mitigate Mine Accidents",2017,"Journal of Failure Analysis and Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216541&doi=10.1007%2fs11668-017-0367-3&partnerID=40&md5=af4b26d2adab41117f017ef8bbf2d0d9","The mining trade involves many complicated and interrelated variables—its complex environment, abundant machinery and a plethora of other contributors to accidents. In both developed and developing countries, mining accidents have caused many casualties. However, a universal risk assessment method for mining accidents is has not yet been implemented. Among risk assessment methods, the bow-tie has been used in different industry processes and systems and has proven effective. In this paper, the bow-tie model is utilized to investigate the relationship among mining accident risks, safety measures and possible consequences. The paper illustrates the hazards of mining accidents using US mine accident data. It also shows how the consequences of mine accidents are summarized by laws and regulations of different countries. This paper also introduces a series safety measures from Chinese safety standards and how the safety measures prevent and mitigate risks. At the end of the paper, a case of mine water inrush is applied using the bow-tie approach. The results show that the method is effective for analyzing mine safety. © 2017 ASM International","Bow-tie method; Hazards; Mine accident; Safety measures; Water inrush","Accidents; Civil defense; Developing countries; Disasters; Groundwater; Hazards; Laws and legislation; Machinery; Risk assessment; Bow-tie methods; Complex environments; Laws and regulations; Mine accidents; Mine water inrush; Risk assessment methods; Safety measures; Water inrush; Safety engineering",2-s2.0-85032216541
"Xie C.-L., Niu S., Xia J.-M., Peng K., Zhang G.-Y., Yang X.-W.","Saccharopolytide A, a new cyclic tetrapeptide with rare 4-hydroxy-proline moieties from the deep-sea derived actinomycete Saccharopolyspora cebuensis MCCC 1A09850",2017,"Natural Product Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032228973&doi=10.1080%2f14786419.2017.1392956&partnerID=40&md5=04d6229c5dbf811e5cf2160190e647e7","A genome mining analysis on the deep-sea derived actinomycete Saccharopolyspora cebuensis MCCC 1A09850 indicated its potential to produce polypeptides. Accordingly, a systematic chemical investigation was conducted, which resulted in the isolation of one new cyclic tetrapeptide (saccharopolytide A, 1) and two known polyketides (2, 3) along with six other miscellaneous compounds (4‒9). Mainly by analysis of the 1D, 2D NMR and MS data, the chemical structure of saccharopolytide A was established as cyclo-(l-Leu-4-hydroxy-l-Pro-l-Phe-4-hydroxy-l-Pro). All isolates were evaluated for anti-allergic and anti-tumor bioactivities. Indol-3-carbaldehyde (4) showed weak anti-allergic effect with IC50 value of 55.75 μg/mL. And 2 showed weak anti-proliferative activity against Hela and H1299 tumor cell lines. Our results consolidate the potential of deep-sea-derived microorganisms to produce structurally interesting compounds. © 2017 Taylor & Francis","Actinomycetes; anti-allergy; anti-tumor; deep-sea; Saccharopolyspora cebuensis; secondary metabolites",,2-s2.0-85032228973
"Caha Z.","Central Europe: Ethical Overlaps of Environmental and Economic Interests in Coming Years",2017,"Science and Engineering Ethics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032012306&doi=10.1007%2fs11948-017-9982-3&partnerID=40&md5=cbe3daa2e16df15a77e4e09b3bca65fe","Despite the size and thanks to the rich brown coal reserves, the Czech Republic is one of the leading energy producers in Europe, and the 7th biggest exporter of electricity in the world. However, following the climate change mitigation, the novel energy policy that enhances the reduction of coal mining is about to be implemented. A preliminary material flow analysis of the Czech energy sector was carried out. The data obtained confirmed that this government act would result in a dramatic reduction of revenues from electricity sales. Conversely, increased costs would be necessary in order to modernize nuclear power plants and promote the production of renewable energy. In addition, the economic analysis revealed that the act might be prejudicial to economic relations in Central and Western-European countries as some of them are significantly dependent on the electricity imported from the Czech Republic. Disputes between engineers and politicians were highlighted. The aforementioned interrelations were subsequently analyzed and a conclusion was made stating that global interests should have the highest moral priority. © 2017 Springer Science+Business Media B.V.","Economics; Energy policy; Material flow analysis; Moral responsibility",,2-s2.0-85032012306
"Weed A.S., Milan J., Schwarzlaender M.","Analyses of nine years of citizen-based biological control monitoring of Dalmatian toadflax, Linaria dalmatica (Plantaginaceae) in Idaho, USA",2017,"BioControl",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032198155&doi=10.1007%2fs10526-017-9848-2&partnerID=40&md5=f976ed29e1f55c01e58978aa2f193d3c","Dalmatian toadflax has been a target for biological control in North America since the 1960s. The stem-mining weevil Mecinus janthiniformis was first released in Canada and the western United States in the mid-1990s. Since 2007, a citizen-based monitoring program in Idaho, USA has supplemented data collection to help evaluate the impact of M. janthiniformis on Dalmatian toadflax abundance and assess changes in the surrounding plant community. We monitored and analysed trends in toadflax, weevil, and the plant community abundance following weevil releases at the regional and site level (34 sites) across the state of Idaho, USA. Significant declines in toadflax cover and stem density were recorded across the majority of sites. Weevil populations have established at all release sites. The mechanistic model indicated that the population dynamics of toadflax at our sites are negatively affected by M. janthiniformis abundance. When averaged across the region, 15 years after weevil release, Dalmatian toadflax stem density and cover declined by 93 and 84%, respectively. We observed significant declines in toadflax abundance in over 75% of the sites. Changes to the surrounding plant community following weevil releases were less consistent among sites. At the regional scale we found evidence for an overall increase in average cover of native perennial grasses and other exotic weeds (primarily annual grasses and exotic forbs) but a decline in native forbs. © 2017 International Organization for Biological Control (outside the USA)","Curculionidae; Post-release assessment; Stem-miner; Weed population dynamics",,2-s2.0-85032198155
"Sivanathan A., Ritchie J.M., Lim T.","A novel design engineering review system with searchable content: knowledge engineering via real-time multimodal recording",2017,"Journal of Engineering Design",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031999183&doi=10.1080%2f09544828.2017.1393655&partnerID=40&md5=abbd329e07c5bc32cb9f62e4ec0f92a3","Cradle to grave product support has been a key issue in the engineering sector over many years, particularly because product engineering legacy knowledge is often lost during the product development process unless rigorously captured in some way. This is particularly the case during formal design reviews at any point during a product's lifecycle where engineering changes are not fully documented or where salient but important aspects of decision making are difficult to document explicitly. Though many software systems are available to support design reviews, they have not necessarily met the expectations of industry. Consequently, traditional knowledge capture methods tend to be time-consuming, costly and disruptive leading to many companies simply giving up on this crucial aspect of product development. This paper presents research carried out with regard to prototyping and testing a potential knowledge engineering capture and reuse solution, demonstrating real-time user-logging using virtual design environments focused on team-based design ‘reviews’. Called the Virtual Aided Design Engineering Review (VADER) system, it provides millisecond precision time-phased knowledge capture in an automatic and unobtrusive manner. Both structured and unstructured data are synthesised via a ubiquitous integration and temporal synchronisation (UbiITS) framework that enables interactive information mapping, retrieval and mining. VADER's frontend includes a virtual reality based 3D model view display as a multiuser collaborative interface and an auxiliary web interface for concurrent access by multiple distributed users during product design discussions. Feedback from engineers using the system demonstrated that this concept is one which believe would substantially enhance their engineering task knowledge capture, rapid retrieval and reuse capability. It was also surmised that, if required, such a system can be extended throughout the whole product development process capturing individual and team-based engineers’ inputs across the whole cradle-to-grave product life cycle. Also, due to its generic nature, this approach is not limited only to engineering applications or virtual environments but can potentially be used in other sectors using computer-based technologies of any kind. © 2017 Informa UK Limited, trading as Taylor & Francis Group","cyber-physical systems; design review; industry 4.0; Knowledge capture; knowledge engineering; multimodal time synchronisation; product lifecycle management; user logging","Cyber Physical System; Decision making; Embedded systems; Knowledge engineering; Knowledge management; Precision engineering; Product design; Product development; Virtual reality; Computer based technologies; Design review; Engineering applications; Interactive informations; Knowledge capture; Multi-modal; Product development process; Product life cycle management; Life cycle",2-s2.0-85031999183
"Sandhu R., Kaur N., Sood S.K., Buyya R.","TDRM: tensor-based data representation and mining for healthcare data in cloud computing environments",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031927995&doi=10.1007%2fs11227-017-2163-y&partnerID=40&md5=aaf1e1b8e12ad06618b2a1c22ec2fc26","Big data analytics proved to be one of the most influential forces in today’s competitive business environments due to its ability to generate new insights by processing a large volume and variety of data. Storing as well as mining these datasets is one of the primary challenges of the big data era. If data are stored in a well-defined pattern, then its updation mining and deletion processes become easy. In this paper, granular computing concept is used to store heterogeneous data in the format of tensor. A multi-dimensional matrix, also known as tensor, stores data in the raw format, and then, raw tensor is replicated to multiple tensors of different abstraction levels based on concept hierarchy of each attribute. Mathematical foundation of tensor formation and query processing are developed. The proposed method is successful in creating tensors of a diabetes dataset proving its applicability. The proposed system provides faster computation, low response time, better privacy and high relevancy as compared to baseline PARAFAC2 and CANDELINC tensor analysis method when run on Microsoft Azure cloud infrastructure. Different levels of information granules in the form of tensors make data storage and its query processing effective. © 2017 Springer Science+Business Media, LLC","Big data; Cloud computing; Data mining; Granular computing; Healthcare data; PARAFAC2; Tensor","Cloud computing; Data mining; Digital storage; Distributed computer systems; Granular computing; Health care; Information granules; Query processing; Tensors; Windows operating system; Cloud computing environments; Cloud infrastructures; Competitive business; Concept hierarchies; Data representations; Mathematical foundations; Multi-dimensional matrices; PARAFAC2; Big data",2-s2.0-85031927995
"Deng S., Wang B., Huang S., Yue C., Zhou J., Wang G.","Self-Adaptive Framework for Efficient Stream Data Classification on Storm",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440452&doi=10.1109%2fTSMC.2017.2757029&partnerID=40&md5=b37f8252a40210e9649a441fcf9ac9f5","In this era of big data, stream data classification which is one of typical data stream applications has become more and more significant and challengeable. In these applications, it is obvious that data classification is much more frequent than model training. The ratio of stream data to be classified is rapid and time-varying, so it is an important problem to classify the stream data efficiently with high throughput. In this paper, we first analyze and categorize the current data stream machine learning algorithms according to their data structures. Then, we propose stream data classification topology (SDC-Topology) on Storm. For the classification algorithms based on the matrix, we propose self-adaptive stream data classification framework (SASDC-Framework) for efficient stream data classification on Storm. In SASDC-Framework, all the data sets arriving at the same unit time are partitioned into subsets with the nearly best partition size and processed in parallel. To select the nearly best partition size for the stream data sets efficiently, we adopt bisection method strategy and inverse distance weighted strategy. Extreme learning machine, which is a fast and accurate machine learning method based on matrix calculating, is used to test the efficiency of our proposals. According to evaluation results, the throughputs based on SASDC-Framework are 8-35 times higher than those based on SDC-Topology and the best throughput is more than 40,000 prediction requests per second in our environment. IEEE","Artificial neural networks; Classification; extreme learning machine (ELM); Learning systems; partition strategy; Proposals; Real-time systems; Storm; Storms; stream data; Throughput; Training","Artificial intelligence; Big data; Data mining; Interactive computer systems; Inverse problems; Knowledge acquisition; Learning algorithms; Learning systems; Matrix algebra; Network function virtualization; Neural networks; Personnel training; Real time systems; Storms; Throughput; Topology; Classification algorithm; Data classification; Extreme learning machine; Inverse distance weighted; Machine learning methods; Proposals; Stream data; Stream data classifications; Classification (of information)",2-s2.0-85032440452
"Kaveri V.V., Maheswari V.","A framework for recommending health-related topics based on topic modeling in conversational data (Twitter)",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032018749&doi=10.1007%2fs10586-017-1263-z&partnerID=40&md5=89b0a8981bc0aaa7bffdf6f56d9e2a16","Social media is an environment where it enables the user’s to create and share content or participate in all social networking activities; it has given rise to information overloading problem and knowledge starvation. Generally, in large conversational dataset like Twitter it is very difficult to identifying public health-related topics. Machine learning is the area which helps us to overcome this situation. By applying probabilistic unsupervised machine learning we have identified latent topics from the text corpora. Topic modeling algorithms are probabilistic generative models which is used to analyze the words of the original texts to identify themes that run through them, how connected to each other by themes, and how the topics change over (Blei in Commun ACM 55(4):77–84, 2012). In this paper we have proposed a framework for recommending health-related topics by applying an enhanced topic model hierarchical latent Dirichlet allocation for identifying public health-related topics and themes in tweets. We have tested our model by exploring tobacco usage across conversational data, as well as tested with the subset of data generated by tobacco related queries. Our test results indicate most of the public health related issues are uncovered in our model. However, this model provides relevant topic across the tobacco subset. © 2017 Springer Science+Business Media, LLC","Data mining; HLDA; Public health; Social media; Social networks; Tobacco use; Topic modeling","Artificial intelligence; Health; Learning systems; Public health; Recommender systems; Social networking (online); Statistics; Tobacco; Generative model; HLDA; Information overloading; Latent Dirichlet allocation; Social media; Topic Modeling; Topic modeling algorithms; Unsupervised machine learning; Data mining",2-s2.0-85032018749
"Zhu X., Zhang S., Hu R., Zhu Y., song J.","Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440732&doi=10.1109%2fTKDE.2017.2763618&partnerID=40&md5=3d1d6692e5a722e617945c9642273cc4","This paper studies a robust unsupervised spectral feature selection based on an efficient strategy of preserving both the local and global structures among training samples. It incorporates the graph matrix construction and the feature selection together into a data mining process using the local and global correlations among the features. In this robust approach, the self-expressiveness of the features is first advocated for removing the effect of irrelevant features. And then, a low-rank constraint on the weight matrix is designed for preserving the global structure among the samples based on the global correlation among the features. Third, both the local and global correlations among the features are applied to learn a dynamic graph matrix from the intrinsic space of original data, aiming to preserve the local structure among the samples. Finally, the graph matrix and the intrinsic space are iteratively updated until they achieve the optimal result of feature selection. Experimental analysis on tens benchmark datasets showed that the proposed approach outperformed the state-of-the-art feature selection methods in terms of classification performance. OAPA","Correlation; dimensionality reduction; Feature extraction; Feature selection; graph matrix; Kernel; Redundancy; Robustness; subspace learning; Training","Benchmarking; Classification (of information); Correlation methods; Data mining; Iterative methods; Matrix algebra; Personnel training; Redundancy; Robustness (control systems); Benchmark datasets; Classification performance; Data mining process; Dimensionality reduction; Experimental analysis; Feature selection methods; Kernel; Subspace learning; Feature extraction",2-s2.0-85032440732
"Hong Z., Chen Y., Mahmassani H.S.","Recognizing Network Trip Patterns Using a Spatio-Temporal Vehicle Trajectory Clustering Algorithm",2017,"IEEE Transactions on Intelligent Transportation Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032455615&doi=10.1109%2fTITS.2017.2754401&partnerID=40&md5=68257b02d86877ad4d97e1355670dd93","This paper presents a spatio-temporal trajectory clustering method for vehicle trajectories in transportation networks to identify heterogeneous trip patterns and explore underlying network assignment mechanisms. The proposed algorithm ST-TOPOSCAN is designed to consider both temporal and spatial information in trajectories. We adopt the time-dependent shortest-path distance measurement and take advantage of topological relations of a predefined network to discover the shared sub-paths among trajectories and construct the clusters. The proposed algorithm is implemented with a trajectory dataset obtained in the Chicago area. The results confirm the method's ability to extract and generate spatio-temporal (sub-)trajectory clusters and identify trip patterns. Extensive numerical experiments verify the method's performance and computational efficiency. Through spatio-temporal data mining, this paper contributes to exploring traffic system dynamics and advancing state-of-the-art spatio-temporal clustering for vehicle trajectories. IEEE","Algorithm design and analysis; Clustering algorithms; Data mining; Data mining; dynamic network trip patterns; Roads; shortest path distance.; Spatial databases; spatio-temporal data clustering; Trajectory; vehicle trajectories","Cluster analysis; Computational efficiency; Data mining; Graph theory; Numerical methods; Traffic control; Trajectories; Vehicles; Algorithm design and analysis; Dynamic network; Roads; Shortest path; Spatial database; Spatio-temporal data; Vehicle trajectories; Clustering algorithms",2-s2.0-85032455615
"Li L., Li W., Zou B., Wang Y., Tang Y.Y., Han H.","Learning With Coefficient-Based Regularized Regression on Markov Resampling",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450025&doi=10.1109%2fTNNLS.2017.2757140&partnerID=40&md5=63d96fcfb4e0a4d50d3e424d34c0d229","Big data research has become a globally hot topic in recent years. One of the core problems in big data learning is how to extract effective information from the huge data. In this paper, we propose a Markov resampling algorithm to draw useful samples for handling coefficient-based regularized regression (CBRR) problem. The proposed Markov resampling algorithm is a selective sampling method, which can automatically select uniformly ergodic Markov chain (u.e.M.c.) samples according to transition probabilities. Based on u.e.M.c. samples, we analyze the theoretical performance of CBRR algorithm and generalize the existing results on independent and identically distributed observations. To be specific, when the kernel is infinitely differentiable, the learning rate depending on the sample size $m$ can be arbitrarily close to O(m&#x207B;&#x00B9;) under a mild regularity condition on the regression function. The good generalization ability of the proposed method is validated by experiments on simulated and real data sets. IEEE","Algorithm design and analysis; Big Data; Coefficient-based regularized regression (CBRR); Kernel; learning rate; Markov processes; Markov resampling; Prediction algorithms; Random variables; Sampling methods; uniformly ergodic Markov chain (u.e.M.c.).","Chains; Data mining; Learning algorithms; Markov processes; Probability; Random variables; Regression analysis; Algorithm design and analysis; Coefficient-based regularized regression (CBRR); Ergodic markov chains; Kernel; Learning rates; Prediction algorithms; Resampling; Sampling method; Big data",2-s2.0-85032450025
"Tayefi M., Saberi-Karimian M., Esmaeili H., Zadeh A.A., Ebrahimi M., Mohebati M., Heidari-Bakavoli A., Azarpajouh M.R., Heshmati M., Safarian M., Nematy M., Parizadeh S.M.R., Ferns G.A., Ghayour-Mobarhan M.","Evaluating of associated risk factors of metabolic syndrome by using decision tree",2017,"Comparative Clinical Pathology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032028623&doi=10.1007%2fs00580-017-2580-6&partnerID=40&md5=9382e87e702ff91dfd1b2d594c562743","Metabolic syndrome is a clustering of metabolic abnormalities that include central obesity, dyslipidemia, insulin resistance, and increased blood pressure. The aim of this study was to evaluate and identify the risk factors associated with metabolic syndrome by using a decision tree algorithm as a data mining tool. A total of 6578 individuals were included in the analysis using a body mass index (BMI) cutoff &gt;= 25 kg/m2 for the definition of overweight, in accordance with International Diabetic Federation (IDF) criteria. Subjects with obesity plus two or more of the criteria for defining metabolic syndrome were included in the metabolic syndrome group. Of the 6578 subjects, 70% (4539 subjects) were selected as a “training” dataset and 30% (2039 cases) were used as the testing dataset to evaluate the performance of decision tree. Two models were evaluated. In model I, age, sex, educational level, marriage and job status, fasted serum triglyceride (TG), total cholesterol (TC), high-density lipoprotein cholesterol (HDL-C), uric acid, fasting blood glucose (FBG), high sensitive C-reactive protein (Hs-CRP), systolic (SBP) and diastolic (DBP) blood pressure, and physical activity level were considered as input variables and in model II, age, gender, Hs-CRP, white blood cell (WBC), red blood cell (RBC), hemoglobin (HGB), hematocrit (HCT), mean corpuscular volume (MCV), mean corpuscular hemoglobin (MCH), Platelets (PLT), red cell distribution width (RDW), and platelet distribution width (PDW) were used as input variables. The validation of the model was assessed by constructing a receiver operating characteristic (ROC) curve. The results showed that in model I, serum fasted TG was the most important associated risk factor for metabolic syndrome. In model II, serum Hs-CRP was identified as a risk factor of metabolic syndrome. The sensitivity, specificity, accuracy, and the area under the ROC curve (AUC) values for model I were 99%, 94%, 97% and 0.972 and for model II were 74%, 77%, 76% and 0.812, respectively. Our findings in model I suggest that the IDF criteria are suitable for identifying individuals within the Iranian population into those with, or without MetS. Furthermore, model II showed that serum Hs-CRP concentrations were identified as a risk factor for metabolic syndrome within the Iranian population. © 2017 Springer-Verlag London Ltd.","Data mining; Decision tree; Metabolic syndrome",,2-s2.0-85032028623
"Gan Y., Luo F., Liu J., Lei B., Zhang T., Liu K.","Feature Extraction Based Multi-Structure Manifold Embedding for Hyperspectral Remote Sensing Image Classification",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032443426&doi=10.1109%2fACCESS.2017.2766242&partnerID=40&md5=33edfc51111f53c7fb42b5f5edbf4bd8","Hyperspectral remote sensing image is a typical high-dimensional data with a large number of redundant informantion, which will impact the classification accuracy. Feature extraction is an effective method to reduce the redundancy of hyperspectral image (HSI) and improve the classification performance. However, most feature extraction methods just consider a single structure information of HSI that will lose some valuable information. To address the drawback, we proposed an unsupervised feature extraction method termed multi-structure manifold embedding (MSME) for HSI classification. Firstly, MSME utilizes sparse representation to obtain the sparse coefficients of HSI data. Then, it constructs a sparse graph and a sparse hypergraph with the sparse coefficients. We use the sparse graph, the sparse hypergraph, and the local linear property to represent different intrinsic structures of HSI. Finally, we construct a feature learning method with these structures to achieve an optimal projection matrix for feature extraction. MSME makes full use of the complementarity of different structures to reveal the intrinsic properties of HSI and improve the discriminating power of features for classification. Experiments on the Salinas and PaviaU data sets show that the proposed MSME algorithm achieves the best classification results than other state-of-the-art methods. OAPA","Feature extraction; feature extraction; graph embedding; Hyperspectral remote sensing image; Hyperspectral sensors; Linear programming; manifold learning; Manifolds; Sparse matrices; sparse representation; Symmetric matrices","Automobile engine manifolds; Clustering algorithms; Data mining; Extraction; Feature extraction; Graph theory; Image classification; Image enhancement; Image processing; Linear programming; Matrix algebra; Remote sensing; Spectroscopy; Graph embeddings; Hyperspectral Remote Sensing Image; Hyperspectral sensors; Manifold learning; Sparse matrices; Sparse representation; Symmetric matrices; Classification (of information)",2-s2.0-85032443426
"Saleemi M., Anjum M., Rehman M.","eServices Classification, Trends, and Analysis: A Systematic Mapping Study",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032433044&doi=10.1109%2fACCESS.2017.2766287&partnerID=40&md5=7a9f537c6bfbe0b649416b1e29114d18","The concept of eServices originated in early 2000s in the field of business and commerce. The popularity of this concept increased over time and various domains emerged by integrated this concept such as eHealth, eAgricuture, eLearning and many more. eServices concept is also used in relation to web services and service oriented computing. Therefore, a thorough study is required to identify to what extent this concept is being employed by research community and how this concept has been emerged overtime. In this research, a systematic mapping study (also known as scoping study) is conducted to classify evidences available on eServices and to identify state of the art research on this topic. Mapping study is a form of systematic literature review (SLR) that provides a systematic procedure to classifying existing information concerning a particular research question in an unbiased manner. The search procedure identified 806 studies of which 318 were selected for full analysis during the years: 2000- 2016. The study findings indicate that the concept of eServices is widely adopted by different domains. The application of eServices is discussed in relation to its implemnatation technology such as webservices, along with other concepts such as service oriented computing, and by integrating it with different deciplines to form new areas such as eGovernment, eBusiness, eHealth, and eLearning. The study further idenified that eServices provision and adoption are the areas discussed excessively, at times by directly involving eService users to study adoption patterns. From the findings, we can conclude that eServices has not only introduced new areas in existing deciplines/domain but also has merged itself with emerging concepts in computing decipline. In coming years, more areas are expected to emerge. The real need is to explore these emerging areas in depth as breadth of eServices utilization is already there. OAPA","Computer science; Data mining; e-services; electronic services; eServices; Ice; Libraries; mapping study; Market research; Systematics","Classification (of information); Computer science; Data mining; Digital libraries; Distributed computer systems; E-learning; Ice; Libraries; Mapping; E- services; Electronic services; Mapping studies; Market researches; Systematics; Web services",2-s2.0-85032433044
"Azhari A., Ozbay U.","Role of Geometry and Stiffness Contrast on Stability of Open Pit Mines Struck by Earthquakes",2017,"Geotechnical and Geological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032032255&doi=10.1007%2fs10706-017-0390-x&partnerID=40&md5=c7c785f629d5159fe98f3082b088a8e4","We develop a database containing 95 historical natural slope and 37 tailing dam failures triggered by earthquake. The database analyses show that earthquake-triggered failures in natural slopes are mostly initiated in the narrow ridges. We also collect published data on 177 open pit mines struck by earthquakes of which 85 mines are located in the seismically active areas. The database indicates no reportable failures triggered by earthquakes in the affected mines. We employ a finite element code to investigate the geometrical and stiffness contrast effects distinguishing the behavior of natural slopes and tailing dams from open pit slopes experiencing earthquake dynamic loading. It is concluded that narrow ridge and the top soil layer in natural slopes and the hill-shaped geometry and unconsolidated top later of tailing dams amplifies the horizontal peak ground velocity by factor of 8 compared to open pit mine slopes. Our numerical modeling of the rock slopes suggests that the typical pit geometry and the competent material in open pit mines boost the slope stability through decreasing the topographical amplification effects. © 2017 Springer International Publishing AG","Natural slopes; Numerical analysis; Open pit mines; Rock slopes; Seismic stability analysis; Site effects; Tailing dams","Dams; Database systems; Dynamic loads; Earthquakes; Geometry; Geophysics; Numerical analysis; Slope stability; Soils; Stiffness; Tailings; Natural slopes; Rock slope; Seismic stability; Site effects; Tailing dam; Open pit mining",2-s2.0-85032032255
"Jenghara M.M., Ebrahimpour-Komleh H., Rezaie V., Nejatian S., Parvin H., Yusof S.K.S.","Imputing missing value through ensemble concept based on statistical measures",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032035373&doi=10.1007%2fs10115-017-1118-1&partnerID=40&md5=0e077a4f0507f0476bdaf1dfa0a70188","Many datasets include missing values in their attributes. Data mining techniques are not applicable in the presence of missing values. So an important step in preprocessing of a data mining task is missing value management. One of the most important categories in missing value management techniques is missing value imputation. This paper presents a new imputation technique. The proposed imputation technique is based on statistical measurements. The suggested imputation technique employs an ensemble of the estimators built to estimate the missing values based on positive and negative correlated observed attributes separately. Each estimator guesses a value for a missed value based on the average and variance of that feature. The average and variance of the feature are estimated from the non-missed values of that feature. The final consensus value for a missed value is the weighted aggregation of the values estimated by different estimators. The chief weight is attribute correlation, and the slight weight is dependent to kernel function such as kurtosis, skewness, number of involved samples and composition of them. The missing values are deliberately produced randomly at different levels. The experimentations indicate that the suggested technique has a good accuracy in comparison with the classical methods. © 2017 Springer-Verlag London Ltd.","Ensemble; Imputation; Kurtosis; Missing value management; Skewness; Statistical distribution structure",,2-s2.0-85032035373
"Khoo S.J., Johar M., Low K.O., Wong K.J.","Interfacial shear strength characterisation of alkali treated bamboo bundle–polyester composites using an improved technique",2017,"Plastics, Rubber and Composites",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032023886&doi=10.1080%2f14658011.2017.1391965&partnerID=40&md5=9d76fe6d0940f4892effbc266d12fc93","This study examined the influences of alkali concentration on the interfacial characteristics of bamboo–polyester. Pull-out tests were carried out using a newly designed jig to minimise the fibre breakage during clamping. Bamboo bundles were embedded at 3, 5, 7 and 10 mm and alkali concentrations ranged from 0, 1, 3, 5 to 7 wt-%. The attenuated total reflectance-Fourier transform infrared spectroscopy spectra revealed hemicelluloses was observed at ∼1030 cm−1. The pull-out results showed that interfacial characteristics were not influenced by the embedded length. Furthermore, the highest apparent interfacial shear strength was attained at 3 wt-% concentration, with approximately three times higher compared to the untreated one. A comparison with data from the literature showed that both untreated and treated bamboo/polyester composites have the weakest interfacial bonding. Scanning electron micrographs revealed that alkali treatment has resulted in interface enhancement through chemical modification, mechanical interlocking and frictional contact. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute.","alkali treatment; Bamboo bundle; interfacial shear strength; polyester; pull-out; scanning electron micrographs","Chemical bonds; Chemical modification; Fourier transform infrared spectroscopy; Polyesters; Scanning electron microscopy; Alkali treatment; Apparent interfacial shear strength; Attenuated total reflectance Fourier transform infrared spectroscopy; Interfacial characteristics; Interfacial shear strength; Mechanical interlocking; Pull out; Scanning electron micrographs; Bamboo",2-s2.0-85032023886
"Cha D., Wang X., Kim J.W.","Assessing lightning and wildfire hazard by land properties and cloud to ground lightning data with association rule mining in Alberta, Canada",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032577383&doi=10.3390%2fs17102413&partnerID=40&md5=46de6ca5f6616232d42e2c32cbfe3405","Hotspot analysis was implemented to find regions in the province of Alberta (Canada) with high frequency Cloud to Ground (CG) lightning strikes clustered together. Generally, hotspot regions are located in the central, central east, and south central regions of the study region. About 94% of annual lightning occurred during warm months (June to August) and the daily lightning frequency was influenced by the diurnal heating cycle. The association rule mining technique was used to investigate frequent CG lightning patterns, which were verified by similarity measurement to check the patterns’ consistency. The similarity coefficient values indicated that there were high correlations throughout the entire study period. Most wildfires (about 93%) in Alberta occurred in forests, wetland forests, and wetland shrub areas. It was also found that lightning and wildfires occur in two distinct areas: frequent wildfire regions with a high frequency of lightning, and frequent wild-fire regions with a low frequency of lightning. Further, the preference index (PI) revealed locations where the wildfires occurred more frequently than in other class regions. The wildfire hazard area was estimated with the CG lightning hazard map and specific land use types. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Association rule mining; Cloud to ground (CG) lightning; Hotspot analysis; Wildfire hazard in Alberta","Association rules; Fires; Forestry; Hazards; Land use; Lightning; Wetlands; Alberta; Cloud-to-ground lightning; High frequency HF; Hot spot; Lightning frequency; Rule mining techniques; Similarity coefficients; Similarity measurements; Clouds",2-s2.0-85032577383
"Wu X., Liu C., Wu G.","Spatial-Temporal Analysis and Stability Investigation of Coastline Changes: A Case Study in Shenzhen, China",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032451669&doi=10.1109%2fJSTARS.2017.2755444&partnerID=40&md5=8a0672671551c815df156941034f2656","Coastlines are some of the most important dynamic linear features on the Earth&#x0027;s surface. Conducting research on coastline changes is essential to improve the environmental management and development of the coast. In this study, Landsat satellite images that cover the entire coastal zone of Shenzhen, which was the first special economic zone in China, are used to study coastline changes. Multitemporal coastline information for Shenzhen from 1988 to 2015 is extracted based on the object-oriented classification approach. The overall classification accuracy of this approach can reach 85&#x0025; or more when it is used to classify the land-use types present within the coastal zone in this study. Moreover, coastline change rates are calculated using a point-based approach. Coastline stability changes are analyzed, and driving rules are explored, using a data-mining approach that addresses natural and social factors. The research results demonstrate that the object-based approach can extract accurate coastline information from Landsat images and that the coastlines in Shenzhen have varied with time and location. The coastline changes in Shenzhen from 1988 to 2015 can be divided into two stages: one displays accelerated change (1988&#x2013;2003), whereas the other displays decelerated change (2003&#x2013;2015). Completely different change characteristics of coastline stability are found between the eastern and western coastlines in Shenzhen, and the differences are mainly reflected in the coastline&#x0027;s morphological changes and varying rules. IEEE","Coastline; coastline change rate (CCR); coastline stability; Data mining; data-mining technology; Earth; objected-based techniques; Remote sensing; Satellites; Sea measurements; Stability criteria","Classification (of information); Coastal zones; Data mining; Earth (planet); Environmental management; Land use; Remote sensing; Research and development management; Satellite imagery; Satellites; Stability; Stability criteria; Coastline; Coastline changes; Data mining technology; objected-based techniques; Sea measurements; Landforms",2-s2.0-85032451669
"Wu H.-H., Küçükyavuz S.","A two-stage stochastic programming approach for influence maximization in social networks",2017,"Computational Optimization and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031917986&doi=10.1007%2fs10589-017-9958-x&partnerID=40&md5=ae6124a112890b45b1c9e368a66e3636","We consider stochastic influence maximization problems arising in social networks. In contrast to existing studies that involve greedy approximation algorithms with a 63% performance guarantee, our work focuses on solving the problem optimally. To this end, we introduce a new class of problems that we refer to as two-stage stochastic submodular optimization models. We propose a delayed constraint generation algorithm to find the optimal solution to this class of problems with a finite number of samples. The influence maximization problems of interest are special cases of this general problem class. We show that the submodularity of the influence function can be exploited to develop strong optimality cuts that are more effective than the standard optimality cuts available in the literature. Finally, we report our computational experiments with large-scale real-world datasets for two fundamental influence maximization problems, independent cascade and linear threshold, and show that our proposed algorithm outperforms the basic greedy algorithm of Kempe et al. (Proceedings of the ninth ACM SIGKDD international conference on knowledge discovery and data mining, KDD’03, New York, NY, USA, ACM, pp 137–146, 2003). © 2017 Springer Science+Business Media, LLC","Independent cascade; Influence maximization; Linear threshold; Social networks; Stochastic programming; Submodularity","Approximation algorithms; Data mining; Optimization; Social networking (online); Stochastic models; Stochastic programming; Stochastic systems; Computational experiment; Greedy approximation algorithms; Influence maximizations; Knowledge discovery and data minings; Linear threshold; Submodular optimizations; Submodularity; Two-stage stochastic programming; Problem solving",2-s2.0-85031917986
"Huang S., Wang H., Li T., Li T., Xu Z.","Robust graph regularized nonnegative matrix factorization for clustering",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031936297&doi=10.1007%2fs10618-017-0543-9&partnerID=40&md5=89a79e04da91f0e4b6efef7e33306362","Nonnegative matrix factorization and its graph regularized extensions have received significant attention in machine learning and data mining. However, existing approaches are sensitive to outliers and noise due to the utilization of the squared loss function in measuring the quality of graph regularization and data reconstruction. In this paper, we present a novel robust graph regularized NMF model (RGNMF) to approximate the data matrix for clustering. Our assumption is that there may exist some entries of the data corrupted arbitrarily, but the corruption is sparse. To address this problem, an error matrix is introduced to capture the sparse corruption. With this sparse outlier matrix, a robust factorization result could be obtained since a much cleaned data could be reconstructed. Moreover, the (Formula presented.)-norm function is used to alleviate the influence of unreliable regularization which is incurred by unexpected graphs. That is, the sparse error matrix alleviates the impact of noise and outliers, and the (Formula presented.)-norm function leads to a faithful regularization since the influence of the unreliable regularization errors can be reduced. Thus, RGNMF is robust to unreliable graphs and noisy data. In order to solve the optimization problem of our method, an iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Experimental results show that the proposed method consistently outperforms many state-of-the-art methods. © 2017 The Author(s)","$$\ell _{1}$$ℓ1-norm function; Clustering; Nonnegative matrix factorization; Robust regularization","Crime; Data mining; Errors; Factorization; Iterative methods; Learning systems; Optimization; Statistics; Clustering; Data reconstruction; Nonnegative matrix factorization; Optimization problems; Regularization errors; Robust regularization; State-of-the-art methods; Updating algorithm; Matrix algebra",2-s2.0-85031936297
"Gómez-Rodríguez C., Alonso-Alonso I., Vilares D.","How important is syntactic parsing accuracy? An empirical evaluation on rule-based sentiment analysis",2017,"Artificial Intelligence Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031905148&doi=10.1007%2fs10462-017-9584-0&partnerID=40&md5=cd6c5001014297f83b2f2ebd5cdff522","Syntactic parsing, the process of obtaining the internal structure of sentences in natural languages, is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech. Sentiment analysis is one example of application for which parsing has recently proven useful. In recent years, there have been significant advances in the accuracy of parsing algorithms. In this article, we perform an empirical, task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art rule-based sentiment analysis system that determines the polarity of sentences from their parse trees. In particular, we evaluate the system using four well-known dependency parsers, including both current models with state-of-the-art accuracy and more innacurate models which, however, require less computational resources. The experiments show that all of the parsers produce similarly good results in the sentiment analysis task, without their accuracy having any relevant influence on the results. Since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms, this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser; and parsing researchers should investigate models that improve speed further, even at some cost to accuracy. © 2017 Springer Science+Business Media B.V.","Artificial intelligence; Natural language processing; Sentiment analysis; Syntactic parsing","Artificial intelligence; Computational linguistics; Context free grammars; Cost benefit analysis; Data mining; Natural language processing systems; Computational costs; Computational resources; Empirical evaluations; Internal structure; Natural language text; Natural languages; Sentiment analysis; Syntactic parsing; Syntactics",2-s2.0-85031905148
"Yang Q., Nanayakkara G.K., Drummer C., Sun Y., Johnson C., Cueto R., Fu H., Shao Y., Wang L., Yang W.Y., Tang P., Liu L.-W., Ge S., Zhou X.-D., Khan M., Wang H., Yang X.","Low-intensity ultrasound-induced anti-inflammatory effects are mediated by several new mechanisms including gene induction, immunosuppressor cell promotion, and enhancement of exosome biogenesis and docking",2017,"Frontiers in Physiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032220542&doi=10.3389%2ffphys.2017.00818&partnerID=40&md5=72fbf6eb4c42d37f07dade2395bc2622","Background: Low-intensity ultrasound (LIUS) was shown to be beneficial in mitigating inflammation and facilitating tissue repair in various pathologies. Determination of the molecular mechanisms underlying the anti-inflammatory effects of LIUS allows to optimize this technique as a therapy for the treatment of malignancies and aseptic inflammatory disorders. Methods: We conducted cutting-edge database mining approaches to determine the anti-inflammatory mechanisms exerted by LIUS. Results: Our data revealed following interesting findings: (1) LIUS anti-inflammatory effects are mediated by upregulating anti-inflammatory gene expression; (2) LIUS induces the upregulation of the markers and master regulators of immunosuppressor cells including MDSCs (myeloid-derived suppressor cells), MSCs (mesenchymal stem cells), B1-B cells and Treg (regulatory T cells); (3) LIUS not only can be used as a therapeutic approach to deliver drugs packed in various structures such as nanobeads, nanospheres, polymer microspheres, and lipidosomes, but also can make use of natural membrane vesicles as small as exosomes derived from immunosuppressor cells as a novel mechanism to fulfill its anti-inflammatory effects; (4) LIUS upregulates the expression of extracellular vesicle/exosome biogenesis mediators and docking mediators; (5) Exosome-carried anti-inflammatory cytokines and anti-inflammatory microRNAs inhibit inflammation of target cells via multiple shared and specific pathways, suggesting exosome-mediated anti-inflammatory effect of LIUS feasible and (6) LIUS-mediated physical effects on tissues may activate specific cellular sensors that activate downstream transcription factors and signaling pathways. Conclusions: Our results have provided novel insights into the mechanisms underlying anti-inflammatory effects of LIUS, and have provided guidance for the development of future novel therapeutic LIUS for cancers, inflammatory disorders, tissue regeneration and tissue repair. © 2017 Yang, Nanayakkara, Drummer, Sun, Johnson, Cueto, Fu, Shao, Wang, Yang, Tang, Liu, Ge, Zhou, Khan, Wang and Yang.","Anti-inflammatory gene induction; Exosomes; Immunosuppressor cells; Ultrasound; Ultrasound for cancer therapy","drug carrier; lipidosome; microRNA; microsphere; nanobead; nanosphere; polymer; transcription factor FOXP3; unclassified drug; Article; B lymphocyte; B1 B cell; biogenesis; CD4+ T lymphocyte; controlled study; exosome; gene expression regulation; gene induction; inflammation; low intensity ultrasound; membrane vesicle; mesenchymal stem cell; myeloid-derived suppressor cell; regulatory T lymphocyte; signal transduction; suppressor cell; target cell; ultrasound; upregulation",2-s2.0-85032220542
"Zhou H., Hirasawa K.","Evolving temporal association rules in recommender system",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031926699&doi=10.1007%2fs00521-017-3217-z&partnerID=40&md5=132c5446635d7f7d6d4ca516f5add01e","This research involves implementation of genetic network programming (GNP) and ant colony optimization (ACO) to solve the sequential rule mining problem for commercial recommendations in time-related transaction databases. Excellent recommender systems should be capable of detecting the customers’ preference in a proactive and efficient manner, which requires exploring customers’ potential needs with an accurate and timely approach. Due to the changing nature of customers’ preferences and the differences with the traditional find-all-then-prune approach, the interesting temporal association rules are extracted by the metaheuristics, genetic algorithms-based method of GNP. Additionally, a useful model is constructed using the obtained rules to forecast future customer needs and an ACO approach to evolve the online recommender system continuously. The methodology is experimentally evaluated in a real-world application by analysing the customer database of an online supermarket. © 2017 The Author(s)","Ant colony optimization (ACO); Data mining; Genetic network programming (GNP); Recommender system; Temporal association rule mining","Ant colony optimization; Artificial intelligence; Association rules; Data mining; Genetic algorithms; Optimization; Sales; Ant Colony Optimization (ACO); Customer database; Genetic network programming; Online recommender systems; Online supermarkets; Temporal association rule; Temporal association rule minings; Transaction database; Recommender systems",2-s2.0-85031926699
"El Sayed A.R., El Chakik A., Alabboud H., Yassine A.","Efficient 3D point clouds classification for face detection using linear programming and data mining",2017,"Imaging Science Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032336452&doi=10.1080%2f13682199.2017.1376772&partnerID=40&md5=dc255bae30b22b3978bbb121bc71ce87","Most of the applications related to security and biometric rely on skin region detection such as face detection, adult 3D objects filtering, and gesture recognition. In this paper, we propose a robust method for skin detection on 3D coloured point clouds. Then, we extend this method to solve the problem of 3D face detection. To do so, we construct a weighted graph from initial coloured 3D point clouds. Then, we present a linear programming algorithm using a predictive model based on a data mining approach to classify and label graph vertices as skin and non-skin regions. Moreover, we apply some refinement rules on skin regions to confirm the presence of a face. Furthermore, we demonstrate the robustness of our method by showing and analysing some experimental results. Finally, we show that our method deals with many data that can be represented by a weighted graph such as 2D images and 3D models. © 2017 The Royal Photographic Society","3D point clouds; data mining; face detection; face model; linear programming; skin detection","Content based retrieval; Data mining; Graphic methods; Linear programming; 3D point cloud; Face modeling; Linear programming algorithm; Predictive modeling; Region detection; Robust methods; Skin Detection; Weighted graph; Face recognition",2-s2.0-85032336452
"Wang J., Wang G., Zhou M.","Bimodal Vein Data Mining via Cross-Selected-Domain Knowledge Transfer",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450388&doi=10.1109%2fTIFS.2017.2766039&partnerID=40&md5=4b8740fb5fecf869e671e50d7e9f857d","Recent success in large-scale image recognition challenge (i.e. ImageNet) fully demonstrates the capability of Deep Neural Network (DNN) in learning complex and semantic representation, and this also motivates the generation of transfer learning model, which fine-tunes state-of-the-art DNN models with other small-scale database for better performance. Driven by such idea, task-specific DNN model fine-tuned from VGG-face is constructed for both gender and identity recognition with hand vein information. Unlike the traditional transfer learning models which fine-tune directly from source to target, we leverage the coarse-to-fine scheme to train the task-specific models in a step-aware way such that the inherent correlation between the neighboring databases could serve as initialization base to relieve the problem of over-fitting, which is inevitable with the small-scaled hand vein database, and also speed up the convergence. Besides, the task-driven network training idea, which involves joint optimization of linear regression classifier and network parameters, is also adopted during training of each model to obtain more discriminative representation for specified tasks. Instead of adopting the trained linear regression classifier for gender and identity classification, the large margin distribution machine (LDM) is introduced to ensure the discriminative and generalization performance of the model simultaneously, and it should be noted that before feeding the gender feature vector into the LDM, a supervised feature selection step is incorporated to improve the classification performance by discarding the redundant feature and highlighting the important ones for gender classification. Rigorous experiments using the lab-made database are conducted to demonstrate the effectiveness and feasibility of the proposed model. What is more, additional experiment with subset of PolyU database illustrates its generalization ability and robustness. IEEE","coarse-to-fine; Databases; Face; Feature extraction; gender classification; Hand vein information; LDM; personal identification; Robustness; supervised feature selection; task-driven; Training; transfer learning; Veins","Data mining; Database systems; Deep learning; Deep neural networks; Feature extraction; Image recognition; Knowledge management; Palmprint recognition; Personnel training; Robustness (control systems); Semantics; Social sciences; Coarse to fine; Face; Gender classification; Hand vein; Personal identification; Task-driven; Transfer learning; Veins; Classification (of information)",2-s2.0-85032450388
"Adeniyi D.A., Wei Z., Yang Y.","Personalised news filtering and recommendation system using Chi-square statistics-based K-nearest neighbour (χ2SB-KNN) model",2017,"Enterprise Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987927752&doi=10.1080%2f17517575.2016.1229500&partnerID=40&md5=3323efb2908261c569ec49807ef26a2f","Recommendation problem has been extensively studied by researchers in the field of data mining, database and information retrieval. This study presents the design and realisation of an automated, personalised news recommendations system based on Chi-square statistics-based K-nearest neighbour (χ2SB-KNN) model. The proposed χ2SB-KNN model has the potential to overcome computational complexity and information overloading problems, reduces runtime and speeds up execution process through the use of critical value of χ2 distribution. The proposed recommendation engine can alleviate scalability challenges through combined online pattern discovery and pattern matching for real-time recommendations. This work also showcases the development of a novel method of feature selection referred to as Data Discretisation-Based feature selection method. This is used for selecting the best features for the proposed χ2SB-KNN algorithm at the preprocessing stage of the classification procedures. The implementation of the proposed χ2SB-KNN model is achieved through the use of a developed in-house Java program on an experimental website called OUC newsreaders’ website. Finally, we compared the performance of our system with two baseline methods which are traditional Euclidean distance K-nearest neighbour and Naive Bayesian techniques. The result shows a significant improvement of our method over the baseline methods studied. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","Chi square; data discretisation; in-house java program; online; real-time; recommendation","Computer software; Data mining; Java programming language; Nearest neighbor search; Pattern matching; Websites; Chi square; Data discretisation; Java program; online; Real time; recommendation; Recommender systems",2-s2.0-84987927752
"Tsai D., Yuste R., Shepard K.L.","Statistically Reconstructed Multiplexing for Very Dense, High-Channel-Count Acquisition Systems",2017,"IEEE Transactions on Biomedical Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441493&doi=10.1109%2fTBCAS.2017.2750484&partnerID=40&md5=e92b1747175078a1ad7d09012c045448","Multiplexing is an important strategy in multichannel acquisition systems. The per-channel antialiasing filters needed in the traditional multiplexing architecture limit its scalability for applications requiring high channel density, high channel count, and low noise. A particularly challenging example is multielectrode arrays for recording from neural systems. We show that conventional approaches must tradeoff recording density and noise performance, at a scale far from the ideal goal of one-to-one mapping between neurons and sensors. We present a multiplexing architecture without per-channel antialiasing filters. The sparsely sampled data are recovered through a compressed sensing strategy, involving statistical reconstruction and removal of the undersampled thermal noise. In doing so, we replace large analog components with digital signal processing blocks, which are much more amenable to scaled CMOS implementation. The resulting statistically reconstructed multiplexing architecture recovers input signals at significantly improved signal-to-noise ratios when compared to conventional multiplexing with antialiasing filters at the same per-channel area. We implement the new architecture in a 65&#x00A0;536-channel neural recording system and show that it is able to recover signals with performance comparable to conventional high-performance, single-channel systems, despite a more than four-orders-of-magnitude increase in channel density. IEEE","Bandwidth; Capacitors; Computer architecture; Data mining; Electrodes; Electrophysiology; interpolation; multielectrode array; Multiplexing; multiplexing; Neurons; sampling","Anti-aliasing; Digital signal processing; Electrophysiology; Interpolation; Multiplexing; Neural networks; Sampling; Signal processing; Signal reconstruction; Signal to noise ratio; Antialiasing filters; Conventional approach; High channel counts; Multi-channel acquisition; Multielectrode arrays; Neural recording systems; Single channel system; Statistical reconstruction; Biomedical signal processing",2-s2.0-85032441493
"Gao L., Qi L., Chen E., Guan L.","Discriminative Multiple Canonical Correlation Analysis for Information Fusion",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032436786&doi=10.1109%2fTIP.2017.2765820&partnerID=40&md5=e7bf33ce22c6148159e0143ff0643410","In this paper, we propose the Discriminative Multiple Canonical Correlation Analysis (DMCCA) for multimodal information analysis and fusion. DMCCA is capable of extracting more discriminative characteristics from multimodal information representations. Specifically, it finds the projected directions which simultaneously maximize the within-class correlation and minimize the between-class correlation, leading to better utilization of the multimodal information. In the process, we analytically demonstrate that the optimally projected dimension by DMCCA can be quite accurately predicted, leading to both superior performance and substantial reduction in computational cost. We further verify that Canonical Correlation Analysis (CCA), Multiple Canonical Correlation Analysis (MCCA) and Discriminative Canonical Correlation Analysis (DCCA) are special cases of DMCCA, thus establishing a unified framework for Canonical Correlation Analysis. We implement a prototype of DMCCA to demonstrate its performance in handwritten digit recognition and human emotion recognition. Extensive experiments show that DMCCA outperforms the traditional methods of serial fusion, CCA, MCCA and DCCA. IEEE","Correlation; Data mining; discriminative multiple canonical correlation analysis (DMCCA); Emotion recognition; Face; Feature extraction; Handwriting recognition; handwritten digit recognition; human emotion recognition; information fusion; multi-feature analysis; Multimedia communication; multimodal analysis","Character recognition; Correlation methods; Data mining; Feature extraction; Information fusion; Multimedia systems; Speech recognition; Canonical correlation analysis; Emotion recognition; Face; Handwriting recognition; Handwritten digit recognition; Human emotion recognition; Multi features; Multi-media communications; Multimodal analysis; Modal analysis",2-s2.0-85032436786
"Li S., Xiong Z., Hu J.","Inferring phase diagrams from X-ray data with background signals using graph segmentation",2017,"Materials Science and Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031815322&doi=10.1080%2f02670836.2017.1389116&partnerID=40&md5=38abe81537541a8bd2a3dd51a7f857ff","Automated composition-structure-processing phase diagram creation is critical for high-throughput experimental material studies. In particular, diffractogram datasets with large background signals are especially difficult to identify the phase regions. In this work, we proposed a novel graph segmentation algorithm from computer vision to solve the phase diagram prediction problem from X-ray diffraction data with large background signals. We introduced a novel background subtraction algorithm with graph-based clustering/segmentation to build the BGPhase algorithm. Experiments on three datasets with the Al–Cu–Mo material family showed that our phase attribution algorithm can achieve high prediction accuracy ranging from 88.6 to 94.8% or with MCC scores ranging from 0.715 to 0.890. The algorithm can be accessed online at http://mleg.cse.sc.edu/bgphase. © 2017 Institute of Materials, Minerals and Mining.","material characterisation; phase attribution; phase mapping; Phase transform; X-ray diffraction (XRD)","Characterization; Graphic methods; Image segmentation; Phase diagrams; X ray diffraction; Background subtraction algorithms; Experimental materials; Graph-based clustering; Material characterisation; phase attribution; Phase diagram prediction; Phase mappings; Phase transform; Clustering algorithms",2-s2.0-85031815322
"Hore S., Das S.K., Banerjee S., Mukherjee S.","An adaptive neuro-fuzzy inference system-based modelling to predict mechanical properties of hot-rolled TRIP steel",2017,"Ironmaking and Steelmaking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988433886&doi=10.1080%2f03019233.2016.1227025&partnerID=40&md5=49c221f4689fe328e8ac99b16fc9dc1e","A model based on adaptive neural network formalism coupled with fuzzy inference system has been developed to predict mechanical properties of hot-rolled TRIP steel. The developed model incorporates a wide range of data containing chemical compositions, thermo-mechanical processing parameters and mechanical properties of hot-rolled TRIP steel. A compact set of process variables has been selected as the model inputs for predicting tensile strength, yield strength, elongation and retained austenite under a given operating condition. The model predictions show that carbon, silicon and manganese content have a significant effect on the retained austenite which increases with the increased amount of these elements. The microalloying elements such as niobium and molybdenum have a little effect on the volume fraction of retained austenite. The present model provides a predictive platform for possible application of these artificial intelligence-based tools for automation, real-time process control and operator guidance in plant operation. © 2016 Institute of Materials, Minerals and Mining.","Coiling temperature; Mechanical properties; Neuro-fuzzy model; Retained austenite; Thermo-mechanical processing; TRIP steel","Artificial intelligence; Austenite; Carbon; Forecasting; Fuzzy neural networks; Fuzzy systems; Mechanical properties; Plasticity; Steel; Tensile strength; Coiling temperature; Neuro-Fuzzy model; Retained austenite; Thermo-mechanical processing; TRIP-steel; Fuzzy inference",2-s2.0-84988433886
"Jia C., Zhang F., Zhu Y., Qi X., Wang Y.","Public data mining plus domestic experimental study defined involvement of the old-yet-uncharacterized gene matrix-remodeling associated 7 (MXRA7) in physiopathology of the eye",2017,"Gene",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030615077&doi=10.1016%2fj.gene.2017.08.018&partnerID=40&md5=3dd7faeedfced369c7386515914a4a31","Matrix-remodeling associated 7 (MXRA7) gene was first reported in 2002 and named so for its co-expression with several genes known to relate with matrix-remodeling. However, not any studies had been intentionally performed to characterize this gene. We started defining the functions of MXRA7 by integrating bioinformatics analysis and experimental study. Data mining of MXRA7 expression in BioGPS, Gene Expression Omnibus and EurExpress platforms highlighted high level expression of Mxra7 in murine ocular tissues. Real-time PCR was employed to measure Mxra7 mRNA in tissues of adult C57BL/6 mice and demonstrated that Mxra7 was preferentially expressed at higher level in retina, corneas and lens than in other tissues. Then the inflammatory corneal neovascularization (CorNV) model and fungal corneal infections were induced in Balb/c mice, and mRNA levels of Mxra7 as well as several matrix-remodeling related genes (Mmp3, Mmp13, Ecm1, Timp1) were monitored with RT-PCR. The results demonstrated a time-dependent Mxra7 under-expression pattern (U-shape curve along timeline), while all other matrix-remodeling related genes manifested an opposite changes pattern (dome-shape curve). When limited data from BioGPS concerning human MXRA7 gene expression in human tissues were looked at, it was found that ocular tissue was also the one expressing highest level of MXRA7. To conclude, integrative assay of MXRA7 gene expression in public databank as well as domestic animal models revealed a selective high expression MXRA7 in murine and human ocular tissues, and its change patterns in two corneal disease models implied that MXRA7 might play a role in pathological processes or diseases involving injury, neovascularization and would healing. © 2017 Elsevier B.V.","Eye; Infection; MXRA7; Neovascularization; Wound healing","collagenase 3; Ecm1 protein, mouse; messenger RNA; Mmp13 protein, mouse; Mmp3 protein, mouse; Mxra7 protein, mouse; protein; scleroprotein; stromelysin; Timp1 protein, mouse; tissue inhibitor of metalloproteinase 1; animal; Bagg albino mouse; C57BL mouse; cornea; cornea disease; eye infection; genetics; human; meta analysis; metabolism; mouse; neovascularization (pathology); vascularization; Animals; Cornea; Corneal Diseases; Extracellular Matrix Proteins; Eye Infections; Humans; Matrix Metalloproteinase 13; Matrix Metalloproteinase 3; Mice; Mice, Inbred BALB C; Mice, Inbred C57BL; Neovascularization, Pathologic; Proteins; RNA, Messenger; Tissue Inhibitor of Metalloproteinase-1",2-s2.0-85030615077
"Zhukovskiy Y., Koteleva N.","Method of Data storing, collection and aggregation for definition of life-cycle resources of electromechanical equipment",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032434390&doi=10.1088%2f1755-1315%2f87%2f3%2f032057&partnerID=40&md5=17e5f1685964567b2163d0dd2fe73277","Analysis of technical and technological conditions for the emergence of emergency situations during the operation of electromechanical equipment of enterprises of the mineral and raw materials complex shows that when developing the basis for ensuring safe operation, it is necessary to take into account not only the technical condition, but also the non-stationary operation of the operating conditions of equipment, and the nonstationarity of operational operating parameters of technological processes. Violations of the operation of individual parts of the machine, not detected in time, can lead to severe accidents at work, as well as to unplanned downtime and loss of profits. That is why, the issues of obtaining and processing Big data obtained during the life cycle of electromechanical equipment, for assessing the current state of the electromechanical equipment used, timely diagnostics of emergency and pre-emergency modes of its operation, estimating the residual resource, as well as prediction the technical state on the basis of machine learning are very important. This article is dedicated to developing the special method of data storing, collection and aggregation for definition of life-cycle resources of electromechanical equipment. This method can be used in working with big data and can allow extracting the knowledge from different data types: the plants' historical data and the factory historical data. The data of the plants contains the information about electromechanical equipment operation and the data of the factory contains the information about a production of electromechanical equipment. © Published under licence by IOP Publishing Ltd.",,"Data acquisition; Data handling; Data mining; Electric power systems; Equipment; Learning systems; Life cycle; Mining machinery; Electromechanical equipments; Emergency situation; Mineral and raw materials complexes; Operating condition; Operating parameters; Technical conditions; Technological conditions; Technological process; Big data",2-s2.0-85032434390
"Feng G.C.","The dynamics of the Chinese film industry: factors affecting Chinese audiences’ intentions to see movies",2017,"Asia Pacific Business Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018515783&doi=10.1080%2f13602381.2017.1294353&partnerID=40&md5=113a0af6f9b3d0e529ade3caaa4220ad","This study attempts to understand the dynamics of the rapidly growing Chinese film industry by relying on a revised reason action model that uses a data-mining approach with aggregated data to examine the determinants of people’s intentions to see movies. The results show that attitude towards seeing movies indicated by online film ratings, collective norms represented by box-office performance and Academy Award win(s) significantly predict intentions to see movies. In addition, a movie’s year of release, star power, country of origin, adaptation from a novel and status as a sequel were significant predictors of aggregated intentions. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Aggregated data; China; data-mining; film industry; movies; ratings","data mining; film industry; Internet; China",2-s2.0-85018515783
"Ermakov A.V., Bessmertnyy A.M.","Special test results evaluation features as development of ""innovations management"" program - NEFU testing area as case-study",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032449887&doi=10.1088%2f1755-1315%2f87%2f8%2f082013&partnerID=40&md5=7be2f02c6e956bcf60c964d87ae9bffa","This article gives an overview on the problems of precision in the results evaluation of the tests carried out in the Northern testing areas. One of the significant features of the facilities under study is that they are limited in quantity. In cases when a facility is taken down, that quantity is normally equal to one. The complexity of modern technological equipment and other circumstances require researchers to take into account and evaluate the potential risks. In order to make the sought-for estimations more precise, ways of improving the test result evaluations algorithms are suggested. In particular, one of the productive methods is the Data Mining technology, which presupposes implementing an intellectual analysis of the data with the aim of extracting useful information from the available database which was attained during the tests and other types of activities. Applying the Data Mining technology is becoming more productive when the scenario analysis is carried out, i.e., the analysis of possible alternative solutions. Another perspective trend is an implementation of an interdisciplinary approach. As a result, researchers are able to carry out a complex evaluation of the test results, which will noticeably increase the value of the given results. © Published under licence by IOP Publishing Ltd.",,"Mining machinery; Software testing; Testing; Alternative solutions; Complex evaluations; Data mining technology; Potential risks; Result evaluation; Scenario analysis; Technological equipments; Data mining",2-s2.0-85032449887
"Kostarev S.N., Sereda T.G.","Development of software and hardware models of monitoring, control, and data transfer to improve safety of downhole motor during drilling",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441887&doi=10.1088%2f1755-1315%2f87%2f3%2f032016&partnerID=40&md5=4f0f538d088efd6f2d8acf1c48a1e382","The article is concerned with the problem of transmitting data from telemetric devices in order to provide automated systems for the electric drive control of oil-extracting equipment. The paper given discusses the possibility to use a logging cable as means of signal transfer. Simulation models of signaling and relay-contact circuits for monitoring critical drive parameters are under discussion. The authors suggest applying the operator ⊕ (excluding OR) to increase anti-jamming effects and to get a more reliable noise filter. © Published under licence by IOP Publishing Ltd.",,"Automation; Data transfer; Digital storage; Electric control equipment; Electric drives; Electric power systems; Filtration; Mining machinery; Automated systems; Drive parameters; Logging cables; Oil extracting; Relay contacts; Signal transfer; Software and hardwares; Transmitting data; Electric machine control",2-s2.0-85032441887
"Nikolaev A.V., Alymenko N.I., Kamenskikh A.A., Alymenko D.N., Nikolaev V.A., Petrov A.I.","Factors defining value and direction of thermal pressure between the mine shafts and impact of the general mine natural draught on ventilation process of underground mining companies",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032452106&doi=10.1088%2f1755-1315%2f87%2f5%2f052020&partnerID=40&md5=cb70e8ef6f971b4e6efa80d4eaa0fcbd","The article specifies measuring data of air parameters and its volume flow in the shafts and on the surface, collected in BKPRU-2 (Berezniki potash plant and mine 2) (Uralkali PJSC) in normal operation mode, after shutdown of the main mine fan (GVU) and within several hours. As a result of the test it has been established that thermal pressure between the mine shafts is active continuously regardless of the GVU operation mode or other draught sources. Also it has been discovered that depth of the mine shafts has no impact on thermal pressure value. By the same difference of shaft elevation marks and parameters of outer air between the shafts, by their different depth, thermal pressure of the same value will be active. Value of the general mine natural draught defined as an algebraic sum of thermal pressure values between the shafts depends only on the difference of temperature and pressure of outer air and air in the shaft bottoms on condition of shutdown of the air handling system (unit-heaters, air conditioning systems). © Published under licence by IOP Publishing Ltd.",,"Air conditioning; Mineral resources; Mining machinery; Plant shutdowns; Air handling systems; Main mine fans; Measuring data; Normal operations; Operation mode; Temperature and pressures; Thermal pressure; Underground mining; Mine shafts",2-s2.0-85032452106
"Ahmed F., Erman J., Ge Z., Liu A.X., Wang J., Yan H.","Detecting and Localizing End-to-End Performance Degradation for Cellular Data Services Based on TCP Loss Ratio and Round Trip Time",2017,"IEEE/ACM Transactions on Networking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032704543&doi=10.1109%2fTNET.2017.2761758&partnerID=40&md5=fd559fe0852e685474345cfa9ca1565a","Providing high end-to-end (E2E) performance experienced by users is critical for cellular service providers to best serve their customers. This paper focuses on the detection and localization of E2E performance degradation (such as slow webpage page loading and unsmooth video playing) at cellular service providers. Detecting and localizing E2E performance degradation is crucial for cellular service providers, content providers, device manufactures, and application developers to jointly troubleshoot root causes. To the best of our knowledge, the detection and localization of E2E performance degradation at cellular service providers has not been previously studied. In this paper, we propose a holistic approach to detecting and localizing E2E performance degradation at cellular service providers across the four dimensions of user locations, content providers, device types, and application types. Our approach consists of three steps: modeling, detection, and localization. First, we use training data to build models that can capture the normal performance of every E2E instance, which means the flows corresponding to a specific location, content provider, device type, and application type. Second, we use our models to detect performance degradation for each E2E instance on an hourly basis. Third, after each E2E instance has been labeled as non-degrading or degrading, we use association rule mining techniques to localize the source of performance degradation. Our system detected performance degradation instances over a period of one week. In $80&#x0025;$ of the detected degraded instances, content providers, device types, and application types were the only factors of performance degradation. IEEE","cellular data networks; Cellular networks; Data models; Degradation; Electronic mail; measurement and modeling.; Mobile handsets; Performance evaluation; Quality-of-experience; Servers","Content based retrieval; Data structures; Degradation; Electronic mail; Mobile telecommunication systems; Servers; Transmission control protocol; Cellular data networks; Cellular network; Mobile handsets; Performance evaluation; Quality of experience (QoE); Quality of service",2-s2.0-85032704543
"Liu B., Chen J., Guo M., Wang X.","Protein remote homology detection and fold recognition based on Sequence-Order Frequency Matrix",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032671561&doi=10.1109%2fTCBB.2017.2765331&partnerID=40&md5=e6d5ca902acc34b9ec6c30f87d2cfc64","Protein remote homology detection and fold recognition are two critical tasks for the studies of protein structures and functions. Currently, the profile-based methods achieve the state-of-the-art performance in these fields. However, the widely used sequence profiles, like Position-Specific Frequency Matrix (PSFM) and Position-Specific Scoring Matrix (PSSM), ignore the sequence-order effects along protein sequence. In this study, we have proposed a novel profile, called Sequence-Order Frequency Matrix (SOFM), to extract the sequence-order information of neighboring residues from Multiple Sequence Alignment (MSA). Combined with two profile feature extraction approaches: Top-n-grams and Smith-Waterman algorithm, the SOFMs are applied to protein remote homology detection and fold recognition, and two predictors called SOFM-Top and SOFM-SW are proposed. Experimental results show that SOFM contains more information content than other profiles, and these two predictors outperform other state-of-the-art methods. It is anticipated that SOFM will become a very useful profile in the studies of protein structures and functions. IEEE","Amino acids; Benchmark testing; Data mining; Feature extraction; Hidden Markov models; Kernel; protein fold recognition; protein remote homology detection; Proteins; Sequence-Order Frequency Matrix; Smith-Waterman Local Alignment algorithm; Top-n-gram","Amino acids; Conformal mapping; Data mining; Extraction; Feature extraction; Hidden Markov models; Markov processes; Benchmark testing; Frequency matrix; Kernel; Local alignment; N-grams; Protein fold recognition; Remote homology detections; Proteins",2-s2.0-85032671561
"Golokhvast K.S., Manakov Yu.A., Bykov A.A., Chayka V.V., Nikiforov P.A., Rogulin R.S., Romanova T.Yu., Karabtsov A.A., Semenikhin V.A.","Some Characteristics of Dust Particles in Atmosphere of Kemerovo City According to Pollution Data of Snow Cover",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032444163&doi=10.1088%2f1755-1315%2f87%2f4%2f042005&partnerID=40&md5=5ab76212bd798832c543a5c6e9860593","The given paper presents the study results of solid particles contained in snow samples, taken on 10 sites in Kemerovo city in spring 2013. The sites were chosen in such a way as to prevent particles flow into the snow cover in other ways, except with atmospheric precipitation. Kuzbass Botanical Garden was chosen as the check point. In 7 out of 10 sampling sites on the territory of Kemerovo city the presence of particles that are particularly dangerous for human health was found. In one of the areas the particles of 200-400 nm size and with a specific surface area of 14,813.34 cm2/cm3 were detected in ecologically significant quantity (8%). © Published under licence by IOP Publishing Ltd.",,"Atmospheric movements; Mining machinery; Atmospheric precipitation; Botanical gardens; Check points; Dust particle; Human health; Sampling site; Snow samples; Solid particles; Snow",2-s2.0-85032444163
"Podgornyj Y.I., Martynova T.G., Skeeba V.Y., Kosilov A.S., Chernysheva A.A., Skeeba P.Y.","Experimental determination of useful resistance value during pasta dough kneading",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032452826&doi=10.1088%2f1755-1315%2f87%2f8%2f082039&partnerID=40&md5=1fa41145074323828fda3d6b6b9dc783","There is a large quantity of materials produced in the form of dry powder or low humidity granulated masses in the modern market, and there is a need to develop new manufacturing machinery and to renew the existing facilities involved in the production of various loose mixtures. One of the machinery upgrading tasks is enhancing its performance. In view of the fact that experimental research is not feasible in full-scale samples, an experimental installation was to be constructed. The article contains its kinematic scheme and the 3D model. The angle of the kneading blade location, the volume of the loose mixture, rotating frequency and the number of the work member double passes were chosen as variables to carry out the experiment. The technique of the experiment, which includes two stages for the rotary and reciprocating movement of the work member, was proposed. The results of the experimental data processing yield the correlations between the load characteristics of the mixer work member and the angle of the blade, the volume of the mixture and the work member rotating frequency, allowing for the recalculation of loads for this type machines. © Published under licence by IOP Publishing Ltd.",,"Data handling; Mining machinery; Experimental data processing; Experimental determination; Experimental installations; Experimental research; Load characteristics; Low humidity; Resistance values; Rotating frequencies; Mixtures",2-s2.0-85032452826
"Tsyganova M.S., Ivashko A.G., Polyshuk I.N., Nabatov R.I., Tsyganova A.I.","Simulation of Decomposition Kinetics of Supercooled Austenite in Powder Steel",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032469198&doi=10.1088%2f1755-1315%2f87%2f9%2f092029&partnerID=40&md5=24679449b25d862b7596b2f496459463","To approve heat treatment of steel modes, quantitative data on austenite decomposition are required. Gaining these data experimentally appears to be extremely complicated. In present work, few approaches to simulate the phase transformation process are proposed considering structure characteristics of powder steels. Results of comparative analysis of these approaches are also given. Predicting the transformation kinetics by simulation is verified for PK40N2M (0.38% C, 2.10% Ni, 0.40% Mo) steel with 3% porosity and PK80 (0.80% C) steel with different porosity using published experimental data. © Published under licence by IOP Publishing Ltd.",,"Austenite; Mining machinery; Molybdenum compounds; Porosity; Austenite decomposition; Comparative analysis; Decomposition kinetics; Phase transformation process; Quantitative data; Structure characteristic; Supercooled austenite; Transformation kinetics; Metadata",2-s2.0-85032469198
"Petrov A.A., Shurov N.I.","Hybrid system of power factor correction",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032452035&doi=10.1088%2f1755-1315%2f87%2f3%2f032031&partnerID=40&md5=0f049676a788ba66faabebd4f05b0564","The paper reviews research data of hybrid of reactive power compensation for Metro electric power substations. Comparative compensation analysis has been made and the control system for the suggested device has been developed; besides, mathematic simulation has been carried out, as a result of which the power factor of substation has been increased from 0.5 to 0.97-0.99. © Published under licence by IOP Publishing Ltd.",,"Electric power factor correction; Electric power systems; Hybrid systems; Mining machinery; Reactive power; Electric power; Mathematic simulations; Power factor corrections; Power factors; Reactive power compensation; Research data; Electric substations",2-s2.0-85032452035
"Oborin P.V., Popov I.P.","Reasons of formation of hard-to-recover reserves of Samotlorskoye field",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032503531&doi=10.1088%2f1755-1315%2f87%2f7%2f072002&partnerID=40&md5=797d8a2bb593ee53115be14229e96e34","Based on the analysis of geological and field data, the dynamics of indicators of development and waterflooding efficiency, a permeability-porosity and hydrodynamic model of the deposit was established, the reasons of formation of hard-to-recover reserves were revealed, and measures on the increase of oil recovery and the reduction of the volume of unproductive expenditures were substantiated. © Published under licence by IOP Publishing Ltd.",,"Minerals; Mining machinery; Oil well flooding; Petrography; Proven reserves; Well flooding; Field data; Hydrodynamic model; Oil recoveries; Oil field development",2-s2.0-85032503531
"Pospehov G.B., Pankratova K.V., Straupnik I.A., Ustiugov D.L.","Problems of land reclamation during liquidation of coalmining enterprises",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032435841&doi=10.1088%2f1755-1315%2f87%2f4%2f042015&partnerID=40&md5=2af50cc4c872563e5dbc0556c3e18e7d","The paper presents data on the influence of coal-mining industry elimination on the deformation of land surface which can cause accidents and destructions of buildings and constructions located nearby the closed pits or mines. The analysis is carried out and the major factors which influence change of the intense deformed condition of the massif of rocks were revealed. The example of the monitoring system which will provide researchers with information for preparation of the project of a pit or a mine closing is presented, and it also will allow one to predict behavior of the massif in the future. © Published under licence by IOP Publishing Ltd.",,"Coal industry; Mining machinery; Coal mining industry; Coal-mining; Land surface; Major factors; Monitoring system; Land reclamation",2-s2.0-85032435841
"Shaptsev V.A.","Indicator of reliability of power grids and networks for environmental monitoring",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032435329&doi=10.1088%2f1755-1315%2f87%2f3%2f032038&partnerID=40&md5=9b6963997fd76b17c35a16037e026dad","The energy supply of the mining enterprises includes power networks in particular. Environmental monitoring relies on the data network between the observers and the facilitators. Weather and conditions of their work change over time randomly. Temperature, humidity, wind strength and other stochastic processes are interconnecting in different segments of the power grid. The article presents analytical expressions for the probability of failure of the power grid as a whole or its particular segment. These expressions can contain one or more parameters of the operating conditions, simulated by Monte Carlo. In some cases, one can get the ultimate mathematical formula for calculation on the computer. In conclusion, the expression, including the probability characteristic function of one random parameter, for example, wind, temperature or humidity, is given. The parameters of this characteristic function can be given by retrospective or special observations (measurements). © Published under licence by IOP Publishing Ltd.",,"Electric power systems; Environmental engineering; Mining machinery; Random processes; Stochastic systems; Analytical expressions; Characteristic functions; Environmental Monitoring; Mathematical formulas; Mining enterprise; Operating condition; Probability of failure; Random parameters; Electric power transmission networks",2-s2.0-85032435329
"Chernova E.V., Chernov D.V.","Current status and application of fine screening technology in China",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032451386&doi=10.1088%2f1755-1315%2f87%2f2%2f022005&partnerID=40&md5=64d8f42d67892afec5bd846a63132e4d","The paper presents data on the design and technical parameters of high frequency vibrating screens, which are produced by Chinese manufacturer - company Landsky Tech Ltd. The technology of high frequency vibration is widely used at mining and metallurgical industries to separate fine and ultra-fine particles from the flow of dry material or pulp. The paper contains different types of screening systems, description, advantages and disadvantages of equipment and test results from mineral processing plants. © Published under licence by IOP Publishing Ltd.",,"Earthmoving machinery; Metallurgy; Mining machinery; Chinese manufacturers; High frequency HF; High frequency vibration; Metallurgical industry; Mineral processing plants; Screening system; Screening technology; Ultrafine particle; Vibrating screens",2-s2.0-85032451386
"Nazarova M.N., Akhmetov R.R., Krainov S.A.","Temperature factors effect on occurrence of stress corrosion cracking of main gas pipeline",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032470448&doi=10.1088%2f1755-1315%2f87%2f6%2f062011&partnerID=40&md5=9038002e54966d01bb37c0f61cb536ce","The purpose of the article is to analyze and compare the data in order to contribute to the formation of an objective opinion on the issue of the growth of stress corrosion defects of the main gas pipeline. According to available data, a histogram of the dependence of defects due to stress corrosion on the distance from the compressor station was constructed, and graphs of the dependence of the accident density due to stress corrosion in the winter and summer were also plotted. Data on activation energy were collected and analyzed in which occurrence of stress corrosion is most likely constructed, a plot of activation energy versus temperature is plotted, and the process of occurrence of stress corrosion by the example of two different grades of steels under the action of different temperatures was analyzed. © Published under licence by IOP Publishing Ltd.",,"Activation energy; Chemical activation; Corrosion; Defects; Gas pipelines; Mineral resources; Mining machinery; Pipelines; Residual stresses; Compressor stations; Most likely; Stress corrosion; Temperature factor; Stress corrosion cracking",2-s2.0-85032470448
"Zilenina V.G., Ulanova O.V., Dornack C.","Study of problem of waste chemical current sources in Russia and in European countries",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032437783&doi=10.1088%2f1755-1315%2f87%2f4%2f042029&partnerID=40&md5=ea57cf647d19345aa6c9d9b08e52334f","This article gives a comparative analysis of handling waste chemical current sources in Russia and in the European countries, presents the effective international documents (Directives, acts) and national legislative acts (state standards, building codes, governmental decrees, etc.), demonstrates the mechanisms for disposal and recycling of waste in the European Union countries. Along with the data of the research works, conducted in other countries during many years B it presents the experimental data on leaching out heavy metals from chemical current sources by municipal solid waste landfill filtrate, depending on the morphological composition of domestic waste in the city of Irkutsk. An important point described in the article, is assessment and prediction of negative impact produced on the environment. © Published under licence by IOP Publishing Ltd.",,"Building codes; Chemical analysis; Filtration; Heavy metals; Land fill; Laws and legislation; Mining machinery; Waste disposal; Chemical current sources; Comparative analysis; European Countries; European Union countries; Legislative acts; Municipal solid waste landfills; Recycling of wastes; State standards; Municipal solid waste",2-s2.0-85032437783
"Maharatkin P.N., Yablokov I.N., Serzhan S.L.","Increasing percentage of uptime of pipeline transport system at production association LLC KINEF",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032434551&doi=10.1088%2f1755-1315%2f87%2f2%2f022013&partnerID=40&md5=6e2b4a4f70aef88eb424b043163d3a2a","The system of the preventive maintenance (PM), accepted at production association LLC KINEF, taking into account safe operation and health assessment, is analyzed. Statistical data of results of diagnostics are processed; options of the increase of the integrated reliability indicator of the system of pipeline transport are offered. The trend lines of a condition change of the pipeline in time and the approximate curves of diagnostic data of nondestructive inspection technique with creation of the long-term forecast of its technical condition was constructed. The option of correction of the accepted PM system with the analysis of quantitative change of an indicator of reliability was developed, by which application of the percentage of uptime will increase from 0.909 to 0.957. © Published under licence by IOP Publishing Ltd.",,"Earthmoving machinery; Mining machinery; Nondestructive examination; Pipelines; Reliability analysis; Health assessments; Integrated reliabilities; Long-term forecast; Non destructive inspection; Pipeline transport; Preventive maintenance (pm); Quantitative changes; Technical conditions; Preventive maintenance",2-s2.0-85032434551
"Zvonarev I.E., Ivanov S.L.","Evaluation of losses in transmission of machinery for development of mineral deposits in conditions of variable load",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032445032&doi=10.1088%2f1755-1315%2f87%2f2%2f022024&partnerID=40&md5=72b57648d3d5ceceeb4772480e33afea","The influence of individual elements of machines transmissions on the operation of the whole system is shown. The approach of determining the resource of operation of systems elements based on the energy theory is presented. The formulas for determining the total energy resource of the reducer are given. The influence of individual elements of the system on each other is indicated. The principle of researching the system by the method of equivalent circuits is substantiated. The weakest places of transmission (gears, bearing supports and shafts) are determined. A mathematical model of a mechanical transmission was developed. To test the adequacy of the mathematical model, the stand for obtaining experimental data was designed. The description of the stand and the principle of its operation are given. Experimental data are presented. A comparative analysis of modeling and experimental data is carried out and the adequacy of the developed mathematical model is proved. The principle of determining the resource of the system as a whole for the element with the minimal resource of work is suggested. © Published under licence by IOP Publishing Ltd.",,"Earthmoving machinery; Energy resources; Mineral resources; Mining machinery; Adequacy of the mathematical models; Bearing support; Comparative analysis; Mechanical transmission; Variable loads; Equivalent circuits",2-s2.0-85032445032
"Kuzyakov O.N., Glukhikh I.N., Sidorova A.E., Andreeva M.A.","Case-based reasoning approach for monitoring multi-phase liquid in pipeline",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032437491&doi=10.1088%2f1755-1315%2f87%2f5%2f052015&partnerID=40&md5=b281e373073805a91a33d9039aed284a","A method based on collecting, integrating and subsequence analyzing data on multi-phase liquid flow condition in pipeline is suggested in the paper. The principles of constructing multi-component liquid control system with the use of ultrasonic signal as a sensing one and initial execution algorithm of ultrasonic emitters-receivers with subsequent change of roles are presented as well. CBR method used while identifying multi-phase condition is an element of processing part of the system - intellectual decision-support system. © Published under licence by IOP Publishing Ltd.",,"Artificial intelligence; Decision support systems; Liquids; Mineral resources; Mining machinery; Pipelines; Signal receivers; Case-based reasoning approaches; Liquid control; Multicomponents; Phase conditions; Phase liquids; Ultrasonic emitters; Ultrasonic signals; Case based reasoning",2-s2.0-85032437491
"Saidov M.A., Perekrestov A.P.","Influence of nanodispersed modifications of magnetite powders on spray nozzle efficiency of diesel engine injector",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032456079&doi=10.1088%2f1755-1315%2f87%2f8%2f082042&partnerID=40&md5=e6bf1027f4c9b9c3d4b3724b6bb309d8","The paper presents data on the impact of new environmental requirements relating to the quality of diesel fuel on the anti-wear properties of fuel. Anti-wear additive is proposed as a material for increasing the tribotechnical characteristics of diesel fuel. This additive consists of diesel fuel with micelles contained in it, formed on the basis of molecules of solid plasticity lubrication of iron oxide (Fe3O4) - magnetite, and with surrounding molecules of oleic acid (C18H34O2). The additive has low shear resistance and increased lubricity of diesel fuel when this additive is introduced into it. © Published under licence by IOP Publishing Ltd.",,"Diesel engines; Diesel fuels; Fuels; Iron compounds; Iron oxides; Magnetite; Mining machinery; Molecules; Spray nozzles; Tribology; Wear resistance; Antiwear additive; Antiwear property; Environmental requirement; Magnetite powder; Nanodispersed; Nozzle efficiency; Shear resistances; Tribotechnical characteristics; Fuel additives",2-s2.0-85032456079
"Pashnin S.V.","Analysis of oxidation of self-baking electrodes (Soederberg electrodes) by means of three-dimensional model",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032461020&doi=10.1088%2f1755-1315%2f87%2f9%2f092020&partnerID=40&md5=809af5b1db0d935d49a0322fd619e3c8","The paper presents the methodology and results of the development of the temperature dependence of the oxidation speed of the self-baking electrode (Soederberg Electrodes) in the ore-thermal furnaces. For the study of oxidation, the working ends of the self-baking electrodes, which were taken out from the ore-thermal furnaces after their scabbings, were used. The temperature of the electrode surface by its height was calculated with the help of the mathematical model of heat work of self-baking electrode. The comparison of electrode surface temperatures with the speed of oxidation of the electrode allowed one to obtain the temperature dependency of the oxidation of the lateral electrode surface. Comparison of the experimental data, obtained in the laboratory by various authors, showed their qualitative coincidence with results of calculations of the oxidation rate presented in this article. With the help of the mathematical model of temperatures fields of electrode, the calculations of the sizes of the cracks, appearing after burnout ribs, were performed. Calculations showed that the sizes of the cracks after the ribs burnout, calculated by means of the obtained temperature dependence, coincide with the experimental data with sufficient accuracy. © Published under licence by IOP Publishing Ltd.",,"Cracks; Mining machinery; Oxidation; Temperature distribution; Electrode surfaces; Lateral electrodes; Oxidation rates; Temperature dependence; Temperature dependencies; Three-dimensional model; Electrodes",2-s2.0-85032461020
"Taskin A.V., Ivannikov I.S., Danilov O.S.","Concentrating precious metals from ash and slag waste of Far Eastern energy enterprises",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032448821&doi=10.1088%2f1755-1315%2f87%2f4%2f042024&partnerID=40&md5=a437d0abdb4e020b96242ccbfa812ece","This work represents the data on the content of gold, noble metals and rare earth metals in ash and slag waste taken from the ash disposal areas of the Far Eastern energy enterprises. Ash and slag objects with increased concentration of Au and Ag were found. The chemical content of samples from researched ash and slag disposal areas is represented in this work. A scheme of dividing ash and slag waste into different mineral fractions is offered in this work. The mechanisms of collecting and distribution of Au, Pt and Ag concentrations in ash and slag wastes were determined during the research. Also, the possibility of gold concentration in heavy non-magnetic fraction is shown. © Published under licence by IOP Publishing Ltd.",,"Gold; Mining machinery; Precious metals; Silver; Waste disposal; Ash disposal; Chemical content; Disposal areas; Gold concentration; Nonmagnetics; Rare earth metals; Slag wastes; Slags",2-s2.0-85032448821
"Vakh E.A., Vakh A.S., Petukhov V.I., Pavlova G.Ya., Tarasenko I.A., Zubtsova A.S.","Study on rare-earth elements distribution in surface waters of Primorsky region",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032511850&doi=10.1088%2f1755-1315%2f87%2f7%2f072005&partnerID=40&md5=879c3d111f22c2e39d37d6d68e6ec47f","Data obtained by the authors show that the background level of the rare-earth element (REE) content in the surface waters of southern Russian Far East is irregular and varies from 0.1 to 1.3 μg/l. The highest concentration of REE (0.48-1.3 μg/l) is characteristic for the rivers of Eastern Sikhote-Alin and Central Sikhote-Alin, catchment areas which are situated within the Sikhote-Alin volcanic belt. The lowest REE concentration was noted in the waters of Western Sikhote-Alin (0.09 μg/l). The profile of REE distribution in fresh waters of different areas of the region is uniform and is characterized with the deficit of Ce and enrichment of medium group REE. © Published under licence by IOP Publishing Ltd.",,"Catchments; Heterojunctions; Minerals; Mining machinery; Petrography; Rare earth elements; Rare earths; Surface waters; Background level; Catchment area; Fresh Water; Russian far east; Volcanic belt; Exploratory geochemistry",2-s2.0-85032511850
"Voytyuk I.N., Kopteva A.V.","Testing the system detection unit for measuring solid minerals bulk density",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032454808&doi=10.1088%2f1755-1315%2f87%2f3%2f032054&partnerID=40&md5=8198f0c9265fe37f3bfd73adde6e33f3","The paper provides a brief description of the system for measuring flux per volume of solid minerals via example of mineral coal. The paper discloses the operational principle of the detection unit. The paper provides full description of testing methodology, as well as practical implementation of the detection unit testing. This paper describes the removal of two data arrays via the channel of scattered anddirect radiation for the detection units of two generations. This paper describes Matlab software to determine the statistical characteristics of the studied objects. The mean value of pulses per cycles, and pulse counting inaccuracy relatively the mean value were determined for the calculation of the stability account of the detection units. © Published under licence by IOP Publishing Ltd.",,"Electric power systems; MATLAB; Mining machinery; Bulk density; Matlab- software; Operational principles; Pulse counting; Solid minerals; Statistical characteristics; Testing methodology; Unit testing; Minerals",2-s2.0-85032454808
"Malikov V.N., Dmitriev S.F., Ishkov A.V., Katasonov A.O., Sagalakov A.M.","Research of aluminium alloys with use of subminiature eddy current transducers",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032438310&doi=10.1088%2f1755-1315%2f87%2f8%2f082031&partnerID=40&md5=0dae0904d45bf1eb079402850ac30718","The measuring system based on subminiature eddy-current transducers has been developed to carry out local investigations of aluminum-magnesium alloy plates for flaws. The Delianna filter has been modified to allow the significant increase of signal-to-noise ratio. A scheme that uses a computer as a generator and receiver of signals from windings is proposed. It is capable of automatically changing the filtering cutoff frequency and operating frequency of the device. The transducer has been tested on a number of aluminum-magnesium alloy plates with flaws. The article presents data on the relationship of eddy-current transducer response to the presence of flaws in alloys as hidden holes at signal frequencies comprised between 300÷700 Hz on an exciting winding. © Published under licence by IOP Publishing Ltd.",,"Aluminum; Aluminum alloys; Cutoff frequency; Filtration; Light metals; Magnesium; Magnesium alloys; Magnesium printing plates; Mining machinery; Signal to noise ratio; Transducers; Winding; Aluminum - Magnesium alloys; Eddy current transducers; Measuring systems; Operating frequency; Signal frequencies; Eddy current testing",2-s2.0-85032438310
"Ismagilov F.R., Vavilov V.E., Bekuzin V.I., Ayguzina V.V.","Determination of Specific Losses in Stator Core of Electromechanical Energy Converter",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032455878&doi=10.1088%2f1755-1315%2f87%2f3%2f032011&partnerID=40&md5=a881cb002aea8529b726e4a0542a5027","The purpose of this paper is the experimental research of the magnetic losses in electrical steels, soft magnetic alloys and amorphous alloys in the wide range of the magnetization reversal frequency and magnetic induction. The comparison between different analytical calculation methods of the stator-core specific losses is presented. The article shows that the known methods with the sufficient accuracy allows the stator-core-specific-loss calculation at frequencies below 400 Hz. However, the discrepancy with the calculations of losses by the different methods at the higher frequencies (above 400 Hz) can reach 2 times. The experimental research at high frequencies showed the 2-3-time discrepancy between the calculated and experimental data. © Published under licence by IOP Publishing Ltd.",,"Alloy steel; Amorphous alloys; Electric power systems; Magnetic materials; Magnetism; Magnetization reversal; Mining machinery; Silicon steel; Analytical calculation; Electrical steels; Electromechanical energy; Experimental research; High frequency HF; Higher frequencies; Loss calculation; Soft magnetic alloys; Stators",2-s2.0-85032455878
"Timokhova O.M., Burmistrova O.N., Sirina E.A.","Optimization of cylinder liner plasma spraying mode",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032454425&doi=10.1088%2f1755-1315%2f87%2f8%2f082049&partnerID=40&md5=1a40e3e4d919c4a705b81247eb7d4870","At present one of the most promising methods to remanufacture worn-out machine parts is plasma spraying. The paper describes the selection of the optimum plasma spraying technique to coat the worn-out wall surface of the diesel engine cylinder liners. All the data have been MathCad processed and the regression models equivalent to the algoristic-type model of plasma spraying have been developed. The experiments have resulted in achieving the optimization parameters, the mean value of which is presented in the paper. The given plasma spraying mode allows one not only to remanufacture the worn-out wall surface of the diesel engine cylinder liners but also to obtain the best coating properties. © Published under licence by IOP Publishing Ltd.",,"Diesel engines; Engines; Mining machinery; Plasma jets; Plasma spraying; Regression analysis; Coating properties; Cylinder liners; MathCAD; Mean values; Optimization parameter; Regression model; Wall surfaces; Engine cylinders",2-s2.0-85032454425
"Almukhametova E.M., Gizetdinov I.A., Kilmamatova E.T., Akimov A.V., Kalinina S.V., Fatkullin I.F.","Use of precipitate formation technology to increase oil recovery under Tarasovskoye field conditions",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032455395&doi=10.1088%2f1755-1315%2f87%2f5%2f052001&partnerID=40&md5=9b5cdf917e8afa3e8c4cc8f08eb35294","The article presents data about using the technology of precipitate formation on the basis of sodium sulfate under conditions of the Tarasovskoye field, located in the Yamalo-Nenets Autonomous District. This technology consists in a sequential injection into the formation of sodium sulfate and calcium chloride, which leads to the formation of a precipitate of calcium sulfate, which eventually blocks flushed zones and water-saturated zones, thereby enabling intensification of oil-saturated areas development. Injection of precipitation systems was carried out in injection well N0 775, the focus of which includes 6 producing wells. The daily production rate of the test production wells after the event has increased more than 2 times. In addition, the authors noted the positive results of changes in formation characteristics: a decrease in permeability, pressure conductivity factor and hydraulic conductivity in a water-saturated zone. © Published under licence by IOP Publishing Ltd.",,"Calcium; Calcium chloride; Chlorine compounds; Hydraulic machinery; Injection (oil wells); Mineral resources; Mining machinery; Sodium; Sodium sulfate; Sulfur compounds; Calcium sulfate; Daily production; Field conditions; Formation characteristics; Injection wells; Precipitation systems; Sequential injection; Test production; Water injection",2-s2.0-85032455395
"Talovina I.V., Duryagina A.M., Vorontsova N.I., Schtyrlyaeva A.A.","Industrial role of nepouite from Elov supergene nickel deposit, Northern Urals",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032476505&doi=10.1088%2f1755-1315%2f87%2f7%2f072004&partnerID=40&md5=26cea0bc7ad25655b5b2792a4f10d357","It has been established that nickel varieties of lizardite with a content of 20.64-51.99% NiO refer to nepouite, which was for the first time determined and described as an important industrial mineral component of ore in the Elov supergene deposit. The data of X-Ray diffraction, complex thermal analysis und SEM of nepouite were obtained. Based on the results of research, this mineral was designated as a serpentine of the lizardite-nepouite series. Mg-nepouite was also determined in this assemblage. NiO content in the nepouite varies from 13.00% to 35.18%, MgO - from 18.29% to 44.61%. The thermal analysis of the serpentine shows that the temperature of the Mg-nepouite destruction is 610-615 °C that is specified to a hydrothermal process. It can be assumed that the serpentines of the lizardite-nepouite series in the weathering crust of the Ural deposits can have both exogenous and low-temperature hydrothermal origin. © Published under licence by IOP Publishing Ltd.",,"Deposits; Kaolinite; Minerals; Mining machinery; Nickel; Nickel compounds; Nickel deposits; Ores; Petrography; Serpentine; Silicate minerals; Temperature; Thermoanalysis; X ray diffraction; Hydrothermal process; Industrial mineral; Lizardite; Low temperatures; Supergene; Weathering crust; Magnesium compounds",2-s2.0-85032476505
"Lobanov D.V., Arhipov P.V., Yanyushkin A.S., Skeeba V.Y.","Physical-chemical processes of diamond grinding",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032445337&doi=10.1088%2f1755-1315%2f87%2f8%2f082029&partnerID=40&md5=821768f6511478fe4a67f9cb3698ace6","The article focuses on the relevance of the research into the problem of diamond abrasive metal-bonded tool performance loss with a view to enhancing the effectiveness of high-strength materials finishing processing. The article presents the results of theoretical and empirical studies of loading layer formation on the surface of diamond wheels during processing high-strength materials. The theoretical part deals with the physical and chemical processes at the contact area of the diamond wheel and work surface with the viewpoint of the electrochemical potentials equilibrium state. We defined dependencies for calculating the loading layer dimensions. The practical part of work centers on various electron-microscopic, spectral and X-ray diffraction studies of the metal-bonded wheel samples during diamond grinding. The analysis of the research results revealed the composition and structure of the loading layer. The validity of the theoretical data is confirmed by sufficient convergence of the calculated values with the results of empirical research. In order to reduce the intensity of loading and improve the cutting properties of metal-bonded diamond abrasive tools, it is recommended to use combined methods for more efficient processing of high-strength materials. © Published under licence by IOP Publishing Ltd.",,"Abrasives; Cutting; Cutting tools; Diamond cutting tools; Diamonds; Finishing; Grinding (machining); Grinding wheels; Metal cutting; Mining machinery; Strength of materials; Wheels; X ray diffraction; Calculated values; Cutting properties; Electrochemical potential; Empirical research; Equilibrium state; High-strength materials; Physical-chemical process; X-ray diffraction studies; Loading",2-s2.0-85032445337
"Rahman P.A., Panchenko A.A., Safarov A.M.","Using neural networks for prediction of air pollution index in industrial city",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032451199&doi=10.1088%2f1755-1315%2f87%2f4%2f042016&partnerID=40&md5=2d5883d0c99e23ba5f834b64c5bbc54f","This scientific paper is dedicated to the use of artificial neural networks for the ecological prediction of state of the atmospheric air of an industrial city for capability of the operative environmental decisions. In the paper, there is also the described development of two types of prediction models for determining of the air pollution index on the basis of neural networks: a temporal (short-term forecast of the pollutants content in the air for the nearest days) and a spatial (forecast of atmospheric pollution index in any point of city). The stages of development of the neural network models are briefly overviewed and description of their parameters is also given. The assessment of the adequacy of the prediction models, based on the calculation of the correlation coefficient between the output and reference data, is also provided. Moreover, due to the complexity of perception of the «neural network code» of the offered models by the ordinary users, the software implementations allowing practical usage of neural network models are also offered. It is established that the obtained neural network models provide sufficient reliable forecast, which means that they are an effective tool for analyzing and predicting the behavior of dynamics of the air pollution in an industrial city. Thus, this scientific work successfully develops the urgent matter of forecasting of the atmospheric air pollution index in industrial cities based on the use of neural network models. © Published under licence by IOP Publishing Ltd.",,"Air pollution; Ecology; Mining machinery; Network coding; Neural networks; Pollution; Weather forecasting; Air pollution index; Atmospheric pollution; Correlation coefficient; Environmental decisions; Neural network model; Scientific papers; Short-term forecasts; Software implementation; Forecasting",2-s2.0-85032451199
"Vertakova Y.V., Babich T.N., Polozhentseva Y.S., Zvyagintsev G.L.","Prospects for development of hydrocarbon raw materials resources reproduction",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032492596&doi=10.1088%2f1755-1315%2f87%2f9%2f092031&partnerID=40&md5=deb29cb15d71b9b107f69d223a4b72f3","The article presents data on the influence of factors in the field of innovative technology of thermocatalytic depolymerization of solid household wastes (SHW) on the efficiency and prospects for the development of technogenic hydrocarbon raw materials resource reproduction. Process thermodynamics, reactions kinetics, the mechanism of thermolysis of secondary polymers in organic solvents have been studied by means of laboratory experiments. It is shown that different morphological groups of wastes dissolve practically at the same rate at temperatures of 250-310°C. A homogeneous product is formed in the liquid phase; the spread of values for the elements lies in the interval of 1.5-4.5 %; technological requirements of the stages of formation of boiler fuels are satisfied. Using the principles of patent analysis, new techniques of processing household waste components are proposed. The basics of energy-efficient and energy-saving processes of technogenic hydrocarbon raw materials resource reproduction have been laid. The possibility of increasing the production payback and intensification is shown. Ecological and demographic safety for population and technical and economic benefits from SHW processing are achieved. © Published under licence by IOP Publishing Ltd.",,"Energy conservation; Energy efficiency; Hydrocarbons; Mining machinery; Organic polymers; Patents and inventions; Thermodynamics; Boiler fuels; Economic benefits; Energy efficient; Household waste; Innovative technology; Laboratory experiments; Liquid Phase; Patent analysis; Hydrocarbon refining",2-s2.0-85032492596
"Timerbaev N.F., Sadrtdinov A.R., Prosvirnikov D.B., Fomin A.A., Stepanov V.V.","Application of software solutions for modeling and analysis of parameters of belt drive in engineering",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032446435&doi=10.1088%2f1755-1315%2f87%2f8%2f082047&partnerID=40&md5=9ba3e485fb1fbacf2ae6c587be533aac","The application of software systems in engineering when developing the belt drive designs and evaluating their characteristics is considered. A technique for calculating and analyzing belt drives is described using the example of calculating V-belt and flat-belt drives using a software solution. As a result of the belt drive analysis, belt profiles, belt cross-sectional dimensions, drive and driven sheave diameters and power parameters are determined, and graphics images of the dependences of belt's prestressing force and the force acting on the shaft from the diameter of the driving sheave are obtained. By approximating the results of calculations, theoretical equations for calculating the power parameters of the belt drives were derived. Carrying out the analysis of belt drives with the use of software solutions allows one to avoid computational errors and to optimize the design and performance. At the same time, a convenient and intuitive interface, as well as an integrated graphical editor, provide visibility of the output data and allow the accelerated engineering analysis of the development object. © Published under licence by IOP Publishing Ltd.",,"Application programs; Belt drives; Drives; Mining machinery; Prestressing; Computational error; Engineering analysis; Graphical editors; Intuitive interfaces; Model and analysis; Prestressing forces; Software solution; Theoretical equation; Digital storage",2-s2.0-85032446435
"Aleksandrov V.I., Vasilyeva M.A., Pomeranets I.B.","Estimation of efficiency of hydrotransport pipelines polyurethane coating application in comparison with steel pipelines",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032432024&doi=10.1088%2f1755-1315%2f87%2f2%2f022001&partnerID=40&md5=2d106dc22585e9ccf595c2f51c9f82ef","The paper presents analytical calculations of specific pressure loss in hydraulic transport of the Kachkanarsky GOK iron ore processing tailing slurry. The calculations are based on the results of the experimental studies on specific pressure loss dependence upon hydraulic roughness of pipelines internal surface lined with polyurethane coating. The experiments proved that hydraulic roughness of polyurethane coating is by the factor of four smaller than that of steel pipelines, resulting in a decrease of hydraulic resistance coefficients entered into calculating formula of specific pressure loss - the Darcy-Weisbach formula. Relative and equivalent roughness coefficients are calculated for pipelines with polyurethane coating and without it. Comparative calculations show that hydrotransport pipelines polyurethane coating application is conductive to a specific energy consumption decrease in hydraulic transport of the Kachkanarsky GOC iron ore processing tailings slurry by the factor of 1.5. The experiments were performed on a laboratory hydraulic test rig with a view to estimate the character and rate of physical roughness change in pipe samples with polyurethane coating. The experiments showed that during the following 484 hours of operation, roughness changed in all pipe samples inappreciably. As a result of processing of the experimental data by the mathematical statistics methods, an empirical formula was obtained for the calculation of operating roughness of polyurethane coating surface, depending on the pipeline operating duration with iron ore processing tailings slurry. © Published under licence by IOP Publishing Ltd.",,"Coatings; Earthmoving machinery; Energy utilization; Estimation; Hydraulic fluids; Hydraulic machinery; Iron; Iron ores; Mining machinery; Pipelines; Polyurethanes; Statistics; Steel pipe; Surface roughness; Tailings; Analytical calculation; Hydraulic resistances; Hydraulic roughness; Hydrotransport pipelines; Mathematical statistics methods; Polyurethane coatings; Roughness coefficient; Specific energy consumption; Pipeline processing systems",2-s2.0-85032432024
"Abramov E.Y., Sopov V.I.","Energy efficiency analysis of two-sided feed scheme of DC traction network with high asymmetry of feeders parameters",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032454401&doi=10.1088%2f1755-1315%2f87%2f3%2f032001&partnerID=40&md5=791d7ab3a163781f35fe88484160e767","In a given research using the example of traction network area with high asymmetry of power supply parameters, the sequence of comparative assessment of power losses in DC traction network with parallel and traditional separated operating modes of traction substation feeders was shown. Experimental measurements were carried out under these modes of operation. The calculation data results based on statistic processing showed the power losses decrease in contact network and the increase in feeders. The changes proved to be critical ones and this demonstrates the significance of potential effects when converting traction network areas into parallel feeder operation. An analytical method of calculation the average power losses for different feed schemes of the traction network was developed. On its basis, the dependences of the relative losses were obtained by varying the difference in feeder voltages. The calculation results showed unreasonableness transition to a two-sided feed scheme for the considered traction network area. A larger reduction in the total power loss can be obtained with a smaller difference of the feeders' resistance and / or a more symmetrical sectioning scheme of contact network. © Published under licence by IOP Publishing Ltd.",,"Electric losses; Electric power systems; Energy efficiency; Feeding; Mining machinery; Analytical method; Calculation results; Comparative assessment; Energy efficiency analysis; Modes of operation; Parallel feeders; Potential effects; Traction substation; Rectifier substations",2-s2.0-85032454401
"Zhilenkov A.A., Chernyi S.G., Nyrkov A.P., Sokolov S.S.","Optimization Problem of Thermal Field on Surface of Revolving Susceptor in Vapor-Phase Epitaxy Reactor",2017,"IOP Conference Series: Earth and Environmental Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450231&doi=10.1088%2f1755-1315%2f87%2f8%2f082060&partnerID=40&md5=926d4591f2c015eb01d0a354ffc498cf","Nitrides of group III elements are a very suitable basis for deriving light-emitting devices with the radiating modes lengths of 200-600 nm. The use of such semiconductors allows obtaining full-color RGB light sources, increasing record density of a digital data storage device, getting high-capacity and efficient sources of white light. Electronic properties of such semi-conductors allow using them as a basis for high-power and high-frequency transistors and other electronic devices, the specifications of which are competitive with those of SiC-based devices. Only since 2000, the technology of cultivation of crystals III-N of group has come to the level of wide recognition by both abstract science, and the industry that has led to the creation of the multi-billion dollar market. And this is despite a rather low level of development of the production technology of devices on the basis of III-N of materials. The progress that has happened in the last decade requires the solution of the main problem, constraining further development of this technology today - ensuring cultivation of III-N structures of necessary quality. For this purpose, it is necessary to solve problems of the analysis and optimization of processes in installations of epitaxial growth, and, as a result, optimization of its constructions. © Published under licence by IOP Publishing Ltd.",,"Digital storage; Electron devices; Electronic properties; Epitaxial growth; Light emission; Light sources; Mining machinery; Optimization; Silicon carbide; Virtual storage; Wide band gap semiconductors; Dollar market; Electronic device; High capacity; High-frequency transistors; Light emitting devices; Optimization problems; Production technology; Thermal field; Problem solving",2-s2.0-85032450231
"Enneti R.K., German R.M., Atre S.V.","Effects of lubricant and part geometry on the ejection characteristics during die compaction",2017,"Powder Metallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023775634&doi=10.1080%2f00325899.2017.1345367&partnerID=40&md5=5bc473d3c2ec28eb1eec81cb5114d606","The ejection of a part following die compaction is a critical step in manufacturing powder metal and ceramic parts as well as pharmaceutical tablets. In this paper, the ejection of die-compacted hollow cylinders of heights 0.5 and 1.2 cm from Fe–2%Cu–0.5%C powders mixed with various amounts of (0.2–0.8 wt-%) of ethylene-bis-stearamide (EBS) was studied to understand the effect of lubricant amount and part geometry on the ejection process. Additionally, the ejection data of gears of 0.5 and 1.2 cm die compacted from Fe–2%Cu–0.5%C powders with 0.8 wt-% EBS was analysed to understand the effect of geometry on the ejection process. Several ejection parameters were found to be sensitive to the amount of EBS as well as the size and shape of the parts. The results from the present study indicated that the major portion of the ejection cycle involved the movement of the part within the die. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute.","Die compaction; ejection curve; lubricant; slide force; strip force","Compaction; Ethylene; Geometry; Lubricants; Powder metals; Powders; Die compaction; ejection curve; Ejection parameters; Ethylene bis-stearamide (EBS); Lubricant amounts; Pharmaceutical tablets; slide force; strip force; Dies",2-s2.0-85023775634
"Zhang M., PI D.","A New Time Series Representation Model and Corresponding Similarity Measure for Fast and Accurate Similarity Detection",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032670920&doi=10.1109%2fACCESS.2017.2764633&partnerID=40&md5=72daf555880da640e169fe84a98ff854","Data representation and similarity measurement are two basic aspects of similarity detection in time series data mining. In this paper, we present two novel approaches to perform similarity detection efficiently and effectively. One is composed of a new time series representation model and a corresponding similarity measure, which is called Fragment Alignment Distance (FAD); the other applies Dynamic Time Warping to the representation model of FAD and is called FAD&#x005F;DTW. The new data representation model is based on the trend information of time series, which can provide a concise yet feature-rich representation of time series. FAD is able to align the segments of time series in linear time, which greatly accelerates the similarity detection process. We extensively compare FAD and FAD&#x005F;DTW with state-of-the-art time series representation models and similarity measures in classification and clustering frameworks. Experimental results from efficiency and effectiveness validations on various data sets demonstrate that FAD and FAD&#x005F;DTW can achieve fast and accurate similarity detection. In particular, FAD is much faster than the other methods. OAPA","Biomedical measurement; Data mining; Data models; Data representation models; Extraterrestrial measurements; Hidden Markov models; Similarity measure; Time measurement; Time series analysis; Time series data mining","Data mining; Data structures; Hidden Markov models; Markov processes; Time measurement; Biomedical measurements; Data representation models; Extraterrestrial measurements; Similarity measure; Time series data mining; Time series analysis",2-s2.0-85032670920
"Cao Y., Hu Y., Deng X., Tian X.","Quality-relevant Batch Process Fault Detection Using a Multiway Multi-subspace CVA Method",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032709782&doi=10.1109%2fACCESS.2017.2764538&partnerID=40&md5=2d3c4461ece24f353cd0cb40b8cee679","For batch process fault detection, regular data-driven methods cannot distinguish qualityirrelevant faults from quality-relevant faults. To solve such problem, we propose a multiway multi-subspace canonical variate analysis (MMCVA) method for the batch processes. Firstly, the combination of batchwise unfolding and variable-wise unfolding is adopted to unfold the three-way process and quality data into two-way data. Then, we use canonical variate analysis to project the process and quality data spaces to three subspaces, a process-quality correlated subspace, a quality-uncorrelated process subspace and a processuncorrelated quality subspace. Fault detection statistics are developed based on the three subspaces. The proposed MMCVA method is capable of indicating the normality or abnormality of the quality variables while detecting a process fault. The simulation results of a fed-batch penicillin fermentation process illustrate the effectiveness of the proposed method. OAPA","batch process; Batch production systems; canonical variate analysis; Correlation; Covariance matrices; Data mining; Fault detection; fault detection; Fault diagnosis; Monitoring; principal component analysis; quality monitoring","Batch data processing; Correlation methods; Covariance matrix; Data mining; Failure analysis; Gait analysis; Monitoring; Principal component analysis; Quality control; Batch process; Batch production; Canonical variate analysis; Covariance matrices; Quality monitoring; Fault detection",2-s2.0-85032709782
"Yan W., Dai H., Chen J.","Surface crack and sand inrush disaster induced by high-strength mining: example from the Shendong coal field, China",2017,"Geosciences Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031794008&doi=10.1007%2fs12303-017-0031-7&partnerID=40&md5=bbdeebefd7b412a95ff0a91508cdd2e4","Sand inrush disaster and ground destruction induced by high-strength mining in the Shendong coal field seriously threaten the normal operation of the mine and cause significant property losses and environmental disruption. The physical simulation experiment demonstrate that the roof of high-strength mining working face can be regarded as a “step beam” structure and broken by sliding instability. The vertical damage state of overlying strata is summarized into three types: slightly, severely and very severely damage. On the basis of in situ data of the working face with the mining height greater than 3 m, the prediction formulas of the caved and fractured zone heights are given. The vertical damage types of working faces 22407 and 22402 are analyzed. Owing to the sliding instability of the roof and the thin bedrock, the surface stepped crack has become widely distributed above the high-strength mining working face. The sand inrush of working face 22402 can be interpreted by the very severely damaged of overburden and the thick aeolian sand aquifer. This work can be used to improve the understanding of mining-induced disaster and establish a disaster prediction model. © 2017 The Association of Korean Geoscience Societies and Springer-Verlag GmbH Germany",,,2-s2.0-85031794008
"Dastgir G., Kawata K., Yoshida Y.","Effect of Forced Relocation on Household Income and Consumption Patterns: Evidence from the Aynak Copper Mine Project in Afghanistan",2017,"Journal of Development Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031729542&doi=10.1080%2f00220388.2017.1385767&partnerID=40&md5=78066be6c219a791d1d52662b8c3b21f","In 2011, the government of Afghanistan and a Chinese mining company relocated an entire village near the Aynak copper mine, where they developed a mining site. This paper investigates the impact of this displacement on affected households’ income and consumption patterns using a difference-in-differences approach and primary household-level data from the villages around the mine in two periods: one just before relocation in 2011 and another in 2015. In 2011, all households of the Wali Kali village, one of the seven project-affected villages, were involuntarily relocated. Project-affected families (PAFs) claim that their traditional earning sources have been inadequately replaced by mine-related earnings and that, being separated geographically, they now face difficulty maintaining social networks that are necessary for their survival. Once lost, rebuilding social networks is not easy in war-trampled Afghanistan. This paper clarifies these shadowy effects of forced relocation and demonstrates that traditional daily labour income was reduced significantly and only partially replaced by income from mine-related activities among those who were relocated and that relocation significantly discouraged the participation in community life, reflecting the losses of social capital among the PAFs due to separation. © 2017 Informa UK Limited, trading as Taylor & Francis Group",,,2-s2.0-85031729542
"Wu Y., Hoi S.C.H., Liu C., Lu J., Sahoo D., Yu N.","SOL: A library for scalable online learning algorithms",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018699843&doi=10.1016%2fj.neucom.2017.03.077&partnerID=40&md5=e9a2d5d3a2c34636560a585463cf9866","SOL is an open-source library for scalable online learning with high-dimensional data. The library provides a family of regular and sparse online learning algorithms for large-scale classification tasks with high efficiency, scalability, portability, and extensibility. We provide easy-to-use command-line tools, python wrappers and library calls for users and developers, and comprehensive documents for both beginners and advanced users. SOL is not only a machine learning toolbox, but also a comprehensive experimental platform for online learning research. Experiments demonstrate that SOL is highly efficient and scalable for large-scale learning with high-dimensional data. © 2017 Elsevier B.V.","High dimensionality; Online learning; Scalable machine learning; Sparse learning","Artificial intelligence; Clustering algorithms; Data mining; E-learning; Learning systems; Sols; High dimensional data; High dimensionality; Large scale classifications; Online learning; Online learning algorithms; Open-source libraries; Scalable machine learning; Sparse learning; Learning algorithms",2-s2.0-85018699843
"Ferreira J.R., Jr, Oliveira M.C., de Azevedo-Marques P.M.","Characterization of Pulmonary Nodules Based on Features of Margin Sharpness and Texture",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732294&doi=10.1007%2fs10278-017-0029-8&partnerID=40&md5=5ed8c7cd376e450ef517ebb1814ffd87","Lung cancer is the leading cause of cancer-related deaths in the world, and one of its manifestations occurs with the appearance of pulmonary nodules. The classification of pulmonary nodules may be a complex task to specialists due to temporal, subjective, and qualitative aspects. Therefore, it is important to integrate computational tools to the early pulmonary nodule classification process, since they have the potential to characterize objectively and quantitatively the lesions. In this context, the goal of this work is to perform the classification of pulmonary nodules based on image features of texture and margin sharpness. Computed tomography scans were obtained from a publicly available image database. Texture attributes were extracted from a co-occurrence matrix obtained from the nodule volume. Margin sharpness attributes were extracted from perpendicular lines drawn over the borders on all nodule slices. Feature selection was performed by different algorithms. Classification was performed by several machine learning classifiers and assessed by the area under the receiver operating characteristic curve, sensitivity, specificity, and accuracy. Highest classification performance was obtained by a random forest algorithm with all 48 extracted features. However, a decision tree using only two selected features obtained statistically equivalent performance on sensitivity and specificity. © 2017 Society for Imaging Informatics in Medicine","Image classification; Lung cancer; Pattern recognition; Pulmonary nodule","Biological organs; Computerized tomography; Decision trees; Diseases; Image classification; Learning algorithms; Learning systems; Pattern recognition; Positron emission tomography; Classification performance; Classification process; Computed tomography scan; Lung Cancer; Pulmonary nodules; Random forest algorithm; Receiver operating characteristic curves; Sensitivity and specificity; Data mining",2-s2.0-85031732294
"Lee S., Park S., Chen C.W.S.","On Fisher’s dispersion test for integer-valued autoregressive Poisson models with applications",2017,"Communications in Statistics - Theory and Methods",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024392889&doi=10.1080%2f03610926.2016.1228970&partnerID=40&md5=cec6cfa43f38ae4611da697c8dd779d0","The integer-valued autoregressive (INAR) model has been widely used in diverse fields. Since the task of identifying the underlying distribution of time-series models is a crucial step for further inferences, we consider the goodness-of-fit test for the Poisson assumption on first-order INAR models. For a test, we employ Fisher’s dispersion test due to its simplicity and then derive its null limiting distribution. As an illustration, a simulation study and real data analysis are conducted for the counts of coal mining disasters, the monthly crime data set from New South Wales, and the annual numbers of worldwide earthquakes. © 2017 Taylor & Francis Group, LLC.","Coal mining disasters in the United Kingdom; Fisher’s dispersion test; goodness of fit test; Poisson with a break; time series of counts","Coal mines; Disasters; Dispersions; Poisson distribution; Goodness-of-fit test; Integer-valued autoregressive; Limiting distributions; Poisson with a break; Real data analysis; Simulation studies; Underlying distribution; United kingdom; Testing",2-s2.0-85024392889
"Suh J., Lee S., Choi Y.","UMineAR: Mobile-tablet-based abandoned mine hazard site investigation support system using augmented reality",2017,"Minerals",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032001662&doi=10.3390%2fmin7100198&partnerID=40&md5=62e97d80f9a499e9ed92068000abc62d","Conventional mine site investigation has difficulties in fostering location awareness and understanding the subsurface environment; moreover, it produces a large amount of hardcopy data. To overcome these limitations, the UMineAR mobile tablet application was developed. It enables users to rapidly identify underground mine objects (drifts, entrances, boreholes, hazards) and intuitively visualize them in 3D using a mobile augmented reality (AR) technique. To design UMineAR, South Korean georeferenced standard-mine geographic information system (GIS) databases were employed. A web database system was designed to access via a tablet groundwater-level data measured every hour by sensors installed in boreholes. UMineAR consists of search, AR, map, and database modules. The search module provides data retrieval and visualization options/functions. The AR module provides 3D interactive visualization of mine GIS data and camera imagery on the tablet screen. The map module shows the locations of corresponding borehole data on a 2D map. The database module provides mine GIS database management functions. A case study showed that the proposed application is suitable for onsite visualization of high-volume mine GIS data based on geolocations; no specialized equipment or skills are required to understand the underground mine environment. UMineAR can be used to support abandoned-mine hazard site investigations. © 2017 by the authors.","Abandoned mine; Geographic information system (GIS); Mine drift; Mining hazards; Mobile augmented reality; Site investigation",,2-s2.0-85032001662
"Eiseman C.S., Davis D.R., Blyth J.A., Wagner D.L., Palmer M.W., Feldman T.S.","A new species of Marmara (Lepidoptera: Gracillariidae: Marmarinae), with an Annotated List of Known Hostplants for the Genus",2017,"Zootaxa",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031489456&doi=10.11646%2fzootaxa.4337.2.2&partnerID=40&md5=c704a39f0756775745dad787651fd163","Larvae of the New World gracillariid moth genus Marmara are primarily stem/bark miners, with some species mining in leaves or fruits. We describe a new species, M. viburnella Eiseman & Davis, which feeds on Viburnum, initially mining the leaves but completing development as a stem miner. The type series is from Nantucket Island, Massachusetts, with observations of leaf mines indicating the species is widespread in the eastern USA. Combining previously published data, our own observations, and other sources, we present a list of known Marmara hostplants, many of which represent undescribed species. Copyright © 2017 Magnolia Press.","Ageniaspis; Barkminer; Leafminer; Quadrastichus; Stem miner; Viburnum",,2-s2.0-85031489456
"Zuo Y., Wu J., Zhang H., Wang D., Xu K.","Complementary Aspect-based Opinion Mining",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032281547&doi=10.1109%2fTKDE.2017.2764084&partnerID=40&md5=1ef4f8f784cc76863377e0d0d92cdfc3","Aspect-based opinion mining is finding elaborate opinions towards a subject such as a product or an event. With explosive growth of opinionated texts on the Web, mining aspect-level opinions has become a promising means for online public opinion analysis. In particular, the boom of various types of online media provides diverse yet complementary information, bringing unprecedented opportunities for cross media aspect-opinion mining. Along this line, we propose CAMEL, a novel topic model for complementary aspect-based opinion mining across asymmetric collections. CAMEL gains information complementarity by modeling both common and specific aspects across collections, while keeping all the corresponding opinions for contrastive study. An auto-labeling scheme called AME is also proposed to help discriminate between aspect and opinion words without elaborative human labeling, which is further enhanced by adding word embedding-based similarity as a new feature. Moreover, CAMEL-DP, a nonparametric alternative to CAMEL is also proposed based on coupled Dirichlet Processes. Extensive experiments on real-world multi-collection reviews data demonstrate the superiority of our methods to competitive baselines. This is particularly true when the information shared by different collections becomes seriously fragmented. Finally, a case study on the public event &#x201C;2014 Shanghai Stampede&#x201D; demonstrates the practical value of CAMEL for real-world applications. IEEE","Analytical models; Aspect-based Opinion Mining; Data mining; Dirichlet Process; Entropy; Labeling; Latent Dirichlet Allocation (LDA); Maximum Entropy Model; Media; Social network services; Word Embedding","Analytical models; Entropy; Labeling; Social aspects; Statistics; Variational techniques; Dirichlet process; Latent dirichlet allocations; Maximum entropy modeling; Media; Opinion mining; Social network services; Word Embedding; Data mining",2-s2.0-85032281547
"Hamasu C., Burroughs C.M., Glenn E., Ball A.L.","Mining Data in Electronic Health Record Systems: Opportunities for Librarians",2017,"Journal of Hospital Librarianship",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031492930&doi=10.1080%2f15323269.2017.1366776&partnerID=40&md5=eb78e61532b90b66d368dd7b544e3b5b","In the past decade, the National Network of Libraries of Medicine (NNLM) and the Medical Library Association recognized an important trend of health sciences librarian involvement in electronic health records (EHR). This trend continues as librarians seek to help solve information challenges in health care settings. In 2016, two regions of the NNLM sponsored a forum focusing on the use of EHR data. Attended by 77 librarians, clinicians and bioinformaticians, the forum confirmed that there are opportunities for librarians to develop roles related to use and analysis of EHR data as evidence for patient care, patient safety and quality improvement. © 2017, Routledge. All rights reserved.","Data science; electronic health record; electronic health record data; hospital librarians; hospital libraries; observational evidence; professional role","adult; electronic health record; human; librarian; library; mining; patient care; patient safety; professional standard; total quality management",2-s2.0-85031492930
"Kasuya S., Zhou X., Tago K., Nishimura S., Jin Q.","Cyber-Enabled Well-Being Oriented Daily Living Support Based on Personal Data Analytics",2017,"IEEE Transactions on Emerging Topics in Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032257198&doi=10.1109%2fTETC.2017.2763966&partnerID=40&md5=dc3d1fe69398df9c69ca8c7e366106d9","We are living in a cyber-physical-social environment with a variety of lifestyles and values. Living support has become important in such a diverse society. Owing to the ability to collect a large amount of personal data or life logs in the cyber-physical-social environment, it is now possible for us to provide living support based on personal data analysis. Moreover, analyzing such data can facilitate a deep understanding of an individual. In this study, we focus on the provision of cyber-enabled well-being oriented daily living support for an individual based on personal data analytics. Three categories of personal data are identified from an individual&#x0027;s daily life data. In this paper, we discuss the basic concept, model, and framework for well-being oriented personal data analysis in order to offer suggestions and advice to improve the living quality of an individual. Finally, we report a feasibility study with an application scenario by using personal and environmental data. IEEE","Algorithm design and analysis; Collaboration; Cyber-Enabled Application; Data analysis; Data mining; Data-Driven User Behavior Analysis; Feature extraction; Personal Data Analytics; Prediction algorithms; Sensors; Well-Being Oriented Living Support","Behavioral research; Cyber Physical System; Data handling; Data reduction; Feature extraction; Information analysis; Quality control; Sensors; Algorithm design and analysis; Collaboration; Data analytics; Prediction algorithms; User behavior analysis; Well being; Data mining",2-s2.0-85032257198
"Hain C.R., Anderson M.C.","Estimating morning change in land surface temperature from MODIS day/night observations: Applications for surface energy balance modeling",2017,"Geophysical Research Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030645661&doi=10.1002%2f2017GL074952&partnerID=40&md5=50afa415e4469420eeca37478a86f592","Observations of land surface temperature (LST) are crucial for the monitoring of surface energy fluxes from satellite. Methods that require high temporal resolution LST observations (e.g., from geostationary orbit) can be difficult to apply globally because several geostationary sensors are required to attain near-global coverage (60°N to 60°S). While these LST observations are available from polar-orbiting sensors, providing global coverage at higher spatial resolutions, the temporal sampling (twice daily observations) can pose significant limitations. For example, the Atmosphere Land Exchange Inverse (ALEXI) surface energy balance model, used for monitoring evapotranspiration and drought, requires an observation of the morning change in LST—a quantity not directly observable from polar-orbiting sensors. Therefore, we have developed and evaluated a data-mining approach to estimate the midmorning rise in LST from a single sensor (two observations per day) of LST from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor on the Aqua platform. In general, the data-mining approach produced estimates with low relative error (5 to 10%) and statistically significant correlations when compared against geostationary observations. This approach will facilitate global, near-real-time applications of ALEXI at higher spatial and temporal coverage from a single sensor than currently achievable with current geostationary data sets. Published 2017. This article is a US Government work and is in the public domain in the United States of America.","drought; evapotranspiration; remote sensing","Atmospheric temperature; Drought; Energy balance; Evapotranspiration; Geostationary satellites; Interfacial energy; Orbits; Radiometers; Remote sensing; Satellite imagery; Surface measurement; Surface properties; Geo-stationary orbits; Geostationary observations; High temporal resolution; Land surface temperature; Moderate resolution imaging spectroradiometer sensors; Spatial resolution; Surface energy balance modeling; Surface energy fluxes; Data mining; data mining; drought; energy balance; estimation method; evapotranspiration; geostationary satellite; land surface; MODIS; observational method; remote sensing; satellite data; surface energy; surface temperature",2-s2.0-85030645661
"Mukund K., Ward S.R., Lieber R.L., Subramaniam S.","Co-expression Network Approach to Studying the Effects of Botulinum Neurotoxin-A",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032286546&doi=10.1109%2fTCBB.2017.2763949&partnerID=40&md5=6e01bc4f32d2d93a9af3c296efa2afd7","Botulinum Neurotoxin A (BoNT-A) is a potent neurotoxin with several clinical applications.The goal of this study was to utilize co-expression network theory to analyze temporal transcriptional data from skeletal muscle after BoNT-A treatment. Expression data for 2000 genes (extracted using a ranking heuristic) served as the basis for this analysis. Using weighted gene co-expression network analysis (WGCNA), we identified 19 co-expressed modules, further hierarchically clustered into 5 groups. Quantifying average expression and co-expression patterns across these groups revealed temporal aspects of muscle&#x0027;s response to BoNT-A. Functional analysis revealed enrichment of group 1 with metabolism; group 5 with contradictory functions of atrophy and cellular recovery; and groups 2 and 3 with extracellular matrix (ECM) and non-fast fiber isoforms. Topological positioning of two highly ranked, significantly expressed genes- Dclk1 and Ostalpha within group 5 suggested possible mechanistic roles in recovery from BoNT-A induced atrophy. Phenotypic correlations of groups with titin and myosin protein content further emphasized the effect of BoNT-A on the sarcomeric contraction machinery in early phase of chemodenervation. In summary, our approach revealed a hierarchical functional response to BoNT-A induced paralysis with early metabolic and later ECM responses and identified putative biomarkers associated with chemodenervation. Additionally, our results provide an unbiased validation of the response documented in our previous work IEEE","Bioinformatics; botoxR; clustering; Co-expression networks; Correlation; cross sectional temporal data; Data mining; gene ranking; muscle; Muscles; Proteins; Rats; Real-time systems; timecourse","Bioinformatics; Computer system recovery; Correlation methods; Data mining; Genes; Interactive computer systems; Machinery; Metabolism; Molecular biology; Muscle; Proteins; Rating; Real time systems; botoxR; clustering; Co-expression networks; Gene ranking; Temporal Data; Time course; Gene expression",2-s2.0-85032286546
"Lee N.-U., Shim J.-S., Ju Y.-W., Park S.-C.","Design and implementation of the SARIMA–SVM time series analysis algorithm for the improvement of atmospheric environment forecast accuracy",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031942810&doi=10.1007%2fs00500-017-2825-y&partnerID=40&md5=f4f8fed584ed91aa979e5e58366908d3","With the recent increased interest in atmospheric pollutants in South Korea, studies on the analysis and forecast of atmospheric pollution using Internet-of-Things technology have been actively conducted. To forecast atmospheric pollution, a multiple regression analysis technique based on statistical techniques, data mining, and an analysis technique combining time series models have typically been used. In terms of accuracy, however, multiple regression analysis is insufficient for analyzing atmospheric environment data in South Korea. In addition, although the time series analysis technique is appropriate for analyzing linear data, it is inappropriate for analyzing atmospheric environment data in South Korea, where linear and nonlinear data are mixed. Therefore, this study proposes a seasonal auto regressive integrated moving average–support vector machine (SARIMA–SVM) time series analysis algorithm, combining time series analysis and nonlinear analysis, for data analysis of atmospheric environment information and improvement of pollution forecast accuracy. The proposed algorithm analyzes the seasonality in environmental contamination by using the SARIMA model, and succeeds in improving accuracy in the contamination forecast through an analysis of linear and nonlinear characteristics by applying an SVM nonlinear regression model. A comparative assessment with the existing atmospheric contamination forecast algorithm was conducted as well. The assessment results show that the forecast accuracy of the proposed algorithm improved by 20.81% for fine dust, and by 43.77% for ozone, compared to the performance of the existing models. © 2017 Springer-Verlag GmbH Germany","Atmospheric pollution; Forecast accuracy; SARIMA model; SVM; Time series analysis","Air pollution; Data mining; Forecasting; Harmonic analysis; Image retrieval; Nonlinear analysis; Pollution; Regression analysis; Support vector machines; Weather forecasting; Atmospheric pollution; Auto-regressive integrated moving average; Environmental contamination; Forecast accuracy; Internet of things technologies; Multiple regression analysis; Nonlinear regression models; Sarima models; Time series analysis",2-s2.0-85031942810
"de Gooyert V., Rouwette E., van Kranenburg H., Freeman E.","Reviewing the role of stakeholders in Operational Research: A stakeholder theory perspective",2017,"European Journal of Operational Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018255940&doi=10.1016%2fj.ejor.2017.03.079&partnerID=40&md5=14444811a209a09764ab45908c66d625","The role of stakeholders in organizational decision-making is gaining more and more attention. Managers find that in order to create value sustainably and ethically, it is necessary to balance the interests of various stakeholders. This trend is reflected in the management literature, where much advancement has been made in what is known as stakeholder theory. In the founding years of stakeholder theory there was a close connection and interchange of ideas between stakeholder theorists and operational researchers. Yet in recent years, the stream of papers that includes both fields has dwindled to a trickle. This lack of theoretical integration is surprising, as Operational Research is in nature a collaborative discipline. In this paper, we discuss three topics in stakeholder theory that can inform and improve studies involving working with stakeholders in Operational Research: instrumental versus moral stakeholder theory; focusing on trade-offs versus focusing on avoiding trade-offs; and focusing on the decision-making organization versus focusing on stakeholder engagement. We then conduct a systematic review of 144 Operational Research articles on the topic ‘stakeholders’. Content analysis of these articles reveals four distinct traditions of working with stakeholders: optimizing, balancing, structuring and involving. We compare the four traditions on goals, role of the analyst, type of data used, and results. Our analysis provides the basis for recommendations to Operational Research practitioners on how to work with stakeholders. © 2017 Elsevier B.V.","Behavioral OR; Implementation; Stakeholder theory; Systematic review","Commerce; Data mining; Decision making; Decision theory; Focusing; Behavioral OR; Content analysis; Implementation; Operational research; Organizational decision making; Stakeholder engagement; Stakeholder theory; Systematic Review; Economic and social effects",2-s2.0-85018255940
"Startek M.P., Nogły J., Gromadka A., Grzebelus D., Gambin A.","Inferring transposons activity chronology by TRANScendence - TEs database and de-novo mining tool",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031502238&doi=10.1186%2fs12859-017-1824-4&partnerID=40&md5=36da3f050e817482410067fa4e9d1bdb","Background: The constant progress in sequencing technology leads to ever increasing amounts of genomic data. In the light of current evidence transposable elements (TEs for short) are becoming useful tools for learning about the evolution of host genome. Therefore the software for genome-wide detection and analysis of TEs is of great interest. Results: Here we describe the computational tool for mining, classifying and storing TEs from newly sequenced genomes. This is an online, web-based, user-friendly service, enabling users to upload their own genomic data, and perform de-novo searches for TEs. The detected TEs are automatically analyzed, compared to reference databases, annotated, clustered into families, and stored in TEs repository. Also, the genome-wide nesting structure of found elements are detected and analyzed by new method for inferring evolutionary history of TEs. We illustrate the functionality of our tool by performing a full-scale analyses of TE landscape in Medicago truncatula genome. Conclusions: TRANScendence is an effective tool for the de-novo annotation and classification of transposable elements in newly-acquired genomes. Its streamlined interface makes it well-suited for evolutionary studies. © 2017 The Author(s).","Evolutionary history; Hill-climbing algorithm; Transposable elements","Computational tools; Effective tool; Evolutionary history; Hill climbing algorithms; Medicago truncatula; Reference database; Technology leads; Transposable elements; Genes",2-s2.0-85031502238
"Riedel I., Guéguen P.","Modeling of damage-related earthquake losses in a moderate seismic-prone country and cost–benefit evaluation of retrofit investments: application to France",2017,"Natural Hazards",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031911969&doi=10.1007%2fs11069-017-3061-6&partnerID=40&md5=f71bf356c7c714e8b123be81f6091763","We performed large-scale earthquake economic loss estimations for France and cost–benefit analyses for several French cities by developing a semiempirical, intensity-based approach. The proposed methodology is inexpensive and easily applicable in case of a paucity of detailed information regarding the specific regional seismic hazard and the structural characteristics of the building stock, which is of particular importance in moderate-to-low seismic hazard regions. The exposure model is derived from census datasets, and the seismic vulnerability distribution of buildings is calculated using data mining techniques. Several hypothetical, large-scale retrofit scenarios are proposed, with increasing levels of investment. These cities, in their respective reinforced states, are then subjected to a series of hazard scenarios. Seismic hazard data for different return periods are calculated from regulatory accelerations from French seismic zoning. Loss estimations for the original (non-reinforced) configuration show high levels of expected building repair and replacement costs for all time spans. Finally, the benefits in terms of damage avoidance are compared with the costs of each retrofit measure. Relatively limited strengthening investments reduce the probability of building collapse, which is the main cause of human casualties. However, the results of this study suggest that retrofitting is, on average, only cost-effective in the parts of France with the highest seismicity and over the longest time horizons. © 2017 Springer Science+Business Media B.V.","Cost–benefit analysis; Earthquake loss; Existing buildings; France; Mitigation actions; Seismic hazard; Vulnerability",,2-s2.0-85031911969
"Kiluk S.","Diagnostic information system dynamics in the evaluation of machine learning algorithms for the supervision of energy efficiency of district heating-supplied buildings",2017,"Energy Conversion and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019049599&doi=10.1016%2fj.enconman.2017.05.006&partnerID=40&md5=fdfd2770d971af06aa612eda3edd1808","Modern ways of exploring the diagnostic knowledge provided by data mining and machine learning raise some concern about the ways of evaluating the quality of output knowledge, usually represented by information systems. Especially in district heating, the stationarity of efficiency models, and thus the relevance of diagnostic classification system, cannot be ensured due to the impact of social, economic or technological changes, which are hard to identify or predict. Therefore, data mining and machine learning have become an attractive strategy for automatically and continuously absorbing such dynamics. This paper presents a new method of evaluation and comparison of diagnostic information systems gathered algorithmically in district heating efficiency supervision based on exploring the evolution of information system and analyzing its dynamic features. The process of data mining and knowledge discovery was applied to the data acquired from district heating substations’ energy meters to provide the automated discovery of diagnostic knowledge base necessary for the efficiency supervision of district heating-supplied buildings. The implemented algorithm consists of several steps of processing the billing data, including preparation, segmentation, aggregation and knowledge discovery stage, where classes of abstract models representing energy efficiency constitute an information system representing diagnostic knowledge about the energy efficiency of buildings favorably operating under similar climate conditions and supplied from the same district heating network. The authors analyzed the evolution of a series of information systems originating from the same knowledge discovery algorithm applied to a sequence of energy consumption-related data. Specifically, the rough sets theory was applied to describe the knowledge base and measure the uncertainty of machine learning predictions of current classification based on a past knowledge base. Fluctuations of diagnostic class membership were identified and provided for the differentiation between returning and novel fault detections, thus introducing the qualities of information system uncertainty and its sustainability. The usability of the new method was demonstrated in the comparison of results for exemplary data mining algorithms implemented on real data from over one thousand buildings. © 2017","District heating; Energy efficiency; Entropy; Information system dynamics; Machine learning","Artificial intelligence; Data handling; District heating; Energy efficiency; Energy utilization; Entropy; Heating; Information systems; Knowledge based systems; Learning algorithms; Learning systems; System theory; Attractive strategies; Classification system; Data mining algorithm; Data mining and knowledge discovery; Diagnostic knowledge; Diagnostic knowledge base; Heating efficiencies; Technological change; Data mining",2-s2.0-85019049599
"Kim K., Hong J.-S.","A hybrid decision tree algorithm for mixed numeric and categorical data in regression analysis",2017,"Pattern Recognition Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027877017&doi=10.1016%2fj.patrec.2017.08.011&partnerID=40&md5=c58c50a5ca40fc03ae174c528f455de2","In many real world problems, the collected data are not always numeric; rather, the data can include categorical variables. Inclusion of different types of variables may lead to complications in regression analysis. Many regression algorithms such as linear regression, support vector regression, and neural networks that train parameters of a model to identify relations between input and output variables, can easily process numeric variables; however, there are additional considerations for categorical variables. On the other hand, a decision tree algorithm estimates a target based on the specified rules; therefore, it can support categorical variables as well as numeric variables. Using this property, a new hybrid model combining a decision tree with another regression algorithm is proposed to analyze mixed data. In the proposed model, the portions explained by categorical variables in target values are estimated by the decision tree and the remaining parts are predicted by any regression algorithm trained by numerical variables. The proposed algorithm was evaluated using 12 datasets selected from real decision problems, and it was confirmed that the proposed algorithm achieved better or comparable accuracy than the comparison methods including the M5 decision tree and the evolutionary tree. In addition, the new hybrid method does not significantly increase computational complexity, even though it builds two separate models, which is an advantage that is in contrast with the M5 decision tree and the evolutionary tree. © 2017 Elsevier B.V.","Decision tree; Hybrid model; Mixed data; Regression","Clustering algorithms; Decision trees; Evolutionary algorithms; Regression analysis; Trees (mathematics); Categorical variables; Decision-tree algorithm; Hybrid decision trees; Hybrid model; Mixed data; Regression; Regression algorithms; Support vector regression (SVR); Data mining",2-s2.0-85027877017
"Kaneda Y., Shibata S., Mineno H.","Multi-modal sliding window-based support vector regression for predicting plant water stress",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026301078&doi=10.1016%2fj.knosys.2017.07.028&partnerID=40&md5=969816b38927a6e6c727285e7bbd3e89","Information communication technology (ICT) is required in the field of agriculture to solve problems arising because of the aging of farmers and shortage of heirs. In particular, environmental sensors and cameras are widely used in existing agricultural support systems for easy data collection. Although the traditional purpose of these systems is naive monitoring and controlling of the environment, the propagation of advanced cultivation is now expected by applying the data to machine learning and data mining technologies. Therefore, we propose a novel multi-modal sliding window-based support vector regression (multi-modal SW-SVR) method for accurate prediction of complicated water stress, which is a plant status, from two data types, namely environmental and plant image data. The proposed method includes two methodologies, SW-SVR and deep neural network (DNN) as a multi-modal feature extractor for SW-SVR. SW-SVR, which we proposed previously, is a suitable learning method for data with time-dependent characteristics, such as plant status. Moreover, we propose a new image feature, remarkable moving objects detected by adjacent optical flow (ROAF), to enable DNN to extract essential features easily for predicting water stress. Compared with existing regression models and features, the proposed multi-modal SW-SVR with ROAF demonstrates more precise and stable water stress prediction. © 2017 The Authors","Deep neural network; Ensemble learning; Image processing; Support vector regression; Water stress","Agricultural machinery; Agriculture; Cultivation; Deep neural networks; Education; Forecasting; Image processing; Learning systems; Object detection; Optical data processing; Plants (botany); Regression analysis; Data mining technology; Ensemble learning; Environmental sensor; Information communication technology; Monitoring and controlling; Support vector regression (SVR); Time-dependent characteristics; Water stress; Deep learning",2-s2.0-85026301078
"Zamani Sabzi H., King J.P., Abudu S.","Developing an intelligent expert system for streamflow prediction, integrated in a dynamic decision support system for managing multiple reservoirs: A case study",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018926039&doi=10.1016%2fj.eswa.2017.04.039&partnerID=40&md5=69de10cb6573e73b29f195c69ddbeed3","Since fresh water is limited while agricultural and human water demands are continuously increasing, optimal prediction and management of streamflows as a source of fresh water is crucially important. This study investigates and demonstrates how data preprocessing and data mining techniques would improve the accuracy of streamflow predictive models. Based on easily accessible Snow Telemetry data (SNOTEL), four streamflow prediction models – autoregressive integrated moving average (ARIMA), artificial neural networks (ANNs), a hybrid-model of ANN and ARIMA (ANN-ARIMA), and an adaptive neuro fuzzy inference system (ANFIS) – were developed and utilized in a streamflow prediction process on Elephant Butte Reservoir. Utilizing the statistical correlation analysis and the extracting importance degrees of predictors led to efficiently select the most effective predictors for daily and monthly streamflow to Elephant Butte Reservoir. For the daily prediction time step, by preprocessing the historical data and extracting and utilizing the extracted climate variability indices through data mining techniques, the ANFIS model achieved a superior streamflow prediction performance for Elephant Butte Reservoir compared to the other three evaluated prediction models. Additionally, for predicting monthly streamflow to the Elephant Butte Reservoir, ANFIS showed significantly higher accuracy than the ANNs. As an optimal application of the developed predictive expert systems, successful integrating the prediction models in integrated reservoir operations balanced the need for a reliable supply of irrigation water against losses through evaporation. The optimal operation plan significantly minimizes the total evaporation loss from both reservoirs by providing the optimal storage levels in both reservoirs. This study provides the conceptual procedures of non-seasonal (ARIMA) model, and since the model is univariate, it demonstrates a strongly-reliable inflow prediction when existing information is limited to streamflow data as a predictor. © 2017 Elsevier Ltd","ANFIS; ANN; ARIMA; Data mining; Hybrid model of ANN-ARIMA; Streamflow prediction","Artificial intelligence; Climate models; Correlation methods; Decision support systems; Digital storage; Evaporation; Expert systems; Forecasting; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Irrigation; Neural networks; Reservoirs (water); Stream flow; Water; Adaptive neuro-fuzzy inference system; ANFIS; ARIMA; Auto-regressive integrated moving average; Hybrid model; Intelligent expert systems; Statistical correlation; Streamflow prediction; Data mining",2-s2.0-85018926039
"Abellán J., Mantas C.J., Castellano J.G.","A Random Forest approach using imprecise probabilities",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028006288&doi=10.1016%2fj.knosys.2017.07.019&partnerID=40&md5=e2d970303cc2e52b5dc38af6bab5e53f","The Random Forest classifier has been considered as an important reference in the data mining area. The building procedure of its base classifier (a decision tree) is principally based on a randomization process of data and features; and on a split criterion, which uses classic precise probabilities, to quantify the gain of information. One drawback found on this classifier is that it has a bad performance when it is applied on data sets with class noise. Very recently, it is proved that a new criterion which uses imprecise probabilities and general uncertainty measures, can improve the performance of the classic split criteria. In this work, the base classifier of the Random Forest is modified using that new criterion, producing also a new single decision tree model. This model join with the randomization process of features is the base classifier of a new procedure similar to the Random Forest, called Credal Random Forest. The principal differences between those two models are presented. In an experimental study, it is shown that the new method represents an improvement of the Random Forest when both are applied on data sets without class noise. But this improvement is notably greater when they are applied on data sets with class noise. © 2017 Elsevier B.V.","Class noise; Classification; Imprecise probabilities; Random Forest; Uncertainty measures","Data mining; Decision trees; Probability; Random processes; Uncertainty analysis; Base classifiers; Class noise; Imprecise probabilities; Random forest classifier; Random forests; Randomization process; Single decision; Uncertainty measures; Classification (of information)",2-s2.0-85028006288
"Yu H., Lee H., Jeon H.","What is 5G? Emerging 5G mobile services and network requirements",2017,"Sustainability (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031427232&doi=10.3390%2fsu9101848&partnerID=40&md5=691de92da3f4cd8eb3227a2765ccf585","In this paper, emerging 5G mobile services are investigated and categorized from the perspective of not service providers, but end-users. The development of 5G mobile services is based on an intensive analysis of the global trends in mobile services. Additionally, several indispensable service requirements, essential for realizing service scenarios presented, are described. To illustrate the changes in societies and in daily life in the 5G era, five megatrends, including the explosion of mobile data traffic, the rapid increase in connected devices, everything on the cloud, hyper-realistic media for convergence services and knowledge as a service enabled by big-data analysis, are examined. Based on such trends, we classify the new 5G services into five categories in terms of the end-users' experience as follows: immersive 5G services, intelligent 5G services, omnipresent 5G services, autonomous 5G services and public 5G services. Moreover, several 5G service scenarios in each service category are presented, and essential technical requirements for realizing the aforementioned 5G services are suggested, along with a competitiveness analysis on 5G services/devices/network industries and the current condition of 5G technologies. © 2017 by the authors.","5G network; 5G services; Network architecture; Technical requirements","competitiveness; convergence; data mining; knowledge; mass media; mobile communication; mobile phone; network analysis",2-s2.0-85031427232
"Daood A., Salman I., Ghneim N.","Comparison study of automatic classifiers performance in emotion recognition of Arabic social media users",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031755098&partnerID=40&md5=4b9e522c96a419706379f911933dc05f","Emotion recognition from text gained a lot of interest in the last years, but some languages such as Arabic (with its different spoken dialects) have not been given such attention. In this paper, we present our work in the Emotion detection of Arabic texts, with a focus on Levantine Twitter Messages. We have constructed a corpus of Arabic Levantine tweets, and annotated it with correspondent emotions. We implemented different methods to automatically classify text messages of individuals to infer their emotional states. We compared the results of different machine learning algorithms, and the inclusion of different features, to determine the best configuration of the emotion recognition system. © 2005 – ongoing JATIT & LLS.","Data mining; Emotion detection from arabic text; Emotional analysis; Syrian dialects; Twitter",,2-s2.0-85031755098
"Mahmood M., Orazalin N.","Green governance and sustainability reporting in Kazakhstan's oil, gas, and mining sector: Evidence from a former USSR emerging economy",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027460087&doi=10.1016%2fj.jclepro.2017.06.203&partnerID=40&md5=7ff6cd4e6d49eb48ccb3e8883de9d2bb","Based on essence of the stakeholder and resource dependence theories, this study examines relationships between corporate board characteristics and sustainability reporting (SR) in oil, gas and mining companies in Kazakhstan. All data on board characteristics and SR were collected manually from annual reports, investment memorandums, sustainability reports, and corporate social responsibility (CSR) reports, while financial data were extracted from financial statements of the companies. The results show that board characteristics such as board size and gender diversity are the most important factors in determining the scope and quality of sustainability information. The findings of the study will be useful for investors, policy makers, regulators and practitioners, as they suggest that effective board characteristics could improve corporate governance (CG) practices, which, in turn, could lead to more transparent and better corporate reporting practices in emerging economies generally and the 15 former USSR countries specifically. © 2017 Elsevier Ltd","Board of directors; Green governance; Kazakhstan; Petroleum and mining industries; Sustainability reporting","Economic and social effects; Sustainable development; Board characteristics; Board of directors; Corporate social responsibilities (CSR); Green governance; Kazakhstan; Sustainability informations; Sustainability report; Sustainability reporting; Economics",2-s2.0-85027460087
"Thammaraksa C., Wattanawan A., Prapaspongsa T.","Corporate environmental assessment of a large jewelry company: From a life cycle assessment to green industry",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027502440&doi=10.1016%2fj.jclepro.2017.06.220&partnerID=40&md5=6e0ab1f92f258091f6dc5d1900ef4155","Jewelry and precious metal industry has positively affected global economy via high profits and incomes as well as potentially harmed the environment and society due to its resource intensiveness and pollution. The objectives of this research are to define significant sources of environmental impact throughout a large jewelry manufacturing value chain, and to understand the consequences of using materials from different sources in order to identify potential solutions towards green industry. Feasible solutions to environmental impact reduction regarding material sourcing, energy consumption and waste treatment are recommended based on the comparison of the results. An environmental assessment is conducted following a life cycle assessment framework provided in ISO 14040:2006 and ISO 14044:2006. The assessment focuses on the impacts of environmental interventions from the value chain associated with the company operations in 2013. All in all one business-as-usual, ten alternative and nine sensitivity scenarios are developed to assess the impacts from the company's operations in 2013, to determine potential impact reductions from feasible solutions and to investigate the robustness of the results. The representative data for manufacturing stage were primarily gathered from the company while the data for other stages are secondary data. Gold and silver mining and refining is the largest contributor from the overall supply chain for all impact categories considered. Using recycled gold and silver is the most effective solution to decrease life cycle negative impacts. The use of gold and silver recycled from high-value industries (i.e. jewelry production) produces less adverse impacts compared to gold and silver recycled from end-of-life electronic waste. According to the statistical data, gold and silver mining and refining could be the suppliers capable of supplying gold and silver because of the market constraints of recycled gold and silver. Green mining operations are introduced as alternatives to reduce impact directly generated from mining operations. For manufacturing, electricity consumption reduction shows a potential impact reduction. Using recycled gold and silver is recommended for jewelry manufacturers who plan to reduce environmental impacts. Using gold and silver from green mining is also recommended when the use of gold and silver from mining is necessary. Reducing electricity consumption in manufacturing is also one option for the company since it could be activated directly. © 2017 Elsevier Ltd","Corporate environmental assessment; Gold and silver mining; Jewelry manufacturing; Life cycle assessment; Precious metal recycling","Data reduction; Electric power utilization; Electronic Waste; Energy utilization; Gold; Industrial research; Life cycle; Manufacture; Precious metals; Recycling; Refining; Silver; Silver refining; Supply chains; Waste treatment; Electricity-consumption; End-of-life electronics; Environmental assessment; Gold and silver; Life Cycle Assessment (LCA); Manufacturing stages; Metal recycling; Precious metal industry; Environmental impact",2-s2.0-85027502440
"Cruz M.D.A., Araújo O.D.Q.F., de Medeiros J.L., de Castro R.D.P.V., Ribeiro G.T., de Oliveira V.R.","Impact of solid waste treatment from spray dryer absorber on the levelized cost of energy of a coal-fired power plant",2017,"Journal of Cleaner Production",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027493736&doi=10.1016%2fj.jclepro.2017.07.061&partnerID=40&md5=fb6db167f05d617168d35508cf519d1f","Coal-fired power plants with semi-dry flue gas desulfurization (semi-dry FGD) system produce daily tones of ashes contaminated with calcium sulphite. To turn this solid waste useful (e.g. to the cement industry) and avoid landfill disposal, the present study suggests a semi-dry FGD solid waste treatment unit, that promotes the dry oxidation of calcium sulfite to calcium sulfate. Sizing of main equipment using pilot-plant and patents data allows economic evaluation of capital expenditure, operational and maintenance costs, and sale of the treated residue, which permits estimation of levelized cost of energy to assess the impact of the technology on the electricity price of a power plant using the proposed solid waste treatment unit. As base case, a Brazilian coal-fired power plant facing decision making process on semi-dry FGD waste destination is selected. Results demonstrate that the semi-dry FGD, without the solid treatment unit, has total levelized cost of energy increased in 0.56% (from 94.44 to 94.97 $/MWh) resulting from solids waste disposal. If the treated semi-dry FGD waste was transferred (at zero revenue) as additive to a cement industry, the levelized cost of energy of the power plant would remain approximately unchanged. This is because the increase of 0.51$/MWh resulting from the investment and operation and maintenance cost of the treatment unit is compensated by the decrease of 0.53$/MWh, in virtue of the avoided waste disposal costs. However, if the commercialization as raw material of the treated semi-dry FGD waste is considered, a reduction of 2.83 $/MWh (∼3%) on the levelized cost of energy (to 92.14 $/MWh) would occur. In both cases, the proposed treatment unit shows small impact on the total power plant levelized cost of energy, besides solving the solid management problems of landfill saturation, land use and costs related to landfill maintenance. Thus, it is adequate to implement the semi-dry FGD waste treatment unit on the power plant in question. The conclusion can be extended to plants with similar design and economic parameters. © 2017 Elsevier Ltd","Calcium sulphite oxidation; Coal-fired power plant; Flue gas desulfurization; Levelized cost of energy; Solid waste treatment; Spray dryer absorbers","Calcium; Cement industry; Cements; Coal; Coal fueled furnaces; Costs; Decision making; Desulfurization; Dryers (equipment); Economics; Fire tube boilers; Flue gases; Fossil fuel power plants; Gas plants; Investments; Land fill; Land use; Maintenance; Mining; Pilot plants; Solid wastes; Waste disposal; Coal-fired power plant; Decision making process; Dry flue gas desulfurizations; Economic evaluations; Flue gas desulfurization; Levelized costs; Operation and maintenance; Spray dryer absorbers; Waste treatment",2-s2.0-85027493736
"Marchese E., Scorpio V., Fuller I., McColl S., Comiti F.","Morphological changes in Alpine rivers following the end of the Little Ice Age",2017,"Geomorphology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028081539&doi=10.1016%2fj.geomorph.2017.07.018&partnerID=40&md5=67fc265ff1babbe7b05f8fead1187dcb","This work investigates the channel changes of Alpine rivers from the end of the Little Ice Age (1850s) to the 1950s, with the aim to determine the possible role of climatic variations occurred in this period before the onset of anthropic pressures (i.e., dams, check-dams, bank protections, and gravel mining). The research was conducted on 17 river catchments of South Tyrol (northern Italy), glaciated and unglaciated. A multitemporal GIS analysis approach was adopted to assess the morphological changes (in terms of channel width and pattern) from three different sources: (i) Austrian cadastral map (1858), (ii) maps from the Italian Institute of Military Geography (1917–1925), and (iii) two aerial photo sets taken in 1945 and 1954. The analysed river network (a total of 480 km) was subdivided into 162 morphologically homogeneous reaches (76 confined, 81 partly confined, and 5 unconfined), with lengths ranging from 630 to 5500 m, slope from 0.3 to 24%, and drained area from 20 to ~ 4000 km2. The statistical relationships among morphological changes and reach- and basin-scale factors were analysed by univariate and multivariate methods, and the relationships between width changes and 36 controlling factors were explored using Principal Component Analysis. The variability in width and morphological pattern changes were very pronounced between and within single rivers, highlighting the value of such a large data set. Overall, the analysed rivers varied their morphological pattern, mostly exhibiting a shift from multithread/transitional to single-thread patterns, but unchanged planform types were also common. Variations in channel width varied substantially among the analysed rivers, which featured narrowing (slightly prevailing) and widening (the least common) as well as many cases of very limited changes. Channel width variations appear statistically, although weakly, related to some morphometric variables; and significant differences emerge comparing glaciated vs. unglaciated basins. Climate-related variations (glacier dynamics and channel disturbance frequency) are argued to be the dominant factors that affected channel variations. © 2017 Elsevier B.V.","Alps; Channel changes; Little Ice Age (LIA); Regional scale analysis; River trajectories",,2-s2.0-85028081539
"Saleti S., Subramanyam R.B.V.","A novel Bit Vector Product algorithm for mining frequent itemsets from large datasets using MapReduce framework",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031396378&doi=10.1007%2fs10586-017-1249-x&partnerID=40&md5=bb4802bf1d94133772a5d3b4ccbdb26f","Frequent itemset mining (FIM) is an interesting sub-area of research in the field of Data Mining. With the increase in the size of datasets, conventional FIM algorithms are not suitable and efforts are made to migrate to the Big Data Frameworks for designing algorithms using MapReduce like computing paradigms. We too interested in designing MapReduce based algorithm. Initially, our Parallel Compression algorithm makes data simpler to handle. A novel bit vector data structure is proposed to maintain compressed transactions and it is formed by scanning the dataset only once. Our Bit Vector Product algorithm follows the MapReduce approach and effectively searches for frequent itemsets from a given list of transactions. The experimental results are present to prove the efficacy of our approach over some of the recent works. © 2017 Springer Science+Business Media, LLC","Big Data; Bit Vector; Compression; Data Mining; Frequent itemset mining; MapReduce framework","Big data; Compaction; Data compression; Vectors; Bit vector; Compression algorithms; Computing paradigm; Data framework; Frequent itemset mining; Large datasets; Mapreduce frameworks; Mining frequent itemsets; Data mining",2-s2.0-85031396378
"Ivica S., Lidija B., Ivana F., Frane Č.K., Tihomir K., Daniela Č.K., Ljiljana P.","Characterization of Croatian Honeys by Right-Angle Fluorescence Spectroscopy and Chemometrics",2017,"Food Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031429598&doi=10.1007%2fs12161-017-1059-z&partnerID=40&md5=c1bfcd9d9900ad0b61bd41dcb404c585","The potential of right-angle fluorescence spectroscopy and selected chemical parameters for discrimination of botanical origin of Croatian honey types (n = 55) previously classified by physicochemical and melissopalynological analyses was evaluated. Systematic step-by-step fluorescence analysis included the measurement of complete excitation-emission matrix (EEM) in the range of excitation wavelengths from 260 to 400 nm, and emission wavelengths from 300 to 600 nm, followed by fluorescence intensity measurement at detected peaks of different excitation/emission wavelengths, and emission spectra recordings at selected specific excitation wavelengths of honey solutions in 50 mmol L−1 phosphate buffer pH 7.0 and methanol. A total of five different sets of emission spectral data for buffer and two for methanolic honey solutions were considered for chemometric analysis of original and normalized emission spectra including principal component analysis and linear discriminant analysis. Additionally, chemical analysis of buffer and methanolic honey solutions included determination of protein, total polyphenol and reactive amino group content. Results showed that right-angle fluorescence spectroscopy of honey solutions has great potential for honey botanical origin discrimination, either by visual comparison of excitation-emission spectra landscapes, or even better, by normalized emission fluorescence spectra recordings at 260 and 280 nm of excitation. Moreover, increased honey discrimination was achieved in conjunction with chemometrics of fluorescence spectras. In addition, two rapid markers/indicators of honey authentication were found. Chestnut honey could be clearly discriminated from the other honey types by simple measurement of fluorescence intensity at 390/470 nm, while sage honeys by measurement of reactive amino group content. © 2017 Springer Science+Business Media, LLC","Characterization; Chemical analysis; Chemometrics; Honey; Reactive amino group content; Right-angle fluorescence","Characterization; Discriminant analysis; Emission spectroscopy; Fluorescence; Fluorescence spectroscopy; Food products; Principal component analysis; Solution mining; Spectrum analysis; Amino group; Chemometrics; Determination of proteins; Excitation emission matrices; Excitation-emission spectra; Fluorescence intensity measurement; Honey; Linear discriminant analysis; Chemical analysis",2-s2.0-85031429598
"Keivanpour S., Ait Kadi D.","Strategic eco-design map of the complex products: toward visualisation of the design for environment",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031409258&doi=10.1080%2f00207543.2017.1388931&partnerID=40&md5=a00ca1c0abfaeea390338e6be7f92298","With growing sustainability and environmental concerns regarding the products, decision-makers and business managers need to integrate sustainability into business strategies and support it via systematic business processes and decision support tools. With the integration of the eco-efficiency attributes to the product development, more data should be analysed in engineering design. This integration also increases the complexity of the design process due to the large volume of data processing and diversity of the different attributes and features of the products. In this paper, we used the stock market metaphor to develop a visual data mining approach to the strategic eco-design assessment of the complex products. We presented a fresh framework using clustering and visualisation techniques to analyse the eco-efficiency profile of the different modules, components and parts of a complex product, and provide an efficient data exploration tool for decision-makers to facilitate processing of eco-design attributes, and strategic objectives at the same time. An illustrative example is provided to show the procedure of the application and the effectiveness of the proposed approach. © 2017 Informa UK Limited, trading as Taylor & Francis Group","clustering; complex products; data mining; strategic decision-making; sustainability; visualisation","Data handling; Data integration; Data mining; Decision making; Decision support systems; Design; Ecodesign; Efficiency; Sustainable development; Visualization; clustering; Complex products; Decision support tools; Design assessments; Environmental concerns; Integrate sustainability; Strategic decision making; Strategic objectives; Product design",2-s2.0-85031409258
"Cho H., Choi W., Lee H.","A method for named entity normalization in biomedical articles: Application to diseases and plants",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031011134&doi=10.1186%2fs12859-017-1857-8&partnerID=40&md5=83f8191d59111ce373a7be6c9b55e45c","Background: In biomedical articles, a named entity recognition (NER) technique that identifies entity names from texts is an important element for extracting biological knowledge from articles. After NER is applied to articles, the next step is to normalize the identified names into standard concepts (i.e., disease names are mapped to the National Library of Medicine's Medical Subject Headings disease terms). In biomedical articles, many entity normalization methods rely on domain-specific dictionaries for resolving synonyms and abbreviations. However, the dictionaries are not comprehensive except for some entities such as genes. In recent years, biomedical articles have accumulated rapidly, and neural network-based algorithms that incorporate a large amount of unlabeled data have shown considerable success in several natural language processing problems. Results: In this study, we propose an approach for normalizing biological entities, such as disease names and plant names, by using word embeddings to represent semantic spaces. For diseases, training data from the National Center for Biotechnology Information (NCBI) disease corpus and unlabeled data from PubMed abstracts were used to construct word representations. For plants, a training corpus that we manually constructed and unlabeled PubMed abstracts were used to represent word vectors. We showed that the proposed approach performed better than the use of only the training corpus or only the unlabeled data and showed that the normalization accuracy was improved by using our model even when the dictionaries were not comprehensive. We obtained F-scores of 0.808 and 0.690 for normalizing the NCBI disease corpus and manually constructed plant corpus, respectively. We further evaluated our approach using a data set in the disease normalization task of the BioCreative V challenge. When only the disease corpus was used as a dictionary, our approach significantly outperformed the best system of the task. Conclusions: The proposed approach shows robust performance for normalizing biological entities. The manually constructed plant corpus and the proposed model are available at http://gcancer.org/plant and http://gcancer.org/normalization , respectively. © 2017 The Author(s).","Disease names; Entity name normalization; Named entity recognition; Neural networks; Plant names; Text mining","Abstracting; Biomedical engineering; Character recognition; Data mining; Linguistics; Natural language processing systems; Neural networks; Semantics; Text processing; Entity name normalization; Medical subject headings; Named entity normalizations; Named entity recognition; National center for biotechnology informations; National library of medicines; Plant names; Text mining; Bioinformatics",2-s2.0-85031011134
"Pipoyan D., Beglaryan M., Costantini L., Molinari R., Merendino N.","Risk assessment of population exposure to toxic trace elements via consumption of vegetables and fruits grown in some mining areas of Armenia",2017,"Human and Ecological Risk Assessment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031493273&doi=10.1080%2f10807039.2017.1381019&partnerID=40&md5=3fad0c9ef3f18847638102567ce902cb","Consumption of unsafe food is one of the most important public health concerns. Trace elements’ contamination caused by direct or indirect activities of mining industries is of importance in this respect. The present study was conducted to assess the chronic dietary exposure and related health risks of trace elements through the intake of selected vegetables and fruits grown under the impact of mining industry in Syunik region (Armenia). Consumption data were obtained via food frequency questionnaire and the concentrations of Cu, Mo, Ni, Cr, Pb, Zn, Hg, and Cd in different fruits and vegetables were determined. Moreover, by combining concentration data with consumption data, estimated daily intake, and target hazard quotient were assessed for each element. The results obtained showed that mean concentrations for Pb and Hg in some vegetables exceeded maximum acceptable levels set by international organizations. Hazard indexes > 1 have been obtained in some cases indicating that for some vegetables (particularly for potato, carrot, maize, onion leaf, grape, bean, beet, sweet pepper, eggplant, and tomato) habitual consumption has a potential to pose adverse health effect to the local population. © 2017 Taylor & Francis Group, LLC","dietary exposure; mining region; trace elements",,2-s2.0-85031493273
"Niu X., Wendt A., Li Z., Agarwal A., Xue L., Gonzales M., Brantley S.L.","Detecting the effects of coal mining, acid rain, and natural gas extraction in Appalachian basin streams in Pennsylvania (USA) through analysis of barium and sulfate concentrations",2017,"Environmental Geochemistry and Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031415820&doi=10.1007%2fs10653-017-0031-6&partnerID=40&md5=2895efdaa5e30fb35c40380ba420e490","To understand how extraction of different energy sources impacts water resources requires assessment of how water chemistry has changed in comparison with the background values of pristine streams. With such understanding, we can develop better water quality standards and ecological interpretations. However, determination of pristine background chemistry is difficult in areas with heavy human impact. To learn to do this, we compiled a master dataset of sulfate and barium concentrations ([SO4], [Ba]) in Pennsylvania (PA, USA) streams from publically available sources. These elements were chosen because they can represent contamination related to oil/gas and coal, respectively. We applied changepoint analysis (i.e., likelihood ratio test) to identify pristine streams, which we defined as streams with a low variability in concentrations as measured over years. From these pristine streams, we estimated the baseline concentrations for major bedrock types in PA. Overall, we found that 48,471 data values are available for [SO4] from 1904 to 2014 and 3243 data for [Ba] from 1963 to 2014. Statewide [SO4] baseline was estimated to be 15.8 ± 9.6 mg/L, but values range from 12.4 to 26.7 mg/L for different bedrock types. The statewide [Ba] baseline is 27.7 ± 10.6 µg/L and values range from 25.8 to 38.7 µg/L. Results show that most increases in [SO4] from the baseline occurred in areas with intensive coal mining activities, confirming previous studies. Sulfate inputs from acid rain were also documented. Slight increases in [Ba] since 2007 and higher [Ba] in areas with higher densities of gas wells when compared to other areas could document impacts from shale gas development, the prevalence of basin brines, or decreases in acid rain and its coupled effects on [Ba] related to barite solubility. The largest impacts on PA stream [Ba] and [SO4] are related to releases from coal mining or burning rather than oil and gas development. © 2017 Springer Science+Business Media B.V.","Historical data; Human impact; Pristine river; Shale gas; Water quality",,2-s2.0-85031415820
"Öge İ.F.","Prediction of cementitious grout take for a mine shaft permeation by adaptive neuro-fuzzy inference system and multiple regression",2017,"Engineering Geology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028056172&doi=10.1016%2fj.enggeo.2017.08.013&partnerID=40&md5=41ec2b83811faa7dd19f9287a684d64a","Cement grouting is a common technique implemented for permeation and ground improvement in civil and mining engineering projects. Basically, it is the injection of cement and water mixture into a fractured rock mass. Due to the presence of water bearing and permeable rock mass, permeation grouting was applied prior to the shaft sinking operation in an underground mine, located in Soma coal basin, Turkey. The Drill-Grout-Drill (DGD) method was used in permeation grouting for a flood prone mine shaft project with a circular pattern covering the proposed shaft opening. Data collection was mainly based on recording borehole data, however, during shaft sinking, field observations were continued to check and validate data, especially the rock mass properties. Widely used classification systems, such as RQD and RMR discontinuity condition rating were selected to define rock mass parameters. The rock mass parameters and the grout take data were pre-processed and cleaned to be used as input for multiple regression modelling and Adaptive Neuro Fuzzy Inference System (ANFIS). Linear, nonlinear, and Box-Cox multiple regression models provided accurate results. ANFIS with subtractive clustering and with manual dictations resulted in improved predictions compared to the regression analysis. Since grouting has great complexity and dependence on numerous variables, particular limitations and omissions had to be defined within the scope of the research. All influential factors could not be interpreted. The methodology and variable conditions are the main novelties of this study and enhance the implementation of the method specifically in the mine project where the study was carried out. © 2017 Elsevier B.V.","ANFIS; Grout take; Grouting; Nonlinear multiple regression; Rock mass","Aquifers; Cements; Coal mines; Concrete construction; Drills; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Grouting; Mining; Mining engineering; Mortar; Regression analysis; Rock mechanics; Rocks; Shaft sinking; Water injection; Adaptive neuro-fuzzy inference system; ANFIS; Discontinuity conditions; Multiple regression model; Multiple regression modelling; Nonlinear multiple regressions; Rock mass; Shaft sinking operations; Mine shafts",2-s2.0-85028056172
"Lin H., Zheng S., Lourenço S.D.N., Jaquin P.","Characterization of coarse soils derived from igneous rocks for rammed earth",2017,"Engineering Geology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027587790&doi=10.1016%2fj.enggeo.2017.08.003&partnerID=40&md5=d106be3b7c390d8f41b650593f99694a","Rammed earth refers to a conventional construction method as well as a material comprising gravel, sand and fine soil. There has been an increasing interest due to it being a sustainable and environmentally friendly material. The strength of rammed earth depends on the material and environment (e.g. temperature and relative humidity). While rammed earth is an established and accepted construction material in some countries (e.g. Australia and New Zealand), its use in South China is mostly related to historical structures of which the UNESCO Tulou buildings in Fujian Province is an example. In addition, South China has a sub-tropical climate with a distinctive rainy, humid and hot season followed by a dry and cool season, and soils are granular due to the underlying igneous geology. With a coarse fraction (sand and gravel) in excess of 80%, their appropriateness as earthen materials is, at first sight, debatable. This paper aims to assess the pertinence of three representative soils from Hong Kong (residual soil, alluvium and completely decomposed granite) for unstabilised rammed earth. Unconfined compressive strength tests and shrinkage measurements at different relative humidity, as well as the organics content are determined. To aid the interpretation of the strength and shrinkage data, a fundamental characterization is conducted, namely of the wettability and particle attributes (size and shape). By comparing the results to established guidelines for earthen construction, the three soils are shown to achieve strengths in the range 0.2–1.4 MPa and shrinkage after equilibration for one week up to 4%, with the residual soil outperforming the others. Moreover, it was found the strength of the soils was influenced by the relative humidity and was strongly dependent on changes of dry density. The wettability results confirmed the higher organics for the alluvium while the more rounded particle shape and greater content of potassium feldspar of the completely decomposed granite corroborated with its lower strength. The results highlight the importance of conducting a fundamental characterization of natural soils for rammed earth via the particle attributes, wettability and mineralogy. © 2017 Elsevier B.V.","Coarse soils; Hong Kong; Particle-scale studies; Rammed earth","Building materials; Compressive strength; Granite; Gravel; Igneous rocks; Minerals; Mining laws and regulations; Shrinkage; Soils; Wetting; Coarse soils; Completely decomposed granite; Conventional constructions; Hong-kong; Particle scale; Rammed earth; Temperature and relative humidity; Unconfined compressive strength; Characterization",2-s2.0-85027587790
"Mao G., Cao R., Chen J.","Analysis on bainite transformation in reheated low-carbon bainite weld metals",2017,"Materials Science and Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019171534&doi=10.1080%2f02670836.2017.1325561&partnerID=40&md5=ff2ec7144aa9d95f95028405d39d20be","A combination of laser scanning confocal microscopy (LSCM) and dilatometry was utilised to simultaneously qualitatively analyse the morphological evolution of bainite by examination of in situ observed micrographs and to quantitatively investigate the amount of bainite transformation by studying dilatometry data. Shorter bainite structures form for smaller prior austenite grain sizes and lower cooling rates, which causes greater bainite transformation to occur in the latter stages of the transformation process. On the other hand, the amount of lath-shape sub-structure increases due to a higher cooling rate. In addition, the surface relief presents greater height and the peak and valley values stay farther away from the horizontal line for specimens at a higher cooling rate. In addition, a lower strain energy per unit volume gives rise to greater bainite transformation compared to that from abundant driving forces. © 2017 Institute of Materials, Minerals and Mining.","bainite transformation; cooling rate; In situ observation; surface relief; weld metals","Austenite; Bainite; Carbon; Cooling; Dilatometers; Metadata; Strain energy; Welds; Bainite transformations; Cooling rates; In-situ observations; Surface reliefs; Weld metal; Bainitic transformations",2-s2.0-85019171534
"Gong X., Zhou H., Chen H.","High-temperature deformation mechanism of γ-TiAl-based alloy with ultrafine grains",2017,"Materials Science and Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019766354&doi=10.1080%2f02670836.2017.1325563&partnerID=40&md5=a9b16867d18ed32cc3abe50640d5fd43","Equiaxed ultrafine duplex alloys γ-TiAl and α2-Ti3Al are prepared by high-energy milling and hot pressing sintering. Microstructures presenting in the mixed and sintered powders are studied by scanning electron microscopy. Structural characteristics are investigated by high-resolution electron microscopy before and after compression at 1100–1200°C. The data reveal grain sizes of 300–500 and ∼100 nm for sintered γ-TiAl and α2-Ti3Al alloys, respectively. High deformation temperatures and low strain rates lead to low peak flow stress, and at such conditions the α2/γ and α2/α2 interfaces are prone to forming dislocation structure. The studies also reveal the presence of large-angle grain boundaries for the and (0002)α2/(0002)α2 interfaces. © 2017 Institute of Materials, Minerals and Mining.","High-energy milling; high-temperature compression; hot pressing sintering; TiAl-based duplex alloy; ultrafine equiaxed duplex alloy","Aluminum; Deformation; Electron microscopy; Grain boundaries; Hot pressing; Mechanical alloying; Milling (machining); Scanning electron microscopy; Sintering; Strain rate; Titanium alloys; Titanium compounds; Deformation temperatures; Dislocation structures; Duplex alloys; High temperature deformation mechanism; High-temperature compression; Hot pressing sintering; Sintered powders; Structural characteristics; High resolution electron microscopy",2-s2.0-85019766354
"Hekmatnejad A., Emery X., Brzovic A., Schachter P., Vallejos J.A.","Spatial modeling of discontinuity intensity from borehole observations at El Teniente mine, Chile",2017,"Engineering Geology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026869322&doi=10.1016%2fj.enggeo.2017.07.012&partnerID=40&md5=cd0acddcf786d062a65c7bf70b055f83","This work addresses the problem of predicting the discontinuity intensity P32 (discontinuity area per unit volume of rock mass) in space and of quantifying the uncertainty in the true P32 values, using information from observed discontinuities intersecting boreholes. This problem is relevant in various fields of engineering, including mining applications, hydrocarbon extraction, groundwater modeling and civil works. The main idea is to calculate experimental P32 values for borehole segments (composites), based on a Terzaghi weighting of the discontinuities that intersect the boreholes. A validation exercise performed on simulated discrete fracture networks demonstrates that the calculated P32 values provide unbiased predictions of the true P32, at both global and local scales, and can therefore be used as experimental data for spatial interpolation purposes. By using geostatistical simulation techniques, the spatial prediction of the P32 and the corresponding measures of uncertainty can be obtained on a block-by-block basis. This methodology is applied to a data set from the El Teniente copper mine, Codelco-Chile. The objective is to map the expected values of the intensity of stockwork veins with a weak infill mineral assemblage and a typical thickness greater than 1 mm, which are referred to as weak veins. Confidence limits on this intensity and its probability of exceeding given critical values are also estimated. The quality of the prediction and of the uncertainty quantification is checked by leave-one-out cross-validation. The resulting confidence limits and probability maps can be used as indicators to define geotechnical domains in the rock mass. © 2017 Elsevier B.V.","Discontinuity area per unit volume; Geostatistical simulation; Spatial interpolation; Spatial uncertainty; Terzaghi correction; Weak veins","Copper mines; Forecasting; Groundwater; Interpolation; Rock mechanics; Rocks; Statistical methods; Geostatistical simulation; Per unit volume; Spatial interpolation; Spatial uncertainty; Weak veins; Boreholes; borehole; computer simulation; discontinuity; geostatistics; interpolation; mine; modeling; rock mass response; spatial analysis; uncertainty analysis; Chile; El Teniente; O'Higgins",2-s2.0-85026869322
"Lewinski N.A., Jimenez I., McInnes B.T.","An annotated corpus with nanomedicine and pharmacokinetic parameters",2017,"International Journal of Nanomedicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032855045&doi=10.2147%2fIJN.S137117&partnerID=40&md5=f0349709a0c46c1d1ed752a443f487f7","A vast amount of data on nanomedicines is being generated and published, and natural language processing (NLP) approaches can automate the extraction of unstructured text-based data. Annotated corpora are a key resource for NLP and information extraction methods which employ machine learning. Although corpora are available for pharmaceuticals, resources for nanomedicines and nanotechnology are still limited. To foster nanotechnology text mining (NanoNLP) efforts, we have constructed a corpus of annotated drug product inserts taken from the US Food and Drug Administration’s Drugs@FDA online database. In this work, we present the development of the Engineered Nanomedicine Database corpus to support the evaluation of nanomedicine entity extraction. The data were manually annotated for 21 entity mentions consisting of nanomedicine physicochemical characterization, exposure, and biologic response information of 41 Food and Drug Administration-approved nanomedicines. We evaluate the reliability of the manual annotations and demonstrate the use of the corpus by evaluating two state-of-the-art named entity extraction systems, OpenNLP and Stanford NER. The annotated corpus is available open source and, based on these results, guidelines and suggestions for future development of additional nanomedicine corpora are provided. © 2017 Lewinski et al.","Corpora; Informatics; Nanotechnology; Natural language processing; Text mining","Article; data mining; drug database; drug exposure; drug labeling; drug response; engineered nanomedicine database; information processing; medical informatics; named entity recognition; nanomedicine; nanotechnology; natural language processing; pharmacokinetic parameters; physical chemistry; reliability",2-s2.0-85032855045
"Lee Y.-J., Park J.-Y.","Identification of future signal based on the quantitative and qualitative text mining: a case study on ethical issues in artificial intelligence",2017,"Quality and Quantity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031427044&doi=10.1007%2fs11135-017-0582-8&partnerID=40&md5=7c832badbcf9ced383f4cad244fe71aa","To foresee the advent of new technologies and their socio-economic impact is a necessity for academia, governments and private enterprises as well. In the future studies, the identification of future signal is one of the renowned techniques for analysis of trends, emerging issue, and gaining future insights. In the Big Data era, recent scholars have proposed using a text mining procedure focusing upon web data such as new social media and academic papers. However, the detection of future signals is still under a developing area of research, and there is much to improve existing methodology as well as developing theoretical foundations. The present study reviews previous literature on identifying emerging issue based on the weak signal detection approach. Then the authors proposed a revised framework that incorporate quantitative and qualitative text mining for assessing the strength of future signals. The authors applied the framework to the case study on the ethical issues of artificial intelligence (hereafter AI). From EBSCO host database, the authors collected text data covering the ethical issues in AI and conducted text mining analysis. Results reveal that emerging ethical issues can be classified as strong signal, weak signal, well-known but not so strong signal, and latent signal. The revised methodology will be able to provide insights for government and business stakeholders by identifying the future signals and their meanings in various fields. © 2017 Springer Science+Business Media B.V.","Artificial intelligence; Data-driven foresight; Ethical issue; Future signal; Text mining",,2-s2.0-85031427044
"Şahin R.","An approach to neutrosophic graph theory with applications",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031418793&doi=10.1007%2fs00500-017-2875-1&partnerID=40&md5=c3dd35a37f00b846bef869e4a2ee36de","Graph theory that can be used to describe the relationships among several individuals has numerous applications in diverse fields such as modern sciences and technology, database theory, data mining, neural networks, expert systems, cluster analysis, control theory, and image capturing. As a generalization of fuzzy set (FS) and intuitionistic fuzzy set (IFS), the concept of neutrosophic set is a more functional tool for handling indeterminate, inconsistent and uncertain information that exist in real life compared to FSs and IFSs. In this paper, we apply the graph theory to the single-valued neutrosophic sets and investigate a new kind of graph structure which is called single-valued neutrosophic graphs and is generalized the results concerning crisp graphs, fuzzy graphs and intuitionistic fuzzy graphs. Then we describe some of their theoretical properties, such as the Cartesian product, composition, union and join. By applying two different procedures to solve single-valued neutrosophic decision-making problems, a neutrosophic graph-based multicriteria decision-making model is developed to consider relationships among the multi-input arguments which cannot be handled well by means of the existing methods. Finally, two illustrative examples are given to demonstrate the applicability, feasibility, effectiveness and advantages of these two proposed approaches. © 2017 Springer-Verlag GmbH Germany","Graph theory; Graph-based multicriteria decision making; Single-valued neutrosophic graph; Single-valued neutrosophic set","Cluster analysis; Control theory; Data mining; Decision making; Decision theory; Expert systems; Fuzzy sets; Fuzzy systems; Graphic methods; Decision-making problem; Intuitionistic fuzzy; Intuitionistic fuzzy sets; Multi criteria decision making; Multi-criteria decision-making models; Neutrosophic sets; Single-valued neutrosophic graph; Uncertain informations; Graph theory",2-s2.0-85031418793
"George S., Ganzha M., Paprzycki M., Fidanova S., Lirkov I.","Building a platform to collect crowdsensing data. Preliminary considerations",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031696442&doi=10.1063%2f1.5007406&partnerID=40&md5=bb87c63eebbef9a058da3d051a465c92","Recent years have seen growing interest in collecting and processing sensor data, in distributed mobile environments. In this context, two, somewhat contradictory, trends have emerged: (1) growing popularity of crowdsourcing-type mechanisms, for (sensor) data collection, and (2) collecting sensed data in data ""silos"", which are not only unavailable to ""outsiders"", but most often incompatible, thus reducing their usability for data mining. Given these limitations in data accessibility, and compatibility, enormous potential for knowledge discovery is lost. To counter this trend, we propose a generic, adaptive, system that will allow voluntary participation in arbitrary crowdsensing initiatives, with the output stored in a standard data format. The system utilizes a rule-based multiagent approach to instructing sensors when to make readings and how to, if necessary, preprocess them, before sharing the data with user-selected initiatives. The initial version of the system has been implemented, and tested in artificial use case scenario. © 2017 Author(s).",,,2-s2.0-85031696442
"Stoimenova M., Voynikova D., Ivanov A., Gocheva-Ilieva S., Iliev I.","Regression trees modeling and forecasting of PM10 air pollution in urban areas",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031682474&doi=10.1063%2f1.5007364&partnerID=40&md5=27a4a7f4184c53f26be9ae339e27a838","Fine particulate matter (PM10) air pollution is a serious problem affecting the health of the population in many Bulgarian cities. As an example, the object of this study is the pollution with PM10 of the town of Pleven, Northern Bulgaria. The measured concentrations of this air pollutant for this city consistently exceeded the permissible limits set by European and national legislation. Based on data for the last 6 years (2011-2016), the analysis shows that this applies both to the daily limit of 50 micrograms per cubic meter and the allowable number of daily concentration exceedances to 35 per year. Also, the average annual concentration of PM10 exceeded the prescribed norm of no more than 40 micrograms per cubic meter. The aim of this work is to build high performance mathematical models for effective prediction and forecasting the level of PM10 pollution. The study was conducted with the powerful flexible data mining technique Classification and Regression Trees (CART). The values of PM10 were fitted with respect to meteorological data such as maximum and minimum air temperature, relative humidity, wind speed and direction and others, as well as with time and autoregressive variables. As a result the obtained CART models demonstrate high predictive ability and fit the actual data with up to 80%. The best models were applied for forecasting the level pollution for 3 to 7 days ahead. An interpretation of the modeling results is presented. © 2017 Author(s).",,,2-s2.0-85031682474
"Zhou X., Hong H., Xing X., Bian K., Xie K., Xu M.","Discovering spatio-temporal dependencies based on time-lag in intelligent transportation data",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013984559&doi=10.1016%2fj.neucom.2016.06.084&partnerID=40&md5=029a5cfadee62fc9f858e8e9f44ca373","Learning spatio-temporal dependency structure is meaningful to characterize causal or statistical relationships. In many real-world applications, dependency structure is often characterized by time-lag between variables. For example, traffic system and climate, time lag is a key feature of hidden temporal dependencies, and plays an essential role in interpreting the cause of discovered temporal dependencies. However, traditional dependencies learning algorithms only use the same time stamp data of variables. In this paper, we propose a method for mining dependencies by considering the time lag. The proposed approach is based on a decomposition of the coefficients into products of two-level hierarchical coefficients, where one represents feature-level and the other represents time-level. Specially, we capture the prior information of time lag in intelligent transportation data. We construct a probabilistic formulation by applying some probabilistic priors to these hierarchical coefficients, and devise an expectation-maximization (EM) algorithm to learn the model parameters. We evaluate our model on both synthetic and real-world highway traffic datasets. Experimental results show the effectiveness of our method. © 2017 Elsevier B.V.","Intelligent transportation data; Spatio-temporal dependency; Time lag","Maximum principle; Dependency structures; Expectation-maximization algorithms; Intelligent transportation; Prior information; Probabilistic formulation; Spatio-temporal dependencies; Statistical relationship; Time lag; Learning algorithms; algorithm; Article; controlled study; data mining; expectation maximization algorithm; learning algorithm; measurement accuracy; measurement precision; priority journal; probability; process optimization; simulation; spatiotemporal dependency; time; time lag; traffic and transport",2-s2.0-85013984559
"Liu M., Cao L., Lu F., Zhen Y.","Multimodal media data understanding and analysis",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006857047&doi=10.1016%2fj.neucom.2016.10.067&partnerID=40&md5=45c7fe301324d2ee07c8e3350e6be5a7",[No abstract available],,"algorithm; automated pattern recognition; breast tumor; classifier; data mining; Editorial; gaze; global positioning system; human; image retrieval; image segmentation; machine learning; multimedia; neuroimaging; priority journal; robotics; sensor; sign language; sleep; support vector machine",2-s2.0-85006857047
"Bu S., Wang L., Han P., Liu Z., Li K.","3D shape recognition and retrieval based on multi-modality deep learning",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014534738&doi=10.1016%2fj.neucom.2016.06.088&partnerID=40&md5=b1f5e1d79de91f8ef46b2f877396ad5c","For 3D shape analysis, an effective and efficient feature is the key to popularize its applications in 3D domain where the major challenge lies in designing an effective high-level feature. The three-dimensional shape contains various useful information including visual information, geometric relationships, and other type properties. Thus the strategy of exploring these characteristics is the core of extracting effective 3D shape features. In this paper, we propose a novel 3D feature learning framework which combines different modality data effectively to promote the discriminability of uni-modal feature by using deep learning. The geometric information and visual information are extracted by Convolutional Neural Networks (CNNs) and Convolutional Deep Belief Networks (CDBNs), respectively, and then two independent Deep Belief Networks (DBNs) are employed to learn high-level features from geometric and visual features. Finally, a Restricted Boltzmann Machine (RBM) is trained for mining the deep correlations between different modalities. Extensive experiments demonstrate that the proposed framework achieves better performance. © 2017 Elsevier B.V.","3D shape; Deep learning; Multi modality; Recognition; Retrieval","Convolution; Deep neural networks; Geometry; Neural networks; 3-D shape; Convolutional neural network; Geometric relationships; Multi modality; Recognition; Restricted boltzmann machine; Retrieval; Three-dimensional shape; Deep learning; Article; artificial neural network; automated pattern recognition; classifier; convolutional deep belief network; convolutional neural network; data extraction; data mining; deep learning; image analysis; image retrieval; machine learning; multimedia; priority journal; restricted Boltzmann machine",2-s2.0-85014534738
"Liu J., Ren H., Wu M., Wang J., Kim H.-J.","Multiple relations extraction among multiple entities in unstructured text",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031124854&doi=10.1007%2fs00500-017-2852-8&partnerID=40&md5=f84852e9eef3defa5bc7355a1015e53a","Relations extraction is a widely researched topic in nature language processing. However, most of the work in the literature concentrate on the methods that are dealing with single relation between two named entities. In the task of multiple relations extraction, traditional statistic-based methods have difficulties in selecting features and improving the performance of extraction model. In this paper, we presented formal definitions of multiple entities and multiple relations and put forward three labeling methods which were used to label entity categories, relation categories and relation conditions. We also proposed a novel relation extraction model which is based on dynamic long short-term memory network. To train our model, entity feature, entity position feature and part of speech feature are used together. These features are used to describe complex relations and improve the performance of relation extraction model. In the experiments, we classified the corpus into three sets which are composed of 0–20 words, 20–35 words and 35+ words sentences. On conll04.corp, the final precision, recall rate and F-measure reached 72.9, 70.8 and 67.9% respectively. © 2017 Springer-Verlag GmbH Germany","Entity; LSTM; Multiple relation; Relation; Relation extraction","Extraction; Long short-term memory; Entity; LSTM; Multiple relation; Relation; Relation extraction; Data mining",2-s2.0-85031124854
"Patnaik S., Temouri Y., Tuffour J., Tarba S., Singh S.K.","Corporate social responsibility and multinational enterprise identity: insights from a mining company's attempt to localise in Ghana",2017,"Social Identities",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031119883&doi=10.1080%2f13504630.2017.1386369&partnerID=40&md5=c2ce000be9d03ffd4879158d59d7cb8c","This paper investigates how a US gold mining multinational enterprise (MNE) – one of the world's largest – operates its subsidiaries in various parts of the world by creating a unique ‘glocal identity’. The US parent company has experienced several significant challenges across its network of subsidiaries. These challenges were mostly linked to the enforcement of the MNE's identity and culture in its host environment. We contribute by describing, in detail, the attempts made by this company to localise its corporate social responsibility practices in Ghana as it sought to gain legitimacy and create an identity that would overcome the issues relating to the liability of foreignness. Our data come from a combination of sources, including questionnaires and detailed semi-structured interviews conducted with the key management employees of the mining company, members and opinion leaders of the company's host communities, and secondary sources. Our main finding is that the construction of a ‘host-friendly’ identity was centred around the mining company's involvement with the Newmont Ahafo Development Foundation. © 2017 Informa UK Limited, trading as Taylor & Francis Group","communities; CSR; Ghana; identity; mining; MNE",,2-s2.0-85031119883
"Madre M.A., Rasekh S., Torres M.A., Diez J.C., Sotelo A.","Improving bulk Ca3Co4O9 thermoelectric materials through Zr doping",2017,"Advances in Applied Ceramics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031119849&doi=10.1080%2f17436753.2017.1386409&partnerID=40&md5=9b127dc3c7ef8879e291e6ccc29163e5","Ca3Co4−xZrxOy polycrystalline ceramics with small Zr substitution have been prepared through the classical solid-state method. X-ray diffraction data have shown that all samples are composed only of Ca3Co4O9 and Ca3Co2O6 phases. Moreover, by increasing Zr substitution up to 0.07, Ca3Co2O6 phase content is decreased. Density measurements have revealed that all samples are very similar, with values around 74% of the theoretical density. Electrical resistivity is decreased in Zr-doped samples, with respect to the pure samples, while Seebeck coefficient is unchanged. Both factors lead to power factor values around 0.33 mW K−2 m−1 at 800°C in 0.07 Zr-doped samples, which are about 65% higher than those obtained for the undoped samples. © 2017 Institute of Materials, Minerals and Mining. Published by Taylor & Francis on behalf of the Institute.","Ceramics; doping; electric properties; microstructure; power factor","Ceramic materials; Doping (additives); Electric power factor; Electric properties; Microstructure; Thermoelectricity; Ceramics; Polycrystalline ceramics; Power factors; Solid state method; Theoretical density; Thermo-Electric materials; X-ray diffraction data; Zr substitution; X ray diffraction",2-s2.0-85031119849
"Ahmed M.","Reservoir-based network traffic stream summarization for anomaly detection",2017,"Pattern Analysis and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030855902&doi=10.1007%2fs10044-017-0659-y&partnerID=40&md5=19c5635518528b8270604b06bb8277e6","Summarization is an important intermediate step for expediting knowledge discovery tasks such as anomaly detection. In the context of anomaly detection from data stream, the summary needs to represent both anomalous and normal data.But streaming data has distinct characteristics, such as one-pass constraint, for which conducting data mining operations are difficult. Existing stream summarization techniques are unable to create summary which represent both normal and anomalous instances. To address this problem, in this paper, a number of hybrid summarization techniques are designed and developed using the concept of reservoir for anomaly detection from network traffic. Experimental results on thirteen benchmark data streams show that the summaries produced from stream using pairwise distance (PSSR) and template matching (TMSSR) techniques can retain more anomalies than existing stream summarization techniques, and anomaly detection technique can identify the anomalies with high true positive and low false positive rate. © 2017 Springer-Verlag London Ltd.","Anomaly detection; Clustering; Reservoir; Stream summary",,2-s2.0-85030855902
"Rodriguez-Esteban R., Jiang X.","Differential gene expression in disease: A comparison between high-throughput studies and the literature",2017,"BMC Medical Genomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031031246&doi=10.1186%2fs12920-017-0293-y&partnerID=40&md5=874491ee9d88e6290f85addb120e1d45","Background: Differential gene expression is important to understand the biological differences between healthy and diseased states. Two common sources of differential gene expression data are microarray studies and the biomedical literature. Methods: With the aid of text mining and gene expression analysis we have examined the comparative properties of these two sources of differential gene expression data. Results: The literature shows a preference for reporting genes associated to higher fold changes in microarray data, rather than genes that are simply significantly differentially expressed. Thus, the resemblance between the literature and microarray data increases when the fold-change threshold for microarray data is increased. Moreover, the literature has a reporting preference for differentially expressed genes that (1) are overexpressed rather than underexpressed; (2) are overexpressed in multiple diseases; and (3) are popular in the biomedical literature at large. Additionally, the degree to which diseases are similar depends on whether microarray data or the literature is used to compare them. Finally, vaguely-qualified reports of differential expression magnitudes in the literature have only small correlation with microarray fold-change data. Conclusions: Reporting biases of differential gene expression in the literature can be affecting our appreciation of disease biology and of the degree of similarity that actually exists between different diseases. © 2017 The Author(s).",,,2-s2.0-85031031246
"Shaheryar A., Khan S., Qaiser H., Khurram A.A., Subhani T.","Mechanical and thermal properties of hybrid carbon fibre–phenolic matrix composites containing graphene nanoplatelets and graphite powder",2017,"Plastics, Rubber and Composites",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031104349&doi=10.1080%2f14658011.2017.1385177&partnerID=40&md5=05763aa23f8c28a25f6c4eb54fb57454","Carbon fibre–phenolic matrix (CF–P) composites containing graphene nanoplatelets (GNPs) were manufactured for improved mechanical and thermal properties. For comparison, micrometer-size pyrolytic graphite powder (GP) was also incorporated in CF–P composites. The loading of carbon fibres was kept constant at 60 wt-% while the quantity of GNPs was varied from 0.1 wt-% to 0.3 wt-% and GP from 1.0 wt-% to 3.0 wt-%. Only GNPs were functionalised by ultraviolet-ozone treatment to improve their dispersion in the matrix while all the composites were manufactured by hand layup method and characterised by scanning electron microscopy, impact, flexural, thermogravimetry and ablation tests. The composite containing 0.3 wt-% GNPs showed considerable improvement in ablation, flexural and impact testing as compared to CF-P composites containing GP. Finally, the ablation mechanisms of post-ablated composites were discussed in the light of available data in the literature. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute.","ablation; Carbon fibre; composite; flexural; GNPs; impact; mechanical; phenolic","Ablation; Carbon fibers; Composite materials; Fibers; Graphene; Graphite; Impact testing; Scanning electron microscopy; Thermodynamic properties; Thermogravimetric analysis; flexural; GNPs; impact; mechanical; phenolic; Carbon phenolic composites",2-s2.0-85031104349
"Tan C., Ji G.","Semisupervised local preserving embedding algorithm based on maximum margin criterion for large-scale data streams",2017,"Concurrency Computation ",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028412176&doi=10.1002%2fcpe.4246&partnerID=40&md5=c75ca5e94af36544cde9870afa258cf3","In the field of machine learning, feature extraction is one of the most important preprocessing in data classification for its effectiveness, and now it has attracted much extensive attention for large-scale data stream preprocessing step, especially in the era of big data. Motivated by the advantages of unsupervised and supervised feature extraction, which are two desirable and promising characteristics for dimension reduction, a new semisupervised local preserving embedding algorithm based on maximum margin criterion (SLPE/MMC) is proposed in this paper. First, the objective functions of maximum margin criterion (MMC) and neighborhood preserving embedding (NPE) are combined to get the first objective function of SLPE/MMC. Then, in order to overcome the out-of-sample problem, a linear transformation is introduced to construct the second objective function. At last, the whole optimal objective function is constructed by combing the two objective functions together. The proposed algorithm has effectively taken advantage of the sample's supervised information and keeps the geometry structure and the class discrimination information of the manifold. Experiments on face datasets Yale, CMU PIE, and AR datasets are performed to evaluate the classification accuracy of SLPE/MMC. The experimental results and time complexity comparisons have demonstrated the effectiveness of the proposed method. Copyright © 2017 John Wiley & Sons, Ltd.","class discrimination information; feature extraction; large-scale data stream; neighborhood geometry relationship","Big data; Classification (of information); Data communication systems; Data reduction; Extraction; Feature extraction; Learning algorithms; Learning systems; Linear transformations; Mathematical transformations; Classification accuracy; Discrimination informations; Embedding algorithms; Geometry relationships; Large scale data; Maximum margin criterions; Neighborhood preserving embedding; Supervised feature extractions; Data mining",2-s2.0-85028412176
"Fan J., Guan C., Ren K., Cui Y., Qiao C.","SPABox: Safeguarding Privacy During Deep Packet Inspection at a MiddleBox",2017,"IEEE/ACM Transactions on Networking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031915563&doi=10.1109%2fTNET.2017.2753044&partnerID=40&md5=657ca55456952f1d23e25b02eec7c3d6","Widely used over the Internet to encrypt traffic, HTTPS provides secure and private data communication between clients and servers. However, to cope with rapidly changing and sophisticated security attacks, network operators often deploy middleboxes to perform deep packet inspection (DPI) to detect attacks and potential security breaches, using techniques ranging from simple keyword matching to more advanced machine learning and data mining analysis. But this creates a problem: how can middleboxes, which employ DPI, work over HTTPS connections with encrypted traffic while preserving privacy? In this paper, we present SPABox, a middlebox-based system that supports both keyword-based and data analysis-based DPI functions over encrypted traffic. SPABox preserves privacy by using a novel protocol with a limited connection setup overhead. We implement SPABox on a standard server and show that SPABox is practical for both long-lived and short-lived connection. Compared with the state-of-the-art Blindbox system, SPABox is more than five orders of magnitude faster and requires seven orders of magnitude less bandwidth for connection setup while SPABox can achieve a higher security level. IEEE","Cryptography; Data privacy; DPI; Malware; middlebox; Middleboxes; Privacy; privacy preserving.; Protocols","Computer crime; Cryptography; Data mining; HTTP; Learning systems; Malware; Network protocols; Deep packet inspection; Deep packet inspection (DPI); Encrypted traffic; Key word matching; Middleboxes; Orders of magnitude; Privacy preserving; Security breaches; Data privacy",2-s2.0-85031915563
"Taboada M., Rodriguez H., Gudivada R.C., Martinez D.","A new synonym-substitution method to enrich the human phenotype ontology",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030859492&doi=10.1186%2fs12859-017-1858-7&partnerID=40&md5=6d12a8e55293895db6f4c76e7830a01d","Background: Named entity recognition is critical for biomedical text mining, where it is not unusual to find entities labeled by a wide range of different terms. Nowadays, ontologies are one of the crucial enabling technologies in bioinformatics, providing resources for improved natural language processing tasks. However, biomedical ontology-based named entity recognition continues to be a major research problem. Results: This paper presents an automated synonym-substitution method to enrich the Human Phenotype Ontology (HPO) with new synonyms. The approach is mainly based on both the lexical properties of the terms and the hierarchical structure of the ontology. By scanning the lexical difference between a term and its descendant terms, the method can learn new names and modifiers in order to generate synonyms for the descendant terms. By searching for the exact phrases in MEDLINE, the method can automatically rule out illogical candidate synonyms. In total, 745 new terms were identified. These terms were indirectly evaluated through the concept annotations on a gold standard corpus and also by document retrieval on a collection of abstracts on hereditary diseases. A moderate improvement in the F-measure performance on the gold standard corpus was observed. Additionally, 6% more abstracts on hereditary diseases were retrieved, and this percentage was 33% higher if only the highly informative concepts were considered. Conclusions: A synonym-substitution procedure that leverages the HPO hierarchical structure works well for a reliable and automatic extension of the terminology. The results show that the generated synonyms have a positive impact on concept recognition, mainly those synonyms corresponding to highly informative HPO terms. © 2017 The Author(s).","Biomedical ontologies; Entity name discovery; Human phenotype ontology; PubMed","Abstracting; Character recognition; Data mining; Gold; Linguistics; Natural language processing systems; Ontology; Text processing; Biomedical ontologies; Biomedical text minings; Enabling technologies; Entity name discovery; Hierarchical structures; Named entity recognition; PubMed; Substitution method; Semantics",2-s2.0-85030859492
"Zheng W., Lin H., Luo L., Zhao Z., Li Z., Zhang Y., Yang Z., Wang J.","An attention-based effective neural model for drug-drug interactions extraction",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030860534&doi=10.1186%2fs12859-017-1855-x&partnerID=40&md5=d226bd1b72c88e4bbc844dbcad215e9e","Background: Drug-drug interactions (DDIs) often bring unexpected side effects. The clinical recognition of DDIs is a crucial issue for both patient safety and healthcare cost control. However, although text-mining-based systems explore various methods to classify DDIs, the classification performance with regard to DDIs in long and complex sentences is still unsatisfactory. Methods: In this study, we propose an effective model that classifies DDIs from the literature by combining an attention mechanism and a recurrent neural network with long short-term memory (LSTM) units. In our approach, first, a candidate-drug-oriented input attention acting on word-embedding vectors automatically learns which words are more influential for a given drug pair. Next, the inputs merging the position- and POS-embedding vectors are passed to a bidirectional LSTM layer whose outputs at the last time step represent the high-level semantic information of the whole sentence. Finally, a softmax layer performs DDI classification. Results: Experimental results from the DDIExtraction 2013 corpus show that our system performs the best with respect to detection and classification (84.0% and 77.3%, respectively) compared with other state-of-the-art methods. In particular, for the Medline-2013 dataset with long and complex sentences, our F-score far exceeds those of top-ranking systems by 12.6%. Conclusions: Our approach effectively improves the performance of DDI classification tasks. Experimental analysis demonstrates that our model performs better with respect to recognizing not only close-range but also long-range patterns among words, especially for long, complex and compound sentences. © 2017 The Author(s).","Attention; Drug-drug interactions; Long short-term memory; Recurrent neural network; Text mining","Brain; Complex networks; Data mining; Long short-term memory; Recurrent neural networks; Semantics; Text processing; Attention; Attention mechanisms; Classification performance; Drug-drug interactions; Experimental analysis; High level semantics; State-of-the-art methods; Text mining; Drug interactions",2-s2.0-85030860534
"Chattopadhyay M., Garg A.K., Mitra S.K.","Herding by Foreign Institutional Investors: An Evidential Exploration for Persistence and Predictability",2017,"Journal of Behavioral Finance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496085&doi=10.1080%2f15427560.2017.1373282&partnerID=40&md5=a86bc324c8acf739561f88736c09b78f","The primary objective of the study is to explore the predictability of herding patterns of foreign institutional investors in the Indian market using high frequency data over a period from January 2003 to June 2014. Herding of an individual stock was measured estimating a simple volume based ratio and persistence of trends was detected using the runs test (Wald and Wolfowitz [1940]) on that ratio. Predictability of herding behavior has been successfully modeled by applying 7 data mining models using various measures of performance. Market regulators may consider our findings to regulate the foreign institutional investors trading to make the financial system more transparent and robust. © 2017 The Institute of Behavioral Finance","Data mining; Foreign institutional investors; Herding; Persistence; Prediction; Runs test",,2-s2.0-85031496085
"Waroquiers D., Gonze X., Rignanese G.-M., Welker-Nieuwoudt C., Rosowski F., Göbel M., Schenk S., Degelmann P., André R., Glaum R., Hautier G.","Statistical analysis of coordination environments in Oxides",2017,"Chemistry of Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031012110&doi=10.1021%2facs.chemmater.7b02766&partnerID=40&md5=700b15a53d4f6ea46b965aaf40cf8704","Coordination or local environments (e.g., tetrahedra and octahedra) are powerful descriptors of the crystalline structure of materials. These structural descriptors are essential to the understanding of crystal chemistry and the design of new materials. However, extensive statistics on the occurrence of local environment are not available even on common chemistries such as oxides. Here, we present the first large-scale statistical analysis of the coordination environments of cations in oxides using a large set of experimentally observed compounds (about 8000). Using a newly developed method, we provide the distribution of local environment for each cation in oxides. We discuss our results highlighting previously known trends and unexpected coordination environments, as well as compounds presenting very rare coordinations. Our work complements the know-how of the solid state chemist with a statistically sound analysis and paves the way for further data mining efforts linking, for instance, coordination environments to materials properties. © 2017 American Chemical Society.",,"Crystal chemistry; Data mining; Positive ions; Statistical methods; Technology transfer; Coordination environment; Crystalline structure; Descriptors; Local environments; Sound analysis; Structural descriptors; Materials properties",2-s2.0-85031012110
"Thimmisetty C.A., Ghanem R.G., White J.A., Chen X.","High-Dimensional Intrinsic Interpolation Using Gaussian Process Regression and Diffusion Maps",2017,"Mathematical Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030847911&doi=10.1007%2fs11004-017-9705-y&partnerID=40&md5=26e7e9ac256f8c3ae9e27ae6741f36ab","This article considers the challenging task of estimating geologic properties of interest using a suite of proxy measurements. The current work recast this task as a manifold learning problem. In this process, this article introduces a novel regression procedure for intrinsic variables constrained onto a manifold embedded in an ambient space. The procedure is meant to sharpen high-dimensional interpolation by inferring non-linear correlations from the data being interpolated. The proposed approach augments manifold learning procedures with a Gaussian process regression. It first identifies, using diffusion maps, a low-dimensional manifold embedded in an ambient high-dimensional space associated with the data. It relies on the diffusion distance associated with this construction to define a distance function with which the data model is equipped. This distance metric function is then used to compute the correlation structure of a Gaussian process that describes the statistical dependence of quantities of interest in the high-dimensional ambient space. The proposed method is applicable to arbitrarily high-dimensional data sets. Here, it is applied to subsurface characterization using a suite of well log measurements. The predictions obtained in original, principal component, and diffusion space are compared using both qualitative and quantitative metrics. Considerable improvement in the prediction of the geological structural properties is observed with the proposed method. © 2017 International Association for Mathematical Geosciences","Diffusion distance; Gaussian process regression; Interpolation on manifold; Intrinsic interpolation; Intrinsic metrics; Kriging","Clustering algorithms; Data mining; Diffusion; Gaussian noise (electronic); Interpolation; Principal component analysis; Regression analysis; Well logging; Diffusion distance; Gaussian process regression; Interpolation on manifolds; Intrinsic metrics; Kriging; Gaussian distribution",2-s2.0-85030847911
"Costa C., Silva V., Bazzurro P.","Assessing the impact of earthquake scenarios in transportation networks: the Portuguese mining factory case study",2017,"Bulletin of Earthquake Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030846996&doi=10.1007%2fs10518-017-0243-2&partnerID=40&md5=2e3a9aad50490a36ac62cb00e54ad22d","This study presents an open-source framework for the evaluation of the consequences of seismic events on transportation systems, and their impact on the surrounding industry. When applied to the specific case of a given company or organization, the framework allows the estimation of expected losses due to the disruption of specific transportation routes. The methodology was applied to a case study of a Portuguese mining factory whose production and exportation rely on the accessibility to strategic regions in the country using the highway and railway networks. Several methodological issues (e.g., spatial correlation in the ground motion, correlation in the damage) are explored within a sensitivity analysis to identify which features can impact seismic performance indicators (collapse and disruption probabilities; repair and disruption time) of specific routes. © 2017 Springer Science+Business Media B.V.","Infrastructure; Open-data; Portugal; Seismic risk; Transportation network","Open systems; Seismic design; Seismology; Sensitivity analysis; Transportation; Infrastructure; Open datum; Portugal; Seismic risk; Transportation network; Transportation routes",2-s2.0-85030846996
"Salawu S., He Y., Lumsden J.","Approaches to Automated Detection of Cyberbullying: A Survey",2017,"IEEE Transactions on Affective Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765182&doi=10.1109%2fTAFFC.2017.2761757&partnerID=40&md5=6dc63ff1c99bd06022b96705b1cbf3d1","Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely; supervised learning, lexicon based, rule based and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Na&#x00EF;ve Bayes to develop predictive models for cyberbullying detection. Lexicon based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rules-based approaches match text to predefined rules to identify bullying and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of quality representative labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field. IEEE","Abuse and crime involving computers; Computers; data mining; Electronic mail; machine learning; natural language processing; Sentiment analysis; sentiment analysis; Social network services; social networking; Supervised learning","Artificial intelligence; Computers; Data mining; Electronic mail; Information services; Learning algorithms; Learning systems; Mobile devices; Natural language processing systems; Social networking (online); Supervised learning; Abuse and crime involving computers; Automated detection; Bibliographic database; Detection approach; Learning-based approach; Literature reviews; Sentiment analysis; Social network services; Computer crime",2-s2.0-85031765182
"Fanaeepour M., Rubinstein B.I.P.","Differentially private counting of users’ spatial regions",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030841627&doi=10.1007%2fs10115-017-1113-6&partnerID=40&md5=0b36c1c880d656abed625280fce84db8","Mining of spatial data is an enabling technology for mobile services, Internet-connected cars and the Internet of Things. But the very distinctiveness of spatial data that drives utility can cost user privacy. Past work has focused upon points and trajectories for differentially private release. In this work, we continue the tradition of privacy-preserving spatial analytics, focusing not on point or path data, but on planar spatial regions. Such data represent the area of a user’s most frequent visitation—such as “around home and nearby shops”. Specifically we consider the differentially private release of data structures that support range queries for counting users’ spatial regions. Counting planar regions leads to unique challenges not faced in existing work. A user’s spatial region that straddles multiple data structure cells can lead to duplicate counting at query time. We provably avoid this pitfall by leveraging the Euler characteristic for the first time with differential privacy. To address the increased sensitivity of range queries to spatial region data, we calibrate privacy-preserving noise using bounded user region size and a constrained inference that uses robust least absolute deviations. Our novel constrained inference reduces noise and promotes covertness by (privately) imposing consistency. We provide a full end-to-end theoretical analysis of both differential privacy and high-probability utility for our approach using concentration bounds. A comprehensive experimental study on several real-world datasets establishes practical validity. © 2017 Springer-Verlag London Ltd.","Differential privacy; Euler histograms; Location privacy; Spatial regions",,2-s2.0-85030841627
"Saha S., Pal M., Konar A.","Triangular membership function based real-time gesture monitoring system for physical disorder detection",2017,"Computing and Visualization in Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030658909&doi=10.1007%2fs00791-017-0281-y&partnerID=40&md5=e18823932c5c63db12bca275f014a0f9","A novel approach to distinguish 25 body gestures enlightening physical disorders in young and elder individuals is explained using the proposed system. Here a well-known human sensing device, Kinect sensor is used which approximates the human body by virtue of 20 body joints and produces a data stream from which skeleton of the human body is traced. Sampling rate of the data stream is 30 frames per second where every frame represents a body gesture. The overall system is bifurcated into two parts. The offline part calculates 19 features from each frame representing a diseased gesture. These features are angle and distance information between 20 body joints. Features correspond to a definite pattern for a specific body gesture. In online part, triangular fuzzy matching based algorithm performs to detect real-time gestures with 90.57% accuracy. For achieving better accuracy, decision tree is enforced to separate sitting and standing body gestures. The proposed approach is observed to outperform several contemporary approaches in terms of accuracy while presenting a simple system which is based on medical knowledge and is capable of distinguishing as large as 25 gestures. © 2017 Springer-Verlag GmbH Germany","Decision tree; Fuzzy membership function; Kinect sensor; Physical disorder; Physiotherapy","Data mining; Decision trees; Physical therapy; Distance information; Frames per seconds; Fuzzy membership function; Gesture monitoring; Kinect sensors; Medical knowledge; Physical disorders; Triangular membership functions; Membership functions",2-s2.0-85030658909
"Lee C.-S., Wang M.-H., Hsiao Y.-C., Tsai B.-H.","Ontology-based GFML agent for patent technology requirement evaluation and recommendation",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030701385&doi=10.1007%2fs00500-017-2859-1&partnerID=40&md5=add4600ea79f42260fc0cfa5462fadd5","Patent technology requirement evaluation and recommendation are critical for patent strategy, patent management, and patent usage in an organization. This paper proposes a patent technology evaluation and recommendation agent based on a soft-computing approach to enhance patent expansibility and technology transfer. First, we investigate the relationship between patent technology and patent owners, such as academic institutes or organizations, and integrate the collected patent data with the characteristics of organizations to establish a popular patent ontology for general academic institutes or organizations. Then, the patent’s suitability for a specific organization is determined based on concepts extracted using Chinese Knowledge Information Processing. Next, we refer to the Japan Patent Office evaluation index and intellectual property quotient to describe the knowledge base and rule base of the patent quality evaluation agent by using fuzzy markup language (FML). A comprehensive patent quality evaluation mechanism is implemented, and the genetic algorithm is adopted to improve the performance of the proposed agent. Additionally, the patent requirement level evaluation mechanism infers the patent requirement level according to the basic information of an organization. Finally, we present a novel FML-based patent requirement recommendation agent to recommend a patent for an organization by considering the suitability of the patent technology for such an organization, the results of the comprehensive patent quality evaluation process, and the results of the evaluation of the demander’s patent requirements. According to the results, the proposed agent is feasible for patent recommendation. In the future, we will combine an intelligent robot with the GFML agent to assist humans or organizations in recommending an appropriate patent. © 2017 Springer-Verlag GmbH Germany","Genetic fuzzy markup language; Intelligent agent; Ontology; Patent evaluation; Patent recommendation","Data mining; Genetic algorithms; Intelligent agents; Intelligent robots; Knowledge based systems; Markup languages; Ontology; Quality control; Societies and institutions; Soft computing; Technology transfer; Academic institutes; Fuzzy markup languages (FML); Genetic fuzzy markup languages; Patent evaluation; Patent recommendation; Recommendation agents; Soft computing approaches; Technology evaluation; Patents and inventions",2-s2.0-85030701385
"Siciliano R., D’Ambrosio A., Aria M., Amodio S.","Analysis of Web Visit Histories, Part II: Predicting Navigation by Nested STUMP Regression Trees",2017,"Journal of Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030720376&doi=10.1007%2fs00357-017-9239-5&partnerID=40&md5=9cbd5243014e8dc6e9ae99586356dac3","This paper constitutes part II of the contribution to the analysis of web visit histories through a new methodological framework for web usage-structure mining considering association rules theory. The aim is to explore through a tree structure the sequence of direct rules (i.e. paths) that characterize a web navigator who keeps standing longer on a web page with respect to the path characterizing navigators who leave the web earlier. A novel tree-based structure is introduced to take into account that the learning sample changes click by click leaving out navigators who drop off from the web after any click. The response variable at each time point is the remaining number of clicks before leaving the web. The split is induced by the predictors that describe the preferred web sections. The methodology introduced results in a Nested Stump Regression Tree that is an hierarchy of stump trees, where a stump is a tree with only one split or, equivalently, with only two terminal nodes. Suitable properties are outlined. As in first part of the contribution to the analysis of the web visit histories, a methodological description is provided by considering a web portal with a fixed set of web sections, i.e. a data set coming from the UCI Machine Learning Repository. © 2017 Classification Society of North America","Recursive partitioning; Sequence rules; Web path; Web Usage-Structure Mining",,2-s2.0-85030720376
"Sridhar N., Thodla R., Gui F., Cao L., Anderko A.","Corrosion-resistant alloy testing and selection for oil and gas production",2017,"Corrosion Engineering Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030838289&doi=10.1080%2f1478422X.2017.1384609&partnerID=40&md5=92f1a0a3de36822b4e818101b16232d2","Corrosion-resistant alloys (CRAs) are employed in severe oil and gas production environments that operate at high pressures and temperatures and contain chlorides, CO2, and H2S. They exhibit high resistance to uniform corrosion in these environments due to their passivity. However, they can suffer from different forms of environmentally assisted cracking (EAC), depending on the environmental and metallurgical conditions. This paper reviews the recent literature of EAC in CRAs and presents an overall framework for evaluating the SCC based on electrochemical modelling of corrosion and repassivation potentials for localised corrosion. The modelling is supported by experimental data on crack growth as a function of environmental variables, alloy content, and potential. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute","CRA; environmentally assisted cracking; hydrogen embrittlement; localised corrosion; SCC; sour service","Corrosion; Corrosion resistance; Cracks; Hydrogen embrittlement; Metal implants; Environmental variables; Environmentally assisted cracking; Localised corrosion; Metallurgical conditions; Oil and gas production; Repassivation potential; Sour services; Uniform corrosion; Corrosion resistant alloys",2-s2.0-85030838289
"Naganna S.R., Deka P.C., Ch S., Hansen W.F.","Factors influencing streambed hydraulic conductivity and their implications on stream–aquifer interaction: a conceptual review",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708945&doi=10.1007%2fs11356-017-0393-4&partnerID=40&md5=43fd81c48501ae6b8166bd38e941e143","The estimation and modeling of streambed hydraulic conductivity (K) is an emerging interest due to its connection to water quality, aquatic habitat, and groundwater recharge. Existing research has found ways to sample and measure K at specific sites and with laboratory tests. The challenge undertaken was to review progress, relevance, complexity in understanding and modeling via statistical and geostatistical approaches, literature gaps, and suggestions toward future needs. This article provides an overview of factors and processes influencing streambed hydraulic conductivity (K) and its role in the stream–aquifer interaction. During our synthesis, we discuss the influence of geological, hydrological, biological, and anthropogenic factors that lead to variability of streambed substrates. Literature examples document findings to specific sites that help to portray the role of streambed K and other interrelated factors in the modeling of hyporheic and groundwater flow systems. However, studies utilizing an integrated, comprehensive database are limited, restricting the ability of broader application and understanding. Examples of in situ and laboratory methods of estimating hydraulic conductivity suggest challenges in acquiring representative samples and comparing results, considering the anisotropy and heterogeneity of fluvial bed materials and geohydrological conditions. Arriving at realistic statistical and spatial inference based on field and lab data collected is challenging, considering the possible sediment sources, processes, and complexity. Recognizing that the K for a given particle size group includes several to many orders of magnitude, modeling of streambed K and groundwater interaction remain conceptual and experimental. Advanced geostatistical techniques offer a wide range of univariate or multi-variate interpolation procedures such as kriging and variogram analysis that can be applied to these complex systems. Research available from various studies has been instrumental in developing sampling options, recognizing the significance of fluvial dynamics, the potential for filtration, transfer, and storage of high-quality groundwater, and importance to aquatic habitat and refuge during extreme conditions. Efforts in the characterization of natural and anthropogenic conditions, substrate materials, sediment loading, colmation, and other details highlight the great complexity and perhaps need for a database to compile relevant data. The effects on streambed hydraulic conductivity due to anthropogenic disturbances (in-stream gravel mining, contaminant release, benthic activity, etc.) are the areas that still need focus. An interdisciplinary (hydro-geo-biological) approach may be necessary to characterize the magnitude and variability of streambed K and fluxes at local, regional scales. © 2017 Springer-Verlag GmbH Germany","Anthropogenic activities; Colmation; Fluvial hydrology; Hydraulic conductivity; Hyporheic zone; Streambed; Stream–aquifer interaction",,2-s2.0-85030708945
"Bendimerad A., Plantevit M., Robardet C.","Mining exceptional closed patterns in attributed graphs",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030691338&doi=10.1007%2fs10115-017-1109-2&partnerID=40&md5=6308b9efa71bf580d2e834ff45e57d7f","Geo-located social media provide a large amount of information describing urban areas based on user descriptions and comments. Such data make possible to identify meaningful city neighborhoods on the basis of the footprints left by a large and diverse population that uses this type of media. In this paper, we present some methods to exhibit the predominant activities and their associated urban areas to automatically describe a whole city. Based on a suitably attributed graph model, our approach identifies neighborhoods with homogeneous and exceptional characteristics. We introduce the novel problem of exceptional subgraph mining in attributed graphs and propose a complete algorithm that takes benefits from closure operators, new upper bounds and pruning properties. We also define an approach to sample the space of closed exceptional subgraphs within a given time budget. Experiments performed on ten real datasets are reported and demonstrated the relevancy of both approaches, and also showed their limits. © 2017 Springer-Verlag London Ltd.","Exceptional subgraph mining; Pattern mining; Urban data analysis",,2-s2.0-85030691338
"Wang Y., Shao L.","Understanding occupancy and user behaviour through Wi-Fi-based indoor positioning",2017,"Building Research and Information",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030529659&doi=10.1080%2f09613218.2018.1378498&partnerID=40&md5=c4e7a3f43d06c1571ed08aba44bdb471","A 30-day monitoring campaign was conducted in a university library building to investigate the usefulness of a novel Wi-Fi-based indoor location system for revealing indoor occupancy patterns and related user behaviour. The system has demonstrated its effectiveness in providing occupancy information with a relatively high degree of granularity and accuracy in this study. The occupancy results revealed that the current 24-hour opening policy for the library during term time did not correlate with usage. On the other hand, the eight-hour library-opening duration during the summer holiday period could be extended to include the early evening hours to benefit user productivity. Four occupancy patterns were identified based on cluster analysis. Most users were found to belong to the short-occupancy one-time visitor type, while a minority were long-occupancy users. The cross-correlations between various occupancy parameters were investigated. For example, the pattern of user arrival times at the library was found to be significantly correlated with their study durations. Further, data analysis showed that the majority of long-occupancy users tended not to have frequent breaks with some taking no break for four hours. This could have implications for their health and wellbeing as well as their productivity. © 2017 Informa UK Limited, trading as Taylor & Francis Group","buildings; data mining; facility management; monitoring; occupancy; occupancy detection; occupancy patterns; time use; user behaviour; Wi-Fi","Buildings; Cluster analysis; Data mining; Indoor positioning systems; Monitoring; Office buildings; Pattern recognition; Productivity; Wi-Fi; Wireless local area networks (WLAN); Facility management; occupancy; Occupancy detections; occupancy patterns; Time use; User behaviour; Behavioral research",2-s2.0-85030529659
"Peng Z., Wang T., Lu W., Huang H., du X., Zhao F., Tung A.K.H.","Mining frequent subgraphs from tremendous amount of small graphs using MapReduce",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030723672&doi=10.1007%2fs10115-017-1104-7&partnerID=40&md5=59df334436dfd43bf9972b59e751d2b1","Frequent subgraph mining from a tremendous amount of small graphs is a primitive operation for many data mining applications. Existing approaches mainly focus on centralized systems and suffer from the scalability issue. Consider the increasing volume of graph data and mining frequent subgraphs is a memory-intensive task, it is difficult to tackle this problem on a centralized machine efficiently. In this paper, we therefore propose an efficient and scalable solution, called MRFSE, using MapReduce. MRFSE adopts the breadth-first search strategy to iteratively extract frequent subgraphs, i.e., all frequent subgraphs with (Formula presented.) edges are generated based on frequent subgraphs with i edges at the ith iteration. In our design, existing frequent subgraph mining techniques in centralized systems can be easily extended and integrated. More importantly, new frequent subgraphs are generated without performing any isomorphism test which is costly and imperative in existing frequent subgraph mining techniques. Besides, various optimization techniques are proposed to further reduce the communication and I/O cost. Extensive experiments conducted on our in-house clusters demonstrate the superiority of our proposed solution in terms of both scalability and efficiency. © 2017 Springer-Verlag London Ltd.","Frequent subgraph mining; Isomorphism-testing-free; MapReduce",,2-s2.0-85030723672
"Gutman M.","Facilitating pre-service teachers to develop Regulation of Cognition with Learning Management System",2017,"Educational Media International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030673486&doi=10.1080%2f09523987.2017.1385164&partnerID=40&md5=6f939c6bdf65c81724e8718e6de8f8f0","The object of the present study is to propose a technologically based method for developing Regulation of Cognition (RC) among pre-service teachers in a pedagogical problem context. The research intervention was carried out by two groups during a Teaching Training Workshop, based on the IMPROVE instructional method, which was implemented in the Learning Management System (LMS). The first group (N = 53) investigated the pedagogical problems with “dual perspectives” (teacher and learner), and the other group (N = 47) analyzed the same problems from a teacher perspective only. The triangulated research design provided three sets of data of RC (e.g., statements on Metacognitive Awareness Inventory, Educational Data Mining, and observations on actual teaching). The results were indicative of the advantage that was obtained by the dual perspective group (LMS+2P), which has manifested in most components of RC, as compared with the single-based intervention (LMS+1P). © 2017 International Council for Educational Media.","help-seeking; LMS; pre-service teacher; Regulation of Cognition",,2-s2.0-85030673486
"Bharara S., Sabitha S., Bansal A.","Application of learning analytics using clustering data Mining for Students’ disposition analysis",2017,"Education and Information Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030527657&doi=10.1007%2fs10639-017-9645-7&partnerID=40&md5=4cec8a3bdf99db7c669e395b22d02621","Learning Analytics (LA) is an emerging field in which sophisticated analytic tools are used to improve learning and education. It draws from, and is closely tied to, a series of other fields of study like business intelligence, web analytics, academic analytics, educational data mining, and action analytics. The main objective of this research work is to find meaningful indicators or metrics in a learning context and to study the inter-relationships between these metrics using the concepts of Learning Analytics and Educational Data Mining, thereby, analyzing the effects of different features on student’s performance using Disposition analysis. In this project, K-means clustering data mining technique is used to obtain clusters which are further mapped to find the important features of a learning context. Relationships between these features are identified to assess the student’s performance. © 2017 Springer Science+Business Media, LLC","Academic analytics; Disposition analytics; Educational data mining; Learning analytics; Learning management systems",,2-s2.0-85030527657
"Liu X., Wang Y., Yan S.","Interferometric SAR Time Series Analysis for Ground Subsidence of the Abandoned Mining Area in North Peixian Using Sentinel-1A TOPS Data",2017,"Journal of the Indian Society of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030724953&doi=10.1007%2fs12524-017-0708-4&partnerID=40&md5=d8274cfe5955f4ebe2adb0ed9a73479c","The North Peixian mining area of China has rich coal resources, with total proven reserves of 2.37 billion tons. However, the underground coal mining activities have resulted in ground collapse, which has caused serious harm to the environment and threatened the lives and properties of local residents. In this study, 12 Sentinel-1A terrain observation by progressive scans (TOPS) mode acquisitions between 30 July 2015 and 13 May 2016 over the abandoned mining area in North Peixian were analyzed using the interferometric synthetic aperture radar (InSAR) time series method to detect the ground subsidence, with the maximum ground subsidence reaching 83 mm/a and an average value of about 12.7 mm/a. The subsidence results derived from the Sentinel-1A TOPS mode dataset were proven to be effective in investigating and monitoring the ground subsidence in the North Peixian mining area. Compared to the rapid deformation during the ongoing period of mining excavation, the ground subsides slowly in abandoned mining areas and shows a linear relationship with time over a relatively long period of time. Spatial correlation between the subsidence distribution and land cover was found, in that the magnitude of the subsidence in urban areas was smaller than that in rural areas, which is associated with the controlled coal mining activities under buildings, railways, and water bodies. The results demonstrate that Sentinel-1A TOPS SAR images can be used to effectively and accurately detect and monitor ground subsidence in a mining area, which is critically important when investigating land subsidence in a large-scale mining area. © 2017 Indian Society of Remote Sensing","Abandoned mining area; Sentinel-1A; Subsidence; TOPS",,2-s2.0-85030724953
"Yang Y., Pouyanfar S., Tian H., Chen M., Chen S., Shyu M.","IF-MCA: Importance Factor-based Multiple Correspondence Analysis for Multimedia Data Analytics",2017,"IEEE Transactions on Multimedia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031808064&doi=10.1109%2fTMM.2017.2760623&partnerID=40&md5=3cc014e3fdc4cf120235bcefa1cdb88f","Multimedia concept detection is a challenging topic due to the well-known class imbalance issue where the data instances are distributed unevenly across different classes. This problem becomes even more prominent when the minority class that contains an extremely small proportion of the data actually represents the concept of interest as occurred in many real-world applications such as frauds in banking transactions and goal events in soccer videos. Traditional data mining approaches often have difficulty handling largely skewed data distributions. To address this issue, in this paper, an Importance Factor based Multiple Correspondence Analysis (IF-MAC) framework is proposed to deal with the imbalanced datasets. Specifically, a Hierarchical Information Gain Analysis (HIGA) method, which is inspired by the decision tree algorithm, is presented for critical feature selection and Importance Factor (IF) assignment. Then the derived IF is incorporated with the Multiple Correspondence Analysis (MCA) algorithm for effective concept detection and retrieval. The comparison results in video concept detection using the disaster dataset and the soccer dataset demonstrate the effectiveness of the proposed framework. IEEE","Algorithm design and analysis; Data mining; Decision trees; Feature extraction; feature selection; Importance factor; information gain; Multimedia communication; multiple correspondence analysis (MCA); Testing; Training","Data handling; Decision trees; Factor analysis; Feature extraction; Multimedia systems; Personnel training; Sports; Testing; Trees (mathematics); Algorithm design and analysis; Importance factors; Information gain; Multi-media communications; Multiple correspondence analysis; Data mining",2-s2.0-85031808064
"Shimomura Y., Nemoto Y., Ishii T., Nakamura T.","A method for identifying customer orientations and requirements for product–service systems design",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030694437&doi=10.1080%2f00207543.2017.1384581&partnerID=40&md5=9afc171caf232be1b322067b36c12f28","For manufacturers, developing product–service systems (PSSs) is getting more important because of the trends of servitisation and creating social value. A PSS is a social system where multiple actors mutually provide products and services. A PSS design, therefore, must take into account various actors as customers. However, existing methods provide an insufficient solution as to how various customers should be handled in an analysis to identify and accommodate various customer preferences and requirements. To tackle this issue, this article proposes a new method of identifying customers’ orientations and requirements for PSS design. The proposed method employs a combination of topic analysis, persona and scenario approaches. The effectiveness of the method is demonstrated with its application to an urban development case. Through the demonstration, its practical benefits are concluded as follows: consistent and logical results of requirement analysis and insights into a new market for manufacturers. © 2017 Informa UK Limited, trading as Taylor & Francis Group","big data; clustering; customer preference; data mining; design for service; machine learning; product–service systems","Big data; Data mining; Learning systems; Manufacture; Sales; Urban growth; clustering; Customer orientation; Customer preferences; Design for services; Developing product; Products and services; Requirement analysis; Service systems; Product design",2-s2.0-85030694437
"Yu X., Tang L., Wu X., Lu H.","Nondestructive Freshness Discriminating of Shrimp Using Visible/Near-Infrared Hyperspectral Imaging Technique and Deep Learning Algorithm",2017,"Food Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030691928&doi=10.1007%2fs12161-017-1050-8&partnerID=40&md5=9a4195eb1ac7c65d89af7d93b4ae00a5","In this study, visible and near-infrared hyperspectral imaging (HSI) technique combined with deep learning algorithm was investigated for discriminating the freshness of shrimp during cold storage. Shrimps were labeled into two freshness grades (fresh and stale) according to their total volatile basic nitrogen contents. Spectral features were extracted from the HSI data by stacked auto-encoders (SAEs)-based deep learning algorithm and then used to classify the freshness grade of shrimp by a logistic regression (LR)-based deep learning algorithm. The results demonstrated that the SAEs–LR achieved satisfactory total classification accuracy of 96.55 and 93.97% for freshness grade of shrimp in calibration (116 samples) and prediction (116 samples) sets, respectively. An image processing algorithm was also developed for visualizing the classification map of freshness grade. Results confirmed the possibility of rapid and nondestructive detecting freshness grade of shrimp by the combination of hyperspectral imaging technique and deep learning algorithm. The SAEs–LR method adds a new tool for the multivariate analysis of hyperspectral image for shrimp quality inspections. © 2017 Springer Science+Business Media, LLC","Cold storage; Detection; Freshness; Hyperspectral imaging; Logistic regression; Stacked auto-encoders","Calibration; Cold storage; Data mining; Deep learning; Digital storage; Error detection; Hyperspectral imaging; Image processing; Imaging techniques; Infrared devices; Learning systems; Multivariant analysis; Regression analysis; Shellfish; Signal encoding; Spectroscopy; Auto encoders; Classification accuracy; Freshness; Image processing algorithm; Logistic regressions; Multi variate analysis; Total volatile basic nitrogens; Visible and near infrared; Learning algorithms",2-s2.0-85030691928
"Li C.-T., Chen H.-Y., Chen R.-H., Hsieh H.-P.","On route planning by inferring visiting time, modeling user preferences, and mining representative trip patterns",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541072&doi=10.1007%2fs10115-017-1106-5&partnerID=40&md5=59bcd143aa4e0f9430f9a093469597a7","Location-based services allow users to perform check-in actions, which record the geo-spatial activities and provide a plentiful source to do more accurate and useful geographical recommendation. In this paper, we present a novel PreferredTime-aware Route Planning (PTRP) problem, which aims to recommend routes whose locations are not only representative but also need to satisfy users’ preference. The central idea is that the goodness of visiting locations along a route is significantly affected by the visiting time and user preference, and each location has its own proper visiting time due to its category and population. We develop a four-stage preference-based time-aware route planning framework. First, since there is usually either noise time on existing locations or no visiting information on new locations, we devise an inference method, LocTimeInf, to predict the location visiting time on routes. Second, considering the geographical, social, and temporal information of users, we propose the GST-Clus method to group users with similar location visiting preferences. Third, we find the representative and popular time-aware location-transition behaviors by proposing Time-aware Transit Pattern Mining (TTPM) algorithm. Finally, based on the mined time-aware transit patterns, we develop a Preferred Route Search (PR-Search) algorithm to construct the final time-aware routes. Experiments on Gowalla and Foursquare check-in data exhibit the promising effectiveness and efficiency of the proposed methods, comparing to a series of competitors. © 2017 Springer-Verlag London Ltd.","Check-in data; Preferred routes; Route planning; Transit patterns; Visiting time",,2-s2.0-85030541072
"Sug H.","Using Machine Learning Methods Jointly to Find Better Set of Rules in Data Mining",2017,"MATEC Web of Conferences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032873302&doi=10.1051%2fmatecconf%2f201712504017&partnerID=40&md5=86aaefc0277034517aa0d71db1227be2","Rough set-based data mining algorithms are one of widely accepted machine learning technologies because of their strong mathematical background and capability of finding optimal rules based on given data sets only without room for prejudiced views to be inserted on the data. But, because the algorithms find rules very precisely, we may confront with the overfitting problem. On the other hand, association rule algorithms find rules of association, where the association resides between sets of items in database. The algorithms find itemsets that occur more than given minimum support, so that they can find the itemsets practically in reasonable time even for very large databases by supplying the minimum support appropriately. In order to overcome the problem of the overfitting problem in rough set-based algorithms, first we find large itemsets, after that we select attributes that cover the large itemsets. By using the selected attributes only, we may find better set of rules based on rough set theory. Results from experiments support our suggested method. © The Authors, published by EDP Sciences, 2017.",,"Artificial intelligence; Computer circuits; Data mining; Learning systems; Optical variables measurement; Association rule algorithm; Data mining algorithm; Machine learning methods; Machine learning technology; Minimum support; Over fitting problem; Rough-set based; Very large database; Rough set theory",2-s2.0-85032873302
"Kaplan E., Gursoy M.E., Nergiz M.E., Saygin Y.","Known Sample Attacks on Relation Preserving Data Transformations",2017,"IEEE Transactions on Dependable and Secure Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030758682&doi=10.1109%2fTDSC.2017.2759732&partnerID=40&md5=650ba4bf9bf5df30fc7473b19b987732","Many data mining applications, such as clustering and k-NN search, rely on distances and relations in the data. Thus,distance preserving transformations, which perturb the data but retain records&#x0027; distances, have emerged as prominent privacy protection methods. In this paper, we present a novel attack on a generalized form of distance preserving transformations, called relation preserving transformations. Our attack exploits not the exact distances between data, but the relationships between the distances. We show that an attacker with very few known samples (4-10) and direct access to relations can retrieve unknown data points with 95% success. Simple additions of noise to relations and/or distances are not sufficient to prevent the attack, as they decrease success rate by only 10%. IEEE","Data mining; data transformations; known sample attacks; protection; security and privacy","Data mining; Nearest neighbor search; Data mining applications; Data points; Data transformation; k-NN search; known sample attacks; Privacy protection; protection; Security and privacy; Data privacy",2-s2.0-85030758682
"Arriagada-Benítez M., Sepúlveda M., Munoz-Gama J., Buijs J.C.A.M.","Strategies to automatically derive a process model from a configurable process model based on event data",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030638119&doi=10.3390%2fapp7101023&partnerID=40&md5=5b70f852e661d37b0e393661555be33f","Configurable process models are frequently used to represent business workflows and other discrete event systems among different branches of large organizations: they unify commonalities shared by all branches and describe their differences, at the same time. The configuration of such models is usually done manually, which is challenging. On the one hand, when the number of configurable nodes in the configurable process model grows, the size of the search space increases exponentially. On the other hand, the person performing the configuration may lack the holistic perspective to make the right choice for all configurable nodes at the same time, since choices influence each other. Nowadays, information systems that support the execution of business processes create event data reflecting how processes are performed. In this article, we propose three strategies (based on exhaustive search, genetic algorithms and a greedy heuristic) that use event data to automatically derive a process model from a configurable process model that better represents the characteristics of the process in a specific branch. These strategies have been implemented in our proposed framework and tested in both business-like event logs as recorded in a higher educational enterprise resource planning system and a real case scenario involving a set of Dutch municipalities. © 2017 by the authors.","Business processes; Business workflows; Configurable process models; Configurable process trees; Discrete event systems; Event logs; Process mining",,2-s2.0-85030638119
"Parolo S., Lacroix S., Kaput J., Scott-Boyer M.-P.","Ancestors’ dietary patterns and environments could drive positive selection in genes involved in micronutrient metabolism—the case of cofactor transporters",2017,"Genes and Nutrition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030470991&doi=10.1186%2fs12263-017-0579-x&partnerID=40&md5=e272155eefc4c854920e720708baea11","Background: During evolution, humans colonized different ecological niches and adopted a variety of subsistence strategies that gave rise to diverse selective pressures acting across the genome. Environmentally induced selection of vitamin, mineral, or other cofactor transporters could influence micronutrient-requiring molecular reactions and contribute to inter-individual variability in response to foods and nutritional interventions. Methods: A comprehensive list of genes coding for transporters of cofactors or their precursors was built using data mining procedures from the HGDP dataset and then explored to detect evidence of positive genetic selection. This dataset was chosen since it comprises several genetically diverse worldwide populations whom ancestries have evolved in different environments and thus lived following various nutritional habits and lifestyles. Results: We identified 312 cofactor transporter (CT) genes involved in between-cell or sub-cellular compartment distribution of 28 cofactors derived from dietary intake. Twenty-four SNPs distributed across 14 CT genes separated populations into continental and intra-continental groups such as African hunter-gatherers and farmers, and between Native American sub-populations. Notably, four SNPs were located in SLC24A3 with one being a known eQTL of the NCKX3 protein. Conclusions: These findings could support the importance of considering individual’s genetic makeup along with their metabolic profile when tailoring personalized dietary interventions for optimizing health. © 2017, The Author(s).","Ancestry; Biological response; Cofactor transport; Dietary habits; Inter-individual variability; Positive selection","trace element; allele; Article; dietary intake; environmental factor; gene identification; gene linkage disequilibrium; gene location; genetic code; genetic database; genetic selection; genetic variability; haplotype; human; intracellular space; last common ancestor; nutritional status; population genetics; population research; single nucleotide polymorphism",2-s2.0-85030470991
"El Sayed A.R., El Chakik A., Alabboud H., Yassine A.","3D face detection based on salient features extraction and skin colour detection using data mining",2017,"Imaging Science Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028655269&doi=10.1080%2f13682199.2017.1358528&partnerID=40&md5=e5e603adbb5733b9bb5cace2dfe18ffc","Face detection has an essential role in many applications. In this paper, we propose an efficient and robust method for face detection on a 3D point cloud represented by a weighted graph. This method classifies graph vertices as skin and non-skin regions based on a data mining predictive model. Then, the saliency degree of vertices is computed to identify the possible candidate face features. Finally, the matching between non-skin regions representing eyes, mouth and eyebrows and salient regions is done by detecting collisions between polytopes, representing these two regions. This method extracts faces from situations where pose variation and change of expressions can be found. The robustness is showed through different experimental results. Moreover, we study the stability of our method according to noise. Furthermore, we show that our method deals with 2D images. © 2017 The Royal Photographic Society.","3D facial point clouds detection; 3D object processing; 3D point clouds saliency; 3D point clouds segmentation; classification; points clouds","Classification (of information); Data mining; 3D object; 3D point cloud; Point cloud; Predictive modeling; Robust methods; Salient features; Salient regions; Weighted graph; Face recognition",2-s2.0-85028655269
"Alawi S.J.S., Shaharanee I.N.M., Jamil J.M.","Profiling Oman education data using data mining approach",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031304950&doi=10.1063%2f1.5005467&partnerID=40&md5=ab93ce487a044f42c29fac9a7d52c944","Nowadays, with a large amount of data generated by many application services in different learning fields has led to the new challenges in education field. Education portal is an important system that leads to a better development of education field. This research paper presents an innovative data mining techniques to understand and summarizes the information of Oman's education data generated from the Ministry of Education Oman ""Educational Portal"". This research embarks into performing student profiling of the Oman student database. This study utilized the k-means clustering technique to determine the students' profiles. An amount of 42484-student records from Sultanate of Oman has been extracted for this study. The findings of this study show the practicality of clustering technique to investigating student's profiles. Allowing for a better understanding of student's behavior and their academic performance. Oman Education Portal contain a large amounts of user activity and interaction data. Analyses of this large data can be meaningful for educator to improve the student performance level and recognize students who needed additional attention. © 2017 Author(s).",,,2-s2.0-85031304950
"Rahman N.A.A., Tan K.L., Lim C.K.","Predictive analysis and data mining among the employment of fresh graduate students in HEI",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031286586&doi=10.1063%2f1.5005340&partnerID=40&md5=530487858b9e11176883d4a277666263","Management of higher education have a problem in producing 100% of graduates who can meet the needs of industry while industry is also facing the problem of finding skilled graduates who suit their needs partly due to the lack of an effective method in assessing problem solving skills as well as weaknesses in the assessment of problem-solving skills. The purpose of this paper is to propose a suitable classification model that can be used in making prediction and assessment of the attributes of the student's dataset to meet the selection criteria of work demanded by the industry of the graduates in the academic field. Supervised and unsupervised Machine Learning Algorithms were used in this research where; K-Nearest Neighbor, Naïve Bayes, Decision Tree, Neural Network, Logistic Regression and Support Vector Machine. The proposed model will help the university management to make a better long-term plans for producing graduates who are skilled, knowledgeable and fulfill the industry needs as well. © 2017 Author(s).",,,2-s2.0-85031286586
"Bu F.","A High-order Clustering Algorithm Based on Dropout Deep Learning for Heterogeneous Data in Cyber-Physical-Social Systems",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773719&doi=10.1109%2fACCESS.2017.2759509&partnerID=40&md5=6a7261ae0841511a868d5c3733d996a2","An explosive growth of cyber-physical-social systems has been witnessed owing to the wide use of various mobile devices recently. A large volume of heterogeneous data has been collected from cyber-physical-social systems in the past few years. Each object in the heterogeneous dataset is typically multi-modal, posing a remarkable challenge on heterogeneous data clustering. In this paper, we propose a high-order k-means algorithm based on the dropout deep learning model for clustering heterogeneous objects in cyber-physical-social systems.We first build three dropout stacked auto-encoders, each with three hidden layers to learn the features for the different modalities of each object. Furthermore, we establish a feature tensor for each object by using the vector outer product to fuse the learned features. At last, we devise a tensor k-means algorithm to cluster the heterogeneous objects based on the tensor distance. We evaluate the proposed high-order k-means algorithm on two representative heterogeneous datasets and results imply that the proposed high-order k-means algorithm can achieve more accurate clustering results than other heterogeneous data clustering methods. OAPA","Algorithm design and analysis; Clustering algorithms; Computer architecture; Cyber-physical-social systems; Data mining; Dropout deep learning model; Feature extraction; Heterogeneous data; High-order clustering; Machine learning; Tensile stress","Cluster analysis; Computer architecture; Cyber Physical System; Data mining; Deep learning; Feature extraction; Learning algorithms; Learning systems; Tensile stress; Tensors; Algorithm design and analysis; Heterogeneous data; High-order; Learning models; Social systems; Clustering algorithms",2-s2.0-85030773719
"Pohjankukka J., Pahikkala T., Nevalainen P., Heikkonen J.","Estimating the prediction performance of spatial models via spatial k-fold cross validation",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022063285&doi=10.1080%2f13658816.2017.1346255&partnerID=40&md5=c26f5985728bb9e4697d3edd5109c19f","In machine learning, one often assumes the data are independent when evaluating model performance. However, this rarely holds in practice. Geographic information datasets are an example where the data points have stronger dependencies among each other the closer they are geographically. This phenomenon known as spatial autocorrelation (SAC) causes the standard cross validation (CV) methods to produce optimistically biased prediction performance estimates for spatial models, which can result in increased costs and accidents in practical applications. To overcome this problem, we propose a modified version of the CV method called spatial k-fold cross validation (SKCV), which provides a useful estimate for model prediction performance without optimistic bias due to SAC. We test SKCV with three real-world cases involving open natural data showing that the estimates produced by the ordinary CV are up to 40% more optimistic than those of SKCV. Both regression and classification cases are considered in our experiments. In addition, we will show how the SKCV method can be applied as a criterion for selecting data sampling density for new research area. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","geographic information science; geographic information systems; spatial data mining; Spatio-temporal data modelling","data mining; GIS; modeling; spatial data",2-s2.0-85022063285
"Wang L., Bao X., Zhou L.","Redundancy Reduction for Prevalent Co-location Patterns",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030789831&doi=10.1109%2fTKDE.2017.2759110&partnerID=40&md5=8d535147e9dfcd4dd0492cc94d058878","Spatial co-location pattern mining is an interesting and important task in spatial data mining which discovers the subsets of spatial features frequently observed together in nearby geographic space. However, the traditional framework of mining prevalent co-location patterns produces numerous redundant co-location patterns, which makes it hard for users to understand or apply. To address this issue, in this paper we study the problem of reducing redundancy in a collection of prevalent co-location patterns by utilizing the spatial distribution information of co-location instances. We first introduce the concept of semantic distance between a co-location pattern and its super-patterns, and then define redundant co-locations by introducing the concept of <formula><tex>$\delta$</tex></formula>-covered, where <formula><tex>$\delta(0\leq \delta\leq 1)$</tex></formula> is a coverage measure. We develop two algorithms RRclosed and RRnull to perform the redundancy reduction for prevalent co-location patterns. The former adopts the post-mining framework that is commonly used by existing redundancy reduction techniques, while the latter employs the mine-and-reduce framework that pushes redundancy reduction into the co-location mining process. Our performance studies on the synthetic and real-world data sets demonstrate that our method effectively reduces the size of the original collection of closed co-location patterns by about 50%. Furthermore, the RRnull method runs much faster than the related closed co-location pattern mining algorithm. IEEE","d-covered; Data mining; Indexes; Measurement; Redundancy; redundancy; semantic distance; Semantics; Spatial co-location pattern; Spatial databases","Data mining; Measurements; Redundancy; Semantics; Co-location patterns; Indexes; Performance study; Redundancy reductions; Semantic distance; Spatial co-location patterns; Spatial data mining; Spatial database; Location",2-s2.0-85030789831
"Gan W., Lin J.C.-W., Fournier-Viger P., Chao H.-C., Zhan J., Zhang J.","Exploiting highly qualified pattern with frequency and weight occupancy",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030318855&doi=10.1007%2fs10115-017-1103-8&partnerID=40&md5=f46d3e992dba2f17bce8528d6192959e","By identifying useful knowledge embedded in the behavior of search engines, users can provide valuable information for web searching and data mining. Numerous algorithms have been proposed to find the desired interesting patterns, i.e., frequent pattern, in real-world applications. Most of those studies use frequency to measure the interestingness of patterns. However, each object may have different importance in these real-world applications, and the frequent ones do not usually contain a large portion of the desired patterns. In this paper, we present a novel method, called exploiting highly qualified patterns with frequency and weight occupancy (QFWO), to suggest the possible highly qualified patterns that utilize the idea of co-occurrence and weight occupancy. By considering item weight, weight occupancy and the frequency of patterns, in this paper, we designed a new highly qualified patterns. A novel Set-enumeration tree called the frequency-weight (FW)-tree and two compact data structures named weight-list and FW-table are designed to hold the global downward closure property and partial downward closure property of quality and weight occupancy to further prune the search space. The proposed method can exploit high qualified patterns in a recursive manner without candidate generation. Extensive experiments were conducted both on real-world and synthetic datasets to evaluate the effectiveness and efficiency of the proposed algorithm. Results demonstrate that the obtained patterns are reasonable and acceptable. Moreover, the designed QFWO with several pruning strategies is quite efficient in terms of runtime and search space. © 2017 Springer-Verlag London Ltd.","Data mining; Frequency; Highly qualified; Weight occupancy; Weight-list",,2-s2.0-85030318855
"Muñoz G., Espinoza D., Goycoolea M., Moreno E., Queyranne M., Letelier O.R.","A study of the Bienstock–Zuckerberg algorithm: applications in mining and resource constrained project scheduling",2017,"Computational Optimization and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030317713&doi=10.1007%2fs10589-017-9946-1&partnerID=40&md5=8cb5110f165084f4bcd6682212e2dd7e","We study a Lagrangian decomposition algorithm recently proposed by Dan Bienstock and Mark Zuckerberg for solving the LP relaxation of a class of open pit mine project scheduling problems. In this study we show that the Bienstock–Zuckerberg (BZ) algorithm can be used to solve LP relaxations corresponding to a much broader class of scheduling problems, including the well-known Resource Constrained Project Scheduling Problem (RCPSP), and multi-modal variants of the RCPSP that consider batch processing of jobs. We present a new, intuitive proof of correctness for the BZ algorithm that works by casting the BZ algorithm as a column generation algorithm. This analysis allows us to draw parallels with the well-known Dantzig–Wolfe decomposition (DW) algorithm. We discuss practical computational techniques for speeding up the performance of the BZ and DW algorithms on project scheduling problems. Finally, we present computational experiments independently testing the effectiveness of the BZ and DW algorithms on different sets of publicly available test instances. Our computational experiments confirm that the BZ algorithm significantly outperforms the DW algorithm for the problems considered. Our computational experiments also show that the proposed speed-up techniques can have a significant impact on the solve time. We provide some insights on what might be explaining this significant difference in performance. © 2017 Springer Science+Business Media, LLC","Column generation; Dantzig–Wolfe; Optimization; RCPSP","Batch data processing; Linear programming; Open pit mining; Optimization; Scheduling; Column generation; Computational experiment; Computational technique; Lagrangian decomposition; Project scheduling problem; RCPSP; Resource constrained project scheduling; Resource-constrained project scheduling problem; Scheduling algorithms",2-s2.0-85030317713
"Jacomin A.-C., Samavedam S., Charles H., Nezis I.P.","iLIR@viral: A web resource for LIR motif-containing proteins in viruses",2017,"Autophagy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029906761&doi=10.1080%2f15548627.2017.1356978&partnerID=40&md5=928860004430bee4d4b7ee8860e77a7d","Macroautophagy/autophagy has been shown to mediate the selective lysosomal degradation of pathogenic bacteria and viruses (xenophagy), and to contribute to the activation of innate and adaptative immune responses. Autophagy can serve as an antiviral defense mechanism but also as a proviral process during infection. Atg8-family proteins play a central role in the autophagy process due to their ability to interact with components of the autophagy machinery as well as selective autophagy receptors and adaptor proteins. Such interactions are usually mediated through LC3-interacting region (LIR) motifs. So far, only one viral protein has been experimentally shown to have a functional LIR motif, leaving open a vast field for investigation. Here, we have developed the iLIR@viral database (http://ilir.uk/virus/) as a freely accessible web resource listing all the putative canonical LIR motifs identified in viral proteins. Additionally, we used a curated text-mining analysis of the literature to identify novel putative LIR motif-containing proteins (LIRCPs) in viruses. We anticipate that iLIR@viral will assist with elucidating the full complement of LIRCPs in viruses. © 2017 The Author(s). Published with license by Taylor & Francis © 2017, © Anne-Claire Jacomin, Siva Samavedam, Hannah Charles, and Ioannis P. Nezis.","AIM; Atg8; database; LC3-interacting region motif; LIR; LIR-containing protein; LIRCP; LRS; prediction; virus","2BC protein; autophagy related protein 8 family; ICP34.5 protein; LC3 interacting region motif containing protein; LC3 protein; membrane protein; Nef protein; nonstructural protein 1; protein 3A; protein M2; TRS1 protein; unclassified drug; viral infectivity factor protein; viral protein; Article; autophagosome; autophagy; bioinformatics; cellular distribution; computer model; data mining; Dengue virus 2; Dengue virus 3; factual database; Herpesviridae; Human cytomegalovirus; Human immunodeficiency virus 1; Influenza A virus; lysosome; nonhuman; protein analysis; protein protein interaction; protein structure; virus; virus inhibition; web browser; wxxl motif; xlir motif; Zika virus",2-s2.0-85029906761
"Uddin M.A., Joolee J.B., Alam A., Lee Y.","Human Action Recognition using Adaptive Local Motion Descriptor in Spark",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030780957&doi=10.1109%2fACCESS.2017.2759225&partnerID=40&md5=92402bd368e2097ba4b73aae84518cd2","Human action recognition plays a significant part in the computer vision and multimedia research society due to its numerous applications. However, despite different approaches proposed to address this problem, some issues regarding the robustness and efficiency of the action recognition still need to be solved. Moreover, due to the speedy development of multimedia applications from numerous origins, e.g. CCTV or video surveillance, there is an increasing demand for parallel processing of the large-scale video data. In this paper, we introduce a novel approach to recognize the human actions. Firstly, we explore Apache Spark with in-memory computing, to resolve the task of human action recognition in the distributed environment. Secondly, we introduce a novel feature descriptor, namely Adaptive Local Motion Descriptor (ALMD) by considering motion and appearance, which is an extension of Local Ternary Pattern used for static texture analysis, and ALMD also generate persistent codes to describe the localtextures. Finally, the Spark MLlib (Machine Learning Library) Random Forest is employed to recognize the human actions. Experimental results show the superiority of the proposed approach over other state-of-the-arts. OAPA","Adaptive Local Motion Descriptor; Data mining; Distributed databases; Feature extraction; Human action recognition; Libraries; Multimedia communication; Random Forest; Spark; Spark MLlib; Sparks; Streaming media","Data mining; Decision trees; Electric sparks; Feature extraction; Learning systems; Libraries; Media streaming; Motion analysis; Multimedia systems; Security systems; Distributed database; Human-action recognition; Local motions; Multi-media communications; Random forests; Streaming media; Image recognition",2-s2.0-85030780957
"Moomen A.-W., Dewan A.","Assessing the spatial relationships between mining and land degradation: evidence from Ghana",2017,"International Journal of Mining, Reclamation and Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969872717&doi=10.1080%2f17480930.2016.1188253&partnerID=40&md5=db2c34b12b2bdf82d4f189c2bca5df13","The relationships between mining and land degradation, their potential socioeconomic impacts and extents were quantified and analysed in this study. A case study was conducted in the Upper West, which is an emerging mining region in Ghana. Land cover, socio-economic and monthly rainfall data were used in GIS. Mining-induced land degradation indices range from 0.02 to 0.80. The Fournier co-efficient model was used to obtain erosivity indices between 42 and 84 mm, bringing mining concessions into severe erosivity zones. The results of this study will facilitate a concerted effort by governments and companies to prioritise sustainable mining and rural development. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","displacement; erosion; land degradation; mapping; Mining; Savanna","Erosion; Mapping; Mining; Rain; Regional planning; displacement; Land degradation; Monthly rainfalls; Rural development; Savanna; Socio-economic impacts; Spatial relationships; Sustainable mining; Geographic information systems; displacement; erosion; erosivity; geological mapping; GIS; land cover; land degradation; mining; rainfall; savanna; socioeconomic impact; Ghana",2-s2.0-84969872717
"Shahiri A.M., Husain W., Rashid N.A.","A proposed framework on hybrid feature selection techniques for handling high dimensional educational data",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031278682&doi=10.1063%2f1.5005463&partnerID=40&md5=f6991d79363bd819e2eef3906d020823","Huge amounts of data in educational datasets may cause the problem in producing quality data. Recently, data mining approach are increasingly used by educational data mining researchers for analyzing the data patterns. However, many research studies have concentrated on selecting suitable learning algorithms instead of performing feature selection process. As a result, these data has problem with computational complexity and spend longer computational time for classification. The main objective of this research is to provide an overview of feature selection techniques that have been used to analyze the most significant features. Then, this research will propose a framework to improve the quality of students' dataset. The proposed framework uses filter and wrapper based technique to support prediction process in future study. © 2017 Author(s).",,,2-s2.0-85031278682
"Sher G., Zhi D., Zhang S.","DRREP: Deep ridge regressed epitope predictor",2017,"BMC Genomics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030313529&doi=10.1186%2fs12864-017-4024-8&partnerID=40&md5=e8370bdccdc9627edb2319758693d55a","Introduction: The ability to predict epitopes plays an enormous role in vaccine development in terms of our ability to zero in on where to do a more thorough in-vivo analysis of the protein in question. Though for the past decade there have been numerous advancements and improvements in epitope prediction, on average the best benchmark prediction accuracies are still only around 60%. New machine learning algorithms have arisen within the domain of deep learning, text mining, and convolutional networks. This paper presents a novel analytically trained and string kernel using deep neural network, which is tailored for continuous epitope prediction, called: Deep Ridge Regressed Epitope Predictor (DRREP). Results: DRREP was tested on long protein sequences from the following datasets: SARS, Pellequer, HIV, AntiJen, and SEQ194. DRREP was compared to numerous state of the art epitope predictors, including the most recently published predictors called LBtope and DMNLBE. Using area under ROC curve (AUC), DRREP achieved a performance improvement over the best performing predictors on SARS (13.7%), HIV (8.9%), Pellequer (1.5%), and SEQ194 (3.1%), with its performance being matched only on the AntiJen dataset, by the LBtope predictor, where both DRREP and LBtope achieved an AUC of 0.702. Conclusion: DRREP is an analytically trained deep neural network, thus capable of learning in a single step through regression. By combining the features of deep learning, string kernels, and convolutional networks, the system is able to perform residue-by-residue prediction of continues epitopes with higher accuracy than the current state of the art predictors. © 2017 The Author(s).","Analytical learning; Continuous epitope; Convolutional network; Deep network; Epitope prediction; Linear epitope; Neural network; String kernel","epitope; algorithm; amino acid sequence; AntiJen database; Article; artificial neural network; data base; deep ridge regressed epitope predictor; Human immunodeficiency virus infection; kernel method; learning; machine learning; mining; nerve cell; Pellequer database; prediction; predictor variable; SEQ194 database; severe acute respiratory syndrome",2-s2.0-85030313529
"Lian J., McGuire M.P., Moore T.W.","FunnelCloud: a cloud-based system for exploring tornado events",2017,"International Journal of Digital Earth",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010701058&doi=10.1080%2f17538947.2017.1279235&partnerID=40&md5=da8fe08ce5fcaac6fc027583b802057d","Recent research has shown an increase in the number of extreme tornado outbreaks per year. The characterization of the spatio-temporal pattern of tornado events is therefore a critical task in the analysis of meteorological data. Currently, there are a large number of available meteorological datasets that can be used for such analysis. However, much of these data are distributed across multiple websites and are not accessible in a central location. This poses a significant challenge for a scientist who is interested in exploring meteorological patterns associated with tornado events. This paper presents a novel system which uses cloud-based technology for integrating, storing, exploring, analyzing, and visualizing meteorological data associated with tornado outbreaks. The system employs a novel NoSQL database schema and web services architecture for data integration and provides a user friendly interface that allows scientists to explore the spatio-temporal pattern of tornado events. Furthermore, scientists can use this interface to analyze the relationship between different meteorological variables and properties of tornado outbreaks using a number of spatio-temporal statistical and data mining methods. The efficacy of the system is demonstrated on a use case centered on the analysis of climatic indicators of large spatio-temporally clustered tornado outbreaks. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","cloud-based system; data integration; NoSQL; spatio-temporal clustering; tornado data warehouse; web-mapping",,2-s2.0-85010701058
"Newenham-Kahindi A., Stevens C.E.","An institutional logics approach to liability of foreignness: The case of mining MNEs in Sub-Saharan Africa",2017,"Journal of International Business Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030326286&doi=10.1057%2fs41267-017-0111-y&partnerID=40&md5=f8d066c0a9853a937ff97e1d4f0e8c69","Prior research on firms’ liability of foreignness (LOF) has emphasized the role of isomorphic behavior in overcoming LOF. However, the literature has not adequately considered how firms can overcome LOF under conditions of institutional complexity, when fundamental differences in firms’ home and host country values, beliefs, and rules may make isomorphic behaviors impossible or undesirable. In this article, we use the emerging research on institutional logics and institutional entrepreneurship to address this important issue by examining case studies of eight foreign mining MNEs experiencing LOF in Sub-Saharan Africa. Based upon our qualitative analysis, we find that MNEs can overcome LOF by co-creating new institutional logics rather than conforming to existing ones. Yet our data show that this is a difficult process, one that may not be capable of being done unilaterally by the MNE. Instead, we find that local employees embedded in both sets of competing institutional logics acted as key intermediaries who facilitated institutional entrepreneurship. Moreover, we found that firms’ implementation strategy matters as well: in some cases, institutional entrepreneurship mitigated LOF; in others, friction returned to varying degrees. © 2017 Academy of International Business","Africa; embedded agency; institutional change; institutional entrepreneurship; institutional logics; intermediary; liability of foreignness; qualitative analysis",,2-s2.0-85030326286
"Dos Santos E.B.F., Pistor R., Gerlich A.P.","Pulse profile and metal transfer in pulsed gas metal arc welding: droplet formation, detachment and velocity",2017,"Science and Technology of Welding and Joining",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013418942&doi=10.1080%2f13621718.2017.1288889&partnerID=40&md5=b54bacd3b3646eec26140aae4a28b4e6","The influence of current profile and pulse parameters on droplet formation and transfer was investigated. One profile has an exponential ramp up and down in the current pulse shape, while the second is nearly square shaped. High-speed photography, synchronised with a high-speed data acquisition system, was used to monitor the droplet formation and transfer. It was found that for long-tail current profile, most of droplet formation and detachment occurs before background current is reached. While, for the nearly square pulse, most of droplet formation and transfer occurs during background current, giving a stable and smooth metal transfer. The arc attachment position was found to vary for the different profiles. Droplet speed was measured, and it was found that it is proportional to the peak current and inversely proportional to background current. Dimensionless process parameters were defined and used to predict droplet speed using a neural networks algorithm. © 2017 Institute of Materials, Minerals and Mining. Published by Taylor & Francis on behalf of the Institute.","droplet speed; metal transfer; neural network; Pulsed GMAW","Data acquisition; Drop formation; Drops; Electric arc welding; Electric welding; Gas metal arc welding; Gas welding; Metals; Neural networks; Speed; Background current; Current profile; Droplet formation; High speed data acquisition system; Metal transfers; Neural networks algorithms; Process parameters; Pulsed gas metal arc welding; High speed photography",2-s2.0-85013418942
"Philips S., Wu H.-Y., Li L.","Using machine learning algorithms to identify genes essential for cell survival",2017,"BMC Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030331000&doi=10.1186%2fs12859-017-1799-1&partnerID=40&md5=3d120fc6be61dd9e769aab4dcff8a721","Background: With the explosion of data comes a proportional opportunity to identify novel knowledge with the potential for application in targeted therapies. In spite of this huge amounts of data, the solutions to treating complex disease is elusive. One reason being that these diseases are driven by a network of genes that need to be targeted in order to understand and treat them effectively. Part of the solution lies in mining and integrating information from various disciplines. Here we propose a machine learning method to mining through publicly available literature on RNA interference with the goal of identifying genes essential for cell survival. Results: A total of 32,164 RNA interference abstracts were identified from 10.5 million pubmed abstracts (2001 - 2015). These abstracts spanned over 1467 cancer cell lines and 4373 genes representing a total of 25,891 cell gene associations. Among the 1467 cell lines 88% of them had at least 1 or up to 25 genes studied in a given cell line. Among the 4373 genes 96% of them were studied in at least 1 or up to 25 different cell lines. Conclusions: Identifying genes that are crucial for cell survival can be a critical piece of information especially in treating complex diseases, such as cancer. The efficacy of a therapeutic intervention is multifactorial in nature and in many cases the source of therapeutic disruption could be from an unsuspected source. Machine learning algorithms helps to narrow down the search and provides information about essential genes in different cancer types. It also provides the building blocks to generate a network of interconnected genes and processes. The information thus gained can be used to generate hypothesis which can be experimentally validated to improve our understanding of what triggers and maintains the growth of cancerous cells. © 2017 The Author(s).","Gene essentiality; Literature mining; Machine learning","Abstracting; Artificial intelligence; Cell culture; Cells; Complex networks; Cytology; Diseases; Genes; Learning systems; RNA; Building blockes; Cancer cell lines; Gene associations; Integrating information; Literature mining; Machine learning methods; RNA interference; Therapeutic intervention; Learning algorithms",2-s2.0-85030331000
"Kakan M., Joseph T.G., Curley M.","Surface Wave Methods for Predicting Oil Sand Properties",2017,"Geotechnical and Geological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328544&doi=10.1007%2fs10706-017-0378-6&partnerID=40&md5=1c3b4c7d7ec57d24bbc74547e5016500","The use of seismic analysis to estimate ground material properties such as bulk modulus, shear modulus or stiffness has been the subject of research in recent decades. In general, the velocity of a generated wave depends on the properties of the ground through which it travels. While in the case of subsurface methods, where the P- and S-wave velocities are measured, surface methods rely on the velocity of surface waves such as Rayleigh waves (Tokimatsu et al. in Soils Found 31(2):153–163, 1991, doi:10.3208/sandf1972.31.2_153; Matthews et al. in Proc ICE-Geotech Eng 119(2):84–95. 1996). The results of previous analyses on passive seismic monitored truck motion on oil sand was collected via an array of 72 geophones spaced at 1 m intervals on an oil sand haul road. This paper describes the principles to determine a ground shear modulus from seismic data. The method applies to stationary envelopes of data filtered to obtain a conclusive comparison. © 2017 Springer International Publishing AG","Geophones; Haul truck; Mining; Rayleigh waves; Seismic analysis; VIMS","Elastic moduli; Mine trucks; Mining; Oil sands; Rayleigh waves; Seismology; Shear strain; Shear waves; Trucks; Geophones; Haul trucks; P- and S-wave velocities; Sand properties; Seismic analysis; Subsurface methods; Surface methods; VIMS; Surface waves",2-s2.0-85030328544
"Schoepfer E., Spröhnle K., Kranz O., Blaes X., Kolomaznik J., Hilgert F., Bartalos T., Kemper T.","Towards a multi-scale approach for an Earth observation-based assessment of natural resource exploitation in conflict regions",2017,"Geocarto International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976288748&doi=10.1080%2f10106049.2016.1195885&partnerID=40&md5=2c7294f10f0a2c8c699d090b4e5a82ab","The exploitation of resources, if not properly managed, can lead to spoiling natural habitats as well as to threatening people’s health, livelihoods and security. The paper discusses a multi-scale Earth observation-based approach to provide independent information related to exploitation activities of natural resources for countries which are experiencing armed conflict. The analyses are based on medium to very high spatial resolution optical satellite data. Object-based image analysis is used for information extraction at these different scales. On a subnational level, conflict-related land cover changes as an indication of potential hot spots for exploitation activities are classified. The regional assessment provides information about potential activity areas of resource exploitation, whereas on a local scale, a site-specific assessment of exploitation areas is performed. The study demonstrates the potential of remote sensing for supporting the monitoring and documentation of natural resource exploitation in conflict regions. © 2016 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Democratic Republic of the Congo; land cover; mining; natural resources; object-based image analysis",,2-s2.0-84976288748
"Krämer A., Friesen M., Shelton T.","Are airline passengers ready for personalized dynamic pricing? A study of German consumers",2017,"Journal of Revenue and Pricing Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030329051&doi=10.1057%2fs41272-017-0122-0&partnerID=40&md5=b71ae87293392e673e4e6933922d307b","Today, dynamic pricing (DP) in most industries is an established form of pricing, and supported by the DP functionality of many revenue management (RM) systems and the general simplification of airline pricing driven by low-cost carriers. The technological changes (NDC, Big data, IoT, etc.) allow further steps for price differentiation culminating in either personalized or personalized dynamic pricing (PDP). PDP as we define it is not to be confused with the traditional DP of today‘s RM practices. Whether appropriate strategies for a 1:1 price setting can be successfully implemented in the market, depends on several factors: (1) technological advances in data mining and the ability to detect customer preferences (2) the ability to accurately determine customer’s willingness-to-pay by predictive analytics; (3) the use of personal data by airlines and its acceptance by consumers as well as (4) the medium-term impact on customer loyalty and the associated risks for the sustainability of the whole airline business model. © 2017 Macmillan Publishers Ltd","Customers’ acceptance; Dynamic pricing; Personalized pricing",,2-s2.0-85030329051
"Hussein R.R.S., Ali A.M.A., Salem H.F., Abdelrahman M.M., Said A.S.A., Abdelrahim M.E.A.","In vitro/in vivo correlation and modeling of emitted dose and lung deposition of inhaled salbutamol from metered dose inhalers with different types of spacers in noninvasively ventilated patients",2017,"Pharmaceutical Development and Technology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948187135&doi=10.3109%2f10837450.2015.1116567&partnerID=40&md5=5780a55e2a2ba7f6fc81611fecf0697f","Substituting spacer by another in noninvasive ventilation (NIV) involves many variables, e.g. total emitted dose (TED), mass median aerodynamic diameter (MMAD), type of spacer, total lung deposition and total systemic absorption, which must be adjusted to ensure patient optimum therapy. Data mining based on artificial neural networks and genetic algorithms were used to model in vitro inhalation process, predict and optimize bioavailability from inhaled doses delivered by metered dose inhaler (MDI) using different spacers in NIV. Modeling of data indicated that in vitro performance of MDI-spacer systems was dependent mainly on fine particle dose (FPD), fine particle fraction (FPF), MMAD and to lesser extent on spacer type. Ex vivo model indicated that amount of salbutamol collected on facemask filter was directly affected by FPF. In vivo model (24hQ) depended directly on spacer type, FPF and TED. Female patients showed higher 0.5hQ and 24hQ values than males. AeroChamber VC spacer demonstrated higher TED and 24hQ in vivo values. Results indicated suitability of MDI-spacer systems in achieving appropriate in vitro inhalation performance. The possibility of modeling and predicting both ex vivo and in vivo capabilities of MDI-spacer systems from knowledge of in vitro attributes enabled detailed focus on important variables required to deliver safe and accurate doses of salbutamol to ventilated patients. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","Fine particle dose; mass median aerodynamic diameter; metered dose inhalers; modeling; noninvasive ventilation; salbutamol; spacer; total emitted dose",,2-s2.0-84948187135
"Ibrahim N., Akhir N.S.M., Hassan F.H.","Predictive analysis effectiveness in determining the epidemic disease infected area",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031311147&doi=10.1063%2f1.5005397&partnerID=40&md5=89e2bc40aa3b0ce712f95cff3071eea2","Epidemic disease outbreak had caused nowadays community to raise their great concern over the infectious disease controlling, preventing and handling methods to diminish the disease dissemination percentage and infected area. Backpropagation method was used for the counter measure and prediction analysis of the epidemic disease. The predictive analysis based on the backpropagation method can be determine via machine learning process that promotes the artificial intelligent in pattern recognition, statistics and features selection. This computational learning process will be integrated with data mining by measuring the score output as the classifier to the given set of input features through classification technique. The classification technique is the features selection of the disease dissemination factors that likely have strong interconnection between each other in causing infectious disease outbreaks. The predictive analysis of epidemic disease in determining the infected area was introduced in this preliminary study by using the backpropagation method in observation of other's findings. This study will classify the epidemic disease dissemination factors as the features for weight adjustment on the prediction of epidemic disease outbreaks. Through this preliminary study, the predictive analysis is proven to be effective method in determining the epidemic disease infected area by minimizing the error value through the features classification. © 2017 Author(s).",,,2-s2.0-85031311147
[No author name available],"AIP Conference Proceedings",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031289738&partnerID=40&md5=88a44e9ee5369308b82f574dde9d53ed","The proceedings contain 153 papers. The topics discussed include: predictive analysis and data mining among the employment of fresh graduate students in HEI; developing a disaster education program for community safety and resilience: the preliminary phase; corporate knowledge repository: adopting academic LMS into corporate environment; process simulation and economic analysis of biodiesel production from waste cooking oil with membrane bioreactor; a CCTV system with SMS alert (CMDSA): an implementation of pixel processing algorithm for motion detection; prediction of soil stress-strain response incorporates mobilised shear strength envelope of granitic residual soil; clustering approach for unsupervised segmentation of malarial plasmodium vivax parasite; aiding pest control management of long-tailed macaques (macaca fascicularis fascicularis) in malaysia by using molecular markers of mitochondrial DNA; antioxidant activity of alstonia angustifolia ethanolic leaf extract; safety of street: the role of street design; fieldwork measurement of indoor environmental quality (IEQ) in Malaysian platinum-rated green office buildings; single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification; and improving entrepreneurial opportunity recognition through web content analytics.",,,2-s2.0-85031289738
"Vriens B., Voegelin A., Hug S.J., Kaegi R., Winkel L.H.E., Buser A.M., Berg M.","Quantification of Element Fluxes in Wastewaters: A Nationwide Survey in Switzerland",2017,"Environmental Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030748636&doi=10.1021%2facs.est.7b01731&partnerID=40&md5=8c94568e56351da1335e4b4bf2372223","The number and quantities of trace elements used in industry, (high-tech) consumer products, and medicine are rapidly increasing, but the resulting emissions and waste streams are largely unknown. We assessed the concentrations of 69 elements in digested sewage sludge and effluent samples from 64 municipal wastewater treatment plants as well as in major rivers in Switzerland. This data set, representative of an entire industrialized country, presents a reference point for current element concentrations, average per-capita fluxes, loads discharged to surface waters, and economic waste-stream values. The spatial distribution of many individual elements could be attributed either to predominant geogenic or to anthropogenic inputs. Per-capita element fluxes ranged from &lt;10 μg day-1 (e.g., Au, In, and Lu) to &gt;1 mg day-1 (e.g., Zn, Sc, Y, Nb, and Gd) and &gt;1 g day-1 (e.g., for P, Fe, and S). Effluent loads of some elements contributed significantly to riverine budgets (e.g., 24% for Zn, 50% for P, and 83% for Gd), indicating large anthropogenic inputs via the wastewater stream. At various locations, precious metal concentrations in sludge were similar to those in profitable mining ores, with total flux values of up to 6.8 USD per capita per year or 15 USD per metric ton of dry sludge. © 2017 American Chemical Society.",,"Budget control; Consumer products; Effluent treatment; Effluents; Gold; Lutetium; Sewage sludge; Sewage treatment plants; Trace elements; Wastewater treatment; Zinc; Anthropogenic inputs; Current element; Digested sewage sludge; Industrialized countries; Metal concentrations; Municipal wastewater treatment plants; Reference points; Wastewater streams; Gadolinium; arsenic; cadmium; carbon monoxide; chromium; copper; cuprous ion; hydrogen peroxide; lead; nickel; river water; surface water; trace element; zinc; zinc ion; concentration (composition); discharge; effluent; human activity; precious metal; quantitative analysis; sewage; sludge; spatial distribution; survey; wastewater treatment; wastewater treatment plant; agriculture; Article; concentration (parameters); digestion; geographic distribution; quality control; risk reduction; river; seasonal variation; sludge; Switzerland; Switzerland",2-s2.0-85030748636
"Bogicevic V., Yang W., Bujisic M., Bilgihan A.","Visual Data Mining: Analysis of Airline Service Quality Attributes",2017,"Journal of Quality Assurance in Hospitality and Tourism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029461167&doi=10.1080%2f1528008X.2017.1314799&partnerID=40&md5=d15fdc6aa50d3486bbc4252fb6a80bcd","The objectives of the study are to identify the key airline quality attributes from online review posts and to examine the effect of identified airline quality attributes on eWOM communication. This study employed data-mining techniques and logistic regression on 901 passenger reviews to evaluate the service quality of passenger airlines. The major contribution was identifying the most salient topics of travelers complimenting and complaining reviews. Passengers’ comments support that service, staff, cabin seat comfort, and entertainment are among the most discussed themes in positive and negative reviews. Additionally, value, seat comfort, staff/service, and catering were found to be significant predictors of airline eWOM. © 2017 Taylor & Francis Group, LLC.","Airline industry; data mining; eWOM; satisfaction; service quality",,2-s2.0-85029461167
"Poorthuis A., Zook M.","Making Big Data Small: Strategies to Expand Urban and Geographical Research Using Social Media",2017,"Journal of Urban Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021411405&doi=10.1080%2f10630732.2017.1335153&partnerID=40&md5=3d2a1e77e96cf49d204906d87f2504af","While exciting, Big Data (particularly geotagged social media data) has proven difficult for many urbanists and social science researchers to use. As a partial solution, we propose a strategy that enables the fast extracting of only relevant data from large sets of geosocial data. While contrary to many Big Data approaches—in which analysis is done on the entire dataset—much productive social science work can use smaller datasets—around the same size as census or survey data—within standard methodological frameworks. The approach we outline in this paper—including the example of a fully operating system—offers a solution for urban researchers interested in these types of data but reluctant to personally build data science skills. © 2017 The Society of Urban Technology.","Big data; data frameworks; data mining; social media; social science methods; twitter","data mining; data set; geographical research; social media; urban society",2-s2.0-85021411405
"Zhang H., Zhang J., Wei X., Zhang X., Zou T., Yang G.","A New Frequent Pattern Mining Algorithm with Weighted Multiple Minimum Supports",2017,"Intelligent Automation and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030623418&doi=10.1080%2f10798587.2017.1316082&partnerID=40&md5=39c48198df9b2d7c0dd28f7a4a2fe02d","Association rules mining is one of the momentous areas in data mining. Frequent patterns mining plays an important role in association rules mining. The effects of traditional frequent patterns mining with same minimum support are highly affected by the value of minimum support. But, for many real datasets, it’s hard to choose the value of minimum support. Too small values of minimum support may cause rules explosion, and too large values may cause rare item dilemma. In this paper we propose an improved approach to extract frequent patterns, which are more interesting to users. Because of the different characteristics of each item, we assign a multiple minimum support and weight based on item support and users’ interests for each item. In order to define the minimum supports of itemsets, we suggest a novel method, which exploits the minimum constraint and maximum constraint to deal with the rare item dilemma and rules explosion problem. The combination of minimum constraint and maximum constraint is based on the weight of the itemset. In this way, we extend the support confidence framework. Experimental results show that the proposed approach is more efficient than other comparing methods. © 2017 TSI® Press.","Association rules; Data mining; Frequent pattern; Interest; Multiple minimum supports",,2-s2.0-85030623418
"Alyahya M.S., Hijazi H.H., Alshraideh H.A., Al-Nasser A.D.","Using decision trees to explore the association between the length of stay and potentially avoidable readmissions: A retrospective cohort study",2017,"Informatics for Health and Social Care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010644438&doi=10.1080%2f17538157.2016.1269105&partnerID=40&md5=26f6767cd724602ebd3d53522adc6edd","Background: There is a growing concern that reduction in hospital length of stay (LOS) may raise the rate of hospital readmission. This study aims to identify the rate of avoidable 30-day readmission and find out the association between LOS and readmission. Methods: All consecutive patient admissions to the internal medicine services (n = 5,273) at King Abdullah University Hospital in Jordan between 1 December 2012 and 31 December 2013 were analyzed. To identify avoidable readmissions, a validated computerized algorithm called SQLape was used. The multinomial logistic regression was firstly employed. Then, detailed analysis was performed using the Decision Trees (DTs) model, one of the most widely used data mining algorithms in Clinical Decision Support Systems (CDSS). Results: The potentially avoidable 30-day readmission rate was 44%, and patients with longer LOS were more likely to be readmitted avoidably. However, LOS had a significant negative effect on unavoidable readmissions. Conclusions: The avoidable readmission rate is still highly unacceptable. Because LOS potentially increases the likelihood of avoidable readmission, it is still possible to achieve a shorter LOS without increasing the readmission rate. Moreover, the way the DT model classified patient subgroups of readmissions based on patient characteristics and LOS is applicable in real clinical decisions. © 2017 Taylor & Francis.","Avoidable readmission; clinical decision support systems; decision trees (DTs); length of stay (LOS)","clinical decision support system; cohort analysis; data mining; decision tree; hospital admission; hospital readmission; human; internal medicine; Jordan; length of stay; logistic regression analysis; major clinical study; model; university hospital",2-s2.0-85010644438
"Wu W., Chen Y., Seng D.","Implementation of Web Mining Algorithm Based on Cloud Computing",2017,"Intelligent Automation and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020234910&doi=10.1080%2f10798587.2017.1316077&partnerID=40&md5=8673df63c148f41adc9694b0e8318a04","The rapid growth of the Internet exceeds all expectations. The analysis and mining of huge amounts of web data is facing a bottleneck in computing power and storage space. Through the use of cloud computing technology, we can facilitate the network access to powerful computing power, storage capacity and infrastructure. Cloud computing can effectively solve the problems by providing a data processing storage center of high reliability and scalability, which will improve the ability to process web data and reduce the requirements of the terminal devices. This paper studies web mining algorithms in a cloud computing environment. The web data mining algorithm and the MapReduce programming model are combined. We study the web mining techniques, especially the K-centers clustering algorithm, explore the combination of web mining algorithms and cloud computing technology and improve the data mining algorithms to adapt to the analysis and processing of mass web data based on cloud computing platforms. Our study constructs a distributed cloud environment using a Hadoop framework. In the experimental environment, we analyze the impact on computational performance by setting different block size parameters. Here, the block size determines the number that the pending data file is split, and the corresponding scale and amount of parallel calculation. © 2017 TSI® Press.","Cloud computing; Hadoop; Map Reduce; Web mining",,2-s2.0-85020234910
"Lai J., Cheng T., Lansley G.","Improved targeted outdoor advertising based on geotagged social media data",2017,"Annals of GIS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030568366&doi=10.1080%2f19475683.2017.1382571&partnerID=40&md5=991709bd65a2be2509e5e1aab1cd5020","With as many as 4 million passenger journeys within the London Underground system every weekday, the advertisement spaces across the stations hold considerable potential. However, the planning of specific advertisements across time and space is difficult to optimize as little is known about passers-by. Therefore, in order to generate detailed and quantifiable spatio-temporal information which is particular to each station area, we have explored local social media data. This research demonstrates how local interests can be mined from geotagged Tweets by using Latent Dirichlet Allocation, an unsupervised topic modelling method. The relative popularity of each of the key topics is then explored spatially and temporally between the station areas. Overall, this research demonstrates the value of using Geographical Information System and text-mining techniques to generate valuable spatio-temporal information on popular interests from Twitter data. © 2017 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","geotagged social media data; LDA; spatio-temporal analysis; Targeted advertisement; topic modelling",,2-s2.0-85030568366
"Miao Y.Z., Ma X.P., Bu S.P.","Research on the Learning Method Based on PCA-ELM",2017,"Intelligent Automation and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023171189&doi=10.1080%2f10798587.2017.1316071&partnerID=40&md5=61cf24737cc855f449c454aca8075eaf","The Single-hidden Layer Feed-forward Neural Network has been widely applied in the fields such as pattern recognition, automatic control and data mining. However, the speed of the traditional learning method, since it is far from enough to satisfy the actual demand has become the main bottleneck, which restricts its development. As one of the new learning methods, the extreme learning machine (ELM) has its own remarkable characteristics, but the fact that ELM is based on the Empirical Risk Minimization may lead to over fitting. In addition, ELM does not consider the weight of error, so its performance will be severely affected when there are outliers in data integration. To solve the above problems, this paper referred to the two algorithms including PCA (Principal Component Analysis) and ELM, and put forward a learning method and prediction model, which combined PCA and ELM. From the results of simulation analysis, as combining advantages of PCA and ELM algorithms, the network structure can be simplified to improve the learning ability and its prediction precision. © 2017 TSI® Press.","Extreme learning machine (ELM); Feed-forward neural network; Principal component analysis (PCA)",,2-s2.0-85023171189
"Nelson B.C., Bowman C., Bowman J.","Designing for Data with Ask Dr. Discovery: Design Approaches for Facilitating Museum Evaluation with Real-Time Data Mining",2017,"Technology, Knowledge and Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015671859&doi=10.1007%2fs10758-017-9313-4&partnerID=40&md5=55070168c19227c09ff6e558f2a4001d","Ask Dr. Discovery is an NSF-funded study addressing the need for ongoing, large-scale museum evaluation while investigating new ways to encourage museum visitors to engage deeply with museum content. To realize these aims, we are developing and implementing a mobile app with two parts: (1) a front-end virtual scientist called Dr. Discovery (Dr. D) for use by museum visitors that doubles as an unobtrusive data-gatherer and (2) a back-end analytics portal to be mined by museum staff, evaluators, and researchers. With the aid of our museum partners, we are developing this app to function as a platform for STEM informal education, research, and data-driven decision-making by museum staff. The Dr. D app has been designed to engage museum visitors, while connecting with an analytic system to make sense, in real time, of the large amounts of data produced by visitors’ use of the app. The analytic system helps museum staff access and interpret ongoing evaluation data, regardless of experience or museum resources, informing the practice of professionals at the front lines of informal STEM education in diverse communities. The design of the Dr. D app incorporates open-source analytic tools that make the gathering and interpretation of contextual information from visitors’ app use accessible to museum staff and educators, building their capacity for using data in their day-to-day work. The same tools are being used by our research team to probe questions about informal learning and motivation, effective application of large datasets for museum evaluation, and ways to encourage and understand use of mobile virtual experiences. In this paper, we describe our theory-based design of the Dr. D app and data analytics and describe findings from initial user testing with our museum partners. © 2017, Springer Science+Business Media Dordrecht.","Data-mining; Informal STEM learning; Instructional design; Museum evaluation","Data mining; Decision making; E-learning; Education; STEM (science, technology, engineering and mathematics); Contextual information; Data driven decision; Diverse community; Informal educations; Informal STEM learning; Instructional designs; Large amounts of data; Real-time data mining; Museums",2-s2.0-85015671859
"Angeli C., Howard S.K., Ma J., Yang J., Kirschner P.A.","Data mining in educational technology classroom research: Can it make a contribution?",2017,"Computers and Education",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020258052&doi=10.1016%2fj.compedu.2017.05.021&partnerID=40&md5=8a2adc7753353085a219e5da072fc8b2","The paper addresses and explains some of the key questions about the use of data mining in educational technology classroom research. Two examples of use of data mining techniques, namely, association rules mining and fuzzy representations are presented, from a study conducted in Europe and another in Australia. Both of these studies examine student learning, behaviors, and experiences within computer-supported classroom activities. In the first study, the technique of association rules mining was used to understand better how learners with different cognitive types interacted with a simulation to solve a problem. Association rules mining was found to be a useful method for obtaining reliable data about learners' use of the simulation and their performance with it. The study illustrates how data mining can be used to advance educational software evaluation practices in the field of educational technology. In the second study, the technique of fuzzy representations was employed to inductively explore questionnaire data. The study provides a good example of how educational technologists can use data mining for guiding and monitoring school-based technology integration efforts. Based on the outcomes, the implications of the study are discussed in terms of the need to develop educational data mining tools that can display results, information, explanations, comments, and recommendations in meaningful ways to non-expert users in data mining. Lastly, issues related to data privacy are addressed. © 2017 Elsevier Ltd","Association rules mining; Educational data mining; Educational technology research; Fuzzy representations","Association rules; Computer software selection and evaluation; Data privacy; Education; Educational technology; Engineering research; Students; Association rules mining; Classroom activity; Educational data mining; Educational software; Fuzzy representations; Questionnaire data; Student learning; Technology Integration; Data mining",2-s2.0-85020258052
"Tan J., Wang F.","Non-traditional data mining applications in Taiwan National Health Insurance (NHI) databases: A Hybrid Mining (HM) case for the framing of NHI decisions",2017,"International Journal of Healthcare Information Systems and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028772349&doi=10.4018%2fIJHISI.2017100103&partnerID=40&md5=d75ee96defbc610ba58fb46561852b15","This study examines time-sensitive applications of data mining methods to facilitate claims review processing and provide policy information for insurance decision-making vis-à-vis the Taiwan National Health Insurance (NHI) databases. In order to obtain the best payment management, a hybrid mining (HM) approach, which has been grounded on the extant knowledge of data mining projects and health insurance domain knowledge, is proposed. Through the integration of data warehousing, online analytic processing, data mining techniques and traditional data analysis in the healthcare field, an easy-to-use decision support platform, which will assist in directing the health insurance decision-making process, is built. Drawing from lessons learned within a case study setting, results showed that not only is HM approach a reliable, powerful, and user-friendly platform for diversified payment decision support, but that it also has great relevance for the practice and acceptance of evidence-based medicine. Essentially, HM approach can provide a critical boost to health insurance decision support; hence, future researchers should develop and improve the approach combined with their own application systems. Copyright © 2017, IGI Global.","Data Mining; Health Decision Support; Hybrid Mining; NHI Databases; NHI Insurance Payment","Data handling; Data warehouses; Database systems; Decision making; Decision support systems; Health; Health insurance; Insurance; Data mining applications; Decision making process; Decision supports; Evidence-based medicine; NHI Insurance Payment; On-line analytic processing; Time sensitive applications; User-friendly platforms; Data mining",2-s2.0-85028772349
"Ma Z., Shi Z., Zhou Y., Xu J., Yu W., Yang Y.","A spatial data mining algorithm for downscaling TMPA 3B43 V7 data over the Qinghai–Tibet Plateau with the effects of systematic anomalies removed",2017,"Remote Sensing of Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028473359&doi=10.1016%2fj.rse.2017.08.023&partnerID=40&md5=2ec056cb76ea0d3c51051e71463bd255","Precipitation plays an important role in the water cycle and in matter and energy exchanges. Acquiring accurate information on precipitation over the Qinghai–Tibet Plateau, which has a limited rain gauge network, has been a great challenge. Downscaling the TRMM Multisatellite Precipitation Analysis (TMPA) 3B43 Version 7 dataset (0.25° resolution) provided an optimal approach to estimating precipitation at 1 km resolution over this highland plateau. Our downscaling assumptions were that non-stationary relationships between precipitation and land surface characteristics occur and have varying two-dimensional scale effects, and that the relationships vary in different sub-regions having differing combinations of land surface characteristics, including vegetation index, topographical factors, and land surface temperatures. We used Cubist (a spatial data mining algorithm) to implement our assumption. Cubist separated the Qinghai–Tibet Plateau into sub-regions according to geographical similarities, and selected the most effective variables over each sub-region to build models. We found that: (1) the downscaled results using this algorithm were more accurate and precise than other commonly used algorithms (e.g., geographically weighted regression) and the original TMPA data at 0.25° resolution; (2) DEM showed limited correlation with precipitation over the Qinghai–Tibet Plateau; and (3) the effects of systematic anomalies in the original TMPA data were removed in the downscaled results based on Cubist. We conclude that Cubist is a promising algorithm able to take hundreds of variables into consideration, and in this study was used to retrieve precipitation estimates at approximately 1 km resolution. © 2017 Elsevier Inc.","Data mining; Downscaling; Precipitation; Qinghai–Tibet Plateau; TMP A3B43 V7","Gages; Geographical regions; Precipitation (chemical); Precipitation (meteorology); Rain; Rain gages; Surface measurement; Down-scaling; Effective variables; Geographically weighted regression; Land surface characteristics; Land surface temperature; Qinghai Tibet plateau; Spatial data mining; TMP A3B43 V7; Data mining; algorithm; data mining; data set; digital elevation model; downscaling; precipitation (climatology); satellite data; spatial data; TRMM; China; Qinghai-Xizang Plateau",2-s2.0-85028473359
"Asif R., Merceron A., Ali S.A., Haider N.G.","Analyzing undergraduate students' performance using educational data mining",2017,"Computers and Education",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020313797&doi=10.1016%2fj.compedu.2017.05.007&partnerID=40&md5=ce09aa5518f1bf76a37f52bec64aa375","The tremendous growth in electronic data of universities creates the need to have some meaningful information extracted from these large volumes of data. The advancement in the data mining field makes it possible to mine educational data in order to improve the quality of the educational processes. This study, thus, uses data mining methods to study the performance of undergraduate students. Two aspects of students' performance have been focused upon. First, predicting students' academic achievement at the end of a four-year study programme. Second, studying typical progressions and combining them with prediction results. Two important groups of students have been identified: the low and high achieving students. The results indicate that by focusing on a small number of courses that are indicators of particularly good or poor performance, it is possible to provide timely warning and support to low achieving students, and advice and opportunities to high performing students. © 2017 Elsevier Ltd","Clustering; Data mining; Decision trees; Performance prediction; Performance progression; Quality of educational processes","Data mining; Decision trees; Education; Forecasting; Trees (mathematics); Academic achievements; Clustering; Educational data mining; Educational process; Low-achieving students; Performance prediction; Performance progression; Undergraduate students; Students",2-s2.0-85020313797
"Lyu H., Wan M., Han J., Liu R., Wang C.","A filter feature selection method based on the Maximal Information Coefficient and Gram-Schmidt Orthogonalization for biomedical data mining",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028037754&doi=10.1016%2fj.compbiomed.2017.08.021&partnerID=40&md5=23fb8208324ca2f00820106eaffbfdb0","A filter feature selection technique has been widely used to mine biomedical data. Recently, in the classical filter method minimal-Redundancy-Maximal-Relevance (mRMR), a risk has been revealed that a specific part of the redundancy, called irrelevant redundancy, may be involved in the minimal-redundancy component of this method. Thus, a few attempts to eliminate the irrelevant redundancy by attaching additional procedures to mRMR, such as Kernel Canonical Correlation Analysis based mRMR (KCCAmRMR), have been made. In the present study, a novel filter feature selection method based on the Maximal Information Coefficient (MIC) and Gram-Schmidt Orthogonalization (GSO), named Orthogonal MIC Feature Selection (OMICFS), was proposed to solve this problem. Different from other improved approaches under the max-relevance and min-redundancy criterion, in the proposed method, the MIC is used to quantify the degree of relevance between feature variables and target variable, the GSO is devoted to calculating the orthogonalized variable of a candidate feature with respect to previously selected features, and the max-relevance and min-redundancy can be indirectly optimized by maximizing the MIC relevance between the GSO orthogonalized variable and target. This orthogonalization strategy allows OMICFS to exclude the irrelevant redundancy without any additional procedures. To verify the performance, OMICFS was compared with other filter feature selection methods in terms of both classification accuracy and computational efficiency by conducting classification experiments on two types of biomedical datasets. The results showed that OMICFS outperforms the other methods in most cases. In addition, differences between these methods were analyzed, and the application of OMICFS in the mining of high-dimensional biomedical data was discussed. The Matlab code for the proposed method is available at https://github.com/lhqxinghun/bioinformatics/tree/master/OMICFS/. © 2017 Elsevier Ltd","Biomedical data mining; Filter feature selection; Gram-Schmidt Orthogonalization (GSO); Maximal Information Coefficient (MIC)","Bandpass filters; Classification (of information); Computational efficiency; Feature extraction; Filtration; MATLAB; Microwave integrated circuits; Redundancy; Biomedical data; Classification accuracy; Degree of relevance; Feature selection methods; Gram-Schmidt orthogonalizations; Kernel canonical correlation analysis; Maximal information; Selection techniques; Data mining",2-s2.0-85028037754
"Park S., Kim D.-Y.","Assessing language discrepancies between travelers and online travel recommendation systems: Application of the Jaccard distance score to web data mining",2017,"Technological Forecasting and Social Change",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016544928&doi=10.1016%2fj.techfore.2017.03.031&partnerID=40&md5=77d5348874703c7e8ee97cf0db3fb155","By using a human-centric approach to online recommender systems, this research aims to estimate the language discrepancies of which travelers and destination marketers describe the travel experiences across 11 tourism destinations in USA. In order to address the research purpose, data has been collected from two different sources that reflect the views of travelers and service providers. Then, a set of text data mining methods (i.e., clustering analysis and Jaccard distance score) was applied to identify the language differences between travelers and CVB websites, according to the following categories: shopping, dining, nightlife/activities, and attractions. Some possible methodological extensions that can improve recommendation capabilities, and managerial implications of these findings are provided. © 2017 Elsevier Inc.","Jaccard distance score; Online recommender system; Smart tourism; Web data mining","Online systems; Recommender systems; Web crawler; Clustering analysis; Jaccard distance; Managerial implications; Online recommender systems; Research purpose; Smart tourism; Travel experiences; Web data mining; Data mining; data mining; language; tourist destination; World Wide Web; United States",2-s2.0-85016544928
"Ghannadpour S.S., Hezarkhani A., Roodpeyma T.","Combination of separation methods and data mining techniques for prediction of anomalous areas in Susanvar, Central Iran",2017,"Journal of African Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026213915&doi=10.1016%2fj.jafrearsci.2017.07.015&partnerID=40&md5=fa018ee7b47ed21a55c337501b2eed07","Structural method U-statistics is an eminent technique for delineating geochemical patterns; on the other hand, it is worthwhile to introduce Mahalanobis distance approach decreasing the background effects and intensifying the correlation factor of points as a powerful non-structural method. Undoubtedly, predicting the anomalous values could play an important role in the inchoate stages of exploration. Therefore, it is essential to find the most accurate approach to separate anomalous values from background and afterward use the results to anticipate each arbitrary sample. In this study, results of the combination between U-statistics & Mahalanobis distance algorithms are used to distinguish anomalous values from background on an accurate point of view. Then, three data mining methods will be applied to produce practical equations and finally determine anomalous values. Separation of geochemical anomalies, based on the combination of the U-statistics and the Mahalanobis distance approaches, would be done; then, under the influence of their results and the other parameters – x and y coordinates and Au and As grades - three data mining methods, K nearest neighbor (K-NN), decision tree, and naïve Bayes classifier, have been applied. For this purpose after separation of anomalous values according to the number of 603 samples by applying above combination, the data mining methods would be utilized to anticipate anomalous values for each unknown point. Finally, in order to judge about the designed networks, training samples would be considered as test samples under the application of the networks. Therefore according to the results, decision tree method would appear as the more powerful approach than the other due to far fewer number of wrong estimated samples and approving high accuracy of the designed network, that is, resubstitution error for this network is noted only 0.0232. Noteworthy is that the numbers of wrong estimated samples are 30 and 61 and the rates of error are 0.0498 and 0.1 for K-NN and naïve Bayes methods respectively. So needless to say that the combination of decision tree method and the introduced anomaly separation approach is much more remarkable as a reliable and efficient technique to approach worthwhile predictions. © 2017 Elsevier Ltd","Anomalous values; Data mining techniques; Mahalanobis distance; Susanvar; U-statistics","algorithm; data mining; exploration; geochemistry; statistical analysis; Iran",2-s2.0-85026213915
"Umamaheswari R., Purnima S.S., Mahesan S.S.","Customer preservence for an organisation using data mining",2017,"International Journal of Civil Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032721454&partnerID=40&md5=a0ddab50eb4bb39657cfb9c1396b17fb","With the fastest growing competitive environment, a technique is required to manage the customer relationship management. The technique should govern customer as their central view point. One such technique is data mining. Data mining is a powerful tool which analyzes, predict and discover the hidden data that is required. Customer relationship management takes customer as their core value, since customer plays a main role in the business environment. This paper describes about management of customer based on data mining. © IAEME Publication","crm; Data Mining",,2-s2.0-85032721454
"Oliveira M.M., Camanho A.S., Walden J.B., Miguéis V.L., Ferreira N.B., Gaspar M.B.","Forecasting bivalve landings with multiple regression and data mining techniques: The case of the Portuguese Artisanal Dredge Fleet",2017,"Marine Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025660951&doi=10.1016%2fj.marpol.2017.07.013&partnerID=40&md5=d66a1bff18b159125142ee094011aee3","This paper develops a decision support tool that can help fishery authorities to forecast bivalve landings for the dredge fleet accounting for several contextual conditions. These include weather conditions, phytotoxins episodes, stock-biomass indicators per species and tourism levels. Vessel characteristics and fishing effort are also taken into account for the estimation of landings. The relationship between these factors and monthly quantities landed per vessel is explored using multiple linear regression models and data mining techniques (random forests, support vector machines and neural networks). The models are specified for different regions in the Portugal mainland (Northwest, Southwest and South) using six years of data 2010–2015). Results showed that the impact of the contextual factors varies between regions and also depends on the vessels target species. The data mining techniques, namely the random forests, proved to be a robust decision support tool in this context, outperforming the predictive performance of the most popular technique used in this context, i.e. linear regression. © 2017","Bivalve fisheries; Data mining; Forecasting; Multiple regression; Random forests; Small scale fisheries","algorithm; artisanal fishery; bivalve; data mining; decision support system; forecasting method; multiple regression; Portugal; Bivalvia",2-s2.0-85025660951
"Chen W., Panahi M., Pourghasemi H.R.","Performance evaluation of GIS-based new ensemble data mining techniques of adaptive neuro-fuzzy inference system (ANFIS) with genetic algorithm (GA), differential evolution (DE), and particle swarm optimization (PSO) for landslide spatial modelling",2017,"Catena",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019976451&doi=10.1016%2fj.catena.2017.05.034&partnerID=40&md5=55cd06d4814a33ecfdc10d88a6c4539f","This paper presents GIS-based new ensemble data mining techniques that involve an adaptive neuro-fuzzy inference system (ANGIS) with genetic algorithm, differential evolution, and particle swarm optimization for landslide spatial modelling. This research was tested in Hanyuan County, which is a landslide-prone area in Sichuan Province, China. Different continuous and categorical landslide conditioning factors according to a literature review and data availability were selected, and their maps were digitized in a GIS environment. These layers are the slope angle, slope aspect, altitude, plan curvature, profile curvature, topographic wetness index, distance to faults, distance to rivers, distance to roads, lithology, normalized difference vegetation index and land use. According to historical information of individual landslide events, interpretation of the aerial photographs, and field surveys supported by the Sichuan Land Resources Bureau of China, 225 landslides were identified in the study area. The landslide locations were divided into two subsets, namely, training and validating (70/30), based on a random selection scheme. In this research, a probability certainty factor (PCF) model was used for the evaluation of the relationship between the landslides and conditioning factors. In the next step, three data mining techniques combined with the ANFIS model, including ANFIS-genetic algorithm (ANFIS-GA), ANFIS-differential evolution (ANFIS-DE), and ANFIS-particle swarm optimization (ANFIS-PSO), were used for the landslide spatial modelling and its zonation. Finally, the landslide susceptibility maps produced by the mentioned models were evaluated by the ROC curve. The results showed that the area under the curve (AUC) of all of the models was > 0.75. At the same time, the highest AUC value was for the ANFIS-DE model (0.844), followed by ANGIS-GA (0.821), and ANFIS-PSO (0.780). In general, the proposed ensemble data mining techniques can be applied for land use planning and management of landslide susceptibility and hazard in the study area and in other areas. © 2017 Elsevier B.V.","ANFIS; Differential evolution; Genetic algorithm; Hanyuan County; Landslide susceptibility; Particle swarm optimization","data mining; ensemble forecasting; field survey; fuzzy mathematics; genetic algorithm; GIS; landslide; literature review; numerical model; optimization; performance assessment; spatial analysis; China; Hanyuan; Sichuan",2-s2.0-85019976451
"Lu C.-X., Lü Y.-H., Ma M., Zhang G.-J., Ma Y.","Data mining for points-selection rules in acupuncture treatment of mammary gland hyperplasia",2017,"Journal of Acupuncture and Tuina Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032200479&doi=10.1007%2fs11726-017-1023-x&partnerID=40&md5=fed4e2c1c787361e4ed323aa17c4f288","Objective: To explore and analyze the points-selection rules in acupuncture treatment of mammary gland hyperplasia (MGH) by data mining and statistical method. Methods: Clinical literatures about the treatment of MGH with acupuncture published in the recent 16 years were retrieved from Chinese Journal Full-text Database (CJFD) and established into a database by Excel. The SPSS 20 version software and Clementine 12.0 version software were adopted to analyze the frequency and association rules of points-selection in the treatment of MGH with acupuncture. Results: The top 3 points used most frequently in acupuncture treatment of MGH were Danzhong (CV 17), Taichong (LR 3) and Zusanli (ST 36); points from the Stomach Meridian of Foot Yangming and Liver Meridian of Foot Jueyin were most commonly used; the commonly selected points were predominantly distributed in thoracic and abdominal regions and lower limbs; emphasis on the combination use of local and distal points; of the specific points, the five Shu-Transmitting points were mostly used; association analysis showed that the associations among Taichong (LR 3), Danzhong (CV 17) and Zusanli (ST 36) were the most significant. Conclusion: The data mining results substantially accord with the general rules of acupuncture-moxibustion theories in traditional Chinese medicine, able to reflect the points-selection principles and features in acupuncture treatment of MGH and provide evidence for the points selection in the treatment of MGH in acupuncture clinic. © 2017, Shanghai Research Institute of Acupuncture and Meridian and Springer-Verlag GmbH Germany.","Acupuncture Therapy; Danzhong (CV 17); Data Mining; Mammary Gland Hyperplasia; Point; Point; Point; Taichong (LR 3); Zusanli (ST 36)",,2-s2.0-85032200479
"Karabacak A., Celik S., Tatliyer A., Keskin I., Erturk Y.E., Eyduran E., Javed Y., Tariq M.M.","Estimation of cold carcass weight and body weight from several body measurements in sheep through various data mining algorithms",2017,"Pakistan Journal of Zoology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031741328&doi=10.17582%2fjournal.pjz%2f2017.49.5.1731.1738&partnerID=40&md5=6e2391772af10d587bd05cd77d025bd0","The goal of the present study was to compare the predictive performance of three data mining algorithms viz., CHAID, Exhaustive CHAID, and CART implemented in the estimation of cold carcass weight (CCW) and body weight (BW) from several body measurements (withers height (WH), chest depth (CD), body length (BL), hearth girth (HG) and leg circumference (LC)) measured from five sheep breeds (Akkaraman (9), Dagliç (10), Kivircik (10), Merinos (10) and Karacabey Merino (8)) reared in Konya province conditions located in the Central Anatolia Region of Turkey. For measuring the predictive performance of three algorithms in Models I and II, goodness of fit criteria (coefficient of determination (R2%), adjusted coefficient of determination (Adj.R2%), coefficient of variation (CV%), SD ratio, Root Mean Square Error (RMSE), Relative Approximation Error (RAE), and Pearson correlation coefficient between actual and predicted values were calculated. For both Models, CHAID and CART were chosen as the best algorithms in the estimation of CCW trait, whereas only CHAID was the ideal tree-based algorithm in the estimation of BW trait. In conclusion, the determination of the best data mining algorithm on the estimation of BW and CCW traits might be utility for further researches linked with characterization of sheep breeds, and sheep breeding in very large flocks. Copyright 2017 Zoological Society of Pakistan.","Body weight; CART; CHAID; Cold carcass; Data mining; Exhaustive CHAID; Regression tree analysis","Ovis aries",2-s2.0-85031741328
"Štepanovský M., Ibrová A., Buk Z., Velemínská J.","Novel age estimation model based on development of permanent teeth compared with classical approach and other modern data mining methods",2017,"Forensic Science International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028028387&doi=10.1016%2fj.forsciint.2017.08.005&partnerID=40&md5=2ac377cb2c5b14819366043179129e35","In order to analyze and improve the dental age estimation in children and adolescents for forensic purposes, 22 age estimation methods were compared to a sample of 976 orthopantomographs (662 males, 314 females) of healthy Czech children and adolescents aged between 2.7 and 20.5 years. All methods are compared in terms of the accuracy and complexity and are based on various data mining methods or on simple mathematical operations. The winning method is presented in detail. The comparison showed that only three methods provide the best accuracy while remaining user-friendly. These methods were used to build a tabular multiple linear regression model, an M5P tree model and support vector machine model with first-order polynomial kernel. All of them have mean absolute error (MAE) under 0.7 years for both males and females. The other well-performing data mining methods (RBF neural network, K-nearest neighbors, Kstar, etc.) have similar or slightly better accuracy, but they are not user-friendly as they require computing equipment and the implementation as computer program. The lowest estimation accuracy provides the traditional model based on age averages (MAE under 0.96 years). Different relevancy of various teeth for the age estimation was found. This finding also explains the lowest accuracy of the traditional averages-based model. In this paper, a technique for missing data replacement for the cases with missing teeth is presented in detail as well as the constrained tabular multiple regression model. Also, we provide free age prediction software based on this wining model. © 2017 Elsevier B.V.","Age estimation; Data mining; Model; Population-specific standards",,2-s2.0-85028028387
"Gholizadeh A., Carmon N., Klement A., Ben-Dor E., Borůvka L.","Agricultural soil spectral response and properties assessment: Effects of measurement protocol and data mining technique",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860136&doi=10.3390%2frs9101078&partnerID=40&md5=0eb3561f5aa1db6c4b31f5f5a708a703","Soil spectroscopy has shown to be a fast, cost-effective, environmentally friendly, non-destructive, reproducible and repeatable analytical technique. Soil components, as well as types of instruments, protocols, sampling methods, sample preparation, spectral acquisition techniques and analytical algorithms have a combined influence on the final performance. Therefore, it is important to characterize these differences and to introduce an effective approach in order to minimize the technical factors that alter reflectance spectra and consequent prediction. To quantify this alteration, a joint project between Czech University of Life Sciences Prague (CULS) and Tel-Aviv University (TAU) was conducted to estimate Cox, pH-H2O, pH-KCl and selected forms of Fe and Mn. Two different soil spectral measurement protocols and two data mining techniques were used to examine seventy-eight soil samples from five agricultural areas in different parts of the Czech Republic. Spectral measurements at both laboratories were made using different ASD spectroradiometers. The CULS protocol was based on employing a contact probe (CP) spectral measurement scheme, while the TAU protocol was carried out using a CP measurement method, accompanied with the internal soil standard (ISS) procedure. Two spectral datasets, acquired from different protocols, were both analyzed using partial least square regression (PLSR) technique as well as the PARACUDA II®, a new data mining engine for optimizing PLSR models. The results showed that spectra based on the CULS setup (non-ISS) demonstrated significantly higher albedo intensity and reflectance values relative to the TAU setup with ISS. However, the majority of statistics using the TAU protocol was not noticeably better than the CULS spectra. The paper also highlighted that under both measurement protocols, the PARACUDA II® engine proved to be a powerful tool for providing better results than PLSR. Such initiative is not only a way to unlock current limitations of soil spectroscopy, but also offers considerable efficiency and cost- and time-saving possibilities, which lead to further improvements in prediction performance of spectral models. © 2017 by the authors.","Data mining; Internal soil standard; Protocol and standard; Soil spectroscopy","Agriculture; Cost effectiveness; Data mining; Engines; Manganese; Reflection; Soils; Analytical algorithms; Effective approaches; Partial least-square regression techniques; Prediction performance; Reflectance spectrum; Soil spectroscopies; Spectral acquisition; Spectral measurement; Soil surveys",2-s2.0-85032860136
"Kılıç Depren S., Aşkın Ö.E., Öz E.","Identifying the classification performances of educational data mining methods: A case study for TIMSS",2017,"Kuram ve Uygulamada Egitim Bilimleri",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029928696&doi=10.12738%2festp.2017.5.0634&partnerID=40&md5=affba4ddce44aa8cd791f98678a9cf47","Educational data mining (EDM) is a rapidly growing research area, and the outputs obtained from EDM shed light on educators’ and education planners’ efforts to make efficient decisions concerning educational strategies. However, a lack of work still exists on using EDM methods for international assessment studies such as the International Association for the Evaluation of Educational Achievement’s Trends in International Mathematics and Science Study (IEA’s TIMSS). This study aims to fill the gap in the current literature on the latest-released TIMSS 2011 data by applying a decision tree, a Bayesian network, a logistic regression, and neural networks. The best performing algorithm in classification based on several performance measures has been found for eighth-grade Turkish students’ mathematics data. During the construction of models, 11 student-based factors have been taken into account. The results show that logistic regression outperforms other algorithms in terms of measuring classification performance. The factor of student confidence has also been found as the most effective factor on eighth-grade students’ mathematics achievement. © 2017 EDAM.","Classification algorithms; Classification performance measures; Educational data mining; Mathematics achievement; TIMSS",,2-s2.0-85029928696
"Kim S., Park K., Kim M.-S., Yang B.R., Choi H.J., Park B.-J.","Data-mining for detecting signals of adverse drug reactions of fluoxetine using the Korea Adverse Event Reporting System (KAERS) database",2017,"Psychiatry Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021171239&doi=10.1016%2fj.psychres.2017.06.038&partnerID=40&md5=7c08aa345ad79a9b17e465c2921f9dfd","Selective serotonin reuptake inhibitors (SSRIs) have become one of the most broadly used medications in psychiatry. Fluoxetine is the first representative antidepressant SSRI drug approved by the Food and Drug Administration (FDA) in 1987. Safety information on fluoxetine use alone was less reported than its combined use with other drugs. There were no published papers on adverse drug reactions (ADRs) of fluoxetine analyzing spontaneous adverse events reports. We detected signals of the adverse drug reactions of fluoxetine by data mining using the Korea Adverse Events Reporting System (KAERS) database. We defined signals in this study by the reporting odds ratios (ROR), proportional reporting ratios (PRR), and information components (IC) indices. The KAERS database included 860,224 AE reports, among which 866 reports contained fluoxetine. We compared the labels of fluoxetine among the United States, UK, Germany, France, China, and Korea. Some of the signals, including emotional lability, myositis, spinal stenosis, paradoxical drug reaction, drug dependence, extrapyramidal disorder, adrenal insufficiency, and intracranial hemorrhage, were not labeled in the six countries. In conclusion, we identified new signals that were not known at the time of market approval. However, certain factors should be required for signal evaluation, such as clinical significance, preventability, and causality of the detected signals. © 2017 Elsevier Ireland Ltd","Adverse Drug Reaction; Data Mining; Fluoxetine; KAERS Database; Pharmacovigilance; Signal; Spontaneous adverse event reporting","C reactive protein; fluoxetine; serotonin uptake inhibitor; abdominal pain; abnormal dreaming; adrenal insufficiency; adverse outcome; alopecia; anorexia; anxiety disorder; Article; asthenia; brain hemorrhage; China; constipation; data mining; depression; disease exacerbation; dizziness; drug dependence; drug surveillance program; dyspepsia; dysphagia; dystonia; epidermolysis; extrapyramidal syndrome; female; France; Germany; hallucination; headache; heart palpitation; hepatitis; human; hyperkinesia; impotence; insomnia; Korea; lactation disorder; maculopapular rash; malaise; male; mania; medication error; mental function assessment; mental instability; muscle hypertonia; muscle spasm; myositis; nausea; nervousness; paradoxical drug reaction; paroniria; priority journal; psychosis; serotonin syndrome; sexual dysfunction; side effect; signal detection; somnolence; speech disorder; stomatitis; stupor; suicide attempt; tremor; United Kingdom; United States; urine incontinence; urticaria; vertebral canal stenosis; vesicular rash; vomiting; weight reduction; xerostomia; yawning",2-s2.0-85021171239
"Karadas K., Kadirhanogullari I.H.","Predicting honey production using data mining and artificial neural network algorithms in apiculture",2017,"Pakistan Journal of Zoology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031754463&doi=10.17582%2fjournal.pjz%2f2017.49.5.1611.1619&partnerID=40&md5=745b20132062ecd0f367d27a260b2295","This survey was conducted on all the 85 beekeeping farms collected with census study method in Igdir province of Turkey with the purpose of determining some factors influencing average honey yield (AHY) per beehive in the year 2014. For this purpose, predictive performances of several data mining algorithms (CART, CHAID, Exhaustive CHAID and MARS) and artificial neural network algorithm (Multilayer Perceptron, MLP) were evaluated comparatively. Several factors thought as independent variables in the survey were age of beekeeper (AB), education level (EL), number of full beehives (NFB), bee race (BR), the time spent in plateau (TSP), feed of autumn and spring (FAS), working period in apiculture during year (WPA), frequency of changing queen (FCQ), and controlling beehives in summer (CFB), respectively. Minimum beekeeping farm numbers for parent and child nodes were arranged as 8:4 in CART, CHAID and Exhaustive CHAID for attaining the best predictive performance in AHY. In the Exhaustive CHAID, only 3 independent variables, NFB, WPA and CFB were found statistically. In the CART algorithm, only NFB, WPA and AB independent variables were found significantly. In the MARS algorithm, significant independent variables were determined to be some main and interaction effects of NFB, FAS, WPA, EL, AB, FCQ and TSP. The significant order of the Pearson coefficients between actual and fitted values in AHY was MARS (0.913a) &gt; ANN (0.885ab) &gt; Exhaustive CHAID (0.786b) &gt; CHAID (0.769b) &gt; CART (0.744). It was concluded that the MARS algorithm having the best predictive accuracy among all the algorithms might offer a good solution to beekeepers in describing interactions of significant independent variables. Copyright 2017 Zoological Society of Pakistan.","Beekeeping; Data mining; Honey yield; Production economics.; Regression tree analysis","Apoidea",2-s2.0-85031754463
"Vathsala H., Koolagudi S.G.","Long-range prediction of Indian summer monsoon rainfall using data mining and statistical approaches",2017,"Theoretical and Applied Climatology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978127340&doi=10.1007%2fs00704-016-1862-2&partnerID=40&md5=0d80dfc91b19c0f2792d30c05bd6bef1","This paper presents a hybrid model to better predict Indian summer monsoon rainfall. The algorithm considers suitable techniques for processing dense datasets. The proposed three-step algorithm comprises closed itemset generation-based association rule mining for feature selection, cluster membership for dimensionality reduction, and simple logistic function for prediction. The application of predicting rainfall into flood, excess, normal, deficit, and drought based on 36 predictors consisting of land and ocean variables is presented. Results show good accuracy in the considered study period of 37years (1969–2005). © 2016, Springer-Verlag Wien.",,"data mining; long range forecast; monsoon; prediction; rainfall; statistical analysis; summer; India",2-s2.0-84978127340
"Hafiz P., Nematollahi M., Boostani R., Jahromi B.N.","Predicting implantation outcome of in vitro fertilization and intracytoplasmic sperm injection using data mining techniques",2017,"International Journal of Fertility and Sterility",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028951240&doi=10.22074%2fifs.2017.4882&partnerID=40&md5=352a4224f8a798036f26a2e969bcc486","Background: In vitro fertilization (IVF) and intracytoplasmic sperm injection (ICSI) are tw o important subsets of the assisted reproductive techniques, used for the treatment of infertility. Predicting implantation outcome of IVF/TCSI or the chance of pregnancy is essential for infertile couples, since these treatments are complex and expensive with a low probability of conception. Materials and Methods: In this cross-sectional study the data of 486 patients were collected using census method. The IVF/ICSI dataset contains 29 variables along with an identifier for each patient that is either negative or positive. Mean accuracy and mean area under the receiver operating characteristic (ROC) ciuve are calculated for the classifiers. Sensitivity, specificity, positive and negative predictive values, and likelihood ratios of classifiers are employed as indicators of performance. The state-of-art classifiers which are candidates for this study include support vector machines, recursive partitioning (RPART), random forest (RF), adaptive boosting, and one-nearest neighbor. Results: RF and RPART outperform the other comparable methods. The results revealed the areas under the ROC curve (AUC) as 84.23 and 82.05%. respectively. The importance of IVF/ICSI features was extracted from the output of RPART. Our findings demonstrate that the probability of pregnancy is low for women aged above 38. Conclusion: Classifiers RF and RPART are better at predicting IVF/ICSI cases compared to other decision makers that were tested in our study. Elicited decision rules of RPART determine useful predictive features of IVF/ICSI. Out of 20 factors, the age of woman, number of developed embryos, and serum estradiol level on the day of human chorionic gonadotropin administration are the tluee best features for such prediction. © 2017, Royan Institute (ACECR). All rights reserved.","Clinical decision support; Data mining; In vitro fertilization; Intracytoplasmic sperm injection","chorionic gonadotropin; estradiol; follitropin; Muellerian inhibiting factor; adult; aged; Article; clinical decision making; controlled study; cross-sectional study; data mining; embryo development; embryo transfer; female; germinal vesicle; hospital admission; human; implantation; in vitro fertilization; infertility; information processing; intracytoplasmic sperm injection; major clinical study; male; maximum likelihood method; metaphase; predictive value; pregnancy outcome; random forest; scoring system; sensitivity and specificity; support vector machine",2-s2.0-85028951240
"Gad A., Manuel A.T., K. R. J., John L., Sajeev R., V. G. S.P., Abdul Jaleel U.C.","Virtual screening and repositioning of inconclusive molecules of beta-lactamase Bioassays—A data mining approach",2017,"Computational Biology and Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027397208&doi=10.1016%2fj.compbiolchem.2017.07.005&partnerID=40&md5=5c22fb33b71c69f25b1ba8a734267691","This study focuses on the best possible way forward in utilizing inconclusive molecules of PubChem bioassays AID 1332, AID 434987 and AID 434955, which are related to beta-lactamase inhibitors of Mycobacterium tuberculosis (Mtb). The inadequacy in the experimental methods that were observed during the invitro screening resulted in an inconclusive dataset. This could be due to certain moieties present within the molecules. In order to reconsider such molecules, insilico methods can be suggested in place of invitro methods For instance, datamining and medicinal chemistry methods: have been adopted to prioritise the inconclusive dataset into active or inactive molecules. These include the Random Forest algorithm for dataminning, Lilly MedChem rules for virtually screening out the promiscuity, and Self Organizing Maps (SOM) for clustering the active molecules and enlisting them for repositioning through the use of artificial neural networks. These repositioned molecules could then be prioritized for downstream drug discovery analysis. © 2017 Elsevier Ltd","Artificial neural networks; Inconclusive molecules; Mycobacterium tuberculosis; PubChem bioassays; Self organizing maps","Conformal mapping; Decision trees; Molecules; Neural networks; Self organizing maps; Active molecules; Drug discovery; Experimental methods; Medicinal chemistry; Mycobacterium tuberculosis; Pubchem; Random forest algorithm; Virtual Screening; Data mining",2-s2.0-85027397208
"Farrell C.-J.L., Nguyen L., Carter A.C.","Data mining for age-related TSH reference intervals in adulthood",2017,"Clinical Chemistry and Laboratory Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028500373&doi=10.1515%2fcclm-2016-1123&partnerID=40&md5=d4e0a0a3efa9c6750b60901537e4cf2d",[No abstract available],"Bhattacharya analysis; reference interval; reference range; thyroid-stimulating hormone (TSH)","thyrotropin; thyrotropin; adult; adulthood; age; aged; analytic method; data mining; female; human; immunoassay analyzer; Letter; male; measurement precision; priority journal; reference value; subclinical hypothyroidism; data mining; standards; Adult; Age Factors; Data Mining; Humans; Reference Values; Thyrotropin",2-s2.0-85028500373
"Barati M., Bai Q., Liu Q.","Mining semantic association rules from RDF data",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021971477&doi=10.1016%2fj.knosys.2017.07.009&partnerID=40&md5=8762b7719f9fa7c02aae0a94181c53a9","The Semantic Web opens up new opportunities for the data mining research. Semantic Web data is usually represented in the RDF triple format (subject, predicate, object). Large RDF-style Knowledge Bases contain hundreds of millions of RDF triples that represent knowledge in a machine-understandable format. Association rule mining is one of the most effective techniques for detecting frequent patterns. In the context of Semantic Web data mining, most existing methods rely on users intervention that is time-consuming and error-prone due to a large amount of data. Meanwhile, rule quality factors (e.g. support and confidence) usually consider knowledge at the instance-level. Namely, these factors disregard the knowledge embedded at the schema-level. In this paper, we demonstrate that ignoring knowledge encoded at the schema-level negatively impacts the interpretation of discovered rules. We introduce an approach called SWARM (Semantic Web Association Rule Mining) that automatically mines Semantic Association Rules from RDF data. The main achievement of SWARM is to reveal common behavioural patterns associated with knowledge at the instance-level and schema-level. We discuss how to utilize knowledge encoded at the schema-level to add more semantics to the rules. We compare the semantic of rules discovered by SWRAM with one of the latest approaches in this field to show the importance of considering schema-level knowledge. Initial experiments performed on RDF-style Knowledge Bases demonstrate the effectiveness of the proposed approach. © 2017 Elsevier B.V.","Association rule mining; Knowledge discovery; Ontology; Semantic Web data","Association rules; Data mining; Ontology; Error prones; Large amounts; Quality factors; RDF data; RDF triples; Semantic associations; Style knowledge; Support and confidence; Semantic Web",2-s2.0-85021971477
"Tanbeer S.K., Hassan M.M., Almogren A., Zuair M., Jeong B.-S.","Scalable regular pattern mining in evolving body sensor data",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966701620&doi=10.1016%2fj.future.2016.04.008&partnerID=40&md5=109ca9ff85953d72a710be1d5781d686","The recent emergence of body sensor networks (BSNs) has made it easy to continuously collect and process various health-oriented data related to temporal, spatial and vital sign monitoring of a patient. As such, discovering or mining interesting knowledge from the BSN data stream is becoming an important issue to promote and assist important decision making in healthcare. In this paper, we focus on mining the inherent regularity of different parameter readings obtained from different body sensors related to vital sign data of a patent for the purpose of following up health condition to prevent some kinds of chronic diseases. Specifically, we design and develop an efficient and scalable regular pattern mining technique that can mine the complete set of periodically/regularly occurring patterns in BSN data stream based on a user-specified periodicity/regularity threshold for the data and the subject. Various experiments in centralized and distributed environment were carried on both real and synthetic data to validate the efficiency of the proposed scalable regular pattern mining technique as compared to state-of-the-art approaches. © 2016 Elsevier B.V.","Body sensor network; Decision support; Healthcare; Parallel and distributed mining; Regular pattern mining","Data communication systems; Data mining; Decision making; Decision support systems; Health care; Patient monitoring; Wearable sensors; Body sensor networks (BSNs); Chronic disease; Decision supports; Distributed environments; Health condition; Regular patterns; State-of-the-art approach; Vital sign monitoring; Body sensor networks",2-s2.0-84966701620
"Yang X., Lo D., Li L., Xia X., Bissyandé T.F., Klein J.","Characterizing malicious Android apps by mining topic-specific data flow signatures",2017,"Information and Software Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018397382&doi=10.1016%2fj.infsof.2017.04.007&partnerID=40&md5=18a6fec0d5b317531c38eb165093d212","Context: State-of-the-art works on automated detection of Android malware have leveraged app descriptions to spot anomalies w.r.t the functionality implemented, or have used data flow information as a feature to discriminate malicious from benign apps. Although these works have yielded promising performance, we hypothesize that these performances can be improved by a better understanding of malicious behavior. Objective: To characterize malicious apps, we take into account both information on app descriptions, which are indicative of apps’ topics, and information on sensitive data flow, which can be relevant to discriminate malware from benign apps. Method: In this paper, we propose a topic-specific approach to malware comprehension based on app descriptions and data-flow information. First, we use an advanced topic model, adaptive LDA with GA, to cluster apps according to their descriptions. Then, we use information gain ratio of sensitive data flow information to build so-called “topic-specific data flow signatures”. Results: We conduct an empirical study on 3691 benign and 1612 malicious apps. We group them into 118 topics and generate topic-specific data flow signature. We verify the effectiveness of the topic-specific data flow signatures by comparing them with the overall data flow signature. In addition, we perform a deeper analysis on 25 representative topic-specific signatures and yield several implications. Conclusion: Topic-specific data flow signatures are efficient in highlighting the malicious behavior, and thus can help in characterizing malware. © 2017 Elsevier B.V.","Data flow signature; Empirical study; Malware characterization; Topic-specific","Computer crime; Data mining; Data transfer; Malware; Automated detection; Data flow; Data-flow information; Empirical studies; Information gain ratio; Malicious behavior; State of the art; Topic-specific; Android (operating system)",2-s2.0-85018397382
"Lucas T., Silva T.C.P.B., Vimieiro R., Ludermir T.B.","A new evolutionary algorithm for mining top-k discriminative patterns in high dimensional data",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021090611&doi=10.1016%2fj.asoc.2017.05.048&partnerID=40&md5=463152eb916833694e78cddf7fe1a80e","This paper presents an evolutionary algorithm for Discriminative Pattern (DP) mining that focuses on high dimensional data sets. DPs aims to identify the sets of characteristics that better differentiate a target group from the others (e.g. successful vs. unsuccessful medical treatments). It becomes more natural to extract information from high dimensionality data sets with the increase in the volume of data stored in the world (30 GB/s only in the Internet). There are several evolutionary approaches for DP mining, but none focusing on high-dimensional data. We propose an evolutionary approach attributing features that reduce the cost of memory and processing in the context of high-dimensional data. The new algorithm thus seeks the best (top-k) patterns and hides from the user many common parameters in other evolutionary heuristics such as population size, mutation and crossover rates, and the number of evaluations. We carried out experiments with real-world high-dimensional and traditional low dimensional data. The results showed that the proposed algorithm was superior to other approaches of the literature in high-dimensional data sets and competitive in the traditional data sets. © 2017 Elsevier B.V.","Discriminative patterns; Evolutionary algorithms; High dimensional data; Subgroup discovery","Clustering algorithms; Data mining; Population statistics; Discriminative patterns; Evolutionary approach; Evolutionary heuristics; Extract informations; High dimensional data; High dimensionality; Medical treatment; Subgroup discovery; Evolutionary algorithms",2-s2.0-85021090611
"Zahraie B., Nasseri M., Nematizadeh F.","Exploring spatiotemporal meteorological correlations for basin scale meteorological drought forecasting using data mining methods",2017,"Arabian Journal of Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030644751&doi=10.1007%2fs12517-017-3211-x&partnerID=40&md5=c760a1e188511d1c962ea47815e97173","In this paper, two data mining methods, support vector machine (SVM) and group method of data handling (GMDH), were used to identify spatiotemporal meteorological correlations, which can be used to forecast basin scale seasonal droughts. Standardized Precipitation Index (SPI) was used as a meteorological drought severity index. The case study of this paper consists of the basins of four major dams in Iran that supply domestic water demands of Tehran, the capital city of Iran. A GMDH and an SVM model optimized by particle swarm optimization (PSO) were used to predict seasonal SPIs in the fall, winter, spring, and some combined seasons. The historical time series of the meteorological variables including air temperature and geopotential height at the surface, and 300, 500, 700, and 850 mbar levels in the geographical zone covering 10 to 60° north latitudes and 0 to 90° east longitudes were selected as the model predictors. Average mutual information (AMI) index was used for feature selection among the mentioned predictors. The selected predictors in the months of April to August were used as the SVM and GMDH inputs. The results showed that the seasonal SPI values could be forecasted by the proposed model with 2- to 5-month lead-time with enough accuracy. Hence, the proposed method can be used in mid-term water resource management in the study area. © 2017, Saudi Society for Geosciences.","Average mutual information (AMI); Group method of data handling (GMDH); Meteorological drought forecasting; Particle swarm optimization (PSO); Standardized precipitation index (SPI); Support vector machine (SVM)",,2-s2.0-85030644751
"Roy K., Choudhary A., Jayapradha J.","Product recommendations using data mining and machine learning algorithms",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031755768&partnerID=40&md5=56630d04eba586c6a5dd057fb7ea3215","Data Mining is a cross-disciplinary field that concentrates on discovering properties of data sets. There are different approaches to discovering properties of data sets and Machine Learning is one of them. Machine Learning is a sub-field of data science that focuses on designing algorithms that can learn from and make predictions on the data. With the increase in the demand for the e-commerce websites, lots of information arises due to which the users face difficulty in finding the relevant information matching their preferences. Thus, we represent a system which will recommend similar food products to the user based on his purchase. The Food Product will be recommended based on the day to day health diseases of the user. The user profile is formed in which health complication of the user is there. The dataset for Recommendation System comprises of 2075 food items. We will apply K-nutrient algorithm to realize the Recommendation System. We will also implement Machine Learning algorithms such as Support Vector Machine (SVM) and Random Forest. In addition to this, the comparison between SVM and Random Forest is performed and SVM outperforms Random Forest algorithm as it shows an increase in the performance. © 2006-2017 Asian Research Publishing Network (ARPN).","Collaborative filtering; Health hazard; Random forest; Recommendation system; Support vector machine; User profile",,2-s2.0-85031755768
"Maacha L., Jaffal M., Jarni A., Kchikach A., Mouguina E.M., Zouhair M., Ennaciri A., Saddiqi O.","A contribution of airborne magnetic, gamma ray spectrometric data in understanding the structure of the Central Jebilet Hercynian massif and implications for mining",2017,"Journal of African Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025177085&doi=10.1016%2fj.jafrearsci.2017.07.012&partnerID=40&md5=c38e887fda873dd23de42a8e4edcda4a","The Central Jebilet massif, well-known for its mining potential, has been explored for several years by the Managem Group, in collaboration with the National Office of Hydrocarbons and Mines (ONHYM), Morocco. This massif was surveyed by high-resolution magnetic and gamma-ray spectrometric methods in 1997 as part of a broader exploration program. In order to better exploit the results of this survey in understanding the structure of the Central Jebilet massif, we performed a series of processing of the collected data. The qualitative interpretation of this data highlighted the various magnetic domains, structures (e.g., lineaments and faults), and mafic intrusions. Euler deconvolution calculations provided estimates of the spatial location and depth of the magnetic sources, and spectral analysis of the magnetic data allowed further refinement of these depth estimates. Quantitative interpretation of some anomalies associated with exposed gossans allowed the characterization of their causative bodies, inferred to be sulfide deposits. The magnetic character of both the potential massive-sulfide bodies and the basic magmatic rocks (gabbro) were determined by the aeromagnetic data. Gamma ray spectrometric data has helped facilitate lithological discrimination and alteration zones, based on the radio-elemental distribution in the area. For example, the Thorium to Potassium ratio (Th/K) was used to highlight potassic alteration zones associated with massive-sulfide deposits in the Central Jebilet. The combined magnetic and radiometric study reveals the magnetic character of the Central Jebilet gossans, due to the content of pyrrhotite, which along with potassic alterations, has been recognized in all the known deposits of the Marrakech region. The results of this geophysical campaign supplement the existing geological and structural maps of Central Jebilet massif. © 2017 Elsevier Ltd","Central Jebilet; Gamma ray spectrometry; Hercynian; Magnetism; Mining exploration; Morocco","aeromagnetic survey; data interpretation; gamma ray spectrometry; Hercynian orogeny; magnetic field; mining; paleomagnetism; Jebilet; Marrakech-Safi; Morocco",2-s2.0-85025177085
"Katpatal Y.B., Patil S.A., Singh C.K.","Estimation of Sediment Yield within Mining Watershed to Assess Impact of Mine Dumps Using Satellite Data: Modified Approach",2017,"Journal of Environmental Engineering (United States)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025831253&doi=10.1061%2f%28ASCE%29EE.1943-7870.0001261&partnerID=40&md5=6d78873e23d8df7029090693afcd861e","Opencast mining disturbs large areas of land, resulting in a decline in the groundwater table, increased sedimentation rates, and groundwater contamination. The sediment yield from the mined areas can be several times greater than from undisturbed areas. The sediment yields can be greater in the regions with large overburden (OB) dumps, which facilitate surface erosion because they consist of unconsolidated loose materials and induce changes in surface morphology. The present study proposes a modified approach using geoinformatics to estimate the sediment yield in a microwatershed dominated by coal mines and OB dumps. A modified equation has been proposed that uses the drainage and the vegetative cover, the major inputs required for sediment yield estimation. The sediment yields have been estimated for the years 1989 and 2007, during which the OB dump area increased significantly. Land use classes were extracted for 1989 and 2007, using multispectral Landsat and Indian remote sensing (IRS) satellite data. Sensitivity analysis was carried out to examine the performance of the modified approach. The outcome of the study corroborates the utility of multidate, multiresolution satellite data for the estimation of sediment yield. In the absence of field data, the study reveals that satellite data provides the information on the impact of the OB dumps on the quantities of the sediment yields as a result of the enhanced erosion rates. The results obtained in the present study of a mining dominated microwatershed will help administrators and authorities to take the necessary preventive measures for controlling the erosion through proper mine management. © 2017 American Society of Civil Engineers.","Coal mine; Micro-watershed; Remote sensing; Sediment yield","Erosion; Groundwater; Groundwater pollution; Remote sensing; Satellites; Sediments; Sensitivity analysis; Watersheds; Ground water table; Groundwater contamination; Modified equation; Multi-resolution satellite datum; Open-cast mining; Preventive measures; Sediment yields; Sedimentation rates; Coal mines; coal mine; drainage; erosion rate; Landsat; overburden; remote sensing; satellite data; sediment yield; vegetation cover; watershed",2-s2.0-85025831253
"Salah S., Akbarinia R., Masseglia F.","Data placement in massively distributed environments for fast parallel mining of frequent itemsets",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016002104&doi=10.1007%2fs10115-017-1041-5&partnerID=40&md5=88f34555380f96896aa87d9f9737bb37","Frequent itemset mining presents one of the fundamental building blocks in data mining. However, despite the crucial recent advances that have been made in data mining literature, few of both standard and improved solutions scale. This is particularly the case when (1) the quantity of data tends to be very large and/or (2) the minimum support is very low. In this paper, we address the problem of parallel frequent itemset mining (PFIM) in very large databases and study the impact and effectiveness of using specific data placement strategies in a massively distributed environment. By offering a clever data placement and an optimal organization of the extraction algorithms, we show that the arrangement of both the data and the different processes can make the global job either completely inoperative or very effective. In this setting, we propose two different highly scalable, PFIM algorithms, namely P2S (parallel-2-steps) and PATD (parallel absolute top-down). P2S algorithm allows discovering itemsets from large databases in two simple, yet efficient parallel jobs, while PATD renders the mining process of very large databases more simple and compact. Its mining process is made up of only one parallel job, which dramatically reduces the running time, the communication cost and the energy power consumption overhead in a distributed computational platform. Our different proposed approaches have been extensively evaluated on massive real-world data sets. The experimental results confirm the effectiveness and scalability of our proposals by the important scale-up obtained with very low minimum supports compared to other alternatives. © 2017, Springer-Verlag London.","Data Placement; Frequent Itemsets; MapReduce; Massive Distribution",,2-s2.0-85016002104
"Tennant M., Stahl F., Rana O., Gomes J.B.","Scalable real-time classification of data streams with concept drift",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018670513&doi=10.1016%2fj.future.2017.03.026&partnerID=40&md5=71010a3a68c9611d92562c1649b0f730","Inducing adaptive predictive models in real-time from high throughput data streams is one of the most challenging areas of Big Data Analytics. The fact that data streams may contain concept drifts (changes of the pattern encoded in the stream over time) and are unbounded, imposes unique challenges in comparison with predictive data mining from batch data. Several real-time predictive data stream algorithms exist, however, most approaches are not naturally parallel and thus limited in their scalability. This paper highlights the Micro-Cluster Nearest Neighbour (MC-NN) data stream classifier. MC-NN is based on statistical summaries of the data stream and a nearest neighbour approach, which makes MC-NN naturally parallel. In its serial version MC-NN is able to handle data streams, the data does not need to reside in memory and is processed incrementally. MC-NN is also able to adapt to concept drifts. This paper provides an empirical study on the serial algorithm's speed, adaptivity and accuracy. Furthermore, this paper discusses the new parallel implementation of MC-NN, its parallel properties and provides an empirical scalability study. © 2017 The Author(s)","Adaptation to concept drift; High velocity data streams; Parallel data stream classification","Big data; Data communication systems; Data mining; Predictive analytics; Scalability; Classification of data; Concept drifts; Data stream; Data stream algorithms; High-throughput data; Parallel data; Parallel implementations; Predictive data mining; Classification (of information)",2-s2.0-85018670513
"Sun Z., Ji X.","HDAC high-dimensional data aggregation control algorithm for big data in wireless sensor networks",2017,"International Journal of Information Technology and Web Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028046969&doi=10.4018%2fIJITWE.2017100105&partnerID=40&md5=90d6fc8fb810955adc1e544b5b01f29e","The process of high-dimensional data is a hot research area in data mining technology. Due to sparsity of the high-dimensional data, there is significant difference between the high-dimensional space and the low-dimensional space, especially in terms of the data process. Many sophisticated algorithms of low-dimensional space cannot achieve the expected effect, even cannot be used in the high-dimensional space. Thus, this paper proposes a High-dimensional Data Aggregation Control Algorithm for Big Data (HDAC). The algorithm uses information to eliminate the dimension not matching with the specified requirements. Then it uses the principal components method to analyze the rest dimension. Thus, the simplest method is used to reduce the calculation of dimensionality reduction as can as it possible. In the process of data aggregation, the self-adaptive data aggregation mechanism is used to reduce the phenomenon of network delay. Finally, the simulation shows that the algorithm can improve the performance of node energy-consumption, rate of the data post-back and the data delay. © 2017, IGI Global.","Big Data; Control Algorithm; Data Aggregation; High-Dimensional Data; Wireless Sensor Network","Algorithms; Clustering algorithms; Data mining; Energy utilization; Wireless sensor networks; Data aggregation; Data mining technology; Dimensionality reduction; Expected effects; High dimensional data; High dimensional spaces; Low-dimensional spaces; Principal Components; Big data",2-s2.0-85028046969
"Li Q., Li G., Niu W., Cao Y., Chang L., Tan J., Guo L.","Boosting imbalanced data learning with Wiener process oversampling",2017,"Frontiers of Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995481288&doi=10.1007%2fs11704-016-5250-y&partnerID=40&md5=9c322e961049862ed9bff8fa8e463560","Learning from imbalanced data is a challenging task in a wide range of applications, which attracts significant research efforts from machine learning and data mining community. As a natural approach to this issue, oversampling balances the training samples through replicating existing samples or synthesizing new samples. In general, synthesization outperforms replication by supplying additional information on the minority class. However, the additional information needs to follow the same normal distribution of the training set, which further constrains the new samples within the predefined range of training set. In this paper, we present the Wiener process oversampling (WPO) technique that brings the physics phenomena into sample synthesization. WPO constructs a robust decision region by expanding the attribute ranges in training set while keeping the same normal distribution. The satisfactory performance of WPO can be achieved with much lower computing complexity. In addition, by integrating WPO with ensemble learning, the WPOBoost algorithm outperformsmany prevalent imbalance learning solutions. © 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg.","AdaBoost; ensemble learning; imbalanced-data learning; oversampling; Wiener process","Adaptive boosting; Artificial intelligence; Data mining; Learning systems; Normal distribution; Computing complexity; Data mining community; Ensemble learning; Imbalanced data; Natural approaches; Over sampling; Research efforts; Wiener process; Random processes",2-s2.0-84995481288
"Bogdanov P., Dereli N., Dang X.-H., Bassett D.S., Wymbs N.F., Grafton S.T., Singh A.K.","Learning about learning: Mining human brain sub-network biomarkers from fMRI data",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031008632&doi=10.1371%2fjournal.pone.0184344&partnerID=40&md5=20110f3d6930f680b3cf8824416abca4","Modeling the brain as a functional network can reveal the relationship between distributed neurophysiological processes and functional interactions between brain structures. Existing literature on functional brain networks focuses mainly on a battery of network properties in “resting state” employing, for example, modularity, clustering, or path length among regions. In contrast, we seek to uncover functionally connected subnetworks that predict or correlate with cohort differences and are conserved within the subjects within a cohort. We focus on differences in both the rate of learning as well as overall performance in a sensorimotor task across subjects and develop a principled approach for the discovery of discriminative subgraphs of functional connectivity based on imaging acquired during practice. We discover two statistically significant subgraph regions: one involving multiple regions in the visual cortex and another involving the parietal operculum and planum temporale. High functional coherence in the former characterizes sessions in which subjects take longer to perform the task, while high coherence in the latter is associated with high learning rate (performance improvement across trials). Our proposed methodology is general, in that it can be applied to other cognitive tasks, to study learning or to differentiate between healthy patients and patients with neurological disorders, by revealing the salient interactions among brain regions associated with the observed global state. The discovery of such significant discriminative subgraphs promises a better data-driven understanding of the dynamic brain processes associated with high-level cognitive functions. © 2017 Bogdanov et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"biological marker; biological marker; adult; Article; brain region; cohort analysis; controlled study; functional connectivity; functional magnetic resonance imaging; functional neuroimaging; human; human experiment; learning; normal human; parietal operculum; sensorimotor function; task performance; visual cortex; young adult; brain mapping; female; image processing; learning; male; nuclear magnetic resonance imaging; parietal lobe; physiology; procedures; psychomotor performance; Adult; Biomarkers; Brain Mapping; Female; Humans; Image Processing, Computer-Assisted; Learning; Magnetic Resonance Imaging; Male; Parietal Lobe; Psychomotor Performance; Visual Cortex",2-s2.0-85031008632
"Gagliardi F., Ambrogio G., Ciancio C., Filice L.","Metamodeling technique for designing reengineered processes by historical data",2017,"Journal of Manufacturing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030150916&doi=10.1016%2fj.jmsy.2017.09.004&partnerID=40&md5=76785475a869fc79227970410c4c06fb","Physical-driven or simulation-driven experiments and data mining algorithms are combined for the identification of input–output relationships in complex manufacturing processes. To overcome time and cost consuming procedures, mathematical models have been applied finding and evaluating the factors that mostly affect the examined responses. In this context, a novel metamodeling technique was developed. This is able to use historical information on similar problems minimizing the amount of data necessary to the design of reengineered processes. The procedure was validated by applying it to the porthole extrusion optimizing the die geometries used for processing profiles characterized by various cross sections. © 2017 The Society of Manufacturing Engineers","Dynamic system; Extrusion; Manufacturing; Metamodeling technique; Optimization; Response surface","Data mining; Dynamical systems; Extrusion; Manufacture; Optimization; Complex manufacturing process; Data mining algorithm; Die geometry; Historical data; Historical information; Meta-modeling technique; Response surface; Design",2-s2.0-85030150916
"Ji H.-K., Sun Q.-S., Yuan Y.-H., Ji Z.-X., Zhang G.-Q., Feng L.","Dual structural consistency based multi-modal correlation propagation projections for data representation",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991070778&doi=10.1007%2fs11042-016-3993-y&partnerID=40&md5=74dfeba2c31312fecbd1e15c9f7eb987","Canonical correlation analysis (CCA) is a powerful tool for analyzing multi-dimensional paired data. However, when facing semi-supervised multi-modal data (Also called multi-view Hou et al. (Pattern Recog 43(3):720–730, 2010) or multi-represented Kailing et al. (Clustering multi-represented objects with noise. In: Proceedings of the eighth Pacific-Asia conference on knowledge discovery and data mining (PAKDD). Sydney, Australia, pp 394–403) data. For convenience, we will uniformly call them multi-modal data hereafter.) which widely exist in real-world applications, CCA usually performs poorly due to ignoring useful supervised information. Meanwhile, due to the limited labeled training samples in the semi-supervised scenario, supervised extensions of CCA suffer from overfitting. Several semi-supervised extensions of CCA have been proposed recently. Nevertheless, they either just utilize the global structural information captured from the unlabeled data, or propagate label information by discovering the affinities just between the labeled and unlabeled data points in advance. In this paper, we propose a robust multi-modal semi-supervised feature extraction and fusion framework, termed as dual structural consistency based multi-modal correlation propagation projections (SCMCPP). SCMCPP guarantees the consistency between representation structure and hypotaxis structure in each modality and ensures the consistency of hypotaxis structure between two different modalities. By iteratively propagating labels and learning affinities, discriminative information of both given labels and estimated labels is utilized to improve the affinity construction and infer the remaining unknown labels. Moreover, probabilistic within-class scatter matrices in each modality and probabilistic correlation matrix between two modalities are constructed to enhance the discriminative power of features. Extensive experiments on several benchmark face databases demonstrate the effectiveness of our approach. © 2016, Springer Science+Business Media New York.","Correlation analysis; Feature extraction and fusion; Multi-modal semi-supervised learning; Structural consistency","Correlation methods; Data mining; Extraction; Feature extraction; Matrix algebra; Supervised learning; Canonical correlation analysis; Correlation analysis; Correlation propagation; Knowledge discovery and data minings; Labeled and unlabeled data; Multi-represented objects; Semi- supervised learning; Structural consistency; Modal analysis",2-s2.0-84991070778
"Barkhordari M., Niamanesh M.","Atrak: a MapReduce-based data warehouse for big data",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018823458&doi=10.1007%2fs11227-017-2037-3&partnerID=40&md5=4e7e67144e1d9dc9cf38adf8cc10f3b7","As warehouse data volumes expand, single-node solutions can no longer analyze the immense volume of data. Therefore, it is necessary to use shared nothing architectures such as MapReduce. Inter-node data segmentation in MapReduce creates node connectivity issues, network congestion, improper use of node memory capacity and inefficient processing power. In addition, it is not possible to change dimensions and measures without changing previously stored data and big dimension management. In this paper, a method called Atrak is proposed, which uses a unified data format to make Mapper nodes independent to solve the data management problem mentioned earlier. The proposed method can be applied to star schema data warehouse models with distributive measures. Atrak increases query execution speed by employing node independence and the proper use of MapReduce. The proposed method was compared to established methods such as Hive, Spark-SQL, HadoopDB and Flink. Simulation results confirm improved query execution speed of the proposed method. Using data unification in MapReduce can be used in other fields, such as data mining and graph processing. © 2017, Springer Science+Business Media New York.","Big data; Data locality; Data warehouse; MapReduce","Data mining; Data warehouses; Information management; Data locality; Data management problems; Data segmentation; Graph processing; Map-reduce; Network congestions; Node connectivity; Processing power; Big data",2-s2.0-85018823458
"Kim J., Naganathan H., Moon S.-Y., Chong W.K.O., Ariaratnam S.T.","Applications of Clustering and Isolation Forest Techniques in Real-Time Building Energy-Consumption Data: Application to LEED Certified Buildings",2017,"Journal of Energy Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025089060&doi=10.1061%2f%28ASCE%29EY.1943-7897.0000479&partnerID=40&md5=228ced2856ead5a99392073930fece9b","Buildings are the largest consumer of energy in the United States from various sectors that includes transportation, industry, commercial, and residential buildings. Leadership in Energy and Environmental Design (LEED) certification program, home energy rating system (HERS), and American Society of Heating, Refrigerating and Air-conditioning Engineers (ASHRAE) standards are developed to improve the energy efficiency of the commercial and residential buildings. However, these programs, codes, and standards are used before or during the design and construction phases. For this reason, it is challenging to track whether buildings still could be energy efficient post construction. The primary purpose of this study was to detect the anomalies from the energy consumption dataset of LEED institutional buildings. The anomalies are identified using two different data mining techniques, which are clustering, and isolation Forest (iForest). This paper demonstrates an integrated data mining approach that helps in evaluating LEED energy and atmosphere (EA) credits after construction. © 2017 American Society of Civil Engineers.",,"Buildings; Commercial refrigeration; Data mining; Energy utilization; Environmental design; Forestry; Housing; Building energy consumption; Certification programs; Design and construction; Institutional building; Integrated data mining; Leadership in energy and environmental designs; Leed-certified buildings; Residential building; Energy efficiency",2-s2.0-85025089060
"Srivastava R., Bhatia M.P.S.","Real-time unspecified major sub-events detection in the twitter data stream that cause the change in the sentiment score of the targeted event",2017,"International Journal of Information Technology and Web Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028043880&doi=10.4018%2fIJITWE.2017100101&partnerID=40&md5=8dbff2dd6f587f20e784283618a6e5ff","Twitter behaves as a social sensor of the world. The tweets provided by the Twitter Firehose reveal the properties of big data (i.e. volume, variety, and velocity). With millions of users on Twitter, the Twitter's virtual communities are now replicating the real-world communities. Consequently, the discussions of real world events are also very often on Twitter. This work has performed the real-time analysis of the tweets related to a targeted event (e.g. election) to identify those potential sub-events that occurred in the real world, discussed over Twitter and cause the significant change in the aggregated sentiment score of the targeted event with time. Such type of analysis can enrich the real-time decision-making ability of the event bearer. The proposed approach utilizes a threestep process: (1) Real-time sentiment analysis of tweets (2) Application of Bayesian Change Points Detection to determine the sentiment change points (3) Major sub-events detection that have influenced the sentiment of targeted event. This work has experimented on Twitter data of Delhi Election 2015. © 2017, IGI Global.","Bayesian Change Point Detection; Big Data Stream; Event Detection; On-Line Micro-Texts; Real Time Sentiment Analysis; Sentiment Analysis; Sentiment Change Point; Text Stream Mining; Twitter","Data communication systems; Data mining; Decision making; Social networking (online); Virtual reality; Change point detection; Change-points; Data stream; Event detection; On-Line Micro-Texts; Sentiment analysis; Text streams; Twitter; Big data",2-s2.0-85028043880
"Dawar S., Goyal V., Bera D.","A hybrid framework for mining high-utility itemsets in a sparse transaction database",2017,"Applied Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018714668&doi=10.1007%2fs10489-017-0932-1&partnerID=40&md5=37288c711f8d698c5c8722e169897169","High-utility itemset mining aims to find the set of items with utility no less than a user-defined threshold in a transaction database. High-utility itemset mining is an emerging research area in the field of data mining and has important applications in inventory management, query recommendation, systems operation research, bio-medical analysis, etc. Currently, known algorithms for this problem can be classified as either 1-phase or 2-phase algorithms. The 2-phase algorithms typically consist of tree-based algorithms which generate candidate high-utility itemsets and verify them later. A tree data structure generates candidate high-utility itemsets quickly by storing some upper bound utility estimate at each node. The 1-phase algorithms typically consist of inverted-list based and transaction projection based algorithms which avoid the generation of candidate high-utility itemsets. The inverted list and transaction projection allows computation of exact utility estimates. We propose a novel hybrid framework that combines a tree-based and an inverted-list based algorithm to efficiently mine high-utility itemsets. Algorithms based on the framework can harness benefits of both types of algorithms. We report experiment results on real and synthetic datasets to demonstrate the effectiveness of our framework. © 2017, Springer Science+Business Media New York.","Data mining; Frequent pattern mining; Mining methods and algorithms; Pattern growth mining; Utility mining","Data mining; Database systems; Information management; Inventory control; Network function virtualization; Query processing; Search engines; Trees (mathematics); Frequent pattern mining; High utility itemset minings; High utility itemsets; Mining methods and algorithms; Pattern growth; Query recommendations; Tree-based algorithms; Utility mining; Medical computing",2-s2.0-85018714668
"Sun B., Cheng W., Bai G., Goswami P.","Correcting and complementing freeway traffic accident data using mahalanobis distance based outlier detection [Ispravljanje i nadopunjavanje podataja o prometnim nesrećama na autocesti putem Mahalanobis udaljenosti na temelju otkrivanja netipičnih vrijednosti]",2017,"Tehnicki Vjesnik",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032512786&doi=10.17559%2fTV-20150616163905&partnerID=40&md5=a99572d44f82589e9644374d27deb6e7","A huge amount of traffic data is archived which can be used in data mining especially supervised learning. However, it is not being fully used due to lack of accurate accident information (labels). In this study, we improve a Mahalanobis distance based algorithm to be able to handle differential data to estimate flow fluctuations and detect accidents and use it to support correcting and complementing accident information. The outlier detection algorithm provides accurate suggestions for accident occurring time, duration and direction. We also develop a system with interactive user interface to realize this procedure. There are three contributions for data handling. Firstly, we propose to use multi-metric traffic data instead of single metric for traffic outlier detection. Secondly, we present a practical method to organise traffic data and to evaluate the organisation for Mahalanobis distance. Thirdly, we describe a general method to modify Mahalanobis distance algorithms to be updatable. © 2017, Strojarski Facultet. All rights reserved.","Accident data; Data labelling; Differential distance; Mahalanobis distance; Outlier detection; Traffic data; Updatable algorithm","Accidents; Data mining; Statistics; User interfaces; Accident data; Data labelling; Differential distance; Mahalanobis distances; Outlier Detection; Traffic data; Data handling",2-s2.0-85032512786
"Ramírez-Gallego S., Krawczyk B., García S., Wózniak M., Benítez J.M., Herrera F.","Nearest neighbor classification for high-speed big data streams using spark",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028953120&doi=10.1109%2fTSMC.2017.2700889&partnerID=40&md5=79802f894d9b8f386aee95a23348df00","Mining massive and high-speed data streams among the main contemporary challenges in machine learning. This calls for methods displaying a high computational efficacy, with ability to continuously update their structure and handle ever-arriving big number of instances. In this paper, we present a new incremental and distributed classifier based on the popular nearest neighbor algorithm, adapted to such a demanding scenario. This method, implemented in Apache Spark, includes a distributed metric-space ordering to perform faster searches. Additionally, we propose an efficient incremental instance selection method for massive data streams that continuously update and remove outdated examples from the case-base. This alleviates the high computational requirements of the original classifier, thus making it suitable for the considered problem. Experimental study conducted on a set of real-life massive data streams proves the usefulness of the proposed solution and shows that we are able to provide the first efficient nearest neighbor solution for high-speed big and streaming data. © 2017 IEEE.","Apache Spark; Big data; Data streams; Distributed computing; Instance reduction; Machine learning; Nearest neighbor","Artificial intelligence; Computer hardware description languages; Computer science; Data communication systems; Data mining; Distributed computer systems; Electric sparks; Learning algorithms; Learning systems; Metadata; Nearest neighbor search; Personnel training; Computational requirements; Data stream; Instance selection; Massive data streams; Memory management; Nearest neighbor algorithm; Nearest neighbor classification; Nearest neighbors; Big data",2-s2.0-85028953120
"Gomes H.M., Bifet A., Read J., Barddal J.P., Enembreck F., Pfharinger B., Holmes G., Abdessalem T.","Adaptive random forests for evolving data stream classification",2017,"Machine Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020727910&doi=10.1007%2fs10994-017-5642-8&partnerID=40&md5=428bbaef33bd2202d8965e9723654220","Random forests is currently one of the most used machine learning algorithms in the non-streaming (batch) setting. This preference is attributable to its high learning performance and low demands with respect to input preparation and hyper-parameter tuning. However, in the challenging context of evolving data streams, there is no random forests algorithm that can be considered state-of-the-art in comparison to bagging and boosting based algorithms. In this work, we present the adaptive random forest (ARF) algorithm for classification of evolving data streams. In contrast to previous attempts of replicating random forests for data stream learning, ARF includes an effective resampling method and adaptive operators that can cope with different types of concept drifts without complex optimizations for different data sets. We present experiments with a parallel implementation of ARF which has no degradation in terms of classification performance in comparison to a serial implementation, since trees and adaptive operators are independent from one another. Finally, we compare ARF with state-of-the-art algorithms in a traditional test-then-train evaluation and a novel delayed labelling evaluation, and show that ARF is accurate and uses a feasible amount of resources. © 2017, The Author(s).","Concept drift; Data stream mining; Ensemble learning; Random forests","Classification (of information); Data communication systems; Decision trees; Learning algorithms; Learning systems; Optimization; Classification performance; Complex optimization; Concept drifts; Data stream mining; Ensemble learning; Parallel implementations; Random forests; State-of-the-art algorithms; Data mining",2-s2.0-85020727910
"Zheng Z., Li P., Zhang X., Li D.","Recognition of opinion leaders in micro-blog based on linked data",2017,"International Journal of Performability Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031927271&doi=10.23940%2fijpe.17.06.p9.878885&partnerID=40&md5=9364262db3f4042f0527cd3d0b39ce81","In view of the lack of subjectivity and accuracy in the traditional micro-blog opinion leader recognition method to measure the important factors of users, a new micro-blog opinion leader recognition method is proposed. This paper used the linked data to describe the micro-blog data, used the association rule mining algorithm to quantitatively determine the important factors that affected the users’ ranking, and constructed the opinion leader recognition model according to the index scoring method. Experiments show that our method using linked data identifies the opinion leaders the same as the standard leaders, is more accurate and has better feasibility than that of traditional data. © 2017 Totem Publisher, Inc. All rights reserved.","Association rule; Linked data; Micro-blog; Opinion leader","Association rules; Data handling; Data mining; Linked datum; Micro-blog; Opinion leaders; Recognition methods; Recognition models; Rule mining algorithms; Scoring methods; Blogs",2-s2.0-85031927271
"Karunaratne P., Karunasekera S., Harwood A.","Distributed stream clustering using micro-clusters on Apache Storm",2017,"Journal of Parallel and Distributed Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003946086&doi=10.1016%2fj.jpdc.2016.06.004&partnerID=40&md5=54c5de5d15f8d2a38a8ac24813f9e29c","The recent need to extract real-time insights from data has driven the need for machine learning algorithms that can operate on data streams. Given the current extreme rates of data generation (around 5000 messages per second), these algorithms need to be able to handle data streams of very high velocity. Many current algorithms do not reach this requirement, in some cases processing only tens of messages per second. In this work we address the problem of limited achievable throughput of stream clustering by developing scalable distributed algorithms based on the micro-clustering paradigm that run on cloud platforms. We present two distributed architectures to execute the algorithms in parallel and implement these architectures on the Apache Storm stream processing platform. We demonstrate that we are able to gain close to an order of magnitude of improvement of performance in our experiments. © 2016 Elsevier Inc.","Distributed data mining; Stream clustering; Stream data mining","Artificial intelligence; Clustering algorithms; Data communication systems; Learning algorithms; Learning systems; Storms; Achievable throughputs; Cloud platforms; Data generation; Distributed architecture; Distributed data mining; Stream clustering; Stream data mining; Stream processing; Data mining",2-s2.0-85003946086
"Silva V., Leite J., Camata J.J., de Oliveira D., Coutinho A.L.G.A., Valduriez P., Mattoso M.","Raw data queries during data-intensive parallel workflow execution",2017,"Future Generation Computer Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009830332&doi=10.1016%2fj.future.2017.01.016&partnerID=40&md5=edd030faeba3613c75a5435c51096557","Computer simulations consume and produce huge amounts of raw data files presented in different formats, e.g., HDF5 in computational fluid dynamics simulations. Users often need to analyze domain-specific data based on related data elements from multiple files during the execution of computer simulations. In a raw data analysis, one should identify regions of interest in the data space and retrieve the content of specific related raw data files. Existing solutions, such as FastBit and RAW, are limited to a single raw data file analysis and can only be used after the execution of computer simulations. Scientific Workflow Management Systems (SWMS) can manage the dataflow of computer simulations and register related raw data files at a provenance database. This paper aims to combine the advantages of a dataflow-aware SWMS and the raw data file analysis techniques to allow for queries on raw data file elements that are related, but reside in separate files. We propose a component-based architecture, named as ARMFUL (Analysis of Raw data from Multiple Files) with raw data extraction and indexing techniques, which allows for a direct access to specific elements or regions of raw data space. ARMFUL innovates by using a SWMS provenance database to add a dataflow access path to raw data files. ARMFUL facilitates the invocation of ad-hoc programs and third party tools (e.g., FastBit tool) for raw data analyses. In our experiments, a real parallel computational fluid dynamics is executed, exploring different alternatives of raw data extraction, indexing and analysis. © 2017 Elsevier B.V.","Dataflow; Index raw data; Raw data analysis; Scientific workflows","Computational fluid dynamics; Data flow analysis; Data handling; Data mining; Extraction; Fluid dynamics; Indexing (of information); Information analysis; Work simplification; Analysis techniques; Component-based architecture; Computational fluid dynamics simulations; Dataflow; Index raw data; Indexing techniques; Scientific workflow managements; Scientific workflows; Information management",2-s2.0-85009830332
"Lin Y.-F., Huang C.-F., Tseng V.S.","A novel methodology for stock investment using high utility episode mining and genetic algorithm",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020833846&doi=10.1016%2fj.asoc.2017.05.032&partnerID=40&md5=675b9b412d5d1761ff4ff9741a4420fe","In this paper, we present a novel methodology for stock investment using the technique of high utility episode mining and genetic algorithms. Our objective is to devise a profitable episode-based investment model to reveal hidden events that are associated with high utility in the stock market. The time series data of stock price and the derived technical indicators, including moving average, moving average convergence and divergence, random index and bias index, are used for the construction of episode events. We then employ the genetic algorithm for the simultaneous optimization on parameters and selection of subsets of models. The empirical results show that our proposed method significantly outperforms the state-of-the-art methods in terms of annualized returns of investment and precision. We also provide a set of Z-tests to statistically validate the effectiveness of our proposed method. Based upon the promising results obtained, we expect this novel methodology can advance the research in data mining for computational finance and provide an alternative to stock investment in practice. © 2017 Elsevier B.V.","Genetic algorithm; High utility episode mining; Stock investment; Technical indicators","Data mining; Electronic trading; Financial markets; Genetic algorithms; Optimization; Computational finance; Episode mining; Investment models; Novel methodology; Simultaneous optimization; State-of-the-art methods; Technical indicator; Time-series data; Investments",2-s2.0-85020833846
"Phillips F.","A perspective on 'Big Data'",2017,"Science and Public Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032017702&doi=10.1093%2fscipol%2fscx012&partnerID=40&md5=c341e07e9a4671768d6f4f7e4708a601","Many of the lessons learned with what passed for big data in the 1980s still apply today. The lessons have to do with deciding whether something is true or merely useful, the role of human creativity in posing questions, the treatment of hypotheses and the role of theory in data mining, skill development, and organizational dynamics. This essay details what has changed in the present era of 'big data', what has remained the same, what we may learn, and what promise the future holds. Important highlights include the role of executives in building a data-based decision culture, and the potential of big data for analyzing diversity rather than regression to means. © The Author 2017. Published by Oxford University Press. All rights reserved.","Analytics; Big data; Business intelligence; Machine learning","analytical framework; data mining; data set; machine learning",2-s2.0-85032017702
"Saleiro P., Rodrigues E.M., Soares C., Oliveira E.","TexRep: A Text Mining Framework for Online Reputation Monitoring",2017,"New Generation Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021716270&doi=10.1007%2fs00354-017-0021-3&partnerID=40&md5=e888c60c5172c07bf76f657ff4405be3","This work aims to understand, formalize and explore the scientific challenges of using unstructured text data from different Web sources for Online Reputation Monitoring. We here present TexRep, an adaptable text mining framework specifically tailored for Online Reputation Monitoring that can be reused in multiple application scenarios, from politics to finance. This framework is able to collect texts from online media, such as Twitter, and identify entities of interest and classify sentiment polarity and intensity. The framework supports multiple data aggregation methods, as well as visualization and modeling techniques that can be used for both descriptive analytics, such as analyze how political polls evolve over time, and predictive analytics, such as predict elections. We here present case studies that illustrate and validate TexRep for Online Reputation Monitoring. In particular, we provide an evaluation of TexRep Entity Filtering and Sentiment Analysis modules using well known external benchmarks. We also present an illustrative example of TexRep application in the political domain. © 2017, Ohmsha, Ltd. and Springer Japan KK.","Online reputation monitoring; Social computing; Text mining","Data visualization; Natural language processing systems; Predictive analytics; Modeling technique; Multiple applications; Multiple data; Online media; Sentiment analysis; Social computing; Text mining; Unstructured texts; Data mining",2-s2.0-85021716270
"Pang Z.-F., Fan J., Zhang J.","Semisupervised data classification via the Mumford–Shah–Potts-type model",2017,"Applied Mathematical Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028018498&doi=10.1016%2fj.apm.2017.05.027&partnerID=40&md5=ee02b20fd7e74a8b4a98b021d9a2a391","More and more high dimensional data are widely used in many real world applications. This kind of data are obtained from different feature extractors, which represent distinct perspectives of the data. How to classify such data efficiently is a challenge. Despite of existence of millions of unlabeled data samples, it is believed that labeling a handful of data such as the semisupervised scheme will remarkably improve the searching performance. However, the performance of semisupervised data classification highly relies on proposed models and related numerical methods. Following from the extension of the Mumford–Shah–Potts-type model in the spatially continuous setting, we propose some efficient data classification algorithms based on the alternating direction method of multipliers and the primal-dual method to efficiently deal with the nonsmoothing problem in the proposed model. The convergence of the proposed data classification algorithms is established under the framework of variational inequalities. Some balanced and unbalanced classification problems are tested, which demonstrate the efficiency of the proposed algorithms. © 2017 Elsevier Inc.","Alternating direction method of multipliers; Mumford–Shah–Potts-type Model; Primal-dual method; Semisupervised data classification","Classification (of information); Clustering algorithms; Numerical methods; Potts model; Variational techniques; Alternating direction method of multipliers; Data classification; Feature extractor; High dimensional data; Primal-dual methods; Searching performance; Semi-supervised; Variational inequalities; Data mining",2-s2.0-85028018498
"Gao H., Jian S., Peng Y., Liu X.","A subspace ensemble framework for classification with high dimensional missing data",2017,"Multidimensional Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962132687&doi=10.1007%2fs11045-016-0393-4&partnerID=40&md5=23a8fe8cbdac237e9fdd8faaab0dc17f","Real world classification tasks may involve high dimensional missing data. The traditional approach to handling the missing data is to impute the data first, and then apply the traditional classification algorithms on the imputed data. This method first assumes that there exist a distribution or feature relations among the data, and then estimates missing items with existing observed values. A reasonable assumption is a necessary guarantee for accurate imputation. The distribution or feature relations of data, however, is often complex or even impossible to be captured in high dimensional data sets, leading to inaccurate imputation. In this paper, we propose a complete-case projection subspace ensemble framework, where two alternative partition strategies, namely bootstrap subspace partition and missing pattern-sensitive subspace partition, are developed for incomplete datasets with even missing patterns and uneven missing patterns, respectively. Multiple component classifiers are then separately trained in these subspaces. After that, a final ensemble classifier is constructed by a weighted majority vote of component classifiers. In the experiments, we demonstrate the effectiveness of the proposed framework over eight high dimensional UCI datasets. Meanwhile, we apply the two proposed partition strategies over data sets with different missing patterns. As indicated, the proposed algorithm significantly outperforms existing imputation methods in most cases. © 2016, Springer Science+Business Media New York.","Extreme learning machine; High dimensional data; Missing data; Subspace ensemble","Clustering algorithms; Data handling; Data mining; Learning systems; Classification algorithm; Classification tasks; Component classifiers; Extreme learning machine; High dimensional data; Missing data; Subspace ensemble; Traditional approaches; Classification (of information)",2-s2.0-84962132687
"Lin W.-C., Tsai C.-F., Hu Y.-H., Jhang J.-S.","Clustering-based undersampling in class-imbalanced data",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019061365&doi=10.1016%2fj.ins.2017.05.008&partnerID=40&md5=74e5609d2c8e685200dbd3cecc433ed3","Class imbalance is often a problem in various real-world data sets, where one class (i.e. the minority class) contains a small number of data points and the other (i.e. the majority class) contains a large number of data points. It is notably difficult to develop an effective model using current data mining and machine learning algorithms without considering data preprocessing to balance the imbalanced data sets. Random undersampling and oversampling have been used in numerous studies to ensure that the different classes contain the same number of data points. A classifier ensemble (i.e. a structure containing several classifiers) can be trained on several different balanced data sets for later classification purposes. In this paper, we introduce two undersampling strategies in which a clustering technique is used during the data preprocessing step. Specifically, the number of clusters in the majority class is set to be equal to the number of data points in the minority class. The first strategy uses the cluster centers to represent the majority class, whereas the second strategy uses the nearest neighbors of the cluster centers. A further study was conducted to examine the effect on performance of the addition or deletion of 5 to 10 cluster centers in the majority class. The experimental results obtained using 44 small-scale and 2 large-scale data sets revealed that the clustering-based undersampling approach with the second strategy outperformed five state-of-the-art approaches. Specifically, this approach combined with a single multilayer perceptron classifier and C4.5 decision tree classifier ensembles delivered optimal performance over both small- and large-scale data sets. © 2017 Elsevier Inc.","Class imbalance; Classifier ensembles; Clustering; Imbalanced data; Machine learning","Artificial intelligence; Data mining; Decision trees; Learning algorithms; Learning systems; Trees (mathematics); Virtual reality; C4.5 Decision tree classifier; Class imbalance; Classifier ensembles; Clustering; Imbalanced data; Large scale data sets; Multi-layer perceptron classifiers; Random under samplings; Classification (of information)",2-s2.0-85019061365
"Chang N.-B., Bai K., Chen C.-F.","Integrating multisensor satellite data merging and image reconstruction in support of machine learning for better water quality management",2017,"Journal of Environmental Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021280402&doi=10.1016%2fj.jenvman.2017.06.045&partnerID=40&md5=435861d3218065c139a0316bb38199e5","Monitoring water quality changes in lakes, reservoirs, estuaries, and coastal waters is critical in response to the needs for sustainable development. This study develops a remote sensing-based multiscale modeling system by integrating multi-sensor satellite data merging and image reconstruction algorithms in support of feature extraction with machine learning leading to automate continuous water quality monitoring in environmentally sensitive regions. This new Earth observation platform, termed “cross-mission data merging and image reconstruction with machine learning” (CDMIM), is capable of merging multiple satellite imageries to provide daily water quality monitoring through a series of image processing, enhancement, reconstruction, and data mining/machine learning techniques. Two existing key algorithms, including Spectral Information Adaptation and Synthesis Scheme (SIASS) and SMart Information Reconstruction (SMIR), are highlighted to support feature extraction and content-based mapping. Whereas SIASS can support various data merging efforts to merge images collected from cross-mission satellite sensors, SMIR can overcome data gaps by reconstructing the information of value-missing pixels due to impacts such as cloud obstruction. Practical implementation of CDMIM was assessed by predicting the water quality over seasons in terms of the concentrations of nutrients and chlorophyll-a, as well as water clarity in Lake Nicaragua, providing synergistic efforts to better monitor the aquatic environment and offer insightful lake watershed management strategies. © 2017 Elsevier Ltd","Enabling technology; Machine learning; Remote sensing; Water quality; Watershed management","chlorophyll a; nitrogen; phosphorus; image analysis; machine learning; reconstruction; remote sensing; satellite data; water management; water quality; watershed; algorithm; aquatic environment; Article; controlled study; data mining; entropy; image enhancement; image processing; image reconstruction; lake; machine learning; multisensor satellite data merging; Nicaragua; quality control; remote sensing; satellite imagery; sea; season; SMart Information Reconstruction; Spectral Information Adaptation and Synthesis Scheme; water management; water quality; water supply; watershed; watershed management",2-s2.0-85021280402
"Kim K., Park O.-J., Yun S., Yun H.","What makes tourists feel negatively about tourism destinations? Application of hybrid text mining methodology to smart destination management",2017,"Technological Forecasting and Social Change",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009476386&doi=10.1016%2fj.techfore.2017.01.001&partnerID=40&md5=f7e9df4cd598dab5895ff014897bc61a","Recently, the Internet has brought a big change in tourists' behavior patterns. Travelers not only reserve hotels and airline tickets online, but also exchange travel information and descriptions of pleasant or unpleasant travel experiences through online review sites and personal travel blogs. In spite of the increasing use of online channels, application of online text data has been limited since the volume of the data set is too large to analyze manually and comprehensively. With recent technological advances in processing big data online, consumer-generated information can be automatically analyzed by artificial intelligence. As an aspect of smart tourism, this study applied the sentiment analysis method to analyze travelers' online reviews of Paris. A total of 19,835 pieces of review data collected from a traveler review site (www.virtualtourist.com) were processed. All reviews were grouped into 14 categories as follows: overview, restaurants, sightseeing, hotels, things to do, night life, transportation, shopping, sporting & outdoors, favorites, off the beaten path, what to pack, tourist traps, warnings and danger, and local customs. Tourists' perception about the service in each category was successfully measured, and as an illustration, we chose “transportation” category that reported relatively low level of service quality for post-hoc analysis to reveal why tourists feel negatively about the transportation service. © 2017 Elsevier Inc.","Sentiment analysis; Smart destination management; Smart tourism; Text mining; User-generated content (UGC)","Data handling; Data mining; Natural language processing systems; Quality control; Sentiment analysis; Smart tourism; Technological advances; Text mining; Tourists' perceptions; Transportation services; Travel information; User generated content (UGC); Big data; data mining; tourism management; tourist behavior; tourist destination; France; Ile de France; Paris; Ville de Paris",2-s2.0-85009476386
"Wang Y., Cao J., Li W., Gu T., Shi W.","Exploring traffic congestion correlation from multiple data sources",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017471875&doi=10.1016%2fj.pmcj.2017.03.015&partnerID=40&md5=9722cb24bbd3df45a2123c9f45e6ae3b","Traffic congestion is a major concern in many cities around the world. Previous work mainly focuses on the prediction of congestion and analysis of traffic flows, while the congestion correlation between road segments has not been studied yet. In this paper, we propose a three-phase framework to explore the congestion correlation between road segments from multiple real world data. In the first phase, we extract congestion information on each road segment from GPS trajectories of over 10,000 taxis, define congestion correlation and propose a corresponding mining algorithm to find out all the existing correlations. In the second phase, we extract various features on each pair of road segments from road network and POI data. In the last phase, the results of the first two phases are input into several classifiers to predict congestion correlation. We further analyze the important features and evaluate the results of the trained classifiers through experiments. We found some important patterns that lead to a high/low congestion correlation, and they can facilitate building various transportation applications. In addition, we found that traffic congestion correlation has obvious directionality and transmissibility. The proposed techniques in our framework are general, and can be applied to other pairwise correlation analysis. © 2017 Elsevier B.V.","Classification; Congestion correlation; Multiple data sources; Traffic congestion","Classification (of information); Data mining; Motor transportation; Roads and streets; Traffic control; Transportation; Gps trajectories; Important features; Mining algorithms; Multiple data sources; Pairwise correlation; Road network; Road segments; Traffic flow; Traffic congestion",2-s2.0-85017471875
"Hwang I., Jang Y.J.","Process Mining to Discover Shoppers' Pathways at a Fashion Retail Store Using a WiFi-Base Indoor Positioning System",2017,"IEEE Transactions on Automation Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018918153&doi=10.1109%2fTASE.2017.2692961&partnerID=40&md5=1b5c9ba8a1cb3af96e9252cabe2da354","We present a preliminary report of a customer pathway analysis in an off-line store. Smart phone WiFi-based positioning technology is used to identify each customer's pathway behavior. The log data containing the space-time information are analyzed using process mining, a tool that provides a comprehensive view of an entire process. The main benefit of process mining is that it provides the topological structure of the processes. We installed a WiFi signal-capturing device in a retail store of a fashion brand in South Korea and collected data over a two-month period. Halfway through the experimental period, we swapped a set of mannequins displayed at the entrance to the store with an item stand. We then compared the customers' pathway behavior before and after the change. Through an analysis based on process mining, we observed a change in the topological structure of the pathway behavior following the change in the display setting. This paper demonstrates the possibilities of analyzing customer behavior using WiFi-based technology and the process mining technique.Note to Practitioners - The main goal of this paper is to demonstrate the possibility of WiFi-based positioning technology and analytical methodology for analyzing indoor movement in the era of Big Data and the Internet of Things. Recently, with advances in communication, sensors, and wearable computing technologies, strong interest has been shown in marketing and retail behavior studies that can capture customer travel data in off-line stores to inform and improve sales and marketing. As the application of off-line store behavior analysis for behavior studies and marketing gains momentum, this paper can be used as a foundation for the development of sensor-based location analysis systems or devices for off-line stores. The focus of this paper is not to investigate customer behavior related to the display change nor the behavioral science related to retail. Rather, we experimentally demonstrate the value of the proposed technology and the process mining technique for future research. © 2004-2012 IEEE.","Indoor location study; process mining; sensor data; WiFi-based location analysis","Data mining; Indoor positioning systems; Radio communication; Retail stores; Smartphones; Topology; Wireless local area networks (WLAN); Customer behavior; Pathway analysis; Positioning technologies; Process mining; South Korea; Space-time informations; Topological structure; Wi-Fi signals; Sales",2-s2.0-85018918153
"Bharti S.K., Pradhan R., Babu K.S., Jena S.K.","Sarcastic sentiment detection based on types of sarcasm occurring in twitter data",2017,"International Journal on Semantic Web and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029741881&doi=10.4018%2fIJSWIS.2017100105&partnerID=40&md5=da20d8fe51b4388b8fe37f97e358c801","In Natural Language Processing (NLP), sarcasm analysis in the text is considered as the most challenging task. It has been broadly researched in recent years. The property of sarcasm that makes it harder to detect is the gap between the literal and its intended meaning. It is a particular kind of sentiment which is capable of flipping the entire sense of a text. Sarcasm is often expressed verbally through the use of high pitch with heavy tonal stress. The other clues of sarcasm are the usage of various gestures such as gently sloping of eyes, hands movements, shaking heads, etc. However, the appearances of these clues for sarcasm are absent in textual data which makes the detection of sarcasm dependent upon several other factors. In this article, six algorithms were proposed to analyze the sarcasm in tweets of Twitter. These algorithms are based on the possible occurrences of sarcasm in tweets. Finally, the experimental results of the proposed algorithms were compared with some of the existing state-of-the-art. Copyright © 2017, IGI Global.","NLP; Opinion Mining; Parsing; POS Tagging; Sarcasm; Sentiment Analysis; Tweets; Twitter Data","Computational linguistics; Data mining; Eye movements; Social networking (online); Opinion mining; Parsing; PoS tagging; Sarcasm; Sentiment analysis; Tweets; Twitter Data; Natural language processing systems",2-s2.0-85029741881
"Song J., Zhang Y., Duan K., Shamim Hossain M., Rahman S.M.M.","TOLA: Topic-oriented learning assistance based on cyber-physical system and big data",2017,"Future Generation Computer Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977668644&doi=10.1016%2fj.future.2016.05.040&partnerID=40&md5=e5c78c1d0b88af84b4e88f182106dd1c","Massive open online courses (MOOC) is a novel educational model emerging in recent years, which is assisted by advanced techniques such as cloud computing, big data and Cyber-Physical Systems (CPS). Through adequate analysis assisted by big data, the quality of education is expected to be extensively improved. Unfortunately, the MOOC data are not fully utilized for educational innovation, because the existing research focuses on the data generated in the online learning but neglects other related data, such as the forum data. In this article, we propose a big-data-driven approach named TOLA for online learning evolution to discover students’ learning pattern and guide courses improvement. Specifically, topic feature is extracted from MOOC forum through Latent Dirichlet Allocation, which is incorporated with other hybrid features. Through experiments, TOLA exhibits good performance in terms of complexity, efficiency and accuracy, facilitating the improvement of the quality of online education. © 2016 Elsevier B.V.","Big data; Collaborative filtering; Cyber-physical system; Latent Dirichlet allocation; Massive open online courses","Collaborative filtering; Data mining; Distance education; Distributed computer systems; E-learning; Education; Education computing; Embedded systems; Online systems; Statistics; Cyber physical systems (CPSs); Cyber-physical systems (CPS); Data-driven approach; Educational innovations; Educational models; Latent Dirichlet allocation; Massive open online course; Quality of education; Big data",2-s2.0-84977668644
"Amagata D., Hara T.","Mining top-k co-occurrence patterns across multiple streams",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028948760&doi=10.1109%2fTKDE.2017.2728537&partnerID=40&md5=8b4f9d9a794258cb79ce109730426e23","The recent Bigdata and IoT era has presented a number of applications that generate objects in a streaming fashion. It is well-known that real-time mining of important patterns from data streams support many domains. In retail markets and social network services, for example, such patterns are itemsets and words that frequently appear in many user-accounts, i.e., co-occurrence patterns. To efficiently monitor co-occurrence patterns, we address the novel problem of mining top-k closed co-occurrence patterns across multiple streams. We employ sliding window setting in this problem, and each pattern is ranked based on count, which is the number of streams that have generated the pattern. Since objects are consecutively generated and deleted, the count of a given pattern is dynamic, which may change the rank of the pattern. This renders a challenge to monitoring the top-k answer in real-time. We propose an index-based algorithm that addresses the challenge and provides the exact answer. Specifically, we propose the CP-Graph, a hybrid index of graph and inverted file structures. The CP-Graph can efficiently compute the count of a given pattern and update the answer while pruning unnecessary patterns. Our experimental study on real datasets demonstrates the efficiency and scalability of our solution. © 1989-2012 IEEE.","Multiple streams; Top-k co-occurrence patterns","Data mining; Data structures; Heuristic algorithms; Interactive computer systems; Monitoring; Silicon; Co-occurrence pattern; Index based algorithm; Index of graphs; Inverted files; Multiple streams; Real-time mining; Sliding Window; Social network services; Real time systems",2-s2.0-85028948760
"Tobback E., Bellotti T., Moeyersoms J., Stankova M., Martens D.","Bankruptcy prediction for SMEs using relational data",2017,"Decision Support Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025820154&doi=10.1016%2fj.dss.2017.07.004&partnerID=40&md5=37f1d4ebd49bb9e1e05e0723480de5bb","Bankruptcy prediction has been a popular and challenging research area for decades. Most prediction models are built using financial figures, stock market data and firm specific variables. We complement such traditional low-dimensional data with high-dimensional data on the company's directors and managers in the prediction models. This information is used to build a network between small and medium-sized enterprises (SMEs), where two companies are related if they share a director or high-level manager. A smoothed version of the weighted-vote relational neighbour classifier is applied on the network and transforms the relationships between companies into bankruptcy prediction scores, thereby assuming that a company is more likely to file for bankruptcy if one of the related companies in its network has already failed. An ensemble model is built that combines the relational model's output scores with structured data and is applied on two data sets of Belgian and UK SMEs. We find that the relational model gives improved predictions over a simple financial model when detecting the riskiest firms. The largest performance increase is found when the relational and financial data are combined, confirming the complementary nature of both data types. © 2017 Elsevier B.V.","Bankruptcy prediction; Data mining; Network analysis; Relational data; SME","Data mining; Electric network analysis; Finance; Managers; Bankruptcy prediction; Ensemble modeling; Financial modeling; High dimensional data; Prediction model; Relational data; Relational Model; Small and medium-sized enterprise; Forecasting",2-s2.0-85025820154
"Yang J., Jiang B., Li B., Tian K., Lv Z.","A Fast Image Retrieval Method Designed for Network Big Data",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031665523&doi=10.1109%2fTII.2017.2657545&partnerID=40&md5=43194fcb535f674e23b03f08b3b46d9f","In the field of big data applications, image information is widely used. The value density of information utilization in big data is very low, and how to extract useful information quickly is very important. So we should transform the unstructured image data source into a form that can be analyzed. In this paper, we proposed a fast image retrieval method which designed for big data. First of all, the feature extraction method is necessary and the feature vectors can be obtained for every image. Then, it is the most important step for us to encode the image feature vectors and make them into database, which can optimize the feature structure. Finally, the corresponding similarity matching is used to determined the retrieval results. There are three main contributions for image retrieval in this paper. New feature extraction method, reasonable elements ranking, and appropriate distance metric can improve the algorithm performance. Experiments show that our method has a great improvement in the effective performance of feature extraction and can also get better search matching results. © 2005-2012 IEEE.","Big data; distance learning; feature ranking; image retrieval","Data mining; Distance education; Extraction; Feature extraction; Image retrieval; Information retrieval; Algorithm performance; Appropriate distances; Big data applications; Effective performance; Feature extraction methods; Feature ranking; Information utilization; Similarity-matching; Big data",2-s2.0-85031665523
"Karpatne A., Atluri G., Faghmous J.H., Steinbach M., Banerjee A., Ganguly A., Shekhar S., Samatova N., Kumar V.","Theory-guided data science: A new paradigm for scientific discovery from data",2017,"IEEE Transactions on Knowledge and Data Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023747083&doi=10.1109%2fTKDE.2017.2720168&partnerID=40&md5=f0f792a1f1cc3a50895de6b6918defdc","Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science. © 1989-2012 IEEE.","Data science; Domain knowledge; Interpretability; Knowledge discovery; Physical consistency; Scientific theory","Data mining; Data structures; Knowledge management; Mathematical models; Numerical models; Quantum chemistry; Atmospheric model; Biological system modeling; Data science; Domain knowledge; Interpretability; physical consistency; Scientific theories; Biological systems",2-s2.0-85023747083
"Aoga J.O.R., Guns T., Schaus P.","Mining Time-constrained Sequential Patterns with Constraint Programming",2017,"Constraints",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020310649&doi=10.1007%2fs10601-017-9272-3&partnerID=40&md5=1a3c425a929024495d53cf1b138863ac","Constraint Programming (CP) has proven to be an effective platform for constraint based sequence mining. Previous work has focused on standard frequent sequence mining, as well as frequent sequence mining with a maximum ’gap’ between two matching events in a sequence. The main challenge in the latter is that this constraint can not be imposed independently of the omnipresent frequency constraint. Indeed, the gap constraint changes whether a subsequence is included in a sequence, and hence its frequency. In this work, we go beyond that and investigate the integration of timed events and constraining the minimum/maximum gap as well as minimum/maximum span. The latter constrains the allowed time between the first and last matching event of a pattern. We show how the three are interrelated, and what the required changes to the frequency constraint are. Key in our approach is the concept of an extension window defined by gap/span and we develop techniques to avoid scanning the sequences needlessly, as well as using a backtracking-aware data structure. Experiments demonstrate that the proposed approach outperforms both specialized and CP-based approaches in almost all cases and that the advantage increases as the minimum frequency threshold decreases. This paper is an extension of the original manuscript presented at CPAIOR’17 [5]. © 2017, Springer Science+Business Media New York.","Constraint programming; Data mining; Gap constraint; Global constraint; Sequential pattern mining; Span constraint; Time constraint","Computer programming; Constraint theory; Constraint programming; Gap constraint; Global constraints; Sequential-pattern mining; Span constraint; Time constraints; Data mining",2-s2.0-85020310649
"Chen J., Cai S., Towey D., Zhu L., Huang R., Ackah-Arthur H., Omari M.","Detecting Implicit Security Exceptions Using an Improved Variable-Length Sequential Pattern Mining Method",2017,"International Journal of Software Engineering and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032023350&doi=10.1142%2fS0218194017500462&partnerID=40&md5=89ec111dd9eeee022a2b26ff190a8c5d","The process of component security testing can produce massive amounts of monitor logs. Current approaches to detect implicit security exceptions (those which cannot be identified by visual inspection alone) compare correct execution sequences with fixed patterns mined from the execution of sequential patterns in the monitor logs. However, this is not efficient and is not suitable for mining large monitor logs. To enable effective mining of implicit security exceptions from large monitor logs, this paper proposes a method based on improved variable-length sequential pattern mining. The proposed method first mines the variable-length sequential patterns from correct execution sequences and from actual execution sequences, thus reducing the number of patterns. The sequential patterns are then detected using the Sunday string-searching algorithm. We conducted an experimental study based on this method, the results of which show that the proposed method can efficiently detect the implicit security exceptions of components. © 2017 World Scientific Publishing Company.","Component testing; implicit security detection; monitor logs; pattern detecting; sequential pattern mining","String searching algorithms; Component security; Component testing; Execution sequences; pattern detecting; Security detection; Sequential patterns; Sequential-pattern mining; Visual inspection; Data mining",2-s2.0-85032023350
"Bansal S., Katyal D., Garg J.K.","A novel strategy for wetland area extraction using multispectral MODIS data",2017,"Remote Sensing of Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028917429&doi=10.1016%2fj.rse.2017.07.034&partnerID=40&md5=c1bb7cf58d54370bc775b126b80e975d","MODIS (Moderate Resolution Imaging Spectro-radiometer) is an imaging sensor onboard Terra/Aqua platform which provides information in 36 spectral bands. Spectral mixing of features due to high data dimensionality and high inter-band correlations constitutes the biggest challenge in the analysis of MODIS data. In view of this, present study attempts to develop a novel strategy involving Principal Component Analysis (PCA), Band to Band Correlation (BTBC) analysis, Stepwise Discriminant Analysis (SDA), and separability analysis to reduce the data dimensionality for extracting reliable and maximum information for wetland areal extent using coarse resolution MODIS (1 km) data. The PCA explains variability in data and removes data redundant information; BTBC analysis eradicates the high correlated bands providing best bands suitable for wetland mapping; SDA evaluates the discriminatory power of different MODIS bands to discriminate the wetlands from other class types. Further, Normalized Difference Vegetation Index (NDVI) and Wetland Model Index (WMI) were also incorporated into the study to improve the classification accuracy. Finally, separability analysis was conducted to optimize the selected MODIS bands and indices. Results of rigorous data mining reveal that out of 24 input layers of MODIS (1 km) data (22 optical MODIS bands, NDVI and WMI), only 4 input layers (WMI, NDVI, MODIS bands – NIR band 2 and SWIR band 6) are best suited for delineation and mapping of wetlands. This study also corroborates the usage of WMI, a newly developed index with combination of visible and short wavelength infra red (SWIR) MODIS bands, as the most optimal input layer to separate wetlands from the other land use class types (barren land, agricultural land, rivers/canals/streams, forest, wasteland/gullied or riverine land). It was observed that the magnitude of mapped wetland areal extent using MODIS (1 km) data varied from 105,053 ha in 2010–2011 to 111,479 ha in 2011–2012 accounting for 0.44% (2010–2011)–0.46% (2011–2012) of the total geographical area of Uttar Pradesh, India. Seasonally, monsoon season displayed maximum wetland area with overall accuracy of 92% whereas summer season exhibited minimum wetland area with overall accuracies varying from 85 to 87% during both the sampling years. The output of the present research work will not only facilitate to improve the wetland area estimates but also provide an important input for climate change predictions in wetlands over large areas. © 2017 Elsevier Inc.","Data mining; MODIS; Separability analysis; Wetland Model Index; Wetlands","Climate change; Discriminant analysis; Image reconstruction; Land use; Mapping; Principal component analysis; Radiometers; Satellite imagery; Vegetation; Wetlands; Classification accuracy; Climate change prediction; MODIS; Multispectral MODIS data; Normalized difference vegetation index; Separability analysis; Stepwise discriminant analysis; Wetland models; Data mining; accuracy assessment; climate change; data mining; geographical region; land use change; MODIS; multispectral image; NDVI; research work; wavelength; wetland; India; Uttar Pradesh",2-s2.0-85028917429
"Zhang Z., Pedrycz W., Huang J.","Efficient frequent itemsets mining through sampling and information granulation",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027500488&doi=10.1016%2fj.engappai.2017.07.016&partnerID=40&md5=8e2660f527c068fe6b277206da32a3c8","In this study, we propose an algorithm forming high quality approximate frequent itemsets from those datasets with a large scale of transactions. The results produced by the algorithm with high probability contain all frequent itemsets, no itemset with support much lower than the minimum support is included, and supports obtained by the algorithm are close to the real values. To avoid an over-estimated sample size and a significant computing overhead, the task of reducing data is decomposed into three subproblems, and sampling and information granulation are used to solve them one by one. Firstly, the algorithm obtains rough support of every item by sampling and removes those infrequent items, so the data are simplified. Then, another sample is taken from the simplified data, and is clustered into some information granules. After data reduction, these granules obtained in this way are mined by the improved Apriori. A tight guarantee for the quality of final results is provided. The performance of the approach is quantified through a series of experiments. © 2017 Elsevier Ltd","Frequent itemsets mining; Information granulation; Sampling","Data mining; Granulation; Sampling; Frequent itemsets minings; High probability; High quality; Information granulation; Minimum support; Real values; Sample sizes; Sub-problems; Information granules",2-s2.0-85027500488
"Jossé G., Schmid K.A., Züfle A., Skoumas G., Schubert M., Renz M., Pfoser D., Nascimento M.A.","Knowledge extraction from crowdsourced data for the enrichment of road networks",2017,"GeoInformatica",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028869146&doi=10.1007%2fs10707-017-0306-1&partnerID=40&md5=7c2efa6a4ff6debc0715dde18f61b8ca","In current navigation systems quantitative metrics such as distance, time and energy are used to determine optimal paths. Yet, a “best path”, as judged by users, might take qualitative features into account, for instance the scenery or the touristic attractiveness of a path. Machines are unable to quantify such “soft” properties. Crowdsourced data provides us with a means to record user choices and opinions. In this work, we survey heterogeneous sources of spatial, spatio-temporal and textual crowdsourced data as a proxy for qualitative information of users in movement. We (i) explore the process of extracting qualitative information from uncertain crowdsourced data sets employing different techniques, (ii) investigate the enrichment of road networks with the extracted information by adjusting its properties and by building a meta-network, and (iii) show how to use the enriched networks for routing purposes. An extensive experimental evaluation of our proposed methods on real-world data sets shows that qualitative properties as captured by crowdsourced data can indeed be used to improve the quality of routing suggestions while not sacrificing their quantitative aspects. © 2017, Springer Science+Business Media, LLC.","Crowdsourced data; Data mining; Knowledge discory; Path computation; Road networks; Routing","Navigation systems; Network routing; Roads and streets; Transportation; Crowdsourced data; Knowledge discory; Path computation; Road network; Routing; Data mining; data mining; GIS; movement; navigation aid; qualitative analysis; quantitative analysis; routing",2-s2.0-85028869146
"Pereira J.C., Teixeira A.J.S., Rodrigues M., Miguel P., Pinto J.S.","Natural transmission of information extraction results to end-users – a proof-of-concept using data-to-text",2017,"OpenAccess Series in Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032615325&doi=10.4230%2fOASIcs.SLATE.2017.20&partnerID=40&md5=3444b2a82685429aa3d652c2db54d343","Information Extraction from natural texts has a great potential in areas such as Tourism and can be of great assistance in transforming customers’ comments in valuable information for Tourism operators, governments and customers. After extraction, information needs to be e ciently transmitted to end-users in a natural way. Systems should not, in general, send extracted information directly to end-users, such as hotel managers, as it can be di cult to read. Naturally, humans transmit and encode information using natural languages, such as Portuguese. The problem arising from the need of e cient and natural transmission of the information to end-user is how to encode it. The use of natural language generation (NLG) is a possible solution, for producing sentences, and, with them, texts. In this paper we address this, with a data-to-text system, a derivation of formal NLG systems that use data as input. The proposed system uses an aligned corpus, which was defined, collected and processed, in about approximately 3 weeks of work. To build the language model were used three di erent in-domain and out-of-domain corpora. The e ects of this approach were evaluated, and results are presented. Automatic metrics, BLEU and Meteor, were used to evaluate the di erent systems, comparing their values with similar systems. Results show that expanding the corpus has a major positive e ect in BLEU and Meteor scores and use of additional corpora (in-domain and out-of-domain) in training language model does not result in significantly di erent performance. The scores obtained, combined with their comparison with other systems performance and informal evaluation by humans of the sentences produced, give additional support for the capabilities of the translation based approach for fast development of data-to-text for new domains. © José Casimiro Pereira, António J. S. Teixeira, Mário Rodrigues, Pedro Miguel, and Joaquim Sousa Pinto","Automatic Translation; Data-to-Text; Natural Language Generation; Opinions; Portuguese; Tourism","Artificial intelligence; Computational linguistics; Encoding (symbols); Information analysis; Information retrieval; Natural language processing systems; Slate; Translation (languages); Automatic translation; Data-to-Text; Natural language generation; Opinions; Portuguese; Tourism; Data mining",2-s2.0-85032615325
"Redman J.S., Natarajan Y., Hou J.K., Wang J., Hanif M., Feng H., Kramer J.R., Desiderio R., Xu H., El-Serag H.B., Kanwal F.","Accurate Identification of Fatty Liver Disease in Data Warehouse Utilizing Natural Language Processing",2017,"Digestive Diseases and Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028776195&doi=10.1007%2fs10620-017-4721-9&partnerID=40&md5=cd1a17ab8124d0ade04e79c689cb2c52","Introduction: Natural language processing is a powerful technique of machine learning capable of maximizing data extraction from complex electronic medical records. Methods: We utilized this technique to develop algorithms capable of “reading” full-text radiology reports to accurately identify the presence of fatty liver disease. Abdominal ultrasound, computerized tomography, and magnetic resonance imaging reports were retrieved from the Veterans Affairs Corporate Data Warehouse from a random national sample of 652 patients. Radiographic fatty liver disease was determined by manual review by two physicians and verified with an expert radiologist. A split validation method was utilized for algorithm development. Results: For all three imaging modalities, the algorithms could identify fatty liver disease with >90% recall and precision, with F-measures >90%. Discussion: These algorithms could be used to rapidly screen patient records to establish a large cohort to facilitate epidemiological and clinical studies and examine the clinic course and outcomes of patients with radiographic hepatic steatosis. © 2017, Springer Science+Business Media, LLC (Outside the USA).","Electronic health records; Epidemiology; Fatty liver; Natural language processing; Nonalcoholic fatty liver disease; Triglycerides","algorithm; Article; computer assisted tomography; data base; disease classification; echography; electronic medical record; fatty liver; human; machine learning; major clinical study; natural language processing; nonalcoholic fatty liver; nuclear magnetic resonance imaging; priority journal; radiologist; data mining; diagnostic imaging; echography; electronic health record; factual database; fatty liver; government; nuclear magnetic resonance imaging; predictive value; procedures; prognosis; United States; veterans health; x-ray computed tomography; Algorithms; Data Mining; Databases, Factual; Electronic Health Records; Fatty Liver; Humans; Magnetic Resonance Imaging; Natural Language Processing; Predictive Value of Tests; Prognosis; Tomography, X-Ray Computed; Ultrasonography; United States; United States Department of Veterans Affairs; Veterans Health",2-s2.0-85028776195
"Cao B., Frank Liu X., Liu J., Tang M.","Domain-aware Mashup service clustering based on LDA topic model from multiple data sources",2017,"Information and Software Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019052872&doi=10.1016%2fj.infsof.2017.05.001&partnerID=40&md5=f144eaf3c2a85628b687c9ca36de6fa1","Context Mashup is emerging as a promising software development method for allowing software developers to compose existing Web APIs to create new or value-added composite Web services. However, the rapid growth in the number of available Mashup services makes it difficult for software developers to select a suitable Mashup service to satisfy their requirements. Even though clustering based Mashup discovery technique shows a promise of improving the quality of Mashup service discovery, Mashup service clustering with high accuracy and good efficiency is still a challenge problem. Objective This paper proposes a novel domain-aware Mashup service clustering method with high accuracy and good efficiency by exploiting LDA topic model built from multiple data sources, to improve the quality of Mashup service discovery. Method The proposed method firstly designs a domain-aware Mashup service feature selection and reduction process by refining characterization of their domains to consolidate domain relevance. Then, it presents an extended LDA topic model built from multiple data sources (include Mashup description text, Web APIs and tags) to infer topic probability distribution of Mashup services, which serves as a basis of Mashup service similarity computation. Finally, K-means and Agnes algorithm are used to perform Mashup service clustering in terms of their similarities. Results Compared with other existing Mashup service clustering methods, experimental results show that the proposed method achieves a significant improvement in terms of precision, recall, F-measure, purity and entropy. Conclusion The results of the proposed method help software developers to improve the quality of Mashup service discovery and Mashup-based software development. In the future, there will be a need to extend the method by considering heterogeneous network information among Mashup, Web APIs, tags, users, and applying it to Mashup discovery for software developers. © 2017 Elsevier B.V.","Domain feature selection and reduction; LDA; Mashup service; Multiple data sources; Service clustering","Cluster analysis; Data mining; Efficiency; Feature extraction; Heterogeneous networks; Probability distributions; Reduction; Software design; Challenge problems; Composite Web services; Mashup service; Multiple data sources; Service clustering; Similarity computation; Software developer; Software development methods; Web services",2-s2.0-85019052872
"Yang L., Li K., Zhang W., Ke Z.","Ant colony classification mining algorithm based on pheromone attraction and exclusion",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964558149&doi=10.1007%2fs00500-016-2151-9&partnerID=40&md5=6c78017a04d3bada8cd7e991f7729822","Ant colony optimization algorithms have been applied successfully in classification rule mining. However, basic ant colony classification mining algorithms generally suffer from problems, such as premature convergence and falling into local optimum easily. Simultaneously, the classification mining algorithms use sequential covering strategy to discover rules, and the interaction between rules is less considered. In this study, a new ant colony classification mining algorithm based on pheromone attraction and exclusion (Ant-MinerPAE) is proposed, in which a new pheromone calculation method is designed and the search is guided by the new probability transfer formula. By contrast,the basic algorithm structure is modified, and the order of the iteration is adjusted. Thus, the problem of rule interaction is mitigated. Ant-MinerPAE can balance the relation of exploration and development of constructing rules, which can make the ants in the search process initially explore and develop in the later period. Our experiments, which use 12 publicly available data sets, show that the predictive accuracy obtained by Ant-MinerPAE implementing the proposed pheromone attraction and exclusion strategy is statistically significantly higher than the predictive accuracy of other rule induction classification algorithms, such as CN2, C4.5 rules, PSO/ACO2, Ant-Miner, and cAnt-MinerPB. Furthermore, the rules discovered by Ant-MinerPAE are considerably simpler than those discovered by its counterparts. © 2016, Springer-Verlag Berlin Heidelberg.","Ant colony algorithm; Classification rule; Data mining; Pheromone attraction and exclusion","Algorithms; Ant colony optimization; Iterative methods; Miners; Optimization; Particle swarm optimization (PSO); Ant colony algorithms; Ant Colony Optimization algorithms; Classification algorithm; Classification mining; Classification rules; Exploration and development; Pheromone attraction and exclusion; Pre-mature convergences; Data mining",2-s2.0-84964558149
"Kalenkova A.A., van der Aalst W.M.P., Lomazova I.A., Rubin V.A.","Process mining using BPMN: relating event logs and process models",2017,"Software and Systems Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944916586&doi=10.1007%2fs10270-015-0502-0&partnerID=40&md5=17952c31349ea77c0da3c89c99db5eee","Process-aware information systems (PAIS) are systems relying on processes, which involve human and software resources to achieve concrete goals. There is a need to develop approaches for modeling, analysis, improvement and monitoring processes within PAIS. These approaches include process mining techniques used to discover process models from event logs, find log and model deviations, and analyze performance characteristics of processes. The representational bias (a way to model processes) plays an important role in process mining. The BPMN 2.0 (Business Process Model and Notation) standard is widely used and allows to build conventional and understandable process models. In addition to the flat control flow perspective, subprocesses, data flows, resources can be integrated within one BPMN diagram. This makes BPMN very attractive for both process miners and business users, since the control flow perspective can be integrated with data and resource perspectives discovered from event logs. In this paper, we describe and justify robust control flow conversion algorithms, which provide the basis for more advanced BPMN-based discovery and conformance checking algorithms. Thus, on the basis of these conversion algorithms low-level models (such as Petri nets, causal nets and process trees) discovered from event logs using existing approaches can be represented in terms of BPMN. Moreover, we establish behavioral relations between Petri nets and BPMN models and use them to adopt existing conformance checking and performance analysis techniques in order to visualize conformance and performance information within a BPMN diagram. We believe that the results presented in this paper can be used for a wide variety of BPMN mining and conformance checking algorithms. We also provide metrics for the processes discovered before and after the conversion to BPMN structures. Cases for which conversion algorithms produce more compact or more complicated BPMN models in comparison with the initial models are identified. © 2015, Springer-Verlag Berlin Heidelberg.","Bisimulation; BPMN (Business Process Model and Notation); Conformance checking; Petri nets; Process discovery; Process mining","Data mining; Petri nets; Robust control; Bisimulations; Business process model; Conformance checking; Process Discovery; Process mining; Algorithms",2-s2.0-84944916586
"Chen W., Hsieh K.-K., Wang L.-C., Bhadra J.","Data-Driven Test Plan Augmentation for Platform Verification",2017,"IEEE Design and Test",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029798915&doi=10.1109%2fMDAT.2017.2713390&partnerID=40&md5=22590f258ce51d288a3fdf8e32c52947","Editor's note: This article points out that the fundamental problem of platform verification is incompleteness of the test plan and proposes an unsupervised learning approach to augment the test plan.-Magdy Abadir, Helic Inc. © 2013 IEEE.","Data Mining; Test Generation; Verification","Verification; Data driven; Test generations; Test plan; Data mining",2-s2.0-85029798915
"Reiner B.I.","Redefining the Practice of Peer Review Through Intelligent Automation Part 1: Creation of a Standardized Methodology and Referenceable Database",2017,"Journal of Digital Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025825911&doi=10.1007%2fs10278-017-0004-4&partnerID=40&md5=d964ca12b2ddea913880edc0d2f89361","Conventional peer review practice is compromised by a number of well-documented biases, which in turn limit standard of care analysis, which is fundamental to determination of medical malpractice. In addition to these intrinsic biases, other existing deficiencies exist in current peer review including the lack of standardization, objectivity, retrospective practice, and automation. An alternative model to address these deficiencies would be one which is completely blinded to the peer reviewer, requires independent reporting from both parties, utilizes automated data mining techniques for neutral and objective report analysis, and provides data reconciliation for resolution of finding-specific report differences. If properly implemented, this peer review model could result in creation of a standardized referenceable peer review database which could further assist in customizable education, technology refinement, and implementation of real-time context and user-specific decision support. © 2017, Society for Imaging Informatics in Medicine.","Data mining; Peer review; Report analysis","Data mining; Automated data mining; Data reconciliation; Decision supports; Intelligent automation; Intrinsic bias; Peer review; Report analysis; Standard of cares; Automation",2-s2.0-85025825911
"Kumar K., Sudhakar P.","Performance study and challenges for algorithms mining rare and correlated items in video dataset",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031716147&partnerID=40&md5=e8ce598f373bd2cbbef751863bcb4277","Data mining research is much occupied with Association rule mining (ARM) wherein these rules attempts to mine frequent items. However, in recent years, there has been an increasing demand for mining the infrequent or rare or minimal correlated items. The point is that interesting relationship among infrequent items has not been discussed much in the literature. In this paper, we conduct a comparative performance study on three such algorithms namely AprioriRare, AprioriInverse and CORI. After studying their pros and cons, we suggest how they can be applied in mining the video transaction datasets. © 2006-2017 Asian Research Publishing Network (ARPN).","Association rule; Correlation; Data mining; Frequent items; Infrequent items",,2-s2.0-85031716147
"Wan Y., Si Y.-W.","A formal approach to chart patterns classification in financial time series",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019924741&doi=10.1016%2fj.ins.2017.05.028&partnerID=40&md5=0010c7911b0fee19bb050898ab6848ab","Classifying chart patterns from input subsequences is a crucial pre-processing step in technical analysis. In this paper, we compile comprehensive formal specifications of 53 chart patterns reported in the literature. A first-order logic representation is chosen to describe the shape and corresponding constraints of each pattern. These formal specifications are formulated in such a way that data mining algorithms can use them for classification without significant modification. These formal specifications are also intended to serve as a reference model for future research in the chart patterns classification area. Using these formal specifications, we perform extensive experiments using real datasets from NYSE Composite (NYSE), Hang Seng Index (HSI), and Amazon (AMZN). The performance of the proposed method is compared against Template Based (TB), Euclidean Distance (ED), and Dynamic Time Warping (DTW) approaches. The experimental results show that the rules translated from the specifications can be effectively used to identify chart patterns from real datasets. © 2017 Elsevier Inc.","Chart patterns; Financial time series; Formal specification; Pattern matching; Technical analysis","Data mining; Financial data processing; Formal logic; Pattern matching; Specifications; Time series; Time series analysis; Chart patterns; Data mining algorithm; Dynamic time warping; Financial time series; Patterns classification; Pre-processing step; Reference modeling; Technical analysis; Formal specification",2-s2.0-85019924741
"Tan A., Wu W., Tao Y.","A set-cover-based approach for the test-cost-sensitive attribute reduction problem",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969812448&doi=10.1007%2fs00500-016-2173-3&partnerID=40&md5=cf8b78caae617660d6550c2e82fff885","In data mining application, the test-cost-sensitive attribute reduction is an important task which aims to decrease the test cost of data. In operational research, the set cover problem is a typical optimization problem and has a long investigation history compared to the attribute reduction problem. In this paper, we employ the methods of set cover problem to deal with the test-cost-sensitive attribute reduction. First, we equivalently transform the test-cost-sensitive reduction problem into the set cover problem by using a constructive approach. It is shown that computing a reduct of a decision system with minimal test cost is equal to computing an optimal solution of the set cover problem. Then, a set-cover-based heuristic algorithm is introduced to solve the test-cost-sensitive reduction problem. In the end, we conduct several numerical experiments on data sets from UCI machine learning repository. Experimental results indicate that the set-cover-based algorithm has superior performances in most cases, and the algorithm is efficient on data sets with many attributes. © 2016, Springer-Verlag Berlin Heidelberg.","Attribute reduction; Decision table; Rough set; Set cover problem; Test cost","Artificial intelligence; Costs; Data mining; Data reduction; Decision tables; Heuristic algorithms; Learning systems; Optimization; Rough set theory; Testing; Attribute reduction; Constructive approach; Data mining applications; Numerical experiments; Optimization problems; Set cover problem; Test cost; UCI machine learning repository; Cost reduction",2-s2.0-84969812448
"Comas D.S., Pastore J.I., Bouchet A., Ballarin V.L., Meschino G.J.","Interpretable interval type-2 fuzzy predicates for data clustering: A new automatic generation method based on self-organizing maps",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024831343&doi=10.1016%2fj.knosys.2017.07.012&partnerID=40&md5=b16a8a61a2e9b20b19ef3aace290631e","In previous works, we proposed two methods for data clustering based on automatically discovered fuzzy predicates which were referred to as SOM-based Fuzzy Predicate Clustering (SFPC) [Meschino et al., Neurocomputing, 147, 47–59 (2015)] and Type-2 Data-based Fuzzy Predicate Clustering (T2-DFPC) [Comas et al., Expert Syst. Appl., 68, 136–150 (2017)]. In such methods, fuzzy predicates allow both data clustering and knowledge discovering about the obtained clusters. This last feature constitutes novelty comparing to other existing approaches and it is a major contribution in the data clustering field. Based on these previous methods, in the present paper a new automatic clustering method based on fuzzy predicates is proposed which uses Self-Organizing Maps (SOMs) and is called Type-2 SOM-based Fuzzy Predicate Clustering (T2-SFPC). The new method does not require any prior knowledge about the clustering addressed. First, a random partition is defined on the dataset to be clustered and SOMs are configured and trained using the resulting data subsets. Second, an automatic clustering approach is applied on the SOM codebooks, discovering representative data of the different clusters, which are called cluster prototypes. Third, interval type-2 membership function formed by Gaussian-shape sub-functions and fuzzy predicates are defined, allowing data clustering and its interpretation. The proposed method preserves all the advantages of the previous methods SFPC and T2-DFPC in relation to the knowledge extraction capabilities and their potential application on distributed clustering and parallel computing, but results obtained on several public datasets tested showed more compactness and separation of the clusters defined by the T2-SFPC, outperforming both the previous methods and the several classical clustering approaches tested, considering internal and external validation indices. Additionally, both clustering interpretation and optimization capabilities are improved by the proposed method when compared to the methods SFPC and T2-DFPC. © 2017 Elsevier B.V.","Fuzzy predicates; Interpretable clustering; Interval type-2 fuzzy logic; Knowledge discovery; Self-organizing maps","Cluster analysis; Clustering algorithms; Computation theory; Conformal mapping; Data mining; Membership functions; Self organizing maps; Distributed clustering; Fuzzy predicates; Interpretable clustering; Interval type-2 fuzzy; Interval type-2 fuzzy logic; Knowledge extraction; Optimization capabilities; Self organizing maps(soms); Fuzzy logic",2-s2.0-85024831343
"Zou X., Feng Y., Li H., Zhu J.","An Adaptive Strips Method for Extraction Buildings from Light Detection and Ranging Data",2017,"IEEE Geoscience and Remote Sensing Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028966909&doi=10.1109%2fLGRS.2017.2723435&partnerID=40&md5=f8856b659ed15c3339f7e75bd91c6c6c","A method is proposed for extracting building points set from light detecting and ranging (LiDAR) data. This proposed method is based on a strip strategy to filter building points and extract the edge point set rapidly and effectively in large-scale urban building groups. This approach divides the LiDAR data into small strips and classifies each strip of data with an adaptive-weight polynomial in the x-or y-direction. The building edge set can then be extracted by utilizing the regional clustering relationships between points. The results of a series of experiments show that our method can not only filter the LiDAR point cloud, which performs better than existing methods, but also determine the building edge set efficiently, with an average accuracy rate of up to 91.1%. © 2017 IEEE.","Edge extraction; filters; light detecting and ranging (LiDAR); strip strategy","Buildings; Data mining; Edge detection; Extraction; Filters (for fluids); Filtration; Tracking radar; Edge extraction; Image edge detection; strip strategy; Strips; Three-dimensional display; Urban areas; Optical radar",2-s2.0-85028966909
"Samie M.E., Hamzeh A.","Community detection in dynamic social networks: A local evolutionary approach",2017,"Journal of Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029508917&doi=10.1177%2f0165551516657717&partnerID=40&md5=cae55c7cd015263ef57652058b392e18","Communities in social networks are groups of individuals who are connected with specific goals. Discovering information on the structure, members and types of changes of communities have always been of great interest. Despite the extensive global researches conducted on these, discovery has not been confirmed yet and researchers try to find methods and improve estimated techniques by using Data Mining tools, Graph Mining tools and artificial intelligence techniques. This paper proposes a novel two-phase approach based on global and local information to detect communities in social network. It explores the global information in the first phase and then exploits the local information in the second phase to discover communities more accurately. It also proposes a novel algorithm which exploits the local information and mines deeply for the second phase. Experimental results show that the proposed method has better performance and achieves more accurate results compared with the previous ones. © Chartered Institute of Library and Information Professionals.","Community detection; genetic algorithm; Lmetric; social network","Data mining; Genetic algorithms; Social networking (online); Artificial intelligence techniques; Community detection; Data-mining tools; Dynamic social networks; Evolutionary approach; Global and local informations; Global informations; Lmetric; Population dynamics",2-s2.0-85029508917
"Zhang M., Ding S., Bian Y.","The online reviews' effects on internet consumer behavior: An exploratory study",2017,"Journal of Electronic Commerce in Organizations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029157996&doi=10.4018%2fJECO.2017100107&partnerID=40&md5=8de059ecfb295bbf282c0d40dc4a42aa","Consumer behaviors have always been the hot spot of the study. With the arrival of the network age and the popularity of e-commerce, the consumption pattern of shopping online is beginning. Until December of 2014, there were 649 million Internet users in China, and it's increasing. In the year of 2014, the number of most mainstream online shoppers has increased by 23.7% and shopping online has become an irresistible trend. New consumption mode can produce a new research subject, the study of Internet consumer behaviors becomes very important. This article used LocoySpider, a data mining software, to mine Taobao online reviews. Through the analysis of online reviews, we can study the effect of online reviews on consumer behaviors. During the study, we set the risk perception as intermediary variable. By the study, we can help the electronic commercial enterprises to attract customers, retain customers and reduce the information search cost. Copyright © 2017, IGI Global.","Consumer Behavior; Data Mining; Online Reviews; Risk Perception","Data mining; Electronic commerce; Internet; Risk perception; Commercial enterprise; Consumption patterns; Data-mining software; Exploratory studies; Information search; Online reviews; Online shoppers; Research subjects; Consumer behavior",2-s2.0-85029157996
"Hornbrook M.C., Goshen R., Choman E., O’Keeffe-Rosetti M., Kinar Y., Liles E.G., Rust K.C.","Early Colorectal Cancer Detected by Machine Learning Model Using Gender, Age, and Complete Blood Count Data",2017,"Digestive Diseases and Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027979337&doi=10.1007%2fs10620-017-4722-8&partnerID=40&md5=19e516f5f4e23e7e44dc30aa309ab99d","Background: Machine learning tools identify patients with blood counts indicating greater likelihood of colorectal cancer and warranting colonoscopy referral. Aims: To validate a machine learning colorectal cancer detection model on a US community-based insured adult population. Methods: Eligible colorectal cancer cases (439 females, 461 males) with complete blood counts before diagnosis were identified from Kaiser Permanente Northwest Region’s Tumor Registry. Control patients (n = 9108) were randomly selected from KPNW’s population who had no cancers, received at ≥1 blood count, had continuous enrollment from 180 days prior to the blood count through 24 months after the count, and were aged 40–89. For each control, one blood count was randomly selected as the pseudo-colorectal cancer diagnosis date for matching to cases, and assigned a “calendar year” based on the count date. For each calendar year, 18 controls were randomly selected to match the general enrollment’s 10-year age groups and lengths of continuous enrollment. Prediction performance was evaluated by area under the curve, specificity, and odds ratios. Results: Area under the receiver operating characteristics curve for detecting colorectal cancer was 0.80 ± 0.01. At 99% specificity, the odds ratio for association of a high-risk detection score with colorectal cancer was 34.7 (95% CI 28.9–40.4). The detection model had the highest accuracy in identifying right-sided colorectal cancers. Conclusions: ColonFlag® identifies individuals with tenfold higher risk of undiagnosed colorectal cancer at curable stages (0/I/II), flags colorectal tumors 180–360 days prior to usual clinical diagnosis, and is more accurate at identifying right-sided (compared to left-sided) colorectal cancers. © 2017, Springer Science+Business Media, LLC.","Area under receiver operating characteristics curve; Blood cell count; Colonoscopy; Colorectal neoplasms; Hemoglobin; Medical informatics computing","adult; aged; Article; blood cell count; colorectal cancer; controlled study; diagnostic accuracy; diagnostic equipment; diagnostic test accuracy study; early cancer; early cancer diagnosis; female; human; machine learning; major clinical study; male; prediction; priority journal; receiver operating characteristic; sex difference; United States; age; algorithm; area under the curve; blood; blood cell count; colonoscopy; Colorectal Neoplasms; computer assisted diagnosis; data mining; early cancer diagnosis; middle aged; odds ratio; pathology; patient referral; predictive value; procedures; register; reproducibility; risk factor; validation study; very elderly; Adult; Age Factors; Aged; Aged, 80 and over; Algorithms; Area Under Curve; Blood Cell Count; Colonoscopy; Colorectal Neoplasms; Data Mining; Diagnosis, Computer-Assisted; Early Detection of Cancer; Female; Humans; Machine Learning; Male; Middle Aged; Odds Ratio; Predictive Value of Tests; Referral and Consultation; Registries; Reproducibility of Results; Risk Factors; ROC Curve; Sex Factors",2-s2.0-85027979337
"Wang G., Yang J., Li R.","Imbalanced SVM-based anomaly detection algorithm for imbalanced training datasets",2017,"ETRI Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032495846&doi=10.4218%2fetrij.17.0116.0879&partnerID=40&md5=32a436ea9fb9618209164c5f9f16a2c3","Abnormal samples are usually difficult to obtain in production systems, resulting in imbalanced training sample sets. Namely, the number of positive samples is far less than the number of negative samples. Traditional Support Vector Machine (SVM)-based anomaly detection algorithms perform poorly for highly imbalanced datasets: The learned classification hyperplane skews toward the positive samples, resulting in a high false-negative rate. This article proposes a new imbalanced SVM (termed ImSVM)- based anomaly detection algorithm, which assigns a different weight for each positive support vector in the decision function. ImSVM adjusts the learned classification hyperplane to make the decision function achieve a maximum GMean measure value on the dataset. The above problem is converted into an unconstrained optimization problem to search the optimal weight vector. Experiments are carried out on both Cloud datasets and Knowledge Discovery and Data Mining datasets to evaluate ImSVM. Highly imbalanced training sample sets are constructed. The experimental results show that ImSVM outperforms over-sampling techniques and several existing imbalanced SVM-based techniques. © 2017 ETRI.","Anomaly detection; Decision function; GMean; Imbalanced training sample set; Support vector machine (SVM)","Classification (of information); Data mining; Geometry; Optimization; Sampling; Signal detection; Vectors; Anomaly detection; Anomaly-detection algorithms; Decision functions; GMean; Knowledge discovery and data minings; Optimal weight vector; Training sample; Unconstrained optimization problems; Support vector machines",2-s2.0-85032495846
"Marzel A., Shilaih M., Turk T., Campbell N.K., Yang W.-L., Böni J., Yerly S., Klimkait T., Aubert V., Furrer H., Calmy A., Battegay M., Cavassini M., Bernasconi E., Schmid P., Metzner K.J., Günthard H.F., Kouyos R.D., the Swiss HIV Cohort Study (SHCS), Bucher H.C., Burton-Jeangros C., Dollenmaier G., Egger M., Elzi L., Fehr J., Fellay J., Fux C.A., Gorgievski M., Haerry D., Hasse B., Hirsch H.H., Hoffmann M., Hösli I., Kahlert C., Kaiser L., Keiser O., Kovari H., Ledergerber B., Martinetti G., de Tejada B.M., Müller N., Nadal D., Nicca D., Pantaleo G., Rauch A., Regenass S., Rickenbach M., Rudin C., Schöni-Affolter F., Schmid P., Schüpbach J., Speck R., Tarr P., Trkola A., Vernazza P., Weber R., Yerly S.","Mining for pairs: shared clinic visit dates identify steady HIV-positive partnerships",2017,"HIV Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017367206&doi=10.1111%2fhiv.12507&partnerID=40&md5=62c2b828196d06298bc9ab42a929f471","Objectives: Here we examined the hypothesis that some stable HIV-infected partnerships can be found in cohort studies, as the patients frequently attend the clinic visits together. Methods: Using mathematical approximations and shuffling to derive the probabilities of sharing a given number of visits by chance, we identified and validated couples that may represent either transmission pairs or serosorting couples in a stable relationship. Results: We analysed 434 432 visits for 16 139 Swiss HIV Cohort Study patients from 1990 to 2014. For 89 pairs, the number of shared visits exceeded the number expected. Of these, 33 transmission pairs were confirmed on the basis of three criteria: an extensive phylogenetic tree, a self-reported steady HIV-positive partnership, and risk group affiliation. Notably, 12 of the validated transmission pairs (36%; 12 of 33) were of a mixed ethnicity with a large median age gap [17.5 years; interquartile range (IQR) 11.8−22 years] and these patients harboured HIV-1 of predominantly non-B subtypes, suggesting imported infections. Conclusions: In the context of the surge in research interest in HIV transmission pairs, this simple method widens the horizons of research on within-pair quasi-species exchange, transmitted drug resistance and viral recombination at the biological level and targeted prevention at the public health level. © 2017 British HIV Association","cohort studies; data mining; epidemiology; HIV; phylogeny; transmission","adult; AIDS patient; ambulatory care; antiretroviral therapy; antiviral resistance; Article; cohort analysis; condom use; data mining; ethnicity; female; follow up; genetic distance; heterosexual female; heterosexual male; HIV serosorting; human; Human immunodeficiency virus 1; Human immunodeficiency virus infected patient; Human immunodeficiency virus infection; major clinical study; male; mathematical analysis; men who have sex with men; phylogenetic tree; priority journal; self report; sexual transmission; Switzerland; unprotected sex; virus load; virus transmission",2-s2.0-85017367206
"San Pedro M.O.Z., Baker R.S., Heffernan N.T.","An Integrated Look at Middle School Engagement and Learning in Digital Environments as Precursors to College Attendance",2017,"Technology, Knowledge and Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020743622&doi=10.1007%2fs10758-017-9318-z&partnerID=40&md5=19830030bf5d6911ff71bcc7615ea4f1","Middle school is an important phase in the academic trajectory, which plays a major role in the path to successful post-secondary outcomes such as going to college. Despite this, research on factors leading to college-going choices do not yet utilize the extensive fine-grained data now becoming available on middle school learning and engagement. This paper uses interaction-based data-mined assessments of student behavior, academic emotions and knowledge from a middle school online learning environment, and evaluates their relationships with different outcomes in high school and college. The data-mined measures of student behavior, emotions, and knowledge are used in three analyses: (1) to develop a prediction model of college attendance; (2) to evaluate their relationships to intermediate outcomes on the path to college attendance such as math and science course-taking during high school; (3) to develop an overall path model between the educational experiences students have during middle school, their high school experiences, and their eventual college attendance. This gives a richer picture of the cognitive and non-cognitive mechanisms that students experience throughout varied phases in their years in school, and how they may be related to one another. Such understanding may provide educators with information about students’ trajectories within the college pipeline. © 2017, Springer Science+Business Media B.V.","Academic emotion; Educational data mining; Educational technology; Engagement; Learning analytics; Middle school learning; Post-secondary outcomes","Computer aided instruction; Data mining; Digital integrated circuits; E-learning; Education; Educational technology; Online systems; Academic Emotions; Educational data mining; Engagement; Learning analytics; Middle school; Post-secondary outcomes; Students",2-s2.0-85020743622
"Ali R.Y., Gunturi V.M.V., Kotz A.J., Eftelioglu E., Shekhar S., Northrop W.F.","Discovering non-compliant window co-occurrence patterns",2017,"GeoInformatica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011000709&doi=10.1007%2fs10707-016-0289-3&partnerID=40&md5=caf31b37cf4b8f4c953594261924b928","Given a set of trajectories annotated with measurements of physical variables, the problem of Non-compliant Window Co-occurrence (NWC) pattern discovery aims to determine temporal signatures in the explanatory variables which are highly associated with windows of undesirable behavior in a target variable. NWC discovery is important for societal applications such as eco-friendly transportation (e.g. identifying engine signatures leading to high greenhouse gas emissions). Challenges of designing a scalable algorithm for NWC discovery include the non-monotonicity of popular spatio-temporal statistical interest measures of association such as the cross-K function which renders the anti-monotone pruning based algorithms (e.g. Apriori) inapplicable for such interest measures. In our preliminary work, we proposed two upper bounds for the cross-K function and a top-down multi-parent tracking approach that uses these bounds for filtering out uninteresting candidate patterns and then applies a minimum support (i.e. frequency) threshold as a post-processing step to filter out chance patterns. In this paper, we propose a novel bi-directional pruning approach (BDNMiner) that combines top-down pruning based on the cross-K function threshold with bottom-up pruning based on the minimum support threshold to efficiently mine NWC patterns. Case studies with real world engine data demonstrates the ability of the proposed approach to discover patterns which are interesting to engine scientists. Experimental evaluation on real-world data show that the proposed approach yields substantial computational savings compared to prior work. © 2017, Springer Science+Business Media New York.","Co-occurrence patterns; Temporal data mining; Transportation","Data mining; Engines; Filtration; Gas emissions; Greenhouse gases; Transportation; Co-occurrence pattern; Computational savings; Experimental evaluation; Explanatory variables; Measures of association; Minimum support thresholds; Scalable algorithms; Temporal data mining; Exhaust systems (engine); algorithm; data mining; spatiotemporal analysis; tracking; transportation",2-s2.0-85011000709
"Scheurwegs E., Cule B., Luyckx K., Luyten L., Daelemans W.","Selecting relevant features from the electronic health record for clinical code prediction",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029533726&doi=10.1016%2fj.jbi.2017.09.004&partnerID=40&md5=b681f0a700d844b2ab93ea9653f353c4","A multitude of information sources is present in the electronic health record (EHR), each of which can contain clues to automatically assign diagnosis and procedure codes. These sources however show information overlap and quality differences, which complicates the retrieval of these clues. Through feature selection, a denser representation with a consistent quality and less information overlap can be obtained. We introduce and compare coverage-based feature selection methods, based on confidence and information gain. These approaches were evaluated over a range of medical specialties, with seven different medical specialties for ICD-9-CM code prediction (six at the Antwerp University Hospital and one in the MIMIC-III dataset) and two different medical specialties for ICD-10-CM code prediction. Using confidence coverage to integrate all sources in an EHR shows a consistent improvement in F-measure (49.83% for diagnosis codes on average), both compared with the baseline (44.25% for diagnosis codes on average) and with using the best standalone source (44.41% for diagnosis codes on average). Confidence coverage creates a concise patient stay representation independent of a rigid framework such as UMLS, and contains easily interpretable features. Confidence coverage has several advantages to a baseline setup. In our baseline setup, feature selection was limited to a filter removing features with less than five total occurrences in the trainingset. Prediction results improved consistently when using multiple heterogeneous sources to predict clinical codes, while reducing the number of features and the processing time. © 2017 Elsevier Inc.","Clinical coding; Data integration; Data representation; EHR mining; Feature selection","Codes (symbols); Data integration; Diagnosis; Filtration; Forecasting; Records management; Clinical coding; Consistent quality; Data representations; Electronic health record; Feature selection methods; Heterogeneous sources; Information sources; Medical specialties; Feature extraction; Article; benchmarking; clinical classification; coding; data mining; electronic health record; health care quality; human; ICD-10-CM; ICD-10-PCS; ICD-9-CM; priority journal; program evaluation; Unified Medical Language System",2-s2.0-85029533726
"Vearncombe J., Riganti A., Isles D., Bright S.","Data upcycling",2017,"Ore Geology Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026728375&doi=10.1016%2fj.oregeorev.2017.07.009&partnerID=40&md5=d9e4f51445e11c312ca2c6215155b260","Mineral exploration and mining are data-driven industries. Here, we emphasize the role of “data upcycling” as a significant contributor to modern exploration. Upcycling can take three basic forms, all aimed at enhancing the veracity of data: (1) re-collection of data, (2) collection of complimentary data, and (3) assessment and innovative portrayal/integration of data. Upcycling of previously collected (or legacy) data allows information to be integrated into modern datasets. This paper offers perspectives and examples on how data upcycling benefits mineral exploration, with short case studies from Western Australia highlighting the role of government, service providers and resource explorers. © 2017","Data upcycling; Fit-for-purpose; JORC; Legacy data; Mineral exploration; QA/QC","Mineral exploration; Minerals; Data upcycling; Fit for purpose; JORC; Legacy data; QA/QC; Data acquisition; mineral exploration; mineral resource; mining; state role; Australia; Western Australia",2-s2.0-85026728375
"Wang T., Tian X., Yu M., Qi X., Yang L.","Stage division and pattern discovery of complex patient care processes",2017,"Journal of Systems Science and Complexity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015623143&doi=10.1007%2fs11424-017-5302-x&partnerID=40&md5=285b84124b2711dee9ac1dd05f729a53","This paper studies the design of a clinical pathway that defines medical service activities within each stage of a patient care process. Much prior research has developed clinical process models that consider the trajectory of services occurring in a care process, by using data mining techniques on process execution logs. A novel approach that provides a more efficient way of clinical pathway design is introduced in this paper. Based on the strategy of TEI@I methodology, the proposed approach integrates statistical methods, optimization techniques and data mining. With the preprocessed data, a complex care process is subsequently divided into several medical stages, and then the patterns of each stage are identified, and thus a clinical pathway is developed. Finally, the proposed method is applied to the real world, using archival data derived from a hospital in Beijing, about three diseases that involve various departments, with an average of 300 samples for each disease. The results of realworld applications demonstrate that the proposed method can automatically and efficiently facilitate clinical pathways design. The main contributions to the field in this paper include (a) a new application of TEI@I methodology in healthcare domain, (b) a novel method for complex processes analysis, (c) tangible evidence of automatic clinical pathways design in the real world. © 2017, Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg.","Clinical pathway; frequent pattern mining; stage division; TEI@I; virtual business","Computer science; Systems science; Clinical pathways; Frequent pattern mining; Healthcare domains; Optimization techniques; Patient care process; Pattern discovery; Pre-processed data; Process execution; Data mining",2-s2.0-85015623143
"Li J., Benediktsson J.A., Zhang B., Yang T., Plaza A.","Spatial Technology and Social Media in Remote Sensing: A Survey",2017,"Proceedings of the IEEE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028474147&doi=10.1109%2fJPROC.2017.2729890&partnerID=40&md5=eb329af784b446d7e4611e976fde89ed","The rapid development of social media data and the associated growth in volume, velocity, and variety has fostered the idea of using these data to guide traditional remote sensing image retrieval and information extraction tasks. Although important progress has been made in recent years in harvesting spatial and temporal data from social media, the exploitation of these data for decision making still needs further investigation, particularly in the context of its integration with remote sensing and geographic information systems. In this paper, we first discuss the relation between localization techniques and spatial technologies, pointing out their similarities and differences. Then, we provide a discussion on location analysis of social media data, and the fusion of multiple data sources, with specific attention to the integration of social media content (including localization) with remote sensing-based spatial technologies. Next, we provide specific examples addressing the use of social media data to perform information extraction from large remote sensing data repositories. Although significant possibilities for the integration of localization and spatial technologies can be seen in the examples provided, our survey suggests that the convergence of remote sensing and social media data will continue to deeply transform these technologies. © 1963-2012 IEEE.","Localization; remote sensing; social media; spatial technologies","Artificial intelligence; Data integration; Data mining; Decision making; Earth (planet); Geographic information systems; Geology; Image reconstruction; Image retrieval; Information analysis; Information retrieval; Information systems; Integration; Social networking (online); Social sciences computing; Surveys; Localization; Localization technique; Multiple data sources; Remote sensing data; Remote sensing image retrieval; Social media; Social network services; Spatial technologies; Remote sensing",2-s2.0-85028474147
"Huang J., Keung J.W., Sarro F., Li Y.-F., Yu Y.T., Chan W.K., Sun H.","Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study",2017,"Journal of Systems and Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024833152&doi=10.1016%2fj.jss.2017.07.012&partnerID=40&md5=69d3f010b51fccd7e2b9e5719f182806","Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation. © 2017 Elsevier Inc.","Cross-validation; Empirical software engineering estimation; Imputation; KNN; Missing data","Autocorrelation; Classification (of information); Computer software selection and evaluation; Data handling; Data mining; Learning algorithms; Motion compensation; Software engineering; Classification accuracy; Cross validation; Empirical Software Engineering; Experimental assessment; Imputation; K nearest neighbor (KNN); Missing data; Missing data imputations; Nearest neighbor search",2-s2.0-85024833152
"Lin W.-C., Ke S.-W., Tsai C.-F.","When should we ignore examples with missing values?",2017,"International Journal of Data Warehousing and Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028038261&doi=10.4018%2fIJDWM.2017100104&partnerID=40&md5=6d77ba5c7c601f4dd374af7e4cf6b2cf","In practice, the dataset collected from data mining usually contains some missing values. It is common practice to perform case deletion by ignoring those data with missing values if the missing rate is certainly small. The aim of this paper is to answer the following question: When should one directly ignore sampled data with missing values? By using different types of datasets having various numbers of attributes, data samples, and classes, it is found that there are some specific patterns that can be considered for case deletion over different datasets without significant performance degradation. In particular, these patterns are extracted to act as the decision rules by a decision tree model. In addition, a comparison is made between cases with deletion and imputation over different datasets with the allowed missing rates and the decision rules. The results show that the classification performance results obtained by case deletion and imputation are similar, which demonstrates the reliability of the extracted decision rules. © 2017, IGI Global.","Case Deletion; Categorical Data; Classification; Data Mining; Imputation; Machine Learning; Missing Values; Numerical Data","Data mining; Decision trees; Learning systems; Categorical data; Classification performance; Decision rules; Decision tree modeling; Imputation; Missing values; Numerical data; Performance degradation; Classification (of information)",2-s2.0-85028038261
"Li E., Xia J., Du P., Lin C., Samat A.","Integrating Multilayer Features of Convolutional Neural Networks for Remote Sensing Scene Classification",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023777322&doi=10.1109%2fTGRS.2017.2711275&partnerID=40&md5=4ca13ba97eab6dbd4a64b0153515e799","Scene classification from remote sensing images provides new possibilities for potential application of high spatial resolution imagery. How to efficiently implement scene recognition from high spatial resolution imagery remains a significant challenge in the remote sensing domain. Recently, convolutional neural networks (CNN) have attracted tremendous attention because of their excellent performance in different fields. However, most works focus on fully training a new deep CNN model for the target problems without considering the limited data and time-consuming issues. To alleviate the aforementioned drawbacks, some works have attempted to use the pretrained CNN models as feature extractors to build a feature representation of scene images for classification and achieved successful applications including remote sensing scene classification. However, existing works pay little attention to exploring the benefits of multilayer features for improving the scene classification in different aspects. As a matter of fact, the information hidden in different layers has great potential for improving feature discrimination capacity. Therefore, this paper presents a fusion strategy for integrating multilayer features of a pretrained CNN model for scene classification. Specifically, the pretrained CNN model is used as a feature extractor to extract deep features of different convolutional and fully connected layers; then, a multiscale improved Fisher kernel coding method is proposed to build a mid-level feature representation of convolutional deep features. Finally, the mid-level features extracted from convolutional layers and the features of fully connected layers are fused by a principal component analysis/spectral regression kernel discriminant analysis method for classification. For validation and comparison purposes, the proposed approach is evaluated via experiments with two challenging high-resolution remote sensing data sets, and shows the competitive performance compared with fully trained CNN models, fine-tuning CNN models, and other related works. © 2017 IEEE.","Convolutional neural networks (CNN); feature fusion; improved Fisher kernel; scene classification; spectral regression kernel discriminant analysis (SRKDA)","Convolution; Convolutional codes; Data mining; Data visualization; Discriminant analysis; Feature extraction; Flow visualization; Image classification; Image reconstruction; Image resolution; Multilayer neural networks; Multilayers; Neural networks; Personnel training; Principal component analysis; Remote sensing; Semantics; Video streaming; Convolutional neural network; Feature fusion; Fisher kernels; Kernel discriminant analysis; Scene classification; Classification (of information); algorithm; artificial neural network; data processing; data set; discriminant analysis; image classification; principal component analysis; remote sensing; spatial resolution",2-s2.0-85023777322
"Ye L., Cao Z., Xiao Y.","DeepCloud: Ground-Based Cloud Image Categorization Using Deep Convolutional Features",2017,"IEEE Transactions on Geoscience and Remote Sensing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022045459&doi=10.1109%2fTGRS.2017.2712809&partnerID=40&md5=eca56af45aca5c3e478e74d037131efe","Accurate ground-based cloud image categorization is a critical but challenging task that has not been well addressed. One of the essential issues that affect the performance is to extract the representative visual features. Nearly all of the existing methods rely on the hand-crafted descriptors (e.g., local binary patterns, CENsus TRsansform hISTogram, and scale-invariant feature transform). Their limited discriminative power indeed leads to the unsatisfactory performance. To alleviate this, we propose ""DeepCloud"" as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features. In the recent years, the deep convolutional neural network (CNN) has achieved the promising results in lots of computer vision and image understanding fields. Nevertheless, it has not been applied to cloud image classification yet. Thus, we actually pay the first effort to fill this blank. Since cloud image classification can be attributed to a multi-instance learning problem, simply employing the convolutional features within CNN cannot achieve the promising result. To address this, Fisher vector encoding is applied to executing the spatial feature aggregation and high-dimensional feature mapping on the raw deep convolutional features. Moreover, the hierarchical convolutional layers are used simultaneously to capture the fine textural characteristics and high-level semantic information in the unified manner. To further leverage the performance, a cloud pattern mining and selection method are also proposed. It targets at finding the discriminative local patterns to better distinguish the different kinds of clouds. The experiments on a challenging ground-based cloud image data set demonstrate the superiority of the proposition over the state-of-the-art methods. © 2017 IEEE.","Convolutional neural network (CNN); deep learning; Fisher vector (FV); ground-based cloud image categorization; pattern mining","Clouds; Convolution; Convolutional codes; Data mining; Deep learning; Deep neural networks; Education; Encoding (symbols); Extraction; Feature extraction; Flow visualization; Image classification; Image segmentation; Imaging systems; Meteorology; Neural networks; Semantics; Cloud image; Convolutional neural network; Fisher vectors; High dimensional feature; Pattern mining; Scale invariant feature transforms; State-of-the-art methods; Textural characteristic; Image processing; artificial neural network; cloud classification; computer vision; data mining; data set; ground-based measurement; hierarchical system; image classification; performance assessment; vector",2-s2.0-85022045459
"Pépin L., Kuntz P., Blanchard J., Guillet F., Suignard P.","Visual analytics for exploring topic long-term evolution and detecting weak signals in company targeted tweets",2017,"Computers and Industrial Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014741656&doi=10.1016%2fj.cie.2017.01.025&partnerID=40&md5=47318369084e9a9f9343a20eadc623b1","Business decision support tools, including social media data analysis, are required to help managers better understand trends and customer opinions. This paper presents a visual analytics-based approach to assist an expert user in tracking topics relative to his/her company from Twitter. Developed for visualizing topic long-term evolution and detecting weak signals, our process is composed of three complementary steps: (i) a time-dependent topic extraction based on a Latent Dirichlet Allocation, (ii) a topic relationship detection based on a dissimilarity which evaluates the topic proximities between consecutive time slots, and (iii) a topic evolution visualization inspired by a Sankey diagram popular in industrial environments to show dynamic relationships in a system. To test our approach, we have used a real-life dataset from the French energy company EDF from which we have analyzed the evolution of a corpus of more than 70 000 tweets related to this company published over one year, and detected different types of evolving patterns hidden by the data volume and commonly masked by fully automatic mining algorithms. © 2017 Elsevier Ltd","Social media; Topic mining; Twitter; Visual analytics; Weak signals","Decision support systems; Long Term Evolution (LTE); Signal detection; Social networking (online); Statistical tests; Statistics; Visualization; Social media; Topic minings; Twitter; Visual analytics; Weak signals; Data mining",2-s2.0-85014741656
"Behadada O., Trovati M., Kontonatsios G., Korkontzelos Y.","A multinomial logistic regression approach for arrhythmia detection",2017,"International Journal of Distributed Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029150671&doi=10.4018%2fIJDST.2017100102&partnerID=40&md5=8426e1269d3e937b6a6478e42a40347d","Cardiovascular diseases are the leading causes on mortality in the world. Consequently, tools and methods providing useful and applicable insights into their assessment play a crucial role in the prediction and managements of specific heart conditions. In this article, we introduce a method based on multi-class Logistic Regression as a classifier to provide a powerful and accurate insight into cardiac arrhythmia, which is one of the predictors of serious vascular diseases. As suggested by our evaluation, this provides a robust, scalable, and accurate system, which can successfully tackle the challenges posed by the utilisation of big data in the medical sector. Copyright © 2017, IGI Global.","Big Data; Knowledge Extraction; Multinomial Logistic Regression; Text Mining","Data mining; Diseases; Regression analysis; Arrhythmia detection; Cardiac arrhythmia; Cardio-vascular disease; Knowledge extraction; Multi-class logistic regressions; Multinomial logistic regression; Text mining; Tools and methods; Big data",2-s2.0-85029150671
"Ruan T., Jiang G.","Analytical methodology for identification of novel per- and polyfluoroalkyl substances in the environment",2017,"TrAC - Trends in Analytical Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028619852&doi=10.1016%2fj.trac.2017.07.024&partnerID=40&md5=614b4705de384b1f0e40969b314d86a1","Continuing regulatory scrutiny of per- and polyfluoroalkyl substances (PFASs) has resulted in usage of fluorinated alternative products. Featured analytical methodologies are thus emerged recently for the identification of novel PFAS contaminants in environmental and biological matrices. In this review, protocols for tracing compositions of unidentified organofluorine components are summarized. Data-mining and chemical recognition techniques by high-resolution mass spectrometry are further overviewed. We also discuss knowledge gaps and future trends on separation and elucidation of emerging PFAS analogues with varied molecular structures. © 2017","Emerging contaminant; High-resolution mass spectrometry; Mass balance; Non-target screening; Per- and polyfluoroalkyl substances; Persistent, bio-accumulative and toxic chemicals; Quantitative structure-property relationship; Transformation; Unidentified organofluorine component","Data mining; Impurities; Mass spectrometry; Emerging contaminant; High resolution mass spectrometry; Mass balance; Non-target screenings; Organofluorine; Polyfluoroalkyl substances; Quantitative structure - property relationships; Toxic chemicals; Transformation; Spectrometry; hydroxyl radical; organofluorine derivative; perfluoroalkyl substance; polyfluoroalkyl substance; unclassified drug; chemical composition; chemical structure; data mining; data processing; ionization; mass spectrometry; nuclear magnetic resonance spectroscopy; organic pollution; physical chemistry; priority journal; quantitative analysis; Review",2-s2.0-85028619852
"Hosseini M., Azar F.T.","A new eigenvector selection strategy applied to develop spectral clustering",2017,"Multidimensional Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960086277&doi=10.1007%2fs11045-016-0391-6&partnerID=40&md5=96ca49792922faf1bf01669b3b9b2d46","Spectral methods are strong tools that can be used for extraction of the data’s structure based on eigenvectors of constructed affinity matrices. In this paper, we aim to propose some new measurement functions to evaluate the ability of each eigenvector of affinity matrix in data clustering. In the proposed strategy, each eigenvector’s elements are clustered by traditional fuzzy c-means algorithm and then informative eigenvectors selection is performed by optimization of an objective function which defined based on three criterions. These criterions are the compactness of clusters, distance between clusters and stability of clustering to evaluate each eigenvector based on considering the structure of clusters which placed on. Finally, Lagrange multipliers method is used to minimize the proposed objective function and extract the most informative eigenvectors. To indicate the merits of our algorithm, we consider UCI Machine Learning Repository databases, COIL20, YALE-B and PicasaWeb as benchmark data sets. Our simulation’s results confirm the superior performance of the proposed strategy in developing spectral clustering compared to conventional clustering methods and recent eigenvector selection based algorithms. © 2016, Springer Science+Business Media New York.","Curse of dimensionality; High-dimensional data; Pattern recognition; Spectral clustering","Artificial intelligence; Cluster analysis; Copying; Data mining; Eigenvalues and eigenfunctions; Function evaluation; Fuzzy clustering; Lagrange multipliers; Learning systems; Matrix algebra; Optimization; Pattern recognition; Structure (composition); Conventional clustering; Curse of dimensionality; Fuzzy C-means algorithms; High dimensional data; Lagrange multipliers method; Measurement function; Spectral clustering; UCI machine learning repository; Clustering algorithms",2-s2.0-84960086277
"Monica D.D., De Frutos-Escrig D., Montanari A., Murano A., Sciavicco G.","Evaluation of temporal datasets via interval temporal logic model checking",2017,"Leibniz International Proceedings in Informatics, LIPIcs",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030698046&doi=10.4230%2fLIPIcs.TIME.2017.11&partnerID=40&md5=72a0bd5bf5b82645748d9542e10bec6f","The problem of temporal dataset evaluation consists in establishing to what extent a set of temporal data (histories) complies with a given temporal condition. It presents a strong resemblance with the problem of model checking enhanced with the ability of rating the compliance degree of a model against a formula. In this paper, we solve the temporal dataset evaluation problem by suitably combining the outcomes of model checking an interval temporal logic formula against sets of histories (finite interval models), possibly taking into account domain-dependent measures/criteria, like, for instance, sensitivity, specificity, and accuracy. From a technical point of view, the main contribution of the paper is a (deterministic) polynomial time algorithm for interval temporal logic model checking over finite interval models. To the best of our knowledge, this is the first application of a (truly) interval temporal logic model checking in the area of temporal databases and data mining rather than in the formal verification setting.","Dataset evaluation; Interval temporal logics; Model checking; Temporal databases","Compliance control; Computer circuits; Data mining; Polynomial approximation; Temporal logic; Dataset evaluation; Evaluation problems; Finite intervals; Interval temporal logic; Polynomial-time algorithms; Temporal Data; Temporal Database; Model checking",2-s2.0-85030698046
"Canault B., Bourg S., Vayer P., Bonnet P.","Comprehensive Network Map of ADME-Tox Databases",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030863020&doi=10.1002%2fminf.201700029&partnerID=40&md5=b766ec20465266542886b3471a48e7ea","In the last decade, many statistical-based approaches have been developed to improve poor pharmacokinetics (PK) and to reduce toxicity of lead compounds, which are one of the main causes of high failure rate in drug development. Predictive QSAR models are not always very efficient due to the low number of available biological data and the differences in the experimental protocols. Fortunately, the number of available databases continues to grow every year. However, it remains a challenge to determine the source and the quality of the original data. The main goal is to identify the relevant databases required to generate the most robust predictive models. In this study, an interactive network of databases was proposed to easily find online data sources related to ADME-Tox parameters data. In this map, relevant information regarding scope of application, data availability and data redundancy can be obtained for each data source. To illustrate the usage of data mining from the network, a dataset on plasma protein binding is selected based on various sources such as DrugBank, PubChem and ChEMBL databases. A total of 2,606 unique molecules with experimental values of PPB were extracted and can constitute a consistent dataset for QSAR modeling. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","ADME-Tox; Database; Network; PPB","Article; data base; data mining; information processing; pharmacokinetic parameters; plasma protein binding; priority journal; quantitative structure activity relation",2-s2.0-85030863020
"Zhang Y., Chen H., Lu J., Zhang G.","Detecting and predicting the topic change of Knowledge-based Systems: A topic-based bibliometric analysis from 1991 to 2016",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024905068&doi=10.1016%2fj.knosys.2017.07.011&partnerID=40&md5=6e65953a136b6d6238a6cf07417fde69","The journal Knowledge-based Systems (KnoSys) has been published for over 25 years, during which time its main foci have been extended to a broad range of studies in computer science and artificial intelligence. Answering the questions: “What is the KnoSys community interested in?” and “How does such interest change over time?” are important to both the editorial board and audience of KnoSys. This paper conducts a topic-based bibliometric study to detect and predict the topic changes of KnoSys from 1991 to 2016. A Latent Dirichlet Allocation model is used to profile the hotspots of KnoSys and predict possible future trends from a probabilistic perspective. A model of scientific evolutionary pathways applies a learning-based process to detect the topic changes of KnoSys in sequential time slices. Six main research areas of KnoSys are identified, i.e., expert systems, machine learning, data mining, decision making, optimization, and fuzzy, and the results also indicate that the interest of KnoSys communities in the area of computational intelligence is raised, and the ability to construct practical systems through knowledge use and accurate prediction models is highly emphasized. Such empirical insights can be used as a guide for KnoSys submissions. © 2017","Bibliometrics; Knowledge-based Systems; Text mining; Topic analysis; Topic detection and tracking","Artificial intelligence; Education; Expert systems; Forecasting; Knowledge based systems; Statistics; Accurate prediction; Bibliometric analysis; Bibliometrics; Evolutionary pathway; Latent Dirichlet allocation; Text mining; Topic analysis; Topic detection and tracking; Data mining",2-s2.0-85024905068
"Jiang S., Chin K.-S., Wang L., Qu G., Tsui K.L.","Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017543687&doi=10.1016%2fj.eswa.2017.04.017&partnerID=40&md5=fc47dfd03d4f349eba6c345d0a0bd37f","A well-performed demand forecasting can provide outpatient department (OPD) managers with essential information for staff scheduling and rostering, considering the non-reservation policy of OPD in China. Based on the results reported by relevant studies, most approaches have focused on forecasting the overall amount of patient flow and ignored the demand for other key resources in OPD or similar department. Moreover, few studies have conducted feature selection before training a forecast model, which is a significant pre-processing operation of data mining and widely applied for knowledge discovery in expert and intelligent system. This study develops a novel hybrid methodology to forecast the patients’ demand for different key resources in OPD, by combining a new feature selection method and a deep learning approach. A modified version of genetic algorithm (MGA) is proposed for feature selection. The key operators of normal genetic algorithm are redesigned to utilize useful information provided by filter-based feature selection and feature combinations. A feedforward deep neural network is introduced as the forecast model, and the initial parameter set is generated from a stacked autoencoder-based pre-training process to overcome the optimization challenges in constructing deep architectures. In order to evaluate the performance of our methodology, it is applied to an OPD located at Northeast China. The results are compared with those obtained from combinations of other feature selection methods and demand forecasting models. Compared with GA and PCA, MGA improves the quality and efficiency of feature selection, with less selected features to get higher forecast accuracy. Pre-trained DNN optimally strengthens the advantage of MGA, compared with MLR, ARIMAX and SANN. The combination of MGA and pre-trained DNN possesses strongest predictive power among all involved combinations. Furthermore, the results of proposed methodology are crucial prerequisites for staff scheduling and resource allocation in OPD. Elite features obtained by MGA can provide practical insights on potential association between manifold feature combinations and demand variance. © 2017 Elsevier Ltd","Deep learning; Demand forecasting in hospital; Feature selection; Modified genetic algorithm; Stacked autoencoder","Data handling; Data mining; Deep learning; Deep neural networks; Filtration; Forecasting; Genetic algorithms; Hospitals; Human resource management; Intelligent systems; Learning systems; Optimization; Personnel training; Scheduling; Auto encoders; Demand forecasting; Feature combination; Feature selection methods; Hybrid methodologies; Modified genetic algorithms; Outpatient departments; Pre-processing operations; Feature extraction",2-s2.0-85017543687
"Xu Z., Xuan J., Liu Y., Choo K.-K.R., Mei L., Hu C.","Building spatial temporal relation graph of concepts pair using web repository",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978112004&doi=10.1007%2fs10796-016-9676-4&partnerID=40&md5=ea6d077e8903cc3d13beeae5237c7b81","Mining semantic relations between concepts underlies many fundamental tasks including natural language processing, web mining, information retrieval, and web search. In order to describe the semantic relation between concepts, in this paper, the problem of automatically generating spatial temporal relation graph (STRG) of semantic relation between concepts is studied. The spatial temporal relation graph of semantic relation between concepts includes relation words, relation sentences, relation factor, relation graph, faceted feature, temporal feature, and spatial feature. The proposed method can automatically generate the spatial temporal relation graph (STRG) of semantic relation between concepts, which is different from the manually generated annotation repository such as WordNet and Wikipedia. Moreover, the proposed method does not need any prior knowledge such as ontology or the hierarchical knowledge base such as WordNet. Empirical experiments on real dataset show that the proposed algorithm is effective and accurate. © 2016, Springer Science+Business Media New York.","Knowledge graph; Semantic relations; Temporal and spatial mining; Web repository","Data mining; Information retrieval; Knowledge based systems; Natural language processing systems; Ontology; Semantics; World Wide Web; Empirical experiments; Hierarchical knowledge; Knowledge graphs; NAtural language processing; Semantic relations; Spatial temporals; Temporal and spatial; Web repositories; Semantic Web",2-s2.0-84978112004
"Stadnik D.A.","Objectives of functional subsystems within the unified industrial system automated design of coal mines",2017,"Ugol'",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030631978&doi=10.18796%2f0041-5790-2017-10-52-56&partnerID=40&md5=6fb1184790be47aa70195b58f2b7f2a4","Analyzing the experience of the global coal mining industry's functioning it is explained functional subsystems, which recommended for inclusion in the unified industrial system automated design of coal mines. This is substantiated in order to improve the methodological base for mineral resource's development of the Russian Federation. This methodological approach will allow to develop innovative design solutions, that are adaptive to the specific conditions the mineral resource's development of coal enterprises.","3D-modeling; Automation design; Base of geological information; Coal deposit; Coal mine; Data mining; Forecasting; Mineral resource's development; Mining management; Natural resources management",,2-s2.0-85030631978
"Shah Z., Mahmood A.N., Tari Z., Zomaya A.Y.","A Technique for Efficient Query Estimation over Distributed Data Streams",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030032058&doi=10.1109%2fTPDS.2017.2693983&partnerID=40&md5=b19e5816e3efedd6434dfe5c9cef82f6","Distributed data stream mining in a sliding window has emerged recently, due to its applications in many domains including large Telecoms and Internet Service Providers, financial tickers, ATM and credit card operations in banks and transactions in retail chains. Many of these large-scale applications prohibit monitoring data centrally at a single location due to their massive volume of the data; therefore, data acquisition, processing, and mining tasks are often distributed to a number of processing nodes, which monitor their local streams and exchange only the summary of data either periodically or on demand. While this offer many advantages, distributed stream applications possess significant challenges including problems related to an online analysis of the recent data, communication efficiency and various estimation of various complex queries. There are few existing techniques which solve problems related to distributed sliding window data stream; however, those techniques are focused on solving only simple problems and require high space, query, and communication cost, which can be a bottleneck for many of these large scale applications. In this paper, we propose an efficient query estimation technique by constructing a small sketch of the data stream. The constructed sketch uses a deterministic sliding window model and can estimate various complex queries, for both centralized and distributed applications; including point queries (i.e., range queries and heavy hitter queries), quantiles, inner product, and self-join size queries, with deterministic guarantees on the precision. The proposed approach improves upon recent existing work for these problems, in terms of the memory and query cost in a centralized setting and in terms of communication cost and merge complexity in a distributed setting. It requires O(1/2log (ϵN)) memory (where 0<ϵ <1 is a user defined parameter), can provide estimates in O(1) time, and processes each incoming record in O(1) amortized time. Detailed experimental analysis, both in centralized and distributed settings demonstrates that in practice the proposed approach uses about six times less memory, and has about eight times less query time when compared to ECM sketches. In a distributed application, the proposed technique also significantly improves (around seven times) on the communication cost between distributed sites. © 1990-2012 IEEE.","distributed data streams; distributed heavy hitters; Distributed query estimation","Costs; Data acquisition; Data communication systems; Data handling; Problem solving; Communication efficiency; Deterministic guarantee; Distributed applications; Distributed data streams; Distributed query; Heavy-hitter; Large-scale applications; User-defined parameters; Query processing",2-s2.0-85030032058
"Srinivasa K.G., Shree Devi B.N.","GPU Based N-Gram String Matching Algorithm with Score Table Approach for String Searching in Many Documents",2017,"Journal of The Institution of Engineers (India): Series B",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031719705&doi=10.1007%2fs40031-017-0295-3&partnerID=40&md5=82e7aef07cf532b915bb67267fddf8c9","String searching in documents has become a tedious task with the evolution of Big Data. Generation of large data sets demand for a high performance search algorithm in areas such as text mining, information retrieval and many others. The popularity of GPU’s for general purpose computing has been increasing for various applications. Therefore it is of great interest to exploit the thread feature of a GPU to provide a high performance search algorithm. This paper proposes an optimized new approach to N-gram model for string search in a number of lengthy documents and its GPU implementation. The algorithm exploits GPGPUs for searching strings in many documents employing character level N-gram matching with parallel Score Table approach and search using CUDA API. The new approach of Score table used for frequency storage of N-grams in a document, makes the search independent of the document’s length and allows faster access to the frequency values, thus decreasing the search complexity. The extensive thread feature in a GPU has been exploited to enable parallel pre-processing of trigrams in a document for Score Table creation and parallel search in huge number of documents, thus speeding up the whole search process even for a large pattern size. Experiments were carried out for many documents of varied length and search strings from the standard Lorem Ipsum text on NVIDIA’s GeForce GT 540M GPU with 96 cores. Results prove that the parallel approach for Score Table creation and searching gives a good speed up than the same approach executed serially. © 2017, The Institution of Engineers (India).","CUDA; GPGPU; N-gram; NVIDIA; Score table; String matching; String searching; Trigram","Big data; Data mining; Digital storage; Graphics processing unit; Learning algorithms; Program processors; CUDA; GPGPU; N-grams; NVIDIA; Score table; String matching; String-searching; Tri grams; String searching algorithms",2-s2.0-85031719705
"Zhao W.X., Liu C., Wen J.-R., Li X.","Ranking and tagging bursty features in text streams with context language models",2017,"Frontiers of Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976402802&doi=10.1007%2fs11704-016-5144-z&partnerID=40&md5=df6061cabb95ea7ccedf0ec1d90b2d99","Detecting and using bursty patterns to analyze text streams has been one of the fundamental approaches in many temporal text mining applications. So far, most existing studies have focused on developing methods to detect bursty features based purely on term frequency changes. Few have taken the semantic contexts of bursty features into consideration, and as a result the detected bursty features may not always be interesting and can be hard to interpret. In this article, we propose to model the contexts of bursty features using a language modeling approach. We propose two methods to estimate the context language models based on sentence-level context and document-level context.We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features. We also use two example text mining applications to qualitatively demonstrate the usefulness of bursty feature ranking and tagging. © 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg.","bursty feature tagging; bursty features; bursty features ranking; context modeling","Computational linguistics; Computer hardware description languages; Data mining; Modeling languages; Natural language processing systems; Semantics; Text processing; bursty feature tagging; bursty features; Context modeling; Feature ranking; Features rankings; Semantic context; Temporal text minings; Topic diversity; Feature extraction",2-s2.0-84976402802
"Wang W., Zhu K., Wang H., Wu Y.-C.J.","The impact of sentiment orientations on successful crowdfunding campaigns through text analytics",2017,"IET Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029797432&doi=10.1049%2fiet-sen.2016.0295&partnerID=40&md5=d3c3461adcdb0e7a8e111268aa6c9e36","The sentiment implied in user generated content represents the authors' personality, attitude, education level and social status. In Crowdfunding, the sentimental factor of the text description may impact the backers' investment intention on the project. The authors study the textual description from the sentimental aspect on the pledge results by employing text mining. The study proves that positive sentiment in the blurb and detailed description promotes the successful campaigns while it should not contain any sentimental factor in title. The predictive analysis shows that the predictive accuracy can be improved 7% based on the baseline model after considering sentimental factors from 64.4% to 71.7%. © The Institution of Engineering and Technology 2017.",,"Computer software; Technology; Baseline models; Predictive accuracy; Social status; Text analytics; Text mining; Textual description; User-generated content; Data mining",2-s2.0-85029797432
"Rasheed W., Neoh Y.Y., Bin Hamid N.H., Reza F., Idris Z., Tang T.B.","Early visual analysis tool using magnetoencephalography for treatment and recovery of neuronal dysfunction",2017,"Computers in Biology and Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019644046&doi=10.1016%2fj.compbiomed.2017.05.005&partnerID=40&md5=b37a065bb02a9a8aa0e7c1e572a8be8e","Functional neuroimaging modalities play an important role in deciding the diagnosis and course of treatment of neuronal dysfunction and degeneration. This article presents an analytical tool with visualization by exploiting the strengths of the MEG (magnetoencephalographic) neuroimaging technique. The tool automates MEG data import (in tSSS format), channel information extraction, time/frequency decomposition, and circular graph visualization (connectogram) for simple result inspection. For advanced users, the tool also provides magnitude squared coherence (MSC) values allowing personalized threshold levels, and the computation of default model from MEG data of control population. Default model obtained from healthy population data serves as a useful benchmark to diagnose and monitor neuronal recovery during treatment. The proposed tool further provides optional labels with international 10-10 system nomenclature in order to facilitate comparison studies with EEG (electroencephalography) sensor space. Potential applications in epilepsy and traumatic brain injury studies are also discussed. © 2017 Elsevier Ltd","Coherence; Default mode connectivity; Functional connectivity","Brain; Brain mapping; Coherent light; Data mining; Data visualization; Electroencephalography; Electrophysiology; Functional neuroimaging; Magnetoencephalography; Neuroimaging; Neurons; Visualization; Default mode connectivity; Functional connectivity; Healthy population; Magnitude squared coherences; Neuroimaging techniques; Neuronal dysfunction; Personalized thresholds; Traumatic Brain Injuries; Population statistics; adult; alpha rhythm; Article; beta rhythm; brain region; brain tumor; clinical article; controlled study; contusion; delta rhythm; epilepsy; female; functional connectivity; functional neuroimaging; gamma rhythm; Glasgow coma scale; human; magnetoencephalography; male; middle aged; nerve cell plasticity; plasticity; priority journal; skull fracture; subdural hematoma; theta rhythm; traumatic brain injury",2-s2.0-85019644046
"Chen L.-C., Hsu C.-L., Lo N.-W., Yeh K.-H., Lin P.-H.","Fraud analysis and detection for real-time messaging communications on social networks",2017,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030229523&doi=10.1587%2ftransinf.2016INI0003&partnerID=40&md5=0e9653e13154484347e1e1e9480dad2b","With the successful development and rapid advancement of social networking technology, people tend to exchange and share information via online social networks, such as Facebook and LINE.Massive amounts of information are aggregated promptly and circulated quickly among people. However, with the enormous volume of human-interactions, various types of swindles via online social networks have been launched in recent years. Effectively detecting fraudulent activities on social networks has taken on increased importance, and is a topic of ongoing interest. In this paper, we develop a fraud analysis and detection system based on realtime messaging communications, which constitute one of the most common human-interacted services of online social networks. An integrated platform consisting of various text-mining techniques, such as natural language processing, matrix processing and content analysis via a latent semantic model, is proposed. In the system implementation, we first collect a series of fraud events, all of which happened in Taiwan, to construct analysis modules for detecting such fraud events. An Android-based application is then built for alert notification when dubious logs and fraud events happen. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","Facebook; Fraud analysis; Latent semantic analysis; Natural language processing; Social networks","Crime; Data mining; Natural language processing systems; Semantics; Social sciences computing; Websites; Facebook; Fraud analysis; Integrated platform; Latent Semantic Analysis; On-line social networks; Social networking technologies; System implementation; Text mining techniques; Social networking (online)",2-s2.0-85030229523
"Sun Y., Wang Z., Li H., Li Y.","A novel ensemble classification for data streams with class imbalance and concept drift",2017,"International Journal of Performability Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031916097&doi=10.23940%2fijpe.17.06.p15.945955&partnerID=40&md5=0aefd3d5d6855603a3ae88204e45bc4d","The processing of streaming data implies new requirements concerning restrictive processing time, limited amount of memory and one scan of incoming instances. One of the biggest challenges facing data stream learning is to deal with concept drift, i.e., the underlying distribution of the data may be evolving over time. Most of the approaches in the literature are under the hypothesis that the distribution of classes is balance. Unfortunately, the class imbalance issue is common in the real-world. And the imbalance issue further increases the difficulty of solving the concept drift problem. Motivated by this challenge, a novel ensemble classification for mining imbalanced streaming data is proposed to overcome both issues simultaneously. The algorithm utilizes the under-sampling and over-sampling techniques to balance the positive and negative instances. Moreover, dynamic weighting strategy was adopted to deal with concept drift. The experimental results on synthetic and real datasets demonstrate that our proposed method performs better than competitive algorithms, especially in situations where there exist concept drift and class imbalance. © 2017 Totem Publisher, Inc. All rights reserved.","Big data; Class imbalance; Concept drift; Data streams; Ensemble classification","Classification (of information); Data handling; Class imbalance; Competitive algorithms; Concept drifts; Data stream; Ensemble classification; Negative instances; Underlying distribution; Weighting strategies; Big data",2-s2.0-85031916097
"Zhang Z., Jia L., Zhang M., Li B., Zhang L., Li F.","Discriminative clustering on manifold for adaptive transductive classification",2017,"Neural Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027419074&doi=10.1016%2fj.neunet.2017.07.013&partnerID=40&md5=cb168a65a93d6da64fa06ff93bb14144","In this paper, we mainly propose a novel adaptive transductive label propagation approach by joint discriminative clustering on manifolds for representing and classifying high-dimensional data. Our framework seamlessly combines the unsupervised manifold learning, discriminative clustering and adaptive classification into a unified model. Also, our method incorporates the adaptive graph weight construction with label propagation. Specifically, our method is capable of propagating label information using adaptive weights over low-dimensional manifold features, which is different from most existing studies that usually predict the labels and construct the weights in the original Euclidean space. For transductive classification by our formulation, we first perform the joint discriminative K-means clustering and manifold learning to capture the low-dimensional nonlinear manifolds. Then, we construct the adaptive weights over the learnt manifold features, where the adaptive weights are calculated through performing the joint minimization of the reconstruction errors over features and soft labels so that the graph weights can be joint-optimal for data representation and classification. Using the adaptive weights, we can easily estimate the unknown labels of samples. After that, our method returns the updated weights for further updating the manifold features. Extensive simulations on image classification and segmentation show that our proposed algorithm can deliver the state-of-the-art performance on several public datasets. © 2017 Elsevier Ltd","Adaptive transductive classification; Discriminative clustering; Label propagation; Manifold learning","Clustering algorithms; Data mining; Image segmentation; Adaptive classification; Discriminative clustering; Extensive simulations; High dimensional data; Label propagation; Low-dimensional manifolds; Manifold learning; State-of-the-art performance; Classification (of information); analytical error; Article; classification algorithm; cluster analysis; discriminant analysis; image reconstruction; image segmentation; nonlinear system; priority journal; simulation; unsupervised machine learning",2-s2.0-85027419074
"Golay J., Leuenberger M., Kanevski M.","Feature selection for regression problems based on the Morisita estimator of intrinsic dimension",2017,"Pattern Recognition",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020014509&doi=10.1016%2fj.patcog.2017.05.008&partnerID=40&md5=3e991ca954a78af3b34e8280f2f30436","Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results, and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. © 2017 Elsevier Ltd","Data mining; Feature selection; Intrinsic dimension; Measure of relevance; Morisita index","Data acquisition; Data mining; Digital storage; Filtration; Information management; Learning systems; Extreme learning machine; Feature selection methods; Graphical representations; Intrinsic dimensions; Measure of relevance; Morisita index; Numerical experiments; Redundant features; Feature extraction",2-s2.0-85020014509
"Wei C.-X., Wu Z., Wali F., Wei W.-B., Bao Y., Luo R.-H., Wang L., Liu G., Tian Y.-C.","Single-shot grating-based x-ray differential phase contrast imaging with a modified analyzer grating",2017,"Chinese Physics B",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031016339&doi=10.1088%2f1674-1056%2f26%2f10%2f108701&partnerID=40&md5=9086820952f268bacf897660ca5c6e5f","X-ray grating interferometer has attracted widely attention in the past years due to its capability in achieving x-ray phase contrast imaging with low brilliance source. However, the widely used phase stepping information extraction method reduces system stability and prolongs data acquisition time by several times compared with conventional x-ray absorptionbased imaging. The mechanical stepping can be avoided by using a staggered grating, but at the cost of low vertical spatial resolution. In this paper, employing a modified staggered grating and the angular signal radiography, we proposed a single-shot grating-based x-ray differential phase contrast imaging with decent vertical spatial resolution. The theoretical framework was deduced and proved by numerical experiments. Absorption, phase, and scattering computed tomography can be performed without phase stepping. Therefore, we believe this fast and highly stable imaging method with decent resolution would be widely applied in x-ray grating-based phase contrast imaging. © 2017 Chinese Physical Society and IOP Publishing Ltd.","computed tomography; decent resolution; single-shot; staggered grating; x-ray phase contrast imaging","Computerized tomography; Data acquisition; Data mining; Image resolution; System stability; Tomography; Information extraction methods; Phase-contrast imaging; Single shots; staggered grating; Theoretical framework; Vertical spatial resolution; X-ray grating interferometers; X-ray phase-contrast imaging; X ray radiography",2-s2.0-85031016339
"Devezas J., Nunes S.","Information extraction for event ranking",2017,"OpenAccess Series in Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032640530&doi=10.4230%2fOASIcs.SLATE.2017.18&partnerID=40&md5=921964f886ec30749363e45fb0e444e0","Search engines are evolving towards richer and stronger semantic approaches, focusing on entity-oriented tasks where knowledge bases have become fundamental. In order to support semantic search, search engines are increasingly reliant on robust information extraction systems. In fact, most modern search engines are already highly dependent on a well-curated knowledge base. Nevertheless, they still lack the ability to e ectively and automatically take advantage of multiple heterogeneous data sources. Central tasks include harnessing the information locked within textual content by linking mentioned entities to a knowledge base, or the integration of multiple knowledge bases to answer natural language questions. Combining text and knowledge bases is frequently used to improve search results, but it can also be used for the query-independent ranking of entities like events. In this work, we present a complete information extraction pipeline for the Portuguese language, covering all stages from data acquisition to knowledge base population. We also describe a practical application of the automatically extracted information, to support the ranking of upcoming events displayed in the landing page of an institutional search engine, where space is limited to only three relevant events. We manually annotate a dataset of news, covering event announcements from multiple faculties and organic units of the institution. We then use it to train and evaluate the named entity recognition module of the pipeline. We rank events by taking advantage of identified entities, as well as partOf relations, in order to compute an entity popularity score, as well as an entity click score based on implicit feedback from clicks from the institutional search engine. We then combine these two scores with the number of days to the event, obtaining a final ranking for the three most relevant upcoming events. © José Devezas and Sérgio Nunes","Academic Events; Entity-Based Ranking; Knowledge Base Population; Named Entity Recognition; Relation Extraction","Artificial intelligence; Data acquisition; Data mining; Information analysis; Information retrieval; Information retrieval systems; Knowledge based systems; Natural language processing systems; Pipelines; Population statistics; Semantic Web; Semantics; Slate; Academic Events; Entity-Based Ranking; Knowledge base; Named entity recognition; Relation extraction; Search engines",2-s2.0-85032640530
"Song X., Yang X., Shao C., Yang J.","Parity symmetrical collaborative representation-based classification for face recognition",2017,"International Journal of Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028007710&doi=10.1007%2fs13042-016-0520-4&partnerID=40&md5=82defddf52c2be98abb4fbb298f36817","Although the subspace-based feature extraction algorithms provided a feasible strategy to deal with the classification of high-dimensional data, most of the existing algorithms are locality-oriented and suffer from many difficulties such as uncertain information associated with dataset and small sample size problem. In this paper, we propose a novel collaborative representation-based classification method using parity symmetry strategy for face recognition. More specifically, we firstly synthesize a set of parity symmetrical images by means of odd–even decomposition theorem, aiming to augment the training set. Secondly, each query sample is represented as a linear combination of the training samples from the extended training set, we then exploit the optimal representation of each reconstructed image with relevant contribution from each class. The final goal of the proposed method is to generate the best parity symmetrical representation of the query sample to perform robust face classification. Experimental results conducted on ORL, FERET, AR, PIE and LFW face databases demonstrate the effectiveness of the proposed method. © 2016, Springer-Verlag Berlin Heidelberg.","CRC; Image recognition; Major relevant contribution; Parity symmetry","Clustering algorithms; Data mining; Domain decomposition methods; Face recognition; Image processing; Image recognition; Classification methods; Collaborative representations; Decomposition theorems; Feature extraction algorithms; High dimensional data; Major relevant contribution; Small sample size problems; Uncertain informations; Classification (of information)",2-s2.0-85028007710
"Uçar E., Uzun E., Tüfekci P.","A novel algorithm for extracting the user reviews from web pages",2017,"Journal of Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029475331&doi=10.1177%2f0165551516666446&partnerID=40&md5=090c234a6b42df58729e55a6a4289452","Extracting the user reviews in websites such as forums, blogs, newspapers, commerce, trips, etc. is crucial for text processing applications (e.g. sentiment analysis, trend detection/monitoring and recommendation systems) which are needed to deal with structured data. Traditional algorithms have three processes consisting of Document Object Model (DOM) tree creation, extraction of features obtained from this tree and machine learning. However, these algorithms increase time complexity of extraction process. This study proposes a novel algorithm that involves two complementary stages. The first stage determines which HTML tags correspond to review layout for a web domain by using the DOM tree as well as its features and decision tree learning. The second stage extracts review layout for web pages in a web domain using the found tags obtained from the first stage. This stage is more time-efficient, being approximately 21 times faster compared to the first stage. Moreover, it achieves a relatively high accuracy of 96.67% in our experiments of review block extraction. © Chartered Institute of Library and Information Professionals.","Efficient extraction; web data extraction; web user reviews","Computational complexity; Decision trees; Extraction; Learning systems; Text processing; Websites; XML; Block extraction; Decision tree learning; Document object model; Extraction process; Sentiment analysis; Time complexity; Web data extraction; Web users; Data mining",2-s2.0-85029475331
"Zhang R., Simon G., Yu F.","Advancing Alzheimer's research: A review of big data promises",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027882057&doi=10.1016%2fj.ijmedinf.2017.07.002&partnerID=40&md5=43937b616b1efdb87be19dd24994f9f7","Objective To review the current state of science using big data to advance Alzheimer's disease (AD) research and practice. In particular, we analyzed the types of research foci addressed, corresponding methods employed and study findings reported using big data in AD. Method Systematic review was conducted for articles published in PubMed from January 1, 2010 through December 31, 2015. Keywords with AD and big data analytics were used for literature retrieval. Articles were reviewed and included if they met the eligibility criteria. Results Thirty-eight articles were included in this review. They can be categorized into seven research foci: diagnosing AD or mild cognitive impairment (MCI) (n = 10), predicting MCI to AD conversion (n = 13), stratifying risks for AD (n = 5), mining the literature for knowledge discovery (n = 4), predicting AD progression (n = 2), describing clinical care for persons with AD (n = 3), and understanding the relationship between cognition and AD (n = 3). The most commonly used datasets are AD Neuroimaging Initiative (ADNI) (n = 16), electronic health records (EHR) (n = 11), MEDLINE (n = 3), and other research datasets (n = 8). Logistic regression (n = 9) and support vector machine (n = 8) are the most used methods for data analysis. Conclusion Big data are increasingly used to address AD-related research questions. While existing research datasets are frequently used, other datasets such as EHR data provide a unique, yet under-utilized opportunity for advancing AD research. © 2017 Elsevier B.V.","Alzheimer's disease; Alzheimer's disease neuroimaging initiative; Electronic health records; Healthcare big data; Healthcare data analytics","Diagnosis; Health care; Neurodegenerative diseases; Neuroimaging; Records management; Alzheimer's disease; Data analytics; Electronic health record; Eligibility criterion; Literature retrieval; Logistic regressions; Mild cognitive impairments (MCI); Research questions; Big data",2-s2.0-85027882057
"Pinto A., Gonçalo Oliveira H., Figueira Á., Alves A.O.","Predicting the Relevance of Social Media Posts Based on Linguistic Features and Journalistic Criteria",2017,"New Generation Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018701930&doi=10.1007%2fs00354-017-0015-1&partnerID=40&md5=db08b14b6632302e150da60c51cee9b2","An overwhelming quantity of messages is posted in social networks every minute. To make the utilization of these platforms more productive, it is imperative to filter out information that is irrelevant to the general audience, such as private messages, personal opinions or well-known facts. This work is focused on the automatic classification of public social text according to its potential relevance, from a journalistic point of view, hopefully improving the overall experience of using a social network. Our experiments were based on a set of posts with several criteria, including the journalistic relevance, assessed by human judges. To predict the latter, we rely exclusively on linguistic features, extracted by Natural Language Processing tools, regardless the author of the message and its profile information. In our first approach, different classifiers and feature engineering methods were used to predict relevance directly from the selected features. In a second approach, relevance was predicted indirectly, based on an ensemble of classifiers for other key criteria when defining relevance—controversy, interestingness, meaningfulness, novelty, reliability and scope—also in the dataset. The first approach achieved a F1-score of 0.76 and an Area under the ROC curve (AUC) of 0.63. But the best results were achieved by the second approach, with the best learned model achieving a F1-score of 0.84 with an AUC of 0.78. This confirmed that journalistic relevance can indeed be predicted by the combination of the selected criteria, and that linguistic features can be exploited to classify the latter. © 2017, Ohmsha, Ltd. and Springer Japan.","Automatic text classification; Information extraction; Natural language processing; Relevance assessment; Social mining","Data mining; Filtration; Forecasting; Information retrieval; Linguistics; Natural language processing systems; Social networking (online); Text processing; Area under the ROC curve; Automatic classification; Automatic text classification; Ensemble of classifiers; Feature engineerings; NAtural language processing; Natural Language Processing Tools; Relevance assessments; Classification (of information)",2-s2.0-85018701930
"Sulieman L., Gilmore D., French C., Cronin R.M., Jackson G.P., Russell M., Fabbri D.","Classifying patient portal messages using Convolutional Neural Networks",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028881398&doi=10.1016%2fj.jbi.2017.08.014&partnerID=40&md5=cfa32285a3ee8a9473b2cf9a61be8c2f","Objective Patients communicate with healthcare providers via secure messaging in patient portals. As patient portal adoption increases, growing messaging volumes may overwhelm providers. Prior research has demonstrated promise in automating classification of patient portal messages into communication types to support message triage or answering. This paper examines if using semantic features and word context improves portal message classification. Materials and methods Portal messages were classified into the following categories: informational, medical, social, and logistical. We constructed features from portal messages including bag of words, bag of phrases, graph representations, and word embeddings. We trained one-versus-all random forest and logistic regression classifiers, and convolutional neural network (CNN) with a softmax output. We evaluated each classifier's performance using Area Under the Curve (AUC). Results Representing the messages using bag of words, the random forest detected informational, medical, social, and logistical communications in patient portal messages with AUCs: 0.803, 0.884, 0.828, and 0.928, respectively. Graph representations of messages outperformed simpler features with AUCs: 0.837, 0.914, 0.846, 0.884 for informational, medical, social, and logistical communication, respectively. Representing words with Word2Vec embeddings, and mapping features using a CNN had the best performance with AUCs: 0.908 for informational, 0.917 for medical, 0.935 for social, and 0.943 for logistical categories. Discussion and conclusion Word2Vec and graph representations improved the accuracy of classifying portal messages compared to features that lacked semantic information such as bag of words, and bag of phrases. Furthermore, using Word2Vec along with a CNN model, which provide a higher order representation, improved the classification of portal messages. © 2017 Elsevier Inc.","Convolutional Neural Network; Patient portals; Text mining; Word embedding; Word2Vec","Classifiers; Convolution; Data mining; Decision trees; Logistics; Neural networks; Semantics; Convolutional neural network; Patient portal; Text mining; Word embedding; Word2Vec; Classification (of information); area under the curve; Article; artificial neural network; classification algorithm; classifier; controlled study; convolutional neural network; deep neural network; electronic health record; gold standard; human; measurement accuracy; natural language processing; priority journal; sensitivity and specificity; text messaging",2-s2.0-85028881398
"Cho S.G., Kim S.B.","Feature network-driven quadrant mapping for summarizing customer reviews",2017,"Journal of Systems Science and Systems Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009253080&doi=10.1007%2fs11518-017-5329-5&partnerID=40&md5=cf438cc37609d4d5c2c123b9bebb99c5","With the rapid growth of e-commerce, customers increasingly write online reviews of the product they purchase. These customer reviews are one of the most valuable sources of information affecting selection of products or services. Summarizing these customer reviews is becoming an interesting area of research, inspiring researchers to develop a more condensed, concise summarization for users. However, most of the current efforts at summarization are based on general product features without feature’s relationship. As a result, these summaries either ignore feedback from customers or do a poor job of reflecting the opinions expressed in customer reviews. To remedy this summarization shortcoming, we propose a feature network-driven quadrant mapping that captures and incorporates opinions from customer reviews. Our focus is on construction of a feature network, which is based on co-occurrence and sematic similarities, and a quadrant display showing the opinions polarity of feature groups. Moreover, the proposed approach involves clustering similar product features, and thus, it is different from standard text summarization based on abstraction and extraction. The summarized results can help customers better understand the overall opinions about a product. © 2017, Systems Engineering Society of China and Springer-Verlag Berlin Heidelberg.","Customer review; feature network; text mining; text summarization; visualization","Data mining; Flow visualization; Mapping; Natural language processing systems; Text processing; Co-occurrence; Customer review; Feature groups; Online reviews; Product feature; Sources of informations; Text mining; Text summarization; Sales",2-s2.0-85009253080
"Zavvos V., Buxton A.T., Evans C., Lambie M., Davies S.J., Topley N., Wilkie M., Summers A., Brenchley P., Goumenos D.S., Johnson T.S.","A prospective, proteomics study identified potential biomarkers of encapsulating peritoneal sclerosis in peritoneal effluent",2017,"Kidney International",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021421916&doi=10.1016%2fj.kint.2017.03.030&partnerID=40&md5=56fa4b825ade5b4f9ced3ec94288d9e1","Encapsulating peritoneal sclerosis (EPS) is a potentially devastating complication of peritoneal dialysis (PD). Diagnosis is often delayed due to the lack of effective and accurate diagnostic tools. We therefore examined peritoneal effluent for potential biomarkers that could predict or confirm the diagnosis of EPS and would be valuable in stratifying at-risk patients and driving appropriate interventions. Using prospectively collected samples from the Global Fluid Study and a cohort of Greek PD patients, we utilized 2D SDSPAGE/ MS and iTRAQ to identify changes in the peritoneal effluent proteome from patients diagnosed with EPS and controls matched for treatment exposure. We employed a combinatorial peptide ligand library to compress the dynamic range of protein concentrations to aid identification of low-abundance proteins. In patients with stable membrane function, fibrinogen γ-chain and heparan sulphate proteoglycan core protein progressively increased over time on PD. In patients who developed EPS, collagen-α1(I), γ-actin and Complement factors B and I were elevated up to five years prior to diagnosis. Orosomucoid-1 and a2-HS-glycoprotein chain-B were elevated about one year before diagnosis, while apolipoprotein A-IV and α1-antitrypsin were decreased compared to controls. Dynamic range compression resulted in an increased number of proteins detected with improved resolution of protein spots, compared to the full fluid proteome. Intelectin-1, dermatopontin, gelsolin, and retinol binding protein-4 were elevated in proteome-mined samples from patients with EPS compared to patients that had just commenced peritoneal dialysis. Thus, prospective analysis of peritoneal effluent uncovered proteins indicative of inflammatory and pro-fibrotic injury worthy of further evaluation as diagnostic/prognostic markers. © 2017 International Society of Nephrology","biomarkers; encapsulating peritoneal sclerosis; peritoneal dialysis; proteomics","a2 heparan sulphate glycoprotein chain B; alpha 1 acid glycoprotein 1; alpha 1 acid glycoprotein 2; alpha 1 antitrypsin; alpha 1B glycoprotein; alpha 2 macroglobulin; alternative complement pathway C3 C5 convertase; antithrombin; apolipoprotein A1; apolipoprotein A4; apolipoprotein C3; beta 2 microglobulin; collagen alpha1; complement component C3c; complement component C4a; complement factor D; complement factor H; complement factor I; cystatin C; dermatopontin; fibrinogen gamma chain; fibrinopeptide A; gamma actin; gelsolin; heparan sulphate proteoglycan core protein; intelectin 1; orosomucoid 1; proteome; retinol binding protein 4; unclassified drug; unindexed drug; adult; aged; Article; cohort analysis; controlled study; data mining; female; human; isobaric tagging for relative and absolute quantification; major clinical study; male; peritoneal dialysis; peritoneal fibrosis; priority journal; prospective study; protein analysis; protein mining; proteomics; two dimensional polyacrylamide gel electrophoresis mass spectrometry",2-s2.0-85021421916
"Maia M.I., Leal J.P.","An emotional word analyzer for Portuguese",2017,"OpenAccess Series in Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644980&doi=10.4230%2fOASIcs.SLATE.2017.17&partnerID=40&md5=617e97a50eb5181cd3fee1f4e50e6725","The analysis of sentiments, emotions and opinions in texts is increasingly important in the current digital world. The existing lexicons with emotional annotations for the Portuguese language are oriented to polarities, classifying words as positive, negative or neutral. To identify the emotional load intended by the author it is necessary also to categorize the emotions expressed by individual words. EmoSpell is an extension of a morphological analyzer with semantic annotations of the emotional value of words. It uses Jspell as the morphological analyzer and a new dictionary with emotional annotations. This dictionary incorporates the lexical base EMOTAIX.PT, which classifies words based on three di erent levels of emotions – global, specific and intermediate. This paper describes the generation of the EmoSpell dictionary using three sources, the Jspell Portuguese dictionary and the lexical bases EMOTAIX.PT and SentiLex-PT. Also, this paper details the web application and web service that exploit this dictionary. It presents also a validation of the proposed approach using a corpus of student texts with di erent emotional loads. The validation compares the analyses provided by EmoSpell with the mentioned emotional lexical bases on the ability to recognize emotional words and extract the dominant emotion from a text. © Maria Inês Maia and José Paulo Leal","Emotion API; Opinion Mining; Sentiment Analysis","Character recognition; Data mining; Semantics; Slate; Emotion API; Emotional words; Morphological analyzer; Opinion mining; Portuguese languages; Semantic annotations; Sentiment analysis; WEB application; Web services",2-s2.0-85032644980
"Choi Y., Lee H.","Data properties and the performance of sentiment classification for electronic commerce applications",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014735254&doi=10.1007%2fs10796-017-9741-7&partnerID=40&md5=3692113a99c8322af3f1aad03670ce84","Sentiment classification has played an important role in various research area including e-commerce applications and a number of advanced Computational Intelligence techniques including machine learning and computational linguistics have been proposed in the literature for improved sentiment classification results. While such studies focus on improving performance with new techniques or extending existing algorithms based on previously used dataset, few studies provide practitioners with insight on what techniques are better for their datasets that have different properties. This paper applies four different sentiment classification techniques from machine learning (Naïve Bayes, SVM and Decision Tree) and sentiment orientation approaches to datasets obtained from various sources (IMDB, Twitter, Hotel review, and Amazon review datasets) to learn how different data properties including dataset size, length of target documents, and subjectivity of data affect the performance of those techniques. The results of computational experiments confirm the sensitivity of the techniques on data properties including training data size, the document length and subjectivity of training /test data in the improvement of performances of techniques. The theoretical and practical implications of the findings are discussed. © 2017, The Author(s).","Comparative analysis; Data properties; Machine learning approach; Opinion mining; Sentiment classification; Sentiment orientation approach","Artificial intelligence; Commerce; Decision trees; Electronic commerce; Information retrieval systems; Learning systems; Trees (mathematics); Comparative analysis; Data properties; Machine learning approaches; Opinion mining; Sentiment classification; Sentiment orientation approach; Classification (of information)",2-s2.0-85014735254
"Guo Y., Wang M., Li X.","An interactive personalized recommendation system using the hybrid algorithm model",2017,"Symmetry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031418749&doi=10.3390%2fsym9100216&partnerID=40&md5=21c59576514c5cb4cb7c2b616ba92340","With the rapid development of e-commerce, the contradiction between the disorder of business information and customer demand is increasingly prominent. This study aims to make e-commerce shopping more convenient, and avoid information overload, by an interactive personalized recommendation system using the hybrid algorithm model. The proposed model first uses various recommendation algorithms to get a list of original recommendation results. Combined with the customer's feedback in an interactive manner, it then establishes the weights of corresponding recommendation algorithms. Finally, the synthetic formula of evidence theory is used to fuse the original results to obtain the final recommendation products. The recommendation performance of the proposed method is compared with that of traditional methods. The results of the experimental study through a Taobao online dress shop clearly show that the proposed method increases the efficiency of data mining in the consumer coverage, the consumer discovery accuracy and the recommendation recall. The hybrid recommendation algorithm complements the advantages of the existing recommendation algorithms in data mining. The interactive assigned-weight method meets consumer demand better and solves the problem of information overload. Meanwhile, our study offers important implications for e-commerce platform providers regarding the design of product recommendation systems. © 2017 by the authors.","Data mining; e-commerce; Hybrid algorithm; Interactive assign; Recommendation system",,2-s2.0-85031418749
"Li X., Jiang Z., Song B., Liu L.","Long-term knowledge evolution modeling for empirical engineering knowledge",2017,"Advanced Engineering Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027507827&doi=10.1016%2fj.aei.2017.08.001&partnerID=40&md5=d57565121b576b5ce31d5a300ec4bf17","In this era of knowledge economy, appropriate management of the rapidly evolving knowledge is a real and urgent issue for factories and enterprises, in order to maintain the competitive edges. However, facing the onerous analysis required for understanding the long-term knowledge evolution, especially the evolving of empirical knowledge in the engineering field, effective and comprehensive modeling methods for knowledge evolution are absent. In this paper, a novel knowledge evolution modeling method is proposed for portraying the long-term evolution of empirical engineering knowledge (EEK) and assisting engineers in comprehending the evolving history. Three phases, EEK elicitation and formalization, EEK networks foundation, and family-tree evolution model construction, are included in the modeling method. This method is developed using natural language processing, semantic similarity calculation, fuzzy neural network prediction, clustering algorithm, and latent topic extraction techniques. To evaluate the performance of the proposed modeling method, an evolution model of empirical knowledge in computer-aided design (CAD) is constructed and then verified. Experimental results show that the proposed method outperforms the former approaches in feasibility and effectiveness, and hence opens up a better way of further understanding the long-term evolution course of EEK. © 2017","Data visualization; Empirical engineering knowledge (EEK); Evolution model; Knowledge evolution; Knowledge representation","Clustering algorithms; Data mining; Data visualization; Fuzzy neural networks; Knowledge representation; Long Term Evolution (LTE); Natural language processing systems; Semantics; Competitive edges; Comprehensive model; Empirical knowledge; Engineering fields; Engineering knowledge; Evolution modeling; Knowledge evolution; Semantic similarity; Computer aided design",2-s2.0-85027507827
"Dahbi A., Balouki Y., Gadi T.","A new method for selecting interesting association rules using genetic algorithm with multiple criteria",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031754895&partnerID=40&md5=94488d9b176e28c6076d3874a0f9935c","Association rules mining is an important topic in the domain of data mining and knowledge discovering, aiming to discover the interesting relation between variables in large datasets. One of the main problems related to the discovery of these associations (that a decision maker faces) is the huge number of association rules extracted. Hence in order to bypass this problem many interestingness measures have been proposed to evaluate the association rules. However, the abundance of these measures caused a new issue, which is the selection of measures that is best suited to the users and the heterogeneity of the evaluation results. To bypass this problem we propose an approach based on genetic algorithm and multi-criteria which permits to discover the interesting association rules without favoring or excluding any measures. The experiments performed on benchmark datasets show a wonderful performance of the proposed approach. © 2006-2017 Asian Research Publishing Network (ARPN).","Association rules; Data mining; Genetic algorithm; Interestingness measures; Multi-criteria decision analysis",,2-s2.0-85031754895
"Miteva M.A., Villoutreix B.O.","Computational Biology and Chemistry in MTi: Emphasis on the Prediction of Some ADMET Properties",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013395137&doi=10.1002%2fminf.201700008&partnerID=40&md5=231080759a6e6cca5820c04eb527c3d7","Our research and teaching group called MTi (Molécules Thérapeutiques in silico) has developed numerous applications available online, thanks to the RPBS platform (Ressource Parisienne en Bioinformatique Structurale), in the field of chemoinformatics, structural bioinformatics and drug design. Since its opening in 2009, over 200 articles/reviews have been reported and involve virtual screening studies, prediction of druggability, analysis of protein-protein interaction inhibitors, development of databases, data mining and knowledge discovery, as well as combined in silico-in vitro work to search for new hits and chemical probes acting on original targets in several therapeutic areas. An international training program has also been developed pertaining to the field of in silico drug design. In this review, we present some tools developed in our laboratory with a special emphasis on the prediction of some ADMET properties, compound collection preparation and 3D-ADMET computations. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","3D-ADMET; ADMET; Molecular Modelling; QSAR; Structural Bioinformatics; Virtual Screening","biology; chemistry; data mining; drug absorption; drug design; drug distribution; drug excretion; drug metabolism; drug screening; molecular dynamics; molecular model; prediction; priority journal; protein protein interaction; Review; structural bioinformatics",2-s2.0-85013395137
"Krittanawong C., Aydar M., Kitai T.","Pokémon Go: Digital health interventions to reduce cardiovascular risk",2017,"Cardiology in the Young",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017521745&doi=10.1017%2fS1047951117000749&partnerID=40&md5=d005463651672f2ade3f6fcc4e85ff9d","Physical activity is associated with a lower risk of coronary heart disease/cardiovascular disease mortality, and current guidelines recommend physical activity for primary prevention in healthy individuals and secondary prevention in patients with coronary heart disease/cardiovascular disease. Over the last decade, playing classic video games has become one of the most popular leisure activities in the world, but is associated with a sedentary lifestyle. In the new era of rapidly evolving augmented reality technology, Pokémon Go, a well-known augmented reality game, may promote physical activity and prevent cardiovascular disease risks - that is, diabetes, obesity, and hypertension. Pokémon Go makes players willing to be physically active for regular and long periods of time. We report on an assessment of regular walking and playing Pokémon Go by performing data mining in Twitter. © Cambridge University Press 2017.","data mining; exercise; physical activity; Pokémon Go; social media","Article; cardiovascular risk; cross-sectional study; data mining; human; mobile application; practice guideline; recreation; risk reduction; social media; walking",2-s2.0-85017521745
"Sun G., Cui T., Guo W., Chen S., Shen J.","A framework of MLaaS for facilitating adaptive micro learning through open education resources in mobile environment",2017,"International Journal of Web Services Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027988121&doi=10.4018%2fIJWSR.2017100103&partnerID=40&md5=cc30b6650b03dad18e98e9cf8f31a7ee","Micro learning becomes popular in online open learning and it is effective and helpful for learning in mobile environment. However, the delivery of open education resources (OERs) is scarcely supported by the current online systems. In this research, the authors introduce an approach to bridge the gap by providing adaptive micro open education resources for individual learners to carry out learning activities in a short time span. They propose a framework for micro learning resource customization and a personalized learner model, which are supported by education data mining (EDM) and learning analysis (LA). A service-oriented architecture for Micro Learning as a Service (MLaaS) is designed to integrate all necessary procedures together as a complete Service for delivering micro OERs, providing a platform for resource sharing and exchanging in peer-to-peer learning environment. Working principle of a key step, namely the computational decision-making of micro OER adaptation, is also introduced. © 2017, IGI Global.","Micro Learning; Open Education Learning Resource; Open Learning; Service Oriented Architecture; Software as a Service","Computer aided instruction; Data mining; Decision making; Distributed computer systems; Information services; Learning systems; Online systems; Service oriented architecture (SOA); Software as a service (SaaS); Learner model; Learning Activity; Learning resource; Micro-learning; Mobile environments; Open Education Resources; Open learning; Resource sharing; Education",2-s2.0-85027988121
"Gu Y., Cheng L.","Classification of class overlapping datasets by Kernel-MTS method",2017,"International Journal of Innovative Computing, Information and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030253089&partnerID=40&md5=99dcd43518b3db0910820b013eee59fc","Class overlapping is one of the bottlenecks in data mining and pattern recognition, and affects the classification accuracy and generalization ability directly. In Mahalanobis-Taguchi System (MTS), the normal samples are used to construct reference space, while the abnormal samples are used to verify the validity of the reference space. If there is a class overlapping between the normal samples and the abnormal samples, the result of classification will be affected. In this paper, kernel function and Mahalanobis distance are combined to form the kernel Mahalanobis distance as an improved measurement scale of the MTS. Experimental results show that Kernel-MTS is suitable for class overlapping classification, and it provides better classification accuracy than the conventional methods. © 2017 ICIC International.","Class overlapping; Classification; Kernel function; Mahalanobis-taguchi system (MTS)","Data mining; Pattern recognition; Class overlapping; Classification accuracy; Conventional methods; Generalization ability; Kernel function; Mahalanobis distances; Mahalanobis-taguchi systems; Measurement scale; Classification (of information)",2-s2.0-85030253089
"Biglari M., Soleimani A., Hassanpour H.","A Cascaded Part-Based System for Fine-Grained Vehicle Classification",2017,"IEEE Transactions on Intelligent Transportation Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030787612&doi=10.1109%2fTITS.2017.2749961&partnerID=40&md5=57b323c6254d0f5cc1fa768887ba8ca4","Vehicle make and model recognition (VMMR) has become an important part of intelligent transportation systems. VMMR can be useful when license plate recognition is not feasible or fake number plates are used. VMMR is a hard, fine-grained classification problem, due to the large number of classes, substantial inner-class, and small inter-class distance. A novel cascaded part-based system has been proposed in this paper for VMMR. This system uses latent support vector machine formulation for automatically finding the discriminative parts of each vehicle category. At the same time, it learns a part-based model for each category. Our approach employs a new training procedure, a novel greedy parts localization, and a practical multi-class data mining algorithm. In order to speed up the system processing time, a novel cascading scheme has been proposed. This cascading scheme applies classifiers to the input image in a sequential manner, based on the two proposed criteria: confidence and frequency. The cascaded system can run up to 80&#x0025; faster with analogous accuracy in comparison with the non-cascaded system. The extensive experiments on our data set and the CompCars data set indicate the outstanding performance of our approach. The proposed approach achieves an average accuracy of 97.01&#x0025; on our challenging data set and an average accuracy of 95.55&#x0025; on CompCars data set. IEEE","Automobiles; cascading scheme; Data mining; Deformable models; deformable part models; Feature extraction; Fine-grained classification; latent support vector machine (SVM).; Support vector machines; Training; vehicle make and model recognition","Automobiles; Deformation; Feature extraction; Intelligent systems; License plates (automobile); Optical character recognition; Personnel training; Support vector machines; Vehicles; cascading scheme; Deformable models; Deformable part models; Fine grained; Latent support vector machines; Model recognition; Data mining",2-s2.0-85030787612
"Cardoso R., Cury A., Barbosa F.","A robust methodology for modal parameters estimation applied to SHM",2017,"Mechanical Systems and Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018763654&doi=10.1016%2fj.ymssp.2017.03.021&partnerID=40&md5=8bf954f9d296ccde6542d384b31f1adb","The subject of structural health monitoring is drawing more and more attention over the last years. Many vibration-based techniques aiming at detecting small structural changes or even damage have been developed or enhanced through successive researches. Lately, several studies have focused on the use of raw dynamic data to assess information about structural condition. Despite this trend and much skepticism, many methods still rely on the use of modal parameters as fundamental data for damage detection. Therefore, it is of utmost importance that modal identification procedures are performed with a sufficient level of precision and automation. To fulfill these requirements, this paper presents a novel automated time-domain methodology to identify modal parameters based on a two-step clustering analysis. The first step consists in clustering modes estimates from parametric models of different orders, usually presented in stabilization diagrams. In an automated manner, the first clustering analysis indicates which estimates correspond to physical modes. To circumvent the detection of spurious modes or the loss of physical ones, a second clustering step is then performed. The second step consists in the data mining of information gathered from the first step. To attest the robustness and efficiency of the proposed methodology, numerically generated signals as well as experimental data obtained from a simply supported beam tested in laboratory and from a railway bridge are utilized. The results appeared to be more robust and accurate comparing to those obtained from methods based on one-step clustering analysis. © 2017 Elsevier Ltd","Automated modal identification; Clustering; Structural dynamics; Structural health monitoring","Automation; Composite beams and girders; Damage detection; Data mining; Modal analysis; Parameter estimation; Patient monitoring; Structural analysis; Structural dynamics; Structural health monitoring; Clustering; Clustering analysis; Modal identification; Modal parameters; Parametric models; Simply supported beams; Stabilization diagrams; Structural condition; Time domain analysis",2-s2.0-85018763654
"Stevens J.R., Resmini R.G., Messinger D.W.","Spectral-Density-Based Graph Construction Techniques for Hyperspectral Image Analysis",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028978916&doi=10.1109%2fTGRS.2017.2718547&partnerID=40&md5=2a796ce9963c788e11e72de986bf834f","The past decade has seen the emergence of many hyperspectral image (HSI) analysis algorithms based on graph theory and derived manifold coordinates. The performance of these algorithms is inextricably tied to the graphical model constructed from the spectral data, i.e., the community structure of the spectral data must be well represented to extract meaningful information. This paper provides a survey of many spectral graph construction techniques currently used by the hyperspectral community and discusses their advantages and disadvantages for hyperspectral analyses. A focus is provided on techniques influenced by spectral density from which the concept of community structure arises. Two inherently density-weighted graph construction techniques from the data mining literature, shared nearest neighbor (NN) and mutual proximity, are also introduced and compared as they have not been previously employed in HSI analyses. Density-based edge allocation is demonstrated to produce more uniform NN lists than nondensity-based techniques by demonstrating an increase in the number of intracluster edges and improved k-NN classification performance. Imposing the mutuality constraint to symmetrify an adjacency matrix is demonstrated to be beneficial in most circumstances, especially in rural (less cluttered) scenes. Surprisingly, many complex edge-reweighting techniques are shown to slightly degrade NN list characteristics. An analysis suggests this condition is possibly attributable to the validity of characterizing spectral density by a single variable representing data scale. As such, these complex edge-reweighting techniques may need to be modified to increase their effectiveness, or simply not be used. © 2017 IEEE.","Graph theory; hyperspectral imaging (HSI); imaging spectroscopy; remote sensing","Data mining; Edge detection; Graph theory; Graphic methods; Image analysis; Imaging techniques; Matrix algebra; Nearest neighbor search; Remote sensing; Social sciences; Spectral density; Spectroscopy; Adaptation models; Algorithm design and analysis; Image edge detection; Imaging spectroscopy; Symmetric matrices; Hyperspectral imaging",2-s2.0-85028978916
"Conde D., Le Gac A.-L., Perales M., Dervinis C., Kirst M., Maury S., González-Melendi P., Allona I.","Chilling-responsive DEMETER-LIKE DNA demethylase mediates in poplar bud break",2017,"Plant Cell and Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029443022&doi=10.1111%2fpce.13019&partnerID=40&md5=cb42852c24aa637c7477e95e3b19316f","Annual dormancy-growth cycle is a developmental and physiological process essential for the survival of deciduous trees in temperate and boreal forests. Seasonal control of shoot growth in woody perennials requires specific genetic programmes responding to environmental signals. The environmental-controlled mechanisms that regulate the shift between winter dormancy and the growth-promoting genetic programmes are still unknown. Here, we show that dynamics in genomic DNA methylation levels are involved in the regulation of dormancy-growth cycle in poplar. The reactivation of growth in the apical shoot during bud break process in spring is preceded by a progressive reduction of genomic DNA methylation in apex tissue. The induction in apex tissue of a chilling-dependent poplar DEMETER-LIKE 10 (PtaDML10) DNA demethylase precedes shoot growth reactivation. Transgenic poplars showing downregulation of PtaDML8/10 caused delayed bud break. Genome-wide transcriptome and methylome analysis and data mining revealed that the gene targets of DEMETER-LIKE-dependent DNA demethylation are genetically associated with bud break. These data point to a chilling-dependent DEMETER-like DNA demethylase mechanisms being involved in the shift from winter dormancy to a condition that precedes shoot apical vegetative growth in poplar. © 2017 John Wiley & Sons Ltd","bud break; DEMETER; DNA demethylases; DNA methylation; poplar; winter dormancy","budburst; data mining; deciduous tree; DNA; dormancy; enzyme; enzyme activity; freezing; genomics; growth; methylation; shoot growth; transgenic plant; winter; Populus",2-s2.0-85029443022
"Zhu J., Wang B., Wu B., Zhang W.","Emotional community detection in social network",2017,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030218826&doi=10.1587%2ftransinf.2016EDP7473&partnerID=40&md5=a8c320d81d979b07e67c4a13c79208dd","Community detection is a pivotal task in data mining, and users' emotional behaviors have an important impact on today's society. So it is very significant for society management or marketing strategies to detect emotional communities in social networks. Based on the emotional homophily of users in social networks, it could confirm that users would like to gather together to form communities according to emotional similarity. This paper exploits multivariate emotional behaviors of users to measure users' emotional similarity, then takes advantage of users' emotional similarity as edge weight to remodel an emotional network and detect communities. The detailed process of detecting emotional communities is as follows: 1) an emotional network is constructed and emotional homophily in experimental dataset is verified; 2) both CNM and BGLL algorithms are employed to detect emotional communities in emotional network, and emotional characters of each community are analyzed; 3) in order to verify the superiority of emotional network for detecting emotional communities, 1 unweighted network and 3 other weighted and undirected networks are constructed as comparison. Comparison experiments indicate that the emotional network is more suitable for detecting emotional communities, the users' emotional behaviors are more similar and denser in identical communities of emotional network than the contrastive networks' communities. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","Emotional community; Emotional homophily; Emotional network; Emotional similarity","Data mining; Marketing; Community detection; Edge weights; Emotional behavior; Emotional community; Emotional similarity; Homophily; Marketing strategy; Undirected network; Population dynamics",2-s2.0-85030218826
"Macander M.J., Frost G.V., Nelson P.R., Swingley C.S.","Regional quantitative cover mapping of tundra plant functional types in Arctic Alaska",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032870113&doi=10.3390%2frs9101024&partnerID=40&md5=f037a0d68d103d18fbca47a5973af028","Ecosystem maps are foundational tools that support multi-disciplinary study design and applications including wildlife habitat assessment, monitoring and Earth-system modeling. Here, we present continuous-field cover maps for tundra plant functional types (PFTs) across ~125,000 km2 of Alaska's North Slope at 30-m resolution. To develop maps, we collected a field-based training dataset using a point-intercept sampling method at 225 plots spanning bioclimatic and geomorphic gradients. We stratified vegetation by nine PFTs (e.g., low deciduous shrub, dwarf evergreen shrub, sedge, lichen) and summarized measurements of the PFTs, open water, bare ground and litter using the cover metrics total cover (areal cover including the understory) and top cover (uppermost canopy or ground cover). We then developed 73 spectral predictors derived from Landsat satellite observations (surface reflectance composites for ~15-day periods from May-August) and five gridded environmental predictors (e.g., summer temperature, climatological snow-free date) to model cover of PFTs using the random forest data-mining algorithm. Model performance tended to be best for canopy-forming PFTs, particularly deciduous shrubs. Our assessment of predictor importance indicated that models for low-statured PFTs were improved through the use of seasonal composites from early and late in the growing season, particularly when similar PFTs were aggregated together (e.g., total deciduous shrub, herbaceous). Continuous-field maps have many advantages over traditional thematic maps, and the methods described here are well-suited to support periodic map updates in tandem with future field and Landsat observations. © 2017 by the authors.","Alaska; Arctic tundra; Landsat; North Slope; Phenology; Plant functional types; Random forest; Reflectance composites; Vegetation mapping","Data mining; Decision trees; Forestry; Landforms; Mapping; Maps; Reflection; Vegetation; Alaska; Arctic tundra; LANDSAT; North Slope; Phenology; Plant functional type; Random forests; Vegetation mapping; Plants (botany)",2-s2.0-85032870113
"You H., Ma Z., Tang Y., Wang Y., Yan J., Ni M., Cen K., Huang Q.","Comparison of ANN (MLP), ANFIS, SVM, and RF models for the online classification of heating value of burning municipal solid waste in circulating fluidized bed incinerators",2017,"Waste Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017355673&doi=10.1016%2fj.wasman.2017.03.044&partnerID=40&md5=c7653f538621fea010265342db80d032","The heating values, particularly lower heating values of burning municipal solid waste are critically important parameters in operating circulating fluidized bed incineration systems. However, the heating values change widely and frequently, while there is no reliable real-time instrument to measure heating values in the process of incinerating municipal solid waste. A rapid, cost-effective, and comparative methodology was proposed to evaluate the heating values of burning MSW online based on prior knowledge, expert experience, and data-mining techniques. First, selecting the input variables of the model by analyzing the operational mechanism of circulating fluidized bed incinerators, and the corresponding heating value was classified into one of nine fuzzy expressions according to expert advice. Development of prediction models by employing four different nonlinear models was undertaken, including a multilayer perceptron neural network, a support vector machine, an adaptive neuro-fuzzy inference system, and a random forest; a series of optimization schemes were implemented simultaneously in order to improve the performance of each model. Finally, a comprehensive comparison study was carried out to evaluate the performance of the models. Results indicate that the adaptive neuro-fuzzy inference system model outperforms the other three models, with the random forest model performing second-best, and the multilayer perceptron model performing at the worst level. A model with sufficient accuracy would contribute adequately to the control of circulating fluidized bed incinerator operation and provide reliable heating value signals for an automatic combustion control system. © 2017 Elsevier Ltd","ANFIS; Circulating fluidized bed incinerators; Heating value; MLP; RF; SVM","Cost effectiveness; Data mining; Decision trees; Deep neural networks; Fluidized bed combustion; Fluidized bed process; Fluidized beds; Fuel additives; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Heating; Incineration; Multilayers; Solid wastes; Support vector machines; Tracking (position); Waste incineration; Adaptive neuro-fuzzy inference system; ANFIS; Circulating fluidized bed; Combustion-control systems; Comparative methodologies; Comprehensive comparisons; Heating value; Multi-layer perceptron neural networks; Municipal solid waste; artificial neural network; incineration; municipal solid waste; support vector machine; Article; artificial neural network; circulating fluidized bed incinerator; classification; combustion; controlled study; cost effectiveness analysis; data mining; fluidized bed; fuzzy system; heat; heating value; incineration; intermethod comparison; mathematical model; municipal solid waste; online system; prediction; priority journal; random forest; reliability; support vector machine",2-s2.0-85017355673
"López-Martín C., Ulloa-Cazarez R.L., García-Floriano A.","Support vector regression for predicting the productivity of higher education graduate students from individually developed software projects",2017,"IET Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029783744&doi=10.1049%2fiet-sen.2016.0304&partnerID=40&md5=db4baa2e880b3121f16b2d6dea137853","Productivity prediction of a software engineer is necessary to determine whether corrective actions are needed and to identify improvement options to produce better results. It can be performed from abstraction levels such as organisation, team project, individual project, or task Software engineering education and training has approached its efforts at individual level. In this study, the authors propose the application of a data mining technique named support vector regression (SVR) to predict the productivity of individuals (i.e. graduate students). Its prediction accuracy was compared with that of a statistical regression model, and with those of two neural networks. After applying a Wilcoxon statistical test, results suggest that an SVR with linear kernel using new and changed lines of code, and programming language experience as independent variables, could be used for predicting the individual productivity of a higher education graduate student, when software projects coded in either Java or C++ programming languages, have been developed by following a disciplined process specifically proposed for academic environments. © The Institution of Engineering and Technology 2017.",,"C++ (programming language); Computer programming; Computer software; Data mining; Education; Education computing; Forecasting; Object oriented programming; Productivity; Regression analysis; Software engineering; Software testing; Academic environment; Corrective actions; Independent variables; Individual levels; Individual productivity; Prediction accuracy; Statistical regression model; Support vector regression (SVR); Students",2-s2.0-85029783744
"Lee W., Yeo Y., Oh S., Cho K.-S., Park Y.-E., Park S.K., Lee S.M., Cho H.S., Park S.-Y.","Compositional analyses of diverse phytochemicals and polar metabolites from different-colored potato (Solanum tubersum L.) tubers",2017,"Food Science and Biotechnology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032568964&doi=10.1007%2fs10068-017-0167-2&partnerID=40&md5=d903699137a19d46ffadbb15b4d35b43","Lipophilic bioactive compounds and hydrophilic primary metabolites from potato (solanum tubersum L.) tubers with different-colored flesh (white-, yellow-, red-, and purple) were characterized. The carotenoid content was relatively higher in red-colored potatoes, in which lutein was most plentiful. Among the other lipophilic compounds analyzed, including policosanols, tocopherols, and phytosterols, octacosanol was measured in the largest amount, followed by β-sitosterol, irrespective of color variations. Forty-three hydrophilics consisting of amino acids, organic acids, sugars, and sugar alcohols and 18 lipophilics were subjected to data-mining processes. The results of multivariate statistical analyses clearly distincted the different varieties and separated red-fleshed potatoes from other color-fleshed potatoes according to abundance of amino acids, sugars, and carotenoids. This study confirmed the metabolic association-related biochemical pathway between metabolite characteristic and color differences in potato tubers. These results can facilitate understanding the metabolic differences among diverse colored potatoes and provide fruitful information for genetic engineering of potato cultivars. © 2017, The Korean Society of Food Science and Technology and Springer Science+Business Media B.V.","Metabolomics; Multivariate analysis; Phytochemicals; Potato; Primary metabolites","Amino acids; Biomolecules; Color; Data mining; Genetic engineering; Metabolism; Metabolites; Pigments; Plants (botany); Sugars; Tubes (components); Metabolomics; Multi variate analysis; Phytochemicals; Potato; Primary metabolite; Multivariant analysis",2-s2.0-85032568964
"Huang J.-N., Hong T.-P., Chiang M.-C.","Reference itemsets: useful itemsets to approximate the representation of frequent itemsets",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969779507&doi=10.1007%2fs00500-016-2172-4&partnerID=40&md5=879b173c1b6f3221a3e43544208afc2b","Deriving frequent itemsets from databases is an important research issue in data mining. The number of frequent itemsets may be unusually large when a low minimum support threshold is given. As such, the design of a compact representation to compress and describe them is an interesting topic. In the past, most related research on compact representation focused on frequent closed itemsets and frequent maximal itemsets. The former is a lossless compact technology that can totally recover all frequent itemsets and their frequencies. Contrarily, the latter may lose some information regarding frequent itemsets, because it reserves frequent itemsets only and is unable to identify their frequency. In this paper, we propose a new compact representation that lies between closed itemsets and maximal itemsets. It can reserve all frequent itemsets and identify their approximate frequency. In addition, an efficient algorithm that corresponds to this new concept is designed to find related key information in databases. Finally, a series of experiments are conducted to show the effectiveness of compact representation and the performance of the proposed algorithm. © 2016, Springer-Verlag Berlin Heidelberg.","Approximate representation; Closed itemset; Data mining; Frequent itemset; Maximal itemset; Reference itemset","Algorithms; Approximate representation; Closed itemset; Compact representation; Frequent closed itemsets; Frequent itemset; Itemset; Maximal itemsets; Minimum support thresholds; Data mining",2-s2.0-84969779507
"Barrientos R.J., Millaguir F., Sánchez J.L., Arias E.","GPU-based exhaustive algorithms processing kNN queries",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024491966&doi=10.1007%2fs11227-017-2110-y&partnerID=40&md5=d25c5e7b3bd7f8f5311707a6f0398f91","Efficient kNN search, or k-nearest neighbors search, is useful, among other fields, in multimedia information retrieval, data mining and pattern recognition problems. A distance function determines how similar the objects are to a given kNN query object. As finding the distance between any given pair of objects (i.e., high-dimensional vectors) is known to be a computationally expensive operation, using parallel computation techniques is an effective way of reducing running times to acceptable values in large databases. In the present work, we offer novel GPU approaches to solving kNN (k-nearest neighbor) queries using exhaustive algorithms based on the Selection Sort, Quicksort and state-of-the-art algorithms. We show that the best approach depends on the k value of the kNN query and achieve a speedup up to 86.4× better than the sequential counterpart. We also propose a multi-core algorithm to be used as reference for the experiments and a hybrid algorithm which combines the proposed algorithms with a state-of-the-art heaps-based method, in which the best performance is obtained with high k values. We also extend our algorithms to be able to deal with large databases that do not fit in GPU memory and whose performance does not deteriorate as database size increases. © 2017, Springer Science+Business Media, LLC.","Exhaustive algorithms; GPU; kNN; Quicksort; Selection Sort","Data mining; Database systems; Graphics processing unit; Membership functions; Nearest neighbor search; Pattern recognition; Query processing; Text processing; Database size increase; K-nearest neighbors; Multi-core algorithm; Multimedia information retrieval; Parallel Computation; Pattern recognition problems; Quicksort; State-of-the-art algorithms; Learning algorithms",2-s2.0-85024491966
"Nadri H., Rahimi B., Timpka T., Sedghi S.","The Top 100 Articles in the Medical Informatics: a Bibliometric Analysis",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027838642&doi=10.1007%2fs10916-017-0794-4&partnerID=40&md5=c3e30621d22527d36828c12550b70f17","The number of citations that a research paper receives can be used as a measure of its scientific impact. The objective of this study was to identify and to examine the characteristics of top 100 cited articles in the field of Medical Informatics based on data acquired from the Thomson Reuters’ Web of Science (WOS) in October, 2016. The data was collected using two procedures: first we included articles published in the 24 journals listed in the “Medical Informatics” category; second, we retrieved articles using the key words: “informatics”, “medical informatics”, “bi﻿omedical informatics”, ”clinical informatics” and “health informatics”. After removing duplicate records, articles were ranked by the number of citations they received. When the 100 top cited articles had been identified, we collected the following information for each record: all WOS database citations, year of publication, journal, author names, authors’ affiliation, country of origin and topics indexed for each record. Citations for the top 100 articles ranged from 346 to 7875, and citations per year ranged from 11.12 to 525. The majority of articles were published in the 2000s (n=43) and 1990s (n=38). Articles were published across 10 journals, most commonly Statistics in medicine (n=71) and Medical decision making (n=28). The articles had an average of 2.47 authors. Statistics and biostatistics modeling was the most common topic (n=71), followed by artificial intelligence (n=12), and medical errors (n=3), other topics included data mining, diagnosis, bioinformatics, information retrieval, and medical imaging. Our bibliometric analysis illustrated a historical perspective on the progress of scientific research on Medical Informatics. Moreover, the findings of the current study provide an insight on the frequency of citations for top cited articles published in Medical Informatics as well as quality of the works, journals, and the trends steering Medical Informatics. © 2017, Springer Science+Business Media, LLC.","Articles; Bibliometric; Medical informatics; Top-cited","artificial intelligence; bioinformatics; biostatistics; data mining; diagnosis; diagnostic imaging; information retrieval; major clinical study; medical decision making; medical error; medical informatics; model; nomenclature; publication; Web of Science",2-s2.0-85027838642
"Labao A.B., Naval P.C., Jr., Yap D.L.T., Yap H.T.","Influencing rural livelihood switching through equipment assets for agroecosystems to alleviate pressure on resources",2017,"Agriculture, Ecosystems and Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026870351&doi=10.1016%2fj.agee.2017.07.016&partnerID=40&md5=2d1bb1386d061ee0c812325581b2d21e","Equipment provision influences livelihood choice by rural communities since it conditions their capacity to produce goods even with limited labor resources. This study examines how variations in equipment levels for fishing, rice cultivation, and copra farming affect the conditional probabilities of households in a rural Philippine community to adopt different livelihood strategies, particularly, to veer away from fishing and to adopt farming. A household survey profiled diverse income sources alongside available agricultural or fishing equipment. To determine a household's equipment index and rank its sophistication, the medoid clustering algorithm from data mining was used. Diverse farming equipment was classified into groups according to technology and ownership. Medoid clustering generated five different livelihood profiles comprising agricultural and non-agricultural work. An ensemble of decision trees created from nonparametric random forest regression predicted the conditional probabilities of a household's livelihood choice depending on equipment levels. Results revealed heterogeneity between the effects of rice and copra equipment − with rice exerting a stronger influence over households to choose farming over fishing. Rice is a potentially greater income earner and typically represents a primary form of livelihood. However, a threshold level of available equipment must be exceeded before households are strongly encouraged to choose rice farming, representing the minimum amount of capital needed to generate sufficient annual income and become a primary form of sustenance. Ultimately, the ideal configuration that will influence households to focus on agriculture is a combination of adequate levels of both rice and copra equipment, since both crops are complementary, addressing the need for agricultural diversification to mitigate various kinds of risk. A greater focus on agriculture would help address chronic food shortages, as well as hopefully alleviate fisheries overexploitation. For this strategy to be effective, however, issues of land ownership, access to irrigation and land degradation must also be addressed. © 2017 Elsevier B.V.","Agricultural equipment; Resilience; Rural livelihood profiles; Social medoid-clustering; Work-shifting propensity","agricultural diversification; agricultural ecosystem; algorithm; cluster analysis; cultivation; data mining; equipment; exploitation; household survey; land degradation; landownership; resource use; rice; rural population; Philippines",2-s2.0-85026870351
"Liu P., Li Z., Zhuo Y., Lin X., Ding S., Khalid M.S., Adio O.S.","Design of Wind Turbine Dynamic Trip-Off Risk Alarming Mechanism for Large-Scale Wind Farms",2017,"IEEE Transactions on Sustainable Energy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030180743&doi=10.1109%2fTSTE.2017.2701348&partnerID=40&md5=b2328e12a9729fd8f898334b1dfdcaf3","The wind power in China is being developed mainly in terms of large scale, long distance, and high clustering. In this scenario, the uncertainty of wind farm operation gradually becomes a significant factor that power grid dispatcher needs to deal with. Among which, trip-off risk of wind turbine results from system disturbance, fault and fault removal of wind farm, etc. Besides, the geographical distribution of wind turbine usually also has the impact on the trip-off risk of wind farm. In view of above factors, existing security analysis methodology is difficult to satisfy the requirement of real-time alarming due to extreme analytical difficulty and huge computing complexity. Therefore, reasonable design of trip-off risk measure index of wind turbine becomes a key issue to implement a dynamic trip-off real-time alarming system. In this paper, a wind turbine trip-off decision tree system is constructed with the discrete characteristics of wind turbine distribution in wind farms taken into account. The decision trees can perform data mining using online information and make fast prediction on wind turbine voltage out-of-limit and trip-off situations under anticipated accident sets. The trip-off risk measure indexes are then output in light of the prediction results, which can provide intuitive risk level information and decision references for operators in wind farms and power systems. The results of case studies show that the proposed method can meet the requirement of prediction accuracy and resolve the contradictions between the amount of modeled nodes, the diversity of anticipated faults and the computation time when performing over-voltage early warning and control strategies. © 2010-2012 IEEE.","Decision trees; online security assessment; risk measure; wind farm trip-off","Data mining; Decision trees; Electric load dispatching; Electric power system interconnection; Electric utilities; Forecasting; Geographical distribution; Risk assessment; Trees (mathematics); Wind power; Wind turbines; Computing complexity; Large-scale wind farms; On-line information; On-line securities; Risk measures; Wind farm; Wind farm operations; Wind power in chinas; Electric power transmission networks",2-s2.0-85030180743
"Le B., Duong H., Truong T., Fournier-Viger P.","FCloSM, FGenSM: two efficient algorithms for mining frequent closed and generator sequences using the local pruning strategy",2017,"Knowledge and Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013101811&doi=10.1007%2fs10115-017-1032-6&partnerID=40&md5=aef81e68ce0b21dcc1d2e45dc154518e","Mining frequent sequences in sequential databases are highly valuable for many real-life applications. However, in several cases, especially when databases are huge and when low minimum support thresholds are used, the cardinality of the result set can be enormous. Consequently, algorithms for discovering frequent sequences exhibit poor performance, showing an important increase in execution time, memory consumption and storage space usage. To address this issue, researchers have studied the tasks of mining frequent closed and generator sequences, as they provide several benefits when compared to the set of frequent sequences. One of the most important benefits is that the cardinalities of frequent closed and generator sequences are generally much less than the cardinality of frequent sequences. Hence, humans find it more convenient to analyze the information provided by closed and generator sequences. Moreover, it was shown that frequent closed sequences have the advantage of being lossless, and they thus preserve information about the frequency of all frequent subsequences, while generator sequences can provide higher accuracy for sequence classification tasks since they are the smallest patterns that characterize groups of sequences. Besides, frequent closed sequences can be combined with generators to produce non-redundant sequential rules and recover the complete set of frequent sequences and their frequencies. This paper proposes two novel algorithms named FCloSM and FGenSM to mine frequent closed and generator sequences efficiently. These algorithms are based on new pruning conditions called extended early elimination (3E) and early pruning techniques named EPCLO and EPGEN, designed to identify non-closed and non-generator patterns early. Based on these techniques, two local pruning strategies called LPCLO and LPGEN are proposed to eliminate non-closed and non-generator patterns more efficiently at two successive levels of the prefix search tree without performing subsequence relation checking. These theoretical results, which are the basis of FCloSM and FGenSM, are mathematically proved and are shown to be more general than those presented in previous work. Extensive experiments show that FCloSM and FGenSM are one to two orders of magnitude faster than the state-of-the-art algorithms for discovering frequent closed sequences (CloSpan, BIDE, ClaSP and CM-ClaSP) and for mining frequent generators (FEAT, FSGP and VGEN), and that FCloSM and FGenSM consume much less memory. © 2017, Springer-Verlag London.","Frequent closed sequence; Frequent sequence; Sequential pattern mining; Vertical data format",,2-s2.0-85013101811
"Kwong T., Wong E., Yue K.","Bringing Abstract Academic Integrity and Ethical Concepts into Real-Life Situations",2017,"Technology, Knowledge and Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020110819&doi=10.1007%2fs10758-017-9315-2&partnerID=40&md5=5361810be7797aa5b795317a78b2e44e","This paper reports the learning analytics on the initial stages of a large-scale, government-funded project which inducts university students in Hong Kong into consideration of academic integrity and ethics through mobile Augmented Reality (AR) learning trails—Trails of Integrity and Ethics (TIEs)—accessed on smart devices. The trails immerse students in collaborative problem solving tasks centred on ethical dilemmas, addressed in real, actual locations where such dilemmas might arise, with contextually appropriate digital advice and information available on hand. Students play out the consequences of their decisions which help reinforce the links between the theoretical concept of academic integrity and ethics and the practical application in everyday contexts. To evaluate the effectiveness of the TIEs, triangulation of different sets of data is adopted and these datasets include user experience surveys, qualitative feedback, clickstream data, and text mining of pre-/post-trail discussion. Thousands of students’ responses and related data gathered are analysed to ascertain the effectiveness of these mobile learning trails in enhancing students’ awareness of AIE issues. The positive learning outcome of the TIEs suggests that this approach can be adopted and applied to a wider scope of the academic curriculum and co-curriculum. © 2017, Springer Science+Business Media Dordrecht.","Academic integrity; Augmented reality; Ethics; Learning analytics; Learning trail; Mobile learning","Abstracting; Augmented reality; Curricula; Data mining; E-learning; Philosophical aspects; Problem solving; Students; Academic integrity; Ethics; Learning analytics; Learning trail; Mobile Learning; Education",2-s2.0-85020110819
"Deng M., He Z., Liu Q., Cai J., Tang J.","Multi-scale approach to mining significant spatial co-location patterns",2017,"Transactions in GIS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006958112&doi=10.1111%2ftgis.12261&partnerID=40&md5=51ebce186be2c21c135d8d0da10b19bd","Spatial co-location pattern mining aims to discover a collection of Boolean spatial features, which are frequently located in close geographic proximity to each other. Existing methods for identifying spatial co-location patterns usually require users to specify two thresholds, i.e. the prevalence threshold for measuring the prevalence of candidate co-location patterns and distance threshold to search the spatial co-location patterns. However, these two thresholds are difficult to determine in practice, and improper thresholds may lead to the misidentification of useful patterns and the incorrect reporting of meaningless patterns. The multi-scale approach proposed in this study overcomes this limitation. Initially, the prevalence of candidate co-location patterns is measured statistically by using a significance test, and a non-parametric model is developed to construct the null distribution of features with the consideration of spatial auto-correlation. Next, the spatial co-location patterns are explored at multi-scales instead of single scale (or distance threshold) discovery. The validity of the co-location patterns is evaluated based on the concept of lifetime. Experiments on both synthetic and ecological datasets show that spatial co-location patterns are discovered correctly and completely by using the proposed method; on the other hand, the subjectivity in discovery of spatial co-location patterns is reduced significantly. © 2016 John Wiley & Sons Ltd","lifetime; multi-scale; pattern reconstruction; significance test; spatial co-location pattern","autocorrelation; data set; mining; spatial analysis; spatial variation; threshold",2-s2.0-85006958112
"Liao H., Zeng A., Zhou M., Mao R., Wang B.-H.","Information mining in weighted complex networks with nonlinear rating projection",2017,"Communications in Nonlinear Science and Numerical Simulation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017439837&doi=10.1016%2fj.cnsns.2017.03.018&partnerID=40&md5=7b0d3d65f2ab21cd83e686951a5937a7","Weighted rating networks are commonly used by e-commerce providers nowadays. In order to generate an objective ranking of online items’ quality according to users’ ratings, many sophisticated algorithms have been proposed in the complex networks domain. In this paper, instead of proposing new algorithms we focus on a more fundamental problem: the nonlinear rating projection. The basic idea is that even though the rating values given by users are linearly separated, the real preference of users to items between the different given values is nonlinear. We thus design an approach to project the original ratings of users to more representative values. This approach can be regarded as a data pretreatment method. Simulation in both artificial and real networks shows that the performance of the ranking algorithms can be improved when the projected ratings are used. © 2017 Elsevier B.V.","Complex networks; E-commerce system; Rating projection; Social interaction","Commerce; Electronic commerce; Rating; Data pre treatments; E-commerce systems; Information mining; Ranking algorithm; Real networks; Representative values; Social interactions; Weighted complex networks; Complex networks",2-s2.0-85017439837
"Wang Y., Bian Z., Lei S., Zhang Y.","Investigating spatial and temporal variations of soil moisture content in an arid mining area using an improved thermal inertia model",2017,"Journal of Arid Land",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028919661&doi=10.1007%2fs40333-017-0065-8&partnerID=40&md5=eb3ef02be3eb43d828769156d4d6469e","Mining operations can usually lead to environmental deteriorations. Underground mining activities could cause an extensive decrease in groundwater level and thus a dramatic variation in soil moisture content (SMC). In this study, the spatial and temporal variations of SMC from 2001 to 2015 at two spatial scales (i.e., the Shendong coal mining area and the Daliuta Coal Mine) were analyzed using an improved thermal inertia model with a long-term series of Landsat TM/OLI (TM=Thematic Mapper and OLI=Operational Land Imager) data. Our results show that at large spatial scale (the Shendong coal mining area), underground mining activities had insignificant negative impacts on SMC and that at small spatial scale (the Daliuta Coal Mine), underground mining activities had significant negative impacts on SMC. Trend analysis of SMC demonstrated that areas with decreasing trend of SMC were mainly distributed in the mined area, indicating that underground mining is a primary cause for the drying trend in the mining region in this arid environment. © 2017, Xinjiang Institute of Ecology and Geography, the Chinese Academy of Sciences and Springer-Verlag GmbH Germany.","mining disturbance; Shendong coal mining area; soil moisture content; spatial-temporal variation; thermal inertia","arid environment; coal mine; coal mining; environmental degradation; environmental impact; Landsat; moisture content; soil moisture; spatiotemporal analysis; thematic mapping; trend analysis",2-s2.0-85028919661
"Arar Ö.F., Ayan K.","A feature dependent Naive Bayes approach and its application to the software defect prediction problem",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020745346&doi=10.1016%2fj.asoc.2017.05.043&partnerID=40&md5=919097c232cc9326699b1ae1067689b1","Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set. © 2017 Elsevier B.V.","Data mining; Discretization; Feature independence; Naive Bayes; Software defect prediction","Application programs; Classification (of information); Classifiers; Defects; Forecasting; NASA; Text processing; Classification methods; Competitive performance; Discretizations; Feature independence; Naive bayes; Naive Bayes approaches; Probabilistic approaches; Software defect prediction; Data mining",2-s2.0-85020745346
"Vranken H.","Sustainability of bitcoin and blockchains",2017,"Current Opinion in Environmental Sustainability",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019733993&doi=10.1016%2fj.cosust.2017.04.011&partnerID=40&md5=6923d847c63e5bb2a6ba99b007633593","Bitcoin is an electronic currency that has become increasingly popular since its introduction in 2008. Transactions in the bitcoin system are stored in a public transaction ledger (‘the blockchain’), which is stored in a decentralized, peer-to-peer network. Bitcoin provides decentralized currency issuance and transaction clearance. The security of the blockchain depends on a compute-intensive algorithm for bitcoin mining, which prevents double spending of bitcoins and tampering with confirmed transactions. This ‘proof-of-work’ algorithm is energy demanding. How much energy is actually consumed, is subject of debate. We argue that this energy consumption currently is in the range of 100–500 MW. We discuss the developments in bitcoin mining hardware. We also briefly outline alternative schemes that are less energy demanding. We finally look at other blockchain applications, and argue that also here energy consumption is not of primary concern. © 2017 Elsevier B.V.",,"computer simulation; currency; currency market; data mining; decentralization; electronic commerce; equipment; fuel consumption; sustainability",2-s2.0-85019733993
"O'Reilly C., Moessner K., Nati M.","Univariate and Multivariate Time Series Manifold Learning",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021093827&doi=10.1016%2fj.knosys.2017.05.026&partnerID=40&md5=1459641517c94d612c0a58a58f223795","Time series analysis aims to extract meaningful information from data that has been generated in sequence by a dynamic process. The modelling of the non-linear dynamics of a signal is often performed using a linear space with a similarity metric which is either linear or attempts to model the non-linearity of the data in the linear space. In this research, a different approach is taken where the non-linear dynamics of the time series are represented using a phase space. Training data is used to construct the phase space in which the data lies on or close to a lower-dimensional manifold. The basis of the non-linear manifold is derived using the kernel principal components derived using kernel principal component analysis where fewer components are retained in order to identify the lower-dimensional manifold. Data instances are projected onto the manifold, and those with a large distance between the original point and the projection are considered to be derived from a different underlying process. The proposed algorithm is able to perform time series classification on univariate and multivariate data. Evaluations on a large number of real-world data sets demonstrate the accuracy of the new algorithm and how it exceeds state-of-the-art performance. © 2017 Elsevier B.V.","Kernel principal component analysis; Multivariate; One-class classification; Time series; Univariate","Data mining; Multivariant analysis; Phase space methods; Time series; Time series analysis; Kernel principal component; Kernel principal component analyses (KPCA); Lower dimensional manifolds; Multivariate; One-class Classification; State-of-the-art performance; Time series classifications; Univariate; Principal component analysis",2-s2.0-85021093827
"Di Gregorio E., Riberi E., Belligni E.F., Biamino E., Spielmann M., Ala U., Calcia A., Bagnasco I., Carli D., Gai G., Giordano M., Guala A., Keller R., Mandrile G., Arduino C., Maffè A., Naretto V.G., Sirchia F., Sorasio L., Ungari S., Zonta A., Zacchetti G., Talarico F., Pappi P., Cavalieri S., Giorgio E., Mancini C., Ferrero M., Brussino A., Savin E., Gandione M., Pelle A., Giachino D.F., De Marchi M., Restagno G., Provero P., Cirillo Silengo M., Grosso E., Buxbaum J.D., Pasini B., De Rubeis S., Brusco A., Ferrero G.B.","Copy number variants analysis in a cohort of isolated and syndromic developmental delay/intellectual disability reveals novel genomic disorders, position effects and candidate disease genes",2017,"Clinical Genetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026304485&doi=10.1111%2fcge.13009&partnerID=40&md5=c7ab631641b8821d26f18d756cd58a10","Background: Array-comparative genomic hybridization (array-CGH) is a widely used technique to detect copy number variants (CNVs) associated with developmental delay/intellectual disability (DD/ID). Aims: Identification of genomic disorders in DD/ID. Materials and methods: We performed a comprehensive array-CGH investigation of 1,015 consecutive cases with DD/ID and combined literature mining, genetic evidence, evolutionary constraint scores, and functional information in order to assess the pathogenicity of the CNVs. Results: We identified non-benign CNVs in 29% of patients. Amongst the pathogenic variants (11%), detected with a yield consistent with the literature, we found rare genomic disorders and CNVs spanning known disease genes. We further identified and discussed 51 cases with likely pathogenic CNVs spanning novel candidate genes, including genes encoding synaptic components and/or proteins involved in corticogenesis. Additionally, we identified two deletions spanning potential Topological Associated Domain (TAD) boundaries probably affecting the regulatory landscape. Discussion and conclusion: We show how phenotypic and genetic analyses of array-CGH data allow unraveling complex cases, identifying rare disease genes, and revealing unexpected position effects. © 2017 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd","array-CGH; autism spectrum disorder; CNV; developmental delay; genomic disorders; intellectual disability","protein; adult; array comparative genomic hybridization; Article; bioinformatics; brain cortex; case report; child; chromosome deletion; chromosome duplication; cohort analysis; comparative genomic hybridization; controlled study; copy number variation; data mining; developmental delay; female; gene function; gene identification; genetic association; genetic conservation; genetic disorder; genomics; human; intellectual impairment; major clinical study; male; medical record review; pathogenicity; position effect; preschool child; priority journal; school child; scoring system; synapse; young adult",2-s2.0-85026304485
"Durden J.M., Durden J.M., Jones D.O.B., Murphy K., Jaeckel A., Van Dover C.L., Christiansen S., Gjerde K., Ortega A., Durden J.M.","A procedural framework for robust environmental management of deep-sea mining projects using a conceptual model",2017,"Marine Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026225318&doi=10.1016%2fj.marpol.2017.07.002&partnerID=40&md5=718d2eb7f5c56caa64c095d45b97aa20","Robust environmental management of deep-sea mining projects must be integrated into the planning and execution of mining operations, and developed concurrently. It should follow a framework indicating the environmental management-related activities necessary at each project phase, and their interrelationships. An environmental management framework with this purpose is presented in this paper; it facilitates the development of environmental information and decision-making throughout the phases of a mining project. It is based environmental management frameworks used in allied industries, but adjusted for unique characteristics of deep-sea mining. It defines the gathering and synthesis of information and its use in decision-making, and employs a conceptual model as a growing repository of claim-specific information. The environmental management activities at each phase have been designed to enable the implementation of the precautionary approach in decision making, while facilitating review of adaptive management measures to improve environmental management as the quantity and quality of data increases and technologies are honed. This framework will ensure fairness and uniformity in the application of environmental standards, assist the regulator in its requirements to protect the environment, and benefit contractors and financiers by reducing uncertainty in the process. © 2017","Adaptive management; Deep-sea mining; Environmental management; Mining Code; Precautionary approach; Project management","adaptive management; conceptual framework; decision making; environmental management; environmental modeling; precautionary principle; project management; submarine mining",2-s2.0-85026225318
"Nunes C.F.G., Padua F.L.C.","A Local Feature Descriptor Based on Log-Gabor Filters for Keypoint Matching in Multispectral Images",2017,"IEEE Geoscience and Remote Sensing Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029175956&doi=10.1109%2fLGRS.2017.2738632&partnerID=40&md5=73cabbbf38b1e93a391aee8065cca1cb","This letter presents a new local feature descriptor for problems related to multispectral images. Most previous approaches are typically based on descriptors designed to work with images uniquely captured in the visible light spectrum. In contrast, this letter proposes a descriptor termed a multispectral feature descriptor (MFD) that is especially developed, such that it can be employed with image data acquired at different frequencies across the electromagnetic spectrum. The performance of the MFD is evaluated by using three data sets composed of images obtained in visible light and infrared spectra, and its performance is compared with those of state-of-the-art algorithms, such as edge-oriented histogram (EOH) and log-Gabor histogram descriptor (LGHD). The experimental results indicate that the computational efficiency of MFD exceeds those of EOH and LGHD, and that the precision and recall values of MFD are statistically comparable to the corresponding values of the forementioned algorithms. © 2004-2012 IEEE.","Image matching; local feature descriptor; log-Gabor filters; multispectral images","Bandpass filters; Computational efficiency; Data mining; Feature extraction; Filtration; Gabor filters; Graphic methods; Image matching; Light; Remote sensing; Standards; Electromagnetic spectra; Histograms; Image edge detection; Local feature descriptor; Log-gabor filter; Multispectral images; Edge detection",2-s2.0-85029175956
"Shahid N., Ilyas M.U., Alowibdi J.S., Aljohani N.R.","Word cloud segmentation for simplified exploration of trending topics on Twitter",2017,"IET Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029783536&doi=10.1049%2fiet-sen.2016.0307&partnerID=40&md5=165c312c4beba5940d2c2c763d92f3ba","Twitter is a popular microblogging platform, with 310 million monthly active users as of the first quarter of 2016. It is a rapidly growing microblogging platform where people share opinions, news on any topic of their interest. More than 7000 tweets are posted every second Due to the enormous volume of data being generated, it becomes difficult to extract useful/meaningful information. Tweets collected from Twitter on a certain topic may consist of numerous conversation threads about relevant subtopics. However, it is difficult to discern these sub-topics if the data is visualised as a single word cloud. The authors transform a corpus of tweets to a spectral domain and evaluate the results from a number of clustering algorithms, including K-means, latent semantic indexing and non-negative matrix factorisation to construct clustered word clouds that helps identify sub-topics under a broader topic. © The Institution of Engineering and Technology 2017.",,"Clustering algorithms; Semantics; Social networking (online); First quarter; Latent Semantic Indexing; Micro-blogging platforms; Non-negative matrix factorisation; Single words; Spectral domains; Trending topics; Word clouds; Data mining",2-s2.0-85029783536
"Ghahramani A., Karvigh S.A., Becerik-Gerber B.","HVAC system energy optimization using an adaptive hybrid metaheuristic",2017,"Energy and Buildings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025164293&doi=10.1016%2fj.enbuild.2017.07.053&partnerID=40&md5=c48d8ec972285faa193435ab67da0311","Previous research efforts, for optimizing energy usage of HVAC systems, require either mathematical models of HVAC systems to be built or they require substantial historical operational data for learning optimal operational settings. We introduce a model-free control policy that begins learning optimal settings with no prior historical data and optimizes HVAC operations. The control policy is an adaptive hybrid metaheuristic that uses real-time data, stored in building automation systems (e.g., gas/electricity consumption, weather, and occupancy). It finds optimal setpoints at the building level and controls setpoints accordingly. The algorithm consists of metaheuristic (k-nearest neighbor stochastic hill climbing), machine learning (regression decision tree), and self-tuning (recursive brute-force search) components. The control policy uses smart selection of daily setpoints as its control basis, making the control schema complementary to legacy building management systems. To evaluate our approach, we used the DOE reference small office building in all U.S. climate zones and simulated different control policies using EnergyPlus. The proposed algorithm resulted in 31.17% energy savings compared to the baseline operations (22.5 °C and 3 K). The algorithm has a superior performance in all climate zones for the goodness of measure (i.e., normalized root mean square error) with a value of 0.047. © 2017 Elsevier B.V.","Adaptive learning; Energy efficiency; HVAC system; Online learning; Optimal control; Setpoint optimization","Adaptive control systems; Automation; Climate control; Data mining; Decision trees; Education; Energy efficiency; Energy utilization; Intelligent buildings; Legacy systems; Machine components; Mean square error; Nearest neighbor search; Office buildings; Optimal control systems; Real time systems; Stochastic systems; Trees (mathematics); Adaptive learning; HVAC system; Online learning; Optimal controls; Set-point optimization; Optimization",2-s2.0-85025164293
"Inkpen D., Liu J., Farzindar A., Kazemi F., Ghazi D.","Location detection and disambiguation from twitter messages",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016613252&doi=10.1007%2fs10844-017-0458-3&partnerID=40&md5=7f4a3eeab7902863439485904cf79df1","A remarkable amount of Twitter messages are generated every second. Detecting the location entities mentioned in these messages is useful in text mining applications. Therefore, techniques for extracting the location entities from the Twitter textual content are needed. In this work, we approach this task in a similar manner to the Named Entity Recognition (NER) task, but we focus only on locations, while NER systems detect names of persons, organizations, locations, and sometimes more (e.g., dates, times). But, unlike NER systems, we address a deeper task: classifying the detected locations into names of cities, provinces/states, and countries in order to map them into physical locations. We approach the task in a novel way, consisting in two stages. In the first stage, we train Conditional Random Fields (CRF) models that are able to detect the locations mentioned in the messages. We train three classifiers: one for cities, one for provinces/states, and one for countries, with various sets of features. Since a dataset annotated with this kind of information was not available, we collected and annotated our own dataset to use for training and testing. In the second stage, we resolve the remaining ambiguities, namely, cases when there exists more than one place with the same name. We proposed a set of heuristics able to choose the correct physical location in these cases. Our two-stage model will allow a social media monitoring system to visualize the places mentioned in Twitter messages on a map of the world or to compute statistics about locations. This kind of information can be of interest to business or marketing applications. © 2017, Springer Science+Business Media New York.","Artificial intelligence; Information extraction; Machine learning; Natural language processing; Social media","Artificial intelligence; Data mining; Information retrieval; Learning algorithms; Learning systems; Natural language processing systems; Random processes; Social networking (online); Statistical tests; Text processing; Conditional random field; Location detection; Marketing application; Named entity recognition; NAtural language processing; Social media; Social media monitoring; Training and testing; Location",2-s2.0-85016613252
"Xu H., Wang W., Qian Y.","Fusing complete monotonic decision trees",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023639187&doi=10.1109%2fTKDE.2017.2725832&partnerID=40&md5=5424957efc3125794f2eb4b9e6955e07","Monotonic classification is a kind of classification task in which a monotonicity constraint exist between features and class, i.e., if sample xi has a higher value in each feature than sample xj, it should be assigned to a class with a higher level than the level of xj 's class. Several methods have been proposed, but they have some limits such as with limited kind of data or limited classification accuracy. In our former work, the classification accuracy on monotonic classification has been improved by fusing monotonic decision trees, but it always has a complex classification model. This work aims to find a monotonic classifier to process both nominal and numeric data by fusing complete monotonic decision trees. Through finding the completed feature subsets based on discernibility matrix on ordinal dataset, a set of monotonic decision trees can be obtained directly and automatically, on which the rank is still preserved. Fewer decision trees are needed, which will serve as base classifiers to construct a decision forest fused complete monotonic decision trees. The experiment results on 10 datasets demonstrate that the proposed method can reduce the number of base classifiers effectively and then simplify classification model, and obtain good classification performance simultaneously. © 1989-2012 IEEE.","Decision tree; Discernibility matrix; Ensemble learning; Feature selection; Monotonic classification","Data mining; Decision trees; Education; Entropy; Feature extraction; Information systems; Matrix algebra; Vegetation; Algorithm design and analysis; Classification accuracy; Classification models; Classification performance; Discernibility matrix; Ensemble learning; Monotonic classifications; Monotonicity constraint; Classification (of information)",2-s2.0-85023639187
"Thakare A.R., Deshpande P.S.","Comparative Search of Entities",2017,"International Journal of Software Engineering and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028853628&doi=10.1142%2fS0218194017500498&partnerID=40&md5=33a35733d86fdadf928fe3aaf91120a6","Nowadays, every manufacturer or retailer displays their product information on various websites. The customer has to visit, the number of such web pages to choose the right product, because the information is not available at one place. There are some websites that show such information in one place, but they are product specific and in general information is manually updated. In this paper, we propose a novel concept of web-spreadsheet, which displays product information by crawling through related web pages and generates information like a spreadsheet where each row represents product information and each column represents product attributes. We are extracting the product name of specified product class using decision tree-based classifier by features obtained using Part of Speech (POS) tagging and distance measure. It also extracts the value-measure pairs of preset attributes using distance measure, POS tagging and Data type. This approach will save a lot of time of comparing different products and customers need not have to scan a number of websites for comparison. We present promising results in various product classes which surpass many existing techniques in the literature. The proposed method can work accurately without initial trained labeled data which is expensive to obtain. © 2017 The Author(s).","attribute value extraction; Entity extraction; product comparison; web spreadsheet","Computational linguistics; Data mining; Decision trees; Extraction; Sales; Spreadsheets; Attribute values; Distance measure; Entity extractions; General information; Part of speech tagging; Product attributes; product comparison; Product information; Websites",2-s2.0-85028853628
"Walker P.B., Mehalick M.L., Glueck A.C., Tschiffely A.E., Cunningham C.A., Norris J.N., Davidson I.N.","A decision tree framework for understanding blast-induced mild Traumatic Brain Injury in a military medical database",2017,"Journal of Defense Modeling and Simulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031121151&doi=10.1177%2f1548512916683841&partnerID=40&md5=b45c6a1cd284c8050de913ea1f0f9374","Personalized medicine is a ubiquitous term that has come to be used to describe a medical model that proposes the customization of healthcare, such that decisions and/or treatments are tailored to each individual patient. Under this type of clinical practice model, diagnostic and prognostic decisions are often based upon selecting the most appropriate therapy based on a patient’s genetic, demographic, and/or other pertinent information. The primary aim of this paper is to use a personalized medicine framework to better understand the relationship between neuropsychological testing and the progression of symptoms in a blast-induced mild Traumatic Brain Injury (mTBI) patient population. In this paper, we extended our earlier work on Constrained Spectral Partitioning (CSP), a graph-based approach that incorporates additional information from separate graphs to help improve the clustering quality on both graphs simultaneously. While our previous work demonstrated the effectiveness of this algorithm in its ability to accurately classify whether symptoms improved or declined over time, that work did not provide any insights into the progression of symptoms. Therefore, this paper sought to identify, from a clinical perspective, whether symptoms increased/decreased over time. To accomplish this, we developed a decision tree classifier to classify symptom progression based on the outputs from our CSP algorithm. We present results from four separate decision tree classifiers that illustrate the adaptability of these algorithms for utilization as decision rules for the treatment of patients following blast-induced mTBI. Decision tree classifier models are useful in the healthcare setting because patient health data (e.g., diagnosis of a condition or a type of treatment) can be imput into the model and, based on the health data variables, a resulting outcome can be suggested, and providers can use this outcome as information to direct their clinical treatment. © 2017, © The Author(s) 2017.","Biomedical informatics; classification algorithms; clustering algorithms; graph theory","Bioinformatics; Brain; Classification (of information); Data mining; Decision trees; Diagnosis; Graph theory; Graphic methods; Health care; Medical computing; Patient treatment; Trees (mathematics); Biomedical informatics; Classification algorithm; Clinical treatments; Decision tree classifiers; Mild traumatic brain injuries; Neuropsychological; Personalized medicines; Spectral partitioning; Clustering algorithms",2-s2.0-85031121151
"Fu L., Liu K., Sun M., Tian C., Sun R., Betanzos C.M., Tallman K.A., Porter N.A., Yang Y., Guo D., Liebler D.C., Yang J.","Systematic and quantitative assessment of hydrogen peroxide reactivity with cysteines across human proteomes",2017,"Molecular and Cellular Proteomics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030309698&doi=10.1074%2fmcp.RA117.000108&partnerID=40&md5=040d6dd328c73cc7dc88d1dd2ea47133","Protein cysteinyl residues are the mediators of hydrogen peroxide (H2O2)-dependent redox signaling. However, site-specific mapping of the selectivity and dynamics of these redox reactions in cells poses a major analytical challenge. Here we describe a chemoproteomic platform to systematically and quantitatively analyze the reactivity of thousands of cysteines toward H2O2 in human cells. We identified &gt;900 H2O2-sensitive cysteines, which are defined as the H2O2-dependent redoxome. Although redox sites associated with antioxidative and metabolic functions are consistent, most of the H2O2-dependent redoxome varies dramatically between different cells. Structural analyses reveal that H2O2-sensitive cysteines are less conserved than their redox-insensitive counterparts and display distinct sequence motifs, structural features, and potential for crosstalk with lysine modifications. Notably, our chemoproteomic platform also provides an opportunity to predict oxidation-triggered protein conformational changes. The data are freely accessible as a resource at http://redox.ncpsb.org/OXID/. © 2017 by The American Society for Biochemistry and Molecular Biology, Inc.",,"ATM protein; cysteine; E1A associated p300 protein; glyceraldehyde 3 phosphate dehydrogenase; histone deacetylase 1; histone deacetylase 3; hydrogen peroxide; mitogen activated protein kinase p38; peroxiredoxin 1; peroxiredoxin 2; peroxiredoxin 3; peroxiredoxin 4; peroxiredoxin 5; peroxiredoxin 6; protein deglycase DJ-1; protein tyrosine phosphatase 1B; proteome; thioredoxin 2; A-431 cell line; Article; bioinformatics; cell specificity; chemoproteomics; conformational transition; controlled study; data mining; disulfide bond; DNA flanking region; HEK293 cell line; Hep-G2 cell line; human; human cell; large scale production; limit of quantitation; liquid chromatography-mass spectrometry; oxidation; oxidation reduction reaction; oxidative stress; parallel reaction monitoring; priority journal; protein analysis; protein conformation; protein structure; proteomics; quantitative analysis; quantitative thiol reactivity profiling; redox stress; structure analysis; tandem mass spectrometry; U2OS cell line",2-s2.0-85030309698
"Shao Y., Lei K., Chen L., Huang Z., Cui B., Liu Z., Tong Y., Xu J.","Fast parallel path concatenation for graph extraction",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021805254&doi=10.1109%2fTKDE.2017.2716939&partnerID=40&md5=757591e1f8251327147cb6d18431ee49","Heterogeneous graph is a popular data model to represent the real-world relations with abundant semantics. To analyze heterogeneous graphs, an important step is extracting homogeneous graphs from the heterogeneous graphs, called homogeneous graph extraction. In an extracted homogeneous graph, the relation is defined by a line pattern on the heterogeneous graph and the new attribute values of the relation are calculated by user-defined aggregate functions. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel graph extraction framework, where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan, which determines the order of concatenating paths and generates the final paths in a divide-and-conquer manner. We introduce a cost model to estimate the cost of a plan and discuss three plan selection strategies, among which the best plan can enumerate paths in O(log(l)) iterations, where l is the length of a pattern. Furthermore, to improve the performance of evaluating aggregate functions, we classify the aggregate functions into three categories, i.e., distributive aggregation, algebraic aggregation, and holistic aggregation. Since the distributive and algebraic aggregations can be computed from the partial paths, we speed up the aggregation by computing partial aggregate values during the path enumeration. © 1989-2012 IEEE.","Graph extraction; Heterogenegous graph; Parallelisim; Path concatenation","Aggregates; Algebra; Cost benefit analysis; Data mining; Electronic mail; Extraction; Feature extraction; Pattern matching; Query processing; Semantics; Computational model; Graph extractions; Heterogenegous Graph; Parallelisim; Path Concatenation; Graph theory",2-s2.0-85021805254
"Venkatraman V., Alsberg B.K.","Predicting CO2 capture of ionic liquids using machine learning",2017,"Journal of CO2 Utilization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025091455&doi=10.1016%2fj.jcou.2017.06.012&partnerID=40&md5=03dee3007779545889068930074bac26","Ionic liquid (IL) based CO2 capture is currently seen as a promising alternative to conventional amine-based solvents. While the possible combinations of cations and anions are numerous, it is time consuming and expensive to carry out experimental measurements for CO2 solubilities for each new IL. Therefore, as a means to rapidly screen suitable ILs as potential solvents for CO2 absorption, we investigate the use of machine learning (ML) based models to establish structure-property relationships between molecular structures of cations and anions and their CO2 solubilities. Over 10,000 IL-CO2 solubility data of 185 ILs measured at different operating temperatures and pressures were extracted from the literature. Using semi-empirically derived geometrical and charge-based molecular descriptors, good agreement with the available experimental measurements was obtained for both single decision tree (mean absolute error of 0.10) and ensemble random forest (mean absolute error of 0.04) approaches. The results were found to be more accurate than those obtained with the quantum chemistry based COSMOtherm predictions. © 2017 Elsevier Ltd.","CO2 capture; Ionic liquids; Machine learning; QSPR","Artificial intelligence; Data mining; Decision trees; Education; Ionic liquids; Learning systems; Liquids; Positive ions; Quantum chemistry; Solubility; CO2 absorption; Mean absolute error; Molecular descriptors; Operating temperature; QSPR; Random forests; Single decision; Structure property relationships; Carbon dioxide",2-s2.0-85025091455
"Li Z., Yuan X., Cui X., Liu X., Wang L., Zhang W., Lu Q., Zhu H.","Optimal experimental conditions for Welan gum production by support vector regression and adaptive genetic algorithm",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031031201&doi=10.1371%2fjournal.pone.0185942&partnerID=40&md5=762bfcbafb6e1bb142802cd2eb5b8925","Welan gum is a kind of novel microbial polysaccharide, which is widely produced during the process of microbial growth and metabolism in different external conditions. Welan gum can be used as the thickener, suspending agent, emulsifier, stabilizer, lubricant, film-forming agent and adhesive usage in agriculture. In recent years, finding optimal experimental conditions to maximize the production is paid growing attentions. In this work, a hybrid computational method is proposed to optimize experimental conditions for producing Welan gum with data collected from experiments records. Support Vector Regression (SVR) is used to model the relationship between Welan gum production and experimental conditions, and then adaptive Genetic Algorithm (AGA, for short) is applied to search optimized experimental conditions. As results, a mathematic model of predicting production of Welan gum from experimental conditions is obtained, which achieves accuracy rate 88.36%. As well, a class of optimized experimental conditions is predicted for producing Welan gum 31.65g/L. Comparing the best result in chemical experiment 30.63g/L, the predicted production improves it by 3.3%. The results provide potential optimal experimental conditions to improve the production of Welan gum. © 2017 Li et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"genetic algorithm; support vector machine; agriculture; algorithm; biosynthesis; carbohydrate diet; chemistry; cloud computing; data mining; support vector machine; bacterial polysaccharide; carbohydrate diet; emulsifying agent; excipient; glucose; lubricating agent; polysaccharide; welan; Agriculture; Algorithms; Cloud Computing; Data Mining; Dietary Carbohydrates; Emulsifying Agents; Excipients; Glucose; Lubricants; Polysaccharides; Polysaccharides, Bacterial; Support Vector Machine",2-s2.0-85031031201
"Funk M., Van Diggelen M.","Feedback conversations: Creating feedback dialogues with a new textual tool for industrial design student feedback",2017,"International Journal of Web-Based Learning and Teaching Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028014917&doi=10.4018%2fIJWLTT.2017100107&partnerID=40&md5=4326863c85a08e78d03a9083a2f686b3","In this paper, the authors describe how a study of a large database of written university teacher feedback in the department of Industrial Design led to the development of a new conceptual framework for feedback and the design of a new feedback tool. This paper focuses on the translation of related work in the area of feedback mechanisms for higher education into a tailored framework for feedback in the area of Industrial Design, the translation of the existing corpus of data into indicators of feedback quality and how feedback is received and further on used by students in their learning process. The newly developed tool structures teacher feedback into very targeted and highly focused feedback dialogues between teacher and individual students tailored to their individual learning process. The tool is described in this paper with respect to conversational aspects. In the future, the tool will be used actively in Industrial Design education, also with the purpose of further investigating how the quality of written feedback evolves and redesigning educational processes around feedback tools. © 2017, IGI Global.","Educational Data Mining; Feedback; Industrial Design; Learning Analytics; Teaching Quality",,2-s2.0-85028014917
"Jończyk M., Sobkowiak A., Trzcinska-Danielewicz J., Skoneczny M., Solecka D., Fronk J., Sowiński P.","Global analysis of gene expression in maize leaves treated with low temperature. II. Combined effect of severe cold (8 °C) and circadian rhythm",2017,"Plant Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027999737&doi=10.1007%2fs11103-017-0651-3&partnerID=40&md5=812b2325741352c142ec434212ddc0ef","Key message: In maize seedlings, severe cold results in dysregulation of circadian pattern of gene expression causing profound modulation of transcription of genes related to photosynthesis and other key biological processes. Abstract: Plants live highly cyclic life and their response to environmental stresses must allow for underlying biological rhythms. To study the interplay of a stress and a rhythmic cue we investigated transcriptomic response of maize seedlings to low temperature in the context of diurnal gene expression. Severe cold stress had pronounced effect on the circadian rhythm of a substantial proportion of genes. Their response was strikingly dual, comprising either flattening (partial or complete) of the diel amplitude or delay of expression maximum/minimum by several hours. Genes encoding central oscillator components behaved in the same dual manner, unlike their Arabidopsis counterparts reported earlier to cease cycling altogether upon cold treatment. Also numerous genes lacking circadian rhythm responded to the cold by undergoing up- or down-regulation. Notably, the transcriptome changes preceded major physiological manifestations of cold stress. In silico analysis of metabolic processes likely affected by observed gene expression changes indicated major down-regulation of photosynthesis, profound and multifarious modulation of plant hormone levels, and of chromatin structure, transcription, and translation. A role of trehalose and stachyose in cold stress signaling was also suggested. Meta-analysis of published transcriptomic data allowed discrimination between general stress response of maize and that unique to severe cold. Several cis- and trans-factors likely involved in the latter were predicted, albeit none of them seemed to have a major role. These results underscore a key role of modulation of diel gene expression in maize response to severe cold and the unique character of the cold-response of the maize circadian clock. © 2017, The Author(s).","Central oscillator; Cold; Data mining; Microarrays; Pathway analysis; Transcription regulation; Zea mays","adaptation; circadian rhythm; cluster analysis; cold; gene expression profiling; gene expression regulation; genetics; maize; physiological stress; plant gene; plant leaf; procedures; reverse transcription polymerase chain reaction; seedling; Adaptation, Physiological; Circadian Rhythm; Cluster Analysis; Cold Temperature; Gene Expression Profiling; Gene Expression Regulation, Plant; Genes, Plant; Plant Leaves; Reverse Transcriptase Polymerase Chain Reaction; Seedlings; Stress, Physiological; Zea mays",2-s2.0-85027999737
"Seo K.-M., Hong W., Kim T.G.","Enhancing model composability and reusability for entity-level combat simulation: A conceptual modeling approach",2017,"Simulation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029482022&doi=10.1177%2f0037549717699872&partnerID=40&md5=1a8b62378856ddf5b59c2efbf633e512","This paper presents a conceptual model design for entity-level combat simulation to enhance model composability and reusability. For conceptual modeling, we first describe the following three problem situations: (1) joint design of logical/physical modeling; (2) flexible model modification during scenario extension; and (3) cooperative modeling with different domain experts. To this end, we propose a two-dimensional model partition method for a combat entity, which partitions the combat entity depending on the functional aspect horizontally and the abstract level vertically. Thus, the proposed method guarantees transparent simplification of the combat entity and facilitates flexible model composition when simulating in the integration and the interoperation environments. Based on the proposed conceptual modeling, empirical measurements demonstrate the enhancement of model composability and reusability during scenario extension for anti-submarine warfare from static decoy to mobile decoy and from pattern-running torpedo to wire-guided torpedo simulations. © The Author(s) 2017.","integration; interoperation; Model composability; model reusability; system of systems; underwater warfare","Data mining; Integration; Military applications; System of systems; Torpedoes; Anti-submarine warfares; Empirical measurement; Functional aspects; Interoperations; Model composability; Model reusability; Two dimensional model; Underwater warfares; Reusability",2-s2.0-85029482022
"Domínguez J.L., Mata J.","Obtaining Significant and Interpretable Rules for Subgroup Discovery Tasks",2017,"IEEE Latin America Transactions",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032629718&doi=10.1109%2fTLA.2017.8071249&partnerID=40&md5=07e26c9ee85babc8b3afba5d6b8fd1b6","In this work, a deterministic method to obtain rules for subgroup discovery tasks is described. It does not employ a previous discretization for the numeric attributes, but obtains conditions for the rules dynamically. To select the final rules, the equivalent sum of sensitivity and specificity has been used. An experimental study and appropriate statistical tests were performed, in a comparison with the classic deterministic algorithms APRIORI-SD and CN2-SD. The proposed method behaved very well, obtaining the best results in six of the eight pair-wise comparisons. From the results obtained, the described approach can be seen as a suitable deterministic way of extracting rules for subgroup discovery problems. © 2003-2012 IEEE.","data mining; deterministic algorithms; subgroup discovery","Electrical engineering; Electronics engineering; Deterministic algorithms; Deterministic methods; Extracting rules; Interpretable rules; Pair-wise comparison; Sensitivity and specificity; Subgroup discovery; Subgroup discovery tasks; Data mining",2-s2.0-85032629718
"Takehara D., Harakawa R., Ogawa T., Haseyama M.","Extracting hierarchical structure of content groups from different social media platforms using multiple social metadata",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018372966&doi=10.1007%2fs11042-017-4717-7&partnerID=40&md5=0ac8186dbd64babaec6aec032950afa1","A novel scheme for retrieving users’ desired contents, i.e., contents with topics in which users are interested, from multiple social media platforms is presented in this paper. In existing retrieval schemes, users first select a particular platform and then input a query into the search engine. If users do not specify suitable platforms for their information needs and do not input suitable queries corresponding to the desired contents, it becomes difficult for users to retrieve the desired contents. The proposed scheme extracts the hierarchical structure of content groups (sets of contents with similar topics) from different social media platforms, and it thus becomes feasible to retrieve desired contents even if users do not specify suitable platforms and do not input suitable queries. This paper has two contributions: (1) A new feature extraction method, Locality Preserving Canonical Correlation Analysis with multiple social metadata (LPCCA-MSM) that can detect content groups without the boundaries of different social media platforms is presented in this paper. LPCCA-MSM uses multiple social metadata as auxiliary information unlike conventional methods that only use content-based information such as textual or visual features. (2) The proposed novel retrieval scheme can realize hierarchical content structuralization from different social media platforms. The extracted hierarchical structure shows various abstraction levels of content groups and their hierarchical relationships, which can help users select topics related to the input query. To the best of our knowledge, an intensive study on such an application has not been conducted; therefore, this paper has strong novelty. To verify the effectiveness of the above contributions, extensive experiments for real-world datasets containing YouTube videos and Wikipedia articles were conducted. © 2017, Springer Science+Business Media New York.","Cross-platform application; Hierarchical structure; Social media platform; Wikipedia; YouTube","Data mining; Feature extraction; Metadata; Search engines; Websites; Cross platform applications; Hierarchical structures; Social media platforms; Wikipedia; YouTube; Social networking (online)",2-s2.0-85018372966
"Creton B.","Chemoinformatics at IFP Energies Nouvelles: Applications in the Fields of Energy, Transport, and Environment",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018577078&doi=10.1002%2fminf.201700028&partnerID=40&md5=8ce542ee0146ddd414bcd89aac6c993c","The objective of the present paper is to summarize chemoinformatics based research, and more precisely, the development of quantitative structure property relationships performed at IFP Energies nouvelles (IFPEN) during the last decade. A special focus is proposed on research activities performed in the “Thermodynamics and Molecular Simulation” department, i. e. the use of multiscale molecular simulation methods in responses to projects. Molecular simulation techniques can be envisaged to supplement dataset when experimental information lacks, thus the review includes a section dedicated to molecular simulation codes, development of intermolecular potentials, and some of their possible applications. Know-how and feedback from our experiences in terms of machine learning application for thermophysical property predictions are included in a section dealing with methodological aspects. The generic character of chemoinformatics is emphasized through applications in the fields of energy, transport, and environment, with illustrations for three IFPEN business units: “Transports”, “Energy Resources”, and “Processes”. More precisely, the review focus on different challenges such as the prediction of properties for alternative fuels, the prediction of fuel compatibility with polymeric materials, the prediction of properties for surfactants usable in chemical enhanced oil recovery, and the prediction of guest-host interactions between gases and nanoporous materials in the frame of carbon dioxide capture or gas separation activities. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","Data mining; Fluids; Machine learning; Nanoporous materials; Property","biofuel; carbon dioxide; polymer; surfactant; carbon footprint; chemoinformatics; energy resource; environment; feedback system; information processing; machine learning; mathematical model; molecular dynamics; oil field; physical chemistry; priority journal; quantitative structure property relation; Review; thermodynamics",2-s2.0-85018577078
"Joan S.P.F., Valli S.","An enhanced text detection technique for the visually impaired to read text",2017,"Information Systems Frontiers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988632647&doi=10.1007%2fs10796-016-9699-x&partnerID=40&md5=97060f5b40c2fb479c5b323b148bc2b1","An enhanced text detection technique (ETDT) is proposed, which is expected to aid the visually impaired to overcome their reading challenges. This work enhances the edge-preserving maximally stable extremal regions (eMSER) algorithm using the pyramid histogram of oriented gradients (PHOG). Histogram of oriented gradients (HOG) derived from different pyramid levels is important while detecting maximally stable extremal regions (MSER) in the ETDT approach because it gives more spatial information when compared to HOG information from a single level. To group text, a four-line, text-grouping method is newly designed for this work. Also, a new text feature, Shapeness Score is proposed, which significantly identifies text regions when combined with the other features based on morphology and stroke widths. Using the feature vector of dimension 10, the J48 decision tree and AdaBoost machine learning algorithms identify the text regions in the images. The algorithm yields better results than the existing benchmark algorithms for the ICDAR 2011 born-digital dataset and must be improved with respect to the scene text dataset. © 2016, Springer Science+Business Media New York.","MSER; PHOG; Shapeness score; Stroke width; Text detection","Adaptive boosting; Artificial intelligence; Data mining; Decision trees; Graphic methods; Learning algorithms; Learning systems; MSER; PHOG; Shapeness score; Stroke widths; Text detection; Character recognition",2-s2.0-84988632647
"Sharafi A., Sanaye-Pasand M., Jafarian P.","Improvement of distance relay zone-3 security using fault and breaker opening generated traveling waves",2017,"International Transactions on Electrical Energy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028667289&doi=10.1002%2fetep.2414&partnerID=40&md5=c2cb54b4364da9b349cb5815ecf9c592","This paper presents a new algorithm for discrimination of short-circuit faults from power system stressed conditions. The proposed algorithm is based on the combination of distance relay zone-3 operation and traveling waves generated by either the fault or circuit-breaker switching. By pick-up of the distance relay zone-3, the algorithm looks for traveling waves arriving from the fault point in the first stage. If a fault generated traveling wave is detected, it would be concluded that a fault has occurred. Otherwise, the algorithm enters the next stage and waits for arriving traveling waves caused by open-switching of the faulted line remote circuit-breaker. If any traveling wave is detected, the system is under fault condition. Otherwise, the reason of zone-3 pick-up is due to a stressed condition. Training a decision tree-based classifier with the proposed features and simulating the method demonstrate that the proposed algorithm is able to provide a reliable remote back-up scheme for protection of transmission lines. Copyright © 2017 John Wiley & Sons, Ltd.","back-up protection; circuit-breaker switching; decision tree; stressed condition; traveling wave","Decision trees; Electric circuit breakers; Electric relays; Pickups; Trees (mathematics); Back-up protections; Distance relay; Fault conditions; Fault point; Remote circuit breaker; Short-circuit fault; Stressed condition; Traveling wave; Data mining",2-s2.0-85028667289
"Rabatel J., Fannes T., Lepailleur A., Le Goff J., Crémilleux B., Ramon J., Bureau R., Cuissart B.","Non a Priori Automatic Discovery of 3D Chemical Patterns: Application to Mutagenicity",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020381417&doi=10.1002%2fminf.201700022&partnerID=40&md5=dd987527f2c1f4fd69c17eaeac9e1638","This article introduces a new type of structural fragment called a geometrical pattern. Such geometrical patterns are defined as molecular graphs that include a labelling of atoms together with constraints on interatomic distances. The discovery of geometrical patterns in a chemical dataset relies on the induction of multiple decision trees combined in random forests. Each computational step corresponds to a refinement of a preceding set of constraints, extending a previous geometrical pattern. This paper focuses on the mutagenicity of chemicals via the definition of structural alerts in relation with these geometrical patterns. It follows an experimental assessment of the main geometrical patterns to show how they can efficiently originate the definition of a chemical feature related to a chemical function or a chemical property. Geometrical patterns have provided a valuable and innovative approach to bring new pieces of information for discovering and assessing structural characteristics in relation to a particular biological phenotype. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","Geometrical pattern; Random Forest; Structural Alerts; Structure-activity relationships; Toxicology","mutagenic agent; Article; atom; automation; calculation; chemical structure; classifier; computer model; conformation; data mining; decision tree; geometry; heuristics; hybridization; hydrogen bond; k nearest neighbor; lipophilicity; metabolic activation; mutagenicity; prediction; priority journal; random forest; structural model; structure activity relation",2-s2.0-85020381417
"Lessmann S., Voß S.","Car resale price forecasting: The impact of regression method, private information, and heterogeneity on forecast accuracy",2017,"International Journal of Forecasting",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025697588&doi=10.1016%2fj.ijforecast.2017.04.003&partnerID=40&md5=321b17465834fcad7482ab9fd0bf9d4a","The paper investigates statistical models for forecasting the resale prices of used cars. An empirical study is performed to explore the contributions of different degrees of freedom in the modeling process to the forecast accuracy. First, a comparative analysis of alternative prediction methods provides evidence that random forest regression is particularly effective for resale price forecasting. It is also shown that the use of linear regression, the prevailing method in previous work, should be avoided. Second, the empirical results demonstrate the presence of heterogeneity in resale price forecasting and identify methods that can automatically overcome its detrimental effect on the forecast accuracy. Finally, the study confirms that the sellers of used cars possess informational advantages over market research agencies, which enable them to forecast resale prices more accurately. This implies that sellers have an incentive to invest in in-house forecasting solutions, instead of basing their pricing decisions on externally generated residual value estimates. © 2017 International Institute of Forecasters","Automotive industry; Data mining; Decision support systems; Forecasting",,2-s2.0-85025697588
"Fazzolari M., Cozza V., Petrocchi M., Spognardi A.","A Study on Text-Score Disagreement in Online Reviews",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026501769&doi=10.1007%2fs12559-017-9496-y&partnerID=40&md5=e59e090854e197e0cf9d52d61659282a","In this paper, we focus on online reviews and employ artificial intelligence tools, taken from the cognitive computing field, to help understand the relationships between the textual part of the review and the assigned numerical score. We move from the intuitions that (1) a set of textual reviews expressing different sentiments may feature the same score (and vice-versa), and (2) detecting and analyzing the mismatches between the review content and the actual score may benefit both service providers and consumers, by highlighting specific factors of satisfaction (and dissatisfaction) in texts. To prove the intuitions, we adopt sentiment analysis techniques and we concentrate on hotel reviews, to find polarity mismatches therein. In particular, we first train a text classifier with a set of annotated hotel reviews, taken from the Booking website. Then, we analyze a large dataset, with around 160k hotel reviews collected from TripAdvisor, with the aim of detecting a polarity mismatch, indicating if the textual content of the review is in line, or not, with the associated score. Using well-established artificial intelligence techniques and analyzing in depth the reviews featuring a mismatch between the text polarity and the score, we find that—on a scale of five stars—those reviews ranked with middle scores include a mixture of positive and negative aspects. The approach proposed here, beside acting as a polarity detector, provides an effective selection of reviews—on an initial very large dataset—that may allow both consumers and providers to focus directly on the review subset featuring a text/score disagreement,which conveniently convey to the user a summary of positive and negative features of the review target. © 2017, Springer Science+Business Media, LLC.","Artificial intelligence; Data mining; Natural language processing; Online reviews; Polarity detection; Social science methods or tools","Artificial intelligence; Classification (of information); Data mining; Hotels; Natural language processing systems; Artificial intelligence techniques; Artificial intelligence tools; Cognitive Computing; Online reviews; Science methods; Sentiment analysis; Service provider; Text classifiers; Feature extraction",2-s2.0-85026501769
"Liu Y., Wang X., Liu D., Liu L.","An adaptive dual clustering algorithm based on hierarchical structure: A case study of settlement zoning",2017,"Transactions in GIS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006049856&doi=10.1111%2ftgis.12246&partnerID=40&md5=40a49958275fdbaf8fbc2ec823ae48c4","Traditional dual clustering algorithms cannot adaptively perform clustering well without sufficient prior knowledge of the dataset. This article aims at accommodating both spatial and non-spatial attributes in detecting clusters without the need to set parameters by default or prior knowledge. A novel adaptive dual clustering algorithm (ADC+) is proposed to obtain satisfactory clustering results considering the spatial proximity and attribute similarity with the presence of noise and barriers. In this algorithm, Delaunay triangulation is utilized to adaptively obtain spatial proximity and spatial homogenous patterns based on particle swarm optimization (PSO). Then, a hierarchical clustering method is employed to obtain clusters with similar attributes. The hierarchical clustering method adopts a discriminating coefficient to adaptively control the depth of the hierarchical architecture. The clustering results are further refined using an optimization approach. The advantages and practicability of the ADC+ algorithm are illustrated by experiments on both simulated datasets and real-world applications. It is found that the proposed ADC+ algorithm can adaptively and accurately detect clusters with arbitrary shapes, similar attributes and densities under the consideration of barriers. © 2016 John Wiley & Sons Ltd","adaptive dual clustering; data mining; Delaunay triangulation; hierarchical structure; rural settlement zoning","algorithm; data mining; hierarchical system; optimization; rural area; settlement pattern; triangulation",2-s2.0-85006049856
"Štajner S., Glavaš G.","Leveraging event-based semantics for automated text simplification",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017614045&doi=10.1016%2fj.eswa.2017.04.005&partnerID=40&md5=503409ea8542f2891ec2c213136ac9e8","Automated Text Simplification (ATS) aims to transform complex texts into their simpler variants which are easier to understand to wider audiences and easier to process with natural language processing (NLP) tools. While simplification can be applied on lexical, syntactic, and discourse level, all previously proposed ATS systems only operated on the first two levels, thus failing at simplifying texts on the discourse level. We present a semantically-motivated ATS system which is the first system that is applied on the discourse level. By exploiting the state-of-the-art event extraction system, it is the first ATS system able to eliminate large portions of irrelevant information from texts, by maintaining only those parts of the original text that belong to factual event mentions. A few handcrafted rules ensure that the output of the system is syntactically simple, by placing each factual event mention in a separate short sentence, while the state-of-the-art unsupervised lexical simplification module, based on using word embeddings, replaces complex and infrequent words with their simpler variants. We perform a thorough evaluation, both automatic and manual, showing that our system produces more readable and simpler texts than the state-of-the-art ATS systems. Our newly proposed post-editing evaluation further reveals that our system requires less human effort for correcting grammaticality and meaning preservation on news articles than the state-of-the-art ATS system. © 2017 Elsevier Ltd","Automated text simplification; Event extraction; Semantics","Data mining; Extraction; Information analysis; Natural language processing systems; Semantics; Automated text simplification; Event extraction; Event-based semantics; First systems; Handcrafted rules; NAtural language processing; News articles; State of the art; Automation",2-s2.0-85017614045
"Costello K.L., Martin J.D., III, Edwards Brinegar A.","Online disclosure of illicit information: Information behaviors in two drug forums",2017,"Journal of the Association for Information Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029713957&doi=10.1002%2fasi.23880&partnerID=40&md5=e3218b82be9252fb19b790889b82548f","Although people disclose illicit activities such as drug use online, we currently know little about what information people choose to disclose and share or whether there are differences in behavior depending on the illicit activity being disclosed. This exploratory mixed-methods study examines how people discuss and disclose the use of two different drugs—marijuana and opioids—on Reddit. In this study, hermeneutic content analysis is employed to describe the type of comments people make in forums dedicated to discussions about illicit drugs. With inductive analysis, seven categories of comments were identified: disclosure, instruction and advice, culture, community norms, moralizing, legality, and banter. Our subsequent quantitative analysis indicates that although the amounts of disclosure are similar in each subreddit, there are more instances of instruction and advice in discussions about opiates, and more examples of banter in comments about marijuana use. In fact, both subreddits have high rates of banter. We argue that banter fosters disclosure in both subreddits, and that banter and disclosure are linked with information-seeking behaviors in online forums. This work has implications for future explorations of disclosure online and for public health interventions aimed at disseminating credible information about drug use to at-risk individuals. © 2017 ASIS&T",,"Health risks; Information retrieval; Community norms; Content analysis; Health interventions; Illicit drug; Implications for futures; Information behavior; Information seeking behaviors; Online forums; Data mining",2-s2.0-85029713957
"Wang H., Forte D., Tehranipoor M.M., Shi Q.","Probing Attacks on Integrated Circuits: Challenges and Research Opportunities",2017,"IEEE Design and Test",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028911644&doi=10.1109%2fMDAT.2017.2729398&partnerID=40&md5=adc22779b845f927f916c59f54fe2ed0",[No abstract available],"Hardware Security; Integrated Circuits; Microprobing; Physical Attacks","Computer aided design; Data mining; Hardware security; Optical sensors; Photonics; Stimulated emission; Timing circuits; Transistors; Wire; Critical challenges; Integrated circuits (ICs); Microprobing; Physical attacks; Research opportunities; Security-critical; Sensitive informations; Integrated circuits",2-s2.0-85028911644
"Wu F., Duan X., Xiao J., Zhao Z., Tang S., Zhang Y., Zhuang Y.","Temporal interaction and causal influence in community-based question answering",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021933453&doi=10.1109%2fTKDE.2017.2720737&partnerID=40&md5=6d1b970ee827589579518390f767e3eb","During the last decade, community-based question answering (CQA) sites have accumulated a vast amount of questions and their crowdsourced answers over time. How to efficiently identify the quality of answers that are relevant to a given question has become an active line of research in CQA. The major challenge of CQA is the accurate selection of high-quality answers w.r.t given questions. Previous approaches tend to model the semantic matching between individual pair of one question and its corresponding answer (how fitting an answer is to a posted question). However, these works ignore the temporal interactions between answers (how previous answers influence the late posted answers). For example, a rational user likely adapts others' opinions, revises his inclinations, and posts a more appropriate answer after understanding the given question and previously posted answers. As a result, this paper devises an architecture named Temporal Interaction and Causal Influence LSTM (TC-LSTM) to effectively leverage not only the causal influence between question-answer (how appropriate an answer is for a given question) but also the temporal interactions between answers-answer (how a high-quality answer gradually forms). In particular, long short-term memory (LSTM) is used to capture the explicit question-answer influence and the implicit answers-answer interactions. Experiments are conducted on SemEval 2015 CQA dataset for answer classification task and Baidu Zhidao Dataset for answer ranking task. The experimental results show the advantage of our model comparing with other state-of-the-art methods. © 1989-2012 IEEE.","Causal influence; Community-based question answering; Long short-term memory; Temporal interaction","Brain; Classification (of information); Computer architecture; Data mining; Memory architecture; Microprocessor chips; Network architecture; Recurrent neural networks; Semantics; Adaptation models; Answer classification; Causal influences; Community-based question answering; Novel architecture; Semantic evaluations; State-of-the-art methods; temporal interaction; Long short-term memory",2-s2.0-85021933453
"Peimankar A., Weddell S.J., Jalal T., Lapthorn A.C.","Evolutionary multi-objective fault diagnosis of power transformers",2017,"Swarm and Evolutionary Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018787152&doi=10.1016%2fj.swevo.2017.03.005&partnerID=40&md5=decf9044e378cfd692a1c332590e5ae2","This paper introduces a two step algorithm for fault diagnosis of power transformers (2-ADOPT) using a binary version of the multi-objective particle swarm optimization (MOPSO) algorithm. Feature subset selection and ensemble classifier selection are implemented to improve the diagnosing accuracy for dissolved gas analysis (DGA) of power transformers. First, the proposed method selects the most effective features in a multi objective framework and the optimum number of features, simultaneously, which are used as inputs to train classifiers in the next step. The input features are composed of DGA performed on the oil of power transformers along with the various ratios of these gases. In the second step, the most accurate and diverse classifiers are selected to create a classifier ensemble. Finally, the outputs of selected classifiers are combined using the Dempster-Shafer combination rule in order to determine the actual faults of power transformers. In addition, the obtained results of the proposed method are compared to three other scenarios: 1) multi-objective ensemble classifier selection without any feature selection step which takes all the features to train classifiers and then applies MOPSO algorithm to find the best ensemble of classifiers, 2) a well-known classifier ensemble technique called random forests, and 3) another powerful decision tree ensemble which is called oblique random forests. The comparison results were favourable to the proposed method and showed the high reliability of this method for power transformers fault classification. © 2017 The Authors","Dissolved gas analysis; Ensemble classifiers; Fault diagnosis; Feature selection; Multi-objective optimization; Power transformers","Classification (of information); Data mining; Decision trees; Failure analysis; Feature extraction; Multiobjective optimization; Optimization; Particle swarm optimization (PSO); Power transformers; Dempster-Shafer combination rules; Dissolved gas analyses (DGA); Dissolved gas analysis; Ensemble classifiers; Ensemble of classifiers; Evolutionary Multi-objectives; Feature subset selection; Multi objective particle swarm optimization; Fault detection",2-s2.0-85018787152
"Kadumuri R.V., Vadrevu R.","LoopX: A Graphical User Interface-Based Database for Comprehensive Analysis and Comparative Evaluation of Loops from Protein Structures",2017,"Journal of Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031098250&doi=10.1089%2fcmb.2016.0197&partnerID=40&md5=3470fb68720ad0f720b7ae456cc26510","Due to their crucial role in function, folding, and stability, protein loops are being targeted for grafting/designing to create novel or alter existing functionality and improve stability and foldability. With a view to facilitate a thorough analysis and effectual search options for extracting and comparing loops for sequence and structural compatibility, we developed, LoopX a comprehensively compiled library of sequence and conformational features of ∼700,000 loops from protein structures. The database equipped with a graphical user interface is empowered with diverse query tools and search algorithms, with various rendering options to visualize the sequence- and structural-level information along with hydrogen bonding patterns, backbone φ, ψ dihedral angles of both the target and candidate loops. Two new features (i) conservation of the polar/nonpolar environment and (ii) conservation of sequence and conformation of specific residues within the loops have also been incorporated in the search and retrieval of compatible loops for a chosen target loop. Thus, the LoopX server not only serves as a database and visualization tool for sequence and structural analysis of protein loops but also aids in extracting and comparing candidate loops for a given target loop based on user-defined search options. © Copyright 2017, Mary Ann Liebert, Inc. 2017.","data mining; database; loop engineering/grafting; protein loops; web-based GUI.",,2-s2.0-85031098250
"Boscarino J.A., Moorman A.C., Rupp L.B., Zhou Y., Lu M., Teshale E.H., Gordon S.C., Spradling P.R., Schmidt M.A., Trinacty C.M., Zhong Y., Holmberg S.D., Holtzman D., For the Chronic Hepatitis Cohort Study (CheCS) Investigators, Holmberg S.D., Teshale E.H., Spradling P.R., Moorman A.C., Xing J., Zhong Y., Gordon S.C., Nerenz D.R., Lu M., Lamerato L., Li J., Rupp L.B., Akkerman N., Oja-Tebbe N., Zhang T., Trudeau S., Zhou Y., Boscarino J.A., Daar Z.S., Smith R.E., Daida Y.G., Trinacty C.M., Wong C.P., Schmidt M.A., Donald J.L., Keast E.M.","Comparison of ICD-9 Codes for Depression and Alcohol Misuse to Survey Instruments Suggests These Codes Should Be Used with Caution",2017,"Digestive Diseases and Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028830654&doi=10.1007%2fs10620-017-4714-8&partnerID=40&md5=6e2ad4f87904a12353f14cf07cdc000c","Background: Research suggests depression and alcohol misuse are highly prevalent among chronic hepatitis C (CHC) patients, which is of clinical concern. Aims: To compare ICD-9 codes for depression and alcohol misuse to validated survey instruments. Methods: Among CHC patients, we assessed how well electronic ICD-9 codes for depression and alcohol misuse predicted these disorders using validated instruments. Results: Of 4874 patients surveyed, 56% were male and 52% had a history of injection drug use. Based on the PHQ-8, the prevalence of depression was 30% compared to 14% based on ICD-9 codes within 12 months of survey, 37% from ICD-9 codes any time before or within 12 months after survey, and 48% from ICD-9 codes any time before or within 24 months after survey. ICD-9 codes predicting PHQ-8 depression had a sensitivity ranging from 59 to 88% and a specificity ranging from 33 to 65%. Based on the AUDIT-C, the prevalence of alcohol misuse was 21% compared to 3–23% using ICD-9 codes. The sensitivity of ICD-9 codes to predict AUDIT-C score ranged from 9 to 35% and specificity from 80 to 98%. Overall 39% of patients reported ever binge drinking, with a sensitivity of ICD-9 to predict binge drinking ranging from 7 to 33% and a specificity from 84 to 98%. More than half of patients had either an ICD-9 code for depression, a survey score indicating depression, or both (59%); more than one-third had the same patterns for alcohol misuse (36%). Conclusions: ICD-9 codes were limited in predicting current depression and alcohol misuse, suggesting that caution should be exercised when using ICD-9 codes to assess depression or alcohol misuse among CHC patients. © 2017, Springer Science+Business Media, LLC.","Alcohol misuse; AUDIT-C; Depression; Hepatitis C; ICD-9 codes; PHQ-8","alcohol; adult; aged; alcohol abuse; alcohol liver disease; Article; binge drinking; Charlson Comorbidity Index; chronic hepatitis C; cohort analysis; controlled study; depression; female; health survey; human; ICD-9; major clinical study; male; middle aged; Patient Health Questionnaire 8; postnatal depression; predictive value; priority journal; sensitivity and specificity; adolescent; alcoholism; classification; comparative study; data mining; depression; electronic health record; Hepatitis C, Chronic; International Classification of Diseases; prevalence; procedures; time factor; United States; young adult; Adolescent; Adult; Aged; Alcoholism; Data Mining; Depression; Electronic Health Records; Female; Health Surveys; Hepatitis C, Chronic; Humans; International Classification of Diseases; Male; Middle Aged; Prevalence; Time Factors; United States; Young Adult",2-s2.0-85028830654
"Jongeling R., Sarkar P., Datta S., Serebrenik A.","On negative results when using sentiment analysis tools for software engineering research",2017,"Empirical Software Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009224950&doi=10.1007%2fs10664-016-9493-x&partnerID=40&md5=0c2a7a393f81629d7d152e2456b8033c","Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to diverging conclusions. Finally, we perform two replications of previously published studies and observe that the results of those studies cannot be confirmed when a different sentiment analysis tool is used. © 2017, The Author(s).","Negative results; Replication study; Sentiment analysis tools","Social aspects; Software engineering; Issue resolutions; Negative results; Product reviews; Replication study; Sentiment analysis; Software developer; Software engineering domain; Stack overflow; Data mining",2-s2.0-85009224950
"Chen Y., Dou P., Yang X.","Improving land use/cover classification with a multiple classifier system using AdaBoost integration technique",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032872274&doi=10.3390%2frs9101055&partnerID=40&md5=5f961f311c1692b86c7920be619c07cd","Guangzhou has experienced a rapid urbanization since 1978 when China initiated the economic reform, resulting in significant land use/cover changes (LUC). To produce a time series of accurate LUC dataset that can be used to study urbanization and its impacts, Landsat imagery was used to map LUC changes in Guangzhou from 1987 to 2015 at a three-year interval using a multiple classifier system (MCS). The system was based on a weighted vector to combine base classifiers of different classification algorithms, and was improved using the AdaBoost technique. The new classification method used support vector machines (SVM), C4.5 decision tree, and neural networks (ANN) as the training algorithms of the base classifiers, and produced higher overall classification accuracy (88.12%) and Kappa coefficient (0.87) than each base classifier did. The results of the experiment showed that, based on the accuracy improvement of each class, the overall accuracy was improved effectively, which combined advantages from each base classifier. The new method is of high robustness and low risk of overfitting, and is reliable and accurate, and could be used for analyzing urbanization processes and its impacts. © 2017 by the authors.","AdaBoost; Classification algorithms; Multiple classifiers system (MCS); Remote sensing","Adaptive boosting; Data mining; Decision trees; Economics; Land use; Remote sensing; Support vector machines; Classification accuracy; Classification algorithm; Classification methods; Integration techniques; Land use/cover change; Multiple classifier systems; Multiple classifiers systems; Neural networks (ANN); Classification (of information)",2-s2.0-85032872274
"Kinawy S.N., Nik Bakht M., El-Diraby T.E.","Mismatches in stakeholder communication: The case of the Leslie and Ferrand transit stations, Toronto, Canada",2017,"Sustainable Cities and Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023601647&doi=10.1016%2fj.scs.2017.06.020&partnerID=40&md5=12b4aed9edbc137acff0d78ed1a68917","Using content analysis, an approach is presented to help extract topics of interest to local community during project planning. This is helpful for fine-tuning and customizing the language used in communication with the public. Hopefully, reducing communication mismatches can help support constructive dialogue that is not “lost in translation”. The extraction of community issues/interest is becoming important also to help guide the development of plans/projects and their features in a manner that meets their needs. The two cases used in this study presented a suitable target for developing and showcasing the proposed approach. There was a reversal of public decision based on community debates/objections. This allowed us to study the mismatches before and after the decision. The proposed approach used a context-based taxonomy of terms and content analysis to compare terms/topics contained in a related twitter account, relevant news articles, and documents/presentations used in public meetings—before and after the decision. The proposed approach was designed to be mostly automatic to help future re-use. Of course, the use of such approach is only one step in a much bigger qualitative and context-specific process. Specific to the two cases, it was observed that news media articles and the contents of twitter chats had higher matching levels in the topics/themes they covered. Contents of public meetings had some levels of mismatching. Particular to the two cases, public official tended to emphasize the technical aspects of the projects with limited/clear analysis of their functions or impacts on community. It is argued that, as a result, public officials should study twitter chats and news articles as they prepare official public documents and presentations to citizens; attempt to specifically address prevalent issues in them; and even use the same nomenclature. Using a (semi) automated tool can be very helpful in this regard. © 2017 Elsevier Ltd","Co-creation and collaborative planning; Community engagement; Content analysis; Information retrieval; Urban infrastructure planning","Information retrieval; Social networking (online); Collaborative planning; Community engagement; Constructive dialogue; Content analysis; Project planning; Stakeholder communications; Technical aspects; Urban infrastructure; Data mining",2-s2.0-85023601647
"Rosseto H.C., Toledo L.D.A.S.D., Francisco L.M.B.D., Esposito E., Lim Y., Valacchi G., Cortesi R., Bruschi M.L.","Nanostructured lipid systems modified with waste material of propolis for wound healing: Design, in vitro and in vivo evaluation",2017,"Colloids and Surfaces B: Biointerfaces",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024406485&doi=10.1016%2fj.colsurfb.2017.07.029&partnerID=40&md5=2806194f127f793d2c6aee23029cf0fe","Propolis, a natural compound that can accelerate the wound healing process, is mainly used as ethanolic extract. The extractive solution may also be obtained from the propolis by-product (BP), transforming this waste material into a pharmaceutical active ingredient. Even if propolis does not show toxicity, when used as an extract over harmed skin or mucosa, the present ethanol content may be harmful to the tissue recovering, besides hindering the drug release. This study describes the development of solid lipid nanoparticles (SLN) and nanostructured lipid carriers (NLC) as topical propolis delivery systems and the investigation of their in vitro and in vivo activities. The extracts were evaluated to guarantee their quality, and the lipid dispersions were characterized with respect to morphology (cryo-TEM), size and diffractometry (X-ray) properties. The occlusive capacity of formulations was also evaluated by an in vitro technique, which determines the occlusion factor. The drug entrapment efficiency (EE), as well as the in vitro drug release profile from the nanoparticulate systems was investigated as well. The size analysis performed through 90 days was favorable to a topical administration and the polydispersity index, though not ideal in all cases due to the high content of resins and gums from the extracts, were relatively stable for the SLN. The propolis extract contributes to the occlusive potential of the formulations. The human immortalized keratinocytes presented good cell viability when tested with both extracts (propolis and BP) freely or entrapped in the systems. SLN modified with propolis material provided an acceleration of the in vivo wound healing process. © 2017 Elsevier B.V.","Formulation; Materials science; Nanoparticles; Nanostructured lipid carriers; Nanotechnology; Propolis by-product; Propolis byproduct","Byproducts; Data mining; Dispersions; Materials science; Nanoparticles; Nanotechnology; Protective coatings; Ethanolic extracts; Formulation; Nanoparticulate system; Nanostructured lipid carrier (NLC); Nanostructured lipid carriers; Polydispersity indices; Solid lipid nanoparticle (SLN); Wound healing process; Quality control; nanomaterial; natural product; propolis; solid lipid nanoparticle; analysis of variance; animal experiment; animal model; Article; cell viability; controlled study; drug design; drug determination; female; high performance liquid chromatography; human; human cell; in vitro study; in vivo study; keratinocyte; mouse; nanomedicine; neutron diffraction; nonhuman; particle size; photon correlation spectroscopy; physical chemistry; priority journal; wound healing; X ray diffraction",2-s2.0-85024406485
"Brenner D.G., Matlen B.J., Timms M.J., Gochyyev P., Grillo-Hill A., Luttgen K., Varfolomeeva M.","Modeling Student Learning Behavior Patterns in an Online Science Inquiry Environment",2017,"Technology, Knowledge and Learning",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023760253&doi=10.1007%2fs10758-017-9325-0&partnerID=40&md5=49130d932d021d265911188de370ebba","This study investigated how the frequency and level of assistance provided to students interacted with prior knowledge to affect learning in the Voyage to Galapagos (VTG) science inquiry-learning environment. VTG provides students with the opportunity to do simulated science field work in Galapagos as they investigate the key biology principles of variation, biological function, and natural selection. Thirteen teachers used the VTG module during their Natural Selection and Evolution curriculum unit. Students (N = 1728) were randomly assigned to one of four assistance conditions (Minimal-, Medium-, Medium–High, or High-Assistance). We predicted we would find an “Expertise Reversal Effect” (Kalyuga et al. in Edu Psychol Rev 194:509–539, 2007), whereby students with little prior knowledge benefit from assistance and students with higher prior knowledge benefit from minimal assistance. However, initial analyses revealed no interaction between prior knowledge and condition on student learning. To further explore results, we grouped students into 5 clusters based on student behaviors recorded during the use of VTG. The effect of assistance conditions within these clusters showed that, in two of the five clusters, results were consistent with the Expertise Reversal Effect. However, in two other clusters, the effect was reversed such that students with low prior knowledge benefited from lower amounts of assistance and vice versa. Though this study has not identified which specific characteristics determine optimal assistance levels, it suggests that prior knowledge is not sufficient for determining when students will differentially benefit from assistance. We propose that other factors such as self-regulated learning should be investigated in future research. © 2017, Springer Science+Business Media B.V.","Bayesian intelligent tutor; Educational data mining; Evolution; Expertise reversal effect; Inquiry learning","Biology; Computer aided instruction; Curricula; E-learning; Students; Teaching; Educational data mining; Evolution; Expertise reversal effect; Inquiry learning; Intelligent tutors; Education",2-s2.0-85023760253
"Gu X., Gu Y., Wu H.","Cascaded Convolutional Neural Networks for Aspect-Based Opinion Summary",2017,"Neural Processing Letters",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014075496&doi=10.1007%2fs11063-017-9605-7&partnerID=40&md5=73643f52056e1f1d6dafe98d4ef9ac22","This paper studies aspect-based opinion summary (AOS) of reviews on particular products. In practice, an AOS system needs to address two core subtasks, aspect extraction and sentiment classification. Most existing approaches to aspect extraction, using linguistic analysis or topic modeling, are general across different products but not precise enough or suitable for particular products. Instead we take a less general but more precise scheme, which directly maps each review sentence into pre-defined aspects. To tackle aspect mapping and sentiment classification, we propose a convolutional neural network (CNN) based method, cascaded CNN (C-CNN). C-CNN contains two levels of convolutional networks. Multiple CNNs at level 1 deal with aspect mapping task. If a review sentence belongs to pre-defined aspect categories, a single CNN at level 2 determines its sentiment polarity. Experimental results show that C-CNN with pre-trained word embedding outperform cascaded SVM with feature engineering. We also build a system called OpiSum with C-CNN. The demo of OpiSum can be found at http://114.215.167.42. © 2017, Springer Science+Business Media New York.","Aspect-based opinion summary; Convolutional neural networks; Data mining; Sentiment classification","Classification (of information); Convolution; Extraction; Linguistics; Mapping; Neural networks; AOS system; Aspect-based opinion summary; Convolutional networks; Convolutional neural network; Feature engineerings; Linguistic analysis; Sentiment classification; Topic Modeling; Data mining",2-s2.0-85014075496
"Gómez L., Karatzas D.","TextProposals: A text-specific selective search algorithm for word spotting in the wild",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019970310&doi=10.1016%2fj.patcog.2017.04.027&partnerID=40&md5=32f1b32383b238c96c009319e8d5a8aa","Motivated by the success of powerful while expensive techniques to recognize words in a holistic way (Goel et al., 2013; Almazán et al., 2014; Jaderberg et al., 2016) object proposals techniques emerge as an alternative to the traditional text detectors. In this paper we introduce a novel object proposals method that is specifically designed for text. We rely on a similarity based region grouping algorithm that generates a hierarchy of word hypotheses. Over the nodes of this hierarchy it is possible to apply a holistic word recognition method in an efficient way. Our experiments demonstrate that the presented method is superior in its ability of producing good quality word proposals when compared with class-independent algorithms. We show impressive recall rates with a few thousand proposals in different standard benchmarks, including focused or incidental text datasets, and multi-language scenarios. Moreover, the combination of our object proposals with existing whole-word recognizers (Almazán et al., 2014; Jaderberg et al., 2016) shows competitive performance in end-to-end word spotting, and, in some benchmarks, outperforms previously published results. Concretely, in the challenging ICDAR2015 Incidental Text dataset, we overcome in more than 10% F-score the best-performing method in the last ICDAR Robust Reading Competition (Karatzas, 2015). Source code of the complete end-to-end system is available at https://github.com/lluisgomez/TextProposals. © 2017 Elsevier Ltd","Grouping; Object proposals; Perceptual organization; Scene text","Aluminum; Benchmarking; Data mining; Silicate minerals; Competitive performance; End-to-end systems; Grouping; Object proposals; Perceptual organization; Scene Text; Search Algorithms; Word recognition; Character recognition",2-s2.0-85019970310
"Umamageswari K., Kalpana R.","Web data extraction from scientific publishers' website using heuristic algorithm",2017,"International Journal of Intelligent Systems and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030238373&doi=10.5815%2fijisa.2017.10.04&partnerID=40&md5=a8e0d8159f01b7586d9a8f503947583b","WWW is a huge repository of information and the amount of information available on the web is growing day by day in an exponential manner. End users make use of search engines like Google, Yahoo, and Bingo etc. for retrieving information. Search engines use web crawlers or spiders which crawl through a sequence of web pages in order to locate the relevant pages and provide a set of links ordered by relevancy. Those indexed web pages are part of surface web. Getting data from deep web requires form submission and is not performed by search engines. Data analytics and data mining applications depend on data from deep web pages and automatic extraction of data from deep web is cumbersome due to diverse structure of web pages. In the proposed work, a heuristic algorithm for automatic navigation and information extraction from journal's home page has been devised. The algorithm is applied to many publishers website such as Nature, Elsevier, BMJ, Wiley etc. and the experimental results show that the heuristic technique provides promising results with respect to precision and recall values. © 2017 MECS.","Deep web; DOM Tree; Information Extraction; JQuery; Structured data; Template; Wrapper; XPATH",,2-s2.0-85030238373
"Lee J.Y., Yoon J.S., Kim B.-H.","A big data analytics platform for smart factories in small and medium-sized manufacturing enterprises: An empirical case study of a die casting factory",2017,"International Journal of Precision Engineering and Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030856220&doi=10.1007%2fs12541-017-0161-x&partnerID=40&md5=6e97347a25643d0f72464e1f24aff1b0","This paper proposes an architecture and system modules for a big data analytics platform to implement smart factories in small and medium-sized enterprises. The big data analytics platform enables small and medium-sized enterprises 1) to achieve the integrated system environment between the legacy system and the platform; 2) to address quality issues by applying analytical models to their factories; and 3) to reduce their financial burdens of infrastructure and experts for the platform through cloud computing. In terms of evaluation, the proposed platform was applied to the factory of a die casting company in South Korea. Using the big data analytics platform that was developed, this paper also introduced the application scenario to identify defects in the die casting process. From this empirical research, we have clarified the difficulties and challenges in applying big data analytics to small and medium-sized manufacturing enterprises. For future works, this paper suggests a manufacturing data analytics library to provide consolidated information, including a data-mining model, its datasets, and preprocessing methods for specific manufacturing problems. © 2017, Korean Society for Precision Engineering and Springer-Verlag GmbH Germany.","Big data analytics platform; Defective casting; Die casting process; Small and medium-sized manufacturing enterprises; Smart factory",,2-s2.0-85030856220
"Saqib Nawaz M., Bilal M., Ikram Ullah Lali M., Mustafa R.U., Aslam W., Jajja S.","Effectiveness of social media data in healthcare communication",2017,"Journal of Medical Imaging and Health Informatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030647428&doi=10.1166%2fjmihi.2017.2148&partnerID=40&md5=e35006fda80bfe19d588ba6a6dbde19c","Applying opinion mining or sentiment analysis techniques on big online data for extracting useful information on any event or topic is gaining more and more interest with growing number of Internet users and recent developments in Information and Communication Technologies (ICT). In this article, we investigate how healthcare professionals (institutes and providers), general public as well as patient uses Twitter as an effective platform for spreading health related information and interaction. An approach is provided for extracting useful and critical information from health and disease related tweets. Experimentally, we train and validate the approach with Support Vector Machine (SVM) in WEKA. Our obtained results show that healthcare professionals and general public uses Twitter for interaction and as a communication tool for up-to date and vital health information exchange, acquisition and decision making. © 2017 American Scientific Publishers All rights reserved.","Health Communication; Healthcare; Opinion Mining; SVM; Twitter",,2-s2.0-85030647428
"Zhang X., Zhang X.","Adaptive multiclass support vector machine for multimodal data analysis",2017,"Pattern Recognition",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019265197&doi=10.1016%2fj.patcog.2017.05.006&partnerID=40&md5=fd4929485c69b6e858f19f820bb87e3a","Multimodal data commonly exists in human lives. Early analysis usually concentrates on mining information based on single modality. Recent studies show that learning tasks could be greatly enhanced by analyzing data from the aspect of multimodality. This paper deals with classifying multimodal data comprised of visual and acoustic contents. Different data features are fused under a hierarchical structure to achieve a good semantic understanding. Then, to accomplish accurate classification, an adaptive support vector machine method (ASVM) is proposed. The method is support vector machine with hyperparameters controlled by a novel and efficient artificial bee colony algorithm. First, a micro colony is set as the number of hyperparameters is usually less than 5. Second, one position inheritance based on roulette wheel selection is used. Third, discarded solutions are mutated by position shift operation instead of random reinitialization. The ASVM method is first verified on classical data sets demonstrating the goodness of the proposed method. Then the proposed method is applied on a multimodal data set. Each sample includes both image and audio data features. Experimental results show that the ASVM method is more effective and robust than the compared methods. © 2017 Elsevier Ltd","Artificial bee colony; Feature selection; Hyperparameter optimization; Multiclass classification; Support vector machine","Evolutionary algorithms; Feature extraction; Modal analysis; Optimization; Semantics; Support vector machines; Vectors; Artificial bee colonies; Artificial bee colony algorithms; Hierarchical structures; Hyper-parameter optimizations; Multi-class classification; Multi-class support vector machines; Multimodal data analysis; Roulette wheel selection; Classification (of information)",2-s2.0-85019265197
"John Basha M., Kaliyamurthie K.P.","Effective linear-time document clustering in text mining using web document categorization",2017,"International Journal of Civil Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032682818&partnerID=40&md5=4b214c9da4997e2d85af74fc029a27d6","Among data mining technique, clustering is one of the most important and traditional concept also an unsupervised learning paradigm. Similarity of a document pairs can be measured by matching of concepts. Finding or extracting the most relevant concept from the documents is a challengeable task. To address this issue, in this paper we introduce a concept of multi view point based similarity measure. Our proposed methods uses multiple point of reference between document pairs to extract more relevant match concept rather than extracting only ideas based on similarity measure. Using multiple view point, gathers more information about a particular topic from many different but relevant sources or concept. This strategy works well with smaller documents but is especially effective with longer documents. By gathering more relevant concepts from the documents with multiple points of reference, the document organization and retrieval can enhance the ability to make the most use of the documents held in storage and make retrieval of ideas as well as relevant task or concept much easier and faster. Experimental results shows that our proposed method efficiently extract more relevant concept. © IAEME Publication.","Concept Mining; Document Clustering; Similarity Measure",,2-s2.0-85032682818
"Almberg K.S., Cohen R.A., Blackley D.J., Laney A.S., Storey E., Halldin C.N.","Linking Compensation and Health Surveillance Data Sets to Improve Knowledge of US Coal Miners' Health",2017,"Journal of Occupational and Environmental Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025814307&doi=10.1097%2fJOM.0000000000001084&partnerID=40&md5=2debb2861d8f4d188c6d1c445c8d3f4b","Objective: Increase knowledge of US coal miners' respiratory health by linking data from the black lung benefits program (BLBP) and the coal workers' health surveillance program (CWHSP). Methods: BLBP claims data from 2000 through 2013 was linked to CWHSP data from 1970 through 2016. Results: Overall, 273,644 miners participated in CWHSP, 37,548 in BLBP, and 22,903 in both programs. Median age of miners at their time of first/only participation in CWHSP was 28 and 32 years, respectively. BLBP claimants were older (median age 59). Thirty-nine percent of BLBP claimants had not participated in CWHSP. The relative contributions of states to participation differed between CWHSP and BLBP. For example, Kentucky miners accounted for 18% of CWHSP participants, but 36% of BLPB participants. Conclusions: Many BLBP claimants never appeared in CWHSP, indicating missed opportunities for secondary prevention. Copyright © 2017 American College of Occupational and Environmental Medicine.",,"adult; Article; billing and claims; coal mining; coal worker; compensation; employment; female; geography; health program; human; Kentucky; knowledge; major clinical study; male; occupational health; occupational lung disease; Pennsylvania; pneumoconiosis; secondary prevention; thorax radiography; Virginia; West Virginia",2-s2.0-85025814307
"Vwima S., Rushigira C., Munguakonkwa G.","Competition between industrial mining and agricultural exploitation in South Kivu: Case of the Luhwindja Chiefdom Community",2017,"Livestock Research for Rural Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030707897&partnerID=40&md5=e251969bf9abbf9b0567f7958fff2154","In South Kivu, the expansion of the phenomenon of implementing mining companies on agricultural land is currently a reality. Competition between subsistence farming and mining by Twangiza Mining BANRO Corporation Company and artisanal gold mining is a situation that concerns rural society, especially agricultural producers in Luhwindja food chiefdom in the territory of Mwenga in South Kivu Province. This phenomenon can never exist without any influence on the farming and household’s food security. It is in this context that this study aims to analyze the socio-economic consequences of mining on agricultural production, on the rural population lives, in general, and in the chiefdom of Luhwidja in particular. The purpose is then to analyze the changes brought about by the mining activities on agricultural activities and the socio-economic life of the people of this community through data collected from interviews with chiefdom, relocated populations and NGOs representatives and from a survey questionnaire involving 50 farmers, 50 agri-miners (an agri-miner is one who practices both farming and mining) and 50 miners occasionally chosen. © 2017 Fundacion CIPAV. All rights reserved.","Competition; Farming; Mining; Socio-economic life",,2-s2.0-85030707897
"Song W., Chen F., Jacobsen H.-A., Xia X., Ye C., Ma X.","Scientific workflow mining in clouds",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030106296&doi=10.1109%2fTPDS.2017.2696942&partnerID=40&md5=2f30552e8bbadec71c6e25ccc0744164","Computing clouds have become the platform of choice for the deployment and execution of scientific workflows. Due to the uncertainty and unpredictability of scientific exploration, the execution plan for a scientific workflow may vary from the definition. It is therefore of great significance to be able to discover actual workflows from execution histories (event logs) to reproduce experimental results and to establish provenance. However, most existing process mining techniques focus on discovering control flow-oriented business processes in a centralized environment, and thus, they are mostly inapplicable to the discovery of data flow-oriented, unstructured scientific workflows in distributed cloud environments. In this paper, we present Scientific Workflow Mining as a Service ({\sf SWMaaS to support both intra-cloud and inter-cloud scientific workflow mining. The approach is implemented as a {ProM}} plug-in and is evaluated on event logs derived from real-world scientific workflows. Through experimental results, we demonstrate the effectiveness and efficiency of our approach. © 1990-2012 IEEE.","direct precedence; event log; inter-cloud; Scientific workflow; workflow mining","Signal processing; direct precedence; event log; Inter clouds; Scientific workflows; Workflow mining; Hardware",2-s2.0-85030106296
"Zaidan B.B., Zaidan A.A., Karim H.A., Ahmad N.N.","A new digital watermarking evaluation and benchmarking methodology using an external group of evaluators and multi-criteria analysis based on ‘large-scale data’",2017,"Software - Practice and Experience",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006395753&doi=10.1002%2fspe.2465&partnerID=40&md5=905ed939dbe6015a9b219dd2e3ec57a2","Digital watermarking evaluation and benchmarking are challenging tasks because of multiple evaluation and conflicting criteria. A few approaches have been presented to implement digital watermarking evaluation and benchmarking frameworks. However, these approaches still possess a number of limitations, such as fixing several attributes on the account of other attributes. Well-known benchmarking approaches are limited to robust watermarking. Therefore, this paper presents a new methodology for digital watermarking evaluation and benchmarking based on large-scale data by using external evaluators and a group decision making context. Two experiments are performed. In the first experiment, a noise gate-based digital watermarking approach is developed, and the scheme for the noise gate digital watermarking approach is enhanced. Sixty audio samples from different audio styles are tested with two algorithms. A total of 120 samples were evaluated according to three different metrics, namely, quality, payload, and complexity, to generate a set of digital watermarking samples. In the second experiment, the situation in which digital watermarking evaluators have different preferences is discussed. Weight measurement with a decision making solution is required to solve this issue. The analytic hierarchy process is used to measure evaluator preference. In the decision making solution, the technique for order of preference by similarity to the ideal solution with different contexts (e.g., individual and group) is utilized. Therefore, selecting the proper context with different aggregation operators to benchmark the results of experiment 1 (i.e., digital watermarking approaches) is recommended. The findings of this research are as follows: (1) group and individual decision making provide the same result in this case study. However, in the case of selection where the priority weights are generated from the evaluators, group decision making is the recommended solution to solve the trade-off reflected in the benchmarking process for digital watermarking approaches. (2) Internal and external aggregations show that the enhanced watermarking approach demonstrates better performance than the original watermarking approach. © 2016 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd. © 2016 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.","digital watermark; evaluation and benchmarking; multi-criteria analysis; multi-criteria decision making techniques","Acoustic noise; Benchmarking; Decision making; Digital watermarking; Economic and social effects; Mathematical operators; Solution mining; Weighing; Aggregation operator; Benchmarking methodology; Benchmarking process; Group Decision Making; Individual decision making; Multi Criteria Analysis; Multi-criteria decision making technique; Robust watermarking; Quality control",2-s2.0-85006395753
"Xu Y., Wu K., Bai Z., Hu Z.","Theoretical analysis of the secondary development of mining-induced surface cracks in the Ordos region",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032196623&doi=10.1007%2fs12665-017-7050-6&partnerID=40&md5=98a94e32115e5c7262460e2672d48fa9","Prior studies have indicated that mining-induced surface dynamic cracks in eastern China only undergo one “expanding-closure” development cycle. However, field measurements from the Bulianta mines of northwestern China demonstrated that ground dynamic cracks in the Ordos region went through two “expanding-closure” cycle. Using in situ measurement data and theoretical analysis, we find that the strata control theory cannot reasonably explain this special phenomenon. In this paper, we propose a novel explanation from the perspective of soil mechanics. A comparison of two field examples in eastern and northwestern China demonstrates that our theoretical explanation is in agreement with in situ observations and therefore is a reasonable interpretation for the secondary development of ground dynamic cracks in the Bulianta mines. This study provides a theoretical basis for the mechanism of mining-induced ground dynamic cracks. © 2017, Springer-Verlag GmbH Germany.","Aeolian region; Coal mining; Ground subsidence; Soil mechanics","Cracks; Dynamics; Soil mechanics; Subsidence; Surface defects; Aeolian region; Coal mining; Development cycle; Field measurement; In-situ measurement; In-situ observations; Northwestern China; Secondary development; Control theory; coal mining; in situ measurement; soil cracking; soil mechanics; subsidence; theoretical study; China; Ordos",2-s2.0-85032196623
"French M., Alem N., Edwards S.J., Blanco Coariti E., Cauthin H., Hudson-Edwards K.A., Luyckx K., Quintanilla J., Sánchez Miranda O.","Community exposure and vulnerability to water quality and availability: a case study in the mining-affected Pazña Municipality, Lake Poopó Basin, Bolivian Altiplano",2017,"Environmental Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020435818&doi=10.1007%2fs00267-017-0893-5&partnerID=40&md5=d45467461a5697749cf9ecd56f4cf56d","Assessing water sources for drinking and irrigation along with community vulnerability, especially in developing and rural regions, is important for reducing risk posed by poor water quality and limited water availability and accessibility. We present a case study of rural mining-agricultural communities in the Lake Poopó Basin, one of the poorest regions on the Bolivian Altiplano. Here, relatively low rainfall, high evaporation, salinization and unregulated mining activity have contributed to environmental degradation and water issues, which is a situation facing many Altiplano communities. Social data from 72 households and chemical water quality data from 27 surface water and groundwater sites obtained between August 2013 and July 2014 were used to develop locally relevant vulnerability assessment methodologies and ratings with respect to water availability and quality, and Chemical Water Quality Hazard Ratings to assess water quality status. Levels of natural and mining-related contamination in many waters (CWQHR ≥ 6; 78% of assessed sites) mean that effective remediation would be challenging and require substantial investment. Although waters of fair to good chemical quality (CWQHR ≤ 5; 22% of assessed sites) do exist, treatment may still be required depending on use, and access issues remain problematic. There is a need to comply with water quality legislation, improve and maintain basic water supply and storage infrastructure, build and operate water and wastewater treatment plants, and adequately and safely contain and treat mine waste. This study serves as a framework that could be used elsewhere for assessing and mitigating water contamination and availability affecting vulnerable populations. © 2017, The Author(s).","Bolivian Altiplano; Mining; Vulnerability; Water quality; Water resources management; Water scarcity","Chemical hazards; Degradation; Digital storage; Groundwater; Investments; Lake pollution; Lakes; Mining; Potable water; Risk assessment; Rural areas; Solid wastes; Surface waters; Waste treatment; Wastewater treatment; Water pollution; Water quality; Water resources; Water supply; Bolivian Altiplano; Surface water and groundwaters; Vulnerability; Vulnerability assessment methodologies; Water and wastewater treatments; Water quality legislation; Water resources management; Water scarcity; Water treatment; ground water; rain; river water; surface water; environmental degradation; groundwater; mine waste; mining; resource management; rural area; rural population; surface water; vulnerability; wastewater treatment plant; water availability; water pollution; water quality; water resource; water supply; agriculture; Article; bioremediation; Bolivian; case study; community; degradation; desalination; environmental exposure; evaporation; flow measurement; health hazard; household; human; legal aspect; mining; nonhuman; river; rural area; seasonal variation; socioeconomics; vulnerable population; waste water management; waste water treatment plant; water availability; water contamination; water flow; water quality; water sampling; water supply; Altiplano; Bolivia; Lake Poopo; Oruro [Bolivia]",2-s2.0-85020435818
"Hosseini A.S.","Sentence-level emotion mining based on combination of adaptive Meta-level features and sentence syntactic features",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028755282&doi=10.1016%2fj.engappai.2017.08.006&partnerID=40&md5=3b5d6fff7d4a2b25929f8517e6473922","Nowadays, because of increasing of text data, recognizing the emotions of text can help to get a better comprehension of context. However, finding emotional information from text is a very complex task because it is needed to automatically understand of human sentences that usually are vague and dependent on context which should be interpreted and represented in different ways. The sense of a word can be inferred by investigating the frequency of occurrence of the word in a large corpus of annotated text. This paper has presented a method for learning adaptive lexicon from existing lexicon resources (static lexicons) to improve performance of emotion detection task for two data sets. The learning of adaptive lexicon would allow us to distinguish between the initial emotion of words in the static lexicons and adaptive emotion of words that is mentioned in context, and we get a better understanding of the emotional orientation of words. Furthermore, this study proposes a novel approach for emotion detection based on the combination of Meta-level features derived from static and adaptive lexicons and sentences syntactic features. To the best of our knowledge, this is the first study that provides a comprehensive analysis of the relative importance of a very diverse feature set for automatic emotion detection. Extensive experiments on ISEAR and Aman data sets show that learning adaptive lexicon enables emotion mining algorithms to be more accurate. © 2017 Elsevier Ltd","Adaptive lexicon; Emotion mining; Meta-level features; Sentence-level analysis; Static lexicons","Character recognition; Syntactics; Adaptive lexicon; Comprehensive analysis; Emotional information; Emotional orientations; Improve performance; Meta levels; Sentence level; Static lexicons; Feature extraction",2-s2.0-85028755282
"El Azhari A., Rhoujjati A., El Hachimi M.L., Ambrosi J.-P.","Pollution and ecological risk assessment of heavy metals in the soil-plant system and the sediment-water column around a former Pb/Zn-mining area in NE Morocco",2017,"Ecotoxicology and Environmental Safety",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021441062&doi=10.1016%2fj.ecoenv.2017.06.051&partnerID=40&md5=95f2460006e91ed6e0d32548b634708e","This study discussed the environmental fate and ecological hazards of heavy metals in the soil-plant system and sediment-water column around the former Pb-Zn mining Zeïda district, in Northeastern Morocco. Spatial distribution, pollution indices, and cluster analysis were applied for assessing Pb, Zn, As, Cu and Cd pollution levels and risks. The geo-accumulation index (Igeo) was determined using two different geochemical backgrounds: i) the commonly used upper crust values, ii) local geochemical background calculated with exploratory data analysis. The soils in the vicinity of the tailings, as well as the sediments downstream of the latter, displayed much higher metal concentrations, Igeo and potential ecology risk coefficient values than other sites, classifying these sites as highly contaminated and severely hazardous. The concentrations of Pb in contaminated sediment samples also exceeded the PEC limits and are expected to cause harmful effects on sediment-dwelling organisms. Based on the comparison with the toxicity limits, the most contaminated plant samples were found around the tailings piles. The metal concentrations in both raw and filtrated water samples were overall below the drinking water standards in samples upstream and downstream of the mining center, indicating that heavy metals levels in the Moulouya River surface waters were not affected by the tailings spill. Cluster analysis suggest that: i) Pb and Zn in sediments were derived from the abandoned tailings and are mainly stored and transported as particle-bound to the bedload, ii) Pb, Zn, and Cu in the soil-plant system were related to the dispersion of tailings materials while As and Cd originated primarily from natural geological background in both the soil-plant and the water-sediment systems. © 2017 Elsevier Inc.","Cluster analysis; Heavy metals; Pollution assessment; Sediment-water column; Soil-plant system","arsenic; cadmium; copper; drinking water; heavy metal; lead; surface water; zinc; cluster analysis; environmental fate; geoaccumulation index; heavy metal; lead; mining; risk assessment; spatial distribution; zinc; Article; concentration (parameters); controlled study; ecotoxicity; environmental impact; environmental parameters; geo accumulation index; geographic distribution; hazard assessment; mine tailings; mining; Morocco; nonhuman; particle size; physical chemistry; risk assessment; sediment; soil acidity; soil pollution; spillage; Stipa tenacissima; surface soil; water pollution; water sampling; Morocco; Moulouya River",2-s2.0-85021441062
"Guo J.-Y., Lu W.-X., Jiang X., Zhang Y., Zhao H.-Q., Miao T.-S.","A quantitative model to evaluate mine geological environment and a new information system for the mining area in Jilin province, mid-northeastern China",2017,"Arabian Journal of Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031747596&doi=10.1007%2fs12517-017-3228-1&partnerID=40&md5=f7d7bb76e5e715273365d8426a1ef23d","The reliability of mine geological environment quality assessment highly depends on a large amount of survey data, a comprehensive evaluation system and an effective evaluation model. Using computer technology to integrate large amount of data can help to ensure the valid management and the effective assessment. Compared with previous studies, this study has improved and enriched the evaluation system and optimized the traditional evaluation method. Moreover, combining geology with computer science, it has developed the evaluation function of mine geological environment and realized the intersection and innovation of the discipline. The specific research content has the following three parts. First, a new design for an evaluation system which can synthetically describe the mine geological environment is presented. Second, a particle swarm optimization (PSO)-support vector machine (SVM) model is established as an alternative to traditional approaches that avoid interference from artificial factors. Third, a new mine geological environmental information system (MGEIS) is presented to efficiently manage data. Then, PSO-SVM evaluation model is embedded in it, and the mine geological environment in Jilin province is assessed by using MGEIS. Decisions can be presented based on the evaluation results in this study to better support the recovery of the local mine geological environment. © 2017, Saudi Society for Geosciences.","Hierarchical factor evaluation system; Mine geological environment information system; Mine geological environmental evaluation; PSO-SVM evaluation model","assessment method; environmental assessment; environmental modeling; environmental quality; environmental research; mining; China; Jilin",2-s2.0-85031747596
"Li L., Lei Y., Xu Q., Wu S., Yan D., Chen J.","Crowding-out effect of coal industry investment in coal mining area: taking Shanxi province in China as a case",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027985380&doi=10.1007%2fs11356-017-9915-3&partnerID=40&md5=b7a03349862dc905941c940c0dd874a3","The rapid development of coal industry in Shanxi province in China has important effects on its economic development. A large amount of money has been invested into the coal industry and other related industries during the recent years. However, research on the investment effect of Shanxi’s coal industry was rare. In order to analyze the investment effect of coal industry, based on the crowding-out effect model, cointegration test, and the data available in Shanxi Statistical Yearbooks, this paper calculates the effect between coal industry investment and other 17 industry investment. The results show that the investment of coal industry produces crowding-out effect on food industry, building materials industry, and machinery industry. Increasing 1% of the coal industry investment can reduce 0.25% of the food industry investment, or 0.6% of building materials industry investment, or 0.52% of the machinery industry investment, which implies that Shanxi province should adjust coal industrial structure, promote the balance development of coal industry and other industries, so as to promote its economic growth. © 2017, Springer-Verlag GmbH Germany.","Coal industry investment; Cointegration test; Impact assessment; Shanxi province; The crowding-out effect model","coal industry; coal mining; economic growth; environmental impact assessment; food industry; industrial investment; industrial structure; machinery; China; Shanxi",2-s2.0-85027985380
"Galica T., Hrouzek P., Mareš J.","Genome mining reveals high incidence of putative lipopeptide biosynthesis NRPS/PKS clusters containing fatty acyl-AMP ligase genes in biofilm-forming cyanobacteria",2017,"Journal of Phycology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024382364&doi=10.1111%2fjpy.12555&partnerID=40&md5=1f002c6be614f65c3761c9ec1ed7f33b","Cyanobacterial lipopeptides have antimicrobial and antifungal bioactivities with potential for use in pharmaceutical research. However, due to their hemolytic activity and cytotoxic effects on human cells, they may pose a health issue if produced in substantial amounts in the environment. In bacteria, lipopeptides can be synthesized via several well-evidenced mechanisms. In one of them, fatty acyl-AMP ligase (FAAL) initiates biosynthesis by activation of a fatty acyl residue. We have performed a bioinformatic survey of the cyanobacterial genomic information available in the public databases for the presence of FAAL-containing non-ribosomal peptide synthetase/polyketide synthetase (NRPS/PKS) biosynthetic clusters, as a genetic basis for lipopeptide biosynthesis. We have identified 79 FAAL genes associated with various NRPS/PKS clusters in 16% of 376 cyanobacterial genomic assemblies available, suggesting that FAAL is frequently incorporated in NRPS/PKS biosynthetases. FAAL was present either as a stand-alone protein or fused either to NRPS or PKS. Such clusters were more frequent in derived phylogenetic lineages with larger genome sizes, which is consistent with the general pattern of NRPS/PKS pathways distribution. The putative lipopeptide clusters were more frequently found in genomes of cyanobacteria that live attached to surfaces and are capable of forming microbial biofilms. While lipopeptides are known in other bacterial groups to play a role in biofilm formation, motility, and colony expansion, their functions in cyanobacterial biofilms need to be tested experimentally. According to our data, benthic and terrestrial cyanobacteria should be the focus of a search for novel candidates for lipopeptide drug synthesis and the monitoring of toxic lipopeptide production. © 2017 Phycological Society of America","cyanobacteria; fatty-acyl AMP ligase; genome mining; lipopeptides; microbial biofilm; non-ribosomal peptide synthesis",,2-s2.0-85024382364
"Maaradji A., Dumas M., Rosa M.L., Ostovar A.","Detecting sudden and gradual drifts in business processes from execution traces",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021917768&doi=10.1109%2fTKDE.2017.2720601&partnerID=40&md5=2e15744116e6d78402a153ec126d92db","Business processes are prone to unexpected changes, as process workers may suddenly or gradually start executing a process differently in order to adjust to changes in workload, season, or other external factors. Early detection of business process changes enables managers to identify and act upon changes that may otherwise affect process performance. Business process drift detection refers to a family of methods to detect changes in a business process by analyzing event logs extracted from the systems that support the execution of the process. Existing methods for business process drift detection are based on an explorative analysis of a potentially large feature space and in some cases they require users to manually identify specific features that characterize the drift. Depending on the explored feature space, these methods miss various types of changes. Moreover, they are either designed to detect sudden drifts or gradual drifts but not both. This paper proposes an automated and statistically grounded method for detecting sudden and gradual business process drifts under a unified framework. An empirical evaluation shows that the method detects typical change patterns with significantly higher accuracy and lower detection delay than existing methods, while accurately distinguishing between sudden and gradual drifts. © 1989-2012 IEEE.","Business process management; Change detection; Concept drift; Process mining","Administrative data processing; Enterprise resource management; Insurance; Scalability; Space research; Business process management; Change detection; Concept drifts; Delays; Process mining; Space explorations; Feature extraction",2-s2.0-85021917768
"Leoni M.D., Marrella A.","Aligning Real Process Executions and Prescriptive Process Models through Automated Planning",2017,"Expert Systems with Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017434946&doi=10.1016%2fj.eswa.2017.03.047&partnerID=40&md5=f7d02609c9addd488076633e88f8052d","Modern organizations execute processes to deliver product and services, whose enactment needs to adhere to laws, regulations and standards. Conformance checking is the problem of pinpointing where deviations are observed. This paper shows how instances of the conformance checking problem can be represented as planning problems in PDDL (Planning Domain Definition Language) for which planners can find a correct solution in a finite amount of time. If conformance checking problems are converted into planning problems, one can seamlessly update to the recent versions of the best performing automated planners, with evident advantages in term of versatility and customization. The paper also reports on results of experiments conducted on two real-life case studies and on eight larger synthetic ones, mainly using the FAST-DOWNWARD planner framework to solve the planning problems due to its performances. Some experiments were also repeated though other planners to concretely showcase the versatility of our approach. The results show that, when process models and event logs are of considerable size, our approach outperforms existing ones even by several orders of magnitude. Even more remarkably, when process models are extremely large and event log traces very long, the existing approaches are unable to terminate because they run out of memory, while our approach is able to properly complete the alignment task. © 2017 Elsevier Ltd","Automated planning; Business process management; Conformance checking; Process mining","Administrative data processing; Automation; Enterprise resource management; Laws and legislation; Planning; Automated planning; Business process management; Conformance checking; Orders of magnitude; Planning domain definition language; Process execution; Process mining; Product and services; Problem solving",2-s2.0-85017434946
"Newman C., Agioutantis Z., Schaefer N.","Development of a new Web-based platform for ground control applications",2017,"Mining Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030325465&partnerID=40&md5=02dcb03145259af4672d674bf17f96cf",[No abstract available],,"control system; data quality; design; information technology; mining; software; underground construction; World Wide Web",2-s2.0-85030325465
"Nasim M., Rextin A., Hayat S., Khan N., Malik M.M.","Data analysis and call prediction on dyadic data from an understudied population",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028710103&doi=10.1016%2fj.pmcj.2017.08.002&partnerID=40&md5=39d19c7727ea56aec83c9eecc0d5d71a","In this paper we predict outgoing mobile phone calls using machine learning and time clusters based approaches. We analyze to which extent the calling activity of mobile phone users is predictable. The premise is that mobile phone users exhibit temporal regularity in their interactions with majority of their contacts. In the sociological context, most social interactions have fairly reliable temporal regularity. If we quantify the extension of this behavior to interactions on mobile phones we expect that pairwise interaction is not merely a result of randomness, rather it exhibits a temporal pattern. To this end, we not only tested our approach on an original mobile phone usage dataset from a developing country, Pakistan, but we also analyzed the famous Reality Mining Dataset and the Nokia Dataset (from a European country), where we found an equitable basis for comparison with our data. Our original data consists of 783 users and more than 12,000 active dyads. Our results show that temporal information about pairwise user interactions can predict future calls with reasonable accuracy. © 2017 Elsevier B.V.","Call prediction; Call-logs; Smartphone; Temporal regularity","Cellular telephones; Developing countries; Forecasting; Learning systems; Mobile phones; Telephone sets; Call-logs; European Countries; Mobile phone usages; Pairwise interaction; Reasonable accuracy; Social interactions; Temporal information; Temporal regularity; Population statistics",2-s2.0-85028710103
"Robin V., Tertre E., Beaucaire C., Regnault O., Descostes M.","Experimental data and assessment of predictive modeling for radium ion-exchange on beidellite, a swelling clay mineral with a tetrahedral charge",2017,"Applied Geochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028545411&doi=10.1016%2fj.apgeochem.2017.07.009&partnerID=40&md5=aae2d17bd79bdc8411a18178eb3bb9ed","The presence of swelling clay minerals, such as smectites, is known to be one of the main parameters controlling the mobility of radium in natural environments due to the high adsorption capacities of these materials. However, experimental data and modeling from literature allowing to investigate the adsorption properties of radium towards smectite in a wide range of acidic conditions (2 &lt; 7) are scarce. Moreover, the role of the smectite crystal chemistry (charge location in the layer) remains unclear. Therefore, experimental data for the adsorption of radium onto a swelling clay mineral with a tetrahedral charge (beidellite) are presented here in the 2–7 pH range. Different batch experiments were conducted in order to investigate the effects of both ionic strength (i.e., from 0.027 to 0.11 M in sodium chloride medium) and presence of competing major cations (i.e., magnesium). Experimental data were interpreted using a multi-site ion-exchange model that takes into account the adsorption of major cations (including H+), which usually compete with trace elements for sorption onto mineral surfaces in natural environments. The ability of the proposed model to predict experimental distribution coefficient (Kd) values under various conditions of ionic strength and aqueous cation compositions was tested, and results were compared with those obtained using previously published models. The evolution of radium adsorption on beidellite with pH variation is better reproduced by using a multi-site ion-exchange model, as proposed in this study, than by using a single-site model as that reported in literature. Finally, the implication of the results obtained in this study for the mobility of radium in natural environments was discussed. The significant role of smectites in the adsorption of radium at low pH conditions (pH ≤ 6) in complex mixtures, which are representative of natural matrices, was demonstrated based on comparison with experimental data previously obtained on organic material and Fe(III)-oxy/hydroxides. The multi-site model proposed in this study for beidellite, coupled with a model from literature for organic matter, was applied in order to predict the role played by the beidellite in the adsorption properties of a virtual complex mixture containing various proportions of beidellite and organic matter. The model shows that smectite can contribute from 80 to 100% to the total adsorption of radium of the complex mixture for pH &lt; 3. Moreover, the predicted results were in good agreement with experimental data from literature obtained with natural peat. Finally, the multi-site ion-exchange model proposed in this study for beidellite could be applied in many natural environments characterized by acidic pH, such as naturally acidic soils (pH ≤ 5) and acid mining environments (pH ≤ 1), where predicting 226Ra exchange onto swelling clay minerals requires accounting for the effects of the major cations present in these systems. © 2017 Elsevier Ltd","Acidic conditions; Beidellite; Ion-exchange; Modeling; Natural environment; Radium; Swelling clay","Adsorption; Biogeochemistry; Biological materials; Clay minerals; Crystal chemistry; Forecasting; Ionic strength; Ions; Minerals; Mixtures; Models; Organic compounds; pH; Positive ions; Radium; Swelling; Trace elements; Acidic conditions; Adsorption properties; Beidellite; Distribution coefficient; High adsorption capacity; Ion exchange modeling; Natural environments; Swelling clay; Ion exchange; acid soil; acidity; adsorption; beidellite; cation; clay mineral; crystal chemistry; inorganic compound; modeling; organic matter; radium; smectite; sodium chloride; sorption; trace element",2-s2.0-85028545411
"Tyulenev M.A., Zhironkin S.A., Litvin O.I., Tyuleneva E.A., Zhironkina O.V., Markov S.O.","Safe and Productive Application of Hydraulic Backhoes in Coal-Bearing Areas of Complex Structured Deposits",2017,"Geotechnical and Geological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016601781&doi=10.1007%2fs10706-017-0228-6&partnerID=40&md5=de705d23b835c748d7993f82f936fee9","The article describes a problem of hydraulic backhoes usage in coal-bearing areas development in complex structured deposits. At present time there is a mass replacement of previously widely used shovels with more adaptive excavation equipment—hydraulic backhoes in open pits of Kuzbass (the largest Russian coal basin, Western Siberia). On the one hand, it opens up new opportunities for reducing of coal losses and better planning of mining operations in coal-bearing area during the development of rock-and-coal blocks. On the other hand, backhoes usage with the replacement of railway transport for trucks significantly complicates rock-and-coal face parameters determination. It also impose new requirements for dispatching the “excavator—transport unit” work chain. In addition, strict requirements to excavator’s maneuverability for rock-and-coal blocks processing actualize research of safe backhoe positioning on a bench. The article describes the results of technological parameters determination for hydraulic backhoes usage in coal-bearing areas development in complex structured deposits. The most important results are the recommendations for choosing the order of heterogeneous (rock and coal) layers processing and justification of excavation blocks parameters for their layer-by-layer extraction that minimizes the coal losses. Particular attention is paid to the definition of excavated layer parameters depending on bucket capacity and excavated substance. Based on these data, the dependence of hydraulic backhoe productivity on the excavated layer height was defined. The study results are presented in the article on the example of Kuzbass coal deposits being developed by several open pit mines. © 2017, Springer International Publishing Switzerland.","Backhoe; Coal-and-rock block; Coal-bearing area; Complex structured deposits; Face; Open pit mining; Subbench processing","Coal; Coal mines; Construction equipment; Deposits; Excavation; Excavators; Hydraulic machinery; Open pit mining; Rocks; Backhoe; Excavation equipment; Face; Mining operations; Parameters determination; Railway transport; Rock block; Technological parameters; Coal deposits; coal mining; equipment; extraction; open pit mine; safety; technology adoption; Russian Federation",2-s2.0-85016601781
"Thorslund J., Jarsjö J., Wällstedt T., Mörth C.M., Lychagin M.Y., Chalov S.R.","Speciation and hydrological transport of metals in non-acidic river systems of the Lake Baikal basin: Field data and model predictions",2017,"Regional Environmental Change",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029778432&doi=10.1007%2fs10113-016-0982-7&partnerID=40&md5=5e25ab6263b39c5cabb86c7bccf56e6c","The speciation of metals in aqueous systems is central to understanding their mobility, bioavailability, toxicity and fate. Although several geochemical speciation models exist for metals, the equilibrium conditions assumed by many of them may not prevail in field-scale hydrological systems with flowing water. Furthermore, the dominant processes and/or process rates in non-acidic systems might differ from well-studied acidic systems. We here aim to increase knowledge on geochemical processes controlling speciation and transport of metals under non-acidic river conditions. Specifically, we evaluate the predictive capacity of a speciation model to novel measurements of multiple metals and their partitioning, under high-pH conditions in mining zones within the Lake Baikal basin. The mining zones are potential hotspots for increasing metal loads to downstream river systems. Metals released from such upstream regions may be transported all the way to Lake Baikal, where increasing metal contamination of sediments and biota has been reported. Our results show clear agreement between speciation predictions and field measurements of Fe, V, Pb and Zn, suggesting that the partitioning of these metals mainly was governed by equilibrium geochemistry under the studied conditions. Systematic over-predictions of dissolved Cr, Cu and Mo by the model were observed, which might be corrected by improving the adsorption database for hydroxyapatite because that mineral likely controls the solubility of these metals. Additionally, metal complexation by dissolved organic matter is a key parameter that needs continued monitoring in the Lake Baikal basin because dependable predictions could not be made without considering its variability. Finally, our investigation indicates that further model development is needed for accurate As speciation predictions under non-acidic conditions, which is crucial for improved health risk assessments on this contaminant. © 2016, The Author(s).","Geochemical modelling; Lake Baikal; Metals; Non-acidic; River system; Speciation",,2-s2.0-85029778432
"Lee S.-H., Ji W., Yang H.-J., Kang S.-Y., Kang D.M.","Reclamation of mine-degraded agricultural soils from metal mining: lessons from 4 years of monitoring activity in Korea",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032591622&doi=10.1007%2fs12665-017-7076-9&partnerID=40&md5=0c805c2f5e2659a8b7b0614e33493b0d","The environmental issues associated with mining have damaged the industry’s substantial global economic value. In particular, the mining industry has a negative legacy of contaminated land. The effective reclamation of contaminated soil is therefore required before former mining land can be further developed for residential and commercial purposes. The objective of this study was to technically evaluate the feasibility of reclamation techniques for agricultural soils contaminated with toxic elements (As, Cd, Cu, Pb, and Zn) associated with metal mining. The reclamation methods investigated were covering without stabilization, covering with stabilization, and exchange with stabilization. The thickness of the soil layer used in covering and exchange was in the range of 30–50 cm. Limestone, furnace slag, and a mixture of limestone and furnace slag were applied as soil amendments. After reclamation, the contamination level in surface tillage soils and crops was monitored regularly. Four years of monitoring data revealed that surface soil contamination levels could be maintained at acceptable levels, although at some sites, the metal levels in crops exceeded legislative limits. Soil reclamation at former mining sites in Korea has not yet been perfected, but the results of this study show that there is potential for safe agricultural operations on large sites in a cost-effective manner, as long as the appropriate control of surface soil contamination and adequate agronomic management is undertaken. © 2017, Springer-Verlag GmbH Germany.","Covering; Exchange; Mined soil; Reclamation; Stabilization","Agriculture; Cadmium; Contamination; Copper; Cost effectiveness; Crops; Ion exchange; Land reclamation; Lead; Limestone; Pollution detection; Reclamation; Remediation; Slags; Soil pollution; Stabilization; Agricultural operations; Agricultural soils; Contaminated lands; Contaminated soils; Contamination levels; Covering; Environmental issues; Monitoring activities; Soils",2-s2.0-85032591622
"Mihelčić M., Šimić G., Leko M.B., Lavrač N., Džeroski S., Šmuc T.","Using redescription mining to relate clinical and biological characteristics of cognitively impaired and Alzheimer’s disease patients",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032749361&doi=10.1371%2fjournal.pone.0187364&partnerID=40&md5=db59cab26557968494bc01622cd0164e","Based on a set of subjects and a collection of attributes obtained from the Alzheimer’s Disease Neuroimaging Initiative database, we used redescription mining to find interpretable rules revealing associations between those determinants that provide insights about the Alzheimer’s disease (AD). We extended the CLUS-RM redescription mining algorithm to a constraint-based redescription mining (CBRM) setting, which enables several modes of targeted exploration of specific, user-constrained associations. Redescription mining enabled finding specific constructs of clinical and biological attributes that describe many groups of subjects of different size, homogeneity and levels of cognitive impairment. We confirmed some previously known findings. However, in some instances, as with the attributes: testosterone, ciliary neurotrophic factor, brain natriuretic peptide, Fas ligand, the imaging attribute Spatial Pattern of Abnormalities for Recognition of Early AD, as well as the levels of leptin and angiopoietin-2 in plasma, we corroborated previously debatable findings or provided additional information about these variables and their association with AD pathogenesis. Moreover, applying redescription mining on ADNI data resulted with the discovery of one largely unknown attribute: the Pregnancy-Associated Protein-A (PAPP-A), which we found highly associated with cognitive impairment in AD. Statistically significant correlations (p ≤ 0.01) were found between PAPP-A and clinical tests: Alzheimer’s Disease Assessment Scale, Clinical Dementia Rating Sum of Boxes, Mini Mental State Examination, etc. The high importance of this finding lies in the fact that PAPP-A is a metalloproteinase, known to cleave insulin-like growth factor binding proteins. Since it also shares similar substrates with A Disintegrin and the Metalloproteinase family of enzymes that act as α-secretase to physiologically cleave amyloid precursor protein (APP) in the non-amyloidogenic pathway, it could be directly involved in the metabolism of APP very early during the disease course. Therefore, further studies should investigate the role of PAPP-A in the development of AD more thoroughly. © 2017 Mihelčić et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,2-s2.0-85032749361
"Yasuda N., Mitsuhashi H.","Learning from Political Change and the Development of MNCs’ Political Capabilities: Evidence from the Global Mining Industry",2017,"Management International Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020482060&doi=10.1007%2fs11575-017-0318-y&partnerID=40&md5=fa5cef21ac4438260323dcf58780f095","Previous studies have suggested that (1) a type of organizational capabilities—namely, political capabilities—are required for multinational corporations (MNCs) to grow in global markets, (2) political capabilities are important for building productive relations with governments in politically risky host countries, and (3) MNCs can develop political capabilities by accumulating foreign experiences. However, empirical studies have found both positive and negative effects of such experiences on global market expansions. This study attributes such mixed findings to our lack of understanding about MNCs’ procurement processes of political capabilities and proposes types of experiences critical for such procurements by focusing on their reactions to political changes in host countries. Using data on the global mining industry and political changes in host countries, we find that MNCs develop political capabilities and thus make entries into politically risky host countries when they accumulate the experience of partially divesting some of their assets after political changes in host countries. We also find that MNCs are less likely to enter such countries if they have more experiences of exiting from host countries following political change. © 2017, Springer-Verlag Berlin Heidelberg.","Foreign direct investment; Market entry; Market exit; Organizational experience; Political capabilities; Political change",,2-s2.0-85020482060
"Lin J.C.-W., Gan W., Fournier-Viger P., Chao H.-C., Hong T.-P.","Efficiently mining frequent itemsets with weight and recency constraints",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018817124&doi=10.1007%2fs10489-017-0915-2&partnerID=40&md5=27805b48f22f70c3b8f6a3fb02a94cfd","In the past, a novel framework named recent weighted frequent itemset mining (RWFIM) and two projection-based algorithms, RWFIM-P and RWFIM-PE, were proposed to consider both the relative importance of items (item weights) and the recency of patterns. However, the projection-and-test mechanism used by these algorithms to discover recent weighted frequent itemsets (RWFIs) in a recursive way may have poor performance when the database is dense or contains long transactions. To address this issue, an efficient tree-based RWFI-Mine algorithm is proposed in this paper for mining RWFIs, which considers both weight and the recency of patterns. A novel Set-enumeration tree called the recent weighted frequent (RWF)-tree and a sorted downward closure property of RWFIs for the RWF-tree are proposed. Moreover, two data structures, named element (E)-table and recent weighted frequent (RWF)-table, are designed to store the information needed for discovering RWFIs. RFWI-Mine discovers RWFIs in a recursive way without candidate generation, thus reducing the computational costs and memory requirements for mining RWFIs. A second improved algorithm named RWFI-EMine algorithm is further proposed to avoid building E-tables and RWF-tables for unpromising itemsets and their child nodes by adopting the Estimated Weight of 2-itemset Pruning (EW2P) strategy. Extensive experiments are conducted on several real-world and synthetic datasets to evaluate the performance of the two proposed algorithms, and the ratio between the number of generated RWFIs and WFIs. Results show that the proposed algorithms outperform not only the traditional PWA algorithm for WFIM, but also the state-of-the-art RWFIM-P and RWFIM-PE algorithms for RWFIM, in terms of runtime, memory usage and scalability. © 2017, Springer Science+Business Media New York.","EW2P strategy; Recency constraint; Set-enumeration RWF-tree; Temporal database; Weighted frequent itemsets","Artificial intelligence; EW2P strategy; Item sets; Recency constraint; Set-enumeration RWF-tree; Temporal Database; Trees (mathematics)",2-s2.0-85018817124
"Masanobu S., Takano S., Fujiwara T., Kanada S., Ono M., Sasagawa H.","Study on hydraulic transport of large solid particles in inclined pipes for subsea mining",2017,"Journal of Offshore Mechanics and Arctic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019703275&doi=10.1115%2f1.4036385&partnerID=40&md5=5a2536d39eb6152d423206e5be6dcadd","For subsea mining, the prediction of pressure loss due to the hydraulic transport of solid particles in the flexible pipe to connect the mining tool and the lifting system is important for the design of mining system. The configuration of the flexible pipe is expected to have an inclined part. In the present paper, the authors developed a mathematical model to predict the pressure loss in inclined pipes. The total pressure loss is expressed by the summation of the loss due to a liquid single-phase flow and the additional loss due to the existence of solid particles. The additional pressure loss can be divided into the variation in static pressure due to the existence of solid particles, the loss due to the particle-topipe wall friction and collisions, and the loss due to the particle-to-particle collisions. The empirical formula in horizontal pipes proposed by the other researchers was applied to the model of the last two losses. Furthermore, we carried out the experiment on hydraulic transport of solid particles in a pipe. In the experiment, alumina beads, glass beads, and gravel were used as the solid particles, and the inclination angles of the pipe were varied to investigate the effect of the pipe inclination on the pressure loss. The calculated pressure loss using the model was compared with the experimental data. As the results of the comparison, it was confirmed that the developed model could be applied to the prediction of the pressure loss in inclined pipes.",,"Alumina; Forecasting; Empirical formulas; Horizontal pipes; Hydraulic transport; Inclination angles; Particle collision; Pipe inclinations; Single-phase flow; Total-pressure loss; Pipe; aluminum oxide; design method; experimental study; friction; numerical model; pipe; single-phase flow; zenith angle",2-s2.0-85019703275
"Li J., Ma X., Zhang J., Tao J., Wang P., Guan X.","Mining repeating pattern in packet arrivals: Metrics, models, and applications",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018239590&doi=10.1016%2fj.ins.2017.04.033&partnerID=40&md5=61d58ca50636ebd9d90b4f90ae2eeeca","A substantial portion of the network traffic can be attributed to autonomous network applications that experience repeating networking patterns. This observation is further signified by the emergence of the Internet of Things (IoT) era that features an enormous number of networked, autonomous sensors. Identifying and characterizing repeating patterns therefore become a critical means to Internet measurement and traffic engineering. In this paper, we propose a novel method that can effectively identify and characterize timing-based repeating patterns from network traffic by overcoming three significant practical challenges, including i) time-scale sensitive, ii) transience, and iii) being interleaved by noises. Our method features a novel metric, namely unpredictability index (UPI), to capture repeating patterns by quantifying the predictability of packet arrivals’ temporal structure from the perspective of hierarchical clustering. An online approach is further developed to incrementally compute UPI upon observing a single packet. Extensive experiments based on synthetic and real-world data have demonstrated that our method can effectively conduct repeating pattern mining. © 2017","Hierarchical clustering; Repeating pattern; Temporal structure; Traffic modeling","Artificial intelligence; Software engineering; Autonomous networks; Hier-archical clustering; Internet measurement; Internet of thing (IOT); Repeating patterns; Temporal structures; Traffic Engineering; Traffic model; Internet of things",2-s2.0-85018239590
"Morway E.D., Thodal C.E., Marvin-DiPasquale M.","Long-term trends of surface-water mercury and methylmercury concentrations downstream of historic mining within the Carson River watershed",2017,"Environmental Pollution",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027574355&doi=10.1016%2fj.envpol.2017.07.090&partnerID=40&md5=a24986307a250de51b2c534c4fa35251","The Carson River is a vital water resource for local municipalities and migratory birds travelling the Pacific Flyway. Historic mining practices that used mercury (Hg) to extract gold from Comstock Lode ore has left much of the river system heavily contaminated with Hg, a practice that continues in many parts of the world today. Between 1998 and 2013, the United States Geological Survey (USGS) collected and analyzed Carson River water for Hg and methylmercury (MeHg) concentrations resulting in a sixteen year record of unfiltered total mercury (uf.THg), filtered (dissolved) Hg (f.THg), total methylmercury (uf.MeHg), filtered MeHg (f.MeHg), and particulate-bound THg (p.THg) and MeHg (p.MeHg) concentrations. This represents one of the longest continuous records of Hg speciation data for any riverine system, thereby providing a unique opportunity to evaluate long-term trends in concentrations and annual loads. During the period of analysis, uf.THg concentration and load trended downward at rates of −0.85% and −1.8% per year, respectively. Conversely, the f.THg concentration increased at a rate of 1.7% per year between 1998 and 2005, and 4.9% per year between 2005 and 2013. Trends in flow-normalized partition coefficients for both Hg and MeHg suggest a statistically significant shift from the particulate to the filtered phase. The upwardly accelerating f.THg concentration and observed shift from the solid phase to the aqueous phase among the pools of Hg and MeHg within the river water column signals an increased risk of deteriorating ecological conditions in the lower basin with respect to Hg contamination. More broadly, the 16-year trend analysis, completed 140 years after the commencement of major Hg releases to the Carson River, provides a poignant example of the ongoing legacy left behind by gold and silver mining techniques that relied on Hg amalgamation, and a cautionary tale for regions still pursuing the practice in other countries. © 2017",,,2-s2.0-85027574355
"Won M., Mishra A., Son S.H.","HybridBaro: Mining Driving Routes Using Barometer Sensor of Smartphone",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028924904&doi=10.1109%2fJSEN.2017.2734919&partnerID=40&md5=7cc8006c9b1020b1731fe66e8d7a4b47","Recent research showed that human mobility is characterized by reproducible patterns, i.e., humans tend to travel a few known places. Timely identification of these significant journeys has prospects for emerging intelligent applications like real-time traffic route recommendation and automated HVAC systems. Existing mobile systems, however, utilize energy-hungry sensors like GPS and gyroscope to detect significant journeys, which make it hard to keep such systems running to continuously monitor driving routes. To address this issue of energy efficiency without compromising the performance, in this paper, a hybrid mobile system based on the barometer sensor of a smartphone is developed. Distinctive elevation signatures of driving routes are captured using the smartphone barometer sensor that is exceptionally energy-efficient and position/orientation-independent. Degraded accuracy due to flat areas with minimal elevation changes is offset by developing an adaptive algorithm that opportunistically obtains GPS locations for a very short period of time when such flat areas are detected in real time. Using over 150 miles of field data, it is demonstrated that the proposed mobile system achieves the mean detection accuracy of 97% with the mean false positive rates of 1.5%. © 2001-2012 IEEE.","driver information systems; Driving route detection; mobile computing","Adaptive algorithms; Advanced driver assistance systems; Barometers; Climate control; Energy efficiency; Global positioning system; Gyroscopes; Mobile computing; Mobile telecommunication systems; Smartphones; Trajectories; Driver information systems; False positive rates; Intelligent applications; Mobile communications; Real time traffics; Route detections; Sensor systems; Timely identification; Real time systems",2-s2.0-85028924904
"Rawashdeh M., Shorfuzzaman M., Artoli A.M., Shamim Hossain M., Ghoneim A.","Mining tag-clouds to improve social media recommendation",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992147597&doi=10.1007%2fs11042-016-4039-1&partnerID=40&md5=34733d6f7bf4e59c2ede220112e7a752","Massive amounts of data are available on social websites, therefore finding the suitable item is a challenging issue. According to recent social statistics, we have more than 930 million people are using WhatsApp with more than 340 million active daily users and 955 million people who access Facebook daily with an average daily photo uploads up to 325 million. The approach presented in this paper employs the collaborative tagging accumulated by huge number of users to improve social media recommendation. Our approach has two phases, in the first phase, we compute the tag-item weight model and in the second phase, we compute the user-tag preference model. After that we employ the two models to find the suitable items tailored to the user’s preferences and recommend the items with the highest score. Also our model can compute the tag score and suggest the tags with the highest weight to the user according to their preferences. The experiment results performed on Flicker and MovieLens prove that our approach is capable to improve the social media recommendation. © 2016, Springer Science+Business Media New York.","Annotation; Collaborative tagging; Recommendation; Social tagging","User interfaces; Annotation; Collaborative tagging; Item weights; Preference modeling; Recommendation; Second phase; Social media; Social tagging; Social networking (online)",2-s2.0-84992147597
"Kordopatis-Zilos G., Papadopoulos S., Kompatsiaris I.","Geotagging Text Content with Language Models and Feature Mining",2017,"Proceedings of the IEEE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028966057&doi=10.1109%2fJPROC.2017.2688799&partnerID=40&md5=79f22b847c2b2ef8bb6272a2505b1132","The large-scale availability of user-generated content in social media platforms has recently opened up new possibilities for studying and understanding the geospatial aspects of real-world phenomena and events. Yet, the large majority of user-generated content lacks proper geographic information (in the form of latitude and longitude coordinates). As a result, the problem of multimedia geotagging, i.e., extracting location information from user-generated text items when this is not explicitly available, has attracted increasing research interest. Here, we present a highly accurate geotagging approach for estimating the locations alluded by text annotations based on refined language models that are learned from massive corpora of social media annotations. We further explore the impact of different feature selection and weighting techniques on the performance of the approach. In terms of evaluation, we employ a large benchmark collection from the MediaEval Placing Task over several years. We demonstrate the consistently superior geotagging accuracy and low median distance error of the proposed approach using various data sets and comparing it against a number of state-of-the-art systems. © 1963-2012 IEEE.","Feature selection; geolocation; geotagging; language model; location estimation","Computational linguistics; Feature extraction; Flow visualization; Location; Multimedia systems; Personnel training; Reliability; Social networking (online); Geo-tagging; Geolocations; Language model; Location estimation; Multi-media communications; Twitter; User-generated content; Geographic information systems",2-s2.0-85028966057
"Varadarajan V., Gupta S., Gupta A.","Variations in the secondary xylem of hardwood trees growing in the oldest iron ore mines of Odisha, India",2017,"Trees - Structure and Function",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019588749&doi=10.1007%2fs00468-017-1562-2&partnerID=40&md5=ca04ea975f2514f98196fe344808021b","Key message: The study gives an insight into the effect of the stress produced due to an anthropogenic activity like mining on the secondary xylem, which is one of the most conservative features of plants. Abstract: The anatomy of the wood of mature trees is supposed to be conservative in nature though variations in them can occur due to the possible stressful conditions around the tree. This novel study is concerned with the effect of mining activities on the wood microscopic features of hardwood trees in and around the forest areas of the largest and oldest active open-cast iron ore mines of India. The work was carried out with systematic sampling from trees belonging to 22 different genera and species. The data of the microstructure analyzed using different statistical tools, such as Student’s t test, ANOVA, and Fisher’s exact test, revealed that there were quite significant variations in the quantitative features of the elements of secondary xylem, though the qualitative features remained more or less the same, with very few significant variations as compared to the control samples of the same species. Estimation of the specific gravity and the iron element content in the stressed and control wood samples was also carried out. The exhaustive results obtained led to the conclusion that Buchanania lanzan was the species which was found to be the sturdiest in the stressed conditions, and on the other hand, the most dominant species of the locality, Shorea robusta, showed the maximum number of variations in its secondary xylem. © 2017, Springer-Verlag Berlin Heidelberg.","Conservative; Hardwood trees; Iron content; Mining activities; Specific gravity; Statistical tools; Variations","Cast iron; Density (specific gravity); Forestry; Iron; Iron mines; Iron ores; Statistical mechanics; Conservative; Iron content; Mining activities; Statistical tools; Variations; Hardwoods; Forestry; Hardwoods; India; Mining; Sampling; Xylem",2-s2.0-85019588749
"Larmat C., Maceira M., Higdon D.M., Anderson D.N.","Joining statistics and geophysics for assessment and uncertainty quantification of three-dimensional seismic Earth models",2017,"Statistical Analysis and Data Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029627988&doi=10.1002%2fsam.11353&partnerID=40&md5=84bf1b794e53cf5059790d32b6052689","Seismic inversions produce seismic models, which are 3-dimensional (3D) images of wave velocity of the entire planet retrieved by fitting seismic measurements made on records of past earthquakes or other seismic events. Computing power of the TeraFlop era, along with the dataflow from new, very dense, seismic arrays, has led to a new generation of 3D seismic Earth models with an unprecedented level of resolution. Here we compare two recent models of western United States from the Dynamic North America (DNA) seismic imaging effort. The two models only differ in the wave propagation that was used for their inversion: one is based on ray theory (RT), and the other on finite frequency (FF). We evaluate the two models using an independent numerical method and statistical tests. We show that they differ in how they produce seismic signals from a subset of earthquakes that were used in the original inversion and were recorded on the US array. This is especially true for measurements done in the Yellowstone area which has a large negative seismic anomaly. This result is of importance for seismologists who have been debating on the practical benefit of using FF in ill-posed Earth inversions. Model evaluation, such as the one reported here, represents an opportunity for collaboration between geophysical and statistical communities. More opportunities should arise with the upcoming Exascale era, which will provide enough computational power to explore together several sources of errors in models with thousands of parameters, opening the way of uncertainty quantification of seismic models. © 2017 The Authors. Statistical Analysis and Data Mining: The ASA Data Science Journal published by Wiley Periodicals, Inc.","permutation test; seismic models; seismology; spectral element method; t-test; uncertainty quantification","Computation theory; Data flow analysis; Geophysics; Numerical methods; Seismology; Uncertainty analysis; Wave propagation; Permutation tests; Seismic model; Spectral element method; T-tests; Uncertainty quantifications; Earthquakes",2-s2.0-85029627988
"Choi B., Kim S.P., Hwang S., Hwang J., Yang C.H., Lee S.","Metabolic characterization in urine and hair from a rat model of methamphetamine self-administration using LC-QTOF-MS-based metabolomics",2017,"Metabolomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028972958&doi=10.1007%2fs11306-017-1257-0&partnerID=40&md5=5260fdbf3e931dce8262e31dfc474789","Introduction: Metabolomics is a powerful tool for understanding the comprehensive changes in metabolic responses associated with specific phenotypes. However, it has limitations with regards to data mining and metabolite assignment. Methamphetamine addiction is a critical issue owing to high potential for recurrence and lack of effective pharmacotherapy. However, the biological basis for methamphetamine addiction is not fully understood and no specific biomarkers have been identified. Objectives: In the present study, the metabolic alterations in rat urine and hair by methamphetamine addiction were evaluated using a methamphetamine self-administration model with LC-QTOF-MS-based metabolomics. Methods: Firstly, an in-house database for 474 (MS mode) and 404 (MS/MS mode) metabolites, based on data on the multi-adduct formation and their fragmentation, was established using both positive and negative electrospray ionization (ESI) modes. Secondly, the metabolic characteristics of rat urine and hair were investigated before and after methamphetamine addiction. Results: By multivariate statistical analysis, a clear clustering of samples collected before and after methamphetamine addiction was achieved. Fourteen (positive ESI) and thirty (negative ESI) ion features were altered in rat urine during methamphetamine addiction and extinction and those features were classified as potential markers for methamphetamine addiction and exposure. In rat hair, a total of 103 ion features for positive and 18 for negative ESI, including functional metabolites such as fatty acid amides and carnitines, were significantly changed. Hair was proposed as a more reliable diagnostic specimen for methamphetamine addiction. Conclusions: These findings provide a description of the metabolic alterations caused by methamphetamine addiction and will enable further studies to discovery of related diagnostic or prognostic markers. © 2017, Springer Science+Business Media, LLC.","Addiction; LC-QTOF-MS; Metabolomics; Methamphetamine; Self-administration","1 methyladenosine; 5 methylcytidine; amino acid; betaine; cytidine derivative; deoxycorticosterone; hippuric acid; lumichrome; methamphetamine; norvaline; oleamide; palmitoylcarnitine; sulfacetamide; unclassified drug; valine; animal experiment; animal model; Article; data base; drug self administration; electrospray; hair; information processing; liquid chromatography; male; mass spectrometer; metabolomics; methamphetamine dependence; nonhuman; place preference; principal component analysis; rat; time of flight mass spectrometry; transesterification; urinalysis",2-s2.0-85028972958
"Perez-Alonso D., Peña-Tejedor S., Navarro M., Rad C., Arnaiz-González Á., Díez-Pastor J.-F.","Decision Trees for the prediction of environmental and agronomic effects of the use of Compost of Sewage Slugde (CSS)",2017,"Sustainable Production and Consumption",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028507722&doi=10.1016%2fj.spc.2017.07.001&partnerID=40&md5=36dac28e359148b6d7c3c42909e1d253","The use of biosolids for soil improvement and for the reduction of inorganic fertilization costs has been a common practice in recent decades and is being used more and more often as inorganic fertilization cost increases. This practice is useful because it can be effective for the recovery of low fertility soils and to recycle urban and industrial waste, but it can also have negative effects. Some components of biosolids, like heavy metals, can have a potential hazard on human or animal health if they reach the edible part of the plant. In addition, there are no mathematical models able to predict both, the increases in yield and quality of the crops by the input of nutrients or the hazards due to the heavy metals incorporated into the soil. Data mining allows the creation of predictive models and in addition, some techniques, such as Decision trees, allow the generation of fast and interpretable models. The dataset used in this study come from an experimental field located in Burgos (Central-Northern Spain), in which, additions of Compost of Sewage Sludge (CSS) have been applied alternatively for 6 years. The main consequence of this application in terms of nutrients is the increase in available phosphorus mainly with the addition of the highest doses of CSS. There have been found correlations between the heavy metal contents in soil and in plant tissues described by Decision trees that are refuted by numerous publications using other classical statistical methodologies. Decision trees make it easy to interpret predictions, opening a new path for interpreting complex datasets commonly present in mid- or long-term agronomic experiences. © 2017 Institution of Chemical Engineers","Compost of sewage sludge (CSS); Data pruning; Heavy metals; Regression tree; Soil amendment; Soil–plant system",,2-s2.0-85028507722
"Foli G., Gawu S.K.Y.","Modified acid–base accounting model validation and pH buffer trend characterisation in mine drainage at the AngloGold Ashanti Obuasi mine in Ghana, West Africa",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031303888&doi=10.1007%2fs12665-017-7005-y&partnerID=40&md5=8b0f04d7bebe65a50eb95bb4c870d79b","Acid–base accounting (ABA) is a static test used to evaluate pre-mining drainage quality of ores with interpretations based on a reference 3-data point model. The method is often complemented with a kinetic test to ensure certainty of results. The challenges associated with both methods compel companies to rely on only the ABA test, thereby compromising on the long-term drainage quality. This paper validates a proposed 4-data point model that was used to establish a 20% increase in the alkaline amendment of ores at the AngloGold Ashanti Obuasi Mine in Ghana. The validation was done using model limits, the robustness of coefficient of determination and model factor sequence variation. Acidification trends and mineralogical data evaluation of tailings were used to characterise pH buffer trends in mine drainage. The modified 4-data point model, which incorporates a vital kinetic test factor into the ABA model, provides a criterion for the adjustment of carbonate amendment value to improve acid neutralisation in the drainage; this would reduce (1) cost of experimentation, (2) turnaround time for analyses, (3) complexities associated with both test methods. From the XRD data, alunite and goethite are present in tailings to provide sustained pH buffering in drainage beyond the scope of the modified model, while the characterised pH buffer trend could be used for monitoring drainage quality. © 2017, Springer-Verlag GmbH Germany.","Acidification; Decommissioned tailing dam; Model modification; Neutralisation; Secondary minerals","Acidification; Alkalinity; Drainage; pH effects; Tailings; Testing; Anglogold ashantis; Coefficient of determination; Data evaluation; Model modification; Model validation; Neutralisation; Sequence variations; Tailing dam; Quality control",2-s2.0-85031303888
"Zhao Y., Yang T., Zhang P., Zhou J., Yu Q., Deng W.","The analysis of rock damage process based on the microseismic monitoring and numerical simulations",2017,"Tunnelling and Underground Space Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020453573&doi=10.1016%2fj.tust.2017.06.002&partnerID=40&md5=12c69347c715cfed4564bacff4e1ce87","In mining process, the mechanical parameters of rock mass are deteriorating continuously. Fixed mechanical parameters were usually selected when using numerical simulation to analyze the damage process of rock mass, which led to the simulation results difficult to meet the actual situation. Microseismic monitoring system is an effective means to monitor the evolution of rock mass damage. It is of great significance to effectively combine the monitoring data with numerical simulation to correct rock mass parameters. In order to study the relationship between source parameters and rock mass damage, microseismic events of No. 15–16 exploratory line were analyzed from the project of Shirengou iron mine. Location verification was carried out on the established microseismic monitoring system and the microseismic events with larger errors were eliminated. The damage and failure characteristics of rock mass in this area were preliminarily analyzed based on the analysis of the temporal and spatial change of microseismic events and source parameters. In addition, based on the study of isotropic releasable strain energy, the damage model of rock mass was established by using the relationship between source parameters and releasable strain energy. The microseismic monitoring results were used as input of numerical simulation through FISH language existing in FLAC3D. Source parameters were used to modify rock mechanical parameters, so as to analyze the damage process of rock mass. Through the analysis of microseismic activity, the damage degree of rock mass and the change of plastic zone, the unstable areas were located, furthermore corresponding preventive measures were put forward. © 2017 Elsevier Ltd","Damage model; Microseismic monitoring; Numerical simulation; Source parameters","Computer simulation; Computer simulation languages; Monitoring; Numerical models; Rock mechanics; Rocks; Seismology; Strain energy; Damage model; Location verification; Mechanical parameters; Preventive measures; Rock mass parameters; Rock mechanical parameters; Source parameters; Temporal and spatial changes; Microseismic monitoring; damage mechanics; induced seismicity; mining; monitoring system; numerical method; rock mass response; source parameters",2-s2.0-85020453573
"Qi S., Sacharidis D., Bouros P., Mamoulis N.","Snapshot and continuous points-based trajectory search",2017,"GeoInformatica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982313543&doi=10.1007%2fs10707-016-0267-9&partnerID=40&md5=a6c0e854f64e1bb8c581361a2eb593f7","Trajectory data capture the traveling history of moving objects such as people or vehicles. With the proliferation of GPS and tracking technologies, huge volumes of trajectories are rapidly generated and collected. Under this, applications such as route recommendation and traveling behavior mining call for efficient trajectory retrieval. In this paper, we first focus on distance-to-points trajectory search; given a collection of trajectories and a set query points, the goal is to retrieve the top-k trajectories that pass as close as possible to all query points. We advance the state-of-the-art by combining existing approaches to a hybrid nearest neighbor-based method while also proposing an alternative, more efficient spatial range-based approach. Second, we investigate the continuous counterpart of distance-to-points trajectory search where the query is long-standing and the set of returned trajectories needs to be maintained whenever updates occur to the query and/or the data. Third, we propose and study two practical variants of distance-to-points trajectory search, which take into account the temporal characteristics of the searched trajectories. Through an extensive experimental analysis with real trajectory data, we show that our range-based approach outperforms previous methods by at least one order of magnitude for the snapshot and up to several times for the continuous version of the queries. © 2016, Springer Science+Business Media New York.","Continuous queries; Spatial proximity; Trajectory search","Information systems; Planning; Continuous queries; Experimental analysis; Nearest neighbors; Real trajectories; Spatial proximity; Temporal characteristics; Tracking technology; Trajectory searches; Trajectories; GPS; spatial data; tracking; trajectory",2-s2.0-84982313543
"Sastry G.S., Ganesha Raj K., Paul M.A., Dhinwa P.S., Sastry K.L.N.","Desertification Vulnerability Assessment Model for a Resource Rich Region: A Case Study of Bellary District, Karnataka, India",2017,"Journal of the Indian Society of Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006351513&doi=10.1007%2fs12524-016-0641-y&partnerID=40&md5=9d2d82b82083779855964da1d691f1c1","Desertification is a global challenge being experienced across countries irrespective of their levels of development. Desertification is a complex negative process involving both natural and human components in terms of their socio-economic attainments. Hence, for identification and assessment of the process, pattern, magnitude and possible impacts of desertification, a multi-disciplinary approach with inter-disciplinary framework of analysis is essential. This study has made such an attempt to develop a comprehensive desertification vulnerability assessment Model on the basis of multi-variate Principal Component Analysis along with the Geographic Information System framework by using natural and socio-economic resources data inputs from census, satellite data and other sources. Bellary district, located in a rapidly growing southern state of India, Karnataka which is afflicted with various natural and development issues such as droughts, backwardness, haphazard mining, over irrigation, and associated effects of land degradation, siltation and water pollution has been chosen for the study. The inter-disciplinary framework based desertification vulnerability assessment model has assessed that 1379.198 km2 area (15.55%) of Bellary district is prone to desertification (based on the satellite data IRS LISS III data of Dec 2005, Feb 2006, March 2006 and April 2006). In addition, 3229.337 km2 (36.40%) is under moderate vulnerability which is fragile. Hence, unless proper development intervention and conservation measures are taken well in advance, almost more than half of Bellary district (51.95%) will be vulnerable to desertification. Spatially, the talukas that are seriously affected and that require development intervention on high priority are: Sandur, Kudligi, Hospet and Bellary which are the prime talukas of the district. © 2016, Indian Society of Remote Sensing.","Complex methodology; Delineation; Desertification; GIS framework; Inter-disciplinary framework; Multi-disciplinary approach; Vulnerability Assessment Model","complexity; desertification; GIS; hazard assessment; interdisciplinary approach; methodology; principal component analysis; satellite data; socioeconomic conditions; vulnerability; water pollution; Bellary; India; Karnataka",2-s2.0-85006351513
"Alkan R.M., Saka M.H., Ozulu İ.M., İlçi V.","Kinematic precise point positioning using GPS and GLONASS measurements in marine environments",2017,"Measurement: Journal of the International Measurement Confederation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019966294&doi=10.1016%2fj.measurement.2017.05.054&partnerID=40&md5=b7f40152dc71539b50d2fb2c33778492","The Precise Point Positioning (PPP) technique has received increased attention in recent years. When employing PPP, users need only a single GNSS receiver data to obtain a cm to dm accuracy level on a global datum, both in static and kinematic modes. This technique has become more popular due to its easy use, simple field operations, no base station(s) requirement, and provides cost effective high accuracy positioning. In satellite-based positioning, the accuracy and reliability of the positioning results are strongly dependent on the number and geometry of visible satellites, and quality of the observations. This issue is more important when the measurements are conducted in environments where satellite signals are blocked or degraded in some places like narrow channels, ravines, congested harbors, coastal areas with intense urbanization, inland waterways, in waters having severe terrain obstructions or surrounded by high mountains. To overcome this limitation, a combination of GPS and GLONASS measurements are proposed, especially to achieve more reliable and accurate solutions by increasing the number of visible satellites. The additional satellite system's observations like GLONASS, are expected to enhance the positioning accuracy and solution availability where especially not enough numbers of GPS satellites are visible. As of today, precise orbits and clock data are available not only for GPS but also for GLONASS. This provides an opportunity to apply the PPP technique to GPS and GLONASS observations. In this study, performance of the PPP method is assessed in a dynamic environment using GPS-only and GPS + GLONASS observations. Accordingly, a kinematic test is carried out at the Obruk Lake Dam in the Çorum province, of Turkey. The collected data is processed using a globally popular on-line processing service: the Canadian Spatial Reference System-Precise Point Positioning Service (CSRS-PPP) operated by the Geodetic Survey Division of Natural Resources Canada (NRCan). The results show that the combined GPS + GLONASS data produced almost the same level of accuracy with the GPS-only data if there is a sufficient number of GPS satellites with good geometry. In the extreme cases, such as the elevation of satellites above 40 degrees, the GPS cannot provide a solution. However, when data and both satellites systems are combined, it is then possible to provide the solution. The results also reveal that a dm-level of accuracy can be achieved with the PPP technique in a dynamic environment. This accuracy level largely meets the requirements of many marine applications, including precise hydrographic surveying, marine geodesy, navigation and oceanography. © 2017 Elsevier Ltd","GLONASS; GPS; Kinematic measurement; On-line PPP service; PPP","Cost effectiveness; Geodesy; Geodetic satellites; Inland waterways; Kinematics; Marine applications; Marine navigation; Orbits; Satellite navigation aids; Satellites; Solution mining; Surveying; Tracking (position); Underwater acoustics; Canadian spatial reference systems; Geodetic survey divisions; GLONASS; High-accuracy positioning; Kinematic measurements; Kinematic precise point positioning; On-line PPP service; Precise point positioning; Global positioning system",2-s2.0-85019966294
"Yang R., Su L., Zhao X., Wan H., Sun J.","Representative band selection for hyperspectral image classification",2017,"Journal of Visual Communication and Image Representation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018355830&doi=10.1016%2fj.jvcir.2017.02.002&partnerID=40&md5=e790bfb3d3acc9a6fc9ea00c4fe3883b","High dimensional curse for hyperspectral images is one major challenge in image classification. In this work, we introduce a novel spectral band selection method by representative band mining. In the proposed method, the distance between two spectral bands is measured by using disjoint information. For band selection, all spectral bands are first grouped into clusters, and representative bands are selected from these clusters. Different from existing clustering-based band selection methods which select bands from each cluster individually, the proposed method aims to select representative bands simultaneously by exploring the relationship among all band clusters. The optimal representative band selection is based on the criteria of minimizing the distance inside each cluster and maximizing the distance among different representative bands. These selected bands can be further applied in hyperspectral image classification. Experiments are conducted on the 92AV3C Indian Pine data set. Experimental results show that the disjoint information-based spectral band distance measure is effective and the proposed representative band selection approach outperforms state-of-the-art methods for high dimensional image classification. © 2017 Elsevier Inc.","Band selection; Disjoint information; Feature selection; High dimensional image; Pattern recognition","Classification (of information); Feature extraction; Independent component analysis; Pattern recognition; Spectroscopy; Band selection; Data set; Disjoint information; Distance measure; High-dimensional; High-dimensional images; Spectral band; State-of-the-art methods; Image classification",2-s2.0-85018355830
"Mertens F., Távora R., Nakano E.Y., Castilhos Z.C.","Information sources, awareness and preventive health behaviors in a population at risk of Arsenic exposure: The role of gender and social networks",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030997299&doi=10.1371%2fjournal.pone.0186130&partnerID=40&md5=586fee12f8113cf43989f7a9dc061c07","The population of Paracatu is at risk of Arsenic (As) exposure associated with long-term exploration of the largest open pit gold mine in Brazil. As part of the interdisciplinary research “The Paracatu project: Arsenic environmental contamination and human health risks assessment in Paracatu-MG”, carried out between 2011 and 2013, we used data disaggregated by gender to identify the sources of As-related information being accessed by inhabitants of Paracatu and to examine if access to these sources was correlated to awareness of As health effects and adoption of behaviors to reduce risk of As exposure. Semi-structured, face-to-face interviews were carried out with 460 participants (294 women and 166 men) to collect data on respondent’s socio-demographic characteristics, use of mass media and social communication networks as sources of information on As issues, the trustworthiness of these information sources, awareness of As health effects, and adoption of behaviors to reduce As exposure. For both men and women, interpersonal communication was used and trusted more frequently than mass media to obtain information on As. Discussion of As issues occurred preferentially among individuals of the same gender and was associated with awareness of As health risks. There are marked differences in variables correlated with the adoption of behaviors to reduce the risk of As exposure between men and women. Discussing As issues with women was associated with adoption of risk-reduction practices for both genders. In contrast, men who discuss As issues with other men were less likely to adopt As exposure prevention behaviors. Finally, adoption was associated with awareness of As health effects for women, but this was not the case for men. Policy implications for decision makers, practitioners and researchers are discussed, based on concrete examples of how gender-specific approaches can effectively guide the formulation and implementation of health promotion campaigns and programs. © 2017 Mertens et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"arsenic; arsenic; adult; aged; Article; awareness; Brazil; controlled study; environmental exposure; female; gender; health behavior; health hazard; health promotion; human; interpersonal communication; male; mass medium; mining; pollution; risk assessment; risk reduction; semi structured interview; social network; health behavior; mass medium; risk factor; Arsenic; Awareness; Brazil; Communication; Environmental Exposure; Female; Health Behavior; Humans; Male; Mass Media; Risk Factors; Social Networking",2-s2.0-85030997299
"Chen S., Xu Y., Zheng C., Ke L.-X.","Marivibrio halodurans gen. nov., sp. nov., a marine bacterium in the family Rhodospirillaceae isolated from underground rock salt",2017,"International Journal of Systematic and Evolutionary Microbiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774790&doi=10.1099%2fijsem.0.002309&partnerID=40&md5=2454b8e9bbb31704d40ba05f4aa4767f","Gram-negative, spiral or curved rod-shaped cells of a bacterial strain, designated ZC80T, were isolated from a rock salt sample collected at Yunnan salt mine, China. Analysis of the strain's 16S rRNA gene sequence revealed a clear affiliation of this novel strain within the family Rhodospirillaceae. Strain ZC80T formed a robust cluster with Pelagibius litoralis CL-UU02T at a 16S rRNA gene sequence similarity level of 88.1 %. Strain ZC80T shared no more than 91.0 % 16S rRNA gene sequence similarity with the type strains of other species in the family Rhodospirillaceae. Strain ZC80T was able to grow in the presence of 2–15 % (w/v) NaCl, and grew at 10–50 °C and pH 6.0–10.0. The major fatty acids were C19: 0 cyclo ω8c (41.3 %). The major isoprenoid quinone was ubiquinone 10 (Q-10). The major polar lipids were phosphatidylglycerol, phosphatidylethanolamine, diphosphatidylglycerol and an unidentified aminolipid. The DNA G+C content of strain ZC80T was 60.8 mol%. On the basis of phylogenetic analyses and chemotaxonomic and physiological data, strain ZC80T is considered to represent a novel species of a new genus in the family Rhodospirillaceae, for which the name Marivibrio halodurans gen. nov., sp. nov. is proposed. The type strain of Marivibrio halodurans is ZC80T (=CGMCC 1.15697T=NBRC 112461T). © 2017 IUMS.","Hypersaline environment; Proteobacteria; Rhodospirillaceae; Rock salt; Salt deposit; Salt mine","cardiolipin; fatty acid; lipid; phosphatidylethanolamine; phosphatidylglycerol; RNA 16S; sodium chloride; ubidecarenone; bacterial DNA; fatty acid; RNA 16S; ubiquinone; Ubiquinone Q2; Article; bacterial growth; bacterial strain; bacterium isolation; controlled study; DNA base composition; gene sequence; Gram negative bacterium; Marivibrio halodurans; nonhuman; nucleotide sequence; Pelagibius litoralis; pH; phylogeny; priority journal; Rhodospirillaceae; type strain; bacterium identification; chemistry; China; classification; DNA sequence; genetics; isolation and purification; mining; phylogeny; Rhodospirillaceae; Bacterial Typing Techniques; Base Composition; China; DNA, Bacterial; Fatty Acids; Mining; Phylogeny; Rhodospirillaceae; RNA, Ribosomal, 16S; Sequence Analysis, DNA; Sodium Chloride; Ubiquinone",2-s2.0-85031774790
"Mehrdad A., Parvini E.","Conductometry and Density Functional Theory studies on the interactions of sodium polystyrenesulfonate with 1–butyl–3–methylimidazolium bromide in aqueous solution",2017,"Journal of Molecular Liquids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027446060&doi=10.1016%2fj.molliq.2017.08.056&partnerID=40&md5=c3532347ce5bbf709503a2102f7ade86","Electrical conductance of sodium polystyrene sulfonate (NaPSS) have been measured in aqueous solutions of 1–butyl–3–methylimidazolium bromide ([BMIm]Br) at three different temperatures T = (288.15, 298.15 and 308.15) K. The scaling theory is applied for the explanation of electrical conductance of polyelectrolytes. From the obtained results can be seen that the fraction of uncondensed counterions is decreased by increasing temperature and concentration of [BMIm]Br. Also, conductivity measurements of 1–butyl–3–methylimidazolium bromide were carried out in aqueous solution of sodium polystyrene sulfonate. Data analysis was performed by the Quint-Viallard (QV) conductivity equation and low concentration chemical model (lcCM). Limiting molar conductivities of [BMIm]Br (Λ0) and the association constant (KA) were determined. Molar conductivity of [BMIm]Br in aqueous solutions of NaPSS was increased with increasing temperature. The values of activation energy for viscous flow and the values of activation enthalpy of charge transport were calculated. The results of Quantum chemical calculations were confirmed the existence of hydrogen bonding between [BMIm]+ and [PSS]−. © 2017 Elsevier B.V.","1–Butyl–3–methylimidazolium bromide; Conductivity; Density Functional Theory; Sodium polystyrene sulfonate","Activation energy; Chemical activation; Chemical analysis; Chemical bonds; Electric conductance; Electric conductivity; Hydrogen bonds; Polyelectrolytes; Polystyrenes; Quantum chemistry; Sodium; Solution mining; Solutions; Temperature; Conductivity equations; Conductivity measurements; Density functional theory studies; Electrical conductance; Increasing temperatures; Limiting molar conductivity; Quantum chemical calculations; Sodium polystyrene sulfonate; Density functional theory",2-s2.0-85027446060
"Dong Z., Wang R., Fan M., Fu X.","Switching and optimizing control for coal flotation process based on a hybrid model",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031816085&doi=10.1371%2fjournal.pone.0186553&partnerID=40&md5=1fab0268c4f1e390fa08bb7588e3e2d4","Flotation is an important part of coal preparation, and the flotation column is widely applied as efficient flotation equipment. This process is complex and affected by many factors, with the froth depth and reagent dosage being two of the most important and frequently manipulated variables. This paper proposes a new method of switching and optimizing control for the coal flotation process. A hybrid model is built and evaluated using industrial data. First, wavelet analysis and principal component analysis (PCA) are applied for signal pre-processing. Second, a control model for optimizing the set point of the froth depth is constructed based on fuzzy control, and a control model is designed to optimize the reagent dosages based on expert system. Finally, the least squares-support vector machine (LS-SVM) is used to identify the operating conditions of the flotation process and to select one of the two models (froth depth or reagent dosage) for subsequent operation according to the condition parameters. The hybrid model is developed and evaluated on an industrial coal flotation column and exhibits satisfactory performance. © 2017 Dong et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"controlled study; expert system; flotation; human; least square analysis; principal component analysis; support vector machine; wavelet analysis; coal mining; fractionation; procedures; support vector machine; Chemical Fractionation; Coal Mining; Humans; Principal Component Analysis; Support Vector Machine; Wavelet Analysis",2-s2.0-85031816085
"Pierron X., Williams I.D., Shaw P.J., Cleaver V.","Using choice architecture to exploit a university Distinct Urban Mine",2017,"Waste Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021327033&doi=10.1016%2fj.wasman.2017.06.034&partnerID=40&md5=af01e29901662ee64cd5e293f715cd60","There are widespread concerns regarding the potential future scarcity of ferrous and non-ferrous materials. However, there are already potentially rich reserves of secondary materials via high ownership of Electrical and Electronic Equipment (EEE) in economically-developed nations. Young people are particularly high consumers of EEE, thus university students and campuses may present an opportunity to harness this potential. University Distinct Urban Mines (DUM) may be used to exemplify how potential reserves of secondary metals may be exploited, and could contribute to the transition from a linear to a circular economy. This study aimed to evaluate small household appliances (SHA) DUM from a UK university, with the objectives to identify and quantify student households’ SHA ownership, WEEE recycling, stockpiling and discarding habits amongst student households, assess and evaluate the monetary potential of SHA DUM at UK level, and propose methods to exploit DUM for universities in the UK. To this purpose, a quantitative survey was undertaken to measure students’ ownership and discarding behaviour with respect to SHA. The amounts of ferrous and non-ferrous materials were then estimated and converted to monetary values from secondary materials market data to appraise the SHA DUM overall value. Thirty-five per cent of SHA are discarded in the general refuse. Broken personal care appliances (PCA) tend to be discarded due to hygiene and small size factors. When in working order, SHA tend to be equally reused, recycled or stockpiled. We conclude that a total of 189 tonnes of ferrous and non-ferrous materials were available via discarding or being stockpiled at the University of Southampton. Extrapolated to UK higher education level, discarded and stockpiled SHA represent a potential worth ∼USD 11 million. To initiate DUM exploitation within Higher Education campuses, we suggest improving users’ choice architecture by providing collection methods specific to broken SHA. © 2017","Choice architecture; Distinct urban mine; Monetary potential; Secondary materials; University students; WEEE","Architecture; Domestic appliances; Economics; Education; Electronic equipment; Iron; Oscillators (electronic); Recycling; Electrical and electronic equipment; Monetary potential; Nonferrous materials; Secondary materials; Secondary materials market; University of Southampton; University students; WEEE; Students; student; survey; university sector; urban area; young population; architecture; decision making; distinct urban mine; domestic waste; environmental planning; household; Kruskal Wallis test; landfill; mining; organization and management; personality; priority journal; quantitative analysis; recycling; Review; university student; urban area; waste management; United Kingdom",2-s2.0-85021327033
"Taylor N.","Realised variance forecasting under Box-Cox transformations",2017,"International Journal of Forecasting",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020706002&doi=10.1016%2fj.ijforecast.2017.04.001&partnerID=40&md5=0daacc1eb2322f1dc96387909e700be9","This paper assesses the benefits of modeling Box-Cox transformed realised variance data. In particular, it examines the quality of realised variance forecasts with and without this transformation applied in an out-of-sample forecasting competition. Using various realised variance measures, data transformations, volatility models and assessment methods, and controlling for data mining issues, the results indicate that data transformations can be economically and statistically significant. Moreover, the quartic root transformation appears to be the most effective in this regard. The conditions under which the use of transformed data is effective are identified. © 2017 International Institute of Forecasters","Forecasting competitions; Loss function; Reality check; Risk; Volatility",,2-s2.0-85020706002
"Deeter A., Dalman M., Haddad J., Duan Z.-H.","Inferring gene and protein interactions using PubMed citations and consensus Bayesian networks",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031822122&doi=10.1371%2fjournal.pone.0186004&partnerID=40&md5=763f2b4984ccd7f52f88f325d13c3603","The PubMed database offers an extensive set of publication data that can be useful, yet inherently complex to use without automated computational techniques. Data repositories such as the Genomic Data Commons (GDC) and the Gene Expression Omnibus (GEO) offer experimental data storage and retrieval as well as curated gene expression profiles. Genetic interaction databases, including Reactome and Ingenuity Pathway Analysis, offer pathway and experiment data analysis using data curated from these publications and data repositories. We have created a method to generate and analyze consensus networks, inferring potential gene interactions, using large numbers of Bayesian networks generated by data mining publications in the PubMed database. Through the concept of network resolution, these consensus networks can be tailored to represent possible genetic interactions. We designed a set of experiments to confirm that our method is stable across variation in both sample and topological input sizes. Using gene product interactions from the KEGG pathway database and data mining PubMed publication abstracts, we verify that regardless of the network resolution or the inferred consensus network, our method is capable of inferring meaningful gene interactions through consensus Bayesian network generation with multiple, randomized topological orderings. Our method can not only confirm the existence of currently accepted interactions, but has the potential to hypothesize new ones as well. We show our method confirms the existence of known gene interactions such as JAKSTAT-PI3K-AKT-mTOR, infers novel gene interactions such as RAS- Bcl-2 and RAS-AKT, and found significant pathway-pathway interactions between the JAK-STAT signaling and Cardiac Muscle Contraction KEGG pathways. © 2017 Deeter et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,2-s2.0-85031822122
"Yue F., Wang G.","COSDF: A novel method for software defect detection based on co-training and SMOTE with density based noise filtering strategy",2017,"ICIC Express Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030110235&partnerID=40&md5=ed6648b31da468bc74ce6207e7de4110","In recent years many machine learning and data mining methods have been proposed for software defect detection. However, the datasets of software defect detection are often imbalanced and only a small portion of instances are labeled in reality. In this paper, considering these practical issues simultaneously, a novel software defect detection method, COSDF, was proposed based on co-training and SMOTE. At the same time, in order to avoid introducing noise from the synthetic instances in SMOTE or new labeled instances in co-training, density based noise filtering strategy is used in the research. Experimental results on six public real-world datasets show that among the compared methods, COSDF gets the best result and COSDF is a potential solution for software defect detection. © 2017.","Co-training; COSDF; Density; Noise filtering; SMOTE; Software defect detection",,2-s2.0-85030110235
"Song J., Liang H., Hong W.","Discretization algorithm based on the minimum gini index and optimization on formal context",2017,"ICIC Express Letters, Part B: Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030213359&partnerID=40&md5=2cc1188073f1e83029334643a575d9cc","The Formal Concept Analysis (FCA) is one of the essential tools to data mining and knowledge discovery; however, the formal context needed during the data processing is binary. Therefore, finding a proper and effective method to scatter the data is playing an essential role to the precision of pattern classification and the generation of formal context. In this paper, we proposed a discretization algorithm based on the minimum Gini index and the optimization on formal context, which realized the function that deals with continuous data in information system and extracts the formal context that can be used in FCA. Firstly, the Gini index of each potential split point is defined, on which the rule of importance to the split points is based. Then the formal contexts are sent to the random forest classifier to calculate the mean error rate. Meanwhile, we compared the method we proposed with other two methods by using the standard data from UCI database. The result shows that the precision of the method we proposed is better than that of the other two methods generally. It is proven that the algorithm is effective. © 2017 ICIC International.","Discretization; FCA; Gini index; Optimization algorithm",,2-s2.0-85030213359
"Tang Y., Xiao Y.","Learning fuzzy semantic cell by principles of maximum coverage, maximum specificity, and maximum fuzzy entropy of vague concept",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021854248&doi=10.1016%2fj.knosys.2017.05.014&partnerID=40&md5=20d8dc256301d6a83bd6fb9b83160515","Concept modeling and learning have important significance in data mining, machine learning and knowledge discovery. In this paper a fuzzy semantic cell which is composed of a prototype P, a distance function d and a probability density function δ of granularity is considered as the smallest unit of vague concepts and the building brick of concept representation. For each fuzzy semantic cell we introduce three fundamental numeric characteristics, prototype P, expectation granularity R and fuzzy entropy H, to characterize the underlying concept. Then a novel learning strategy for the fuzzy semantic cell is proposed by using the principles of maximum coverage, maximum specificity, and maximum fuzzy entropy. Furthermore a granularity control factor λ is introduced into the learning strategy in order to make these principles coordinate with each other. The ultimate goal is to obtain a fuzzy semantic cell from a given data set which is the most appropriate to describe the data set. Finally the fuzzy semantic cell learning algorithm as well as the crisp semantic cell learning algorithm is formulated. We test the proposed methods on synthetic data and real-world data to demonstrate their feasibility and validity. © 2017","Concept modeling; Expectation granularity; Fuzzy entropy; Fuzzy semantic cell; Justifiable granularity; Prototype theory","Cells; Cytology; Entropy; Learning algorithms; Learning systems; Probability density function; Semantics; Concept model; Expectation granularity; Fuzzy entropy; Fuzzy semantics; Justifiable granularity; Prototype theory; Education",2-s2.0-85021854248
"Wang W., Bai X., Xia F., Bekele T.M., Su X., Tolba A.","From triadic closure to conference closure: the role of academic conferences in promoting scientific collaborations",2017,"Scientometrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025618498&doi=10.1007%2fs11192-017-2468-x&partnerID=40&md5=380295057ecf9b8a6ce0375c9c9461c1","An academic conference is not only a venue for publishing papers but also a nursery room for new scientific encounters. While previous research has investigated scientific collaboration mechanisms based on the triadic closure and focal closure, in this paper, we propose a new collaboration mechanism named conference closure. Conference closure means that scholars involved in a common conference may collaborate with each other in the future. We analyze the extent to which scholars will meet new collaborators from both the individual and community levels by using 22 conferences in the field of data mining extracted from DBLP digital library. Our results demonstrate the existence of conference closure and this phenomenon is more remarkable in conferences with high field rating and large scale attendees. Scholars involved in multiple conferences will encounter more collaborators from the conferences. Another interesting finding is that although most conference attendees are junior scholars with few publications, senior scholars with fruitful publications may gain more collaborations during the conference. Meanwhile, the conference closure still holds if we control the productivity homophily. Our study will shed light on evaluating the impact of a conference from the social function perspective based on the index of conference closure. © 2017, Akadémiai Kiadó, Budapest, Hungary.","Conference closure; Scientific collaboration; Triadic closure",,2-s2.0-85025618498
"Zou J., Zhao Q., Yang W., Wang F.","Occupancy detection in the office by analyzing surveillance videos and its application to building energy conservation",2017,"Energy and Buildings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026464209&doi=10.1016%2fj.enbuild.2017.07.064&partnerID=40&md5=8411ef7c32da272483dce41241b8c803","Indoor occupancy measurement plays an indispensable role in occupant-based intelligent control of building systems for energy conservation. In this paper, a novel algorithm is proposed to detect occupancy by analyzing the office surveillance videos. The algorithm uses a cascade classifier to detect human head, consisting of pre-classifier, main classifier and clustering analyzer. The pre-classifier uses three frame difference algorithm to search motion windows and employs a HOG-SVM module to filter most non-head areas quickly. The main classifier employs a convolution neural network to classify head windows with high recall and precision. The clustering analyzer utilizes K-means clustering to fuse sequential frames and verify the head detection result to further improve the accuracy. The advantages of the three stages are enhanced by separately determined parameters and then united by the particular combination. The innovation yields an outstanding overall performance. The algorithm is tested on the dataset of 80-h surveillance videos in an office. The experimental results show that the accuracy (correctness for presence of head) of occupancy measurement reaches up to 95.3% and the computational cost for a measurement is just 721 ms. It is applicable to both off-line data mining of stored videos and on-line detection of occupancy by intelligent video surveillance. © 2017 Elsevier B.V.","Building energy conservation; Cascade classifier; Fusion mechanism; Human head detection; Occupancy measurement; Video analysis","Classification (of information); Clustering algorithms; Energy conservation; Filtration; Historic preservation; Monitoring; Building energy conservation; Cascade classifiers; Fusion mechanism; Human head; Video analysis; Security systems",2-s2.0-85026464209
"Guo Y., Liang Z., Hou X., Zhang Z.","Diverse gene expression patterns in response to anticancer drugs between human and mouse cell lines revealed by a comparative transcriptomic analysis",2017,"Molecular Medicine Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028699686&doi=10.3892%2fmmr.2017.7176&partnerID=40&md5=e42d49c7376dea540e699037953634d9","The aim of the present study was to perform comparative genomics using gene expression profile datasets of mice and humans who had been treated with anticancer drugs, to determine the similarities and differences in the antitumor mechanisms in the two mammals. This involved data mining of antitumor gene expression regulation, and screening of genetic loci from experimental mouse models of antitumor targets, to provide a theoretical basis of drug design. Subsequently, 9 overlapping genes with opposite expression patterns were identified across mouse and human cell lines that were treated with a specific cyclin-dependent kinase 4/6 inhibitor, PD0332991. These genes included LIM homeobox 2, adenomedullin, bone marrow stromal cell antigen 1, caveolin 1, histone cluster 1 (HIST1) H2B family member C, HIST1 H3 family member F, low density lipoprotein-receptor related protein 11, prolyl 4-hydroxylase subunit α1 and torsin family 3 member A. In addition, the janus kinase-signal transducer and activator of transcription signaling pathway, Toll-like receptor signaling pathway, T cell receptor signaling pathway and the nucleotide-binding oligomerization domain-like receptor signaling pathway were identified as candidate pathways for explaining antitumor mechanisms.","Anticancer; Human; Mouse; Transcriptomic","adrenomedullin; bone marrow stromal cell antigen 1; caveolin 1; H2B family member C; HIST1 H3 family member F; histone; histone cluster 1; Janus kinase; LIM homeobox 2; low density lipoprotein receptor related protein; low density lipoprotein receptor related protein 11; microRNA; n ethylmaleimide sensitive factor; nucleotide binding oligomerization domain like receptor; palbociclib; procollagen proline 2 oxoglutarate 4 dioxygenase; prolyl 4 hydroxylase subunit alpha1; SNARE protein; STAT protein; T lymphocyte receptor; toll like receptor; torsin family 3 member A; transcriptome; tumor suppressor protein; unclassified drug; animal cell; antineoplastic activity; antitumor gene; Article; breast cancer cell line; comparative study; controlled study; down regulation; gene expression regulation; gene identification; gene locus; genetic difference; genetic screening; genetic variability; human; human cell; JAK-STAT signaling; microarray analysis; mouse; nonhuman; overlapping gene; pharmacogenetics; signal transduction; TLR signaling; transcriptomics; treatment response; upregulation",2-s2.0-85028699686
"Bian S., Jin D., Li R., Xie X., Gao G., Sun W., Li Y., Zhai L., Li X.","Genome-wide analysis of CCA1-like proteins in soybean and functional characterization of GmMYB138a",2017,"International Journal of Molecular Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030102952&doi=10.3390%2fijms18102040&partnerID=40&md5=a63e58312ab5fdbf79d6d9c5ad7abba4","Plant CIRCADIAN CLOCK ASSOCIATED1 (CCA1)-like proteins are a class of single-repeat MYELOBLASTOSIS ONCOGENE (MYB) transcription factors generally featured by a highly conserved motif SHAQK(Y/F)F, which play important roles in multiple biological processes. Soybean is an important grain legume for seed protein and edible vegetable oil. However, essential understandings regarding CCA1-like proteins are very limited in soybean. In this study, 54 CCA1-like proteins were identified by data mining of soybean genome. Phylogenetic analysis indicated that soybean CCA1-like subfamily showed evolutionary conservation and diversification. These CCA1-like genes displayed tissue-specific expression patterns, and analysis of genomic organization and evolution revealed 23 duplicated gene pairs. Among them, GmMYB138a was chosen for further investigation. Our protein-protein interaction studies revealed that GmMYB138a, but not its alternatively spliced isoform, interacts with a 14-3-3 protein (GmSGF14l). Although GmMYB138a was predominately localized in nucleus, the resulting complex of GmMYB138a and GmSGF14l was almost evenly distributed in nucleus and cytoplasm, supporting that 14-3-3s interact with their clients to alter their subcellular localization. Additionally, qPCR analysis suggested that GmMYB138a and GmSGF14l synergistically or antagonistically respond to drought, cold and salt stresses. Our findings will contribute to future research in regard to functions of soybean CCA1-like subfamily, especially regulatory mechanisms of GmMYB138a in response to abiotic stresses. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","14-3-3; CCA1-like proteins; MYB; Protein interaction; Soybean","anthocyanin; circadian clock associated 1 like protein; gibberellin; isoflavonoid; protein; reactive oxygen metabolite; salicylic acid; unclassified drug; abiotic stress; Arabidopsis; Article; chromosomal localization; circadian rhythm; flowering; genome-wide association study; germination; homeostasis; leaf senescence; oxidative stress; phylogeny; polymerase chain reaction; protein analysis; protein interaction; reverse transcription polymerase chain reaction; seed dormancy; soybean; stress",2-s2.0-85030102952
"Bellini P., Cenni D., Nesi P., Paoli I.","Wi-Fi based city users’ behaviour analysis for smart city",2017,"Journal of Visual Languages and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028592958&doi=10.1016%2fj.jvlc.2017.08.005&partnerID=40&md5=12ff651b5ce9378e9e3155fd97424cd3","Monitoring, understanding and predicting city user behaviour (hottest places, trajectories, flows, etc.) is one the major topics in the context of Smart City management. People flow surveillance provides valuable information about city conditions, useful not only for monitoring and controlling the environmental conditions, but also to optimize the deliverying of city services (security, clean, transport,..). In this context, it is mandatory to develop methods and tools for assessing people behaviour in the city. This paper presents a methodology to instrument the city via the placement of Wi-Fi Access Points, AP, and to use them as sensors to capture and understand city user behaviour with a significant precision rate (the understanding of city user behaviour is concretized with the computing of heat-maps, origin destination matrices and predicting user density). The first issue is the positioning of Wi-Fi AP in the city, thus a comparative analyses have been conducted with respect to the real data (i.e., cab traces) of the city of San Francisco. Several different positioning methodologies of APs have been proposed and compared, to minimize the cost of AP installation with the aim of producing the best origin destination matrices. In a second phase, the methodology was adopted to select suitable AP in the city of Florence (Italy), with the aim of observing city users behaviour. The obtained instrumented Firenze Wi-Fi network collected data for 6 months. The data has been analysed with data mining techniques to infer similarity patterns in AP area and related time series. The resulting model has been validated and used for predicting the number of AP accesses that is also related to number of city users. The research work described in this paper has been conducted in the scope of the EC funded Horizon 2020 project Resolute (http://www.resolute-eu.org), for early warning and city resilience. © 2017 Elsevier Ltd","GPS; People flows; Sensor positioning; Smart city; Wi-Fi access point location",,2-s2.0-85028592958
"Huang Q., Feng J., Fang Q., Ng W., Wang W.","Query-aware locality-sensitive hashing scheme for lp norm",2017,"VLDB Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021723431&doi=10.1007%2fs00778-017-0472-7&partnerID=40&md5=bf8f741be6a94df3db37c41d045c9f97","The problem of c-Approximate Nearest Neighbor (c-ANN) search in high-dimensional space is fundamentally important in many applications, such as image database and data mining. Locality-Sensitive Hashing (LSH) and its variants are the well-known indexing schemes to tackle the c-ANN search problem. Traditionally, LSH functions are constructed in a query-oblivious manner, in the sense that buckets are partitioned before any query arrives. However, objects closer to a query may be partitioned into different buckets, which is undesirable. Due to the use of query-oblivious bucket partition, the state-of-the-art LSH schemes for external memory, namely C2LSH and LSB-Forest, only work with approximation ratio of integer c≥ 2. In this paper, we introduce a novel concept of query-aware bucket partition which uses a given query as the “anchor” for bucket partition. Accordingly, a query-aware LSH function under a specific lp norm with p∈ (0 , 2 ] is a random projection coupled with query-aware bucket partition, which removes random shift required by traditional query-oblivious LSH functions. The query-aware bucket partitioning strategy can be easily implemented so that query performance is guaranteed. For each lp norm (p∈ (0 , 2 ]) , based on the corresponding p-stable distribution, we propose a novel LSH scheme named query-aware LSH (QALSH) for c-ANN search over external memory. Our theoretical studies show that QALSH enjoys a guarantee on query quality. The use of query-aware LSH function enables QALSH to work with any approximation ratio c&gt; 1. In addition, we propose a heuristic variant named QALSH+ to improve the scalability of QALSH. Extensive experiments show that QALSH and QALSH+ outperform the state-of-the-art schemes, especially in high-dimensional space. Specifically, by using a ratio c&lt; 2 , QALSH can achieve much better query quality. © 2017, Springer-Verlag GmbH Germany.","lp Norm; Locality-sensitive hashing; Nearest neighbor search; p-Stable distributions; Query-aware","Image retrieval; Nearest neighbor search; Approximation ratios; High dimensional spaces; Locality sensitive hashing; Lp-norm; P-stable; Partitioning strategies; Query-aware; State-of-the-art scheme; Query processing",2-s2.0-85021723431
"De Grijs R., Bono G.","Clustering of Local Group Distances: Publication Bias or Correlated Measurements? V. Galactic Rotation Constants",2017,"Astrophysical Journal, Supplement Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032731197&doi=10.3847%2f1538-4365%2faa8b71&partnerID=40&md5=34d847a0e855019a7476b00c0e57a4bc","As part of an extensive data mining effort, we have compiled a database of 162 Galactic rotation speed measurements at R 0 (the solar Galactocentric distance), Θ0. Published between 1927 and 2017 June, this represents the most comprehensive set of values since the 1985 meta-analysis that led to the last revision of the International Astronomical Union's recommended Galactic rotation constants. Although we do not find any compelling evidence for the presence of ""publication bias"" in recent decades, we find clear differences among the values and the ratios resulting from the use of different tracer populations. Specifically, young tracers (including OB and supergiant stars, masers, Cepheid variables, H ii regions, and young open clusters), as well as kinematic measurements of Sgr A∗ near the Galactic Center, imply a significantly larger Galactic rotation speed at the solar circle and a higher ratio Θ0/R0 (i.e., Θ0 = 247 ± 3km s-1 and Θ0/R0 =29.81 ± 0.32km s-1 kpc-1; statistical uncertainties only) than any of the tracers dominating the Galaxy's mass budget (i.e., field stars and the H i/CO distributions). Using the latter to be most representative of the bulk of the Galaxy's matter distribution, we arrive at an updated set of Galactic rotation constants, Θ0 = 225 ± 3 (statistical) ± 10 (systematic) km s-1, R0 = 8.3 ± 0.2 (statistical) ± 0.4 (systematic) kpc, and Θ0/R0 = 27.12 ± 0.39 (statistical) ± 1.78 (systematic) km s-1 kpc-1 © 2017. The American Astronomical Society. All rights reserved.","astronomical databases: miscellaneous; Galaxy: fundamental parameters; Galaxy: kinematics and dynamics; publications, bibliography; reference systems",,2-s2.0-85032731197
"Barh D., García-Solano M.E., Tiwari S., Bhattacharya A., Jain N., Torres-Moreno D., Ferri B., Silva A., Azevedo V., Ghosh P., Blum K., Conesa-Zamora P., Perry G.","BARHL1 is downregulated in alzheimer’s disease and may regulate cognitive functions through ESR1 and multiple pathways",2017,"Genes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030557336&doi=10.3390%2fgenes8100245&partnerID=40&md5=622482ae49d75838c864303bd7161b76","The Transcription factor BarH like homeobox 1 (BARHL1) is overexpressed in medulloblastoma and plays a role in neurogenesis. However, much about the BARHL1 regulatory networks and their functions in neurodegenerative and neoplastic disorders is not yet known. In this study, using a tissue microarray (TMA), we report for the first time that BARHL1 is downregulated in hormone-negative breast cancers and Alzheimer’s disease (AD). Furthermore, using an integrative bioinformatics approach and mining knockout mouse data, we show that: (i) BARHL1 and Estrogen Receptor 1 (ESR1) may constitute a network that regulates Neurotrophin 3 (NTF3)- and Brain Derived Neurotrophic Factor (BDNF)-mediated neurogenesis and neural survival; (ii) this is probably linked to AD pathways affecting aberrant post-translational modifications including SUMOylation and ubiquitination; (iii) the BARHL1-ESR1 network possibly regulates β-amyloid metabolism and memory; and (iv) hsa-mir-18a, having common key targets in the BARHL1-ESR1 network and AD pathway, may modulate neuron death, reduce β-amyloid processing and might also be involved in hearing and cognitive decline associated with AD. We have also hypothesized why estrogen replacement therapy improves AD condition. In addition, we have provided a feasible new mechanism to explain the abnormal function of mossy fibers and cerebellar granule cells related to memory and cognitive decline in AD apart from the Tau and amyloid pathogenesis through our BARHL1-ESR1 axis. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Alzheimer’s disease; Bioinformatics; Estrogen; MicroRNA; Signaling",,2-s2.0-85030557336
"Torró L., Proenza J.A., Camprubí A., Nelson C.E., Domínguez H., Carrasco C., Reynoso-Villafaña R., Melgarejo J.C.","Towards a unified genetic model for the Au-Ag-Cu Pueblo Viejo district, central Dominican Republic",2017,"Ore Geology Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032683686&doi=10.1016%2fj.oregeorev.2017.07.002&partnerID=40&md5=5ccb16f7a4f37f5443a881faaec0a0e6","The origin of the world-class Pueblo Viejo deposit, central Dominican Republic, remains controversial. In this article we scale up the area of study to the Cabirma del Cerro mining concession, which abuts the Pueblo Viejo mining area, and integrate new data on the stratigraphy, lithogeochemistry and U-Pb geochronology of volcanogenic units with previous descriptions of the geology of the Pueblo Viejo deposit. Volcanic, volcaniclastic and sedimentary lithofacies of the ore-hosting Los Ranchos Formation reveal a progression from deposition in submarine to subaerial environments. Volcanic and hyaloclastite deposits are of exclusive basaltic composition, and were deposited at ca. 122–112 Ma. Lithogeochemistry of basaltic rocks indicates tholeiitic and boninitic affinities. Chemostratigraphic relationships denote a progressive shift from LREE-depleted low-Ti island-arc tholeiitic and boninitic lavas to LREE-richer island-arc tholeiitic lavas. Basaltic deposits were intruded by tonalite batholiths and plagiorhyolite stocks at ca. 113–109 Ma. These acid intrusives have tholeiitic, M-type affinities. Subsequently, diorites with transitional tholeiitic-calc-alkaline, and monzodiorites and andesitic domes with calc-alkaline affinities were emplaced at 109–106 Ma. The progressive lithogeochemical changes throughout the magmatic sequence of the Los Ranchos Formation mirror those in so-called subduction-initiation ophiolites. Hydrothermal alteration and sulfide mineralization are ubiquitous in the area of study, and include volcanogenic massive sulfide (VMS) and epithermal deposit types. By assessing the location of these mineralization types in the stratigraphic sequence, an integrated metallogenic model with two distinct mineralization events is presented: (1) VMS mineralization took place during the subduction-initiation stage of the island-arc, and was genetically associated to LREE-depleted tholeiitic and boninitic basalts; subsequently (2) porphyry Cu(-Mo)-high sulfidation epithermal mineralization at ca. 112 Ma was most likely connected to a widespread episode of M-type acid magmatism of tholeiitic affinity during the steady-state or true-subduction regime. © 2017 Elsevier B.V.","Caribbean island-arc; Epithermal; Metallogenic evolution; Pueblo Viejo; VMS","Basalt; Binary alloys; Clay alteration; Cobalt deposits; Copper alloys; Deposits; Geochronology; Gold alloys; Lead; Lead alloys; Lithology; Silver alloys; Stratigraphy; Sulfur compounds; Tectonics; Ternary alloys; Uranium alloys; Volcanoes; Epithermal; Epithermal mineralization; Hydrothermal alterations; Island arc; Metallogenic evolution; Pueblo Viejo; Stratigraphic sequences; Volcanogenic massive sulfides; Mineralogy",2-s2.0-85032683686
"Lu B., Feng J.","Coal working face imaging by seismic interferometry - Using conveyer belt noise as source",2017,"Journal of Seismic Exploration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032696928&partnerID=40&md5=ff71b18d7211c58edd10e4bbb0a6a7ae","The real-time monitoring of stress conditions around the working faces of coal mines has been found to be an effective method to prevent geological disasters, such as roof collapses and water bursts. In this research study, a seismic interferometry method was proposed based on conveyor belt noise, for the purpose of implementing working face real-time imaging. In order to examine the seismic interferometry induced by conveyor belt noise, a stationary phase integration analysis of the cross-correlation of the thread seismic source was first conducted. Then, a numerical simulation of the discrete linear array noise traces was discussed. The analysis showed that the seismic interferometry of the thread seismic sources produced some fake events, which were observed prior to the true events. Therefore, the arrival-time of true events could be picked up by using a simple cross-correlation. The results of this study's field data suggested that the conveyor belt noise was a wide band signal. At the same time, it was also found to have an intense time structure. The Green's function, which was retrieved using deconvolution interferometry, displayed a higher time resolution than those obtained by the cross-correlation. Finally, this study's field data illustrated that the conveyor belt noise could be potentially applied to monitor the stress variations around longwall mining panels. As a result, the proposed method was expected to be an effective method for the reduction of coal mine disasters. © 2017 Geophysical Press Ltd.","Coal working face; Conveyer belt; Noise imaging; Passive source; Seismic interferometry",,2-s2.0-85032696928
"Tusong K., Guo X., Meng S., Liu X., Ma J.","Comparative analysis of the transcriptome of the overwintering desert beetle Microdera punctipennis",2017,"Cryobiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026812373&doi=10.1016%2fj.cryobiol.2017.06.009&partnerID=40&md5=902beb2c61b9aeb45a68a84722259404","The cold tolerance mechanisms of insect have been studied extensively on the model species Drosophila and a few other species at the transcriptional level. However studies on insects that inherit strong cold tolerance are limited. Cold hardy Tenebrionid beetle Microdera punctipennis is endemic to Gurbantonggut Desert, northwest of China. However, its genomic information is lacking. To investigate the overwintering mechanisms of M. punctipennis adult, RNA-seq was performed on the winter adults and the control adults that were kept in laboratory at 30 °C. A total of 175,247 unigenes were acquired with an average length of 645 bp. By using DESeq package, we identified 3367 unigenes that were up-regulated and 7988 down-regulated in the winter adults compared with the controls. To further our understanding of these differentially expressed genes (DEGs), Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were performed. Pathway analysis showed that the “ECM-receptor interaction”, “PI3K-Akt signaling pathway”, “Estrogen signaling pathway”, “Tight junction”, and “Regulation of actin cytoskeleton”, etc. might play important roles in M. punctipennis overwintering. The DEGs results from the RNA-Seq were confirmed partially by qRT-PCR for 13 DEGs, which showed high consistence with a Pearson's correlation coefficient of 0.851. Overall, the sequence data will provide basic information for subsequent bioinformatical analysis and mining of the genes responsible for cold tolerance in M. punctipennis, as well as for understanding the molecular mechanisms of desert beetle overwintering. © 2017 Elsevier Inc.","Beetle; Low temperature; Overwintering; Tenebrionidae; Transcriptome","transcriptome; adult; Article; beetle; China; cold tolerance; comparative study; controlled study; endemic species; gene control; gene identification; gene ontology; Microdera punctipennis; nonhuman; overwintering; priority journal; RNA sequence; signal transduction; Tenebrionidae",2-s2.0-85026812373
"Ikemi H.","Geologically constrained changes to landforms caused by human activities in the 20th century: A case study from Fukuoka Prefecture, Japan",2017,"Applied Geography",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029357239&doi=10.1016%2fj.apgeog.2017.08.001&partnerID=40&md5=a01bed6f6849e1317eb727d26add6a97","Human activity has been recognized to be an important geomorphic agent, and the resulting changes to landforms and land cover are regarded as a global problem. Although there has been much research into the relationships between geomorphic processes and types of land use such as agriculture, mining, and urbanization, it is important to clarify spatiotemporal human impacts on topography on a regional scale when predicting future changes in land cover. This study examined changes in land use to clarify the distribution and impact of anthropogenic changes to landforms, as well as the influence of geology on the extent of these changes. In a case study from Fukuoka Prefecture, Japan, changes in land use over the last century were analyzed using a geographic information system (GIS). The study area, which covers approximately 4930 km2, has experienced urban development since 1950 and has a current population of over 5 million. Land use data were prepared using paper-based early editions of topographic maps. Subsequently, the distribution of anthropogenic landforms was evaluated by comparing landforms with regional geological data. GIS analysis using our prepared land use data, landform data, and regional geological data has clarified the following characteristics of the study area. (1) Land uses prior to 1950 were constrained by topographic relief. After 1950, land use was characterized by urban sprawl. Urban areas expanded and contained both higher elevations and steeper slopes at their margins. The relationships between land uses and landforms during this urbanization are unclear. (2) The area of urban land increased in the geological regions with Paleogene sedimentary rocks (PSD) and Mesozoic granitic rocks (GR) during the 20th century. The largest coal mining area in Japan was located in the PSD geological regions, and ancient iron working was common in the GR geological regions, particularly during the 7th century. This result indicates that the land use distribution, especially urban areas in sloping terrain, is related to the regional geology. (3) Deforestation related to land use resulted in steeper terrain in forest land in the PSD and GR geological regions. These changes to landforms in forest areas occurred as a result of rapid urban sprawl and have created many new boundaries between forest areas with steeper slopes and urban areas with gentler slopes. This phenomenon may have caused an increase in the frequency of sediment-related disasters. This case study indicates that predictions of anthropogenic changes to landform, which are important for the assessment of global climate change and natural hazards, must clarify the relationships between land uses, landforms, and regional geology. © 2017 Elsevier Ltd","Anthropogenic landform; GIS; Land cover; Land use; Regional geology","anthropogenic effect; geomorphological response; GIS; human activity; land cover; land use change; landform evolution; regional geology; twentieth century; Fukuoka [Kyushu]; Japan; Kyushu",2-s2.0-85029357239
"Hoseinzade Z., Mokhtari A.R.","A comparison study on detection of key geochemical variables and factors through three different types of factor analysis",2017,"Journal of African Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026628247&doi=10.1016%2fj.jafrearsci.2017.07.025&partnerID=40&md5=7f420a9be78fc1bf0402d3c137d9fd7a","Large numbers of variables have been measured to explain different phenomena. Factor analysis has widely been used in order to reduce the dimension of datasets. Additionally, the technique has been employed to highlight underlying factors hidden in a complex system. As geochemical studies benefit from multivariate assays, application of this method is widespread in geochemistry. However, the conventional protocols in implementing factor analysis have some drawbacks in spite of their advantages. In the present study, a geochemical dataset including 804 soil samples collected from a mining area in central Iran in order to search for MVT type Pb-Zn deposits was considered to outline geochemical analysis through various fractal methods. Routine factor analysis, sequential factor analysis, and staged factor analysis were applied to the dataset after opening the data with (additive logratio) alr-transformation to extract mineralization factor in the dataset. A comparison between these methods indicated that sequential factor analysis has more clearly revealed MVT paragenesis elements in surface samples with nearly 50% variation in F1. In addition, staged factor analysis has given acceptable results while it is easy to practice. It could detect mineralization related elements while larger factor loadings are given to these elements resulting in better pronunciation of mineralization. © 2017 Elsevier Ltd","Geochemistry; Iran; MVT; Sequential factor analysis; Staged factor analysis","comparative study; factor analysis; mineralization; paragenesis; soil chemistry; Iran",2-s2.0-85026628247
"Ranjan K., Prakash Y.P., Kapilesh B.","Estimation of vibration parameters based on rock mass rating (RMR) for blast in rocky sites",2017,"Disaster Advances",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029678074&partnerID=40&md5=e3706c4b16379645971ce564b38d7d7c","Quarrying, mining and construction activity pose a serious problem to environment considering the ground vibrations, air blasts and fly rocks generated due to the blasting involved in these activities. If the blasting is not planned well, then it may lead to some serious damage to life and property. As the countries are developing and cities expanding, there is hardly any distance left between the settlements and quarries/mines. Therefore, an impact assessment of the blast induced ground vibrations is now an important pre-requisite for future operations of quarries/mines. In the recent past, the topic of blast loads on structures has received considerable attention of researchers. Site specific empirical models for blast induced vibration parameters like Peak Particle Velocity (PPV), Peak ground acceleration (PGA) and Peak Particle Displacement (PPD) are commonly used for blast resistant designs. However, these empirical models are not able to consider the variation in the rock properties such as UCS, RQD, discontinuities in rocks and its conditions and water table location etc. Hence, in this study, a total of 156 blast data from various sites have been collected and used to propose a generalized empirical model to estimate PPV in terms of RMR. Standard errors and coefficients of correlation for prediction of blast induced vibration parameters by various empirical models are obtained with respect to the observed soil field data. The present empirical model has been compared with the models of other researchers and was found in good agreement. The present model, having maximum coefficient of correlation, can be directly used in calculation of PPV. In the absence of field blast vibration data, the present model will be very useful to evaluate blast vibration parameter by using only basic rock property specified in terms of RMR.","Blast loads; Blast vibration; Empirical equations; Peak Particle Velocity (PPV); Rock Mass Rating (RMR)","blasting; empirical analysis; equation; loading; mine; quarry; rock mass classification; rock property; vibration",2-s2.0-85029678074
"MacMillan G.A., Chételat J., Heath J.P., Mickpegak R., Amyot M.","Rare earth elements in freshwater, marine, and terrestrial ecosystems in the eastern Canadian Arctic",2017,"Environmental Science: Processes and Impacts",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031810031&doi=10.1039%2fc7em00082k&partnerID=40&md5=08dd103c2a39b37ccc8cdcbb975d5ef5","Few ecotoxicological studies exist for rare earth elements (REEs), particularly field-based studies on their bioaccumulation and food web dynamics. REE mining has led to significant environmental impacts in several countries (China, Brazil, U.S.), yet little is known about the fate and transport of these contaminants of emerging concern. Northern ecosystems are potentially vulnerable to REE enrichment from prospective mining projects at high latitudes. To understand how REEs behave in remote northern food webs, we measured REE concentrations and carbon and nitrogen stable isotope ratios (∂15N, ∂13C) in biota from marine, freshwater, and terrestrial ecosystems of the eastern Canadian Arctic (N = 339). Wildlife harvesting and tissue sampling was partly conducted by local hunters through a community-based monitoring project. Results show that REEs generally follow a coherent bioaccumulation pattern for sample tissues, with some anomalies for redox-sensitive elements (Ce, Eu). Highest REE concentrations were found at low trophic levels, especially in vegetation and aquatic invertebrates. Terrestrial herbivores, ringed seal, and fish had low total REE levels in muscle tissue (∑REE for 15 elements &lt;0.1 nmol g-1), yet accumulation was an order of magnitude higher in liver tissues. Age- and length-dependent REE accumulation also suggest that REE uptake is faster than elimination for some species. Overall, REE bioaccumulation patterns appear to be species- and tissue-specific, with limited potential for biomagnification. This study provides novel data on the behaviour of REEs in ecosystems and will be useful for environmental impact assessment of REE enrichment in northern regions. © 2017 The Royal Society of Chemistry.",,"carbon 13; cerium; europium; fresh water; lanthanide; nitrogen 15; animal hunting; animal tissue; aquatic invertebrate; Arctic; Article; bioaccumulation; biota; Canada; concentration (parameters); ecosystem; ecotoxicology; environmental impact assessment; environmental monitoring; fish; food web; freshwater environment; harvesting; herbivore; marine environment; muscle tissue; nonhuman; oxidation reduction state; Pinnipedia; priority journal; species difference; terrestrial ecosystem; tissue specificity; trophic level; vegetation; wildlife",2-s2.0-85031810031
"Zhu L., Lu C., Dong Z.Y., Hong C.","Imbalance Learning Machine-Based Power System Short-Term Voltage Stability Assessment",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031673422&doi=10.1109%2fTII.2017.2696534&partnerID=40&md5=8649418be72687a194f23b6b94b8a0bf","In terms of machine learning-based power system dynamic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a challenging class imbalance problem. Besides, short-term feature extraction from scarce instability seems extremely difficult for conventional learning machines. Faced with such a dilemma, this paper develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is embedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribution. Cost-sensitive learning is employed to intensify bias toward those scarce yet valuable unstable cases. Furthermore, an incremental learning strategy is put forward for online monitoring, contributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the assessment scheme. © 2005-2012 IEEE.","Class imbalance; cost-sensitive; incremental learning; phasor measurements; shapelets; short-term voltage stability","Electric power system stability; Learning systems; Phasor measurement units; Stability; Class imbalance; Cost-sensitive; Incremental learning; Shapelets; Short-term voltage stability; System stability",2-s2.0-85031673422
"Yang D., Huang C., Wang M.","A social recommender system by combining social network and sentiment similarity: A case study of healthcare",2017,"Journal of Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028296233&doi=10.1177%2f0165551516657712&partnerID=40&md5=a15e4be9a7a50de02f095c0af8bbb7f7","Social recommender systems aim to support user preferences and help users make better decisions in social media. The social network and the social context are two vital elements in social recommender systems. In this contribution, we propose a new framework for a social recommender system based on both network structure analysis and social context mining. Exponential random graph models (ERGMs) are able to capture and simulate the complex structure of a micro-blog network. We derive the prediction formula from ERGMs for recommending micro-blog users. Then, a primary recommendation list is created by analysing the micro-blog network structure. In the next step, we calculate the sentiment similarities of micro-blog users based on a sentiment feature set which is extracted from users' tweets. Sentiment similarities are used to filter the primary recommendation list and find users who have similar attitudes on the same topic. The goal of those two steps is to make the social recommender system much more precise and to satisfy users' psychological preferences. At the end, we use this new framework deal with big real-world data. The recommendation results of diabetes accounts of Weibo show that our method outperforms other social recommender systems. © Chartered Institute of Library and Information Professionals.","Exponential random graph model (ERGM); framework; healthcare; sentiment; social recommender system","Blogs; Filtration; Graph theory; Health care; Social networking (online); Complex structure; Exponential random graph model; framework; Network structures; sentiment; Sentiment features; Social context; Social recommender systems; Recommender systems",2-s2.0-85028296233
"Taheri A., Lee Y., Medina M.A.G.","A Modified Coal Mine Roof Rating Classification System to Design Support Requirements in Coal Mines",2017,"Journal of The Institution of Engineers (India): Series D",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029769142&doi=10.1007%2fs40033-016-0131-y&partnerID=40&md5=6ee421d456b429e83c2585338785a223","The coal mine roof rating (CMRR) classification system has been applied in a number of coal mines worldwide including Australia. However, the current system cannot be used directly to design support measures in underground mines. Two case studies, the Eliza Hill project in Australia and Tabas coal mine in Iran were analyzed to assess the impact of various rock properties and gallery geometry on stability and to modify the CMRR classification system. Having considered the CMRR system as a working classification system, applicable information and related coal mine data were selected from the two case records. The CMRR value was evaluated and analysed by undertaking correlation between CMRR and factor of safety, followed by a parametric study based on various rock properties and gallery geometries. To improve the applicability of the current system, the CMRR system was then modified by adding additional parameters, namely, the width of roof span and the density of overburden rock. Consequently, based on the modified CMRR system (mCMRR) roof support requirements were recommended to select the suitable rock bolting system including length and spacing of rock bolt. Numerical modelling were then undertaken to verify the support requirements recommended. The support requirements recommended by the mCMRR were found to be relatively identical with numerical analysis results. Support systems proposed by mCMRR can assist mining engineers to assess the stability of underground coal mines or verify the results of other design tools. © 2017, The Institution of Engineers (India).","Coal mine roof rating; RMR; Rock mass classification; Support system; Underground coal mine","Classification (of information); Coal; Mine roof control; Mine roof supports; Rating; Rock bolting; Rock mechanics; Rocks; Roofs; Safety factor; System stability; Classification system; Coal mine roof ratings; Parametric study; Rock mass classification; Support requirements; Support systems; Underground coal mine; Underground mine; Coal mines",2-s2.0-85029769142
"Li K., Liu L., Shang S., Wang Y., Zhan Y., Song J., Zhang X., Chang Y.","cDNA cloning, expression and immune function analysis of a novel Rac1 gene (AjRac1) in the sea cucumber Apostichopus japonicus",2017,"Fish and Shellfish Immunology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028735204&doi=10.1016%2fj.fsi.2017.08.027&partnerID=40&md5=23659eccafdfdc23a15ccea42672268a","The ras-related C3 botulinum toxin substrate 1 (Rac1) belongs to Ras homolog (Rho) small GTPases subfamily. As an important molecular switch, Rac1 regulates various processes in the cell, especially in cellular immune response. With attempt to clarify characters and functions of Rac1 in sea cucumbers, full length cDNA of a Rac1 homolog in the sea cucumber Apostichopus japonicus (AjRac1) was cloned by transcriptome database mining and rapid amplification of cDNA ends (RACE) techniques. The open reading frame of AjRac1 is 579 bp encoding a protein with a length of 192 aa. Sequence analysis showed that AjRac1 is highly conserved as compared to those from other eukaryotic species. Phylogenetic analysis revealed that amino acid sequence of AjRac1 closely related to those from Strongylocentrotus purpuratus. Results of expression analysis showed that AjRac1 exhibited a relative high expression in blastula stage, adult coelomocytes and respiratory tree in A. japonicus. The transcription of AjRac1 in adult coelomocytes altered significantly at 4 h- and 12 h-after Vibrio splendidus infection, respectively, which indicated that AjRac1 involved in sea cucumber innate immunity. All data presented in this study will deepen our understanding of characterizations and immunological functions of Rac1 in sea cucumbers. © 2017 Elsevier Ltd","Apostichopus japonicus; Expression analysis; Ras-related C3 botulinum toxin substrate 1; Vibrio splendidus infection","complementary DNA; messenger RNA; Rac1 protein; 5' untranslated region; adult; amino acid sequence; animal cell; Apostichopus japonicus; Article; controlled study; embryo; gene amplification; gene expression; gene function; molecular cloning; molecular model; molecular weight; nonhuman; open reading frame; phylogeny; polyadenylation; polymerase chain reaction; protein phosphorylation; sequence alignment; sequence analysis; Strongylocentrotus purpuratus",2-s2.0-85028735204
"Yao W., He T., Xia K.","Dynamic mechanical behaviors of Fangshan marble",2017,"Journal of Rock Mechanics and Geotechnical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030159451&doi=10.1016%2fj.jrmge.2017.03.019&partnerID=40&md5=c50fc53008f11dfba5335ac9b040128f","Dynamic strength parameters are extensively used in mining engineering and rock mechanics. However, there are no widely accepted dynamic failure models for rocks. In this study, the dynamic punching shear strength, uniaxial compressive strength (UCS) and tensile strength of fine-grained Fangshan marble (FM) are first measured by using a split Hopkinson pressure bar (SHPB) system. The pulse-shaping technique is then implemented to maintain the dynamic force balance in SHPB tests. Experimental results show that the dynamic punching shear strength, UCS and tensile strength increase with the loading rate. A recently developed dynamic Mohr-Coulomb theory is then used to interpret the testing data. In this model, the angle of internal friction ϕ is assumed to be independent of loading rate and is obtained using the static strength values. According to the dynamic Mohr-Coulomb theory, the dynamic UCS and the dynamic tensile strength are predicted from the dynamic punching shear strength. Furthermore, based on this dynamic theory, the dynamic UCS is predicted from the dynamic tensile strength. The consistency between the predicted and measured dynamic strengths demonstrates that the dynamic Mohr-Coulomb theory is applicable to FM. © 2017 Institute of Rock and Soil Mechanics, Chinese Academy of Sciences","Dynamic Mohr-Coulomb model; Dynamic strengths; Fangshan marble; Split Hopkinson pressure bar (SHPB)","compressive strength; Coulomb criterion; dynamic property; marble; Mohr theory; rock mechanics; shear strength; tensile strength",2-s2.0-85030159451
"Faridullah F., Umar M., Alam A., Sabir M.A., Khan D.","Assessment of heavy metals concentration in phosphate rock deposits, Hazara basin, Lesser Himalaya Pakistan",2017,"Geosciences Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027097821&doi=10.1007%2fs12303-017-0013-9&partnerID=40&md5=fbc1a06c4501de912d8cd011bad3cf05","Naturally occurring phosphate rock is the only essential source for the production of fertilizers. Heavy metals and phosphate concentrations are quite higher in phosphate rock formed by sedimentary processes. This detail study was conducted to evaluate the heavy metal concentrations in part of the Hazara region, which is the only source of phosphate fertilizer in the country. Heavy metals are considered as one of the main pollutant responsible for environmental contamination of soil. This study included the concentration of phosphorite in the Hazara region in the three useful forms: total phosphorite, extractable phosphorite and water soluble forms. The phosphorous extracted from sedimentary deposits used to maintain the natural content in agricultural soil which was being depleted due to the regular practice of crop harvest. The data collected during this studies were statically analyzed which refers the significant variations in P, Zn, Cr, Mn, As, Cu, Fe, Ni, & Pb. Zn and Cr concentration in Hazara phosphates. Heavy metals for instance Pb, Cr, Zn and Cu present in higher amount than usable limits and may create environmental pollution (air, surface & groundwater and soil) and health issues of humans. Therefore it is recommended that managing remedial steps are necessary around the mining regions to avoid environmental and health issues. © 2017, The Association of Korean Geoscience Societies and Springer-Verlag GmbH Germany.","contamination; fertilizer; heavy metals pollutants; Lesser Himalaya; phosphorites; soil",,2-s2.0-85027097821
"Tuoane-Nkhasi M., van Eeden A.","Spatial patterns and correlates of mortality due to selected non-communicable diseases among adults in South Africa, 2011",2017,"GeoJournal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973137352&doi=10.1007%2fs10708-016-9725-z&partnerID=40&md5=c4ef5712bb4c2b39b0248d57bc6026a8","South Africa has a high prevalence of non-communicable diseases (NCDs) compared to other African countries. Government has predicted that NCDs will be a major threat of health in the country in the next 20–30 years and recommended a significant reduction in NCDs. This research studies spatial patterns of deaths resulting from chronic lower respiratory diseases (CLRD), malignant neoplasms of digestive organs (MNDO), diabetes mellitus (DM) and cerebrovascular diseases (CVD) among adults aged 15–69 years in 2011 at municipality level. The study uses secondary data from the civil registration system; population census; and supplementary information from the Business Register and a database of public health facilities. Moran’s Index, hot spot analysis and geographically weighted regression were applied to measure spatial autocorrelation of age-standardised death rates; to identify municipalities of high risk of deaths due to selected NCDs; and associated socio-economic and demographic factors. Clusters of high risk of deaths from selected NCDs were identified, with those for CLRD and MNDO mainly in the urban municipalities around the mid and western parts of the country while for DM and CVD they occurred mostly in the urban and rural north-eastern part, as well as in the urban municipalities of Northern Cape for CVD. Hot spots showed differential socio-economic and demographic risk factors in NCDs, which included sex, urbanisation, density of public hospitals; death occurrence in health facilities; smoking; asbestos roofing; mining and quarrying businesses. Findings highlight the importance of community-specific interventions and the need for a neighbourhood-based approach in tackling NCDs. © 2016, Springer Science+Business Media Dordrecht.","Age-standardised death rates; Geographically weighted regression; Hot spot analysis; Non-communicable diseases; Spatial analysis","Diseases; Health; Health risks; Hospitals; Risk assessment; Spatial variables measurement; Death rates; Geographically weighted regression; Hot spot; Non-communicable disease; Spatial analysis; Population statistics; adult; asbestos; correlation; database; disease prevalence; mortality; public health; regression analysis; risk factor; spatial analysis; urban area; urbanization; Northern Cape; South Africa",2-s2.0-84973137352
"Dar F.A., Ganai J.A., Ahmed S., Satyanarayanan M.","Groundwater trace element chemistry of the karstified limestone of Andhra Pradesh, India",2017,"Environmental Earth Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032034281&doi=10.1007%2fs12665-017-6972-3&partnerID=40&md5=c67d707c4116fb549477db5f3743c3b7","Groundwater trace element geochemistry was evaluated in the Narji Limestone which possesses a range of substituted trace elements in the mineral matrix. The data indicate three interrelated processes control water–rock interactions and elemental concentrations in this aquifer. Dissolution influenced by pH–Eh, recharge processes and residence time controls Ca2+, Mg2+, Mn, Al, Cd, Ba, Sr, Co, Li, Rb, V, Fe, Pb, As, Si and Cu concentrations, while the weathering of quartzites and shales releases minute quantities of Na+, K+, Cl−, Si and F into groundwater. A significant amount of Na+, K+, Cl−, NO3 −, SO4 2−, Al, Fe, Cd, As, Pb, Ni, Zn, Mn, Sr, Cl, Br, Sb, Ag, Mo, Co and Cu has leached into groundwater from the use of fertilizers/manures and decayed organic matter in cultivated areas. High evapotranspiration has concentrated the chemicals by about 84%. Urbanization, land-use changes, mining and local industries have significantly increased Cd, Se, Ni, Zn, V, Fe, Cd, Sb, Ag and Cr concentrations in groundwater. The composition of groundwater is also affected by sewage, waste disposal and industrial and commercial activities. The chemical properties of about 18% of groundwater samples suggest the role of dissolution/weathering, 46% of samples have been influenced by the effects of evaporation and agricultural activities while 29% of the samples have been affected by natural hydrogeochemical processes and the effects of agricultural and industrial contamination. The quality of groundwater in the area generally conforms to the WHO recommended limits for drinking. Increasing anthropogenic interactions are likely to degrade the groundwater as karst is highly prone to contamination. This demands further investigations to ensure that the groundwater remains suitable for its intended uses in the future. © 2017, Springer-Verlag GmbH Germany.","Geochemistry; India; Limestone; Quality; Trace element","Agriculture; Aluminum; Aquifers; Dissolution; Evapotranspiration; Geochemistry; Groundwater; Groundwater pollution; Groundwater resources; Hydrochemistry; Image quality; Industrial waste disposal; Land use; Lead; Limestone; Manganese; Nickel; Pollution; Process control; Sewage; Silver; Sodium; Trace elements; Waste disposal; Weathering; Zinc; Agricultural activities; Cu concentrations; Element chemistry; Elemental concentrations; Hydrogeochemical process; India; Industrial contamination; Trace element geochemistry; Groundwater geochemistry; groundwater; hydrogeochemistry; karst; limestone; qualitative analysis; trace element; water chemistry",2-s2.0-85032034281
"Wang Y., He J., Zhang S., Yang Q., Wang B., Liu Z., Wu X.","Knockdown of Immature Colon Carcinoma Transcript 1 Inhibits Proliferation and Promotes Apoptosis of Non–Small Cell Lung Cancer Cells",2017,"Technology in Cancer Research and Treatment",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028089860&doi=10.1177%2f1533034616657977&partnerID=40&md5=e61ae7119876a228cf45f8a8fe9fb533","Non–small cell lung cancer, as the most frequent type lung cancer, has lower survival rate of 5 years, despite improvements in surgery and chemotherapy. Previous studies showed immature colon carcinoma transcript 1 is closely related to tumorigenesis of human cancer cells. In the present study, we found immature colon carcinoma transcript 1 was overexpressed in lung cancer tissues using Oncomine database mining, and the biological effect of immature colon carcinoma transcript 1 was investigated in non–small cell lung cancer cell lines 95D and A549. Lentivirus-mediated RNA interference was used to knock down immature colon carcinoma transcript 1 expression in 95D and A549 cells in vitro, and the knockdown efficiency was determined using quantitative real-time polymerase chain reaction and Western blot assay. Knockdown of immature colon carcinoma transcript 1 significantly suppressed non–small cell lung cancer cell proliferation and colony formation ability confirmed by 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide and colony formation assay. Flow cytometry was applied to measure cell cycle arrest, and the result showed the cell cycle arrested in G2/M phase in 95D cells and arrested in G0/G1 phase in A549 cells. Furthermore, we measured the levels of cell cycle–associated proteins by Western blot analysis and found immature colon carcinoma transcript 1–mediated cell proliferation inhibition appeared due to downregulation of cell cycle activator cyclin D1 and upregulation of cell cycle inhibitor p21. In addition, immature colon carcinoma transcript 1 silencing significantly induced non–small cell lung cancer cell apoptosis by annexin V/7-amino-actinomycin D double-staining assay. All our data suggest that immature colon carcinoma transcript 1 may play an important role for non–small cell lung cancer cell proliferation and could be a potential molecular target for diagnosing and treating human non–small cell lung cancer. © 2016, © The Author(s) 2016.","cell cycle; cell proliferation; ICT1; NSCLC; RNAi",,2-s2.0-85028089860
"Shimray P.W., Bajaj D., Srivastava R., Daware A., Upadhyaya H.D., Kumar R., Bharadwaj C., Tyagi A.K., Parida S.K.","Identifying Transcription Factor Genes Associated with Yield Traits in Chickpea",2017,"Plant Molecular Biology Reporter",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027847095&doi=10.1007%2fs11105-017-1044-0&partnerID=40&md5=a19c94122e376e2ccced6ed361a3b9fe","Identification of potential transcription factor (TF) gene-derived natural SNP allelic variants regulating pod and seed yield component traits by large-scale mining and genotyping of SNPs in natural germplasm accessions coupled with high-resolution association mapping is vital for understanding the complex genetic architecture of quantitative yield traits in chickpea. In these perspectives, the current study employed a genome-wide GBS (genotyping-by-sequencing) and targeted gene amplicon resequencing-based simultaneous SNP discovery and genotyping assays, which discovered 1611 novel SNPs from 736 TF genes physically mapped on eight chromosomes and unanchored scaffolds of kabuli chickpea genome. These SNPs were structurally and functionally annotated in diverse synonymous and non-synonymous coding as well as non-coding regulatory and intronic sequence components of chickpea TF genes. A high-resolution genetic association analysis was performed by correlating the genotyping information of 1611 TF gene-based SNPs with multi-location/years field phenotyping data of six major pod and seed yield traits evaluated in a constituted association panel (326 desi and kabuli germplasm accessions) of chickpea. This essentially identified 27 TF gene-derived SNPs exhibiting significant association with six major yield traits, namely days to 50% flowering (DF), plant height (PH), branch number (BN), pod number (PN), seed number (SN) and seed weight (SW) in chickpea. These trait-associated SNPs individually and in combination explained 10–23% and 32% phenotypic variation respectively for the studied yield component traits. Interestingly, novel non-synonymous coding SNP allelic variants in five potential candidate TF genes encoding SBP (squamosal promoter binding protein), SNF2 (sucrose non-fermenting 2), GRAS [Gibberellic acid insensitive (GAI)-Repressor of GAI (RGA)-SCARECROW (SCR)], bZIP (basic leucine zipper) and LOB (lateral organ boundaries)-domain proteins associated strongly with DF, PH, BN, PN, SN and SW traits respectively were found most promising in chickpea. The functionally relevant molecular signatures (TFs and natural SNP alleles) delineated by us have potential to accelerate marker-assisted genetic enhancement by developing high pod and seed yielding cultivars of chickpea. © 2017, Springer Science+Business Media, LLC.","Association mapping; Chickpea; Desi; Kabuli; SNP; Transcription factor",,2-s2.0-85027847095
"Bornhorst T.J., Mathur R.","Copper isotope constraints on the genesis of the keweenaw peninsula native copper district, Michigan, USA",2017,"Minerals",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031403220&doi=10.3390%2fmin7100185&partnerID=40&md5=2132f8a2a9b18d3914923e297317024b","The Keweenaw Peninsula native copper district of Michigan, USA is the largest concentration of native copper in the world. The copper isotopic composition of native copper was measured from stratabound and vein deposits, hosted by multiple rift-filling basalt-dominated stratigraphic horizons over 110 km of strike length. The δ65Cu of the native copper has an overall mean of +0.28‰and a range of-0.32‰to +0.80‰(excluding one anomalous value). The data appear to be normally distributed and unimodal with no substantial differences between the native copper isotopic composition from the wide spread of deposits studied here. This suggests a common regional and relatively uniform process of derivation and precipitation of the copper in these deposits. Several published studies indicate that the ore-forming hydrothermal fluids carried copper as Cu1+, which is reduced to Cu0 during the precipitation of native copper. The δ65Cu of copper in the ore-forming fluids is thereby constrained to +0.80‰or higher in order to yield the measured native copper values by reductive precipitation. The currently accepted hypothesis for the genesis of native copper relies on the leaching of copper from the rift-filling basalt-dominated stratigraphic section at a depth below the deposits during burial metamorphism. Oxidative dissolution of copper from magmatic source rocks with magmatic δ65Cu of 0‰ ± 0.3‰ is needed to obtain the copper isotopic composition of the metamorphogenic ore-forming hydrothermal fluids. In order to accommodate oxidative dissolution of copper from the rift-filling basalt source rocks, the copper needs to have been sited in native copper. Magmatic native copper in basalt is likely stable when the magma is low in sulfur. Low sulfur is predicted by the lack of sulfide minerals in the ore deposits and in the rift-filling basalt-dominated section, which are source rocks, the same rocks through which the ore fluids moved upwards, and the host rocks for the native copper ores. When combined with geologic evidence and inferences, the copper isotopic composition of native copper helps to further constrain the genetic model for this unique mining district. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Copper isotope; Keweenaw Peninsula; Michigan; Native copper; Ore genesis",,2-s2.0-85031403220
"Sas Z., Doherty R., Kovacs T., Soutsos M., Sha W., Schroeyers W.","Radiological evaluation of by-products used in construction and alternative applications; Part I. Preparation of a natural radioactivity database",2017,"Construction and Building Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020469944&doi=10.1016%2fj.conbuildmat.2017.05.167&partnerID=40&md5=6c61d27e06535aad0c62bf57e3e87614","To get an insight into the radiological features of potentially reusable by-products in the construction industry a review of the reported scientific data is necessary. This study is based on the continuously growing database of the By-BM (H2020-MSCA-IF-2015) project (By-products for Building Materials). Selection criteria were defined for manual data mining in such a way to avoid the collection of too heterogeneous datasets. Currently, the By-BM database contains individual data of about 431 by-products and 1095 construction and raw materials. The By-BM database only consists out of measurement information on individual samples and not out of processed data that only gives a rough summary (such as only a range or average) of experimental results. As a consequence of the statistical analysis of the data, it was found that in the case of the construction materials the natural isotope content had a wider distribution than the by-products. However, the average of the Ra-226, Th-232 and K-40 contents of reported by-products were 2.00, 2.11 and 0.48, while the median was found 1.97, 1.24 and 0.53 times higher than the construction materials, respectively. The calculated Radium equivalent concertation was greater than the accepted value for residential properties of 370 Bq/kg in the event of 10.3% of total construction materials and 42.4% of by-products, while the I-indexes were above 1.0 index value with 17.3% and 58.2%, respectively. From the obtained data, it can be concluded that the reuse of industrial by-products in construction materials for residential purposes, without due diligence, can pose elevated risks to residents as a result of their high-volume usage. © 2017 Elsevier Ltd","Building materials; By-products; Database; I-index; Mixing; Natural radiation; Reuse","Byproducts; Construction industry; Data mining; Database systems; Housing; Mixing; Heterogeneous datasets; Industrial by-products; Measurement information; Natural radiation; Natural radioactivity; Radiological evaluation; Radiological features; Reuse; Building materials",2-s2.0-85020469944
"Yang S., Zhou P., Duan K., Hossain M.S., Alhamid M.F.","emHealth: Towards Emotion Health Through Depression Prediction and Intelligent Health Recommender System",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030158799&doi=10.1007%2fs11036-017-0929-3&partnerID=40&md5=c91a2ccfb37f54d176a7170d5be6d16e","Depression is an important mental disease of global concern. Its complicated etiology and chronic clinical features make it difficult for users to be conscious of their own depression emotion and seriously threaten the patient’s life safety. With the development of e-commerce, intelligent recommender system has brought new opportunities to personalized health monitoring for the users with emotional distress. Therefore, this paper puts forward the emHealth system, which is an intelligent health recommendation system with depression prediction for emotion health. This paper explores the monitoring and improvement of users psychological and physiological conditions by pushing personalized therapy solutions to patients with emotional distress. Specifically, this paper first proposes the system architecture of emHealth. Then, we design personalized mobile phone Apps to collect emotional data of users with tendentious depressive mood, and find the five main external characteristics of depression by Pearson correlation analysis. We divide 1047 volunteers data into training set and test set, and construct prediction model of depression using decision tree and support vector machine algorithms. For the different external factors that lead to depression, we give personalized recommendation and intelligent decision-making solution, and push related emotional improvement suggestions to guide users behavior. Finally, a specific application scene is demonstrated where patient’s family member carry out psychological counseling for the patient, to verify the practicability and validity of the system. The beneficial effects of this system can meet the needs of the electronic market and can be promoted and popularized. © 2017 Springer Science+Business Media, LLC","Depression detection; E-commerce; Recommender systems; Smart devices","Commerce; Correlation methods; Data mining; Decision making; Decision trees; Electronic commerce; Forecasting; Health; Patient treatment; Trees (mathematics); External characteristic; Intelligent decision making; Intelligent recommender system; Pearson correlation analysis; Personalized recommendation; Physiological condition; Smart devices; Support vector machine algorithm; Recommender systems",2-s2.0-85030158799
"Kim D., Yu S.J.","Hotel review mining for targeting strategy: Focusing on chinese free independent traveler",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030314259&partnerID=40&md5=d087eb37bf3c6cded60437044f4d5edf","This study aims to construct an appropriate targeting strategy on the hotel guests who belong to a specific cultural boundary by comparing according to text analysis of 12,540 reviews written by Chinese FIT (Free Independent Traveler) and 34,330 by people from others covering 20 hotels in Seoul from Jan. 2013 to June. 2016. This study consists of three steps. In step I, in order to practice topic modeling, we examined using LDA (Latent Dirichlet Allocation). We extracted the keywords for the experiment of the next step. In step II, we investigated which words the keywords extracted from the topic model are linked with by using text-link analysis to make up for the limitations of LDA in terms of interpretation of data. Based on two steps above, in step III, we found the hidden concepts related to keywords and constructed the targeting strategy on Chinese FIT for hotel decision makers. © 2005 - Ongoing JATIT & LLS.","Chinese FIT; LDA (latent dirichlet allocation); Targeting stategy; Text mining; Text-link analysis",,2-s2.0-85030314259
"Yeo H.J.","Medical service improvement through patient’s queue decision mining",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030331572&partnerID=40&md5=014d20e845129f0a4bc086c23b460963","Since a hospital adapt computer system based service such as patient’s profile and disease database, payment, and reservation, the data have been stored and analyzed for better performance of patient’s care. Despite of computer based patient’s healthcare service which is core service of hospital, one of the major complain comes from patients, the waiting queue is not well solved because of hospital physical resource limitation. In that, an effective service route to reduce patient’s queue has been studied in diverse ways. In this research we clarify the service route and find out solution to reduce queue time for better patients service through sample hospital’s the orthopedics department patient data. This research utilize ProM software to clarify service process and suggest process innovation via queueing theory not process mining way because only doctor schedule is suitable to change in the process. © 2005 - Ongoing JATIT & LLS.","Clinical pathway; Medical process; Medical service; Queue theory",,2-s2.0-85030331572
"Taboada S., Kenny N.J., Riesgo A., Wiklund H., Paterson G.L.J., Dahlgren T.G., Glover A.G.","Mitochondrial genome and polymorphic microsatellite markers from the abyssal sponge Plenaster craigi Lim & Wiklund, 2017: tools for understanding the impact of deep-sea mining",2017,"Marine Biodiversity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030181944&doi=10.1007%2fs12526-017-0786-0&partnerID=40&md5=5029fa92e42b0a899263ad8d666c68c1","The abyssal demosponge Plenaster craigi is endemic to the Clarion - Clipperton Zone (CCZ) in the NE Pacific, a region with abundant seafloor polymetallic nodules and of potential interest for mining. Plenaster craigi encrusts on these nodules and is an abundant component of the ecosystem. To assess the impact of mining operations, it is crucial to understand the genetics of this species, because its genetic diversity and connectivity across the area may be representative of other nodule-encrusting invertebrate epifauna. Here we describe and characterize 14 polymorphic microsatellite markers from this keystone species using Illumina MiSeq, tested for 75 individuals from three different areas across the CCZ, including an Area of Particular Environmental Interest (APEI-6) and two areas within the adjacent UK1 mining exploration area. The number of alleles per locus ranged from 3 to 30 (13.33 average alleles for all loci across areas). Observed and expected heterozygosity ranged from 0.909–0.048 and from 0.954–0.255, respectively. Several loci displayed significant deviation from the Hardy-Weinberg equilibrium, which appears to be common in other sponge studies. The microsatellite loci described here will be used to assess the genetic structure and connectivity on populations of the sponge across the CCZ, which will be invaluable for monitoring the impact of mining operations on its habitat. Also, we provide the annotated mitochondrial genome of P. craigi, compare its arrangement with other closely related species, and discuss the phylogenetic framework for the sponge after Maximum Likelihood and Bayesian Inference analyses using nucleotide and amino acid sequences data sets separately. © 2017 The Author(s)","Clarion-Clipperton Zone; Conservation genetics; Marine protected area; Next-generation sequencing; Polymetallic nodules; Population genetics",,2-s2.0-85030181944
"Chung D., Lawson A., Zheng W.J.","A statistical framework for biomedical literature mining",2017,"Statistics in Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021697702&doi=10.1002%2fsim.7384&partnerID=40&md5=f9f3fde854ced1c9d2f5bfcfcd275310","In systems biology, it is of great interest to identify new genes that were not previously reported to be associated with biological pathways related to various functions and diseases. Identification of these new pathway-modulating genes does not only promote understanding of pathway regulation mechanisms but also allow identification of novel targets for therapeutics. Recently, biomedical literature has been considered as a valuable resource to investigate pathway-modulating genes. While the majority of currently available approaches are based on the co-occurrence of genes within an abstract, it has been reported that these approaches show only sub-optimal performances because 70% of abstracts contain information only for a single gene. To overcome such limitation, we propose a novel statistical framework based on the concept of ontology fingerprint that uses gene ontology to extract information from large biomedical literature data. The proposed framework simultaneously identifies pathway-modulating genes and facilitates interpreting functions of these new genes. We also propose a computationally efficient posterior inference procedure based on Metropolis–Hastings within Gibbs sampler for parameter updates and the poor man's reversible jump Markov chain Monte Carlo approach for model selection. We evaluate the proposed statistical framework with simulation studies, experimental validation, and an application to studies of pathway-modulating genes in yeast. The R implementation of the proposed model is currently available at https://dongjunchung.github.io/bayesGO/. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd.","Bayesian hierarchical model; biological pathway; gene ontology; literature search; ontology fingerprint","analytic method; Article; controlled study; DNA fingerprinting; gene function; gene ontology; human; medical literature; medical research; process model; theory validation",2-s2.0-85021697702
"Ma Y., Hou G., Xin B.","Green process innovation and innovation benefit: The mediating effect of firm image",2017,"Sustainability (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030461094&doi=10.3390%2fsu9101778&partnerID=40&md5=c36fa63c5452d7cd53cc0706547064e3","By evaluating green process innovation and its innovator's benefit including short- and long-term dimensions, we first analyzed the relationship between green process innovation and its benefits. Second, we set up a regression model to test the hypotheses using 267 survey data from coal mining firms in China. Finally, we verified the positive relationship between green process innovation and its long-term benefit, and the non-significant relationship between green process innovation and its short-term benefit, and the mediating effect played by firm image in the long run. © 2017 by the authors.","Firm benefit; Firm image; Green process innovation; Mediating effect","coal mining; firm size; hypothesis testing; manufacturing; China",2-s2.0-85030461094
"Xu Y., Wu C., Zheng K., Niu X., Lu T.","Feature selection to mine joint features from high-dimension space for android malware detection",2017,"KSII Transactions on Internet and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030870106&doi=10.3837%2ftiis.2017.09.026&partnerID=40&md5=17ba5be5c54ff228cc999ec0ab151e56","Android is now the most popular smartphone platform and remains rapid growth. There are huge number of sensitive privacy information stored in Android devices. Kinds of methods have been proposed to detect Android malicious applications and protect the privacy information. In this work, we focus on extracting the fine-grained features to maximize the information of Android malware detection, and selecting the least joint features to minimize the number of features. Firstly, permissions and APIs, not only from Android permissions and SDK APIs but also from the developer-defined permissions and third-party library APIs, are extracted as features from the decompiled source codes. Secondly, feature selection methods, including information gain (IG), regularization and particle swarm optimization (PSO) algorithms, are used to analyze and utilize the correlation between the features to eliminate the redundant data, reduce the feature dimension and mine the useful joint features. Furthermore, regularization and PSO are integrated to create a new joint feature mining method. Experiment results show that the joint feature mining method can utilize the advantages of regularization and PSO, and ensure good performance and efficiency for Android malware detection. © 2017 KSII.","Android malware detection; Feature selection; Joint features; Particle swarm optimization; Regularization","Computer crime; Feature extraction; Malware; Mining; Particle swarm optimization (PSO); Android malware; Feature dimensions; Feature selection methods; High dimensions; Information gain; Particle swarm optimization algorithm; Privacy information; Regularization; Android (operating system)",2-s2.0-85030870106
"Reichman D., Collins L.M., Malof J.M.","On Choosing Training and Testing Data for Supervised Algorithms in Ground-Penetrating Radar Data for Buried Threat Detection",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782689&doi=10.1109%2fTGRS.2017.2750920&partnerID=40&md5=856f0617ec84ea25db85b46dbe9b2822","Ground-penetrating radar (GPR) is one of the most popular and successful sensing modalities that have been investigated for landmine and subsurface threat detection. Many of the detection algorithms applied to this task are supervised and therefore require labeled examples of threat and nonthreat data for training. Training data most often consist of 2-D images (or patches) of GPR data, from which features are extracted and provided to the classifier during training and testing. Identifying desirable training and testing locations to extract patches, which we term ""keypoints,"" is well established in the literature. In contrast, however, a large variety of strategies have been proposed regarding keypoint utilization (e.g., how many of the identified keypoints should be used at threat, or nonthreat, locations). Given a variety of keypoint utilization strategies that are available, it is very unclear: 1) which strategies are best or 2) whether the choice of strategy has a large impact on classifier performance. We address these questions by presenting a taxonomy of existing utilization strategies and then evaluating their effectiveness on a large data set using many different classifiers and features. We analyze the results and propose a new strategy, called PatchSelect, which outperforms other strategies across all experiments. IEEE","Algorithm design and analysis; Data mining; Ground penetrating radar; Ground-penetrating radar (GPR); landmine detection; Sensors; Taxonomy; Testing; Training; training","Bombs (ordnance); Classification (of information); Data mining; Explosives; Geological surveys; Landmine detection; Personnel training; Radar; Sensors; Taxonomies; Testing; Algorithm design and analysis; Classifier performance; Detection algorithm; Ground Penetrating Radar; Ground penetrating radar (GPR); Supervised algorithm; Training and testing; Utilization strategy; Ground penetrating radar systems",2-s2.0-85030782689
"Gonzalez O., O’Rourke H.P., Wurpts I.C., Grimm K.J.","Analyzing Monte Carlo Simulation Studies With Classification and Regression Trees",2017,"Structural Equation Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030183125&doi=10.1080%2f10705511.2017.1369353&partnerID=40&md5=cb1133d8b187eb381d227ff800e72df6","Monte Carlo simulations are an important tool for researchers to study statistical properties of estimators, such as parameter bias, or the limits of various modeling approaches. Typically, the immense amount of data produced by Monte Carlo studies is analyzed with regression or analysis of variance, and researchers are faced with making arbitrary decisions regarding what effects to report and what interactions to test. Understanding current limitations, we propose a classification and regression trees (CART) approach from the statistical learning and data mining field to analyze Monte Carlo simulation data. We demonstrate the advantages of the CART approach and several extensions by reanalyzing and interpreting results from one published Monte Carlo study and one fully reproducible simulation example. Results suggest that CART is able to arrive at the same conclusions as current descriptive and inferential approaches and, at the same time, provide additional insight on the complex interactions among simulation factors. © 2017, Routledge. All rights reserved.","classification and regression trees; data mining; interactions; Monte Carlo simulations","Beam plasma interactions; Data mining; Forestry; Intelligent systems; Regression analysis; Trees (mathematics); Classification and regression tree; Current limitation; Model approach; Simulation example; Statistical learning; Statistical properties; Monte Carlo methods",2-s2.0-85030183125
"Brown L.A., Williams J., Taylor L., Thomson R.J., Nolan P.M., Foster R.G., Peirson S.N.","Meta-analysis of transcriptomic datasets identifies genes enriched in the mammalian circadian pacemaker",2017,"Nucleic acids research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031696444&doi=10.1093%2fnar%2fgkx714&partnerID=40&md5=03371df5b25316c4553fe03fe908aba3","The master circadian pacemaker in mammals is located in the suprachiasmatic nuclei (SCN) which regulate physiology and behaviour, as well as coordinating peripheral clocks throughout the body. Investigating the function of the SCN has often focused on the identification of rhythmically expressed genes. However, not all genes critical for SCN function are rhythmically expressed. An alternative strategy is to characterize those genes that are selectively enriched in the SCN. Here, we examined the transcriptome of the SCN and whole brain (WB) of mice using meta-analysis of publicly deposited data across a range of microarray platforms and RNA-Seq data. A total of 79 microarrays were used (24 SCN and 55 WB samples, 4 different microarray platforms), alongside 17 RNA-Seq data files (7 SCN and 10 WB). 31 684 MGI gene symbols had data for at least one platform. Meta-analysis using a random effects model for weighting individual effect sizes (derived from differential expression between relevant SCN and WB samples) reliably detected known SCN markers. SCN-enriched transcripts identified in this study provide novel insights into SCN function, including identifying genes which may play key roles in SCN physiology or provide SCN-specific drivers. © The Author(s) 2017. Published by Oxford University Press on behalf of Nucleic Acids Research.",,"transcriptome; animal; brain chemistry; C57BL mouse; circadian rhythm; data mining; DNA microarray; gene ontology; gene regulatory network; genetics; information processing; male; meta analysis; molecular genetics; mouse; physiology; sequence analysis; suprachiasmatic nucleus; Animals; Brain Chemistry; Circadian Clocks; Circadian Rhythm; Data Mining; Datasets as Topic; Gene Ontology; Gene Regulatory Networks; Male; Mice; Mice, Inbred C57BL; Molecular Sequence Annotation; Oligonucleotide Array Sequence Analysis; Sequence Analysis, RNA; Suprachiasmatic Nucleus; Transcriptome",2-s2.0-85031696444
"Oh H.-J., Lee S.","Shallow landslide susceptibility modeling using the data mining models artificial neural network and boosted tree",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030115764&doi=10.3390%2fapp7101000&partnerID=40&md5=fb8e0d28a354093a1702065a88e408a6","The main purpose of this paper is to present some potential applications of sophisticated data mining techniques, such as artificial neural network (ANN) and boosted tree (BT), for landslide susceptibility modeling in the Yongin area, Korea. Initially, landslide inventory was detected from visual interpretation using digital aerial photographic maps with a high resolution of 50 cm taken before and after the occurrence of landslides. The debris flows were randomly divided into two groups: training and validation sets with a 50:50 proportion. Additionally, 18 environmental factors related to landslide occurrence were derived from the topography, soil, and forest maps. Subsequently, the data mining techniques were applied to identify the influence of environmental factors on landslide occurrence of the training set and assess landslide susceptibility. Finally, the landslide susceptibility indexes from ANN and BT were compared with a validation set using a receiver operating characteristics curve. The slope gradient, topographic wetness index, and timber age appear to be important factors in landslide occurrence from both models. The validation result of ANN and BT showed 82.25% and 90.79%, which had reasonably good performance. The study shows the benefit of selecting optimal data mining techniques in landslide susceptibility modeling. This approach could be used as a guideline for choosing environmental factors on landslide occurrence and add influencing factors into landslide monitoring systems. Furthermore, this method can rank landslide susceptibility in urban areas, thus providing helpful information when selecting a landslide monitoring site and planning land-use. © 2017 by the authors.","Artificial neural network; Boosted tree; Landslide inventory; Landslide susceptibility",,2-s2.0-85030115764
"Wu Y., Tong Y., Zhu X., Wu X.","NOSEP: Nonoverlapping Sequence Pattern Mining With Gap Constraints",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030767667&doi=10.1109%2fTCYB.2017.2750691&partnerID=40&md5=ba13b74b8ce3b9f824e0dec3eb0c2a22","Sequence pattern mining aims to discover frequent subsequences as patterns in a single sequence or a sequence database. By combining gap constraints (or flexible wildcards), users can specify special characteristics of the patterns and discover meaningful subsequences suitable for their own application domains, such as finding gene transcription sites from DNA sequences or discovering patterns for time series data classification. Due to the inherent complexity of sequence patterns, including the exponential candidate space with respect to pattern letters and gap constraints, to date, existing sequence pattern mining methods are either incomplete or do not support the Apriori property because the support ratio of a pattern may be greater than that of its subpatterns. Most importantly, patterns discovered by these methods are either too restrictive or too general and cannot represent underlying meaningful knowledge in the sequences. In this paper, we focus on a nonoverlapping sequence pattern (NOSEP) mining task with gap constraints, where an NOSEP allows sequence letters to be flexibly and maximally utilized for pattern discovery. A new Apriori-based NOSEP mining algorithm is proposed. NOSEP is a complete pattern mining algorithm, which uses a specially designed data structure, Nettree, to calculate the exact occurrence of a pattern in the sequence. Experimental results and comparisons on biology DNA sequences, time series data, and Gazelle datasets demonstrate the efficiency of the proposed algorithm and the uniqueness of NOSEPs compared to other methods. IEEE","Algorithm design and analysis; Data mining; DNA; Gap constraint; Nettree; nonoverlapping; Pattern matching; pattern matching; Ports (Computers); sequence pattern mining; Time series analysis","Bioinformatics; DNA; DNA sequences; Pattern matching; Time series analysis; Transcription; Algorithm design and analysis; Gap constraint; Nettree; Nonoverlapping; Ports (Computers); Sequence pattern mining; Data mining",2-s2.0-85030767667
"Yang G., Han X., Wang C., Ding Y., Liu K., Tian D., Yao L.","The basicity analysis of sintered ore using laser-induced breakdown spectroscopy (LIBS) combined with random forest regression (RFR)",2017,"Analytical Methods",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029883274&doi=10.1039%2fc7ay01389b&partnerID=40&md5=43581868a393e8c1614246fe19f9b581","The basicity of sintered ore, which is related to the melting point of the sinter, is vital to ore mining and blast-furnace smelting. Laser-induced breakdown spectroscopy (LIBS) with random forest regression (RFR) has been applied for measuring the basicity of sintered ore, which can be defined by the concentrations of oxides: CaO, SiO2, Al2O3 and MgO. In this work, thirty sintered ore samples are used, of which twenty samples are used for the calibration set to construct the random forest regression (RFR) calibration model for the above-mentioned oxides and ten samples are used for the test set. The characteristic lines of the main components in the sintered ore are identified using the National Institute of Standards and Technology (NIST) database. Two model parameters (the number of decision trees-ntree and the number of random variables-mtry) of the RFR were optimized by out-of-bag (OOB) error estimation for improving the predictive accuracy of the RFR model. The RFR model was applied to sample measurements and the results were compared with partial least squares regression (PLSR) models. The RFR model has shown better predictive capabilities than the PLSR model. In order to verify the stability of the RFR model, fifty measurements were made and the relative standard deviation (RSD) of the data is between 0.27% and 0.59%. Therefore, LIBS combined with RFR could be a promising method for real-time online, rapid analysis in mining and mineral processing industries. © The Royal Society of Chemistry 2017.",,"Atomic emission spectroscopy; Blast furnaces; Decision trees; Laser induced breakdown spectroscopy; Least squares approximations; Ores; Regression analysis; Sintering; Smelting; Blast-furnace smelting; Characteristic lines; Laserinduced breakdown spectroscopy (LIBS); Mineral processing industry; National Institute of Standards and Technology; Partial least squares regressions (PLSR); Predictive capabilities; Relative standard deviations; Ore analysis",2-s2.0-85029883274
"Luna J.M., Padillo F., Pechenizkiy M., Ventura S.","Apriori Versions Based on MapReduce for Mining Frequent Patterns on Big Data",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030756215&doi=10.1109%2fTCYB.2017.2751081&partnerID=40&md5=7ec17a1765ae7d55fa5ca88d8df784f9","Pattern mining is one of the most important tasks to extract meaningful and useful information from raw data. This task aims to extract item-sets that represent any type of homogeneity and regularity in data. Although many efficient algorithms have been developed in this regard, the growing interest in data has caused the performance of existing pattern mining techniques to be dropped. The goal of this paper is to propose new efficient pattern mining algorithms to work in big data. To this aim, a series of algorithms based on the MapReduce framework and the Hadoop open-source implementation have been proposed. The proposed algorithms can be divided into three main groups. First, two algorithms [Apriori MapReduce (AprioriMR) and iterative AprioriMR] with no pruning strategy are proposed, which extract any existing item-set in data. Second, two algorithms (space pruning AprioriMR and top AprioriMR) that prune the search space by means of the well-known anti-monotone property are proposed. Finally, a last algorithm (maximal AprioriMR) is also proposed for mining condensed representations of frequent patterns. To test the performance of the proposed algorithms, a varied collection of big data datasets have been considered, comprising up to 3 &#x00B7; 10#x00B9;&#x2078; transactions and more than 5 million of distinct single-items. The experimental stage includes comparisons against highly efficient and well-known pattern mining algorithms. Results reveal the interest of applying MapReduce versions when complex problems are considered, and also the unsuitability of this paradigm when dealing with small data. IEEE","Algorithm design and analysis; Big Data; Big data; Computer science; Data mining; Hadoop; MapReduce; Open source software; pattern mining; Proposals","Big data; Computer science; Iterative methods; Open source software; Open systems; Software engineering; Algorithm design and analysis; Hadoop; Map-reduce; Pattern mining; Proposals; Data mining",2-s2.0-85030756215
"Yang Y., Li Z., Wang W., Tao D.","An adaptive semi-supervised clustering approach via multiple density-based information",2017,"Neurocomputing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012914444&doi=10.1016%2fj.neucom.2016.11.061&partnerID=40&md5=737c3b61946e9be0851c2729eacdb423","Since multimedia information has been dramatically increasing, multimedia data mining has drawn much more attentions than ever. As one of important mining tasks, clustering provided underpinning techniques for discovering the intrinsic structure and condensing information over large amount of multimedia data. Although many approaches have been proposed to improve performance and accuracy of clustering process in semi-supervised learning way, they are still quite sensitive to the assumptions of cluster structures and algorithm parameters setups. In this paper, we propose an adaptive semi-supervised clustering approach via multiple density-based information. It can automatically determine sets of important density-based parameters in use of both labeled and unlabeled data. Based on the different sets of density-based parameters, our approach is able to adaptively identify complex cluster structures in different size, shape and density without knowing the number of clusters; moreover, it is quite insensitive to the noise. Our approach has been generally evaluated on synthetic data and a collection of benchmark data sets, and yields promising clustering results. Furthermore, it has been also applied on Yale face database B, and simulation results specifically show its potential for practical application in face recognitions. © 2017","Constraint information; Constraint-based clustering; Density-based clustering; Semi-supervised learning","Clustering algorithms; Data acquisition; Face recognition; Supervised learning; Constraint information; Constraint-based clustering; Density-based Clustering; Labeled and unlabeled data; Multimedia data mining; Multimedia information; Semi- supervised learning; Semi-supervised Clustering; Data mining; Article; Bayesian learning; calculation; classifier; cluster analysis; data analysis; density; density based adaptive semi supervised clustering; facial recognition; learning algorithm; mathematical computing; measurement accuracy; priority journal; process optimization; simulation",2-s2.0-85012914444
"Lekha S., M S.","Real-Time Non-Invasive Detection and Classification of Diabetes using Modified Convolution Neural Network",2017,"IEEE Journal of Biomedical and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030749367&doi=10.1109%2fJBHI.2017.2757510&partnerID=40&md5=ae8544e54ef56295db8d1a36b9766b76","Non-invasive diabetes prediction has been gaining prominence over the last decade. Among many human serums evaluated, human breath emerges as a promising option with acetone levels in breath exhibiting a good correlation to blood glucose levels. Such correlation establishes acetone as an acceptable bio-marker for diabetes. The most common data analysis strategies to analyze the bio-markers in breath for disease detection use feature extraction and classification algorithms. However snags such as computational cost and lack of optimal feature selection on application to real time signals reduce the efficiency of such analysis. This paper explores the use of a 1 dimensional (1D) modified convolution neural network (CNN) algorithm which combines feature extraction and classification techniques. The approach proposed in this paper is found to significantly reduce the limitations associated with using these techniques individually and thereby improving the classifier&#x0027;s performance further. This paper proposes to apply a modified 1D CNN on real-time breath signals obtained from an array of gas sensors. The experimentation and the performance of the system is carried out and evaluated. IEEE","acetone; breath; Convolution; convolution neural network; Diabetes; diabetes; Feature extraction; Gas detectors; Neural networks; Non-invasive; Sensor arrays","Acetone; Biomedical signal processing; Classification (of information); Convolution; Data mining; Extraction; Gas detectors; Medical problems; Neural networks; Sensor arrays; Analysis strategies; breath; Computational costs; Convolution neural network; Feature extraction and classification; Non-invasive; Non-invasive detection; Optimal feature selections; Feature extraction",2-s2.0-85030749367
"Gao S., Li X., Yu Z., Qin Y., Zhang Y.","Combining paper cooperative network and topic model for expert topic analysis and extraction",2017,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012883542&doi=10.1016%2fj.neucom.2016.12.074&partnerID=40&md5=893eaf5b52a538f88bc5588bd80d6b9a","Paper cooperation network embodies expert topic similarity in an extent, thus, a novel method is proposed for expert topic analysis and extraction by combining paper cooperation network and topic model. In the method, we extract each paper’ author information and construct an expert cooperation network. At the same time, by means of LDA model, a probabilistic topic model is also built to analyze papers’ latent topics. Then, by making full use of the feature that adjacent nodes in the expert cooperation network share similar themes distribution, we makes a constraint on expert topic distribution in Gibbs sampling process of solving the probabilistic topic model. Experimental results on NIPS dataset show that the proposed method can effectively extract expert topics, and the expert paper cooperation network plays a very good supporting role on the extracting task. © 2017","Expert topic analysis; Expert topic extraction; Gibbs sampling; Paper cooperation network; Probabilistic topic model","Paper; Probability distributions; Cooperation networks; Gibbs sampling; Probabilistic topic models; Topic analysis; Topic extraction; Data mining; algorithm; Article; Bayes theorem; calculation; controlled study; expert paper cooperation network; latent dirichlet allocation; mathematical computing; Monte Carlo method; Poisson distribution; priority journal; probabilistic topic model; probability; statistical analysis; statistical model",2-s2.0-85012883542
"Chen F., Ji R., Su J., Cao D., Gao Y.","Predicting Microblog Sentiments via Weakly Supervised Multi-Modal Deep Learning",2017,"IEEE Transactions on Multimedia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030762454&doi=10.1109%2fTMM.2017.2757769&partnerID=40&md5=f8c7843ece374b15bc510ebc03eba56d","Predicting sentiments of multi-modal microblogs composing of text, image, and emoticon has attracted everincreasing research focus recently. The key challenge lies in the difficulty of collecting sufficient amount of training labels to train a discriminative model for multi-modal prediction. One potential solution is to exploit the labels collected from social media users, which is however restricted by negative effect of label noise. Besides, we have quantitatively found that sentiments in different modalities may be independent, which disables the usage of previous multi-modal sentiment analysis schemes in our problem. In this paper, we introduce a Weakly Supervised Multi-modal Deep Learning (WS-MDL) scheme towards robust and scalable sentiment prediction. WS-MDL learns convolutional neural networks iteratively and selectively from &#x201C;weak&#x201D; emoticon labels, which are cheaply available and noise containing. In particular, to filter out the label noise and to capture the modality dependency, a probabilistic graphical model is introduced to simultaneously learn discriminative multi-modal descriptors and infer the confidence of label noise. Extensive evaluations are conducted in a million-scale, real-world microblog sentiment dataset crawled from SinaWeibo.We have validated the merits of the proposed scheme by quantitatively showing its superior performance over several state-of-the-art and alternative approaches. IEEE","Deep learning; Multi-modality; Sentiment prediction; Weakly supervised learning","Data mining; Deep learning; Flow visualization; Learning systems; Neural networks; Supervised learning; Multi modality; Noise measurements; Sentiment analysis; Social network services; Weakly supervised learning; Forecasting",2-s2.0-85030762454
"Arulmurugan R., Sabarmathi K.R., Anandakumar H.","Classification of sentence level sentiment analysis using cloud machine learning techniques",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030089501&doi=10.1007%2fs10586-017-1200-1&partnerID=40&md5=7fd8eac69874528dcdb35e2188199839","Cloud machine learning (CML) techniques offer contemporary machine learning services, with pre-trained models and a service to generate own personalized models. This paper presents a completely unique emotional modeling methodology for incorporating human feeling into intelligent systems. The projected approach includes a technique to elicit emotion factors from users, a replacement illustration of emotions and a framework for predicting and pursuit user’s emotional mechanical phenomenon over time. The neural network based CML service has better training concert and enlarged exactness compare to other large scale deep learning systems. Opinions are important to almost all human activities and cloud based sentiment analysis is concerned with the automatic extraction of sentiment related information from text. With the rising popularity and availability of opinion rich resources such as personal blogs and online appraisal sites, new opportunities and issues arise as people now, actively use information technologies to explore and capture others opinions. In the existing system, a segmentation ranking model is designed to score the usefulness of a segmentation candidate for sentiment classification. A classification model is used for predicting the sentiment polarity of segmentation. The joint framework is trained directly using the sentences annotated with only sentiment polarity, without the use of any syntactic or sentiment annotations in segmentation level. However the existing system still has issue with classification accuracy results. To improve the classification performance, in the proposed system, cloud integrate the support vector machine, naive bayes and neural network algorithms along with joint segmentation approaches has been proposed to classify the very positive, positive, neutral, negative and very negative features more effectively using important feature selection. Also to handle the outliers we apply modified k-means clustering method on the given dataset. It is used to cloud cluster the outliers and hence the label as well as unlabeled features is handled efficiently. From the experimental result, we conclude that the proposed system yields better performance than the existing system. © 2017 Springer Science+Business Media, LLC","Classification; Cloud clustering; Cloud machine learning; Segmentation; Sentiment analysis","Artificial intelligence; Classification (of information); Cluster analysis; Clustering algorithms; Data mining; Image segmentation; Intelligent systems; Learning algorithms; Statistics; Classification accuracy; Classification models; Classification performance; Machine learning techniques; Modified k-means clustering; Neural network algorithm; Sentiment analysis; Sentiment classification; Learning systems",2-s2.0-85030089501
"Sofianto I.A., Inagaki T., Kato K., Itoh M., Tsuchikawa S.","Modulus of elasticity prediction model on sugi (Cryptomeria japonica) lumber using online near-infrared (NIR) spectroscopic system",2017,"International Wood Products Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031419270&doi=10.1080%2f20426445.2017.1378398&partnerID=40&md5=490032cfe30bb5dfe49c0003975f57f4","In this study, static bending measurements and online NIR spectra acquisitions were combined to construct modulus of elasticity (MOE) prediction model for sugi lumber. NIR spectra were acquired from tangential surface of sugi lumbers at a speed of 120 m min−1 to assess its effectiveness in the wood industry. Cross-validation partial least squares regression (CV-PLSR) and test-set-validation partial least squares regression (TSV-PLSR) analyses were employed for analysing the data. The second derivative (2d) spectra with 19 smoothing points (Savitzky–Golay algorithm, second polynomial) gave the best result as spectral pre-processing treatment with the lowest root mean square error of cross-validation and the highest coefficient of determination for cross-validation based on the optimum number of latent variables as assessed from the minimum validation residual variance value in the CV-PLSR analysis. These 2d spectra were then used in the TSV-PLSR analysis for 100 repetitions to check the robustness of the calibration. © 2017 IWSc, The Wood Technology Society of the Institute of Materials, Minerals and Mining","lumber; MOE; NIR; online spectroscopic system; sugi (Cryptomeria japonica)","Elastic moduli; Least squares approximations; Lumber; Mean square error; Coefficient of determination; Cross validation; Cryptomeria japonica; Partial least squares regression; Residual variance; Root mean square errors; Second derivatives; Spectroscopic systems; Infrared devices",2-s2.0-85031419270
"Sherief A.","Mining Dynamics: Using Data Mining Techniques to Analyze Multi-agent Learning",2017,"Journal of Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031113633&doi=10.1515%2fjisys-2016-0136&partnerID=40&md5=8adf6a86179ad19c4173df78ca7dd581","Analyzing the learning dynamics in multi-agent systems (MASs) has received growing attention in recent years. Theoretical analysis of the dynamics was only possible in simple domains and simple algorithms. When one or more of these restrictions do not apply, theoretical analysis becomes prohibitively difficult, and researchers rely on experimental analysis instead. In experimental analysis, researchers have used some global performance metric(s) as a rough approximation to the internal dynamics of the adaptive MAS. For example, if the overall payoff improved over time and eventually appeared to stabilize, then the learning dynamics were assumed to be stable as well. In this paper, we promote a middle ground between the thorough theoretical analysis and the high-level experimental analysis. We introduce the concept of mining dynamics and propose data-mining-based methodologies to analyze multi-agent learning dynamics. Using our methodologies, researchers can identify clusters of learning parameter values that lead to similar performance, and discover frequent sequences in agent dynamics. We verify the potential of our approach using the well-known iterated prisoner's dilemma (with multiple states) domain. © 2017 Walter de Gruyter GmbH, Berlin/Boston.","68T05 Learning and adaptive systems; 68T42 Agent technology","Data mining; Dynamics; Learning systems; Agent technology; Experimental analysis; Global performance; Iterated prisoner's dilemma; Learning and adaptive system; Learning parameters; Multi-agent learning; Rough approximations; Multi agent systems",2-s2.0-85031113633
"Khoshahval S., Farnaghi M., Taleai M.","Spatio-temporal pattern mining on trajectory data using ARM",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032334747&doi=10.5194%2fisprs-archives-XLII-4-W4-395-2017&partnerID=40&md5=6afdf2e5f7d95c6c3eeec59a212e9a14","Preliminary mobile was considered to be a device to make human connections easier. But today the consumption of this device has been evolved to a platform for gaming, web surfing and GPS-enabled application capabilities. Embedding GPS in handheld devices, altered them to significant trajectory data gathering facilities. Raw GPS trajectory data is a series of points which contains hidden information. For revealing hidden information in traces, trajectory data analysis is needed. One of the most beneficial concealed information in trajectory data is user activity patterns. In each pattern, there are multiple stops and moves which identifies users visited places and tasks. This paper proposes an approach to discover user daily activity patterns from GPS trajectories using association rules. Finding user patterns needs extraction of user's visited places from stops and moves of GPS trajectories. In order to locate stops and moves, we have implemented a place recognition algorithm. After extraction of visited points an advanced association rule mining algorithm, called Apriori was used to extract user activity patterns. This study outlined that there are useful patterns in each trajectory that can be emerged from raw GPS data using association rule mining techniques in order to find out about multiple users' behaviour in a system and can be utilized in various location-based applications.","Apriori algorithm; Association rule mining; Frequent pattern mining; Location-based application; User trajectory","ARM processors; Association rules; Data mining; Extraction; Remote sensing; Application capability; Apriori algorithms; Daily activity patterns; Frequent pattern mining; Location-based applications; Rule mining algorithms; Rule mining techniques; Spatiotemporal patterns; Trajectories",2-s2.0-85032334747
"Ayinde B.O., Zurada J.M.","Deep Learning of Constrained Autoencoders for Enhanced Understanding of Data",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030765474&doi=10.1109%2fTNNLS.2017.2747861&partnerID=40&md5=4f2e294f174ff91823d0aaa95c53c862","Unsupervised feature extractors are known to perform an efficient and discriminative representation of data. Insight into the mappings they perform and human ability to understand them, however, remain very limited. This is especially prominent when multilayer deep learning architectures are used. This paper demonstrates how to remove these bottlenecks within the architecture of non-negativity constrained autoencoder. It is shown that using both L1 and L2 regularizations that induce non-negativity of weights, most of the weights in the network become constrained to be non-negative, thereby resulting into a more understandable structure with minute deterioration in classification accuracy. Also, this proposed approach extracts features that are more sparse and produces additional output layer sparsification. The method is analyzed for accuracy and feature interpretation on the MNIST data, the NORB normalized uniform object data, and the Reuters text categorization data set. IEEE","Computer architecture; Data mining; Data models; Decoding; Deep learning (DL); Encoding; Feature extraction; part-based representation; receptive field; sparse autoencoder (SAE); Training; white-box model.","Computer architecture; Data structures; Decoding; Deep learning; Encoding (symbols); Feature extraction; Learning systems; Network architecture; Personnel training; Text processing; Auto encoders; Classification accuracy; Feature extractor; Learning architectures; Part-based representation; Receptive fields; Text categorization; White-box models; Data mining",2-s2.0-85030765474
"Daneshtalab S., Rastiveis H.","Decision level fusion of orthophoto and Lidar data using confusion matrix information for lnad cover classification",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032356524&doi=10.5194%2fisprs-archives-XLII-4-W4-59-2017&partnerID=40&md5=5a69f914803a23058c3d6a365f0f7874","Automatic urban objects extraction from airborne remote sensing data is essential to process and efficiently interpret the vast amount of airborne imagery and Lidar data available today. The aim of this study is to propose a new approach for the integration of highresolution aerial imagery and Lidar data to improve the accuracy of classification in the city complications. In the proposed method, first, the classification of each data is separately performed using Support Vector Machine algorithm. In this case, extracted Normalized Digital Surface Model (nDSM) and pulse intensity are used in classification of LiDAR data, and three spectral visible bands (Red, Green, Blue) are considered as feature vector for the orthoimage classification. Moreover, combining the extracted features of the image and Lidar data another classification is also performed using all the features. The outputs of these classifications are integrated in a decision level fusion system according to the their confusion matrices to find the final classification result. The proposed method was evaluated using an urban area of Zeebruges, Belgium. The obtained results represented several advantages of image fusion with respect to a single shot dataset. With the capabilities of the proposed decision level fusion method, most of the object extraction difficulties and uncertainty were decreased and, the overall accuracy and the kappa values were improved 7% and 10%, respectively.","Classification; Confusion matrix; Decision level fusion; Lidar data; Orthophoto; SVM","Aerial photography; Data mining; Extraction; Image enhancement; Image fusion; Image resolution; Image retrieval; Matrix algebra; Optical radar; Remote sensing; Support vector machines; Accuracy of classifications; Airborne remote sensing; Classification results; Confusion matrices; Decision level fusion; LIDAR data; Ortho photos; Support vector machine algorithm; Classification (of information)",2-s2.0-85032356524
"Bordbari R., Maghsoudi Y., Salehi M.","Built-up area detection based on subspace projections using polarimetric SAR data",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032361931&doi=10.5194%2fisprs-archives-XLII-4-W4-37-2017&partnerID=40&md5=4818f05fd48d0c0a998eb72597bd9acc","The task of detecting and identifying objects remotely has long been an area of intense interest and active research. Active sensing of objects with radio waves is a whole new domain of target detection which is made available by radar remote sensors. Land cover/use information extraction is one of the most important applications of radar remote sensing, especially in urban areas. In this paper, we take a new look at the built-up area extraction problem in polarimetric SAR (PolSAR) data and assume canonical scattering mechanisms as our signal sources which combination of them with appropriate weight fractions formed a scattering vector of each pixel. The set of the scattering mechanisms is divided into two groups: the scattering mechanism of built-up area, and non-objected scattering mechanisms. Then, we describe a technique which simultaneously annihilates the effect of non-objected scattering mechanisms, and detects the presence of a scattering mechanism of interest. The experimental results on several quad-polarimetric datasets show the significant agreement with expected results, while saving computational complexity.","Built-up; Detection; Helix; Polarimetric SAR; Scattering mechanisms","Data mining; Error detection; Object detection; Polarimeters; Radar; Satellite communication systems; Synthetic aperture radar; Built-up; Built-up area detection; Helix; Polarimetric SAR; Polarimetric SAR data; Radar remote sensing; Radar remote sensors; Scattering mechanisms; Remote sensing",2-s2.0-85032361931
"Chindenga E., Scott M.S., Gurajena C.","Semantics based service orchestration in IoT",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032630933&doi=10.1145%2f3129416.3129438&partnerID=40&md5=f42e6d3a0148328a5c71d052f7c61cef","Internet of Things (IoT) presents a dynamic global revolution in the Internet where physical and virtual “things” communicate and share information. There arises a need to allow heterogeneous “things” to seamlessly interoperate, interact and exchange information and subsequently share services. Services are represented as functionalities that are offered by the “things”. In IoT context, service orchestration refers to identifying which devices or software components are required to formulate the requested service. IoT devices can offer different services based on the context and semantics. Service orchestration provides an approach to integration and interoperability which decouples applications from each other, and enhances capabilities to centrally manage and monitor components. This paper investigates requirements for semantic interoperability and exposes current challenges in IoT interoperability as a means of facilitating services orchestration in IoT. The paper proposes a semantic platform that allows heterogeneous devices to collaborate thereby enabling dynamic service orchestration. The proposed platform provides a common ontology-based framework for representing semantics allowing for a consistent information exchange format. The approach used combines domain-specific ontologies to obtain device and service abstraction for the purposes of process mining and analysis of service orchestration in heterogonous IoT environments. This approach was evaluated using a prototype IoT environment and mining techniques developed in the Process Mining Framework (ProM). The results revealed that semantically enriching service annotation and device descriptions through ontologies can successfully enable interoperability in heterogeneous IoT environments. Semantically enriched descriptions enable multiple perspectives of analysis on service orchestration processes and expose the steps involved in service request and provision in an a typical IoT setup. © 2017 Association for Computing Machinery.","Internet of Things; Ontology; Process mining; Semantics; Service Orchestration","Data mining; Engineers; Information dissemination; Interoperability; Ontology; Semantics; Domain-specific ontologies; Heterogeneous devices; Information exchanges; Internet of Things (IOT); Process mining; Semantic interoperability; Service orchestration; Software component; Internet of things",2-s2.0-85032630933
"Dufourq E., Bassett B.A.","Automated classification of text sentiment",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032643076&doi=10.1145%2f3129416.3129420&partnerID=40&md5=1e8b21cd205c85b5085c31ea750d949c","The ability to identify sentiment in text, referred to as sentiment analysis, is one which is natural to adult humans. This task is, however, not one which a computer can perform by default. Identifying sentiments in an automated, algorithmic manner will be a useful capability for business and research in their search to understand what consumers think about their products or services and to understand human sociology. Here we propose two new Genetic Algorithms (GAs) for the task of automated text sentiment analysis. The GAs learn whether words occurring in a text corpus are either sentiment or amplifier words, and their corresponding magnitude. Sentiment words, such as’horrible’, add linearly to the final sentiment. Amplifier words in contrast, which are typically adjectives/adverbs like’very’, multiply the sentiment of the following word. This increases, decreases or negates the sentiment of the following word. The sentiment of the full text is then the sum of these terms. This approach grows both a sentiment and amplifier dictionary which can be reused for other purposes and fed into other machine learning algorithms. We report the results of multiple experiments conducted on large Amazon data sets. The results reveal that our proposed approach was able to outperform several public and/or commercial sentiment analysis algorithms. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.","Genetic algorithm; Machine learning; Sentiment analysis","Artificial intelligence; Automation; Data mining; Engineers; Genetic algorithms; Learning systems; Natural language processing systems; Text processing; Automated classification; New genetic algorithms; Sentiment analysis; Text corpora; Learning algorithms",2-s2.0-85032643076
"Tamimi E., Ebadi H., Kiani A.","Hybrid optimization of object-based classification in high-resolution images using continous ant colony algorithm with emphasis on building detection",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032359810&doi=10.5194%2fisprs-archives-XLII-4-W4-271-2017&partnerID=40&md5=99b1221c33bf0bdca61fb55863c908f8","Automatic building detection from High Spatial Resolution (HSR) images is one of the most important issues in Remote Sensing (RS). Due to the limited number of spectral bands in HSR images, using other features will lead to improve accuracy. By adding these features, the presence probability of dependent features will be increased, which leads to accuracy reduction. In addition, some parameters should be determined in Support Vector Machine (SVM) classification. Therefore, it is necessary to simultaneously determine classification parameters and select independent features according to image type. Optimization algorithm is an efficient method to solve this problem. On the other hand, pixel-based classification faces several challenges such as producing salt-paper results and high computational time in high dimensional data. Hence, in this paper, a novel method is proposed to optimize objectbased SVM classification by applying continuous Ant Colony Optimization (ACO) algorithm. The advantages of the proposed method are relatively high automation level, independency of image scene and type, post processing reduction for building edge reconstruction and accuracy improvement. The proposed method was evaluated by pixel-based SVM and Random Forest (RF) classification in terms of accuracy. In comparison with optimized pixel-based SVM classification, the results showed that the proposed method improved quality factor and overall accuracy by 17% and 10%, respectively. Also, in the proposed method, Kappa coefficient was improved by 6% rather than RF classification. Time processing of the proposed method was relatively low because of unit of image analysis (image object). These showed the superiority of the proposed method in terms of time and accuracy.","Ant colony optimization algorithm; High spatial resolution image; Hybrid optimization; Object-based classification","Artificial intelligence; Clustering algorithms; Data mining; Decision trees; Image classification; Image enhancement; Image processing; Image resolution; Object detection; Optimization; Pixels; Remote sensing; Support vector machines; Ant Colony Optimization algorithms; Automatic building detection; Classification parameters; High spatial resolution images; Hybrid optimization; Object-based classifications; Optimization algorithms; Pixel based classifications; Ant colony optimization",2-s2.0-85032359810
"Lokhande S.K., Dhawale S.A., Pathak S.S., Gautam R., Jain M.C., Bodhe G.L.","Appraisal of noise level dissemination surrounding mining and industrial areas of Keonjhar, Odisha: A comprehensive approach using noise mapping",2017,"Archives of Acoustics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031046215&doi=10.1515%2faoa-2017-0044&partnerID=40&md5=6f1709eb67e26f057f3854257861edf7","Noise mapping is a well-established practice among the European nations, and it has been follow for almost two decades. Recently, as per guidelines of the Directorate General of Mines Safety (DGMS), India, noise mapping has been made mandatory in the mining expanses. This study is an effort to map the noise levels in nearby areas of mines in the northern Keonjhar district. The motive of this study is to quantify the existing A-weighted time-average sound level (LAeq, T ) in the study area to probe its effects on the human dwellings and noise sensitive areas with the probability of future development of the mines, roads, and industrial and commercial zone. The LAeq, T was measured at 39 identified locations, including industrial, commercial, residential, and sensitive zones, 15 open cast mines, 3 major highways, and 3 haulage roads. With the utilisation of Predictor LimA Software and other GIS tools, the worked out data is mapped and noise contours are developed for the visualisation and identification of the extent and distribution of sound levels across the study area. This investigation discloses that the present noise level at 60% of the locations in silence and residential zone exposed to significantly high noise levels surpasses the prescribed limit of Central Pollution Control Board (CPCB), India. The observed day and night time LAeq, Tlevel of both zones ranged between 43.2-62.2 dB(A) and 30.5-53.4 dB(A), respectively, whereas, the average Ldn values vary between 32.7 and 51.2 dB(A). The extensive mobility of heavy vehicles adjoining the sensitive areas and a nearby plethora of open cast mines is the leading cause of exceeded noise levels. The study divulges that the delicate establishments like schools and hospitals are susceptible to high noise levels throughout the day and night. A correlation between observed and software predicted values gives R2 of 0.605 for Ld, 0.217 for Ln, and 0.524 for Ldn. Finally, the mitigation measure is proposed and demonstrated using a contour map showing a significant reduction in the noise levels by 0-5.3 dB(A). © Polish Academy of Sciences & Institute of Fundamental Technological Research (IPPT PAN) 2017.","GIS; mining; noise mapping; noise prediction; predictor LimA","Geographic information systems; Housing; Mapping; Mining; Noise pollution; Pollution control; Transportation; Central pollution control boards; High noise levels; Industrial area; Mitigation measures; Noise mapping; Noise predictions; predictor LimA; Sensitive zones; Acoustic noise",2-s2.0-85031046215
"Rahimi M.M., Hakimpour F.","Towards a cloud based smart traffic management framework",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032390780&doi=10.5194%2fisprs-archives-XLII-4-W4-447-2017&partnerID=40&md5=0091f832e13a9fa57400e1336a00e9c0","Traffic big data has brought many opportunities for traffic management applications. However several challenges like heterogeneity, storage, management, processing and analysis of traffic big data may hinder their efficient and real-time applications. All these challenges call for well-adapted distributed framework for smart traffic management that can efficiently handle big traffic data integration, indexing, query processing, mining and analysis. In this paper, we present a novel, distributed, scalable and efficient framework for traffic management applications. The proposed cloud computing based framework can answer technical challenges for efficient and real-time storage, management, process and analyse of traffic big data. For evaluation of the framework, we have used OpenStreetMap (OSM) real trajectories and road network on a distributed environment. Our evaluation results indicate that speed of data importing to this framework exceeds 8000 records per second when the size of datasets is near to 5 million. We also evaluate performance of data retrieval in our proposed framework. The data retrieval speed exceeds 15000 records per second when the size of datasets is near to 5 million. We have also evaluated scalability and performance of our proposed framework using parallelisation of a critical pre-analysis in transportation applications. The results show that proposed framework achieves considerable performance and efficiency in traffic management applications.","Big data; Cloud computing; Hadoop; Traffic management","Cloud computing; Data integration; Digital storage; Indexing (materials working); Information management; Network function virtualization; Remote sensing; Storage management; Traffic control; Distributed environments; Distributed framework; Evaluation results; Hadoop; Real-time application; Scalability and performance; Technical challenges; Traffic management; Big data",2-s2.0-85032390780
"Krometis L.-A., Gohlke J., Kolivras K., Satterwhite E., Marmagas S.W., Marr L.C.","Environmental health disparities in the Central Appalachian region of the United States",2017,"Reviews on Environmental Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029631951&doi=10.1515%2freveh-2017-0012&partnerID=40&md5=8fa7ddbeed0d949eef96cfb50512fb56","Health disparities that cannot be fully explained by socio-behavioral factors persist in the Central Appalachian region of the United States. A review of available studies of environmental impacts on Appalachian health and analysis of recent public data indicates that while disparities exist, most studies of local environmental quality focus on the preservation of nonhuman biodiversity rather than on effects on human health. The limited public health studies available focus primarily on the impacts of coal mining and do not measure personal exposure, constraining the ability to identify causal relationships between environmental conditions and public health. Future efforts must engage community members in examining all potential sources of environmental health disparities to identify effective potential interventions. © 2017 Leigh-Anne Krometis et al.","Air quality; Central Appalachia; coal mining; environmental health; health disparities; rural health; water quality","air quality; biodiversity; coal mining; environmental health; environmental impact; health disparity; human; public health; rural health; United States; water quality",2-s2.0-85029631951
"He H., Wang J., Li Y., Song Z., Sivaprasada M.","Thermodynamic analysis of hot water leaching of sulphur from desulphurisation slag by Eh–pH diagrams of the Ca-S-H2O system",2017,"Transactions of the Institutions of Mining and Metallurgy, Section C: Mineral Processing and Extractive Metallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029907545&doi=10.1080%2f03719553.2017.1379145&partnerID=40&md5=787c42865884cfeb60b5899f62f28ed0","Eh–pH diagrams of a Ca-S-H2O system at different conditions were described with thermodynamic calculations to investigate the leaching of sulphur from desulphurisation slag. The influences of temperature, oxygen partial pressure and activity of the dissolved species on the hydrothermal leaching of sulphur were systematically studied. The results show that the leaching reactions occur spontaneously under normal conditions and that the sulphur leaches into the leachate as S2−, HS−, H2S(aq), S(s) and (Formula presented.). At higher temperatures, HS− converts into S2− or H2S(aq). Additionally, H2S(aq), HS−, S2− oxidise to elemental sulphur or (Formula presented.) more easily. Moreover, the sulphur in CaSO4(s) would readily dissolve into the leachate as (Formula presented.) and (Formula presented.) ions. When the oxygen partial pressure increases, S2− tends to be oxidised into (Formula presented.). However, the effect of the oxygen pressure on the equilibrium between O2 and H2O decreases when increasing the oxygen pressure. When the activity of the dissolved species is reduced, elemental sulphur converts into other ions in redox reactions. Simultaneously, the predominant area of Ca(OH)2(s) decreases, which indicates that Ca(OH)2(s) may dissolve into Ca2+ and increase the loss of calcium. Experiments were conducted to verify the thermodynamic analysis. The data confirm that the leaching of sulphur from desulphurisation slag by wet processes is feasible. © 2017 Institute of Materials, Minerals and Mining and The AusIMM Published by Taylor & Francis on behalf of the Institute and The AusIMM","Ca-S-H2O system; Desulphurisation slag; Eh–pH diagrams; leaching of sulphur","Calcium; Desulfurization; Dissolved oxygen; Leaching; Oxygen; Partial pressure; Redox reactions; Shotcreting; Slags; Thermoanalysis; Thermodynamic properties; Thermodynamics; Elemental sulphur; Leaching reaction; Normal condition; Oxygen partial pressure; Oxygen pressure; pH diagrams; Thermo dynamic analysis; Thermodynamic calculations; Sulfur",2-s2.0-85029907545
"Larrosa N.O., Akid R., Ainsworth R.A.","Corrosion-fatigue: a review of damage tolerance models",2017,"International Materials Reviews",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029895420&doi=10.1080%2f09506608.2017.1375644&partnerID=40&md5=2b377a356d6c59a090a0391302104c7c","The synergistic combination of mechanical fatigue stresses and environmental agents acting together can be more detrimental than that of the summation of the contributions of each mechanism acting separately. Major attempts to understand the contribution of the different agents (microstructure, chemical composition of environment, temperature, loading conditions, etc.) have been reported in the literature. Nevertheless, current knowledge is insufficient to address life estimation with a sound physical basis from the initiation of localised corrosion (such as pitting) to the estimation of crack propagation. Major simplifications and assumptions have been required in the development of life prediction methodologies. This paper reviews recent efforts made by the different interested parties, in both academia and industry, in the development of corrosion fatigue (CF) lifetime prediction procedures. The paper mainly focuses on the methodologies proposed in the literature for oil and gas, nuclear, energy generation and aerospace applications, dealing with pitting CF damage in aluminium alloys, carbon and stainless steels. The transition of a pit into a small crack (SC) and its growth is influenced by the interaction of the pit stress/strain concentration and the local environmental conditions, making the modelling of this stage of the utmost complexity. A major trend in the models reviewed in this paper is to simplify the analysis by assuming the pit (a volumetric defect) as a sharp crack, decouple the CF problem and account for the mechanical and environmental contributions separately. These procedures heavily rely on fitting experimental data and exhibit low generality in terms of application to varying system conditions. There is a clear opportunity in this field to develop mechanistically based methodologies, considering the inherent dependence of the damage mechanism on the interaction of environmental, metallurgical and mechanical features, allowing more realistic lifetime estimates and defect tolerance arguments, where pit-to-crack transition and SC initiation stages pose a significant challenge. Abbreviation: ASME: American Society of Mechanical Engineers; API: American Petroleum Institute; BP: British Petroleum; BS: British Standards; BWR: Boiling Water Reactor; CF: Corrosion fatigue; DNV: Det Norske Veritas; FCGR: Fatigue crack growth rate; FCI: Fatigue crack initiation; FCP: Fatigue crack propagation; FFS: Fitness for service; HA: Hydrogen assisted; HRR: Hutchinson, Rice and Rosengren stress fields; LC: Long crack; LEFM: Linear Elastic Fracture Mechanics; S-N: Stress vs. number of cycles © 2017 Institute of Materials, Minerals and Mining and ASM International Published by Taylor & Francis on behalf of the Institute and ASM International","Corrosion fatigue; damage tolerance; life assessment methods; pit-to-crack transition; pitting corrosion","Aerospace applications; Alloy steel; Boiling water reactors; Brittle fracture; Carbon; Corrosion; Corrosion fatigue; Crack propagation; Cracks; Damage detection; Defects; Fatigue crack propagation; Fatigue damage; Fatigue of materials; Fits and tolerances; Fracture; Fracture mechanics; Pitting; Stresses; American Petroleum Institute; American society of mechanical engineers; Environmental contributions; Fatigue crack initiation; Life assessment; Life-prediction methodology; Linear elastic fracture mechanics; Pit-to-crack transitions; Damage tolerance",2-s2.0-85029895420
"Ge Z., Song Z., Ding S.X., Huang B.","Data Mining and Analytics in the Process Industry: the Role of Machine Learning",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030772750&doi=10.1109%2fACCESS.2017.2756872&partnerID=40&md5=bccaef13df9476d05a03e9cc8ec8cf15","Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computa-tional engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry. OAPA","Analytical models; Data analytics; Data mining; Data mining; Data models; Industries; Machine learning; Machine learning algorithms; Manufacturing; Predictive models; Process industry","Analytical models; Artificial intelligence; Data structures; Decision making; Industry; Learning algorithms; Learning systems; Manufacture; Pattern recognition; Predictive analytics; Supervised learning; Application status; Data analytics; Predictive models; Process industries; State of the art; Data mining",2-s2.0-85030772750
"Lee G., Yun U.","Performance and characteristic analysis of maximal frequent pattern mining methods using additional factors",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029805475&doi=10.1007%2fs00500-017-2820-3&partnerID=40&md5=08cc755bce3bc5cbd992b4c5caf6ef6f","Various data mining methods have been proposed to handle large-scale data and discover interesting knowledge hidden in the data. Maximal frequent pattern mining is one of the data mining techniques suggested to solve the fatal problem of traditional frequent pattern mining approach. While traditional approach may extract an enormous number of pattern results according to threshold settings, maximal frequent pattern mining approach mines a smaller number of representative patterns, which allow users to analyze given data more efficiently. In this paper, we describe various recent maximal frequent pattern mining methods using additional factors and conduct performance evaluation in order to analyze their detailed characteristics. © 2017 Springer-Verlag GmbH Germany","Data mining; Knowledge discovery; Maximal frequent pattern; Pattern mining; Representative pattern","Soft computing; Software engineering; Characteristic analysis; Data mining methods; Frequent pattern mining; Maximal frequent pattern; Maximal frequent pattern minings; Pattern mining; Representative patterns; Traditional approaches; Data mining",2-s2.0-85029805475
"Wu R., Chen W., Tang L., Fan J.","High Risk Tree Mining Method for Analysis of Power System Risk Device Set",2017,"Dianli Xitong Zidonghua/Automation of Electric Power Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031741356&doi=10.7500%2fAEPS20161228011&partnerID=40&md5=ccc9834843f1e2827a4e46e3901d1c49","The rapid accumulation of big data in the power grid dispatching and control system provides a sufficient condition for the risk analysis of power grid equipment. Based on the analysis of characteristics of big data in the dispatching and control system, a universal analysis framework of big data in power dispatching and control system is built, and on basis of the application of risk management and control in power grid, a data mining method for high risk equipment based on high risk tree (HRT) is proposed. From the perspective of multiple factor analysis of equipment risk, considering impact of equipment importance and equipment hidden danger on equipment risk, equipment risk influence degree is defined, equipment importance indicators and equipment hidden danger indicators are proposed. The equipment risk value as the target of mining high risk equipment, the construction of HRT retaining the original database of equipment risk value and equipment risk prior knowledge information is for mining high risk equipment set, specifically the high risk equipment risk set meeting the threshold will be obtained according to the HRT path information. Finally, the proposed method is simulated based on massive historical alarm data in dispatching and control system. The simulation results show that HRT can quickly deal with the alarm data to get high risk equipment set meeting the conditions, and it can reflect the potential association between high risk equipment, so it will provide reference for the follow-up of power grid risk management and control. © 2017 Automation of Electric Power Systems Press.","Big data; Data mining; High risk tree; Risk influence degree","Big data; Construction equipment; Control systems; Data mining; Electric load dispatching; Electric power transmission networks; Equipment; Forestry; Information management; Power control; Risk analysis; Risk assessment; Risk management; Trees (mathematics); Analysis frameworks; Data mining methods; High risk tree; Influence degree; Management and controls; Multiple factor analysis; Power grid dispatching; Power grid equipment; Electric power system control",2-s2.0-85031741356
"Majdara A., Nooshabadi S.","Efficient data structures for density estimation for large high-dimensional data",2017,"Proceedings - IEEE International Symposium on Circuits and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032671188&doi=10.1109%2fISCAS.2017.8050592&partnerID=40&md5=a57af880beb1000a705e9f862a6896ce","Density estimation is a fundamental part of statistical analysis and data mining. In high-dimensional domains, parametric methods and the commonly used non-parametric methods like histograms or Kernel estimators fail to perform properly. In this paper, we present computationally efficient data structures for efficient implementation of the Bayesian sequential partitioning (BSP), as a framework for density estimation in high-dimensional domain. Simulation results are presented to analyze the performance for large high-dimensional datasets. © 2017 IEEE.","Bayesian Sequential Partitioning; Classification; High-dimensional; Multiveriate Density Estimation; Non-parametric","Classification (of information); Clustering algorithms; Data mining; Data structures; Bayesian; Computationally efficient; Density estimation; Efficient data structures; High dimensional datasets; High-dimensional; Large high-dimensional data; Non-parametric; Parameter estimation",2-s2.0-85032671188
"Povoda L., Burget R., Dutta M.K., Sengar N.","Genetic optimization of big data sentiment analysis",2017,"2017 4th International Conference on Signal Processing and Integrated Networks, SPIN 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032815200&doi=10.1109%2fSPIN.2017.8049932&partnerID=40&md5=ba04692aa68b904cf896d7a0e1780895","This paper deals with opinion mining from unstructured textual documents. The proposed method focuses on approach with minimum preliminary requirements about the knowledge of the analysed language and thus it can be deployed to any language. The proposed method builds on artificial intelligence, which consists of Support Vector Machines classifier, Big Data analysis and genetic algorithm optimization. To make the optimization feasible together with big data approach we have proposed GA operators, which significantly accelerate conversion to the accurate solutions. In this work we outperformed the traditional approaches (which use language dependent text preprocessing) for text valence classification with the highest achieved accuracy 90.09 %. The data set for validation was Czech texts. © 2017 IEEE.","artificial intelligence; big data; data mining; opinion mining; sentiment analysis; text mining; text valence classification","Artificial intelligence; Data mining; Genetic algorithms; Natural language processing systems; Signal processing; Text processing; Genetic optimization; Genetic-algorithm optimizations; Opinion mining; Sentiment analysis; Text mining; Text preprocessing; Textual documents; Traditional approaches; Big data",2-s2.0-85032815200
"Homayoun S., Dehghantanha A., Ahmadzadeh M., Hashemi S., Khayami R.","Know Abnormal, Find Evil: Frequent Pattern Mining for Ransomware Threat Hunting and Intelligence",2017,"IEEE Transactions on Emerging Topics in Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030788798&doi=10.1109%2fTETC.2017.2756908&partnerID=40&md5=91c89457500ccac517c3f0ebbabd8b12","Emergence of crypto-ransomware has significantly changed the cyber threat landscape. A crypto ransomware removes data custodian access by encrypting valuable data on victims&#x0027; computers and requests a ransom payment to reinstantiate custodian access by decrypting data. Timely detection of ransomware very much depends on how quickly and accurately system logs can be mined to hunt abnormalities and stop the evil. In this paper we first setup an environment to collect activity logs of 517 Locky ransomware samples, 535 Cerber ransomware samples and 572 samples of TeslaCrypt ransomware. We utilize Sequential Pattern Mining to find Maximal Frequent Patterns (MFP) of activities within different ransomware families as candidate features for classification using J48, Random Forest, Bagging and MLP algorithms. We could achieve 99% accuracy in detecting ransomware instances from goodware samples and 96.5% accuracy in detecting family of a given ransomware sample. Our results indicate usefulness and practicality of applying pattern mining techniques in detection of good features for ransomware hunting. Moreover, we showed existence of distinctive frequent patterns within different ransomware families which can be used for identification of a ransomware sample family for building intelligence about threat actors and threat profile of a given target. IEEE","Buildings; Computers; crypto ransomware; Cryptography; Feature extraction; Malware; Malware; Mathematical model; ransomware; ransomware detection; ransomware family detection; Tools","Buildings; Computer crime; Computers; Cryptography; Data mining; Decision trees; Feature extraction; Intelligent buildings; Mathematical models; Tools; Activity logs; Building intelligences; Cyber threats; Frequent pattern mining; Maximal frequent pattern; Pattern mining; Random forests; Sequential-pattern mining; Malware",2-s2.0-85030788798
"Gentil M.-L., Cuggia M., Fiquet L., Hagenbourger C., Le Berre T., Banâtre A., Renault E., Bouzille G., Chapron A.","Factors influencing the development of primary care data collection projects from electronic health records: A systematic review of the literature",2017,"BMC Medical Informatics and Decision Making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029854314&doi=10.1186%2fs12911-017-0538-x&partnerID=40&md5=f6ce3d425a15aa9b8335693a28eda868","Background: Primary care data gathered from Electronic Health Records are of the utmost interest considering the essential role of general practitioners (GPs) as coordinators of patient care. These data represent the synthesis of the patient history and also give a comprehensive picture of the population health status. Nevertheless, discrepancies between countries exist concerning routine data collection projects. Therefore, we wanted to identify elements that influence the development and durability of such projects. Methods: A systematic review was conducted using the PubMed database to identify worldwide current primary care data collection projects. The gray literature was also searched via official project websites and their contact person was emailed to obtain information on the project managers. Data were retrieved from the included studies using a standardized form, screening four aspects: projects features, technological infrastructure, GPs' roles, data collection network organization. Results: The literature search allowed identifying 36 routine data collection networks, mostly in English-speaking countries: CPRD and THIN in the United Kingdom, the Veterans Health Administration project in the United States, EMRALD and CPCSSN in Canada. These projects had in common the use of technical facilities that range from extraction tools to comprehensive computing platforms. Moreover, GPs initiated the extraction process and benefited from incentives for their participation. Finally, analysis of the literature data highlighted that governmental services, academic institutions, including departments of general practice, and software companies, are pivotal for the promotion and durability of primary care data collection projects. Conclusion: Solid technical facilities and strong academic and governmental support are required for promoting and supporting long-term and wide-range primary care data collection projects. © 2017 The Author(s).","Data collection; Data mining; Electronic health records; Governance; Primary care; Secondary use; Stakeholders","adult; Canada; data mining; electronic health record; extraction; female; general practice; human; male; manager; Medline; primary medical care; software; speech; systematic review; United Kingdom; United States; veterans health",2-s2.0-85029854314
"Saha T.K., Katebi A., Dhifli W., Hasan M.A.","Discovery of Functional Motifs from the Interface Region of Oligomeric Proteins using Frequent Subgraph Mining",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030759252&doi=10.1109%2fTCBB.2017.2756879&partnerID=40&md5=5021896d4963447c1dd9f826ae89c74e","Modeling the interface region of a protein complex paves the way for understanding its dynamics and functionalities. Existing works model the interface region of a complex by using different approaches, such as, the residue composition at the interface region, the geometry of the interface residues, or the structural alignment of interface regions. These approaches are useful for ranking a set of docked conformation or for building scoring function for protein-protein docking, but they do not provide a generic and scalable technique for the extraction of interface patterns leading to functional motif discovery. In this work, we model the interface region of a protein complex by graphs and extract interface patterns of the given complex in the form of frequent subgraphs. To achieve this we develop a scalable algorithm for frequent subgraph mining. We show that a systematic review of the mined subgraphs provides an effective method for the discovery of functional motifs that exist along the interface region of a given protein complex. IEEE","Bio-Informatics; Buildings; Data mining; Databases; Electronic mail; Frequent Subgraph Mining; Functional Motifs; Interfacial Network; Proteins","Buildings; Complex networks; Data mining; Database systems; Electronic mail; Nucleic acid sequences; Frequent subgraph mining; Frequent subgraphs; Functional motifs; Interface patterns; Oligomeric proteins; Protein-protein docking; Scalable algorithms; Structural alignments; Proteins",2-s2.0-85030759252
"Saldana-Perez A.M.M., Moreno-Ibarra M., Tores-Ruiz M.","Classification of traffic related short texts to analyse road problems in urban areas",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032626265&doi=10.5194%2fisprs-archives-XLII-4-W3-91-2017&partnerID=40&md5=987feefac131d9e1ba8e5bd202ef3636","The Volunteer Geographic Information (VGI) can be used to understand the urban dynamics. In the classification of traffic related short texts to analyze road problems in urban areas, a VGI data analysis is done over a social media’s publications, in order to classify traffic events at big cities that modify the movement of vehicles and people through the roads, such as car accidents, traffic and closures. The classification of traffic events described in short texts is done by applying a supervised machine learning algorithm. In the approach users are considered as sensors which describe their surroundings and provide their geographic position at the social network. The posts are treated by a text mining process and classified into five groups. Finally, the classified events are grouped in a data corpus and geo-visualized in the study area, to detect the places with more vehicular problems. © Authors 2017. CC BY 4.0 License.","Classification; Data Analysis; Human sensors; Machine Learning; Traffic; Volunteered Geographic Information","Accidents; Artificial intelligence; Classification (of information); Data handling; Data reduction; Information analysis; Learning algorithms; Learning systems; Roads and streets; Smart city; Supervised learning; Telecommunication traffic; Transportation; Car accidents; Geographic information; Human sensors; Supervised machine learning; Traffic event; Traffic-related; Urban dynamics; Volunteered geographic information; Data mining",2-s2.0-85032626265
"Coro G., Panichi G., Scarponi P., Pagano P.","Cloud computing in a distributed e-infrastructure using the web processing service standard",2017,"Concurrency Computation ",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022208375&doi=10.1002%2fcpe.4219&partnerID=40&md5=27da1cb95ccf829a30366ac1eea7b4d7","New Science paradigms have recently evolved to promote open publication of scientific findings as well as multi-disciplinary collaborative approaches to scientific experimentation. These approaches can face modern scientific challenges but must deal with large quantities of data produced by industrial and scientific experiments. These data, so-called Big Data, require to introduce new computer science systems to help scientists cooperate, extract information, and possibly produce new knowledge out of the data. E-infrastructures are distributed computer systems that foster collaboration between users and can embed distributed and parallel processing systems to manage big data. However, in order to meet modern Science requirements, e-Infrastructures impose several requirements to computational systems in turn, eg, being economically sustainable, managing community-provided processes, using standard representations for processes and data, managing big data size and heterogeneous representations, supporting reproducible Science, collaborative experimentation, and cooperative online environments, managing security and privacy for data and services. In this paper, we present a cloud computing system (gCube DataMiner) that meets these requirements and operates in an e-Infrastructure, while sharing characteristics with state-of-the-art cloud computing systems. To this aim, DataMiner uses the web processing service standard of the open geospatial consortium and introduces features like collaborative experimental spaces, automatic installation of processes and services on top of a flexible and sustainable cloud computing architecture. We compare DataMiner with another mature cloud computing system and highlight the benefits our system brings, the new paradigms requirements it satisfies, and the applications that can be developed based on this system. Copyright © 2017 John Wiley & Sons, Ltd.","big data processing; cloud computing; data mining; distributed systems; e-Infrastructures; parallel processing; Science 2.0; WPS","Big data; Cloud computing; Computer architecture; Data handling; Data mining; Information management; Network security; Online systems; Parallel processing systems; Web services; Cloud computing architectures; Collaborative approach; Distributed and parallel processing; Distributed systems; E-infrastructures; Open geospatial consortium; Parallel processing; Science 2.0; Distributed computer systems",2-s2.0-85022208375
"Ma J., Qiao Y., Hu G., Huang Y., Sangaiah A.K., Zhang C., Wang Y., Zhang R.","De-anonymizing Social Networks with Random Forest Classifier",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030775994&doi=10.1109%2fACCESS.2017.2756904&partnerID=40&md5=afcc2c85fb2cc63943028a3cd8ed8aa5","Personal privacy is facing severe threats as social networks are sharing user data with advertisers, application developers and data mining researchers. Although these data are anonymized by removing personal information, such as user identity, nickname or address information, personal information still could not be protected effectively. In order to arouse the attention of people from academia and industry for privacy protection, we propose a random forest method to de-anonymize social networks. First, we convert the social network de-anonymization problem into a binary classification problem between node pairs. In order to partition large sparse social networks, we use the spectral partition method to partition large graphs into a number of small subgraphs. And then we use the features of the network structure to train the random forest classifier. As a result, candidate node pairs from anonymous network and auxiliary network can be classified as matched pair by the random forest classifier. Furthermore, we improve the efficiency of our solution through parallelizing proposed method. The experiments conducted on the real datasets show that our solution&#x2019;s Area Under the Curve (AUC) is 19&#x0025; higher than baseline methods on average. Besides that, we test the robustness of the proposed algorithm by adding some noisy data, and the result demonstrates that our solution has good robustness. OAPA","Algorithm design and analysis; Classification algorithms; Data privacy; De-anonymization; Feature extraction; graph partition; network structure; Privacy; random forest; Robustness; social network analysis; Social network services","Data privacy; Decision trees; Feature extraction; Robustness (control systems); Social networking (online); Algorithm design and analysis; Anonymization; Classification algorithm; Graph partition; Network structures; Random forests; Social network services; Data mining",2-s2.0-85030775994
"Wei H., Yu J.X., Lu C.","String Similarity Search: A Hash-Based Approach",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773741&doi=10.1109%2fTKDE.2017.2756932&partnerID=40&md5=2da19bb9b08d7a240c262d1157e1eb49","String similarity search is a fundamental query that has been widely used for DNA sequencing, error-tolerant query auto-completion, and data cleaning needed in database, data warehouse and data mining. In this paper, we study string similarity search based on edit distance that is supported by many database management systems such as Oracle and PostgreSQL. Given the edit distance, ed(s, t), between two strings, s and t, the string similarity search is to find every string t in a string database D which is similar to a query string s such that ed<formula><tex>$(s,t)\leq \tau$</tex></formula> for a given threshold <formula><tex>$\tau$</tex></formula>. In the literature, most existing work take a filter-and-verify approach, where the filter step is introduced to reduce the high verification cost of two strings by utilizing an index built offline for D. The two up-to-date approaches are prefix filtering and local filtering. In this paper, we study string similarity search where strings can be either short or long. Our approach can support long strings, which are not well supported by the existing approaches due to the size of the index built and the time to build such index. We propose two new hash-based labeling techniques, named OX label and XX label, for string similarity search. We assign a hash-label, <formula><tex>${\mathrm H}_{s}$</tex></formula> to a string s, and prune the dissimilar strings by comparing two hash-labels, Hs and Ht, for two strings s and t in the filter step. The key idea behind is to take the dissimilar bit-patterns between two hash-labels. We discuss our hash-based approaches, address their pruning power, and give the algorithms. Our hash-based approaches achieve high efficiency, and keep its index size and index construction time one order of magnitude smaller than the existing approaches in our experiment at the same time. IEEE","Cleaning; Data mining; DNA; edit distance; hash; Indexes; Search problems; Sequential analysis; Similarity search","Cleaning; Data mining; Data warehouses; DNA; DNA sequences; Filtration; Gene encoding; Edit distance; hash; Indexes; Search problem; Sequential analysis; Similarity search; Query processing",2-s2.0-85030773741
"Zhao W., Guan Z., Chen L., He X., Cai D., Wang B., Wang Q.","Weakly-supervised Deep Embedding for Product Review Sentiment Analysis",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773560&doi=10.1109%2fTKDE.2017.2756658&partnerID=40&md5=ac5e58bb55fbd7f1acfa04f6576edee4","Product reviews are valuable for upcoming buyers in helping them make decisions. To this end, different opinion techniques have been proposed, where judging a review sentence's orientation (e.g. positive or negative) is one of their key challenges. Recently, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. We propose a novel deep learning framework for product review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learning a high level representation (an embedding space) which captures the general sentiment distribution of sentences through rating information; (2) adding a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. We explore two kinds of low level network structure for modeling review sentences, namely, convolutional feature extractors and long short-term memory. To evaluate the proposed framework, we construct a dataset containing 1.1M weakly labeled review sentences and 11,754 labeled review sentences from Amazon. Experimental results show the efficacy of the proposed framework and its superiority over baselines. IEEE","Deep learning; Feature extraction; Machine learning; Neural networks; opinion mining; Sentiment analysis; sentiment classification; Syntactics; Training; weak-supervision","Data mining; Deep learning; Feature extraction; Learning systems; Long short-term memory; Network function virtualization; Neural networks; Personnel training; Syntactics; Feature extractor; Learning frameworks; Network structures; Opinion mining; Rating information; Sentiment analysis; Sentiment classification; weak-supervision; Classification (of information)",2-s2.0-85030773560
"Mukherjee P., Bentzien J., Bosanac T., Mao W., Burke M., Muegge I.","Kinase Crystal Miner: A Powerful Approach to Repurposing 3D Hinge Binding Fragments and Its Application to Finding Novel Bruton Tyrosine Kinase Inhibitors",2017,"Journal of Chemical Information and Modeling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029911812&doi=10.1021%2facs.jcim.7b00213&partnerID=40&md5=8600e70541f735068621a9239944b106","Protein kinases represent an important target class for drug discovery because of their role in signaling pathways involved in disease areas such as oncology and immunology. A key element of many ATP-competitive kinase inhibitors is their hinge-binding motif. Here, we describe Kinase Crystal Miner (KCM) - a new approach developed at Boehringer Ingelheim (BI) that harvests the existing crystallographic information on kinase-inhibitor co-crystal structures from internal and external databases. About 1000 unique three-dimensional kinase inhibitor hinge binding motifs have been extracted from structures covering more than 180 different protein kinases. These hinge binding motifs along with their attachment vectors have been combined in the KCM for the purpose of scaffold hopping, kinase screening deck design, and interactive structure-based design. Prospective scaffold hopping using the KCM identified two potent and selective Bruton tyrosine kinase (BTK) inhibitors with hinge binding fragments novel to BTK. © 2017 American Chemical Society.",,"Amino acids; Bins; Diagnosis; Enzyme inhibition; Miners; Proteins; Boehringer ingelheim; Cocrystal structure; Crystallographic information; External database; Kinase inhibitors; Signaling pathways; Structure-based designs; Tyrosine kinase inhibitor; Enzymes; ligand; protein binding; protein kinase inhibitor; protein tyrosine kinase; antagonists and inhibitors; chemistry; data mining; drug development; human; metabolism; molecular docking; procedures; protein conformation; X ray crystallography; Crystallography, X-Ray; Data Mining; Drug Discovery; Humans; Ligands; Molecular Docking Simulation; Protein Binding; Protein Conformation; Protein Kinase Inhibitors; Protein-Tyrosine Kinases",2-s2.0-85029911812
"Xiao X.-H., Xiao P.-W., Dai F., Li H.-B., Zhang X.-B., Zhou J.-W.","Large Deformation Characteristics and Reinforcement Measures for a Rock Pillar in the Houziyan Underground Powerhouse",2017,"Rock Mechanics and Rock Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029801648&doi=10.1007%2fs00603-017-1329-5&partnerID=40&md5=6b16928e12e590ecd06f960b61d51bab","The underground powerhouse of the Houziyan Hydropower Station is under the conditions of high geo-stress and a low strength/stress ratio, which leads to significant rock deformation and failures, especially for rock pillars due to bidirectional unloading during the excavation process. Damages occurred in thinner rock pillars after excavation due to unloading and stress concentration, which will reduce the surrounding rock integrity and threaten the safety of the underground powerhouse. By using field investigations and multi-source monitoring data, the deformation and failure characteristics of a rock pillar are analyzed from the tempo-spatial distribution features. These results indicate that significant deformation occurred in the rock pillar when the powerhouse was excavated to the fourth layer, and the maximum displacement reached 107.57 mm, which occurred on the main transformer chamber upstream sidewall at an elevation of 1721.20 m. The rock deformation surrounding the rock pillar is closely related to the excavation process and has significant time-related characteristics. To control large deformation of the rock pillar, thru-anchor cables were used to reinforce the rock pillar to ensure the stability of the powerhouse. The rock deformation surrounding the rock pillar decreases gradually and forms a convergent trend after reinforcement measures are installed based on the analysis of the temporal characteristics and the rock pillar deformation rate. © 2017 Springer-Verlag GmbH Austria","Large deformation; Reinforcement measures; Rock pillar; Tempo-spatial features; Underground powerhouse","Anchor cables; Deformation; Excavation; Reinforcement; Room and pillar mining; Underground power plants; Unloading; Deformation and failures; Deformation Characteristics; Distribution features; Reinforcement measures; Rock pillar; Spatial features; Temporal characteristics; Underground powerhouse; Rocks",2-s2.0-85029801648
[No author name available],"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",2017,"SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032450265&partnerID=40&md5=64c5ca63fd50466dead4be3fe74593ca","The proceedings contain 82 papers. The topics discussed include: trustable virtual machine scheduling in a cloud; search lookaside buffer: efficient caching for index data structures; architectural implications on the performance and cost of graph analytics systems; incentivizing self-capping to increase cloud utilization; Mithril: mining sporadic associations for cache prefetching; no data left behind: real-time insights from a complex data ecosystem; remote memory in the age of fast networks; workload analysis and caching strategies for search advertising systems; prism: a proxy architecture for datacenter networks; and towards automatic parameter tuning of stream processing systems.",,,2-s2.0-85032450265
"Zhou X., Zhang X., Jiao W.","Evaluating software evolution based on pattern mining",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032496986&doi=10.1145%2f3131704.3131723&partnerID=40&md5=7696a13289d8ff04cf62bbec3257d4a1","Software systems need constantly maintaining or adapting to continuously meet the changing business requirements. The process of maintenance or adaptation is software evolution. In general, people hope to evaluate software evolution for guiding software maintenances. By evaluating how well software maintenances follow the positive evolutionary trends, developers can assert whether it is necessary to redevelop or even refactor newly released or maintained versions of software to enforce the software evolution back on track. In this paper, we propose an approach to evaluating software evolution based on API usage patterns, which are the accumulations and summarizations of people's software design and development experience. In the approach, better software evolution is considered as the process of reusing more usage patterns, and software evolution is evaluated based on how well software reuses usage patterns in the process of evolution. Our work consists of three parts. First, we use a graph-based algorithm to mine usage patterns from different open-source software. Second, we use the patterns to evaluate the evolutions of software systems and accordingly analyze the important changes during software evolution. Third, we compare different approaches and analyze which approach can reflect the process of software evolution accurately. Our experiments on several open source programs show that our approach is more effective than other approaches on identifying the great change events during software evolution. © 2017 Association for Computing Machinery.","API usage pattern mining; Empirical studies; Software evaluation; Software evolution","Application programming interfaces (API); Computer software; Computer software maintenance; Computer software selection and evaluation; Data mining; Graphic methods; Maintenance; Open systems; Software design; Software engineering; Business requirement; Empirical studies; Graph-based algorithms; Open source projects; Software design and development; Software evaluation; Software Evolution; Usage patterns; Open source software",2-s2.0-85032496986
"Ranjan N.M., Prasad R.S.","Automatic text classification using BPLion-neural network and semantic word processing",2017,"Imaging Science Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029898678&doi=10.1080%2f13682199.2017.1376781&partnerID=40&md5=14dadcc498808a868b897b374178e67f","Text mining has become a major research topic in which text classification is the important task for finding the relevant information from the new document. Accordingly, this paper presents a semantic word processing technique for text categorization that utilizes semantic keywords, instead of using independent features of the keywords in the documents. Hence, the dimensionality of the search space can be reduced. Here, the Back Propagation Lion algorithm (BP Lion algorithm) is also proposed to overcome the problem in updating the neuron weight. The proposed text classification methodology is experimented over two data sets, namely, 20 Newsgroup and Reuter. The performance of the proposed BPLion is analysed, in terms of sensitivity, specificity, and accuracy, and compared with the performance of the existing works. The result shows that the proposed BPLion algorithm and semantic processing methodology classifies the documents with less training time and more classification accuracy of 90.9%. © 2017 The Royal Photographic Society","back propagation algorithm; hyponymy; Lion optimization algorithm; semantic words; Text classification","Backpropagation; Backpropagation algorithms; Classification (of information); Data mining; Information retrieval systems; Natural language processing systems; Optimization; Semantic Web; Semantics; Word processing; Automatic text classification; Classification accuracy; Hyponymy; Optimization algorithms; Processing technique; Semantic processing; Text categorization; Text classification; Text processing",2-s2.0-85029898678
"Yu H., Xia X., Zhao X., Qiu W.","Combining collaborative filtering and topic modeling for more accurate android mobile app library recommendation",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032505472&doi=10.1145%2f3131704.3131721&partnerID=40&md5=4d8cc4d7b7ef8712e09615a8367c36c6","The applying of third party libraries is an integral part of many mobile applications. With the rapid development of mobile technologies, there are many free third party libraries for developers to download and use. However, there are a large number of third party libraries which always iterate rapidly, it is hard for developers to find available libraries within them. Several previous studies have proposed approaches to recommend third party libraries, which works in the scenario where a developer knows some required libraries, and needs to find other relevant libraries with limited knowledge. In the paper, to further improve the performance of app library recommendation, we propose an approach which combines collaborative filtering and topic modeling techniques. In the collaborative filtering component, given a new app, our approach recommends libraries by using its similar apps. In the topic modelling component, our approach first extracts the topics from the textual description of mobile apps, and given a new app, our approach recommends libraries based on the libraries used by the apps which has similar topic distributions.We perform experiments on a set of 1,013 apps, and the results show that our approach improves the state-of-the-art by a substantial margin. © 2017 Association for Computing Machinery.","Android App; Collaborative Filtering; Library Recommendation; Topic Modeling","Android (operating system); Data mining; Libraries; Integral part; Mobile applications; Mobile apps; Mobile Technology; State of the art; Textual description; Third parties; Topic Modeling; Collaborative filtering",2-s2.0-85032505472
"Luna J.M., Pechenizkiy M., del Jesus M.J., Ventura S.","Mining Context-Aware Association Rules Using Grammar-Based Genetic Programming",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708834&doi=10.1109%2fTCYB.2017.2750919&partnerID=40&md5=d333a7a4f3da222b2d36c29d74d66dbb","Real-world data usually comprise features whose interpretation depends on some contextual information. Such contextual-sensitive features and patterns are of high interest to be discovered and analyzed in order to obtain the right meaning. This paper formulates the problem of mining context-aware association rules, which refers to the search for associations between itemsets such that the strength of their implication depends on a contextual feature. For the discovery of this type of associations, a model that restricts the search space and includes syntax constraints by means of a grammar-based genetic programming methodology is proposed. Grammars can be considered as a useful way of introducing subjective knowledge to the pattern mining process as they are highly related to the background knowledge of the user. The performance and usefulness of the proposed approach is examined by considering synthetically generated datasets. A posteriori analysis on different domains is also carried out to demonstrate the utility of this kind of associations. For example, in educational domains, it is essential to identify and understand contextual and context-sensitive factors that affect overall and individual student behavior and performance. The results of the experiments suggest that the approach is feasible and it automatically identifies interesting context-aware associations from real-world datasets. IEEE","Association rules; context awareness; contextual features","Association rules; Genetic algorithms; Back-ground knowledge; Context- awareness; Contextual feature; Contextual information; Grammar-based genetic programming; Posteriori analysis; Real-world datasets; Sensitive features; Genetic programming",2-s2.0-85030708834
"Hu W., Tian G., Kang Y., Yuan C., Maybank S.","Dual Sticky Hierarchical Dirichlet Process Hidden Markov Model and Its Application to Natural Language Description of Motions",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030697462&doi=10.1109%2fTPAMI.2017.2756039&partnerID=40&md5=1e6814320f43a867b1192103f8a7c910","In this paper, a new nonparametric Bayesian model called the dual sticky hierarchical Dirichlet process hidden Markov model (HDP-HMM) is proposed for mining activities from a collection of time series data such as trajectories. All the time series data are clustered. Each cluster of time series data, corresponding to a motion pattern, is modeled by an HMM. Our model postulates a set of HMMs that share a common set of states (topics in an analogy with topic models for document processing), but have unique transition distributions. For the application to motion trajectory modeling, topics correspond to motion activities. The learnt topics are clustered into atomic activities which are assigned predicates. We propose a Bayesian inference method to decompose a given trajectory into a sequence of atomic activities. On combining the learnt sources and sinks, semantic motion regions, and the learnt sequence of atomic activities, the action represented by the trajectory can be described in natural language in as automatic a way as possible. The effectiveness of our dual sticky HDP-HMM is validated on several trajectory datasets. The effectiveness of the natural language descriptions for motions is demonstrated on the vehicle trajectories extracted from a traffic scene. IEEE","HDP-HMM; Motion pattern learning; Natural language description; Sticky prior","Atoms; Bayesian networks; Inference engines; Markov processes; Semantics; Time and motion study; Time series; Trajectories; Document-processing; Hdp-hmm; Motion pattern; Natural languages; Non-parametric Bayesian modeling; Sticky hierarchical dirichlet process; Sticky prior; Vehicle trajectories; Hidden Markov models",2-s2.0-85030697462
"Klinkenberg J., Terboven C., Lankes S., Muller M.S.","Data mining-based analysis of HPC center operations",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032637445&doi=10.1109%2fCLUSTER.2017.23&partnerID=40&md5=5dbc2b7ee2eabde0788d44f920b5805f","Size and complexity of contemporary High Performance Computing (HPC) systems increases permanently. While the reliability of a single component and compute node is high, the huge amount of components comprising these systems results in the fact that defects happen regularly. This drives the need to manage failure situations. Common issues are component failures or node soft lock-ups that typically lead to crashes of the user jobs that are scheduled on the affected node, and may cause undesired downtime. One approach to mitigate the impact of such problems is to predict node failures with a sufficient lead time in order to take proactive measures. However, accurate prediction is a challenging task.The literature describes several approaches that focus on gathering and analyzing system event logs in order to create prediction models. In this paper, we present a different approach by using descriptive statistics and supervised machine learning to create a prediction model from monitoring data. Our approach is based on the assumption, that features of a certain time frame before a critical event (i. e., a failure or soft lock-up) can serve as an indicator. Consequently, our model is trained with monitoring data from critical and healthy time frames. The evaluation with standard monitoring data collected from the HPC systems at RWTH Aachen University shows that our classifier is able to locate potentially failing nodes with a 10-fold cross precision of 98% and recall of 91 %. © 2017 IEEE.","Big data; Failure prediction; Large scale; Machine learning; Monitoring","Artificial intelligence; Cluster computing; Computer architecture; Data mining; Digital storage; Forecasting; Learning systems; Locks (fasteners); Monitoring; Silicon compounds; Supervised learning; Accurate prediction; Component failures; Descriptive statistics; Failure prediction; High performance computing systems; Large scale; Proactive measures; Supervised machine learning; Big data",2-s2.0-85032637445
"Maione C., Turra C., Fernandes E.A.N., Bacchi M.A., Barbosa F., Jr., Barbosa R.M.","Finding the Most Significant Elements for the Classification of Organic Orange Leaves: A Data Mining Approach",2017,"Analytical Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029715434&doi=10.1080%2f00032719.2017.1286667&partnerID=40&md5=d71ab3ce094af41c7a27c20946aa266d","Brazil is the world’s largest producer of oranges. The Brazilian conventional citrus crop requires repeated application of agrochemicals to achieve satisfactory levels of productivity. The organic citriculture is an alternative production system, which is environmentally friendly and offers a safe food to consumers. However, it is difficult to determine if a food or plant was cultivated in organic or conventional system by just common observation, which makes the customers of organic food market vulnerable against fraudulent entrepreneurs. In this study, we present a data mining approach for the study of Brazilian organic citrus leaves which can aid in the certification of authenticity of the citrus leaves. The elemental composition is determined by inductively coupled plasma-mass spectrometry (ICP-MS). We developed classification models based on support vector machines and artificial neural networks capable of predicting whether a citrus leaf is organic or conventional through analysis of the concentration levels of the 14 chemical elements (Al, Ba, Co, Cr, Cs, Cu, Fe, Mg, Mn, Ni, Rb, Si, Sr, and V) found in both types of leaves. Feature selection filter methods are used to determine the most relevant elements for the classification process. Our best model obtained was a support vector machine with approximately 88% prediction accuracy. The elements Mn, Mg, and Rb were evaluated as the most significant for the classification decision. This is the first paper which addresses the problem of classification of organic orange leaves based on chemical composition. The presented methodology is useful for attesting authenticity of organic citrus leaves and can be adapted for other organic food or substances. © 2017 Taylor & Francis.","Citrus leaves; classification; data mining; feature selection; inductively coupled plasma-mass spectrometry (ICP-MS)",,2-s2.0-85029715434
"Yang J., Liu M., Lu J., Miao Y., Hossain M.A., Alhamid M.F.","Botanical Internet of Things: Toward Smart Indoor Farming by Connecting People, Plant, Data and Clouds",2017,"Mobile Networks and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029738420&doi=10.1007%2fs11036-017-0930-x&partnerID=40&md5=df588df736d972fa4a13906f1cf81c3a","With rapid development of a new generation of communication technology, sensor technology as well as big data technology, the application scenes of Internet of Things (IOT) based on these technologies increase constantly in extensive fields, for instance, there are great contributions of Internet of Things in fields such as smart home, intelligent transportation, intelligent healthcare, intelligent monitoring as well as intelligent agriculture. At present, as for intelligent agriculture, the main focus is monitoring agricultural environment with IOT and M2M technology. In the era of population explosion, agricultural resources such as farmland become more and more insufficient, a indoor intelligent agricultural IOT system is designed and implemented by the author in order to attack this conundrum. And the system directs an new trend for agricultural development. With the capability of parallel extension, the system can connect to large-scale indoor farms gradually thus to make these farms combining with each other organically. Finally, information mining shall be achieved based on large amount of sensing data by utilizing the big data technology and machine learning algorithms, and those derived information shall be adopted as critical reference to support indoor agricultural activities. © 2017 Springer Science+Business Media, LLC","Big data; Cloud computing; Internet of things; Machine learning; Smart green house; Smart indoor farming","Agricultural machinery; Agriculture; Artificial intelligence; Automation; Cloud computing; Data mining; Engineering education; Green computing; Intelligent buildings; Internet of things; Learning algorithms; Learning systems; Agricultural activities; Agricultural development; Agricultural environments; Agricultural resources; Communication technologies; Intelligent transportation; Internet of Things (IOT); Smart indoor farming; Big data",2-s2.0-85029738420
"Guzman E., Ibrahim M., Glinz M.","A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032809108&doi=10.1109%2fRE.2017.88&partnerID=40&md5=4ef8fae0ed769ffbef63d7bf78a21625","Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements. © 2017 IEEE.","requirements elicitation; software evolution; text mining; Twitter; user feedback","Data mining; Learning systems; Requirements engineering; Social networking (online); Requirements elicitation; Software Evolution; Text mining; Twitter; User feedback; Application programs",2-s2.0-85032809108
"Sreepathi S., Kumar J., Mills R.T., Hoffman F.M., Sripathi V., Hargrove W.W.","Parallel Multivariate Spatio-Temporal Clustering of Large Ecological Datasets on Hybrid Supercomputers",2017,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032647541&doi=10.1109%2fCLUSTER.2017.88&partnerID=40&md5=f54bd448852f8b5a78b381753ff24124","A proliferation of data from vast networks of remote sensing platforms (satellites, unmanned aircraft systems (UAS), airborne etc.), observational facilities (meteorological, eddy covariance etc.), state-of-The-Art sensors, and simulation models offer unprecedented opportunities for scientific discovery. Unsupervised classification is a widely applied data mining approach to derive insights from such data. However, classification of very large data sets is a complex computational problem that requires efficient numerical algorithms and implementations on high performance computing (HPC) platforms. Additionally, increasing power, space, cooling and efficiency requirements has led to the deployment of hybrid supercomputing platforms with complex architectures and memory hierarchies like the Titan system at Oak Ridge National Laboratory. The advent of such accelerated computing architectures offers new challenges and opportunities for big data analytics in general and specifically, large scale cluster analysis in our case. Although there is an existing body of work on parallel cluster analysis, those approaches do not fully meet the needs imposed by the nature and size of our large data sets. Moreover, they had scaling limitations and were mostly limited to traditional distributed memory computing platforms. We present a parallel Multivariate Spatio-Temporal Clustering (MSTC) technique based on k-means cluster analysis that can target hybrid supercomputers like Titan. We developed a hybrid MPI, CUDA and OpenACC implementation that can utilize both CPU and GPU resources on computational nodes. We describe performance results on Titan that demonstrate the scalability and efficacy of our approach in processing large ecological data sets. © 2017 IEEE.","Big data analytics; GPU application; Hybrid supercomputing; Parallel k-means clustering","Classification (of information); Cluster analysis; Cluster computing; Complex networks; Computational efficiency; Computer architecture; Data handling; Data mining; Ecology; Fighter aircraft; Graphics processing unit; Memory architecture; Network architecture; Remote sensing; Space platforms; Supercomputers; Unmanned aerial vehicles (UAV); Data analytics; High performance computing (HPC); Hybrid supercomputing; K-means clustering; Oak ridge National Laboratory; Remote sensing platforms; Spatio-temporal clustering; Unsupervised classification; Big data",2-s2.0-85032647541
"Rath M., Rempel P., Mader P.","The IlmSeven Dataset",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032834514&doi=10.1109%2fRE.2017.18&partnerID=40&md5=1d45efd2e3c4ec79d754f083d21301cd","Developing new ideas and algorithms or comparing new findings in the field of requirements engineering and management implies a dataset to work with. Collecting the required data is time consuming, tedious, and may involve unforeseen difficulties. The need for datasets often forces re-searchers to collect data themselves in order to evaluate their findings. However, comparing results with other publications is especially difficult on proprietary datasets. A big obstacle is the reproduction of a previously used dataset, which may include subtle preprocessing steps not explicitly mentioned by the original authors. Providing a predefined dataset avoids these problems. It establishes a common baseline and enables direct comparison for benchmarking. This paper provides a well defined dataset consisting of seven open source software projects. It contains a large number of typed development artifacts and links between them. Enriched with additional metadata, such as time stamps, versions, and component information, the dataset allows answering a broad range of research questions. © 2017 IEEE.","data collection; data mining; mining software repositories; requirements analysis; traceability","Data mining; Open source software; Open systems; Requirements engineering; Software engineering; Data collection; Mining software repositories; Open source software projects; Pre-processing step; Requirements analysis; Research questions; Time stamps; traceability; Data acquisition",2-s2.0-85032834514
"Johann T., Stanik C., Alizadeh A.M.B., Maalej W.","SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032796598&doi=10.1109%2fRE.2017.71&partnerID=40&md5=8fbb2f39da61966de6d1c53a04a3fea8","A main advantage of app stores is that they aggregate important information created by both developers and users. In the app store product pages, developers usually describe and maintain the features of their apps. In the app reviews, users comment these features. Recent studies focused on mining app features either as described by developers or as reviewed by users. However, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e.g. allowing analysts to identify which app features are actually being reviewed and which are not. In this paper, we propose SAFE, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them. We manually build 18 part-of-speech patterns and 5 sentence patterns that are frequently used in text referring to app features. We then apply these patterns with several text pre-and post-processing steps. A major advantage of our approach is that it does not require large training and configuration data. To evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps. The extraction precision and recall outperformed two state-of-the-art approaches. For well-maintained app pages such as for Google Drive our approach has a precision of 87% and on average 56% for 10 evaluated apps. SAFE also matches 87% of the features extracted from user reviews to those extracted from the app descriptions. © 2017 IEEE.","App Store Analytics; Data Mining; Data-Driven Requirements; Software Feature; User Reviews","Data mining; Digital storage; Extraction; Requirements engineering; App stores; Data driven; Part Of Speech; Post processing; Precision and recall; Simple approach; Software features; User reviews; Application programs",2-s2.0-85032796598
"Si H., Wang L., Zhang J., Liu Z.","A solid-discrete-based method for extracting the cutter-workpiece engagement in five-axis flank milling",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029765910&doi=10.1007%2fs00170-017-1097-0&partnerID=40&md5=7d97400d2c752777758ec7956dc5c3ea","Five-axis flank milling has been widely used in industry because of its high material removal rate, low tool loss, and achievement of good surface quality. The cutting force, which is the basis for studying the milling process, is mainly determined by the cutting coefficients, spindle speed, feed rate, and cutter-workpiece engagement (CWE). However, due to the complex spatial motion of the cutting tool, extracting the CWE with high precision and efficiency in five-axis flank milling has become a challenging topic. This paper proposes a novel and integrated solid-discrete-based method to identify the CWE data in a flank milling workpiece with ruled surfaces. In this method, the tool path and cutting location source (CLS) file of certain tool/workpiece combinations are obtained by commercial CAM software. Two cutting types, first cut and following cut, are considered. First, the point cloud data of the machining surface are extracted, and the instantaneous contact profile of the tool and the workpiece is obtained according to the tool geometry information at different cutting location points. The feasible contact surface is then constructed based on an analysis of the surface normal and instantaneous feed direction at the arbitrary tool point. After trimming the contact profile by the feasible contact boundary, the exact CWE can finally be extracted. Compared with the existing methods for CWE extraction, the proposed method replaces the solid model with discrete coordinate points in the calculation process and is thus more suitable for system integration. Experiments and applications show that this method has high accuracy and computational efficiency. © 2017 Springer-Verlag London Ltd.","Cutter-workpiece engagement; Five-axis flank milling; Ruled surface; Solid-discrete-based method","Computational efficiency; Cutting tools; Data mining; Efficiency; Calculation process; Cutting coefficients; Discrete coordinates; Flank milling; Machining surfaces; Material removal rate; Ruled surfaces; Workpiece; Milling (machining)",2-s2.0-85029765910
"Kandhro A.H., Shoombuatong W., Nantasenamat C., Prachayasittikul V., Nuchnoi P.","The MicroRNA interaction network of lipid diseases",2017,"Frontiers in Genetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030116629&doi=10.3389%2ffgene.2017.00116&partnerID=40&md5=034c1ebb6d8d52fc4185a42512330d95","Background: Dyslipidemia is one of the major forms of lipid disorder, characterized by increased triglycerides (TGs), increased low-density lipoprotein-cholesterol (LDL-C), and decreased high-density lipoprotein-cholesterol (HDL-C) levels in blood. Recently, MicroRNAs (miRNAs) have been reported to involve in various biological processes; their potential usage being a biomarkers and in diagnosis of various diseases. Computational approaches including text mining have been used recently to analyze abstracts from the public databases to observe the relationships/associations between the biological molecules, miRNAs, and disease phenotypes. Materials and Methods: In the present study, significance of text mined extracted pair associations (miRNA-lipid disease) were estimated by one-sided Fisher's exact test. The top 20 significant miRNA-disease associations were visualized on Cytoscape. The CyTargetLinker plug-in tool on Cytoscape was used to extend the network and predicts new miRNA target genes. The Biological Networks Gene Ontology (BiNGO) plug-in tool on Cytoscape was used to retrieve gene ontology (GO) annotations for the targeted genes. Results: We retrieved 227 miRNA-lipid disease associations including 148 miRNAs. The top 20 significant miRNAs analysis on CyTargetLinker provides defined, predicted and validated gene targets, further targeted genes analyzed by BiNGO showed targeted genes were significantly associated with lipid, cholesterol, apolipoprotein, and fatty acids GO terms. Conclusion: We are the first to provide a reliable miRNA-lipid disease association network based on text mining. This could help future experimental studies that aim to validate predicted gene targets. © 2017 Kandhro, Shoombuatong, Nantasenamat, Prachayasittikul and Nuchnoi.","Dyslipidemia; Interaction network; Lipid diseases; MicroRNA; Text mining","apolipoprotein; fatty acid; high density lipoprotein cholesterol; low density lipoprotein cholesterol; microRNA; Article; bioinformatics; computer model; CyTargetLinker; Cytoscape; data base; dyslipidemia; Fisher exact test; gene ontology; genetic analysis; hypercholesterolemia; hyperlipidemia; information retrieval; molecular biology; molecular genetics; protein RNA binding; software; theory construction",2-s2.0-85030116629
"Williams G., Mahmoud A.","Mining Twitter Feeds for Software User Requirements",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032817137&doi=10.1109%2fRE.2017.14&partnerID=40&md5=eee9e05b133157f492000f528b2eaddd","Twitter enables large populations of end-users of software to publicly share their experiences and concerns about software systems in the form of micro-blogs. Such data can be collected and classified to help software developers infer users' needs, detect bugs in their code, and plan for future releases of their systems. However, automatically capturing, classifying, and presenting useful tweets is not a trivial task. Challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of irrelevant information and spam. Motivated by these challenges, this paper reports on a three-fold study that is aimed at leveraging Twitter as a main source of software user requirements. The main objective is to enable a responsive, interactive, and adaptive data-driven requirements engineering process. Our analysis is conducted using 4,000 tweets collected from the Twitter feeds of 10 software systems sampled from a broad range of application domains. The results reveal that around 50% of collected tweets contain useful technical information. The results also show that text classifiers such as Support Vector Machines and Naive Bayes can be very effective in capturing and categorizing technically informative tweets. Additionally, the paper describes and evaluates multiple summarization strategies for generating meaningful summaries of informative software-relevant tweets. © 2017 IEEE.","summarization; text classification; Twitter; user requirements","Application programs; Classification (of information); Computer software; Requirements engineering; Social networking (online); Text processing; Informative tweets; Requirements engineering process; Software developer; summarization; Technical information; Text classification; Twitter; User requirements; Program debugging",2-s2.0-85032817137
"Thelwall M.","Social media analytics for YouTube comments: potential and limitations",2017,"International Journal of Social Research Methodology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029896102&doi=10.1080%2f13645579.2017.1381821&partnerID=40&md5=432335126bdcfb8d17813d7e63686dc2","The need to elicit public opinion about predefined topics is widespread in the social sciences, government and business. Traditional survey-based methods are being partly replaced by social media data mining but their potential and limitations are poorly understood. This article investigates this issue by introducing and critically evaluating a systematic social media analytics strategy to gain insights about a topic from YouTube. The results of an investigation into sets of dance style videos show that it is possible to identify plausible patterns of subtopic difference, gender and sentiment. The analysis also points to the generic limitations of social media analytics that derive from their fundamentally exploratory multi-method nature. © 2017 Informa UK Limited, trading as Taylor & Francis Group","gender; issues; opinion mining; sentiment; Social media analytics; YouTube comments",,2-s2.0-85029896102
"Dwivedi D.","Reduction kinetics of iron ore pellets with coal and coal dust: reduction relation with crystalline phases",2017,"Transactions of the Institutions of Mining and Metallurgy, Section C: Mineral Processing and Extractive Metallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029899394&doi=10.1080%2f03719553.2017.1375766&partnerID=40&md5=d94a45bc449a073c7847b004d56060c1","Mining sites are source of iron ore fines which can be reduced by coal and coal dust and crystalline phases of iron ore plays important role in this regards. In this experiment, Author validated experimental data with the chemical kinetic model. No significant difference has been observed in the degree of reduction on the type of coal used as the reducing agent. The activation energy was higher (124.71 KJ) while choosing coal as a reducing agent than the coal + coal dust as a reducing agent (59.75 KJ). The highest reaction rate constant was observed at 1000°C. Abnormal swelling was noted at 850 and 900°C, whereas shrinkage was noted at 950 and 1000°C. © 2017 Institute of Materials, Minerals and Mining and The AusIMM Published by Taylor & Francis on behalf of the Institute and The AusIMM","coal; coal dust; hair line cracking; Iron ore pellets; reduction kinetics; shrinkage; swelling","Activation energy; Coal; Crystalline materials; Iron; Iron ore pellets; Iron ore reduction; Iron ores; Kinetics; Ore pellets; Ore reduction; Ores; Pelletizing; Rate constants; Reducing agents; Reduction; Shrinkage; Swelling; Chemical kinetic model; Crystalline phasis; Degree of reduction; Iron ore fines; Mining sites; Reduction kinetics; Coal dust",2-s2.0-85029899394
"Nayebi M., Ruhe G.","Optimized Functionality for Super Mobile Apps",2017,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032818225&doi=10.1109%2fRE.2017.72&partnerID=40&md5=715ef13f6f46de2e651ca46a6f3749bf","Functionality of software products often does not match user needs and expectations. The closed set-up of systems and information is replaced by wide access to data of users and competitor products. This shift offers completely new opportunities to approach requirements elicitation and subsequent planning of software functionality. This is, in particular true for app store markets. App stores are markets for many small sized software products which provide an open platform for users to provide feedback on using apps. Moreover, the functionality and status of similar software products can be retrieved. While this is a competitive risk, it is at the same time an opportunity.In this paper, we envision a new release planning approach that leverages the new opportunities for decision making. We propose a new model using bi-criterion integer programming. We make suggestions for optimized super app functionality that are based on two key aspects: (i) the estimated value of features, and (ii) the cohesiveness between newly added features and cohesiveness between existing and the features to be added. The information on these attributes comes from reasoning on feature composition of existing similar apps. The approach is applicable to the development of new product releases as well as to the creation of completely new apps. We illustrate the applicability of our model by a small example and outline directions for future research. © 2017 IEEE.","app store mining; integer programming; new product design; Release planning; super app design","Application programs; Commerce; Decision making; Product design; Requirements engineering; App stores; New product design; Release planning; Release planning approaches; Requirements elicitation; Software functionality; Software products; Systems and information; Integer programming",2-s2.0-85032818225
"Aodeng S., Gao Z.","Otorhinolaryngology publication from Chinese authors: a 11-year survey of the literature",2017,"Acta Oto-Laryngologica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029898791&doi=10.1080%2f00016489.2017.1373853&partnerID=40&md5=fc66d3d7cc34b98b09f3b2f7b1a04de1","Objective: Scientific publication is the reflection of the capability of not only an individual scholar, but also a group, even a country. Over the past few decades, Chinese researchers have made great progress in medical scientific field. However, the status about the quantity and quality of the publications in otorhinolaryngology have not been reported. The aim of this study was to compare the output by Chinese authors from three regions of China: Mainland China (MC), Taiwan (TW) and Hong Kong (HK). Methods: Literature was retrieved from the 43 otorhinolaryngology journals based on the subject category ‘otorhinolaryngology’ of the Science Citation Index Expanded (2015) from Web of Science Core Collection. The first authors of these articles were limited in three regions of China: MC, TW and HK from 2006 to 2016 by using the data mining software Thomson Data Analyzer (TDA). Evaluation criteria are based on total number of articles, impact factors (IFs), citations, articles published in high-impact journals and funding support. Result: A total of 59,832 articles were published worldwide in 43 otorhinolaryngology related journals from 2006 to 2016. Publications from MC was rapidly increasing and the total number contributed the most articles of the China (1931/3362, 57.44%), followed by TW (1220/3362, 36.29%) and HK (211/3362, 6.28%). The quantity of annual publications from MC has exceeded that of TW since 2010. MC was in the first place for cumulative IFs, but the last place for average IF. For total and average citations, MC was in the same situation of IF. Acta Otolaryngol was the most popular journal to choose in MC, and for TW and HK was Head & Neck. Conclusions: The total number of otorhinolaryngology articles in China increased markedly from 2006 to 2016, especially for MC. Despite the rapid growth in the number of articles from MC, the quality was not that satisfactory. © 2017 Acta Oto-Laryngologica AB (Ltd)","Chinese authors; citations; funding support; impact factor (IF); Otorhinolaryngology; science citation index expanded (SCIE)",,2-s2.0-85029898791
"Yu T., Wang X., Shami A.","Recursive Principal Component Analysis based Data Outlier Detection and Sensor Data Aggregation in IoT Systems",2017,"IEEE Internet of Things Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030705086&doi=10.1109%2fJIOT.2017.2756025&partnerID=40&md5=9b2d9a47019916a1bbf7613a7575a674","Internet of Things (IoT) is emerging as the underlying technology of our connected society, which enables many advanced applications. In IoT-enabled applications, information of application surroundings is gathered by networked sensors, especially wireless sensors due to their advantage of infrastructure-free deployment. However, the pervasive deployment of wireless sensor nodes generate massive amount of sensor data, and data outliers are frequently incurred due to the dynamic nature of wireless channels. As operation of IoT systems relies on sensor data, data redundancy and data outliers could significantly reduce the effectiveness of IoT applications or even mislead systems into unsafe conditions. In this paper, a cluster-based data analysis framework is proposed using recursive principal component analysis (R-PCA), which can aggregate the redundant data and detect the outliers in the meantime. More specifically, at a cluster head, spatially correlated sensor data collected from cluster members are aggregated by extracting the principal components (PCs), and potential data outliers are determined by the abnormal squared prediction error (SPE) score, which is defined as the square of residual value after extraction of PCs. With R-PCA, the parameters of PCA model can be recursively updated to adapt to the changes in IoT systems. Cluster-based data analysis framework also releases the computational and processing burdens on sensor nodes. Practical databases based simulations have confirmed that the proposed framework efficiently aggregates the correlated sensor data with high recovery accuracy. The data outlier detection accuracy is also improved by the proposed method compared to other existing algorithms. IEEE","Algorithm design and analysis; Anomaly detection; Correlation; Data aggregation; Data Aggregation; Data analysis; Internet of Things (IoT); Outlier Detection; Principal component analysis; Recursive Principal Component Analysis (R-PCA).; Wireless sensor networks","Aggregates; Correlation methods; Data handling; Data mining; Data reduction; Information analysis; Internet of things; Sensor nodes; Statistics; Wireless sensor networks; Algorithm design and analysis; Anomaly detection; Data aggregation; Internet of Things (IOT); Outlier Detection; Recursive principal component analysis; Principal component analysis",2-s2.0-85030705086
"Dong B., Wang H.W., Monreale A., Pedreschi D., Giannotti F., Guo W.","Authenticated Outlier Mining for Outsourced Databases",2017,"IEEE Transactions on Dependable and Secure Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031101070&doi=10.1109%2fTDSC.2017.2754493&partnerID=40&md5=f79feb6c0ba3d2b28b470e8250e46c14","The Data-Mining-as-a-Service (DMaS) paradigm is becoming the focus of research, as it allows the data owner (client) who lacks expertise and/or computational resources to outsource their data and mining needs to a third-party service provider (server). Outsourcing, however, raises some issues about result integrity: how could the client verify the mining results returned by the server are both sound and complete? In this paper, we focus on outlier mining, an important mining task. Previous verification techniques use an authenticated data structure (ADS) for correctness authentication, which may incur much space and communication cost. In this paper, we propose a novel solution that returns a probabilistic result integrity guarantee with much cheaper verification cost. The key idea is to insert a set of artificial records (ARs) into the dataset, from which it constructs a set of artificial outliers (AOs) and artificial non-outliers (ANOs). The AOs and ANOs are used by the client to detect any incomplete and/or incorrect mining results with a probabilistic guarantee. The main challenge that we address is how to construct ARs so that they do not change the (non-)outlierness of original records, while guaranteeing that the client can identify ANOs and AOs without executing mining. Furthermore, we build a strategic game and show that a Nash equilibrium exists only when the server returns correct outliers. Our implementation and experiments demonstrate that our verification solution is efficient and lightweight. IEEE","Anomaly detection; Authentication; authentication; Databases; game theory; outlier mining; Outsourcing; outsourcing; probabilistic guarantees; Probabilistic logic; Servers","Computation theory; Data mining; Database systems; Game theory; Outsourcing; Probabilistic logics; Servers; Statistics; Anomaly detection; Authenticated data structures; Computational resources; Outlier Mining; Outsourced Databases; Probabilistic guarantees; Third-party service providers; Verification techniques; Authentication",2-s2.0-85031101070
"Kao H.-A., Hsieh Y.-S., Chen C.-H., Lee J.","Quality prediction modeling for multistage manufacturing based on classification and association rule mining",2017,"MATEC Web of Conferences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030172030&doi=10.1051%2fmatecconf%2f201712300029&partnerID=40&md5=4be61ef4cbc3cba6dc7cfac1675ce537","For manufacturing enterprises, product quality is a key factor to assess production capability and increase their core competence. To reduce external failure cost, many research and methodology have been introduced in order to improve process yield rate, such as TQC/TQM, Shewhart CycleDeming's 14 Points, etc. Nowadays, impressive progress has been made in process monitoring and industrial data analysis because of the Industry 4.0 trend. Industries start to utilize quality control (QC) methodology to lower inspection overhead and internal failure cost. Currently, the focus of QC is mostly in the inspection of single workstation and final product, however, for multistage manufacturing, many factors (like equipment, operators, parameters, etc.) can have cumulative and interactive effects to the final quality. When failure occurs, it is difficult to resume the original settings for cause analysis. To address these problems, this research proposes a combination of principal components analysis (PCA) with classification and association rule mining algorithms to extract features representing relationship of multiple workstations, predict final product quality, and analyze the root-cause of product defect. The method is demonstrated on a semiconductor data set. © 2017 The Authors, published by EDP Sciences.",,"Association rules; Cost benefit analysis; Data mining; Industrial research; Inspection; Machinery; Manufacture; Precision engineering; Principal component analysis; Process monitoring; In-process monitoring; Manufacturing enterprise; Multistage manufacturing; Principal components analysis; Production capabilities; Quality prediction models; Rule mining algorithms; Semiconductor data; Quality control",2-s2.0-85030172030
"Nuradilah Azman S., Ishak I., Mohd Sharef N., Sidi F.","Towards an Enhanced Aspect-based Contradiction Detection Approach for Online Review Content",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030158093&doi=10.1088%2f1742-6596%2f892%2f1%2f012006&partnerID=40&md5=d869ac574d508ddf204376e889a4a03f","User generated content as such online reviews plays an important role in customer's purchase decisions. Many works have focused on identifying satisfaction of the reviewer in social media through the study of sentiment analysis (SA) and opinion mining. The large amount of potential application and the increasing number of opinions expresses on the web results in researchers interest on sentiment analysis and opinion mining. However, due to the reviewer's idiosyncrasy, reviewer may have different preferences and point of view for a particular subject which in this case hotel reviews. There is still limited research that focuses on this contradiction detection in the perspective of tourism online review especially in numerical contradiction. Therefore, the aim of this paper to investigate the type of contradiction in online review which mainly focusing on hotel online review, to provide useful material on process or methods for identifying contradiction which mainly on the review itself and to determine opportunities for relevant future research for online review contradiction detection. We also proposed a model to detect numerical contradiction in user generated content for tourism industry. © Published under licence by IOP Publishing Ltd.",,"Physics; Detection approach; Large amounts; Online reviews; Opinion mining; Purchase decision; Sentiment analysis; Tourism industry; User-generated content; Data mining",2-s2.0-85030158093
"Bernardo A., Cervero A., Esteban M., Tuero E., Casanova J.R., Almeida L.S.","Freshmen program withdrawal: Types and recommendations",2017,"Frontiers in Psychology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029687406&doi=10.3389%2ffpsyg.2017.01544&partnerID=40&md5=40597ff797a4008a42f444f758297ead","University program dropout is a problem that has important consequences not only for the student that leaves but also for the institution in which the withdrawal occurs. Therefore, higher education institutions must study the problem in greater depth to establish appropriate prevention measures in the future. However, most research papers currently focus primarily on the characteristics of students who leave university, rather than on those who choose to pursue alternative courses of study and therefore fail to take into account the different kinds of abandonment. The aim of this paper is to identify the different types of dropout to define their characteristics and propose some recommendations. Thus, an ex post facto study was carried out on a sample of 1,311 freshmen from a university in the north of Spain using data gathered using an ad-hoc designed questionnaire, applied by telephone or an online survey, and completed with data available in the university data warehouse. A descriptive analysis was performed to characterize the sample and identify five different groups, including 1. Students persisting in their initiated degree 2. Students who change of program (within the same university) 3. Students transferring to a different university 4. Students enrolling in non-higher-education studies 5. Students that quit studying. Also, data mining techniques (decision trees) were applied to classify the cases and generate predictive models to aid in the design of differentiated intervention strategies for each of the corresponding groups. © 2017 Bernardo, Cervero, Esteban, Tuero, Casanova and Almeida.","Dropout; Performance; Persistence; Undergraduate student; University",,2-s2.0-85029687406
"Lin S.-C., Huang Y.","A histogram statistical method for the detection of localized faults in deep groove ball bearing",2017,"MATEC Web of Conferences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030173626&doi=10.1051%2fmatecconf%2f201712300009&partnerID=40&md5=5c598df0759462df9b16c0e444ed3617","This study aims to use the histogram statistical method to establish a deep groove ball bearing fault diagnosis strategy. First, statistical indicators are used to excavate the fault characteristics buried in the vibration signal, and use the histogram to define the characteristic area for fault diagnosis. The results show that the indicators 1, 3, 6 have better statistical differences. Based on this, the accuracy of pattern recognition for all test data is 100%. Finally, the statistical significance of ball damage was significant, and the results showed high correlation (56∼73%). The correlation between inner race damage model was 49∼57% and healthy model was 52%. As the inner race damage and health model in the statistical sense, there are some similar, so there is a relatively high correlation. In the future research work, it will be committed to mining more representative indicators to enhance the relevance of abnormal characteristics. © 2017 The Authors, published by EDP Sciences.",,"Ball bearings; Failure analysis; Fault detection; Graphic methods; Machinery; Manufacture; Pattern recognition; Precision engineering; Statistical methods; Statistics; Abnormal characteristics; Bearing fault diagnosis; Fault characteristics; Localized fault; Statistical differences; Statistical indicators; Statistical significance; Vibration signal; Deep groove ball bearings",2-s2.0-85030173626
"Ghomi H., Fu L., Bagheri M., Miranda-Moreno L.F.","Identifying vehicle driver injury severity factors at highway-railway grade crossings using data mining algorithms",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032824117&doi=10.1109%2fICTIS.2017.8047900&partnerID=40&md5=902c10ac4107646de3affe4651ac46d7","The purpose of this paper is to identify vehicle driver injury severity factors of highway-railway grade crossing (HRGC) accidents in order to detect interactions as well as dissimilarities among accident factors. At this aim, data mining techniques were used to analyze the interaction of multiple factors in large databases. This paper applies Classification-Regression Tree (CART) and Association Rules algorithms on the U.S. Federal Railroad Administration (FRA) HRGC accident database for the period of 2006-2013 to identify vehicle driver injury severity factors at HRGCs. Both the classification trees and the rules discovery were effective in providing meaningful insights about accident factors and their interaction. The results of the two algorithms were never contradictory. Furthermore, most of the findings of this study were consistent with the results of previous studies which used different analytical techniques, such as probabilistic models of accident injury severity. The results show that train speed, type of road vehicle, driver age and gender, position of road vehicle before accident, type of accident and highway pavement type are the key factors influencing the driver injury severity. © 2017 IEEE.","Association Rules; Classification-Regression Tree (CART); Data mining; Highway railway grade crossing (HRGC); Severity of accidents","Accidents; Association rules; Classification (of information); Driver training; Highway accidents; Railroad accidents; Railroad crossings; Railroads; Transportation; Trees (mathematics); Vehicles; Classification regression; Classification trees; Data mining algorithm; Driver injury severities; Federal Railroad Administration; Highway-railway grade crossing; Multiple factors; Probabilistic models; Data mining",2-s2.0-85032824117
"Tang C., Wang H., Liu J., Liu C.","A design for AIS data warehouse of traffic flow",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032808669&doi=10.1109%2fICTIS.2017.8047874&partnerID=40&md5=1814a739e6015536f9865450efae2647","AIS information has obvious characteristics of large data and is one of best sources to describe vessel traffic flow. The choice of types of data storage systems is critical for AIS data. AIS database holds inherent deficiencies in data mining of vessel traffic flow, spending much time on data pre-processing, low executing efficiency and thus greatly increasing server loads. However, AIS data warehouse would provide important and direct support for this mining. The study, first, concludes the primary characteristics of a data warehouse of traffic flow, and the relationship with AIS database, then discusses the mining demands, finally, a star schema is adopted for the multi-dimensioned design. This design would lay fundamentals for the setup of data warehouse and the mining of potential patterns of traffic flow. © 2017 IEEE.","AIS; data warehouse; multi-dimensional design; traffic flow","Artificial intelligence; Data mining; Data storage equipment; Data warehouses; Digital storage; Waterway transportation; Data preprocessing; Data storage systems; Large data; Multi dimensional; Server loads; Star schema; Traffic flow; Data handling",2-s2.0-85032808669
"Wang K., Yan X., Yuan Y., Jiang X., Lodewijks G., Negenborn R.R.","Study on route division for ship energy efficiency optimization based on big environment data",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032789752&doi=10.1109%2fICTIS.2017.8047752&partnerID=40&md5=7879425b94e2ea1cf526f9fb941b4379","In the case of the global energy crisis and the higher sound of energy saving and emission reduction, how to take effective management measures of ship energy efficiency to achieve the goal of energy saving and emission reduction, put forward a new challenge for the development of shipping technology. The application of big data technology provides a new idea for the research of ship energy efficiency optimization management. The energy efficiency management level of the operating ship can be improved by the analysis and mining of the big data. In this paper, a big data analysis platform for ship energy efficiency management based on the widely used Hadoop platform architecture is designed. Afterward, due to the huge amount of involved data on the energy efficiency management which has exceeded the processing ability of traditional solutions, the big data analysis method is used to achieve the route division according to environmental factors, thus to lay the foundation for speed optimization in different segments of a route. Finally, a simple decision-making method of optimal engine speed based on the result of route division is proposed, which could improve ship energy efficiency and hence reduce CO2 emission. © 2017 IEEE.","big data; energy saving and emission reduction; Hadoop; ship energy efficiency optimization","Big data; Data handling; Data mining; Decision making; Emission control; Energy conservation; Energy policy; Gas emissions; Information analysis; Information management; Ships; Data analysis methods; Decision-making method; Effective management; Efficiency managements; Energy efficiency optimizations; Energy saving and emission reductions; Environmental factors; Hadoop; Energy efficiency",2-s2.0-85032789752
"Le B., Luong P.","Optimized cardinality-based generalized itemset mining using transaction ID and numeric encoding",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029597028&doi=10.1007%2fs10489-017-1058-1&partnerID=40&md5=88192ef63e2271966a31f8a3fa104a82","In recent years, generalization-based data mining techniques have become an interesting topic for many data scientists. Generalized itemset mining is an exploration technique that focuses on extracting high-level abstractions and correlations in a database. However, the problem that domain experts must always deal with is how to manage and interpret a large number of extracted patterns from a massive database of transactions. In generalized pattern mining, taxonomies that contain abstraction information for each dataset are defined, so the number of frequent patterns can grow enormously. Therefore, exploiting knowledge turns into a difficult and costly process. In this article, we introduce an approach that uses cardinality-based constraints with transaction id and numeric encoding to mine generalized patterns. We applied transaction id to support the computation of each frequent itemset as well as to encode taxonomies into a numeric type using two simple rules. We also attempted to apply the combination of cardinality cons- traints and closed or maximal patterns. Experiments show that our optimizations significantly improve the performance of the original method, and the importance of comprehensive information within closed and maximal patterns is worth considering in generalized frequent pattern mining. © 2017 Springer Science+Business Media, LLC","Cardinality constraints; Closed itemset; Generalized itemset; Maximal itemset; Optimization","Abstracting; Encoding (symbols); Optimization; Signal encoding; Taxonomies; Cardinality constraints; Closed itemset; Comprehensive information; Exploration techniques; Frequent pattern mining; Generalized pattern; High-level abstraction; Itemset; Data mining",2-s2.0-85029597028
"Yang H., Shen L., Xiang Y., Yao Z., Liu X.","Freeway incident duration prediction using Bayesian network",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032808461&doi=10.1109%2fICTIS.2017.8047887&partnerID=40&md5=14994fcd5b63e08b78230687e4fcd149","In general, incident management and information dissemination strategies will benefit from the prediction of incident durations in real time. This study investigates the development of an incident duration prediction algorithm based on a detailed historical incident management database. A total of 2,629 incidents extracted by a data filter process were used in this study to predict the incident durations. A data mining technique, namely the Bayesian Network (BN) method is applied to develop incident duration prediction models. Based on the sequence of the incident duration process, two models were developed. The prediction accuracy of BN model for stage one is 45.7588%, while BN model for stage two has much higher prediction accuracy 72.5751 %. When compared with other data mining models, the goodness-of-fit results suggest that the BN model is advantageous in terms of higher prediction accuracy and the convenience of application. © 2017 IEEE.","Bayesian Network; Incident Duration Prediction; Incident Management; Intelligent Transportation System","Barium compounds; Bayesian networks; Filtration; Forecasting; Information dissemination; Intelligent systems; Data filter; Data mining models; Goodness of fit; Incident duration; Incident Management; Intelligent transportation systems; Prediction accuracy; Real time; Data mining",2-s2.0-85032808461
"Liu L., Wu C., Zhang H., Hasan A.H.N., Chu W., Charles A.","Research on taxi drivers' passenger hotspot selecting patterns based on GPS data: A case study in Wuhan",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032835185&doi=10.1109%2fICTIS.2017.8047802&partnerID=40&md5=1d489cc0d38cd7bcbf9ff91b810568b1","Taxis are the most important components for public transit systems. The model of taxis is more flexible and convenient for passengers compared to other transit model like buses or railway. However, the dynamic of taxi locations will have influence on transit efficiency. Therefore, the hotspot selecting patterns when unoccupied with regards to different income level drivers were investigated in this study. Two month GPS data of 7200 taxis in Wuhan was used as the data samples. The Wuhan City was firstly Mapped Meshing and divided into 4623 grids. After preprocessing the taxi GPS data and taxi drivers' income level classification (top, ordinary and bottom), the pickup points were filtered and matched with all these grids. Heat and grid probability for passenger demand hotspot were proposed and analyzed in this study. The results showed that the correlation value between top drivers and heat are not always higher than ordinary taxi drivers but the correlation value between grid probability and top taxi drivers are always high at all time slots. The finding of this study reveals that the high income taxis drivers have the high ability for cruising to the closed hotspot having high grid probability compared with middle or low income drivers. It therefore recommended that, if such experience could be represented and learned by other taxi drivers, the efficiency of taxi transit systems will be improved. Furthermore, if the information of real time heat and grid probability of hotspot could be broadcasted to taxi drivers, it will be beneficial for taxi transit systems as well. © 2017 IEEE.","data mining; GPS trace data; hotspot selection pattern; map meshing; passenger demand; Taxi Transit Systems","Data mining; Efficiency; Global positioning system; Mass transportation; Probability; Real time systems; Transportation; Correlation value; GPS traces; Hot spot; Income levels; Mapped meshing; Passenger demands; Public transit systems; Transit systems; Taxicabs",2-s2.0-85032835185
"Djokic-Petrovic M., Cvjetkovic V., Yang J., Zivanovic M., Wild D.J.","PIBAS FedSPARQL: A web-based platform for integration and exploration of bioinformatics datasets",2017,"Journal of Biomedical Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029772636&doi=10.1186%2fs13326-017-0151-z&partnerID=40&md5=85183da4c0b64e3378cf85071fae025c","Background: There are a huge variety of data sources relevant to chemical, biological and pharmacological research, but these data sources are highly siloed and cannot be queried together in a straightforward way. Semantic technologies offer the ability to create links and mappings across datasets and manage them as a single, linked network so that searching can be carried out across datasets, independently of the source. We have developed an application called PIBAS FedSPARQL that uses semantic technologies to allow researchers to carry out such searching across a vast array of data sources. Results: PIBAS FedSPARQL is a web-based query builder and result set visualizer of bioinformatics data. As an advanced feature, our system can detect similar data items identified by different Uniform Resource Identifiers (URIs), using a text-mining algorithm based on the processing of named entities to be used in Vector Space Model and Cosine Similarity Measures. According to our knowledge, PIBAS FedSPARQL was unique among the systems that we found in that it allows detecting of similar data items. As a query builder, our system allows researchers to intuitively construct and run Federated SPARQL queries across multiple data sources, including global initiatives, such as Bio2RDF, Chem2Bio2RDF, EMBL-EBI, and one local initiative called CPCTAS, as well as additional user-specified data source. From the input topic, subtopic, template and keyword, a corresponding initial Federated SPARQL query is created and executed. Based on the data obtained, end users have the ability to choose the most appropriate data sources in their area of interest and exploit their Resource Description Framework (RDF) structure, which allows users to select certain properties of data to enhance query results. Conclusions: The developed system is flexible and allows intuitive creation and execution of queries for an extensive range of bioinformatics topics. Also, the novel ""similar data items detection"" algorithm can be particularly useful for suggesting new data sources and cost optimization for new experiments. PIBAS FedSPARQL can be expanded with new topics, subtopics and templates on demand, rendering information retrieval more robust. © 2017 The Author(s).","Bioinformatics; Data integration; Data mining and information retrieval; Federated SPARQL query; Ontologies",,2-s2.0-85029772636
"Xue Y., Fan L., Liu A.G.","ITAIS: A novel framework for enhancing traffic safety",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032788557&doi=10.1109%2fICTIS.2017.8047882&partnerID=40&md5=3f9441f0bdd96d19d77b1eb67e9b7d85","Traffic safety is considered as a key issue in public health. Traffic collisions bring pain and suffering, and cause large amount of losses around the world each year. In this paper, a novel framework, called iTAIS, is proposed to enhance traffic safety using data mining and mobile computing techniques. iTAIS can provide users with driving tips based on their locations, which help to reduce the occurrence of potential traffic collisions. iTAIS consists of two main components: key factors identification and smart client applications. iTAIS first uses clustering algorithm to analyze the traffic collision data, which was collected from 2006 to 2010 in the city of Regina, Canada. In this step, traffic collisions will be grouped based on the locations of occurrence, and the key factors that contribute to the occurrence of collisions in each group will be identified respectively. In the second step, two smart client applications are designed to provide users with driving tips based on their locations. Experimental results show that iTAIS can effectively identify the key factors that contribute to the occurrence of traffic collisions occurred on different roads under various circumstances. Also, the two smart client applications can efficiently help users gain easy access to obtaining the driving tips, and help to further enhance traffic safety in the city of Regina. © 2017 IEEE.","data mining; mobile computing; traffic collision; traffic safety","Accident prevention; Clustering algorithms; Data mining; Location; Mobile computing; Safety engineering; Smart city; Key Issues; Large amounts; Smart client; Traffic collisions; Traffic safety; Highway accidents",2-s2.0-85032788557
"Chen C., Chen X.","Scheduling optimization in restricted channels based on the agent technology and Bayesian Network",2017,"2017 4th International Conference on Transportation Information and Safety, ICTIS 2017 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032785871&doi=10.1109%2fICTIS.2017.8047779&partnerID=40&md5=e2b2a229324e181cd94165d7c6b92fda","In ports and inland waterway, there are a considerable proportion restricted channels, in which vessels must navigate in a set route and time. The transit capacities of inland waterways and ports are limited by those restricted channels significantly, which can only be scheduled by experienced supervisors of local harbor and maritime administration manually. In fact, the scheduling optimization requires much experience and knowledge. Hence, not mangy supervisors are capable of optimizing those channels satisfactorily. This paper proposed an optimization scheduling approach based on the Agent technology and Bayesian Network. At the very beginning, three dimensional model of a specified channel is developed. Subsequently, the characteristics of vessel behaviors are extracted from AIS data with the help of Bayesian Network. On that basis, the models of vessel behaviors are built by the Agent technology. Different alternatives of scheduling can be simulated. Eventually, the performance of different scheduling might be obtained in the simulation of environment. This approach is based on data mining, Agent simulation and Bayesian Network; it does not need the manual experience any longer, which is capable of choosing the efficient scheduling plan in different scenarios. This approach improves the transit capacity of inland waterways and ports. © 2017 IEEE.","Agent; Bayesian Network; restricted channels; scheduling optimization","Agents; Bayesian networks; Data mining; Ports and harbors; Scheduling; Supervisory personnel; Agent simulation; Efficient scheduling; Maritime administrations; Optimization scheduling; restricted channels; Scheduling optimization; Three-dimensional model; Transit capacity; Inland waterways",2-s2.0-85032785871
"Hao F., Pei Z., Park D.-S., Yang L.T., Jeong Y.-S., Park J.-H.","Iceberg Clique queries in large graphs",2017,"Neurocomputing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019657800&doi=10.1016%2fj.neucom.2015.12.142&partnerID=40&md5=d23844c142520e4e62c634eb170195c1","This paper investigates the Iceberg Clique (IC) queries in a large graph, specially, given a user-specified threshold θ, an IC query reports the cliques where the number of vertices exceeds ⌊θ|V|⌋. Toward this end, a practical IC query theorem is formally proposed and proved. With this proposed query theorem, a formal context and its corresponding iceberg concept lattice are first constructed from an input graph topology by Modified Adjacency Matrix; then, we prove that the IC queries problem is equivalent to finding the iceberg equiconcepts whose number of elements exceeds ⌊θ|V|⌋. Theoretical analysis and experimental results demonstrate that the proposed query algorithm is feasible and efficient for finding the iceberg cliques from large graphs. © 2017","Formal context; Iceberg Clique; Iceberg concept lattice","Graph theory; Information analysis; Integrated circuits; Sea ice; Topology; Adjacency matrices; Formal contexts; Iceberg Clique; Iceberg concept lattices; Input graphs; Large graphs; Query algorithms; Query processing; algorithm; Article; conceptual framework; correlation analysis; data mining; formal concept analysis; Iceberg Concept Lattice theory; mathematical analysis; priority journal",2-s2.0-85019657800
"Boselli R., Cesarini M., Marrara S., Mercorio F., Mezzanzanica M., Pasi G., Viviani M.","WoLMIS: a labor market intelligence system for classifying web job vacancies",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029603583&doi=10.1007%2fs10844-017-0488-x&partnerID=40&md5=5523d9c7f15a8f170b8532f024337a89","In the last decades, an increasing number of employers and job seekers have been relying on Web resources to get in touch and to find a job. If appropriately retrieved and analyzed, the huge number of job vacancies available today on on-line job portals can provide detailed and valuable information about the Web Labor Market dynamics and trends. In particular, this information can be useful to all actors, public and private, who play a role in the European Labor Market. This paper presents WoLMIS, a system aimed at collecting and automatically classifying multilingual Web job vacancies with respect to a standard taxonomy of occupations. The proposed system has been developed for the Cedefop European agency, which supports the development of European Vocational Education and Training (VET) policies and contributes to their implementation. In particular, WoLMIS allows analysts and Labor Market specialists to make sense of Labor Market dynamics and trends of several countries in Europe, by overcoming linguistic boundaries across national borders. A detailed experimental evaluation analysis is also provided for a set of about 2 million job vacancies, collected from a set of UK and Irish Web job sites from June to September 2015. © 2017 Springer Science+Business Media, LLC","Information systems; Knowledge discovery; Labor market intelligence; Machine learning; Text classification","Commerce; Data mining; Education; Employment; Information systems; Intelligent systems; Learning systems; Text processing; European agency; European labor markets; Experimental evaluation; Job seekers; Labor markets; Text classification; Vocational education and training; Web resources; Classification (of information)",2-s2.0-85029603583
"Wu J., Pan S., Zhu X., Zhang C., Yu P.S.","Multiple Structure-View Learning for Graph Classification",2017,"IEEE Transactions on Neural Networks and Learning Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029418788&doi=10.1109%2fTNNLS.2017.2703832&partnerID=40&md5=8d6bca858413ed1f3775638455ee3763","Many applications involve objects containing structure and rich content information, each describing different feature aspects of the object. Graph learning and classification is a common tool for handling such objects. To date, existing graph classification has been limited to the single-graph setting with each object being represented as one graph from a single structure-view. This inherently limits its use to the classification of complicated objects containing complex structures and uncertain labels. In this paper, we advance graph classification to handle multigraph learning for complicated objects from multiple structure views, where each object is represented as a bag containing several graphs and the label is only available for each graph bag but not individual graphs inside the bag. To learn such graph classification models, we propose a multistructure-view bag constrained learning (MSVBL) algorithm, which aims to explore substructure features across multiple structure views for learning. By enabling joint regularization across multiple structure views and enforcing labeling constraints at the bag and graph levels, MSVBL is able to discover the most effective substructure features across all structure views. Experiments and comparisons on real-world data sets validate and demonstrate the superior performance of MSVBL in representing complicated objects as multigraph for classification, e.g., MSVBL outperforms the state-of-the-art multiview graph classification and multiview multi-instance learning approaches. IEEE","Graph; graph classification; multiview learning; subgraph mining.","Directed graphs; Complex structure; Content information; Graph; Graph classification; Multi-instance learning; Multi-view learning; Multiple structures; Subgraph mining; Classification (of information)",2-s2.0-85029418788
"QingHua J.","Data mining and management system design and application for college student mental health",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032833918&doi=10.1109%2fICITBS.2016.96&partnerID=40&md5=0bd6f0f4e0f029721cd04eaf5482e036","Developing college students' psychological data management system is one of the important means to prevent college students from psychological crisis. So, data mining technology based on BP neural network is used in the current students' psychological data management system to improve its operation effectiveness. Design method and the feasibility of applying data mining technology in psychological crisis prevention is investigated. MATLAB 2014a is used to realize the kernel of the psychological data mining and the kernel is embedded into student psychological management system to improve the effectiveness of data mining technology in psychological crisis prevention. © 2016 IEEE.","BP neural network; Data mining; Mental health","Big data; Data mining; Design; Neural networks; Smart city; Students; BP neural networks; Crisis preventions; Data management system; Data mining and management; Data mining technology; Design and application; Mental health; Psychological managements; Information management",2-s2.0-85032833918
"Jianwei L.","The design and implementation of building energy saving potential online diagnosis method based on data mining techniques",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032804147&doi=10.1109%2fICITBS.2016.41&partnerID=40&md5=5e2c5aed75237c219b4fe4a87df15a91","In modern architecture, advanced building automation system is widely used for automatic monitoring and control of all kinds of building systems, but the actual operation data of the building which is stored in the system is not fully utilized. Firstly, this paper analyzed the characteristics of data mining technology and its application in the field of architecture, and then according to the selection of 82 buildings each functional area and the typical day energy consumption data as a sample, and by using multiple regression analysis data mining method, a new method of building energy saving potential was designed, finally by fitting the sample data, the results show that the proposed method can provide a powerful tool and method for the construction of energy saving potential. © 2016 IEEE.","Building Energy Saving Potential; Data Mining; Online Diagnosis Method","Automation; Buildings; Data mining; Energy conservation; Energy utilization; Intelligent buildings; Regression analysis; Smart city; Building automation systems; Building energy saving; Data mining technology; Design and implementations; Energy consumption datum; Energy saving potential; Multiple regression analysis; On-line diagnosis; Big data",2-s2.0-85032804147
"Guang Z., Wang Y., Wang Q.","Enterprise financial audit modeling research based on data mining",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806712&doi=10.1109%2fICITBS.2016.77&partnerID=40&md5=3d78590a07731f131f66e037ef4fa239","In the daily audit work, informationization brings huge amounts of business data. Auditors face huge amounts of data, and personnel allocation has no planned way. In order to solve these practical problems, we introduce the data mining technology. Isolated point algorithm based on unit is applied to computer network audit. Boundary cell value of the isolated point algorithm is adjusted. By means of Benford law, we track data, find out abnormal data, and carry out preliminary data filtering. Then the outlier algorithm based on cell is used in calculation, and improve the accuracy and efficiency of finding isolated point. The experiment results show that improved algorithm achieves a good effect. © 2016 IEEE.","Accuracy; Data mining; Financial audit; Isolated point","Data mining; Smart city; Accuracy; Data mining technology; Financial audit; Informationization; Isolated point; Outlier algorithms; Point algorithms; Practical problems; Big data",2-s2.0-85032806712
"Li T., Tan W., Li X.","Data mining algorithm for correlation analysis of industrial alarms",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029599255&doi=10.1007%2fs10586-017-1170-3&partnerID=40&md5=d3a16a1aaeb5f153c5b374c4e86b2511","Alarm is important in industrial safety management. The technique to capture the correlation information of alarm variables, especially from historical alarm data, is very beneficial for risk prediction and prevention, but the task is very difficult because many alarm tags are associated with a single process variable and often alarm tags are tackled with any specific process variables. In this paper, a general weight-based multi-state sequential algorithm for correlation analysis is applied for alarm data to improve the validity and accuracy of alarm clustering combined with the traditional agglomerative hierarchical clustering algorithm. To make the direction between alarm variables, this paper proposes a vector correlation concept and use conditional probability to measure the alarm correlation among different tags comparable. The method breaks through the limitations of the traditional research on alarm variables correlation that distinguishes sequential alarms with non sequential alarms, or treats differently between regular and irregular alarms. Furthermore, a two-dimensional matrix is used to show the vector correlation of alarm variables intuitively and visually. The data mining algorithm is shown to be able to find out the vector correlation of alarm variables effectively and correctly when applied in the analysis of power plant alarm data. © 2017 Springer Science+Business Media, LLC","Alarm; Clustering; Similarity algorithm; Vector correlation","Accident prevention; Alarm systems; Cluster analysis; Correlation methods; Data mining; Risk management; Vectors; Agglomerative hierarchical clustering; Alarm; Clustering; Conditional probabilities; Correlation analysis; Data mining algorithm; Similarity algorithm; Vector correlations; Clustering algorithms",2-s2.0-85029599255
"Yanbin Y., Lijuan Z., Mengjun L., Ling S.","Early warning of traffic accident in Shanghai based on large data set mining",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032795761&doi=10.1109%2fICITBS.2016.149&partnerID=40&md5=ea4d932408072f223fd61a170fde8090","Through the classification and regression analysis on traffic accident statistics in Shanghai from July 2014 to April 2015, the paper puts forward a forecasting model of traffic accident incidences, by which we provides the index system of traffic accident, including month, week, weather and wind speed. Using this model to calculate the range of traffic accident simultaneously. Finally, making decisions and recommendations for controlling traffic accidents and rescue related based on analyzing safe levels, which has important guiding significance to the traffic accident prevention and traffic safety management in our country. © 2016 IEEE.","Data mining; traffic accident; regression analysis; incidence; safety levels Introduction","Accident prevention; Accidents; Data mining; Decision making; Highway accidents; Regression analysis; Smart city; Wind; Accident statistic; Forecasting modeling; Guiding significances; Index systems; Large datasets; Making decision; Safety level; Traffic safety; Big data",2-s2.0-85032795761
"Yan H., Hu H.","A study on association algorithm of smart campus mining platform based on big data",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032838923&doi=10.1109%2fICITBS.2016.11&partnerID=40&md5=62a378b72dcee8972078cc372fa8f9ff","In this thesis, jumping out of the traditional data statistic analysis method, the author constructs smart campus data platform and uses Apriori algorithm to make the association analysis on the learning and living data of students in campus in order to excavate their marks, basic information, attendance and states of internet use. In this way, the university can better guide the students to study based on the results of association analysis. © 2016 IEEE.","Apriori Association Algorithm; Big Data; Smart Campus","Data mining; Education; Smart city; Apriori algorithms; Association algorithms; Association analysis; Data platform; Data statistics; Internet use; Smart Campus; Big data",2-s2.0-85032838923
"Na C., Xin C.","The research of large scale data processing platform based on the spark",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032822662&doi=10.1109%2fICITBS.2016.36&partnerID=40&md5=56626110962d1736d93d0dd2360a0907","With the development of technologies of cloud computing and distributed cluster, the concept of big data was extended widely and deeply in volume and value, and data mining that plays an important role in exploring big data was attracted unprecedented attention in recent years. Traditional data mining algorithms is incapable to deal with massive dataset. MapReduce has been successfully applied in many big data problems, however, it lacks the ability to efficiently support paralyzed, iterative learning. To address the above problems, we give an integrated solution based on the Spark framework, not only process massive data efficiently, but also with a favorable scalability, which can satisfy the demand of many kinds of data mining tasks. Further we propose a framework applied in traffic field. © 2016 IEEE.","Data Mining; Massive dataset; Spark","Cluster computing; Data handling; Data mining; Distributed computer systems; Electric sparks; Iterative methods; Smart city; Data mining algorithm; Data mining tasks; Data problems; Distributed clusters; Integrated solutions; Iterative learning; Large-scale data processing; Massive dataset; Big data",2-s2.0-85032822662
"Yu L.","Application research of SVM-based mining algorithm in evaluation of college english teaching",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032823418&doi=10.1109%2fICITBS.2016.124&partnerID=40&md5=a8330f391996b905b19e709ce9ef80b3","It is a problem of obtaining fair, accurate and fast evaluation of teacher in education and teaching, which is also an important premise of modern management in colleges. There exist some disadvantages as being subjective, poor accuracy and complex operation in traditional schemes. We proposed an improved method by combining data mining algorithm and the evaluation indicators of English teachers. SVM is used to classify the sample data. Then we attain the training model through training the sample data in the evaluation system and take the intelligent evaluation and analysis on the prediction data with the training model. Our method is testified to have advantage in comprehensive performance and application value by experiments. © 2016 IEEE.","Data mining; English teaching; Evaluation; SVM","Data mining; Education; Smart city; Teaching; Application research; College english teachings; Comprehensive performance; Data mining algorithm; English teaching; Evaluation; Evaluation indicators; Intelligent evaluation; Big data",2-s2.0-85032823418
"Yin W.","Knowledge acquisition method for large-scale bilingual corpus based on web mining",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806552&doi=10.1109%2fICITBS.2016.76&partnerID=40&md5=db5174622215ed3f979103120689d177","This paper describes a method to acquire multi-word translational equivalences from English-Chinese parallel corpora based on Web mining. To solve the correspondence problem of multiple word, N-gram model is adopted to extract candidate translate units. Then the co-occurrence information is used to acquire subject words related to resource proper noun from search engine. The subject terms translation is adopted to perform language-crossed extension, and the extended query will obtain bilingual abstract resources with high quality from the search engine. We also extract the candidate translate units such as compound words and phrases, based on frequency change information and adjacency information, and make final selection of proper nouns integrated transliteration features, statistical features and template features. The experiments show that the translation mining method proposed in this paper has good performance. © 2016 IEEE.","Corpus; Knowledge acquisition; OOV; Translation","Data mining; Knowledge acquisition; Mining; Search engines; Smart city; Translation (languages); Bilingual corpora; Co-occurrence informations; Corpus; Correspondence problems; English-chinese parallel corpora; Statistical features; Translation minings; Translational equivalence; Big data",2-s2.0-85032806552
"Raafat H.M., Hossain M.S., Essa E., Elmougy S., Tolba A.S., Muhammad G., Ghoneim A.","Fog Intelligence for Real-time IoT Sensor Data Analytics",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030635995&doi=10.1109%2fACCESS.2017.2754538&partnerID=40&md5=de906b39991cff302705619759a79239","The evolution of the Internet of things (IoT) and the continuing increase in the number of sensors connected to the Internet impose big challenges regarding the management of the resulting deluge of data and network latency. Uploading sensor data over the web does not add value. Therefore, an efficient knowledge extraction technique is badly needed to reduce the amount of data transfer and to help simplify the process of knowledge management. Homoscedasticity and statistical features extraction are introduced in this paper as novelty detection enabling techniques, which help extract the important events in sensor data in real time when used with neural classifiers. Experiments have been conducted on a fog computing platform. System performance has been also evaluated on an occupancy dataset and showed promising results. OAPA","Cloud computing; Data mining; Data models; Edge computing; Feature extraction; Fog computing; Internet of Things (IoT); Levenes Test; Novelty detection; Real-time systems; sensor signals; Statistical Features","Cloud computing; Data mining; Data structures; Data transfer; Distributed computer systems; Extraction; Feature extraction; Fog; Information management; Interactive computer systems; Knowledge management; Real time systems; Edge computing; Internet of Things (IOT); Novelty detection; Sensor signals; Statistical features; Internet of things",2-s2.0-85030635995
"Liu S., Zhang J., Xiang Y., Zhou W.","Fuzzy-based Information Decomposition for Incomplete and Imbalanced Data Learning",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030749649&doi=10.1109%2fTFUZZ.2017.2754998&partnerID=40&md5=4ff532d7ab7aa672900a3debf665c127","Class imbalance and missing values are two critical problems in pattern classification. Researchers have proposed a number of techniques to address each of the problems. However, no single technique can solve the two problems. Moreover, the simple combination approach cannot accurately classify the imbalanced data with missing values. This paper develops a fuzzy-based information decomposition (FID) method to simultaneously address these two problems. In the new FID method, the two different problems are treated as the same missing data estimation problem. In particular, FID rebalances the training data by creating synthetic samples for the minority class. The proposed scheme has two steps: weighting and recovery. In the weighting step, the weights produced by the fuzzy membership functions are used to quantify the contribution of the observed data to the missing estimation. In the recovery step, missing values will be estimated by taking into account different contribution of the observed data. To evaluate the performance of the new FID method, a large number of classification experiments have been carried out on 27 well-known datasets. The results show that the FID method significantly outperforms other 10 state-of-the-art individual methods and 8 combination methods when missing values and imbalanced data present at the same time. IEEE","Australia; classification; Estimation; fuzzy logic; imbalanced data; incomplete data; Knowledge discovery; Learning systems; sampling; Sampling methods; Training; Training data","Classification (of information); Data mining; Estimation; Fuzzy logic; Learning systems; Membership functions; Personnel training; Sampling; Australia; Imbalanced data; Incomplete data; Sampling method; Training data; Problem solving",2-s2.0-85030749649
"Yu C.","Research on emergency logistics decision support system design under data ming & WebGIS technology",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032833741&doi=10.1109%2fICITBS.2016.150&partnerID=40&md5=328b899f416b322bf9c228f7a1e807e3","The modern enterprise production and operation management of the external macro environment and market demand in the fast dynamic changes and developments in emergency logistics needs all types of emergencies that caused a test of market which demand rapid response capability and logistics operational level. Firstly, the function of emergency logistics decision support system overview and summary of the proposed the emergency logistics decision support system should be able to the auxiliary logistics intelligent decision-making program generates and dynamic monitoring of emergency logistics system, followed by the data mining and WebGIS technology and to build a the emergency logistics decision support system model based on both the system model is composed of data layer, control layer, functional layer, and human-computer interaction layer. It is composed of four parts, the last of the two key system technology improvement and implementation. Solving the bottleneck restricting the operating efficiency of the system and the system the WebGIS layer output can effectively improve the emergency responses capability of the logistics system, perception and the ability to control costs. © 2016 IEEE.","Data Ming; Decision Support System; Design.; Emergency Logistics; WebGIS","Artificial intelligence; Big data; Commerce; Computer control systems; Data mining; Decision making; Design; Human computer interaction; Logistics; Smart city; Data Ming; Emergency logistics; Emergency logistics systems; Enterprise production; Intelligent decision making; Operating efficiency; Technology improvement; Web-GIS; Decision support systems",2-s2.0-85032833741
"Yair O., Talmon R., Coifman R.R., Kevrekidis I.G.","Reconstruction of normal forms by learning informed observation geometries from data",2017,"Proceedings of the National Academy of Sciences of the United States of America",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029549395&doi=10.1073%2fpnas.1620045114&partnerID=40&md5=d6c0dc587915df880916e20efe11565e","The discovery of physical laws consistent with empirical observations is at the heart of (applied) science and engineering. These laws typically take the form of nonlinear differential equations depending on parameters; dynamical systems theory provides, through the appropriate normal forms, an “intrinsic” prototypical characterization of the types of dynamical regimes accessible to a given model. Using an implementation of data-informed geometry learning, we directly reconstruct the relevant “normal forms”: a quantitative mapping from empirical observations to prototypical realizations of the underlying dynamics. Interestingly, the state variables and the parameters of these realizations are inferred from the empirical observations; without prior knowledge or understanding, they parametrize the dynamics intrinsically without explicit reference to fundamental physical quantities. © 2017, National Academy of Sciences. All rights reserved.","Data analysis; Dynamical systems; Empirical models; Geometry; Graph theory","algorithm; Article; data analysis; data mining; data processing; empirical research; geometry; mathematical model; nonlinear system; observation; priority journal; quantitative analysis; systems theory",2-s2.0-85029549395
"Thilagavathy R., Sabitha R.","Using cloud effectively in concept based text mining using grey wolf self organizing feature map",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572201&doi=10.1007%2fs10586-017-1159-y&partnerID=40&md5=a7b15e998d580dc9e01226a032c04aec","Cloud computing is considered to be an integral aspect in all business and this is expected to change the information technology (IT) landscape. This has been based on the model that delivers services on the internet using the pay-as-you go model that has several advantages like the no up-front cost, a lower IT staff, and a lower operation cost. A technology that is made use of for retrieval of data from huge database is known as text mining. This is used by cloud for efficiently retrieving data from the data centres of cloud. In providing navigation as well as mechanisms for browsing intuitively, text document clustering has an important role. This is done by organizing huge amounts of information into smaller number of clusters. Bag of words (BoW) is a representation that is used for the clustering of these methods but in many case it is not satisfactory as relations that exist between terms that don’t co-occur are ignored. To handle this problem a document level and sentence level integration of the concepts is made. This increases the space of the feature vector and also brings down the clustering algorithm’s efficiency. In order to overcome this a self-organizing feature map (SOFM) based algorithm makes use of the concepts of genetic algorithm (GA) along with grey wolf optimization (GWO) which are considered popular in the SOFM. The goal of the SOFM-GA is to find an optimal topology of network (the number of neurons and their array dimension) along with an optimal training parameter like the scheduling of learning rate and the annealing of neighborhood width. The SOFM-GWO and the GWO-based approach to the formation of SOFM are compared with the SOM standard relating to quality and the weights and map generated. The results of the experiment show that this method achieved better results. © 2017 Springer Science+Business Media, LLC","Clustering; Concept based mining; Organizing feature map algorithm (SOFM) and Grey wolf optimization (GWO); Text mining","Classifiers; Conformal mapping; Data mining; Genetic algorithms; Information retrieval; Optimization; Self organizing maps; Text processing; Vector spaces; Array dimensions; Clustering; Concept-based; Feature map; Number of clusters; Optimal topologies; Text Document Clustering; Text mining; Clustering algorithms",2-s2.0-85029572201
"Hu H., Yan H.","A study on discovery method of hot topics based on smart campus big data platform",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032822803&doi=10.1109%2fICITBS.2016.10&partnerID=40&md5=8dec2d09efc2ee39527db594f93cc0c7","To study and pose an modified discovery method for campus hot topics. The method is to extract word segmentation and keywords from the campus news collected on information big data platform through ICTCLAS word segmentation system, construct the text knowledge representation model based on vector space modal and finally add up the word frequency of headlines to determine k initial clustering centers and improve K-Means algorithm so as to obtain the campus hot topics. © 2016 IEEE.","Big Data; Discovery Method of Hot Topics; K-Means algorithm; Smart Campus","Computational linguistics; Data mining; Knowledge representation; Smart city; Vector spaces; Hot topics; Initial clustering centers; k-Means algorithm; Smart Campus; Vector space modals; Word frequencies; Word segmentation; Word segmentation systems; Big data",2-s2.0-85032822803
"Jing S., Ho Z.-P., Niu Z.","A term mining approach of interview case study on enterprise lean production",2017,"Total Quality Management and Business Excellence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012935194&doi=10.1080%2f14783363.2017.1289084&partnerID=40&md5=ac700b8c6c7e9acdff8bc7611be1b0e9","Lean production improves the capacity of enterprise management innovation (EMI). When enterprises implement lean production, companies need internal and external incentives to drive this forward. In order to make internal and external incentives cooperate, promote the successful implementation of lean production, and improve the capacity of EMI, analysing the driving factors of lean production for internal and external incentives is important. This study adapted enterprise case study method, focused on interviews of different levels of enterprise personnel, and tried to find the key driving factors for internal and external incentives in lean production. In this study, a term mining approach was used to analyse interview texts. After the interview of case study, the experiments proceeded on the basis of the proposed model and approaches. Then interview case texts were analysed by text-mining techniques; this study found important clues in enterprise lean production, which were described as follows: (1) Enterprises should improve their ability in management innovation via completing their objectives of lean production, both in internal and external environments; (2) when an enterprise completes lean operations, the enterprise needs some internal and external incentives to drive their actions; and (3) motivators for different levels of enterprise personnel, such as senior managers, middle managers, junior managers, and basic production personnel and lean experts, were different. The enterprise should operate the different roles in each level of personnel for each enterprise management and innovation mission. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","case study; data mining; enterprise management innovation; lean production; terms mining approach",,2-s2.0-85012935194
"Fan H., Lu C., Chen H.","Exploring the spatial agglomeration characteristics of cigarette brand sales in Guizhou province, China",2017,"2017 6th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032814268&doi=10.1109%2fAgro-Geoinformatics.2017.8047048&partnerID=40&md5=c950a8603dc900a52f5d4044734d32e2","Spatial clustering analysis is one kind of spatial data mining tools to explore the spatial auto-correlation of things that occur in a particular space, that is, whether the observed values of the spatial variables are related to the spatial position where they occur. In this paper, the cigarettes in 2013 in Guizhou province, China were collected, and the spatial correlation analysis was carried out on the five categories of cigarettes sales in county-level area in this Province. Some valuable spatial aggregation and distribution characteristic of Guizhou cigarette sales were mined out. Experiments and experimental results show that the kind of spatial pattern diming tool is a potential tool for providing valuable spatial reference for Cigarette sales policy development and the progressive realization of the precise cigarettes marketing in this province. © 2017 IEEE.","cigarette precision marketing; Guizhou province; spatial auto-correlation; spatial distribution pattern","Commerce; Data mining; Marketing; Sales; Tobacco; Distribution characteristics; Guizhou Province; Precision marketings; Spatial aggregation; Spatial clustering; Spatial correlation analysis; Spatial data mining; Spatial distribution patterns; Spatial variables measurement",2-s2.0-85032814268
"Zhao H., Liu H., Ding Z., Fu Y.","Consensus Regularized Multi-View Outlier Detection",2017,"IEEE Transactions on Image Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030708882&doi=10.1109%2fTIP.2017.2754942&partnerID=40&md5=fe54ea99f4b53621b0c7088a25db9e8b","Identifying different types of data outliers with abnormal behaviors in multi-view data setting is challenging due to the complicated data distributions across different views. Conventional approaches achieve this by learning a new latent feature representation with the pairwise constraint on different view data. In this paper, we argue that the existing methods are expensive in generalizing their models from two-view data to three-view (or more) data, in terms of the number of introduced variables and detection performance. To address this, we propose a novel multi-view outlier detection method with a consensus regularization on the latent representations. Specifically, we explicitly characterize each kind of outliers by the intrinsic cluster assignment labels and sample-specific errors. Moreover, we make a thorough discussion about the proposed consensusregularization and the pairwise-regularization. Correspondingly, an optimization solution based on augmented Lagrangian multiplier method is proposed and derived in details. In the experiments, we evaluate our method on five well-known machine learning datasets with different outlier settings. Further, to show its effectiveness in real-world computer vision scenario, we tailor our proposed model to saliency detection and face reconstruction applications. The extensive results of both standard multi-view outlier detection task and the extended computer vision tasks demonstrate the effectiveness of our proposed method. IEEE","Anomaly detection; Computational modeling; Computer vision; consensus regularization; Data models; Face; Feature extraction; multi-view learning; Optimization; outlier detection","Computer vision; Constrained optimization; Data handling; Data structures; Feature extraction; Lagrange multipliers; Learning systems; Optimization; Statistics; Anomaly detection; Computational model; consensus regularization; Face; Multi-view learning; Outlier Detection; Data mining",2-s2.0-85030708882
"Rong F.","Design of tourism resources management based on artificial intelligence",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032814703&doi=10.1109%2fICITBS.2016.97&partnerID=40&md5=6a141fc3c152df27f3f3fbc809d396cb","In the era of Web 2.0, it provides great chances for users to obtain rich information. Particularly, tourists want to easily get tremendous information about their tourism plans. Therefore, in this paper, we aim to design a tourism resources management system and guide the tourists to plan their travel routes. The proposed tourism resources management system contains information organization and filtering module and contents module. Information organization and filtering module includes three function modules: RSS filtering, Collaborative filtering, and Tag filtering. On the other hand, contents module contains various types of data sources, such as Blog, Microblog, Social bookmark, Wiki, and SNS. The main innovation of this paper is to introduce artificial intelligence technique in tourism resources management, that is, we proposed a novel heterogeneous Web data extraction method to generate tourism website contents. Finally, experimental results prove that the proposed tourism resources management system is able to effectively integrate tourism information and provide high quality information service for tourists. © 2016 IEEE.","Artificial Intelligence; Heterogeneous Web data extraction; Hidden conditional random fields; Tourism resources management; Web 2.0","Artificial intelligence; Big data; Collaborative filtering; Data mining; Extraction; Information filtering; Information services; Smart city; Websites; Artificial intelligence techniques; Hidden conditional random fields; High quality information; Information organization; Resources management; Tourism websites; Web 2.0; Web data extraction; Information management",2-s2.0-85032814703
"Dong Y., Li W., Yu H.","Intelligence extraction method of domain terms for Chinese web documents based on hierarchical combination strategy",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032804511&doi=10.1109%2fICITBS.2016.57&partnerID=40&md5=790e832e79abd3732ef7e730057eaef7","Domain terms extraction based on Chinese Web document is an important step in the field of Chinese text information into machine recognition, and it is also the technical basis of intelligence information processing in domain ontology construction, text knowledge mining, and etc. The traditional methods of terms extracting are: Based on the dictionary, based on the rules and the statistical method. But each of methods contains some limitations, such as in single word processing, synonyms merging and so on. In order to solve these problems, the intelligence extraction method based on hierarchical combination strategy is proposed. It is divided into three layers: The first layer is the document preprocessing layer, the second layer is the words preparation layer, and the third layer is the term extraction layer. The effectiveness of this method is verified by experimental data in the field of weapon and equipment. Comparing with the traditional method, it is proved that this method has good accuracy and recall rate for the domain terms extraction based on the Chinese Web documents. © 2016 IEEE.","Intelligence information processing Introduction; Layering Combination Strategy; Term extraction; Web text mining","Big data; Character recognition; Extraction; Ontology; Smart city; Text processing; Word processing; Combination strategies; Document pre-processing; Domain ontology constructions; Intelligence information processing Introduction; Machine recognition; Term extraction; Weapon and equipments; Web text mining; Data mining",2-s2.0-85032804511
"Li Q.","Mobile user network behavior analysis based on improved fuzzy c-means clustering",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032835303&doi=10.1109%2fICITBS.2016.81&partnerID=40&md5=9405cb6fe075d5cbfb09fbadaa3df83f","Because the traditional mobile user network behavior analysis lacks of data mining and analysis means, which leads to corporate market and user demand separated. So, we propose an integrated approach and find an effective solution based on fuzzy c-means to analyze the mobile user network behavior. Because FCM algorithm is easy to fall into local minimum value and is sensitive to the initial value, we propose a kind of improved fuzzy c-means clustering algorithm based on artificial fireflies. The artificial firefly with global optimization ability is used to obtain the optimal solution, which is taken as the initial clustering center of FCM algorithm. The experiment results show that the proposed scheme has good result. © 2016 IEEE.","Artificial firefly; Fuzzy c-means; Network behavior analysis","Behavioral research; Big data; Bioluminescence; Data mining; Fire protection; Fuzzy clustering; Fuzzy systems; Global optimization; Optimization; Smart city; Artificial firefly; Effective solution; Fuzzy C mean; Improved fuzzy c-means clustering; Initial clustering centers; Integrated approach; Network behavior analysis; Optimization ability; Clustering algorithms",2-s2.0-85032835303
"Kang M., Lee J.-G.","An experimental analysis of limitations of MapReduce for iterative algorithms on Spark",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029577286&doi=10.1007%2fs10586-017-1167-y&partnerID=40&md5=258c85cb6a0228e22387e0ded0ff7e02","MapReduce is the most popular framework for distributed processing. Recently, the scalability of data mining and machine learning algorithms has significantly improved with help from MapReduce. However, MapReduce does not handle iterative algorithms very efficiently. The problem is that many data mining and machine learning algorithms are iterative by nature. In order to overcome the limitations of MapReduce, many advanced distributed systems have been developed, including HaLoop, iMapReduce, Twister, and Spark. In this paper, we identify and categorize the limitations of MapReduce in handling iterative algorithms, and then, experimentally investigate the consequences of these limitations by using the most flexible and stable distributed system, Spark. According to our experiment results, the network I/O overhead was the primary factor that affected system performance the most. The disk I/O overhead also affected system performance, but it was less significant than the network I/O overhead. For the synchronization overhead, it affected system performance only when the static data was not cached. © 2017 Springer Science+Business Media, LLC","Hadoop; HaLoop; iMapReduce; Iterative algorithms; Spark; Twister","Artificial intelligence; Data mining; Electric sparks; Iterative methods; Learning systems; Hadoop; HaLoop; iMapReduce; Iterative algorithm; Twister; Learning algorithms",2-s2.0-85029577286
"Lesnik K.L., Liu H.","Predicting Microbial Fuel Cell Biofilm Communities and Bioreactor Performance using Artificial Neural Networks",2017,"Environmental Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029697414&doi=10.1021%2facs.est.7b01413&partnerID=40&md5=7703f330d717121562f96f4cfa6487c4","The complex interactions that occur in mixed-species bioelectrochemical reactors, like microbial fuel cells (MFCs), make accurate predictions of performance outcomes under untested conditions difficult. While direct correlations between any individual waste stream characteristic or microbial community structure and reactor performance have not been able to be directly established, the increase in sequencing data and readily available computational power enables the development of alternate approaches. In the current study, 33 MFCs were evaluated under a range of conditions including eight separate substrates and three different wastewaters. Artificial Neural Networks (ANNs) were used to establish mathematical relationships between wastewater/solution characteristics, biofilm communities, and reactor performance. ANN models that incorporated biotic interactions predicted reactor performance outcomes more accurately than those that did not. The average percent error of power density predictions was 16.01 ± 4.35%, while the average percent error of Coulombic efficiency and COD removal rate predictions were 1.77 ± 0.57% and 4.07 ± 1.06%, respectively. Predictions of power density improved to within 5.76 ± 3.16% percent error through classifying taxonomic data at the family versus class level. Results suggest that the microbial communities and performance of bioelectrochemical systems can be accurately predicted using data-mining, machine-learning techniques. © 2017 American Chemical Society.",,"Biofilms; Data mining; Errors; Forecasting; Fuel cells; Learning systems; Microorganisms; Neural networks; Bio-electrochemical reactors; Bio-electrochemical systems; Bioreactor performance; Machine learning techniques; Mathematical relationship; Microbial communities; Microbial community structures; Microbial fuel cells (MFCs); Microbial fuel cells; antimicrobial activity; artificial neural network; biofilm; bioreactor; community structure; electrochemical method; fuel cell; microbial community; performance assessment; prediction; amplicon; Article; artificial neural network; biofilm; bioreactor; community structure; data mining; machine learning; microbial community; microbial fuel cell; nonhuman; prediction; reactor operation; taxonomy; waste water",2-s2.0-85029697414
"Na Y., Fengyue S., Lin K., Yujin L., Bingjie Y.","Resource management and scheduling optimization of open pit mining area based on computer aided system",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032784956&doi=10.1109%2fICITBS.2016.100&partnerID=40&md5=26ae92753de9df30fcf8dcbf2dc10754","Production scheduling system of open pit mine is the key process of mining production, and the optimization effect of the production scheduling system of mineral resources has direct impact on the production efficiency and production efficiency of open pit mining. Based on the actual problems in mining producing, this article adopts multi-objective optimization to propose a optimized model for tramcar transport dispatching organization. Then a FLSPT rapid algorithm is used to put forward a shortest path algorithm for resource scheduling, which can obtain the shortest path between two points. Then an optimal planning algorithm of open pit truck and shovels are proposed to provide real-Time scheduling and maximize the mine production resources. The case analysis shows that our method has the advantages of high precision and high efficiency, and it is easy to be implemented, which also shows practical value. © 2016 IEEE.","Distribution; Open pit mining; Path optimization; Position; Scheduling","Big data; Computer resource management; Efficiency; Graph theory; Mineral resources; Multiobjective optimization; Optimization; Production control; Scheduling; Scheduling algorithms; Smart city; Computer-aided systems; Distribution; Optimal planning algorithms; Path optimizations; Position; Production scheduling system; Resource management and scheduling; Shortest path algorithms; Open pit mining",2-s2.0-85032784956
"Calefato F., Lanubile F., Maiorano F., Novielli N.","Sentiment Polarity Detection for Software Development",2017,"Empirical Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029604461&doi=10.1007%2fs10664-017-9546-9&partnerID=40&md5=b13c0262746fbedbe7467cc60773a732","The role of sentiment analysis is increasingly emerging to study software developers’ emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers’ communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines. © 2017 Springer Science+Business Media, LLC","Communication Channels; Sentiment Analysis; Social Software Engineering; Stack Overflow; Word Embedding","Communication channels (information theory); Data mining; File editors; Gold; Semantics; Software engineering; Misclassifications; Semantic features; Sentiment analysis; Social software engineering; Software developer; Stack overflow; Technical jargon; Word Embedding; Software design",2-s2.0-85029604461
"Wang Q., Mao Z., Wang B., Guo L.","Knowledge Graph Embedding: A Survey of Approaches and Applications",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030627463&doi=10.1109%2fTKDE.2017.2754499&partnerID=40&md5=a27db194a2cc66996bacec79c674fa33","Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth. IEEE","Knowledge discovery; knowledge graph embedding; latent factor models; Market research; Matrix decomposition; Semantics; Statistical relational learning; Systematics; Tensile stress; tensor/matrix factorization models; Training","Data mining; Extraction; Personnel training; Semantics; Tensile stress; Factorization model; Knowledge graphs; Latent factor models; Market researches; Matrix decomposition; Statistical relational learning; Systematics; Vector spaces",2-s2.0-85030627463
"Shuai H., Yinghua Z., Yukun G., Zhian H.","Study on 3D tunnel modeling based on DXF",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032796456&doi=10.1109%2fICITBS.2016.39&partnerID=40&md5=f145d20d2e75424a97bd8f013395c488","Building 3D tunnel model is one of the important work in underground mining management, the methods to build 3D tunnel model by secondary development of commercial software or using OpenGL are not convenient and efficient. After analyzing the structure of DXF file, a method that building 3D tunnel model based on DXF file is proposed. On the basis of designing the tunnel section type and parameters, the modeling process is given in details. The result shows the method has a good efficiency and more convenient to realize. © 2016 IEEE.","3D model; DXF format; Tunnel","Application programming interfaces (API); Smart city; Tunnels; 3-d modeling; Commercial software; DXF format; Modeling process; Secondary development; Tunnel model; Tunnel sections; Underground mining; Big data",2-s2.0-85032796456
[No author name available],"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",2017,"Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032788497&partnerID=40&md5=81e0d3f7da6a41f27e4d4b02381f3797","The proceedings contain 141 papers. The topics discussed include: a fast path planning algorithm research based on region-partition; analysis on key technologies of traffic prediction and path guidance in intelligent transportation; current situation of energy saving and emission reduction of urban public transportation and green development suggestion; early warning of traffic accident in shanghai based on large data set mining; simulation analysis of train speed measurement technology based on urban rail transit system; the application of sensing technology to internet of vehicles; the assessment system of residential community open-up strategy for intelligent transportation system; a novel feature extraction method for hyperspectral image classification; and a study of collision detection algorithm based on cloud computing model.",,,2-s2.0-85032788497
"Jiang Z., Liu S., Malekian R.","Analysis of a Whole-Space Transient Electromagnetic Field in 2.5-Dimensional FDTD Geoelectric Modeling",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640066&doi=10.1109%2fACCESS.2017.2754521&partnerID=40&md5=ba60973d4aa1d2a858d6162c5501dc98","Mine water inrush poses a serious threat to the safe production of coal mines in China. The transient electromagnetic method (TEM) on the ground has been applied to explore water-bearing structures, but the resolution is low. Therefore, some geophysicists in China moved the TEM onto underground coal mine roadways and obtained good results at the end of the last century. Although the TEM has been applied in mining for many years, there are so few theoretical studies that the data interpretation is not accurate. It is necessary to study the transient electromagnetic field diffusion in the entire space with physical or numerical simulation methods. First, based on the diffusion equations, we deduced the wave number domain equations, whose whole-space electromagnetic field is excited by a 3-D source in a 2-D geoelectric model; then, we derived the 2.5-D finite-difference time domain equations. At the beginning of the calculation, we gave the grid nodes near the source the initial values with the cosine filtering method. To improve the calculating efficiency, the time intervals gradually increased with time. At the end of the calculation, we transformed the calculating results from the wave number domain to the space domain by fitting the segmented exponential function. Compared with the analytical solutions, the numerical solutions are accurate, and the algorithm is reliable and efficient. The simulation results of a collapse-column model show that the transient electromagnetic field diffusion in the entire space is dominated by low-resistivity bodies. © 2013 IEEE.","Finite-difference time domain method; geoelectric model; transient electromagnetic field; wave number domain; whole-space","Coal; Coal mines; Diffusion; Electromagnetic field effects; Electromagnetic fields; Exponential functions; Finite difference method; Finite difference time domain method; Groundwater; Mathematical models; Numerical methods; Numerical models; Transient analysis; Coal mining; Geoelectric models; Transient electromagnetic fields; Wave number domain; whole-space; Time domain analysis",2-s2.0-85030640066
"Örestig J., Lindgren S.","Local Moral Economies: The Space, Place, and Locality of Social Media Mobilisation",2017,"Globalizations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013127623&doi=10.1080%2f14747731.2017.1286175&partnerID=40&md5=05d0c8d815e0dd504607d5b59bf0517e","This study is a case study of a locally rooted environmental campaign on the Swedish island of Gotland. We aim to enhance the understanding of how locality is manifested in social movements that emerge in today’s networked world. We analyse how the double goals of speaking to, as well as beyond, the local context came into expression in the movement’s social media activities. We draw on data from tweets and Facebook posts and include interactions between activists and critics as well as the resources linked to in the posts. Analysis indicates that the conflict must be seen as spanning across local, national and global levels. In line with earlier research, activists used social media to link their struggle with other struggles. Also, it was used to charge the local struggle with symbolic content by framing it as one of many struggles between local communities, authorities and multinational corporations. Beyond this, posts from the island signalled dedication to the history and long-term interests of the community. We argue that future studies should recognise the crucial role that reciprocity norms in the local community can play for outcomes of conflicts and that the notion of a ‘local moral economy’ can be used to reach a deeper understanding of this. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","activism; mining struggles; moral economy; social media",,2-s2.0-85013127623
"Eppelbaum L.","Quantitative examination of piezoelectric/seismoelectric anomalies from near-surface targets",2017,"Geosciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030695836&doi=10.3390%2fgeosciences7030090&partnerID=40&md5=f6ba1d05df8136dc00e5a1460bb3bda2","The piezoelectric and seismo-electrokinetic phenomena are manifested by electrical and electromagnetic processes that occur in rocks under the influence of elastic oscillations triggered by shots or mechanical impacts. Differences in piezoelectric properties between the studied targets and host media determine the possibilities of the piezoelectric/seismoelectric method application. Over a long time, an interpretation of obtained data is carried out by the use of methods developed in seismic prospecting. Examination of nature of piezoelectric/seismoelectric anomalies observed in subsurface indicates that these may be related (mainly) to electric potential field. In this paper, it is shown that quantitative analysis of piezoelectric/seismoelectric anomalies may be performed by the advanced and reliable methodologies developed in magnetic prospecting. Some examples from mining geophysics (Russia) and ancient metallurgical site (Israel) confirm applicability of the suggested approach. © 2017 by the author. Licensee MDPI, Basel, Switzerland.","Archaeology; Interpretation methodology; Piezoelectric/seismoelectric anomalies; Quantitative analysis; Subsurface geophysics",,2-s2.0-85030695836
"Parmar K., Dafale N., Pal R., Tikariha H., Purohit H.","An Insight into Phage Diversity at Environmental Habitats using Comparative Metagenomics Approach",2017,"Current Microbiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572046&doi=10.1007%2fs00284-017-1357-0&partnerID=40&md5=e4c239e47252bec3c8df84ccb3d2c5e5","Bacteriophages play significant role in driving microbial diversity; however, little is known about the diversity of phages in different ecosystems. A dynamic predator–prey mechanism called “kill the winner” suggests the elimination of most active bacterial populations through phages. Thus, interaction between phage and host has an effect on the composition of microbial communities in ecosystems. In this study, secondary phage metagenome data from aquatic habitats: wastewater treatment plant (WWTP), fresh, marine, and hot water spring habitat were analyzed using MG-RAST and STAMP tools to explore the diversity of the viruses. Differential relative abundance of phage families—Siphoviridae (34%) and Myoviridae (26%) in WWTP, Myoviridae (30%) and Podoviridae (23%) in fresh water, and Myoviridae (41%) and Podoviridae (8%) in marine—was found to be a discriminating factor among four habitats while Rudiviridae (9%), Globuloviridae (8%), and Lipothrixviridae (1%) were exclusively observed in hot water spring. Subsequently, at genera level, Bpp-1-likevirus, Chlorovirus, and T4-likevirus were found abundant in WWTP, fresh, and marine habitat, respectively. PCA analysis revealed completely disparate composition of phage in hot water spring from other three ecosystems. Similar analysis of relative abundance of functional features corroborated observations from taxa analysis. Functional features corresponding to phagepackaging machinery, replication, integration and excision, and genetransfer discriminated among four habitats. The comparative metagenomics approach exhibited genetically distinct phage communities among four habitats. Results revealed that selective distribution of phage communities would help in understanding the role of phages in food chains, nutrient cycling, and microbial ecology. Study of specific phages would also help in controlling environmental pathogens including MDR bacterial populations using phage therapy approach by selective mining and isolation of phages against specific pathogens persisting in a given environment. © 2017 Springer Science+Business Media, LLC",,,2-s2.0-85029572046
"Park S.H., Synn J., Kwon O.H., Sung Y.","Apriori-based text mining method for the advancement of the transportation management plan in expressway work zones",2017,"Journal of Supercomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572442&doi=10.1007%2fs11227-017-2142-3&partnerID=40&md5=309a0575b310e25d06137157d12847c0","This study contributes to knowledge by advancing the transportation management plan (TMP) development efforts for expressway work zones. Using text mining techniques to a large-scale transportation data set that contains descriptively narrated texts, this research analyzes the association between words related to the type of work being performed and the type of lane closure in expressway work zone areas. It found that recurrent everyday tasks and bridge repair works tend to cause shoulder lane closure, while works—such as tunnel repair, night work, pavement, median barrier, road surface repair, and line marking—are more associated with main lane closure. Moreover, the findings further clarify the characteristic patterns shared between the number of closed lanes, and the respective lane position in two- and three-lane expressways. These offer significant insights into the decision-making process for the development of work zone TMPs, which can further be integrated into the various components of TMP to make the plan more effective and, at the same time, ensure an efficient throughput flow throughout the work zone, reduced congestion, and improved safety. © 2017 Springer Science+Business Media, LLC","Association analysis; Big data; Text mining; Transportation management plan; Work zone","Big data; Decision making; Repair; Road and street markings; Shoulders (road); Thermomechanical pulping process; Association analysis; Association between words; Decision making process; Large-scale transportation; Text mining; Text mining techniques; Transportation management; Work zones; Data mining",2-s2.0-85029572442
"Segatori A., Bechini A., Ducange P., Marcelloni F.","A Distributed Fuzzy Associative Classifier for Big Data",2017,"IEEE Transactions on Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030642308&doi=10.1109%2fTCYB.2017.2748225&partnerID=40&md5=ed2382f27543fefc1c9e9b0863587a54","Fuzzy associative classification has not been widely analyzed in the literature, although associative classifiers (ACs) have proved to be very effective in different real domain applications. The main reason is that learning fuzzy ACs is a very heavy task, especially when dealing with large datasets. To overcome this drawback, in this paper, we propose an efficient distributed fuzzy associative classification approach based on the MapReduce paradigm. The approach exploits a novel distributed discretizer based on fuzzy entropy for efficiently generating fuzzy partitions of the attributes. Then, a set of candidate fuzzy association rules is generated by employing a distributed fuzzy extension of the well-known FP-Growth algorithm. Finally, this set is pruned by using three purposely adapted types of pruning. We implemented our approach on the popular Hadoop framework. Hadoop allows distributing storage and processing of very large data sets on computer clusters built from commodity hardware. We have performed an extensive experimentation and a detailed analysis of the results using six very large datasets with up to 11,000,000 instances. We have also experimented different types of reasoning methods. Focusing on accuracy, model complexity, computation time, and scalability, we compare the results achieved by our approach with those obtained by two distributed nonfuzzy ACs recently proposed in the literature. We highlight that, although the accuracies result to be comparable, the complexity, evaluated in terms of number of rules, of the classifiers generated by the fuzzy distributed approach is lower than the one of the nonfuzzy classifiers. IEEE","Associative classifier (AC); Automobiles; Big Data; big data; Complexity theory; Computational modeling; Data mining; fuzzy AC (FAC); fuzzy FP-Growth; Hadoop; Itemsets; MapReduce.; Programming","Association rules; Automobiles; Computer hardware; Data mining; Digital storage; Mathematical programming; Associative classifiers; Complexity theory; Computational model; FP growths; fuzzy AC (FAC); Hadoop; Item sets; Map-reduce; Big data",2-s2.0-85030642308
"Yulianti E., Chen R., Scholer F., Croft B., Sanderson M.","Document Summarization for Answering Non-Factoid Queries",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030629423&doi=10.1109%2fTKDE.2017.2754373&partnerID=40&md5=3c7d213e39f2e4fa52dcc52f16eb1e65","We formulate a document summarization method to extract passage-level answers for non-factoid queries, referred as answer-biased summaries. We propose to use external information from related Community Question Answering (CQA) content to better identify answer bearing sentences. Three optimization-based methods are proposed: (i) query-biased; (ii) CQA-answer-biased; and (iii) expanded-query-biased, where expansion terms were derived from related CQA content. A learning-to-rank-based method is also proposed that incorporates features extracted from related CQA content. Our results show that even if a CQA answer does not contain a perfect answer to a query, their content can be exploited to improve the extraction of answer-biased summaries from other corpora. The quality of CQA content is found to impact on the accuracy of optimization-based summaries, though medium quality answers enable the system to achieve a comparable (and in some cases superior) accuracy to state-of-the-art techniques. The learning-to-rank-based summaries, on the other hand, are not significantly influenced by CQA quality. We provide a recommendation of the best use of our proposed approaches in regard to the availability of different quality levels of related CQA content. As a further investigation, the reliability of our approaches was tested on another publicly available dataset. IEEE","answer-biased summaries; CQA; Data mining; document summarization; Feature extraction; Google; Knowledge discovery; learning-to-rank; non-factoid queries; Optimization; optimization; Search engines; Web search","Data mining; Extraction; Feature extraction; Natural language processing systems; Optimization; Search engines; answer-biased summaries; Document summarization; Google; Learning to rank; non-factoid queries; Web searches; Query processing",2-s2.0-85030629423
"Wang P., Wu Q., Shen C., Dick A., Hengel A.V.D.","FVQA: Fact-based Visual Question Answering",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030653985&doi=10.1109%2fTPAMI.2017.2754246&partnerID=40&md5=b7146e2980bbe299cbfe0ef52861ec3f","Visual Question Answering (VQA) has attracted much attention in both computer vision and natural language processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, and the models built upon them, have focused on questions which are answerable by direct analysis of the question and image alone. The set of such questions that require no external information to answer is interesting, but very limited. It excludes questions which require common sense, or basic factual knowledge to answer, for example. Here we introduce FVQA (Fact-based VQA), a VQA dataset which requires, and supports, much deeper reasoning. FVQA primarily contains questions that require external information to answer. We thus extend a conventional visual question answering dataset, which contains image-question-answer triplets, through additional image-question-answer-supporting fact tuples. Each supporting-fact is represented as a structural triplet, such as &lt;Cat,CapableOf,ClimbingTrees&#x003E;. IEEE","Cognition; Knowledge Base; Knowledge based systems; Knowledge discovery; Natural languages; Recurrent neural networks; Recurrent Neural Networks; Training; Visual Question Answering; Visualization","Data mining; Flow visualization; Knowledge based systems; Natural language processing systems; Personnel training; Recurrent neural networks; Cognition; Direct analysis; External informations; Factual knowledge; Knowledge base; Natural languages; Question Answering; Sources of informations; Visual languages",2-s2.0-85030653985
"Masum Z.H.","Mining stock category association on Tehran stock market",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029598527&doi=10.1007%2fs00500-017-2835-9&partnerID=40&md5=c420787c70426d4f41a2195f95fd2da8","Following the recent efforts made to achieve a predictable capital market, this study attempted to explore the interlocking relationships between the stock returns of companies listed on Tehran stock exchange (TSE). For that purpose, data concerning 36 industry classes between 2000 and 2013 were examined through clustering and association rule. Preparation and initial refining of data suggested that only 25 out of 36 industries met the requirement for 13-year membership at TSE. Finally, a total of 249,061 records were evaluated, and the results were presented in the form of several rules and recommendations for investors. The results suggested that there were no two-item rules (rules with one antecedent) within industries. The best rules entailed three and four items with a lift of more than two, confidence more than 81% and support more than 1%. © 2017 Springer-Verlag GmbH Germany","Association rule; Clustering; Stock return","Association rules; Commerce; Financial markets; Clustering; Stock returns; Tehran stock exchanges; Investments",2-s2.0-85029598527
"McIntosh L.D., Juehne A., Vitale C.R.H., Liu X., Alcoser R., Lukas J.C., Evanoff B.","Repeat: A framework to assess empirical reproducibility in biomedical research",2017,"BMC Medical Research Methodology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029593391&doi=10.1186%2fs12874-017-0377-6&partnerID=40&md5=0870a512c447b7b3468ae82f5a8267b8","Background: The reproducibility of research is essential to rigorous science, yet significant concerns of the reliability and verifiability of biomedical research have been recently highlighted. Ongoing efforts across several domains of science and policy are working to clarify the fundamental characteristics of reproducibility and to enhance the transparency and accessibility of research. Methods: The aim of the proceeding work is to develop an assessment tool operationalizing key concepts of research transparency in the biomedical domain, specifically for secondary biomedical data research using electronic health record data. The tool (RepeAT) was developed through a multi-phase process that involved coding and extracting recommendations and practices for improving reproducibility from publications and reports across the biomedical and statistical sciences, field testing the instrument, and refining variables. Results: RepeAT includes 119 unique variables grouped into five categories (research design and aim, database and data collection methods, data mining and data cleaning, data analysis, data sharing and documentation). Preliminary results in manually processing 40 scientific manuscripts indicate components of the proposed framework with strong inter-rater reliability, as well as directions for further research and refinement of RepeAT. Conclusions: The use of RepeAT may allow the biomedical community to have a better understanding of the current practices of research transparency and accessibility among principal investigators. Common adoption of RepeAT may improve reporting of research practices and the availability of research outputs. Additionally, use of RepeAT will facilitate comparisons of research transparency and accessibility across domains and institutions. © 2017 The Author(s).","Accessibility; Ehr; Electronic health records; Replication; Reproducibility; Secondary data re-use; Transparency",,2-s2.0-85029593391
"Cui H., Zhang K., Fang Y., Sobolevsky S., Ratti C., Horn B.","A clustering validity index based on pairing frequency",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030637493&doi=10.1109%2fACCESS.2017.2743985&partnerID=40&md5=4f2e68b0662e4cbaab3b7bd807478060","Clustering is an important problem which have been applied in many research areas. However, there is a large variety of clustering algorithms and each could produce quite different results depending on the choice of algorithm and input parameters, so how to evaluate clustering quality and find out the optimal clustering algorithm is important. Various clustering validity indices are proposed under this background. Traditional clustering validity indices can be divided into two categories: internal and external. The former is mostly based on compactness and separation of data points which is measured by the distance between clusters&#x2019; centroids, ignoring the shape and density of clusters. The latter needs external information which is unavailable in most cases. In this paper, we propose a new clustering validity index for both fuzzy and hard clustering algorithms. Our new index uses pairwise pattern information from a certain number of interrelated clustering results, which focus more on logical reasoning than geometrical features. The proposed index overcomes some shortcomings of traditional indices. Experiments show that the proposed index performs better compared with traditional indices on the artificial and real datasets. Furthermore, we applied the proposed method to solve two existing problems in telecommunication fields. One is to cluster Serving GPRS Support Nodes (SGSNs) in the city Chongqing based on service characteristics, the other is to analyze users&#x2019; preference. OAPA","Clustering algorithms; clustering analysis; clustering validity; Cognition; Data mining; Density measurement; fuzzy c-means; Indexes; pairwise pattern; Partitioning algorithms; Urban areas","Cluster analysis; Data mining; Density measurement (specific gravity); Quality control; Clustering analysis; Clustering validity; Cognition; Fuzzy C mean; Indexes; pairwise pattern; Partitioning algorithms; Urban areas; Clustering algorithms",2-s2.0-85030637493
"Ribeiro J., Viveiros D., Ferreira J., Lopez-Gil A., Dominguez-Lopez A., Martins H.F., Perez-Herrera R., Lopez-Aldaba A., Duarte L., Pinto A., Martin-Lopez S., Baierl H., Jamier R., Rougier S., Auguste J.-L., Teodoro A.C., Gonçalves J.A., Esteban O., Santos J.L., Roy P., Lopez-Amo M., Gonzalez-Herraez M., Baptista J.M., Flores D.","ECOAL project-Delivering Solutions for Integrated monitoring of coal-related fires supported on optical fiber sensing technology",2017,"Applied Sciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029795026&doi=10.3390%2fapp7090956&partnerID=40&md5=47f26ebf35d3f0496677fa67da18111f","The combustion of coal wastes resulting from mining is of particular environmental concern, and the importance of proper management involving real-time assessment of their status and identification of probable evolution scenarios is recognized. Continuous monitoring of the combustion temperature and emission levels of certain gases allows for the possibility of planning corrective actions to minimize their negative impact on the surroundings. Optical fiber technology is well suited to this purpose and here we describe the main attributes and results obtained from a fiber optic sensing system projected to gather data on distributed temperature and gas emissions in these harsh environments. © 2017 by the authors.","Coal mining; Coal waste deposits; Environmental impact; Gas detection; Optical fiber sensing; Spontaneous coal combustion; Temperature measurement",,2-s2.0-85029795026
"Chiche A., Meshesha M.","An intelligent network intrusion detcetion system using data mining and knowledge based system",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029717727&partnerID=40&md5=cbfc5f788059222e1b81a215d83b1b90","In this study, an intelligent network intrusion detection and prevention system is presented for detecting network attacks that incorporates a knowledge based system and data mining techniques. To extract hidden knowledge from KDDCup’99 dataset, hybrid data mining process is used. The intrusion dataset for the study is collected from MIT Lincon lab. A predictive model is constructed on total datasets of 63, 661 instances using JRip rule induction, Naïve Bayes,J48 decision tree and Multilayer Perceptron (MLP) Neural Network. During training 99.91% prediction accuracy is achieved by J48 decision tree. So, the J48 model is integrated with knowledge based system automatically for designing intelligent network intrusion detection and prevention system. In addition, knowledge is acquired, represented and organized in the knowledge based so as to suggest possible prevention for detected attacks. Evaluation results show that the proposed system registers 91.43% accuracy in network intrusion detection and 85% in user acceptance testing. This indicates that the performance of the proposed system is promising to design an intelligent network intrusion detection system that can effectively predict and provide a prevention mechanism. The system cannot update the knowledge of prevention techniques automatically which need further researches. © 2005 - Ongoing JATIT & LLS.","Data mining; Intrusion prevention; Knowledge based system; Network intrusion detection",,2-s2.0-85029717727
"Yang J., Xu Q.","A Highly Efficient Quantitative Transaction Data Mining Model",2017,"Proceedings - 2016 International Conference on Digital Home, ICDH 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032335429&doi=10.1109%2fICDH.2016.047&partnerID=40&md5=d83c51c917cfabf7b6086d67929c0ab3","This paper focused on discrete time Geo/G/1 queue with Bernoulli gated service. We proposed a new algorithm to calculate stock's money flow by the probability generating function (P.G.F.) of stationary waiting time and stationary queue length. We did program by data mining of a variety of stock trading. The new algorithm could get any stock's money flow. We have established a series of quantitative trading strategies based on the new money flow model. These strategies could guide investors to grasp an upward trend or avoid a down trend. The quantitative investment trading strategies can be automated trading. It could reduce investors' subjective operability and increase return rate. The new algorithm was suitable for index trading, futures trading, stock trading, and ETF trading. © 2016 IEEE.","Data mining; Geo/G/1 queue; Money flow","Commerce; Data mining; Digital devices; Financial markets; Queueing theory; Automated trading; Bernoulli gated service; Discrete-time geo/g/1 queues; Geo/G/1 queue; Money flow; Probability generating functions; Stationary waiting time; Trading strategies; Electronic trading",2-s2.0-85032335429
"Nakata K., Orihara R., Mizuoka Y., Takagi K.","A Comprehensive Big-Data-Based Monitoring System for Yield Enhancement in Semiconductor Manufacturing",2017,"IEEE Transactions on Semiconductor Manufacturing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030648574&doi=10.1109%2fTSM.2017.2753251&partnerID=40&md5=2a268003a8fec2dd7d2deb60eb3796b5","In this paper, we focus on yield analysis task where engineers identify the cause of failure from wafer failure map patterns and manufacturing histories. We organize yield analysis task into the following three stages, namely, failure map pattern monitoring, failure cause identification, and failure recurrence monitoring, and incorporate machine learning and data mining technologies into each stage to support engineers' work. The important point is that big data analysis enables comprehensive and long-term monitoring automation. We make use of fast and scalable methods of clustering and pattern mining and realize daily comprehensive monitoring with massive manufacturing data. We also apply deep learning, which has been an innovative core technology of machine learning in recent years, to classification of wafer failure map patterns, and explore its performance in detail. Finally, these machine learning and data mining techniques are integrated into an automated monitoring system with interfaces familiar to engineers to attain large yield enhancement. © 1988-2012 IEEE.","Data mining; deep learning; machine learning; pattern recognition; semiconductor defects","Artificial intelligence; Big data; Crystal defects; Data mining; Deep learning; Engineers; History; Learning systems; Manufacture; Neural networks; Pattern recognition; Scalability; Semiconductor device manufacture; Semiconductor device models; Semiconductor devices; Silicon wafers; Automated monitoring systems; Comprehensive monitoring; Data mining technology; Long term monitoring; Monitoring system; Scalable methods; Semiconductor manufacturing; Yield enhancement; Monitoring",2-s2.0-85030648574
"Yuan J., Chen M., Jiang T., Li T.","Complete tolerance relation based parallel filling for incomplete energy big data",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021312786&doi=10.1016%2fj.knosys.2017.06.027&partnerID=40&md5=8449ba92d4ecfbcd4b32e751247fe24e","With the approaching of cloud and big data computing era, renewable energy such as solar energy is increasingly integrated into data center power provisioning systems. Nevertheless, the power statistics collection may not be possible or available due to the fact that renewable energy supply exhibits intermittency, time varying behavior (e.g. shortage or failure), resulting in missing data. In this paper, we propose a filling algorithm based on complete tolerance class to solve the missing of energy big data issue. Note that traditional method based on rough sets will likely to fail when there is severe missing data, and its solution on tolerance relation and tolerance class is more complex, which is not suitable for the large scale and the time varying energy big data. Our proposed algorithm expands the tolerance relation into the complete tolerance relation to partition the complete tolerance class. Moreover, our algorithm fills the missing attribute values of the energy big data in data center, which ensures the data integrity and improve the classification accuracy. We further parallelize and optimize our algorithm on state-of-the-art Spark cluster computing framework. In addition, we propose the adaptive management architecture that handles incomplete energy big data in green data centers. Our proposed architecture integrates the techniques for preprocessing energy data, filling incomplete energy data and building decision model. It increases the power assignment efficiency between solar power and utility, while enhancing load performance and service availability. As a result, it can provide better service for green data centers. We perform comprehensive experiments on an energy data set and the results show the Completing Incomplete Big Data (CIBD) algorithm can guarantee the completeness of data while improving the filling accuracy by 10% compared to general filling algorithms such as MEAN or ERS. The proposed algorithm and architecture show more benefit as the data missing rate increases. We further utilize the filled data to establish the random forest model and yield desirable results. Compared to the Hadoop based filling algorithm, the processing speed of the CIBD algorithm improves by 50% on the 4GB data size. © 2017 Elsevier B.V.","Adaptive management architecture of incomplete energy big data; Complete toleration class; Green data center; Incomplete energy big data; Parallel filling on Spark","Cluster computing; Clustering algorithms; Computer architecture; Data mining; Decision trees; Distributed computer systems; Filling; Green computing; Information management; Rough set theory; Solar energy; Adaptive Management; Classification accuracy; Complete toleration class; Green data centers; Proposed architectures; Random forest modeling; Service availability; Time varying behavior; Big data",2-s2.0-85021312786
"Hernandez I., Zhang Y.","Using predictive analytics and big data to optimize pharmaceutical outcomes",2017,"American Journal of Health-System Pharmacy",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029168068&doi=10.2146%2fajhp161011&partnerID=40&md5=655213975f3101266b1ff348bdb07647","Purpose: The steps involved, the resources needed, and the challenges associated with applying predictive analytics in healthcare are described, with a review of successful applications of predictive analytics in implementing population health management interventions that target medication-related patient outcomes. Summary: In healthcare, the term big data typically refers to large quantities of electronic health record, administrative claims, and clinical trial data as well as data collected from smartphone applications, wearable devices, social media, and personal genomics services; predictive analytics refers to innovative methods of analysis developed to overcome challenges associated with big data, including a variety of statistical techniques ranging from predictive modeling to machine learning to data mining. Predictive analytics using big data have been applied successfully in several areas of medication management, such as in the identification of complex patients or those at highest risk for medication noncompliance or adverse effects. Because predictive analytics can be used in predicting different outcomes, they can provide pharmacists with a better understanding of the risks for specific medication-related problems that each patient faces. This information will enable pharmacists to deliver interventions tailored to patients' needs. In order to take full advantage of these benefits, however, clinicians will have to understand the basics of big data and predictive analytics. Conclusion: Predictive analytics that leverage big data will become an indispensable tool for clinicians in mapping interventions and improving patient outcomes. © Copyright 2017, American Society of Health-System Pharmacists, Inc. All rights reserved.","Big data; Medication management; Pharmaceutical outcomes; Population health management; Predictive analytics","administrative claims (health care); adverse outcome; Article; data analysis; data mining; data processing; electronic health record; genomics; health care management; health care personnel management; high risk patient; human; machine learning; medical terminology; medication compliance; medication therapy management; mobile application; patient coding; patient risk; pharmaceutical care; pharmacist attitude; predictive value; priority journal; process optimization; risk benefit analysis; smartphone; social media; treatment outcome",2-s2.0-85029168068
"Wu T.-Y., Tseng Y.-M., Huang S.-S., Lai Y.-C.","Non-Repudiable Provable Data Possession Scheme with Designated Verifier in Cloud Storage Systems",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030631147&doi=10.1109%2fACCESS.2017.2753243&partnerID=40&md5=0b48fd0c9ccfd3f677599d85c8164c7b","In cloud storage systems, users can upload their data along with associated tags (authentication information) to cloud storage servers. To ensure the availability and integrity of the outsourced data, provable data possession (PDP) schemes convince verifiers (users or third parties) that the outsourced data stored in the cloud storage server is correct and unchanged. Recently, several PDP schemes with designated verifier (DV-PDP) were proposed to provide the flexibility of arbitrary designated verifier. A designated verifier (private verifier) is trustable and designated by a user to check the integrity of the outsourced data. However, these DV-PDP schemes are either inefficient or insecure under some circumstances. In this paper, we propose the first non-repudiable PDP scheme with designated verifier (DV-NRPDP) to address the non-repudiation issue and resolve possible disputations between users and cloud storage servers. We define the system model, framework and adversary model of DV-NRPDP schemes. Afterward, a concrete DV-NRPDP scheme is presented. Based on the computing discrete logarithm assumption, we formally prove that the proposed DV-NRPDP scheme is secure against several forgery attacks in the random oracle model. Comparisons with the previously proposed schemes are given to demonstrate the advantages of our scheme. © 2013 IEEE.","Cloud storage; data integrity; designated verifier; non-repudiation; private auditability; provable data possession","Cloud computing; Data mining; Data structures; Servers; Cascading style sheets; Cloud storages; Computational model; Data integrity; Designated verifiers; Non repudiation; Private auditability; Provable data possessions; Security; Digital storage",2-s2.0-85030631147
"Kang F., Xu Z., Cheng F., Zhou Q., Liu B., Cao B., Wang T., Mei J.","Yoroad: A monitoring service for ride sharing systems",2017,"Proceedings - 2017 IEEE 2nd International Congress on Internet of Things, ICIOT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032299292&doi=10.1109%2fIEEE.ICIOT.2017.24&partnerID=40&md5=076e7913e801bb9b7ad79fefd650c728","In recent years, many ride-sharing systems have been widely used for helping people travel around. These systems generate a lot of data every day, e.g., the trajectories and the transactions. But as far as we know, there is no such monitoring system or platform for ride sharing systems to manage and observe this set of data in an effective way. Hence, in this paper, we illustrate YoRoad, a general monitoring service for ride-sharing system. YoRoad consists of three main modules: (1) moving objects tracking module is used for monitoring the drivers real-time locations, where moving object indexing and spatial query processing technique is embedded, (2) user management module, where the information for each driver and rider is maintained, and (3) data statistics module, where we perform some data analysis such as the total distance saving comparison between ride-sharing and non-ride-sharing, trajectory data mining and location preference or travel pattern discovery based on the transaction data. © 2017 IEEE.","Monitoring Service; Moving objects; Ride sharing; Trajectory","Data mining; Indexing (materials working); Internet of things; Query languages; Trajectories; Monitoring services; Monitoring system; Moving object indexing; Moving objects; Real-time location; Ride-sharing; Spatial query processing; Trajectory data minings; Information management",2-s2.0-85032299292
"Wu Z., Shang P.","Nonlinear transformation on the transfer entropy of financial time series",2017,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018971663&doi=10.1016%2fj.physa.2017.04.103&partnerID=40&md5=d376b7207fa845e0b527a0dc94fe2b2c","Transfer entropy (TE) now is widely used in the data mining and economic field. However, TE itself demands that time series intend to be stationary and meet Markov condition. Naturally, we are interested in investigating the effect of the nonlinear transformation of the two series on the TE. Therefore, the paper is designed to study the TE of five nonlinear “volatile” transformations based on the data which are generated by the linear modeling and the logistic maps modeling, as well as the dataset that come from financial markets. With only one of the TE of nonlinear transformations fluctuating around the TE of original series, the TE of others all have increased with different degrees. © 2017 Elsevier B.V.","Financial time series; Nonlinear transformation; Transfer entropy","Data mining; Entropy; Finance; Financial data processing; Linear transformations; Time series; Economic fields; Financial time series; Linear modeling; Logistic maps; Markov condition; Non-linear transformations; Original series; Transfer entropy; Mathematical transformations",2-s2.0-85018971663
"Fang S.-H., Fei Y.-X., Xu Z., Tsao Y.","Learning Transportation Modes from Smartphone Sensors Based on Deep Neural Network",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028993063&doi=10.1109%2fJSEN.2017.2737825&partnerID=40&md5=321f476b2bfa3a790cdbb5f917efb795","In recent years, the importance of user information has increased rapidly for context-aware applications. This paper proposes a deep learning mechanism to identify the transportation modes of smartphone users. The proposed mechanism is evaluated on a database that contains more than 1000 h of accelerometer, magnetometer, and gyroscope measurements from five transportation modes, including still, walk, run, bike, and vehicle. Experimental results confirm the effectiveness of the proposed mechanism, which achieves approximately 95% classification accuracy and outperforms four well-known machine learning methods. Meanwhile, we investigated the model size and execution time of different algorithms to address practical issues. © 2001-2012 IEEE.","big data; deep learning; mobile phone; sensors; Transportation mode","Accelerometers; Artificial intelligence; Big data; Data mining; Data structures; Deep neural networks; Feature extraction; Learning systems; Mobile phones; Neural networks; Sensors; Smartphones; Transportation; Biological neural networks; Classification accuracy; Context aware applications; Learning mechanism; Machine learning methods; Practical issues; Transportation mode; User information; Deep learning",2-s2.0-85028993063
"Villamil M.D.P., Barrera D., Velasco N., Bernal O., Fajardo E., Urango C., Buitrago S.","Strategies for the quality assessment of the health care service providers in the treatment of Gastric Cancer in Colombia",2017,"BMC Health Services Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029502888&doi=10.1186%2fs12913-017-2440-8&partnerID=40&md5=cb741dea267d2fe411cc043ecfebb010","Background: While, at its inception in 1993, the health care system in Colombia was publicized as a paradigm to be copied across the developing world, numerous problems in its implementation have led to, what is now, an inefficient and crisis-ridden health system. Furthermore, as a result of inappropriate tools to measure the quality of the health service providers, several corruption scandals have arisen in the country. This study attempts to tackle this situation by proposing a strategy for the quality assessment of the health service providers (Entidades Promotoras de Salud, EPS) in the Colombian health system. In particular, as a case study, the quality of the treatment of stomach cancer is analyzed. Methods: The study uses two complementary techniques to address the problem. These techniques are applied based on data of the treatment of gastric cancer collected on a nation-wide scale by the Colombian Ministry of Health and Welfare. First, Data Envelopment Analysis (DEA) and the Malmquist Index (MI) are used to establish the most efficient EPS's within the system, according to indicators such as opportunity indicators. Second, sequential clustering algorithm, related to process mining a field of data mining, is used to determine the medical history of all patients and to construct typical care pathways of the patients belonging to efficient and inefficient EPS's. Lastly, efforts are made to identify traits and differences between efficient and inefficient EPS's. Results: Efficient and inefficient EPS were identified for the years 2010 and 2011. Additionally, a Malmquist Index was used to calculate the relative changes in the efficiency of the health providers. Using these efficiency rates, the typical treatment path of patients with gastric cancer was found for two EPSs: one efficient and another inefficient. Finally, the typical traits of the care pathways were established. Conclusions: Combining DEA and process mining proved to be a powerful approach understanding the problem and gaining valuable insight into the inner workings of the Colombian Health System, especially in terms of the treatment process performed by health care providers in critical illnesses such as cancer. However, no sufficiently compelling results were found to establish the contribution of such a combination to evaluate the quality in the delivery of health services. © 2017 The Author(s).","Clustering; Data mining; DEA; Process mining; Quality assessment",,2-s2.0-85029502888
"Murthy K.R., Ghosh A.","Noisy-free Length Discriminant Analysis with cosine hyperbolic framework for dimensionality reduction",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016401105&doi=10.1016%2fj.eswa.2017.03.034&partnerID=40&md5=c0a1409b4846723c8ad1f7a4ea1d3f0e","Dimensionality Reduction (DR) is very useful and popular in many application areas of expert and intelligent systems, such as machine learning, finance, data and text mining, multimedia mining, image processing, anomaly detection, defense applications, bioinformatics and natural language processing. DR is widely applied for better data visualization and improving learning in all the above fields. In this manuscript, we propose a novel DR approach namely, Noisy-free Length Discriminant Analysis (NLDA) by developing Noisy-free Relevant Pattern Selection (NRPS). Traditional pattern selection methods discriminate boundary and non-boundary patterns with the help of class information and nearest neighbors. And these methods completely ignore noisy patterns which may degrade the performance of subsequent subspace learning. To overcome this, we develop Noisy-free Relevant Pattern Selection (NRPS), in which data instances are partitioned into boundary, non-boundary and noisy patterns. With the help of noisy-free boundary and non-boundary patterns, Noisy-free Length Discriminant Analysis (NLDA) has been proposed by developing new within and between-class scatters. These scatters model discriminations between lengths (L2-norms) of different class instances by considering only boundary and non-boundary patterns, while ignoring noisy patterns. A cosine hyperbolic frame work has been developed to formulate the objective of NLDA. Moreover, NLDA can also model the discrimination of multimodal data as different class data may consist of different lengths. Experimental study conducted on the synthesized data, UCI, and leeds butterfly databases. Moreover, an experimental study over human and computer interaction, i.e., face recognition (one of the application areas of expert and intelligent systems), has been performed. And, these studies prove that the proposed method can produce better discriminated subspace compare to the state-of-the-art methods. © 2017 Elsevier Ltd","Cosine hyperbolic; Dimensionality reduction; Discriminant analysis; Face recognition; Relevant patterns; Subspace learning","Data mining; Data visualization; Discriminant analysis; Face recognition; Human computer interaction; Image processing; Intelligent systems; Learning algorithms; Learning systems; Natural language processing systems; Between class scatter; Cosine hyperbolic; Dimensionality reduction; Model discrimination; NAtural language processing; Relevant patterns; State-of-the-art methods; Subspace learning; Pattern recognition",2-s2.0-85016401105
"Tellez E.S., Miranda-Jiménez S., Graff M., Moctezuma D., Siordia O.S., Villaseñor E.A.","A case study of Spanish text transformations for twitter sentiment analysis",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017181677&doi=10.1016%2fj.eswa.2017.03.071&partnerID=40&md5=6cd4edb5f1782c214667f346d4e9f2bf","Sentiment analysis is a text mining task that determines the polarity of a given text, i.e., its positiveness or negativeness. Recently, it has received a lot of attention given the interest in opinion mining in micro-blogging platforms. These new forms of textual expressions present new challenges to analyze text because of the use of slang, orthographic and grammatical errors, among others. Along with these challenges, a practical sentiment classifier should be able to handle efficiently large workloads. The aim of this research is to identify in a large set of combinations which text transformations (lemmatization, stemming, entity removal, among others), tokenizers (e.g., word n-grams), and token-weighting schemes make the most impact on the accuracy of a classifier (Support Vector Machine) trained on two Spanish datasets. The methodology used is to exhaustively analyze all combinations of text transformations and their respective parameters to find out what common characteristics the best performing classifiers have. Furthermore, we introduce a novel approach based on the combination of word-based n-grams and character-based q-grams. The results show that this novel combination of words and characters produces a classifier that outperforms the traditional word-based combination by 11.17% and 5.62% on the INEGI and TASS’15 dataset, respectively. © 2017 Elsevier Ltd","Error-robust text representations; Opinion mining; Sentiment analysis","Classification (of information); Computational linguistics; Natural language processing systems; Text processing; Grammatical errors; Lemmatization; Micro-blogging platforms; Opinion mining; Sentiment analysis; Text representation; Weighting scheme; Word n-grams; Data mining",2-s2.0-85017181677
"Agnihotri D., Verma K., Tripathi P.","Variable Global Feature Selection Scheme for automatic classification of text documents",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016569326&doi=10.1016%2fj.eswa.2017.03.057&partnerID=40&md5=173997875e470d7f96bde7b71ec82bcc","The feature selection is important to speed up the process of Automatic Text Document Classification (ATDC). At present, the most common method for discriminating feature selection is based on Global Filter-based Feature Selection Scheme (GFSS). The GFSS assigns a score to each feature based on its discriminating power and selects the top-N features from the feature set, where N is an empirically determined number. As a result, it may be possible that the features of a few classes are discarded either partially or completely. The Improved Global Feature Selection Scheme (IGFSS) solves this issue by selecting an equal number of representative features from all the classes. However, it suffers in dealing with an unbalanced dataset having large number of classes. The distribution of features in these classes are highly variable. In this case, if an equal number of features are chosen from each class, it may exclude some important features from the class containing a higher number of features. To overcome this problem, we propose a novel Variable Global Feature Selection Scheme (VGFSS) to select a variable number of features from each class based on the distribution of terms in the classes. It ensures that, a minimum number of terms are selected from each class. The numerical results on benchmark datasets show the effectiveness of the proposed algorithm VGFSS over classical information science methods and IGFSS. © 2017 Elsevier Ltd","Feature selection; Text analysis; Text document classification; Text mining","Data mining; Filtration; Information retrieval systems; Numerical methods; Text processing; Automatic classification; Benchmark datasets; Classical information; Discriminating power; Important features; Text analysis; Text document classifications; Text mining; Feature extraction",2-s2.0-85016569326
"Altaher A., Osman A.H.","An intelligent approach for predicting social media impact on brand building",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029712731&partnerID=40&md5=6f72487bec2a47f8a70b113dcc9a658b","Social media networks such as Twitter and Facebook plays important roles in many aspects of our lives and affects many of our decisions. This paper presents a data mining model consists of different five classification and regression algorithms to predict the significant performance metrics of posts announced in the Facebook pages of the brands. The algorithms utilized in the model include the Generalized Liner Regression (GLR), Normal Regression (NR), support Vector Machine, Neural Network, and CHAID decision tree classifier. Using a dataset contained a 790 published posts in the cosmetic brand, the Lifetime post consumers achieved the best posts performance metrics with an average accuracy of 0.82 among all the algorithms in the proposed model, followed by the Lifetime post total reach performance metric with an average accuracy of 0.79. The findings of this research potentially help the manager's in making the right decisions regarding whether to publish a post. © 2005 - Ongoing JATIT & LLS.","Brand building; Classification; Data mining; Performance metrics; Social media",,2-s2.0-85029712731
"Wu J., Lu B., Liang Z.","Performance Prediction of Room Air Conditioners and Optimization of Control Strategy for Energy Conservation",2017,"Heat Transfer Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029594674&doi=10.1080%2f01457632.2017.1370322&partnerID=40&md5=970a470afe731738c5c52339392515c3","Long-term performance of room air conditioners (RAC) is an important evaluation index to estimate RAC actual energy conservation efficiency. The environmental condition change and long-term use will result in degradation of RAC performance, which increases energy consumption. To solve the problem, an optimization method was proposed. The operating data was collected by a RAC online monitoring system. The hidden relationship among operating parameters was discovered by data mining method, and then an artificial neural network prediction model was developed. After training, this model had been validated to be effective in predicting energy efficiency ratio (EER) with absolute fraction of variance of 0.990 and root mean square error of 0.291. Moreover, the prediction results of return air temperature, relative humidity and EER agreed well with measured values. Based on the prediction results and historical operating database, an optimal control strategy was proposed to improve the operating performance of RAC. By applying the control strategy the energy saving rate was theoretically about 9% in the stable operation case and 2.61% in all actual operating period. © 2017 Taylor & Francis Group, LLC",,"Air conditioning; Data mining; Domestic appliances; Energy conservation; Energy utilization; Forecasting; Mean square error; Neural networks; Optimal control systems; Energy efficiency ratio; Environmental conditions; Neural network prediction model; On-line monitoring system; Optimal control strategy; Performance prediction; Return-air temperatures; Root mean square errors; Energy efficiency",2-s2.0-85029594674
"Noonan J., Williams W.P., Shan X.","Investigation of antimicrobial peptide genes associated with fungus and insect resistance in maize",2017,"International Journal of Molecular Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029617850&doi=10.3390%2fijms18091938&partnerID=40&md5=a3f3db5dc33cb513b7a36354566f1db2","Antimicrobial peptides (AMPs) are small defense proteins present in various organisms. Major groups of AMPs include beta-barrelin, hevein, knottin, lipid transfer protein (LTP), thionin, defensin, snakin, and cyclotide. Most plant AMPs involve host plant resistance to pathogens such as fungi, viruses, and bacteria, whereas a few plant AMPs from the cyclotide family carry insecticidal functions. In this research, a genome-wide investigation on antimicrobial peptide genes in maize genome was conducted. AMPs previously identified from various plant species were used as query sequences for maize genome data mining. Thirty-nine new maize AMPs were identified in addition to seven known maize AMPs. Protein sequence analysis revealed 10 distinguishable maize AMP groups. Analysis of mRNA expression of maize AMP genes by quantitative real-time polymerase chain reaction (qRT-PCR) revealed different expression patterns in a panel of 10 maize inbred lines. Five maize AMP genes were found significantly associated with insect or fungus resistance. Identification of maize antimicrobial peptide genes will facilitate the breeding of host plant resistance and improve maize production. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Cyclotide; Defensin; Hevein; Host plant resistance; Maize; Plant antimicrobial peptides; Snakin","beta barrelin; cyclotide; defensin; genomic DNA; hevein; lipid transfer protein; plant protein; polypeptide antibiotic agent; snakin; unclassified drug; amino acid sequence; antifungal susceptibility; antimicrobial peptide gene; Article; Aspergillus flavus; bioinformatics; data mining; DNA extraction; DNA polymorphism; fungus immunity; gene; gene expression; gene identification; inbred strain; insect resistance; maize; nonhuman; phylogenetic tree; phylogeny; protein expression; protein motif; real time polymerase chain reaction; reverse transcription polymerase chain reaction; RNA extraction; sequence analysis",2-s2.0-85029617850
"Lo S.L., Chiong R., Cornforth D.","An unsupervised multilingual approach for online social media topic identification",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016407931&doi=10.1016%2fj.eswa.2017.03.029&partnerID=40&md5=03b1ae4a42c2af4071147b07570b49ec","Social media data can be valuable in many ways. However, the vast amount of content shared and the linguistic variants of languages used on social media are making it very challenging for high-value topics to be identified. In this paper, we present an unsupervised multilingual approach for identifying highly relevant terms and topics from the mass of social media data. This approach combines term ranking, localised language analysis, unsupervised topic clustering and multilingual sentiment analysis to extract prominent topics through analysis of Twitter's tweets from a period of time. It is observed that each of the ranking methods tested has their strengths and weaknesses, and that our proposed ‘Joint’ ranking method is able to take advantage of the strengths of the ranking methods. This ‘Joint’ ranking method coupled with an unsupervised topic clustering model is shown to have the potential to discover topics of interest or concern to a local community. Practically, being able to do so may help decision makers to gauge the true opinions or concerns on the ground. Theoretically, the research is significant as it shows how an unsupervised online topic identification approach can be designed without much manual annotation effort, which may have great implications for future development of expert and intelligent systems. © 2017 Elsevier Ltd","Multilingual analysis; Social media; Topic identification; Unsupervised learning","Decision making; Intelligent systems; Online systems; Social networking (online); Unsupervised learning; Implications for futures; Multilingual analysis; Multilingual approach; Online social medias; Sentiment analysis; Social media; Social media datum; Topic identification; Data mining",2-s2.0-85016407931
"Liu Q., Xu X., Tao Y., Wang X.","An Improved Decision Tree Method Base on RELIEFF for Medical Diagnosis",2017,"Proceedings - 2016 International Conference on Digital Home, ICDH 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032377793&doi=10.1109%2fICDH.2016.037&partnerID=40&md5=5a517755aaec0d58c1e714370b2bb2a4","There emerges an increasing need to mine and analyze the health data from smart home medical systems and community medical organizations. Regarding to the influence of irrelevant attributes, in this study, an improved C4.5 decision tree method based on RELIEFF attribute weighting techniques is proposed for medical diagnosis. This method includes two steps: the first step is to delete the irrelevant attributes for the classification using the RELIEFF algorithm, the second one is to build effective disaggregated model for medical diagnosis using C4.5 decision tree. Two experiments for UCI dermatology data and health examination data have been conducted respectively. Results prove that the proposed method can improve the accuracy of medical data classification for illness diagnosis, as a valuable decision support tool for doctors. © 2016 IEEE.","C4.5 algorithm; decision tree; feature selection (FS); medical diagnosis; RELIEFF","Automation; Computer aided diagnosis; Data mining; Decision support systems; Decision trees; Digital devices; Intelligent buildings; Trees (mathematics); Attribute weighting; C4.5 algorithm; C4.5 decision tree method; C4.5 decision trees; Decision support tools; Decision tree method; Health examinations; ReliefF; Diagnosis",2-s2.0-85032377793
"Mao Y., Zhang Z., Fan D.","Hybrid Feature Selection Based on Improved Genetic Algorithm for Stock Prediction",2017,"Proceedings - 2016 International Conference on Digital Home, ICDH 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032335667&doi=10.1109%2fICDH.2016.052&partnerID=40&md5=83c149942c0783e3621c942cfbf60a8f","This paper proposes a hybrid feature selection approach based on Genetic Algorithm (GA) to predict stock return. This hybrid feature selection approach combines the advantages of three filter feature selection approaches with an improved Genetic Algorithm (IGA) to identify an optimum feature subset and to increase the classification accuracy and scalability. We propose IGA by improving the initial population generating and genetic oprators using the results of filter approaches as some prior information instead of probability and random selection and bring in multiple population Genetic Algorithm (MPGA). The effectiveness of the proposed approach is evaluated in comparison with GA using A-shares data traded on the Shanghai Stock Exchanges. The experimental results show that the proposed hybrid feature selection approach significantly outperforms the single filter approaches for optimum feature selection and these features are good indicators for the prediction of return. © 2016 IEEE.","Data mining; Hybrid feature selection; Improved genetic algorithm; Stock market prediction","Commerce; Data mining; Digital devices; Electronic trading; Filtration; Financial markets; Forecasting; Genetic algorithms; Investments; Classification accuracy; Hybrid feature selections; Initial population; Multiple population genetic algorithms; Prior information; Shanghai stock exchanges; Stock market prediction; Stock predictions; Feature extraction",2-s2.0-85032335667
"Duong Q.-H., Fournier-Viger P., Ramampiaro H., Nørvåg K., Dam T.-L.","Efficient high utility itemset mining using buffered utility-lists",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539168&doi=10.1007%2fs10489-017-1057-2&partnerID=40&md5=18cfe86eb293dd7e2d1cf8a59631cccf","Discovering high utility itemsets in transaction databases is a key task for studying the behavior of customers. It consists of finding groups of items bought together that yield a high profit. Several algorithms have been proposed to mine high utility itemsets using various approaches and more or less complex data structures. Among existing algorithms, one-phase algorithms employing the utility-list structure have shown to be the most efficient. In recent years, the simplicity of the utility-list structure has led to the development of numerous utility-list based algorithms for various tasks related to utility mining. However, a major limitation of utility-list based algorithms is that creating and maintaining utility-lists are time consuming and can consume a huge amount of memory. The reasons are that numerous utility lists are built and that the utility-list intersection/join operation to construct a utility-list is costly. This paper addresses this issue by proposing an improved utility-list structure called utility-list buffer to reduce the memory consumption and speed up the join operation. This structure is integrated into a novel algorithm named ULB-Miner (Utility-List Buffer for high utility itemset Miner), which introduces several new ideas to more efficiently discover high utility itemsets. ULB-Miner uses the designed utility-list buffer structure to efficiently store and retrieve utility-lists, and reuse memory during the mining process. Moreover, the paper also introduces a linear time method for constructing utility-list segments in a utility-list buffer. An extensive experimental study on various datasets shows that the proposed algorithm relying on the novel utility-list buffer structure is highly efficient in terms of both execution time and memory consumption. The ULB-Miner algorithm is up to 10 times faster than the FHM and HUI-Miner algorithms and consumes up to 6 times less memory. Moreover, it performs well on both dense and sparse datasets. © 2017 Springer Science+Business Media, LLC","Itemset mining; Pattern mining; Utility list; Utility list buffer; Utility mining","Artificial intelligence; Itemset mining; Pattern mining; Utility list; Utility list buffer; Utility mining; Miners",2-s2.0-85029539168
"Zhang B., Yang C., Zhu H., Shi P., Gui W.","Controllable-domain-based fuzzy rule extraction for copper removal process control",2017,"IEEE Transactions on Fuzzy Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030634879&doi=10.1109%2fTFUZZ.2017.2751000&partnerID=40&md5=de224170e4217651e73b2a988fe18481","In copper removal process control, the commonly used technique is the so called rule based control, which is largely dependent upon the operators' experience, likely leading to unstable process production due to each individual's characters and favors. In this paper, to enhance the effectiveness of process control, a controllable-domain-based fuzzy rule extraction strategy is proposed. New definitions of representative controlled samples are introduced, by which the input variable space is divided several controllable domains by applying positive and unlabeled learning algorithm. Also, the unreasonable removed and the controllable domains are accordingly determined. Then, support vector machine method is employed to extract fuzzy control rules for different domains. Finally an industrial experiment is presented to demonstrate the effectiveness and advantages of the developed new design scheme. IEEE","Algorithm design and analysis; Classification algorithms; Copper; copper removal; Data mining; fuzzy logic; positive and unlabeled learning (PU learning); Process control; Production; rule extraction; support vector machine (SVM); Support vector machines","Copper; Data mining; Extraction; Fuzzy control; Fuzzy inference; Fuzzy logic; Fuzzy rules; Learning algorithms; Production; Support vector machines; Algorithm design and analysis; Classification algorithm; Copper removal; Pu learning; Rule extraction; Process control",2-s2.0-85030634879
"Boididou C., Middleton S.E., Jin Z., Papadopoulos S., Dang-Nguyen D.-T., Boato G., Kompatsiaris Y.","Verifying information with multimedia content on twitter: A comparative study of automated approaches",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029476245&doi=10.1007%2fs11042-017-5132-9&partnerID=40&md5=82809ad479d7d2a11553d6b2971af2bb","An increasing amount of posts on social media are used for disseminating news information and are accompanied by multimedia content. Such content may often be misleading or be digitally manipulated. More often than not, such pieces of content reach the front pages of major news outlets, having a detrimental effect on their credibility. To avoid such effects, there is profound need for automated methods that can help debunk and verify online content in very short time. To this end, we present a comparative study of three such methods that are catered for Twitter, a major social media platform used for news sharing. Those include: a) a method that uses textual patterns to extract claims about whether a tweet is fake or real and attribution statements about the source of the content; b) a method that exploits the information that same-topic tweets should be also similar in terms of credibility; and c) a method that uses a semi-supervised learning scheme that leverages the decisions of two independent credibility classifiers. We perform a comprehensive comparative evaluation of these approaches on datasets released by the Verifying Multimedia Use (VMU) task organized in the context of the 2015 and 2016 MediaEval benchmark. In addition to comparatively evaluating the three presented methods, we devise and evaluate a combined method based on their outputs, which outperforms all three of them. We discuss these findings and provide insights to guide future generations of verification tools for media professionals. © 2017 Springer Science+Business Media, LLC","Credibility; Fake detection; Multimedia; Social media; Trust; Twitter; Veracity; Verification","Classification (of information); Data mining; Supervised learning; Verification; Credibility; Fake detection; Multimedia; Social media; Trust; Twitter; Veracity; Social networking (online)",2-s2.0-85029476245
"Geraldeli Rossi R., Andrade Lopes A.D., Oliveira Rezende S.","Using bipartite heterogeneous networks to speed up inductive semi-supervised learning and improve automatic text categorization",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020925655&doi=10.1016%2fj.knosys.2017.06.016&partnerID=40&md5=0d34368bf3b7306c3b29990d4218bf62","Due to the volume of texts available in digital form, the organization, management and knowledge extraction are laborious and frequently impossible to be handled. To automatically cope with these tasks, usually classification models are generated through supervised learning techniques. Unfortunately, this type of learning usually demands a huge human effort to label large volume of texts to build accurate classification models. Since collecting unlabeled texts is easy and inexpensive in several domains, the generation of classification models through inductive semi-supervised learning has been highlighted in recent years. Inductive semi-supervised learning allows to build a classification model using labeled and unlabeled texts. In this scenario, the goal is to augment the set of labeled documents with unlabeled documents to better discriminate class patterns. Hence, fewer texts must be previously labeled. However, semi-supervised learning algorithms that consider texts represented in a vector space model usually obtain unsatisfactory classification performances and are surpassed by semi-supervised learning algorithms that consider texts represented in a network. Nevertheless, despite the classification performances, effective approaches based on networks are generated through the similarities among documents and the classification of a new document are also based on the computation of similarities. This implies to set parameters and compute similarities to both generation the networks and classification of new documents. This approach is not feasible to generate fast responses and consequently to classify a huge volume of texts. In this article, we propose an approach to induce a classification model through semi-supervised learning considering text collections represented by bipartite heterogeneous networks. Bipartite networks are easily and quickly generated, leading to classification performance equivalent or better than other approaches based on network or vector space model and allows a fast classification of new documents. The results presented in this article demonstrate that the proposed approach is able to (i) speed up semi-supervised learning, (ii) speed up the classification of new documents and (iii) surpass classification performance of other existing inductive semi-supervised learning techniques. © 2017","Bipartite heterogeneous network; Graph-based learning; Label propagation; Text classification; Transductive learning","Classification (of information); Data mining; Education; Heterogeneous networks; Information retrieval systems; Learning algorithms; Learning systems; Text processing; Vector spaces; Automatic text categorization; Classification performance; Graph-based learning; Label propagation; Semi- supervised learning; Semi-supervised learning techniques; Text classification; Transductive learning; Supervised learning",2-s2.0-85020925655
"Dai Y., Wang Y., Xue J., Liu X., Liu B., Guo X.","Research of Segmentation Method on Image of Lingwu Long Jujubes Based on a New Extraction Model of Hue",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028940958&doi=10.1109%2fJSEN.2017.2731376&partnerID=40&md5=a3a33f4a061ec18f0c7164ecb60a2cc8","This paper studies on the image segmentation method of Lingwu Long Jujubes based on a new extraction model of hue to improve the accuracy of extracting images of Lingwu Long Jujubes. According to the characteristics that color components of Lingwu Long Jujubes in RGB color space have different distribution in a shadow environment or others, a extraction model of hue aiming at images of Lingwu Long Jujubes based on stage treatment of R component is built to extract hue information. And the difference between the target object and the background object is increased by using color difference. Then, the image is segmented by optimal threshold obtained by combining the maximum entropy and the mathematical criteria to achieve the adaptive adjustment of the segmentation threshold. Finally, the segmented image will be obtained through dealing with mathematical morphology. By comparing the segmentation effect of 30 Lingwu Long Jujubes images with artificial methods and other methods, it proves that the color image segmentation method of Lingwu Long Jujubes based on a new extraction model of hue has good effect to extract the object region. The accuracy of the segmentation rate is up to 92.6883%. The time that the algorithm run is 1.3107 s. © 2001-2012 IEEE.","Adaptive threshold; extraction model of hue; image segmentation; Lingwu Long Jujubes","Color; Colorimetry; Data mining; Extraction; Filtration; Image processing; Image segmentation; Mathematical morphology; Sensors; Adaptive thresholds; Colored noise; Extraction model; Histograms; Image color analysis; Lingwu Long Jujubes; Color image processing",2-s2.0-85028940958
"Hakawati M., Yacob Y., Raof R.A.A., Amir A., Mohammed J.M., Al-Hodiani E.S.","Conditional inclusion dependencies for improving xml data consistency",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029698118&partnerID=40&md5=77c2819d45bf620885e8fa9cb3549456","Without any doubt, XML data model considered the most dominant document type over the web with more than 60% of the total; nevertheless, their quality is not as expected. XML integrity constraint just as its relational counterpart played an important role to keep XML dataset as consistent as possible, but their ability to solve data quality issues is still intangible. The main reason is old-fashioned data dependencies introduced mainly to keep schema consistent rather than data consistent. In this paper, a conditional version of XML inclusion dependencies (XCIND) is proposed for data quality issues and justify the ability to use inclusion dependencies for data quality issues. XCIND Notations will extend XIND and shift its mission from schema design to data quality by providing pattern tableaus. Moreover, a set of minimal XCIND dependencies will be discovered and learned using a set of mining algorithms. Finally, the ability to use XCIND to detect data inconsistencies will be inspected using denial quires between mined rules and XML tree. © 2005 - Ongoing JATIT & LLS.","Data Cleaning; Data Quality; Integrity Constraints; XML",,2-s2.0-85029698118
"Samiri M.Y., Najib M., Elfazziki A., Abourraja M.N., Boudebous D., Bouain A.","APRICOIN: An Adaptive Approach for Prioritizing High-Risk Containers Inspections",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030641935&doi=10.1109%2fACCESS.2017.2746838&partnerID=40&md5=0ea4b02ddd60265193d13c97e8f1c593","Risk evaluation of the containers remains a difficult task, often due to incomplete or ambiguous information on containers. In addition, the evaluation process needs to be adapted on an ongoing basis to cope with emerging risk factors. Furthermore, high-risk container inspection is commonly hindered by a low inspection capacity, which leads to a major issue: how can we prioritize the container inspection if the number of suspect containers exceeds the inspection capacity? Container inspection prioritizing may be the answer. In this paper, we propose a novel approach for adaptively prioritizing container inspection (APRICOIN). First, we enhance the container information flow to alleviate the problem of incomplete information by proposing an enhanced container descriptive. Second, we introduce the APRICOIN algorithm, which combines frequent pattern mining and a fuzzy logic system, to assess the container's risk score. The frequent pattern growth algorithm is proposed to retrieve the key criteria for evaluating container risk. This is done through mining frequent criteria sets within the historic data set of container inspections by customs. The mined frequent criteria sets are used to assess fuzzy inference rules which are periodically readjusted to integrate new key criteria. Thereafter, the fuzzy logic system uses the obtained fuzzy inference rules to calculate a container's risk score. Our major contribution consists of providing a new adaptive approach for assessing a container's risk through combining frequent criteria mining and fuzzy logic. An illustrative study and a comparison with alternative approaches are performed to validate the proposed algorithm. © 2013 IEEE.","computational and artificial intelligence; intelligent container; marine transportation; risk analysis; Risk assessment","Computation theory; Computer circuits; Fuzzy inference; Fuzzy logic; Inference engines; Inspection; Inspection equipment; Risk analysis; Risk assessment; Standards; Terrorism; Computational and artificial intelligences; Container inspection; Frequent pattern growth; Frequent pattern mining; Fuzzy inference rules; Incomplete information; Inference algorithm; Marine transportation; Containers",2-s2.0-85030641935
"Dong L., Shu W., Sun D., Li X., Zhang L.","Pre-alarm system based on real-time monitoring and numerical simulation using internet of things and cloud computing for tailings dam in mines",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030636067&doi=10.1109%2fACCESS.2017.2753379&partnerID=40&md5=0acb1d0bf09bc3a7ea8ca9fe31ec3577","The tailings dam, a necessary facility to maintain the normal operation of mining enterprises, is a hazard source of human-caused debris flow with high potential energy. The real-time pre-alarm for the instability of tailings dam is vital to ensure the normal mining and safety of human lives and properties. Based on the internet of things and 5G wireless networks, the multiple and key information system of tailings dam is constructed using the sensor data, which include the stability indexes like phreatic line, reservoir water level, internal and external deformation of the tailings dam. The cloud platform is applied to predict the future state of the phreatic line based on real-time monitoring data, where the equation of phreatic line can be obtained. The numerical simulation model is established by considering the predicted equation of phreatic line, limit equilibrium state parameters, reservoir water level, and rainfall. Then, the safety factor, random reliability, and interval non-probabilistic reliability can be solved out through the cloud platform. Combined with the trend of real-time monitoring deformation, as well as calculated dynamic safety factor, random reliability, and interval non-probabilistic reliability, the stable or dangerous warning signals of tailings dam can be obtained by the remote real-time pre-alarm system. The main solved method for the key parameters and pre-alarm process are presented through a case study. It is proved that the pre-alarm system is efficient and real-time for the tailings dam stability with the integration and mutual validation of plenty of key information. OAPA","Cloud computing; Cloud computing; internet of things; interval non-probabilistic reliability; Mathematical model; Monitoring; pre-alarm system; Real-time systems; Safety; Stability analysis; tailings dam","Accident prevention; Alarm systems; Cloud computing; Dams; Deformation; Distributed computer systems; Embankment dams; Interactive computer systems; Internet of things; Mathematical models; Monitoring; Numerical models; Potential energy; Reliability; Reliability analysis; Reservoirs (water); Safety factor; Stability; System stability; Tailings; Water levels; Wireless sensor networks; Limit equilibrium; Mining enterprise; Non-probabilistic reliability; Normal operations; Real time monitoring; Reservoir water level; Stability analysis; Tailings dam; Real time systems",2-s2.0-85030636067
"Bohloul M.R., Arab Sadeghabadi M., Peyghambarzadeh S.M., Dehghani M.R.","CO2 absorption using aqueous solution of potassium carbonate: Experimental measurement and thermodynamic modeling",2017,"Fluid Phase Equilibria",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020293418&doi=10.1016%2fj.fluid.2017.05.023&partnerID=40&md5=d62b348c5105d4b453ac56e917672e9e","Potassium carbonate solution is a widely used solvent for CO2 removal due to its benefit in energy consumption and other economic concerns like degradation and corrosion problem. In this study, the solubility of CO2 in aqueous solutions of potassium carbonate was measured using pressure-decay method at temperatures of 313.15 K, 323.15 K, and 333.15 K, different pressures (up to 1.2 MPa), and different solution concentrations of 15 %wt., 20 %wt., and 30 %wt. Also, two equations of state were utilized to anticipate CO2 solubility in aqueous solution of potassium carbonate. The model is a combination of chemical equilibrium in the liquid phase and physical equilibrium between the liquid and vapor phases. For vapor-liquid equilibrium calculations, Peng-Robinson equation of state (PR-EOS) was used to present the fugacity coefficient in the vapor phase. The activity coefficient in the liquid phase was presented by Pitzer equation. Also, the values of the primitive interaction coefficients (λCO2,K+ and μCO2,CO2,K+) were optimized using the available literature data of the similar system. The results demonstrated that the performance of the thermodynamic modeling procedure was acceptable and the average absolute relative deviation (AARD) between the experimental and the predicted data was less than 2.7%. © 2017 Elsevier B.V.","Pitzer equation; Potassium carbonate solution; Primitive interaction coefficients; Solubility of CO2","Binary mixtures; Carbonation; Energy utilization; Equations of state; Equations of state of liquids; Liquids; Phase equilibria; Potassium; Solubility; Solution mining; Solutions; Average absolute relative deviations; Chemical equilibriums; Fugacity co-efficients; Interaction coefficient; Peng-Robinson equation of state; Pitzer equations; Solubility of CO; Solution concentration; Carbon dioxide",2-s2.0-85020293418
"Partin M.R., Gravely A.A., Burgess J.F., Jr., Haggstrom D.A., Lillie S.E., Nelson D.B., Nugent S.M., Shaukat A., Sultan S., Walter L.C., Burgess D.J.","Contribution of patient, physician, and environmental factors to demographic and health variation in colonoscopy follow-up for abnormal colorectal cancer screening test results",2017,"Cancer",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028998380&doi=10.1002%2fcncr.30765&partnerID=40&md5=4cccbb002a1b583ca91fa037bcb0139d","BACKGROUND: Patient, physician, and environmental factors were identified, and the authors examined the contribution of these factors to demographic and health variation in colonoscopy follow-up after a positive fecal occult blood test/fecal immunochemical test (FOBT/FIT) screening. METHODS: In total, 76,243 FOBT/FIT-positive patients were identified from 120 Veterans Health Administration (VHA) facilities between August 16, 2009 and March 20, 2011 and were followed for 6 months. Patient demographic (race/ethnicity, sex, age, marital status) and health characteristics (comorbidities), physician characteristics (training level, whether primary care provider) and behaviors (inappropriate FOBT/FIT screening), and environmental factors (geographic access, facility type) were identified from VHA administrative records. Patient behaviors (refusal, private sector colonoscopy use) were estimated with statistical text mining conducted on clinic notes, and follow-up predictors and adjusted rates were estimated using hierarchical logistic regression. RESULTS: Roughly 50% of individuals completed a colonoscopy at a VHA facility within 6 months. Age and comorbidity score were negatively associated with follow-up. Blacks were more likely to receive follow-up than whites. Environmental factors attenuated but did not fully account for these differences. Patient behaviors (refusal, private sector colonoscopy use) and physician behaviors (inappropriate screening) fully accounted for the small reverse race disparity and attenuated variation by age and comorbidity score. Patient behaviors (refusal and private sector colonoscopy use) contributed more to variation in follow-up rates than physician behaviors (inappropriate screening). CONCLUSIONS: In the VHA, blacks are more likely to receive colonoscopy follow-up for positive FOBT/FIT results than whites, and follow-up rates markedly decline with advancing age and comorbidity burden. Patient and physician behaviors explain race variation in follow-up rates and contribute to variation by age and comorbidity burden. Cancer 2017;123:3502-12. Published 2017. This article is a US Government work and is in the public domain in the USA. © 2017 American Cancer Society","colonoscopy; colorectal neoplasms; diagnostic services; early detection of cancer; health services accessibility; mass screening; veterans health","abnormal laboratory result; adult; age distribution; aged; Article; Asian; attitude to health; Black person; cancer screening; Caucasian; colonoscopy; colorectal cancer; comorbidity; demography; environmental factor; ethnic difference; female; follow up; general practitioner; health behavior; health care access; health care disparity; health care utilization; health status; health variation; Hispanic; human; inappropriate screening; major clinical study; male; marriage; middle aged; occult blood test; patient attitude; patient factor; physician attitude; physician factor; priority journal; private sector colonoscopy use; screening refusal; screening test; sex difference; social aspect; staff training; very elderly; age; analysis of variance; colonoscopy; Colorectal Neoplasms; doctor patient relation; early cancer diagnosis; environment; ethnology; factual database; multivariate analysis; occult blood; procedures; public hospital; retrospective study; risk assessment; statistics and numerical data; survival analysis; United States; Age Factors; Aged; Analysis of Variance; Colonoscopy; Colorectal Neoplasms; Databases, Factual; Early Detection of Cancer; Environment; Female; Follow-Up Studies; Health Behavior; Hospitals, Veterans; Humans; Male; Middle Aged; Multivariate Analysis; Occult Blood; Physician-Patient Relations; Retrospective Studies; Risk Assessment; Sex Factors; Survival Analysis; United States",2-s2.0-85028998380
"Lakshmi M.N.S., Radhika Y.","Effective approach for intrusion detection using KSVM and R",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029710950&partnerID=40&md5=e26cd99de01d543b64ac4cb8b77351a9","Nowadays there is an incredible escalation of the usage of computers over various networks and application domains, which in turn increases the security threats in terms of intrusions. An intrusion may happen either internally or externally and the traditional approaches used in intrusion detection are unable to meet the requirements of preventing and detecting an intrusion. For the detection of different attacks, intrusion detection occupied important work for the maintaining of privacy and reliability in network resource. In this paper, the methodologies of Data Mining has been used for increasing the performance in the IDS, and to handle Some of the problems like data Preparation, pre-processing of the data, data classification and Intrusion detection are being solved using different techniques like Dynamic Data Preparation (DDP), Hybrid Rule-based Pre-processing, Efficient Kernel Based Support Vector Machine (EKBSVM) and Decisive Neural net using R (DNR) respectively. The proposed techniques have produced better accuracy, specification and minimized the false alarm rate (FAR). © 2005 - Ongoing JATIT & LLS.","Anomaly Detection; Classification; DDoS; Intrusion Detection; Misuse Detection",,2-s2.0-85029710950
"Fang L., Fang H., Tian Y., Yang T., Zhao J.","The alliance relationship analysis of international terrorist organizations with link prediction",2017,"Physica A: Statistical Mechanics and its Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018783454&doi=10.1016%2fj.physa.2017.04.068&partnerID=40&md5=e48e10ccbb7139e1815b92e7dfdd6472","Terrorism is a huge public hazard of the international community. Alliances of terrorist organizations may cause more serious threat to national security and world peace. Understanding alliances between global terrorist organizations will facilitate more effective anti-terrorism collaboration between governments. Based on publicly available data, this study constructed a alliance network between terrorist organizations and analyzed the alliance relationships with link prediction. We proposed a novel index based on optimal weighted fusion of six similarity indices, in which the optimal weight is calculated by genetic algorithm. Our experimental results showed that this algorithm could achieve better results on the networks than other algorithms. Using this method, we successfully digged out 21 real terrorist organizations alliance from current data. Our experiment shows that this approach used for terrorist organizations alliance mining is effective and this study is expected to benefit the form of a more powerful anti-terrorism strategy. © 2017 Elsevier B.V.","Link prediction; Network analysis; Terrorist organization; The alliance relationship","Electric network analysis; Forecasting; Genetic algorithms; National security; Societies and institutions; Alliance networks; International community; International terrorists; Link prediction; Relationship analysis; Similarity indices; Terrorist organization; The alliance relationship; Terrorism",2-s2.0-85018783454
"Czikhardt R., Papco J., Bakon M., Liscak P., Ondrejka P., Zlocha M.","Ground stability monitoring of undermined and landslide prone areas by means of sentinel-1 multi-temporal InSAR, case study from Slovakia",2017,"Geosciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029799010&doi=10.3390%2fgeosciences7030087&partnerID=40&md5=6e29a458881555b11f9ec9c964f1f8a9","Multi-temporal synthetic aperture radar interferometry techniques (MT-InSAR) are nowadays a well-developed remote sensing tool for ground stability monitoring of areas afflicted by natural hazards. Its application capability has recently been emphasized by the Sentinel-1 satellite mission, providing extensive spatial coverage, regular temporal sampling and free data availability. We perform MT-InSAR analysis over the wider Upper Nitra region in Slovakia, utilizing all Sentinel-1 images acquired since November 2014 until March 2017. This region is notable for its extensive landslide susceptibility as well as intensive brown coal mining. We focus on two case studies, being impaired by recent activation of these geohazards, which caused serious damage to local structures. We incorporate a processing chain based on open-source tools, combining the current Sentinel Application Platform (SNAP) and Stanford Method for Persistent Scatterers (StaMPS) implementation. MT-InSAR results reveal substantial activity at both case studies, exceeding the annual displacement velocities of 30 mm/year. Moreover, our observations are validated and their accuracy is confirmed via comparison with ground truth data from borehole inclinometers and terrestrial levelling. Detected displacement time series provide valuable insight into the spatio-temporal evolution of corresponding deformation phenomena and are thus complementary to conventional terrestrial monitoring techniques. At the same time, they not only demonstrate the feasibility of MT-InSAR for the assessment of remediation works, but also constitute the possibility of operational monitoring and routine landslide inventory updates, regarding the free Sentinel-1 data. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Land subsidence; Landslide; Multi-temporal InSAR; Sentinel-1; Slope deformation; Undermining",,2-s2.0-85029799010
"Ha J.-H., Abbas Bukhari S.Z., Lee J., Song I.-H.","The membrane properties of alumina-coated alumina support layers and alumina-coated diatomite–kaolin composite support layers",2017,"Advances in Applied Ceramics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029508235&doi=10.1080%2f17436753.2017.1369658&partnerID=40&md5=1d416941170e8311af8a5dfb2958babe","Porous ceramic membranes are a current research focus because of their outstanding thermal and chemical stability. Recent research has utilised inexpensive natural materials such as diatomite to reduce the expense of these porous ceramic membranes. However, insufficient data exist for microfiltration applications using the diatomite-based membranes. The measured membrane properties of alumina-coated alumina support layers and alumina-coated diatomite–kaolin composite support layers have been compared. These experiments have been used to determine whether the average pore size could be reduced effectively by controlling the thickness of the alumina coating layer, while maintaining acceptable water permeability. The membrane properties of the alumina-coated alumina support layers and the alumina-coated diatomite–kaolin composite support layers were examined using the scanning electron microscopy, mercury porosimetry, and a dead-end microfiltration system. © 2017 Institute of Materials, Minerals and Mining. Published by Taylor & Francis on behalf of the Institute.","Alumina coating layer; alumina support layer; diatomite–kaolin composite support layer; microfiltration; pore characteristics","Ceramic materials; Ceramic membranes; Chemical stability; Coatings; Kaolin; Membranes; Microfiltration; Pore size; Scanning electron microscopy; Alumina coating; Alumina support; Composite support; Dead end microfiltration; Membrane properties; Pore characteristics; Porous ceramic membranes; Thermal and chemical stabilities; Alumina",2-s2.0-85029508235
"Barale V., Dusart J., Assouline M., Niceta F.","European Atlas of the Seas: “a picture is worth a thousand words”",2017,"Journal of Coastal Conservation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029470492&doi=10.1007%2fs11852-017-0560-2&partnerID=40&md5=a0ea054d0c2c7219fa27a9d9e6d9afda","The European Atlas of the Seas offers a snapshot of environmental and socio-economic features that characterize the coastal and marine environment. The latest release (Version 4) addresses the public in general, but also non-specialist experts involved with environmental issues, human activities or policies related to Europe’s coasts and seas. The information content of the Atlas comprises a series of geographical layers, subdivided in “background maps”, “thematic maps” (i.e. maritime Europe, natural setting, sea bottom, sea level rise, security, transport, tourism, energy, wind, fisheries and fish consumption) and “do-it-yourself maps” (dealing with marine knowledge, nature and environment, socio-economics, fisheries, aquaculture, transport, energy, sea bed mining, coastal tourism, Maritime Spatial Planning, integrated maritime surveillance, and international ocean governance). All maps follow consistent cartographic rules and can be extracted for external use. The Atlas database is updated regularly, but historical data remain accessible after the updates, so that time series may be constructed. Tools for map exploration and combination can be used to combine together more layers, providing professional users with analysis and interpretation capabilities, to couple data into graphical indicators. The Atlas aims to supports also policy making, on marine environment, maritime issues and economic sectors, both within and outside the European Institutions (e.g. on Common Fisheries Policy or Maritime Spatial Planning). Further, it expands the same support to near-coastal issues and matters related to land-sea interactions. The web application for accessing Atlas contents offers links to other Marine Information Systems, and is available to a broad audience from computers, tablets and mobile devices. © 2017 The Author(s)","Atlas; Coastal and marine environment; European seas; Maritime socio-economics",,2-s2.0-85029470492
"Alhaj B.A., Maghari A.Y.A.","Predicting User Entries by Using Data Mining Algorithms",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032264727&doi=10.1109%2fPICICT.2017.24&partnerID=40&md5=17ed6cdcfdae4168c03518816bcd21ef","The information systems are widely spread in most official institutions, and become certified in all areas of our life such as education, health and entertainment. Usability is one of the most important factors, which encourages users to deal with these systems or refuse it. Data mining is the process of finding correlations or patterns among dozens of fields in large relational databases. In this paper we analyze the stored data in database of Palestinian Government decisions system in order to study the relationship between some attributes. Accordingly, we can find patterns that help us to make the system more user-friendly by offering suggestions to the users during data entry process. Naive Bayes, Rule Induction, K-NN, and Decision Tree methods are applied to the stored data in order to produce a prediction model that predicts entries to the user during the entry process, which can make the entry system more user-friendly. The experiment result shows the Naïve Bayes is the best model among the other techniques by achieving the highest accuracy of 68.41%. Future efforts can apply this model in the Government decisions system of Palestinian Ministers Council in Gaza. © 2017 IEEE.","Data Mining; Entry Prediction; Recommender Systems; Usability","Decision trees; Forecasting; Recommender systems; Trees (mathematics); Data mining algorithm; Decision tree method; Government decisions; Official institutions; Prediction model; Relational Database; Rule induction; Usability; Data mining",2-s2.0-85032264727
"Dhanaseelan R., Jeya Sutha M.","Diagnosis of coronary artery disease using an efficient hash table based closed frequent itemsets mining",2017,"Medical and Biological Engineering and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029426001&doi=10.1007%2fs11517-017-1719-6&partnerID=40&md5=fbe2f141377eee88ef07e230acb448fd","This paper proposes an efficient hash table based closed frequent itemsets (HCFI) mining algorithm to envisage coronary artery disease early. HCFI algorithm generates closed frequent itemsets efficiently by performing intersection operation on transaction id’s of itemset without considering the name of item/itemset. The employed hash table reduces search efficiency to O(1) or constant time. HCFI algorithm is applied on the UCI (University of California, Irvine) Cleveland dataset, a biological database of cardiovascular disease to generate closed frequent itemsets on the dataset. The findings of HCFI algorithm are (1) it determines a set of distinguished features to differentiate a ‘healthy’ and a ‘sick’ class. The features such as heart status being normal, oldpeak being less than or equal to 1.2, slope being up, number of vessels colored being zero, absence of exercise-induced angina, maximum heart rate achieved between 151 and 180 are referred as ‘healthy’ class. The features like chest pain are being asymptomatic, heart-status being reversible defect, slope being flat, and presence of exercise-induced-angina and serum cholesterol being greater than 240 indicate a presumption of heart disease to both genders. (2) It predicts that females have less chance of coronary heart disease than males. This algorithm is also compared with two other state-of-the-art-algorithms ‘NAFCP’ (N-list based algorithm for mining frequent closed patterns) and ‘PredictiveApriori’ to show the effectiveness of the proposed algorithm. © 2017 International Federation for Medical and Biological Engineering","Association rule mining; Closed frequent Itemsets; Coronary artery disease; Hash table; Support and confidence","Bioinformatics; Cardiology; Data mining; Data structures; Diagnosis; Heart; Cardio-vascular disease; Closed frequent itemsets; Coronary artery disease; Hash table; Intersection operation; State-of-the-art algorithms; Support and confidence; University of California; Diseases",2-s2.0-85029426001
"Abed A.A., El-Halees A.M.","Detecting Subjectivity in Staff Perfomance Appraisals by Using Text Mining: Teachers Appraisals of Palestinian Government Case Study",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032260329&doi=10.1109%2fPICICT.2017.25&partnerID=40&md5=fb86c75c7b3d34b9581777d91281990b","The objective of this work is to propose a text mining based approach that supports Human Resources Management (HRM) in detecting subjectivity in staff performance appraisals. The approach detects three domain-driven clues of subjectivity in reviews, where each clue represents a level of subjectivity. A considerable effort has been directed to detecting subjectivity in opinion reviews. However, to the best of our knowledge, there is no previous work that detects subjectivity in staff appraisals. For proving our approach, we applied it to the teachers' appraisals of the Palestinian government. According to our experiments, we found that the approach is effective regarding our evaluations, where we used: expert opinion, precision, recall, accuracy and F-measure. In the first level, we reached the F-measure of 88%, in the second level, we used expert staff's opinion, where they decided the percentage of duplication to be 85% and in the third level, we achieved the best average F-measure of 84%. © 2017 IEEE.","Human Resources Management; Opinion Mining; Staff Appraisal; Subjectivity Detection; Text Mining","Human resource management; Personnel; Teaching; Expert opinion; Human resources management; Opinion mining; Performance appraisal; Second level; Staff Appraisal; Text mining; Third level; Data mining",2-s2.0-85032260329
"Ahmed W.A.M., El-Halees A.M.","Arabic Opinion Mining Using Parallel Decision Trees",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032262583&doi=10.1109%2fPICICT.2017.28&partnerID=40&md5=e48cf6460a8347c6206c4c53af610c33","Opinion mining is an interested area of research, which epitomize the customer reviews of a product or service and express whether the opinions are positive or negative. Various methods have been proposed as classifiers for opinion mining such as Naïve Bayesian, and Support vector machine, these methods classify opinion without giving us the reasons about why the instance opinion is classified to certain class. Therefore, in our work, we investigate opinion mining of Arabic text at the document level, by applying decision trees classification classifier to have clear, understandable rule, also we apply parallel decision trees classifiers to have efficient results. We applied parallel decision trees on two Arabic corpus of text documents by using parallel implementation of RapidMiner tools. In case of applying parallel decision tree family on OCA we get the best results of accuracy (93.83%), f-measure (93.22) and consumed time 42 Sec at thread 4, one of the resulted rule is Urdu language lines. In case of applying parallel decision tree family on BHA we get the best results of accuracy (90.63%), f-measure (82.29) and consumed time 219 Sec at thread 4, one of the resulted rule is Urdu language lines. © 2017 IEEE.","Arabic text; Classification; Decision tree; Machine learning; Opinion Extraction; Opinion mining; Parallel Decision Tree; Sentiment Analysis; Sentiment Classification","Classification (of information); Data mining; Information retrieval systems; Learning systems; Text processing; Arabic texts; Opinion extraction; Opinion mining; Parallel decision trees; Sentiment analysis; Sentiment classification; Decision trees",2-s2.0-85032262583
"El-Halees A.M.","Arabic Opinion Mining Using Distributed Representations of Documents",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032266298&doi=10.1109%2fPICICT.2017.15&partnerID=40&md5=ba64bfcc54dc3699042b1f1bc5a050d1","Nowadays, many people express their opinions using user generated contains such as social media, forums and reviews. Opinion mining is a field of study that extracts sentiments from user generated contents. Because of the complexity of the Arabic language, extracting those opinions are challenging. Better representation of reviews can help to improve extraction of opinions. The traditional way of representing opinion documents is using Bag-of-Words where the word is presented in fixed-length. The problem of this presentation is that it loses the order of the word and it ignores grammatical structure and lexicon-dependent. To overcome these limitations, distributed representations can be employed. It is based on learning vector representations of words, which also called 'word embeddings'. It can make the performance of natural language processing tasks have better performance with the help of learning algorithms. This representation uses neural networks and makes the learned vectors explicitly encode many linguistic patterns. In this study, we used distributed representations for Arabic opinion mining and compare it with Bag of Words (BOW) representation. We applied them on four benchmark datasets. Then, we used four machine learning methods which are Support Vector Machine, Logistic Regression and Random Forest. Using f-measure metric, we found that, in all datasets and all methods we used in our experiment, the distributed representations have better performance than bag-of-words representation. © 2017 IEEE.","Arabic language; Bag-of-words; Distributed representations; Opinion mining","Data mining; Decision trees; Learning systems; Natural language processing systems; Arabic languages; Bag of words; Distributed representation; Grammatical structure; Logistic regressions; Machine learning methods; Opinion mining; User-generated content; Learning algorithms",2-s2.0-85032266298
"Wang C., Meng X., Guo Q., Weng Z., Yang C.","Automating Characterization Deployment in Distributed Data Stream Management Systems",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030634583&doi=10.1109%2fTKDE.2017.2751606&partnerID=40&md5=5e56d3f78a3af768e1e37a7b828b20f9","Distributed data stream management systems (DDSMS) are usually composed of upper layer relational query systems (RQS) and lower layer stream processing systems (SPS). When users submit new queries to RQS, a query planner needs to be converted into a directed acyclic graph (DAG) consisting of tasks which are running on SPS. Based on different query requests and data stream properties, SPS need to configure different deployments strategies. However, how to dynamically predict deployment configurations of SPS to ensure the processing throughput and low resource usage is a great challenge. This article presents OrientStream, a framework for automating characterization deployment in DDSMS using incremental machine learning techniques. By introducing the data-level, query plan-level, operator-level and cluster-level's four-level feature extraction mechanism, we firstly use the different query workloads as training sets to predict the resource usage by DDSMS, and select the optimal resource configuration from candidate settings based on the current query requests and stream properties, then migrate the operator state by introducing dynamic reconfiguration. Finally, we validate our approach on the open source SPS--Storm. In view of the application scenarios with long monitoring cycle and non-frequent data fluctuation, Experiments show that OrientStream can reduce CPU usage of 8%-15% and memory usage of 38%-48% respectively. IEEE","Delta-sigma modulation; Dynamic scheduling; Incremental Learning; Modeling and Prediction; Predictive models; Real-time systems; Relational Query System; Storms; Stream Processing System; Topology; Training","Data mining; Database systems; Delta sigma modulation; Digital to analog conversion; Directed graphs; Dynamic models; Feature extraction; Forecasting; Information management; Interactive computer systems; Learning systems; Modulators; Personnel training; Real time systems; Storms; Topology; Dynamic scheduling; Incremental learning; Modeling and predictions; Predictive models; Relational queries; Stream processing systems; Search engines",2-s2.0-85030634583
"Baraka R.S., Breem S.N.A.","Automatic Arabic Text Summarization for Large Scale Multiple Documents Using Genetic Algorithm and MapReduce",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032293366&doi=10.1109%2fPICICT.2017.32&partnerID=40&md5=146aecda08700dc5db08080a4a4dd8ab","Multi document summarization focuses on extracting the most significant information from a collection of textual documents. Most summarization techniques require the data to be centralized, which may not be feasible in many cases due to computational and storage limitations. The huge increase of data emerging by the progress of technology and the various sources makes automatic text summarization of large scale of data a challenging task. We propose an approach for automatic text summarization of large scale Arabic multiple documents using Genetic algorithm and MapReduce parallel programming model. The approach insures scalability, speed and accuracy in summary generation. It eliminates sentence redundancy and increases readability and cohesion factors between the sentences of summaries. The experiments resulted in acceptable precision and recall scores. This indicates that the system successfully identifies the most important sentences. In Addition to all to that, the approach provided up to 10x speedup score, which is faster than on a single machine. Therefore, it can deal with large-scale datasets successfully. Finally, the efficiency score of the proposed approach indicates that the large data set utilizes the available resources up to 62%. © 2017 IEEE.","Hadoop; MapReduce; Parallel Genetic Algorithm; Text Mining; Text Summarization","Data mining; Digital storage; Genetic algorithms; Natural language processing systems; Parallel programming; Hadoop; Map-reduce; Parallel genetic algorithms; Text mining; Text summarization; Text processing",2-s2.0-85032293366
"Wang L., Du Y., Liu W.","Aligning observed and modelled behaviour based on workflow decomposition",2017,"Enterprise Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973161342&doi=10.1080%2f17517575.2016.1193633&partnerID=40&md5=3bb0aca3a41c56114c446286732a2188","When business processes are mostly supported by information systems, the availability of event logs generated from these systems, as well as the requirement of appropriate process models are increasing. Business processes can be discovered, monitored and enhanced by extracting process-related information. However, some events cannot be correctly identified because of the explosion of the amount of event logs. Therefore, a new process mining technique is proposed based on a workflow decomposition method in this paper. Petri nets (PNs) are used to describe business processes, and then conformance checking of event logs and process models is investigated. A decomposition approach is proposed to divide large process models and event logs into several separate parts that can be analysed independently; while an alignment approach based on a state equation method in PN theory enhances the performance of conformance checking. Both approaches are implemented in programmable read-only memory (ProM). The correctness and effectiveness of the proposed methods are illustrated through experiments. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","alignment; conformance checking; Petri nets; Process mining; workflow decomposition","Alignment; Data mining; Petri nets; Business Process; Conformance checking; Decomposition approach; Decomposition methods; Petri nets (PNs); Process mining; Process model; State equations; Equations of state",2-s2.0-84973161342
"Zhang Z., Jiang W., Qin J., Zhang L., Li F., Zhang M., Yan S.","Jointly Learning Structured Analysis Discriminative Dictionary and Analysis Multiclass Classifier",2017,"IEEE Transactions on Neural Networks and Learning Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030627070&doi=10.1109%2fTNNLS.2017.2740224&partnerID=40&md5=09c417d21913d7350b3a6f52f6e81f9a","In this paper, we propose an analysis mechanism-based structured analysis discriminative dictionary learning (ADDL) framework. The ADDL seamlessly integrates ADDL, analysis representation, and analysis classifier training into a unified model. The applied analysis mechanism can make sure that the learned dictionaries, representations, and linear classifiers over different classes are independent and discriminating as much as possible. The dictionary is obtained by minimizing a reconstruction error and an analytical incoherence promoting term that encourages the subdictionaries associated with different classes to be independent. To obtain the representation coefficients, ADDL imposes a sparse l2,1-norm constraint on the coding coefficients instead of using l&#x2080; or l&#x2081; norm, since the l&#x2080;- or l&#x2081;-norm constraint applied in most existing DL criteria makes the training phase time consuming. The code-extraction projection that bridges data with the sparse codes by extracting special features from the given samples is calculated via minimizing a sparse code approximation term. Then we compute a linear classifier based on the approximated sparse codes by an analysis mechanism to simultaneously consider the classification and representation powers. Thus, the classification approach of our model is very efficient, because it can avoid the extra time-consuming sparse reconstruction process with trained dictionary for each new test data as most existing DL algorithms. Simulations on real image databases demonstrate that our ADDL model can obtain superior performance over other state of the arts. IEEE","Analysis multiclass classifier; analytical incoherence promotion; Dictionaries; Encoding; Image reconstruction; Learning systems; Machine learning; projective sparse representation (SR); structured analysis discriminative dictionary learning (ADDL).; Training; Training data","Codes (symbols); Data mining; Encoding (symbols); Glossaries; Image reconstruction; Learning systems; Personnel training; analytical incoherence promotion; Discriminative dictionaries; Multi-class classifier; Sparse representation; Training data; Classification (of information)",2-s2.0-85030627070
"Salas A., Georgakis P., Nwagboso C., Ammari A., Petalas I.","Traffic event detection framework using social media",2017,"2017 IEEE International Conference on Smart Grid and Smart Cities, ICSGSC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032362878&doi=10.1109%2fICSGSC.2017.8038595&partnerID=40&md5=a25953e4ebc09913df8e87be8d207515","Traffic incidents are one of the leading causes of non-recurrent traffic congestions. By detecting these incidents on time, traffic management agencies can activate strategies to ease congestion and travelers can plan their trip by taking into consideration these factors. In recent years, there has been an increasing interest in Twitter because of the real-time nature of its data. Twitter has been used as a way of predicting revenues, accidents, natural disasters, and traffic. This paper proposes a framework for the real-time detection of traffic events using Twitter data. The methodology consists of a text classification algorithm to identify traffic related tweets. These traffic messages are then geolocated and further classified into positive, negative, or neutral class using sentiment analysis. In addition, stress and relaxation strength detection is performed, with the purpose of further analyzing user emotions within the tweet. Future work will be carried out to implement the proposed framework in the West Midlands area, United Kingdom. © 2017 IEEE.","Incident Detection; Intelligent Transport Systems; Sentiment Analysis; Smart Cities; Twitter","Classification (of information); Data mining; Disasters; Electric power transmission networks; Intelligent systems; Smart city; Smart power grids; Social networking (online); Text processing; Traffic control; Incident detection; Intelligent transport systems; Real-time detection; Relaxation strength; Sentiment analysis; Text classification; Traffic management; Twitter; Traffic congestion",2-s2.0-85032362878
"Alattar A.M., Oudah M.A.H.","An Adaptive Active Contour Model for Building Extraction from Aerial Images",2017,"Proceedings - 2017 Palestinian International Conference on Information and Communication Technology, PICICT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032290254&doi=10.1109%2fPICICT.2017.22&partnerID=40&md5=c8d5761eef8f606b7f7bc7852ddcb65f","Building extraction from aerial images is one of the recent topics of remote sensing used in many applications such as urban planning, disaster management, military planning, and Geographic Information Systems (GIS). One of the commonly used approaches in building extraction is Active Contour Model (ACM), also called snake model, for its ability to extract contours of structured and unstructured shapes of objects. However, using traditional ACM model in building extraction faces the problem of narrowly concave contour regions. In this research, we propose to solve the deep concavities problem with the use of a concavity index which adaptively determines the rigidity coefficient of the snake points located in the deeply narrow segments of the contour. Our adaptive model was tested on different sets of buildings extracted from aerial images. Results were evaluated using two evaluation approaches. One in terms of accuracy, precision and recall, and the other in terms of the Error Distance Ratio (ERd) which is the average ratio of distance between each snake point and the true edge map point (by pixels). Result were compared with the GVF snake model in terms of both accuracy and execution time. © 2017 IEEE.","ACM Model; Building extraction; GVF Model; Snake Model","Buildings; Data mining; Disaster prevention; Disasters; Extraction; Information management; Military applications; Military mapping; Military photography; Remote sensing; ACM model; Active contour model; Building extraction; Evaluation approach; GVF model; Precision and recall; Rigidity coefficient; Snake model; Image processing",2-s2.0-85032290254
"Horne B.D., Adali S., Sikdar S.","Identifying the social signals that drive online discussions: A case study of reddit communities",2017,"2017 26th International Conference on Computer Communications and Networks, ICCCN 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032277746&doi=10.1109%2fICCCN.2017.8038388&partnerID=40&md5=7521c1428d3dd98e99b7657a01165bbf","Increasingly people form opinions based on information they consume on online social media. As a result, it is crucial to understand what type of content attracts people's attention on social media and drive discussions. In this paper we focus on online discussions. Can we predict which comments and what content gets the highest attention in an online discussion? How does this content differ from community to community? To accomplish this, we undertake a unique study of Reddit involving a large sample comments from 11 popular subreddits with different properties. We introduce a large number of sentiment, relevance, content analysis features including some novel features customized to reddit. Through a comparative analysis of the chosen subreddits, we show that our models are correctly able to retrieve top replies under a post with great precision. In addition, we explain our findings with a detailed analysis of what distinguishes high scoring posts in different communities that differ along the dimensions of the specificity of topic and style, audience and level of moderation. © 2017 IEEE.",,"Computer networks; Data mining; Comparative analysis; Content analysis; Online discussions; Online social medias; Social media; Social signals; Social networking (online)",2-s2.0-85032277746
"Le B.T., Xiao D., Okello D., He D., Xu J., Doan T.T.","Coal exploration technology based on visible-infrared spectra and remote sensing data",2017,"Spectroscopy Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029454588&doi=10.1080%2f00387010.2017.1354889&partnerID=40&md5=2249dd5b23ef9f0ba8e28e1d9d205105","In the modern society, coal is used as the main source of energy. In this paper, based on the theory of the remote sensing, the distributions of the coal mine area through the satellite imagery are measured. First, the satellite pictures of coal mining regions in Quangninh of Vietnam and Huolinhe of China were gathered as the experimental data. Second, spectrometer was used to measure the spectral data of the coal samples of these two regions. The measured data provide comprehensive and accurate spectral characteristics of the coal. Then the classification model can be built by the improved extreme learning machines algorithm based on the measured data and the remote sensing data. Finally, the distribution image of the coal mine area is obtained accurately based on the classification model. © 2017 Taylor & Francis.","Coal; extreme learning machines; remote sensing; visible-infrared spectra",,2-s2.0-85029454588
"Partanen J.I., Partanen L.J., Vahteristo K.P.","Traceable Thermodynamic Quantities for Dilute Aqueous Sodium Chloride Solutions at Temperatures from (0 to 80) °c. Part 1. Activity Coefficient, Osmotic Coefficient, and the Quantities Associated with the Partial Molar Enthalpy",2017,"Journal of Chemical and Engineering Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029509529&doi=10.1021%2facs.jced.7b00091&partnerID=40&md5=91bab0365e8eed276df3bb8f02ce40de","We present fully traceable two-parameter Hückel equations (with parameters B and b1) for the activity coefficient of sodium chloride and for the osmotic coefficient of water in aqueous NaCl solutions at temperatures from (0 to 80) °C. These equations apply within experimental error to all thermodynamic data available for these solutions at least up a molality of 0.2 mol·kg-1. In our previous study (J. Chem. Eng. Data 2016, 61, 286-306), these equations were successfully tested against the literature results of electrochemical, isopiestic, and cryoscopic measurements usually in the temperature range from (0 to 25) °C. There, a constant value was employed for B, whereas a linear model with respect to the temperature was utilized for b1. The linear model was determined from the values of b1 at 0 °C and at 25 °C obtained from freezing-point depression data and from isopiestic and cell-potential difference data, respectively. In the present study, these two b1 values are utilized alongside the constant value of parameter B but a new quadratic model is presented for the temperature dependence of b1. The third data point required for this model is obtained from the direct vapor pressure measurements of Gibbard et al. (J. Chem. Eng. Data 1974, 19, 281-288) at 75 °C. The results obtained with this quadratic equation for b1 agree well with the test results of the linear model in the previous paper (see the citation above) up to 25 °C. The most important new test results above that temperature are reported here. Our quadratic model has additionally been tested with all the high-precision calorimetric data available in the literature for NaCl solutions. In this first part (Part 1) of the study, the test results from the thermodynamic quantities associated with partial molar enthalpy are reported. In the forthcoming second part (Part 2) of the study, the results of the quantities associated with the heat capacity of NaCl solutions will be considered. In the tests of these two parts, all calculations dealing with calorimetric data are performed in a new way. Both the calorimetric data and the vapor pressure data (from both direct and isopiestic measurements) can be predicted using the new Hückel equations within experimental error in dilute NaCl solutions from (0 to 80) °C. For comparison, also other Hückel models are considered and at best these apply up to the molality of the saturated NaCl solution at various temperatures. Following the success of the new models, new values for the activity coefficients, osmotic coefficients, relative apparent molar enthalpies, and relative partial molar enthalpies for NaCl solutions at rounded molalities are reported at the end of this Article. We have good reasons to believe that the new values contain the most reliable ones available for the given thermodynamic quantities. (Graph Presented). © 2017 American Chemical Society.",,"Activity coefficients; Calorimetry; Enthalpy; Hydrostatic pressure; Osmosis; Pressure measurement; Sodium; Sodium chloride; Specific heat; Temperature; Temperature distribution; Thermodynamics; Vapor pressure; Aqueous NaCl solutions; Aqueous sodium chloride solutions; Freezing-point depression; Partial molar enthalpy; Relative apparent molar enthalpy; Relative partial molar enthalpy; Temperature dependence; Thermodynamic quantities; Solution mining",2-s2.0-85029509529
"Dwyer T., Martin F.","Sharing News Online: Social media news analytics and their implications for media pluralism policies",2017,"Digital Journalism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024505594&doi=10.1080%2f21670811.2017.1338527&partnerID=40&md5=077f64553d7fd906e614aff90c33f844","While the term churnalism in this special issue speaks to the negative impacts of extensive news re-use, one pervasive form of news redistribution, social media news sharing, has had the more positive connotations of creative engagement, political participation and cross-promotion. Yet this reading of commendary culture is, as José Van Dijck suggests, largely ideological, anchored in the Silicon Valley rhetorics that support the data capture, data mining and behavioural advertising activities of social media businesses. Our paper critically analyses journalisms’ increasing dependence on social media news-sharing analytics and the implications for news media diversity. We first examine how sharing analytics function as a novel form of news commodification, influencing reporting and editorial practices, with possible implications for news media diversity. We then map the news-sharing ecology, looking at the interlinked business models, ownership patterns and industrial power of social news intermediaries such as Facebook, Twitter, Gigya, Chartbeat and Newswhip, and how these relationships reinforce the significance of analytics to news production. Finally, we propose how the use of news analytics could also help in tracking the changes wrought by social media news sharing, particularly in developing a media policy framework for monitoring digital news diversity and pluralism. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","audience metrics; media diversity and pluralism; news analytics; news commodification; social media news sharing",,2-s2.0-85024505594
"Suh Y., Kim G., Seol H.","Roadmapping for prioritisation of smartphone feature requirements based on user experiences",2017,"Technology Analysis and Strategic Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997605014&doi=10.1080%2f09537325.2016.1259468&partnerID=40&md5=0a5b92056e922e48223fb6845c3de2dc","Since the life cycle of smartphones is becoming shorter, users are demanding new and improved smartphone features for updated smartphones, but it is difficult to know which features are the most important to users. In Internet user communities, users increasingly review smartphone functions and share information about their experiences such as complaints, problems, and satisfaction, and these user experiences have been fruitful sources for manufacturers that lead to smartphone improvements. Focusing on these user experiences, this paper proposes a systematic roadmapping process including prioritisation of smartphone feature requirements. By prioritising smartphone feature requirements, new and improved versions of smartphones that reflect user needs can be planned based on a product–market roadmap. The systematic approach consists of three parts: user-driven quality function deployment (QFD), a frequent pattern (FP)-tree algorithm, and a product–market roadmap. First, we collected data extracted from text mining in Internet user communities to construct a user-driven QFD. Second, using user experiences related to smartphone features, we applied the FP-tree algorithm to algorithmically derive a priority list of smartphone feature requirements. Finally, based on the result of the FP-tree of smartphones, we proposed guidelines to construct a product–market roadmap. It is expected that a versioning strategy can be formulated through this prioritised product–market roadmap. Furthermore, covering each step from data collection to roadmapping, this study suggests a systematic process for prioritisation of smartphone feature requirements based on user experiences. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","FP-tree; prioritisation; roadmapping; smartphones; User experience","algorithm; guideline; information and communication technology; Internet; mobile phone; technology adoption",2-s2.0-84997605014
"Jo J., Purushotham P.M., Han K., Lee H.-R., Nah G., Kang B.-C.","Development of a genetic map for onion (Allium cepa L.) using reference-free genotyping-by-sequencing and SNP assays",2017,"Frontiers in Plant Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030862762&doi=10.3389%2ffpls.2017.01606&partnerID=40&md5=ec35921793550da0c5fcbdee74f0c76f","Single nucleotide polymorphisms (SNPs) play important roles as molecular markers in plant genomics and breeding studies. Although onion (Allium cepa L.) is an important crop globally, relatively few molecular marker resources have been reported due to its large genome and high heterozygosity. Genotyping-by-sequencing (GBS) offers a greater degree of complexity reduction followed by concurrent SNP discovery and genotyping for species with complex genomes. In this study, GBS was employed for SNP mining in onion, which currently lacks a reference genome. A segregating F2 population, derived from a cross between ‘NW-001’ and ‘NW-002,’ as well as multiple parental lines were used for GBS analysis. A total of 56.15 Gbp of raw sequence data were generated and 1,851,428 SNPs were identified from the de novo assembled contigs. Stringent filtering resulted in 10,091 high-fidelity SNP markers. Robust SNPs that satisfied the segregation ratio criteria and with even distribution in the mapping population were used to construct an onion genetic map. The final map contained eight linkage groups and spanned a genetic length of 1,383 centiMorgans (cM), with an average marker interval of 8.08 cM. These robust SNPs were further analyzed using the high-throughput Fluidigm platform for marker validation. This is the first study in onion to develop genome-wide SNPs using GBS. The resulting SNP markers and developed linkage map will be valuable tools for genetic mapping of important agronomic traits and marker-assisted selection in onion breeding programs. © 2017 Jo, Purushotham, Han, Lee, Nah and Kang.","Fluidigm; Genotyping-by-sequencing; Linkage map; Onion; Single nucleotide polymorphism (SNP)",,2-s2.0-85030862762
"Al-Fatlawi A.H., Fatlawi H.K., Ling S.H.","Recognition physical activities with optimal number of wearable sensors using data mining algorithms and deep belief network",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032206916&doi=10.1109%2fEMBC.2017.8037456&partnerID=40&md5=0954e5fe666e880d8a19f8addecfb208","Daily physical activities monitoring is benefiting the health care field in several ways, in particular with the development of the wearable sensors. This paper adopts effective ways to calculate the optimal number of the necessary sensors and to build a reliable and a high accuracy monitoring system. Three data mining algorithms, namely Decision Tree, Random Forest and PART Algorithm, have been applied for the sensors selection process. Furthermore, the deep belief network (DBN) has been investigated to recognise 33 physical activities effectively. The results indicated that the proposed method is reliable with an overall accuracy of 96.52% and the number of sensors is minimised from nine to six sensors. © 2017 IEEE.","Data mining; Decision Tree; Deep Belief Network; Features selection; Monitoring Physical activities; PART algorithm; Random Forest",,2-s2.0-85032206916
"Cerone A.","Model mining: Integrating data analytics, modelling and verification",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029435950&doi=10.1007%2fs10844-017-0474-3&partnerID=40&md5=b491b1328400d6fe037d6229441d2943","Process mining techniques have been developed in the ambit of business process management to extract information from event logs consisting of activities and then produce a graphical representation of the process control flow, detect relations between components involved in the process and infer data dependencies between process activities. These process characterisations allow the analyst to discover an annotated visual representation of the conceptual model or the performance model of the process, check conformance with an a priori model to detect deviations and extend the a priori model with quantitative information such as frequencies and performance data. However, a process model yielded by process mining techniques is more similar to a representation of the process behaviour rather than an actual model of the process: it often consists of a huge number of states and interconnections between them, thus resulting in a spaghetti-like net which is hard to interpret or even read. In this paper we propose a novel technique, which we call model mining, to derive an abstract but concise and functionally structured model from event logs. Such a model is not a representation of the unfolded behaviour, but comprises, instead, a set of formal rules for generating the system behaviour, thus supporting more powerful predictive capabilities. The set of rules can be either inferred directly from the events logs (constructive mining) or refined by sifting a plausible a priori model using the event logs as a sieve until a reasonably concise model is achieved (refinement mining). We use rewriting logic as the formal framework in which to perform model mining and implement our framework using the Maude rewrite system. Once the final formal model is attained, it can be used, within the same rewriting logic framework, to predict future evolutions of the behaviour through simulation, to carry out further validation or to analyse properties through model checking. Finally, we illustrate our approach on two case studies from two different application fields, ecology and collaborative learning. © 2017 Springer Science+Business Media, LLC","Application to ecosystem modelling; Application to social network analysis; Formal methods; Model-driven approaches; Process mining; Rewrite systems","Administrative data processing; Carry logic; Computer circuits; Data mining; Ecology; Ecosystems; Enterprise resource management; Formal methods; Formal verification; Business process management; Ecosystem modelling; Graphical representations; Model driven approach; Predictive capabilities; Process mining; Quantitative information; Rewrite systems; Model checking",2-s2.0-85029435950
"Harmouche J., Fourer D., Auger F., Borgnat P., Flandrin P.","The Sliding Singular Spectrum Analysis: a Data-Driven Non-Stationary Signal Decomposition Tool",2017,"IEEE Transactions on Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030232031&doi=10.1109%2fTSP.2017.2752720&partnerID=40&md5=9acccb7f6a1ff31e0170ffae1c8d9506","Singular Spectrum Analysis (SSA) is a signal decomposition technique that aims at expanding signals into interpretable and physically meaningful components (e.g. sinusoids, noise, etc.). This article presents new theoretical and practical results about the separability of the SSA and introduces a new method called sliding SSA. First, the SSA is combined with an unsupervised classification algorithm to provide a fully automatic data-driven component extraction method for which we investigate the limitations for components separation in a theoretical study. Second, the detailed automatic SSA method is used to design an approach based on a sliding analysis window which provides better results than the classical SSA method when analyzing non-stationary signals with a variable number of components. Finally, the proposed sliding SSA method is compared to the Empirical Mode Decomposition (EMD) and to the synchrosqueezed Short-Time Fourier Transform (STFT), applied on both synthetic and real-world signals. IEEE","Empirical Mode Decomposition; non-stationary signals; Singular Spectrum Analysis; Synchrosqueezing","Data mining; Fourier series; Signal distortion; Spectrum analysis; Component extraction; Empirical Mode Decomposition; Nonstationary signals; Short time Fourier transforms; Signal decomposition technique; Singular spectrum analysis; Synchrosqueezing; Unsupervised classification; Signal processing",2-s2.0-85030232031
"Moharana U.C., Sarmah S.P.","Joint replenishment of associated spare parts using clustering approach",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029437344&doi=10.1007%2fs00170-017-0909-6&partnerID=40&md5=c215f1ba16909874817ae7179ab2984c","Most often, we observe that certain spare parts are used together for various maintenance activities of equipment. If spare parts are used frequently in a group, then there exists some hidden dependency among these parts. Sometimes the entire maintenance work cannot be accomplished due to the shortage of a single spare in the group. These parts are termed here as associated spare items. These parts can be determined by frequent itemsets mining. In this paper, the groups of spare parts are found out through hierarchical clustering method using support and weighted support of the data mining measure. A methodology is proposed to incorporate both support and weighted support values for computing an overall cost of replenishment for joint replenishment policy for the associated spares. Finally, a case study is presented to illustrate the validation of our model and compared the results among the group with normal support and group with weighted support. © 2017 Springer-Verlag London Ltd.","Case study; Inventory management; Itemset mining; Joint replenishment; Support","Cluster analysis; Inventory control; Supports; Clustering approach; Frequent itemsets minings; Hierarchical clustering methods; Inventory management; Itemset mining; Joint replenishment; Maintenance activity; Weighted supports; Data mining",2-s2.0-85029437344
"Palomba F., Panichella A., Zaidman A., Oliveto R., De Lucia A.","The Scent of a Smell: An Extensive Comparison between Textual and Structural Smells",2017,"IEEE Transactions on Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030217493&doi=10.1109%2fTSE.2017.2752171&partnerID=40&md5=08076642c9fa25afdcc104371b29f29e","Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change- and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove. IEEE","Code Smells; Data mining; Detectors; Empirical Study; Large scale integration; Maintenance engineering; Mining Software Repositories; Software systems; Tools","Computer software maintenance; Data mining; Detectors; LSI circuits; Maintainability; Odors; Structural analysis; Structural properties; Tools; Code smell; Empirical studies; Mining software repositories; Program comprehension; Software maintenance and evolution; Software repository mining; Software systems; Sources of informations; Codes (symbols)",2-s2.0-85030217493
"Lu Y., Wang H., Landis S., Maciejewski R.","A Visual Analytics Framework for Identifying Topic Drivers in Media Events",2017,"IEEE Transactions on Visualization and Computer Graphics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030264468&doi=10.1109%2fTVCG.2017.2752166&partnerID=40&md5=b582454e19e8cfa8ac1fb4bfd961e4ec","Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored. IEEE","Causality Modeling; Media; Media Annotation; Semantic Similarity; Semantics; Social Media; Social network services; Time series analysis; Tools; Visual analytics; Visual Analytics","Natural language processing systems; Semantics; Time series analysis; Tools; Visualization; Causality modeling; Media; Media Annotations; Semantic similarity; Social media; Social network services; Visual analytics; Data mining",2-s2.0-85030264468
"Ashton M., Gluhovic D., Sinnott S.B., Guo J., Stewart D.A., Hennig R.G.","Two-Dimensional Intrinsic Half-Metals with Large Spin Gaps",2017,"Nano Letters",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029350199&doi=10.1021%2facs.nanolett.7b01367&partnerID=40&md5=586baba5929ffce5fb02756e9ceb00cb","Through a systematic search of all layered bulk compounds combined with density functional calculations employing hybrid exchange-correlation functionals, we predict a family of three magnetic two-dimensional (2D) materials with half-metallic band structures. The 2D materials, FeCl2, FeBr2, and FeI2, are all sufficiently stable to be exfoliated from bulk layered compounds. The Fe2+ ions in these materials are in a high-spin octahedral d6 configuration leading to a large magnetic moment of 4 μB. Calculations of the magnetic anisotropy show an easy-plane for the magnetic moment. A classical XY model with nearest neighbor coupling estimates critical temperatures, Tc, for the Berezinskii-Kosterlitz-Thouless transition ranging from 122 K for FeI2 to 210 K for FeBr2. The quantum confinement of these 2D materials results in unusually large spin gaps, ranging from 4.0 eV for FeI2 to 6.4 eV for FeCl2, which should defend against spin current leakage even at small device length scales. Their purely spin-polarized currents and dispersive interlayer interactions should make these materials useful for 2D spin valves and other spintronic applications. © 2017 American Chemical Society.","data mining; density functional theory; half-metal; iron dihalides; magnetism; spintronics; two-dimensional materials","Data mining; Electron energy analyzers; Hybrid materials; Iron compounds; Magnetic anisotropy; Magnetic moments; Magnetism; Magnetoelectronics; Berezinskii-Kosterlitz-Thouless transition; Dihalides; Half metallic band structure; Half metals; Interlayer interactions; Nearest-neighbor coupling; Spin polarized currents; Two-dimensional materials; Density functional theory",2-s2.0-85029350199
"Gee D., Bateson L., Sowter A., Grebby S., Novellino A., Cigna F., Marsh S., Banton C., Wyatt L.","Ground motion in areas of abandoned mining: Application of the intermittent SBAS (ISBAS) to the Northumberland and Durham Coalfield, UK",2017,"Geosciences (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029783975&doi=10.3390%2fgeosciences7030085&partnerID=40&md5=434488593452c3b683ea7e112b7b1466","In this paper, we investigate land motion and groundwater level change phenomena using differential interferometric synthetic aperture radar (DInSAR) over the Northumberland and Durham coalfield in the United Kingdom. The study re-visits earlier research that applied a persistent scatterers interferometry (PSI) technique to ERS (European Remote Sensing) and ENVISAT (Environmental Satellite) data. Here, the Intermittent Small Baseline Subset (ISBAS) DInSAR technique is applied to ERS, ENVISAT and Sentinel-1 SAR datasets covering the late 1990s, the 2000s and the mid-2010s, respectively, to increase spatial coverage, aid the geological interpretation and consider the latest Sentinel-1 data. The ERS data identify surface depressions in proximity to former collieries, while all three data sets ascertain broad areas are experiencing regional scale uplift, often occurring in previously mined areas. Uplift is attributed to increases in pore pressure in the overburden following the cessation of groundwater pumping after mine closure. Rising groundwater levels are found to correlate to ground motion measurements at selected monitoring sites, most notably in the surrounding area of Ashington. The area is divided by an impermeable EW fault; to the south, surface heave was identified as groundwater levels rose in the 1990s, whereas to the north, this phenomenon occurred two decades later in the 2010s. The data emphasize the complexity of the post-mining surface and subsurface environment and highlight the benefit that InSAR, utilizing the ISBAS technique, can provide in its characterization. © 2017 by the authors.","Coal mining; DInSAR; Ground motion; Intermittent SBAS; Subsidence",,2-s2.0-85029783975
"Yu J., Malekian R., Chang J., Su B.","Modeling of whole-space transient electromagnetic responses based on FDTD and its application in the mining industry",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030258644&doi=10.1109%2fTII.2017.2752230&partnerID=40&md5=cfaa770be6e7d4db4e19bcba0bb03255","Hidden, water-abundant areas in coal mines pose a serious threat to mine safety and production. Underground transient electromagnetic method (TEM) is one of the most effective means of detecting water-abundant areas in front of the roadway head. Traditional TEM theories and applications are interpreted mainly on the vertical component. In this study, multi-component responses of TEM in underground roadways were modeled using the finite-difference time-domain (FDTD) method. Physical simulation was also used for advanced detection of TEM in the roadway. Both the numerical and physical simulation results show that the horizontal component is more sensitive to the location of water-abundant areas. The results of the joint interpretation with both horizontal and vertical components were verified in a practical coal mine application, indicating that it is feasible to use the horizontal component in interpreting TEM data. Thus, the horizontal component could serve as a new approach for coal mine TEM data processing and interpretation. IEEE","Advanced detection; Coal mine industry; finite-difference time-domain (FDTD); Numerical modeling; Transient electromagnetic method","Coal; Coal industry; Data handling; Finite difference time domain method; Numerical methods; Numerical models; Time domain analysis; Transient analysis; Advanced detections; Finite -difference time domains (FDTD); Joint interpretation; Mine industry; Physical simulation; Transient electromagnetic methods; Transient electromagnetic response; Underground transients; Coal mines",2-s2.0-85030258644
"Venugopalan J., Chanani N., Maher K., Wang M.D.","Combination of static and temporal data analysis to predict mortality and readmission in the intensive care",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032204476&doi=10.1109%2fEMBC.2017.8037382&partnerID=40&md5=17d82f06e23633401d67074ec1b62451","There are approximately 4 million intensive care unit (ICU) admissions each year in the United States with costs accounting for 4.1% of national health expenditures. Unforeseen adverse events contribute disproportionately to these costs. Thus, there has been substantial research in developing clinical decision support systems to predict and improve ICU outcomes such as ICU mortality, prolonged length of stay, and ICU readmission. However, the data in the ICU is collected at diverse time intervals and includes both static and temporal data. Common methods for static data mining such as Cox and logistic regression and methods for temporal data analysis such as temporal association rule mining do not model the combination of both static and temporal data. This work aims to overcome this challenge to combine static models such as logistic regression and feedforward neural networks with temporal models such as conditional random fields(CRF). We demonstrate the results using adult patient records from a publicly available database called Multi-parameter Intelligent Monitoring in Intensive Care - II (MIMIC-II). We show that the combination models outperformed individual models of logistic regression, feed-forward neural networks and conditional random fields in predicting ICU mortality. The combination models also outperform the static models of logistic regression and feed-forward neural networks for the prediction of 30 day ICU readmissions when tested using Matthews correlation coefficient and accuracy as the metrics. © 2017 IEEE.",,,2-s2.0-85032204476
"Zaccaria G.M., Rosati S., Castagneri C., Ferrero S., Ladetto M., Boccadoro M., Balestra G.","Data quality improvement of a multicenter clinical trial dataset",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032210164&doi=10.1109%2fEMBC.2017.8037043&partnerID=40&md5=a7a79de171b2accabeabdb3e8ca5dae4","Medical datasets are usually affected by several problems, such as missing values, inconsistencies, redundancies, that can influence the data mining process and the extraction of useful knowledge. For these reasons, a preprocessing phase should be performed for improving the overall quality of data and, consequently, of the information that may be discovered from them. In this study we applied five steps of data preprocessing to improve the quality of a large dataset derived from a multicenter clinical trial. Our dataset included 298 patients enrolled in a prospective, multicenter, clinical trial, characterized by 22 input variables and one class variable (MIPI value). In particular, data coming from different medical centers were firstly integrated to obtain a homogeneous dataset. The latter was normalized to scale all variables into smaller and similar intervals. Then, all missing values were estimated by means of an imputation step. The complete dataset was finally discretized and reduced to remove redundant variables and decrease the amount of data to be managed. The improvement of data quality after each step was evaluated by means of the patients' classification accuracy using the KNN classifier. Our results showed that the proposed pipeline produced an increment of more than 20% of the classification performances. Moreover, the highest growth of accuracy was obtained after missing value imputation, whereas the discretization and feature selection steps allowed for a significant reduction of variables to be managed, without any deterioration of the information contained in data. © 2017 IEEE.",,,2-s2.0-85032210164
"Nilsson A.E., Aragonés M.M., Torralvo F.A., Dunon V., Angel H., Komnitsas K., Willquist K.","A review of the carbon footprint of Cu and Zn production from primary and secondary sources",2017,"Minerals",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031404282&doi=10.3390%2fmin7090168&partnerID=40&md5=8acadb2343dda5937f17c77fffa341a5","Copper (Cu) and zinc (Zn) with their unique properties are central for economic growth, quality of life, and the creation of new jobs. The base-metal producing sector is, however, under growing public pressure in respect to energy and water requirements and needs to meet several challenges, including increased demand and lower ore grades, which are generally associated with larger resource use. The development of technologies for metal production from secondary sources is often motivated by increased sustainability, and this paper aims to provide further insights about one specific aspect of sustainability—namely, climate change. The paper presents a review of carbon footprints (CF) for Cu and Zn produced from primary and secondary raw materials by analyzing data taken from scientific literature and the Ecoinvent database. Comparisons are carried out based on the source of data selected as a reference case. The data available in the literature indicate that secondary production of Cu and Zn has the potential to be more beneficial compared to primary production regarding the impact on climate change. However, the technologies used today for the production of both metals from secondary sources are still immature, and more research on this topic is needed. The general variation of data suggests that the standardization of a comparison is needed when assessing the environmental benefits of production in line with the principles of waste valorization, the zero waste approach, and circular economy. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Carbon footprint; Circular economy; Cu; Metal production; Mining; Secondarymaterials; Zn",,2-s2.0-85031404282
"Seo J., Mendelevitch O.","Identifying frauds and anomalies in Medicare-B dataset",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032176738&doi=10.1109%2fEMBC.2017.8037652&partnerID=40&md5=32c568222214d81ea33b51bc4b3a39a5","Healthcare industry is growing at a rapid rate to reach a market value of $7 trillion dollars world wide. At the same time, fraud in healthcare is becoming a serious problem, amounting to 5% of the total healthcare spending, or $100 billion dollars each year in US. Manually detecting healthcare fraud requires much effort. Recently, machine learning and data mining techniques are applied to automatically detect healthcare frauds. This paper proposes a novel PageRank-based algorithm to detect healthcare frauds and anomalies. We apply the algorithm to Medicare-B dataset, a real-life data with 10 million healthcare insurance claims. The algorithm successfully identifies tens of previously unreported anomalies. © 2017 IEEE.",,,2-s2.0-85032176738
"Parsa M., Panda P., Sen S., Roy K.","Staged Inference using Conditional Deep Learning for energy efficient real-time smart diagnosis",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032225123&doi=10.1109%2fEMBC.2017.8036767&partnerID=40&md5=e85d2fad654b025dc6ba4db58bd90769","Recent progress in biosensor technology and wearable devices has created a formidable opportunity for remote healthcare monitoring systems as well as real-time diagnosis and disease prevention. The use of data mining techniques is indispensable for analysis of the large pool of data generated by the wearable devices. Deep learning is among the promising methods for analyzing such data for healthcare applications and disease diagnosis. However, the conventional deep neural networks are computationally intensive and it is impractical to use them in real-time diagnosis with low-powered on-body devices. We propose Staged Inference using Conditional Deep Learning (SICDL), as an energy efficient approach for creating healthcare monitoring systems. For smart diagnostics, we observe that all diagnoses are not equally challenging. The proposed approach thus decomposes the diagnoses into preliminary analysis (such as healthy vs unhealthy) and detailed analysis (such as identifying the specific type of cardio disease). The preliminary diagnosis is conducted real-time with a low complexity neural network realized on the resource-constrained on-body device. The detailed diagnosis requires a larger network that is implemented remotely in cloud and is conditionally activated only for detailed diagnosis (unhealthy individuals). We evaluated the proposed approach using available physiological sensor data from Physionet databases, and achieved 38% energy reduction in comparison to the conventional deep learning approach. © 2017 IEEE.",,,2-s2.0-85032225123
"Balan B., Caruso T., Martinelli F.","Gaining insight into exclusive and common transcriptomic features linked with biotic stress responses in Malus",2017,"Frontiers in Plant Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030846983&doi=10.3389%2ffpls.2017.01569&partnerID=40&md5=cbcf47b829096efbbe073ba77a2b5d99","Identifying key information in transcriptomic data is very important, especially when the “omic” study deals with plant responses to stresses in field conditions where a high number of variables and disturbing factors may affect the analysis. In this meta-analysis we collected 12 transcriptomic works in Malus in order to identify which key genes, proteins, gene categories are involved in general plant pathological conditions and those features linked with exclusive biotic stress responses. Those genes that are only related with molecular responses to pathogen attacks and those linked with other plant physiological processes were identified. A pipeline composed by pathway and gene set enrichment analysis, protein-protein interaction networks and gene visualization tools was employed. A total of 13,230 genes of the 12 studies were analyzed with functional data mining tools: 5,215 were upregulated, 8,015 were downregulated. Gene set enrichment analysis pointed out that photosynthesis was inhibited by Erwinia amylovora and fungal pathogens. Different hormonal crosstalk was linked with responses to different pathogens. Gibberellin-related pathways, ABA-related were mostly repressed by fungal pathogens. Relating to transcription factors, genes encoding MYBs and WRKY2 were downregulated by fungal pathogens and 12 WRKYs were commonly regulated by different biotic stresses The protein-protein interaction analysis discovered the presence of several proteins affected by more than one biotic stress including a WRKY40 and some highly interactive proteins such as heat shock proteins. This study represents a first preliminary curated meta-analysis of apple transcriptomic responses to biotic stresses. © 2017 Balan, Caruso and Martinelli.","Biotic stresses; Malus; Meta-analysis; Protein-protein interaction network; Transcriptomics",,2-s2.0-85030846983
"Wei S., Song Y., Tang J., Liu Z., Wang Q., Lin B., Feng J., Hou L., Danzhen W.","Geochronology, geochemistry, Sr–Nd–Hf isotopic compositions, and petrogenetic and tectonic implications of Early Cretaceous intrusions associated with the Duolong porphyry–epithermal Cu–Au deposit, central Tibet",2017,"International Geology Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029419599&doi=10.1080%2f00206814.2017.1369178&partnerID=40&md5=bfad44b0079cdb1028560f09b8283bb5","This article reports new zircon laser ablation-multicollector-inductively coupled plasma-mass spectrometry U–Pb and Hf isotope, whole-rock major and trace element, and Sr–Nd isotope data for mineralized and barren intrusions associated with the Duolong porphyry–epithermal copper–(gold) deposit (DPCD, a mining camp containing several individual deposits) in the western Qiangtang Terrane (QT), central Tibet. These data are used to further our understanding of the geological evolution of this region. The mineralized and barren DPCD intrusions are typical I-type granitoids that were synchronously emplaced at ca. 112.6–125.9 Ma. These igneous rocks show arc affinities that are characterized by enrichments in the light rare earth elements (LaN/YbN = 4.08–15.23) and the light ion lithophile elements (Rb, Th, U, K, and Pb), and depletions in the high field strength elements (Nb, Ta, and Ti). They have 87Sr/86Sr(i) values of 0.7046–0.7079, Nd(t) values of –6.0 to +1.1, and two-stage Nd model ages of ca. 823–1410 Ma. Zircons from these intrusive rocks have variable but generally positive εHf(t) values (–2.7 to +13.7) and relatively young zircon Hf crustal model ages of 335–1351 Ma. Combining these data with geochemical data reported in recent studies, we infer that the mineralized and barren DPCD intrusions formed in a continental marginal arc setting and likely originated from a common parental magma that was result of magma mixing of juvenile crust-derived basaltic melts and old lower crust-derived melts. The formation of the DPCD intrusions indicates that the Bangongco–Nujiang oceanic lithosphere was still undergoing northward subduction beneath the western QT at ca. 112.6–125.9 Ma, suggesting in turn that the oceanic basin have not closed completely during the Early Cretaceous. These new data also indicate that the processes that occur during the subduction of oceanic crust in continental marginal arc settings produce and preserve juvenile crustal material, leading to net continental crust vertical growth and thickening. © 2017 Informa UK Limited, trading as Taylor & Francis Group","Bangongco–Nujiang suture zone; Duolong deposit; Early Cretaceous intrusions; juvenile lower crust; petrogenesis; tectonomagmatic evolution; Tibet",,2-s2.0-85029419599
"Al-Zanbouri Z., Ding C.","Selecting Green Data Mining Services",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032343280&doi=10.1109%2fSCC.2017.79&partnerID=40&md5=11b95e550367cf20deace53bec824e10","Nowadays, there is a big increase in the usage ofdata analytic applications and services because of the growth inthe data produced from many different sources. The QoSproperties such as response time, reliability and latency of theseservices are important factors to decide which services to select.As we know, the energy consumption is becoming a big issue as aresult of IT expansion. Therefore, establishing a QoS-based webservice selection approach that considers energy consumption asone of the essential QoS properties represents a significant steptowards selecting the greener web service. This paper presents anexperimental study of energy consumption and latency behaviorof data mining algorithms running as web services. Our studyshows that, there is a strong relation between the datasetproperties such as dataset size, number of attributes, data type,and QoS attributes energy consumption and latency. Based onthe findings from our study, a prediction system is built whichcan be used to predict the energy consumption and latency valuesfor data mining web services on a given dataset, and then theseservices can be ranked according to their predicted energy andlatency values. Experimental results show the effectiveness of ourprediction and service selection system. © 2017 IEEE.","Quality of Service (QoS); Service; Webs services","Data mining; Energy utilization; Quality of service; Websites; Data mining algorithm; Data mining web services; Data-mining services; Prediction systems; QoS attributes; Service; Service selection; Webs services; Web services",2-s2.0-85032343280
"Deng L., Li D.","Multimedia data stream information mining algorithm based on jointed neural network and soft clustering",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029180972&doi=10.1007%2fs11042-017-4964-7&partnerID=40&md5=ab08def804585dd9c3007e45126a65e5","As one non-surveillance study method, soft clustering is well applied in the data mining, the imagery processing, the pattern recognition, the spatial remote sensing technology and the characteristic extraction and so on state-of-the-art applications in many domains all have the widespread application. Inspired by the combination of neural network and soft computing model, in this paper, we propose the novel multimedia data stream information mining model based on jointed neural network and soft clustering. In the self-training process, we train a learner on a tagged sample set and then use the learner to mark an unlabeled sample that it considers to be highly reliable and add the newly labeled sample to the original training set, then use this new training set to re-train the learner and repeat the above process until the iteration condition is terminated. To better play to the performance of the main processor and then save address space resources with the wishbone protocol implementation the separation of the high-low speed hierarchical interconnection structure GPU is applied. Experimental result proves the effectiveness of the proposed model. © 2017 Springer Science+Business Media, LLC","Data Stream; GPU; Information Mining; Multimedia; Neural Network; Soft Clustering; Systematic Design","Data communication systems; Graphics processing unit; Iterative methods; Media streaming; Neural networks; Pattern recognition; Remote sensing; Soft computing; Data stream; Information mining; Multimedia; Soft clustering; Systematic designs; Data mining",2-s2.0-85029180972
"Palpanas T.","The parallel and distributed future of data series mining",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032371580&doi=10.1109%2fHPCS.2017.155&partnerID=40&md5=864870f19c80eb85d4b187fd97ec81f2","There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of sequences, or data series. Examples of such applications come from biology, astronomy, entomology, the web, and other domains. It is not unusual for these applications to involve numbers of data series in the order of hundreds of millions to billions, which are often times not analyzed in their full detail due to their sheer size. In this work, we describe past efforts in designing techniques for indexing and mining truly massive collections of data series, based on indexing techniques for fast similarity search, an operation that lies at the core of many mining algorithms. We show that there are two bottlenecks in mining such massive datasets, namely, the time taken to build the index, and the time required to answer exactly similarity queries. In response to these challenges, we discuss novel techniques that adaptively create data series indexes, allowing users to correctly answer queries before the indexing task is finished. We also show how our methods allow mining on datasets that would otherwise be completely untenable, including the first published experiments using one billion data series. Moreover, we present our vision for the future in big sequence management and mining research: we argue that more efforts should concentrate on parallel (including modern hardware optimization opportunities) and distributed solutions, which have until now been largely unexploited. © 2017 IEEE.",,"Indexing (of information); Designing techniques; Distributed solutions; Hardware optimization; Indexing techniques; Massive data sets; Mining algorithms; Similarity query; Similarity search; Query processing",2-s2.0-85032371580
"Yuan W., Wang Q., Fan J., Li H.","Mining land subsidence monitoring using sentinel-1 SAR data",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031007372&doi=10.5194%2fisprs-archives-XLII-2-W7-655-2017&partnerID=40&md5=8085bed07f0a1982451b77750cea1d81","In this paper, DInSAR technique was used to monitor land subsidence in mining area. The study area was selected in the coal mine area located in Yuanbaoshan District, Chifeng City, and Sentinel-1 data were used to carry out DInSAR techniqu. We analyzed the interferometric results by Sentinel-1 data from December 2015 to May 2016. Through the comparison of the results of DInSAR technique and the location of the mine on the optical images, it is shown that DInSAR technique can be used to effectively monitor the land subsidence caused by underground mining, and it is an effective tool for law enforcement of over-mining.","DInSAR; Interferogram; Land subsidence; Mining area; Sentinel-1","Coal mines; Geometrical optics; Interferometry; Synthetic aperture radar; D-inSAR; Interferograms; Land subsidence; Mining areas; Sentinel-1; Subsidence",2-s2.0-85031007372
"Tan C., Yan S.","Spatiotemporal data organization and application research",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031037349&doi=10.5194%2fisprs-archives-XLII-2-W7-1363-2017&partnerID=40&md5=80138d89f46f23eae0f97ba8e0a9eea8","Organization and management of spatiotemporal data is a key support technology for intelligence in all fields of the smart city. The construction of a smart city cannot be realized without spatiotemporal data. Oriented to support intelligent applications, this paper proposes an organizational model for spatiotemporal data, and details the construction of a spatiotemporal big data calculation, analysis, and service framework for highly efficient management and intelligent application of spatiotemporal data for the entire data life cycle. © Authors 2017. CC BY 4.0 License.","Information fusion; Smart city; Spatiotemporal data; Spatiotemporal data mining; Spatiotemporal object","Data mining; Information fusion; Life cycle; Smart city; Application research; Efficient managements; Intelligent applications; Organization and management; Organizational modeling; Spatio-temporal data; Spatio-temporal data mining; Spatio-temporal objects; Big data",2-s2.0-85031037349
"Wu X., Zurita-Milla R., Kraak M.-J., Izquierdo-Verdiguier E.","Clustering-based approaches to the exploration of spatio-temporal data",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031009856&doi=10.5194%2fisprs-archives-XLII-2-W7-1387-2017&partnerID=40&md5=f56ad5922d448a713894a9bfb40cac9a","As one spatio-temporal data mining task, clustering helps the exploration of patterns in the data by grouping similar elements together. However, previous studies on spatial or temporal clustering are incapable of analysing complex patterns in spatio-temporal data. For instance, concurrent spatio-temporal patterns in 2D or 3D datasets. In this study we present two clustering algorithms for complex pattern analysis: (1) the Bregman block average co-clustering algorithm with I-divergence (BBAC-I) which enables the concurrent analysis of spatio-temporal patterns in 2D data matrix, and (2) the Bregman cube average tri-clustering algorithm with I-divergence (BCAT-I) which enables the complete partitional analysis in 3D data cube. Here the use of the two clustering algorithms is illustrated by Dutch daily average temperature dataset from 28 weather stations from 1992 to 2011. For BBAC-I, it is applied to the averaged yearly dataset to identify station-year co-clusters which contain similar temperatures along stations and years, thus revealing patterns along both spatial and temporal dimensions. For BCAT-I, it is applied to the temperature dataset organized in a data cube with one spatial (stations) and two nested temporal dimensions (years and days). By partitioning the whole dataset into clusters of stations and years with similar within-year temperature similarity, BCAT-I explores the spatio-temporal patterns of intra-annual variability in the daily temperature dataset. As such, both BBAC-I and BCAT-I algorithms, combined with suitable geovisualization techniques, allow the exploration of complex spatial and temporal patterns, which contributes to a better understanding of complex patterns in spatio-temporal data. © Authors 2017. CC BY 4.0 License.","Co-clustering; Data mining; Geovisualization; Spatio-temporal data; Tri-clustering","Cluster analysis; Cobalt compounds; Data mining; Geometry; Co-clustering; Geo visualizations; Intra-annual variability; Spatial and temporal patterns; Spatio-temporal data; Spatio-temporal data mining; Spatiotemporal patterns; Tri-clustering; Clustering algorithms",2-s2.0-85031009856
"Duan J., Wang L., Hu X.","The effect of spatial autocorrelation on spatial co-location pattern mining",2017,"IEEE CITS 2017 - 2017 International Conference on Computer, Information and Telecommunication Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032438696&doi=10.1109%2fCITS.2017.8035297&partnerID=40&md5=9a9ebaa5513d279a0204659e1bdb30ee","spatial co-location pattern mining is an important part of spatial data mining, and the purpose is to discover the coexistence spatial feature sets whose instances are frequently located together in a geographic space. However, it ignores the existence of autocorrelation features that is not associated with surrounding features. For example, 'cactus' and 'Jerusalem artichoke' are two common plants in the desert, and it is easy to get prevalent pattern 'cactus, Jerusalem artichoke' for the existing spatial co-location pattern mining frameworks, but biologists have determined that 'Jerusalem artichoke' is a spatial autocorrelation feature so that above pattern is meaningless. To avoid getting prevalent patterns that contain spatial autocorrelation features, we propose an algorithm to find and remove the spatial autocorrelation feature from spatial data sets, so that we can get really meaningful prevalent co-location patterns, and the experimental results over synthetic/real data sets show the effectiveness and feasibility of our method. © 2017 IEEE.","co-location pattern mining; spatial autocorrelation; spatial data mining","Autocorrelation; Location; Autocorrelation features; Co-location patterns; Jerusalem artichoke; Spatial autocorrelations; Spatial co-location patterns; Spatial data mining; Spatial features; Surrounding feature; Data mining",2-s2.0-85032438696
"Marozzo F., Duro F.R., Blas J.G., Carretero J., Talia D., Trunfio P.","Evaluating a data-aware scheduling approach to reduce processing costs of dmcf workflows",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032357491&doi=10.1109%2fHPCS.2017.107&partnerID=40&md5=2441fe01fa5a8587e807942f07149726","As scientific data analysis applications become more and more complex, there is a great need to simplify the definition and execution of such applications, particularly when dealing with large datasets. The Data Mining Cloud Framework (DMCF) is a system allowing domain experts to design and execute complex data analysis workflows on cloud platforms, relying on cloud storage services for every I/O operation. In order to enhance I/O operations, we propose the integration of DMCF with Hercules, an in-memory I/O solution that can be used in combination with DMCF as an alternative to cloud storage services to improve the I/O performance of workflow executions. The integration between DMCF and Hercules is based on a data-aware scheduler that exploits data locality and in-memory I/O to reduce run time of workflows. The goal of this work is to evaluate the reduction of processing costs obtained by using the proposed data-aware scheduler, compared to the costs obtained with the original DMCF solution on the same workflow. The evaluation performed on a 32-node cloud cluster results in 52% reduction of I/O time, which results in 8% total execution time reduction, and 9% of cloud services cost reduction. © 2017 IEEE.","Cost reduction; Data locality; In-memory storage; Microsoft Azure; Workflows","Cost reduction; Costs; Data handling; Data mining; Digital storage; Information analysis; Scheduling; Windows operating system; Cloud storage services; Data locality; Data-aware scheduling; Memory storage; MicroSoft; Scientific data analysis; Work-flows; Workflow execution; Data reduction",2-s2.0-85032357491
"Tan J., Osborne B.","Analysis of big data from space",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030973087&doi=10.5194%2fisprs-archives-XLII-2-W7-1367-2017&partnerID=40&md5=4718422734da3ff405c2c83d0c3f004e","Massive data have been collected through various space mission. To maximize the investment, the data need to be exploited to the fullest. In this paper, we address key topics on big data from space about the status and future development using the system engineering method. First, we summarized space data including operation data and mission data, on their sources, access way, characteristics of 5Vs and application models based on the concept of big data, as well as the challenges they faced in application. Second, we gave proposals on platform design and architecture to meet the demand and challenges on space data application. It has taken into account of features of space data and their application models. It emphasizes high scalability and flexibility in the aspects of storage, computing and data mining. Thirdly, we suggested typical and promising practices for space data application, that showed valuable methodologies for improving intelligence on space application, engineering, and science. Our work will give an interdisciplinary knowledge to space engineers and information engineers. © Authors 2017. CC BY 4.0 License.","Big data; Cloud computing; Data intensive; Data mining; Platform design; Space data","Cloud computing; Data mining; Digital storage; Application models; Data intensive; High scalabilities; Key topics; Massive data; Platform design; Space data; Space missions; Big data",2-s2.0-85030973087
"Wu T., Zhou Y., Zhang L.","Artistic visualization of trajectory data using cloud model",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031011408&doi=10.5194%2fisprs-archives-XLII-2-W7-1393-2017&partnerID=40&md5=01e776d19964afe76e69ed60a88d73b7","Rapid advance of location acquisition technologies boosts the generation of trajectory data, which track the traces of moving objects. A trajectory is typically represented by a sequence of timestamped geographical locations. Data visualization is an efficient means to represent distributions and structures of datasets and reveal hidden patterns in the data. In this paper, we explore a cloud modelbased method for the generation of stylized renderings of trajectory data. The artistic visualizations of the proposed method do not have the goal to allow for data mining tasks or others but instead show the aesthetic effect of the traces of moving objects in a distorted manner. The techniques used to create the images of traces of moving objects include the uncertain line using extended cloud model, stroke-based rendering of geolocation in varying styles, and stylistic shading with aesthetic effects for print or electronic displays, as well as various parameters to be further personalized. The influence of different parameters on the aesthetic qualities of various painted images is investigated, including step size, types of strokes, colour modes, and quantitative comparisons using four aesthetic measures are also involved into the experiment. The experimental results suggest that the proposed method is with advantages of uncertainty, simplicity and effectiveness, and it would inspire professional graphic designers and amateur users who may be interested in playful and creative exploration of artistic visualization of trajectory data. © Authors 2017. CC BY 4.0 License.","Artistic style; Data visualization; Spatiotemporal data analysis; Trajectory data; Visual analytics","Cloud computing; Data mining; Rendering (computer graphics); Trajectories; Uncertainty analysis; Visualization; Aesthetic qualities; Artistic style; Geographical locations; Quantitative comparison; Spatio-temporal data; Stroke based rendering; Trajectory data; Visual analytics; Data visualization",2-s2.0-85031011408
"Albertoni R., Martino M.D., Quarati A.","Linked thesauri quality assessment and documentation for big data discovery",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032374092&doi=10.1109%2fHPCS.2017.16&partnerID=40&md5=43f6a9510c9a9283de49110874719347","Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary. © 2017 IEEE.","AHP; DQV; Linked data; Metadata; Quality; Thesauri","Data handling; Data mining; Decision making; Image quality; Metadata; Semantics; Thesauri; Analytic hierarchy; Cross-disciplinary; Cross-organizations; Linked datum; Multi criteria decision making; Multi disciplines; Quality assessment; Semantic foundation; Big data",2-s2.0-85032374092
"Hu G., Wu B., Chen J.","The Mining of Activity Dependence Relation Based on Business Process Models",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032390516&doi=10.1109%2fSCC.2017.64&partnerID=40&md5=d5ccb52e504d8a9c532654bee97597a0","With the development of process recommendation, dynamic adaptation and automatic modeling, the requirement of explicit and formalized expression of activity dependence relation in the business domain is becoming more and more urgent. However, these relations more exist in the minds of domain experts or in the unstructured documents, which leads process modeling and adaptation are a time-consuming and error-prone process. To solve this problem, a relation mining method is proposed for obtaining activity dependence relations. The formal description of these relations is defined in control flow perspective, which is expressed as serial-dependence relations and parallel-dependence relations in the form of three tuples after analyzing all the control flow patterns. And a mining algorithm is proposed for mining these two types of relations based on the process model. The correctness and performance of this algorithm are verified by a large number of experiments, and the experimental results show this method can quickly and accurately extract all the activity dependence relations from the existing process models in a business domain. © 2017 IEEE.","Activity dependence relation; Business process management; Information system; Pattern discovery; Process mining; Workflow pattern","Administrative data processing; Enterprise resource management; Information management; Information systems; Mining; Business process management; Dependence relation; Pattern discovery; Process mining; Workflow patterns; Data mining",2-s2.0-85032390516
"Huang J., Deng M., Zhang Y., Liu H.","Complex road intersection modelling based on low-frequency GPS track data",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031004383&doi=10.5194%2fisprs-archives-XLII-2-W7-23-2017&partnerID=40&md5=c2afa8c70d8647773e235aacaf273198","It is widely accepted that digital map becomes an indispensable guide for human daily traveling. Traditional road network maps are produced in the time-consuming and labour-intensive ways, such as digitizing printed maps and extraction from remote sensing images. At present, a large number of GPS trajectory data collected by floating vehicles makes it a reality to extract high-detailed and up-to-date road network information. Road intersections are often accident-prone areas and very critical to route planning and the connectivity of road networks is mainly determined by the topological geometry of road intersections. A few studies paid attention on detecting complex road intersections and mining the attached traffic information (e.g., connectivity, topology and turning restriction) from massive GPS traces. To the authors' knowledge, recent studies mainly used high frequency (1s sampling rate) trajectory data to detect the crossroads regions or extract rough intersection models. It is still difficult to make use of low frequency (20-100s) and easily available trajectory data to modelling complex road intersections geometrically and semantically. The paper thus attempts to construct precise models for complex road intersection by using low frequency GPS traces. We propose to firstly extract the complex road intersections by a LCSS-based (Longest Common Subsequence) trajectory clustering method, then delineate the geometry shapes of complex road intersections by a K-segment principle curve algorithm, and finally infer the traffic constraint rules inside the complex intersections. © Authors 2017. CC BY 4.0 License.","Crowdsourcing trajectory data; K-segment principle curve; Longest Common Subsequence; Modelling complex road intersections; Traffic rules","Complex networks; Data mining; Motor transportation; Remote sensing; Roads and streets; Topology; Trajectories; Transportation; Accident prone areas; Longest common subsequences; Remote sensing images; Road intersections; Topological geometries; Traffic rules; Trajectory clustering; Trajectory data; Traffic control",2-s2.0-85031004383
"Gu Y.Z., Qin K., Chen Y.X., Yue M.X., Guo T.","Parallel spatiotemporal spectral clustering with massive trajectory data",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031031983&doi=10.5194%2fisprs-archives-XLII-2-W7-1173-2017&partnerID=40&md5=e4eae0c480db788f59bfaeed6d860290","Massive trajectory data contains wealth useful information and knowledge. Spectral clustering, which has been shown to be effective in finding clusters, becomes an important clustering approaches in the trajectory data mining. However, the traditional spectral clustering lacks the temporal expansion on the algorithm and limited in its applicability to large-scale problems due to its high computational complexity. This paper presents a parallel spatiotemporal spectral clustering based on multiple acceleration solutions to make the algorithm more effective and efficient, the performance is proved due to the experiment carried out on the massive taxi trajectory dataset in Wuhan city, China. © Authors 2017. CC BY 4.0 License.","DTW; Multi-thread; Spatiotemporal clustering; Spectral clustering; Trajectory data; Urban computing","Clustering algorithms; Data mining; Taxicabs; Multi-thread; Spatio-temporal clustering; Spectral clustering; Trajectory data; Urban computing; Trajectories",2-s2.0-85031031983
"Li T., He T., Wang Z., Zhang Y., Chu D.","Unraveling Process Evolution by Handling Concept Drifts in Process Mining",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350208&doi=10.1109%2fSCC.2017.63&partnerID=40&md5=d366b9b6972d691a510002785662440d","Business processes change over time in response to changing circumstances, it is crucial for process managers to discover and understand such changes from event logs. While existing techniques in process mining cannot efficiently detect and locate concept drifts in uncompleted event logs. Hence, an appropriate method, which could prompt to unravel the process evolution, may greatly assist organizations to manage the flexibility and change of business processes in the context of business process management (BMP). In this paper, we first proposed an extensible feature and employed it with the sliding window technique and heuristic miner for detecting and locating concept drifts in uncompleted event logs. Then, Genetic Process Mining (GPM) was improved by the Weight Heuristic Miner (WHM) and Differential Evolution (DE) for mining the new process model of an evolving process. Finally, experimental results on four event logs which are created based on a real-world business process have validated the proposed method. © 2017 IEEE.","Concept drift detection; GPM; Process change; Process evolution; Process mining; The new process model","Administrative data processing; Enterprise resource management; Evolutionary algorithms; Miners; Optimization; Concept drifts; Process change; Process evolution; Process mining; Process Modeling; Data mining",2-s2.0-85032350208
"Asthana S., Megahed A., Becker V., Nakamura T., Gajananan K.","A Cognitive Prioritization for Reports Generated in Resource Constrained Applications",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032271340&doi=10.1109%2fSCC.2017.60&partnerID=40&md5=c58a3345820650dbe4af93c967656633","In many resource constrained web/cloud applications, users are given the ability to generate different kinds of reports after selecting some criteria for each report to be produced. Such criteria could, for example, be filters on certain report attributes. With limited computing resources, it is critical to prioritize the reports requested by users at any certain period of time. Then, that prioritization can be fed into any of the known web services scheduling algorithms. In this paper, we provide a novel cognitive prioritization approach that takes into consideration the free-form user-input text about the criticality of the reports as well as their aforementioned structured attributes/filters. Our method consists of a predictive model that uses the structured data to predict the report completion time as well as another text-mining model that uses the user's text to weight its importance. Then, both outputs are combined with the user profile to come up with the final prioritization. We apply our methodology to real data taken from the report generation of a real-world application of IT service deal pricing and show results that illustrate the effectiveness of our approach. © 2017 IEEE.","Analytics; Predictive Analytics; Report Generation; Text Mining; Web Services","Data mining; Filtration; Predictive analytics; Scheduling algorithms; Websites; Analytics; Completion time; Computing resource; Predictive modeling; Prioritization; Report generation; Structured data; Text mining; Web services",2-s2.0-85032271340
"Du L., Ben J., Li Y., Wang R.","Generation algorithm of discrete line in multi-dimensional grids",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031043275&doi=10.5194%2fisprs-archives-XLII-2-W7-11-2017&partnerID=40&md5=3db2159245ae4e4d46d3f701080e49ad","Discrete Global Grids System (DGGS) is a kind of digital multi-resolution earth reference model, in terms of structure, it is conducive to the geographical spatial big data integration and mining. Vector is one of the important types of spatial data, only by discretization, can it be applied in grids system to make process and analysis. Based on the some constraint conditions, this paper put forward a strict definition of discrete lines, building a mathematic model of the discrete lines by base vectors combination method. Transforming mesh discrete lines issue in n-dimensional grids into the issue of optimal deviated path in n-minus-one dimension using hyperplane, which, therefore realizing dimension reduction process in the expression of mesh discrete lines. On this basis, we designed a simple and efficient algorithm for dimension reduction and generation of the discrete lines. The experimental results show that our algorithm not only can be applied in the two-dimensional rectangular grid, also can be applied in the two-dimensional hexagonal grid and the three-dimensional cubic grid. Meanwhile, when our algorithm is applied in two-dimensional rectangular grid, it can get a discrete line which is more similar to the line in the Euclidean space. © Authors 2017. CC BY 4.0 License.","Dimension reduction; Discrete line; Grid; Multi-dimension; Vector","Data integration; Data mining; Geometry; Mesh generation; Vectors; Dimension reduction; Discrete global grids; Discrete line; Grid; Multi dimensions; Multi-dimensional grids; Simple and efficient algorithms; Two-dimensional rectangular; Big data",2-s2.0-85031043275
"Vaduva C., Danisor C., Datcu M.","Temporal analysis of SAR imagery for permanent and evolving Earth land cover behavior assessment",2017,"2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032358593&doi=10.1109%2fMulti-Temp.2017.8035260&partnerID=40&md5=09c4ded4c8d42f26570be5bd527ff5e4","In the era of constantly increasing Earth Observation (EO) data collections, information extraction and data analysis should be enhanced with a multi-temporal component enabled by the temporal resolution of satellite missions and create handy, yet powerful tools for those applications involving monitoring of land cover. The image time series, as results of the satellite revisiting period, gives you insights not only on a certain area, but also on its representation at different moments of time. In order to limit the issues that might arise due to irregular time sampling of multispectral data, the authors propose a Synthetic Aperture Radar (SAR) image time series for analysis. To this point, the main goal is to mine the satellite image time series (SITS) for understanding the temporal behaviour of an area in terms of evolution and persistency. The paper introduces an analytical approach, combining coherent and no coherent analysis of SAR SITS content. We propose the Latent Dirichlet Allocation model to extract categories of evolution from the SAR SITS and techniques which study statistical and coherent proprieties of the targets to identify the structures with stable electromagnetic characteristics over time, named Persistent Scatterers (PS). The obtained results indicate an evolutionary character hidden inside the persistent class. The results obtained on 30 ERS images encourages further analysis on Sentinel 1 data. © 2017 IEEE.","Categories of evolution; Latent Dirichlet Allocation; Persistent Scatterers; SAR image time series","Data mining; Image analysis; Information analysis; Remote sensing; Satellites; Space-based radar; Statistics; Synthetic aperture radar; Time series; Time series analysis; Categories of evolution; Earth observation data; Electromagnetic characteristic; Latent Dirichlet allocation; Multi-spectral data; Persistent scatterers; SAR Images; Synthetic aperture radar (SAR) images; Radar imaging",2-s2.0-85032358593
"Yang S., Huang G., Xiang Y., Zhou X., Chi C.-H.","Modeling User Preferences on Spatiotemporal Topics for Point-of-Interest Recommendation",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032371348&doi=10.1109%2fSCC.2017.33&partnerID=40&md5=68a02dce11951b4ac4b009c40d1221d2","With the development of the location-based social networks (LBSNs) and the popular of mobile devices, a plenty of user's check-in data accumulated enough to enable personalized Point-of-Interest recommendations services. In this paper, we propose a scheme of modeling user's preferences on spatiotemporal topics (UPOST scheme) for accurate individualized POI recommendation. In the UPOST scheme, we discover temporal topics from semantic locations (i.e., people's description words for a location) to learn users' preferences. UPOST infers user's preference for different types of places during different periods by learning the spatiotemporal topics from the historical semantic locations of users. We have developed two algorithms under the UPOST scheme: The time division LDA algorithm (TDLDA) and the time adaptive topic discovery algorithm (TATD). In TDLDA, we divide the check-in dataset into different time segments and use one LDA for one segment. Then we improve TDLDA further by developing a new TATD algorithm to discover spatiotemporal topics. The experimental results demonstrate the effectiveness of our UPOST scheme, both TDLDA and TATD outperform the counterpart method that do not consider semantic locations. © 2017 IEEE.","Recommendation; Social computing; Spatiotemporal; Trajectory mining","Data mining; Location; Semantics; Location-based social networks; Point of interest; Recommendation; Semantic locations; Social computing; Spatiotemporal; Trajectory minings; User's preferences; Location based services",2-s2.0-85032371348
"Whitney C.D., Loulergue F.","Towards a verified parallel implementation of frequent itemset mining",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032354729&doi=10.1109%2fHPCS.2017.138&partnerID=40&md5=40c326814bc65f7729df46e0c9b2dbea","Information technologies have allowed for the rapid growth of both data acquisition and data storage. With this growth comes the challenge of extracting useful information. One piece of information that is interesting to academics and industry is the relationships between items in a large data set. One approach is to find the relationships between items by calculating how frequently the items appear together in a subset. This is known as the frequent itemset mining problem. The problem goes as follows, given a database with sets of items, find the items that occur frequently together in a subset. © 2017 IEEE.",,"Data acquisition; Data storage; Frequent itemset mining; Large datasets; Parallel implementations; Rapid growth; Digital storage",2-s2.0-85032354729
"Albrecht F., Hölbling D., Friedl B.","Assessing the agreement between eo-based semi-automated landslide maps with fuzzy manual landslide delineation",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031037270&doi=10.5194%2fisprs-archives-XLII-2-W7-439-2017&partnerID=40&md5=6f59abeaff56bb61590341746375598b","Landslide mapping benefits from the ever increasing availability of Earth Observation (EO) data resulting from programmes like the Copernicus Sentinel missions and improved infrastructure for data access. However, there arises the need for improved automated landslide information extraction processes from EO data while the dominant method is still manual delineation. Object-based image analysis (OBIA) provides the means for the fast and efficient extraction of landslide information. To prove its quality, automated results are often compared to manually delineated landslide maps. Although there is awareness of the uncertainties inherent in manual delineations, there is a lack of understanding how they affect the levels of agreement in a direct comparison of OBIA-derived landslide maps and manually derived landslide maps. In order to provide an improved reference, we present a fuzzy approach for the manual delineation of landslides on optical satellite images, thereby making the inherent uncertainties of the delineation explicit. The fuzzy manual delineation and the OBIA classification are compared by accuracy metrics accepted in the remote sensing community. We have tested this approach for high resolution (HR) satellite images of three large landslides in Austria and Italy. We were able to show that the deviation of the OBIA result from the manual delineation can mainly be attributed to the uncertainty inherent in the manual delineation process, a relevant issue for the design of validation processes for OBIA-derived landslide maps.","Accuracy; Agreement; Fuzzy delineation; Landslides; Object-based image analysis; Remote sensing; Uncertainty","Automation; Contracts; Data mining; Image analysis; Image enhancement; Remote sensing; Uncertainty analysis; Accuracy; Earth observation data; Fuzzy delineation; Object based image analysis; Object based image analysis (OBIA); Optical satellite images; Uncertainty; Validation process; Landslides",2-s2.0-85031037270
"Cole S.V., Buhler J.","Mercator: A GPGPU framework for irregular streaming applications",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376926&doi=10.1109%2fHPCS.2017.111&partnerID=40&md5=07245ef5ba3be742610eb8cdb9b8ca34","GPUs have a natural affinity for streaming applications exhibiting consistent, predictable dataflow. However, many high-impact irregular streaming applications, including sequence pattern matching, decision-tree and decision-cascade evaluation, and large-scale graph processing, exhibit unpredictable dataflow due to data-dependent filtering or expansion of the data stream. Existing GPU frameworks do not support arbitrary irregular streaming dataflow tasks, and developing application-specific optimized implementations for such tasks requires expert GPU knowledge. We introduce MERCATOR, a lightweight framework supporting modular CUDA streaming application development for irregular applications. A developer can use MERCATOR to decompose an irregular application for the GPU without explicitly remapping work to threads at runtime. MERCATOR applications are efficiently parallelized on the GPU through a combination of replication across blocks and queueing between nodes to accommodate irregularity. We quantify the performance impact of MERCATOR's support for irregularity and illustrate its utility by implementing a biological sequence comparison pipeline similar to the well-known NCBI BLASTN algorithm. MERCATOR code is available by request to the first author. © 2017 IEEE.","GPU; Irregular computation; Parallel computing; SIMD; Streaming dataflow","Bioinformatics; Data flow analysis; Data mining; Decision trees; Parallel processing systems; Pattern matching; Program processors; Trees (mathematics); Biological sequences; Dataflow; Irregular applications; Irregular computations; Lightweight frameworks; Optimized implementation; SIMD; Streaming applications; Graphics processing unit",2-s2.0-85032376926
"Ma M., He B., Guan Y., Zhang H., Song S.","Assessment of global wind energy resource utilization potential",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031047528&doi=10.5194%2fisprs-archives-XLII-2-W7-1283-2017&partnerID=40&md5=93f72d12d72053d2e920a6a7f624ab0f","Development of wind energy resource (WER) is a key to deal with climate change and energy structure adjustment. A crucial issue is to obtain the distribution and variability of WER, and mine the suitable location to exploit it. In this paper, a multicriteria evaluation (MCE) model is constructed by integrating resource richness and stability, utilization value and trend of resource, natural environment with weights. The global resource richness is assessed through wind power density (WPD) and multi-level wind speed. The utilizable value of resource is assessed by the frequency of effective wind. The resource stability is assessed by the coefficient of variation of WPD and the frequency of prevailing wind direction. Regression slope of long time series WPD is used to assess the trend of WER. All of the resource evaluation indicators are derived from the atmospheric reanalysis data ERA-Interim with spatial resolution 0.125°. The natural environment factors mainly refer to slope and land-use suitability, which are derived from multiresolution terrain elevation data 2010 (GMTED 2010) and GlobalCover2009. Besides, the global WER utilization potential map is produced, which shows most high potential regions are located in north of Africa. Additionally, by verifying that 22.22% and 48.89% operational wind farms fall on medium-high and high potential regions respectively, the result can provide a basis for the macroscopic siting of wind farm. © Authors 2017. CC BY 4.0 License.","Assessment; Data mining; ERA-Interim; Global; Remote sensing; Utilization potential; Wind energy resource","Climate change; Data mining; Electric utilities; Energy resources; Land use; Remote sensing; Wind; Wind effects; Assessment; Atmospheric reanalysis data; Coefficient of variation; Energy structure adjustments; Era interims; Global; Multi-criteria evaluation; Prevailing wind directions; Wind power",2-s2.0-85031047528
"Nguyen T., Meger N., Rigotti C., Pothier C., Trouve E., Gourmelen N.","Handling coherence measures of displacement field time series: Application to Greenland ice sheet glaciers",2017,"2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032392830&doi=10.1109%2fMulti-Temp.2017.8035228&partnerID=40&md5=c25f69f3bbe1ff9d3382bf5ad2f49382","Displacement Field Time Series (DFTS) are often derived from Satellite Image Time Series to study dynamic systems such as glaciers. This analysis can be performed with pattern-based data mining techniques that search DFTS for all possible displacement evolutions. Nevertheless, existing pattern oriented methods do not take into account the coherence measures coming along with such DFTS data. This paper introduces an approach for handling the coherence when extracting displacement evolutions. In addition to defining a coherence notion for these evolutions, we show that focusing on the coherent patterns allows to prune the search space. Reported experiments exhibit consistent displacement evolutions of Greenland ice sheet glaciers. © 2017 IEEE.",,"Data mining; Glaciers; Remote sensing; Space optics; Time series; Displacement field; Greenland Ice Sheet; Satellite images; Search spaces; Image analysis",2-s2.0-85032392830
"Sun J., Xiang H.","Research on key technology of mining remote sensing dynamic monitoring information system",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031027968&doi=10.5194%2fisprs-archives-XLII-2-W7-893-2017&partnerID=40&md5=e8d0b3c8d2866da2b75d2111afac7470","Problems exist in remote sensing dynamic monitoring of mining are expounded, general idea of building remote sensing dynamic monitoring information system is presented, and timely release of service-oriented remote sensing monitoring results is established. Mobile device-based data verification subsystem is developed using mobile GIS, remote sensing dynamic monitoring information system of mining is constructed, and ""timely release, fast handling and timely feedback"" rapid response mechanism of remote sensing dynamic monitoring is implemented. © Authors 2017. CC BY 4.0 License.","Dynamic monitoring; Mining; Mobile GIS; Remote sensing; Tiantidu","Geographic information systems; Information systems; Mining; Mobile devices; Monitoring; Data verification; Dynamic monitoring; Key technologies; Mobile GIS; Remote sensing monitoring; Service Oriented; Tiantidu; Timely feedback; Remote sensing",2-s2.0-85031027968
"Maisenbacher M., Weidlich M.","Handling Concept Drift in Predictive Process Monitoring",2017,"Proceedings - 2017 IEEE 14th International Conference on Services Computing, SCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032375987&doi=10.1109%2fSCC.2017.10&partnerID=40&md5=48d3f89af25ceaafbb9fbe2d4a16110d","Predictive process monitoring emerged as a technique to anticipate the outcome of a running instance of a business process. To this end, it first constructs a forecast model based on an encoding of traces of past process executions that are labelled with the prediction target. This model is then used to predict the outcome of a running process instance. However, existing approaches neglect that real-world processes are subject to continuous change, so that prediction models need to adapt to concept drift. In this paper, we take up ideas on incremental learning from general data mining and present a paradigm for predictive process monitoring under concept drift. It is grounded in a systematic experimental study that answers the questions of which encoding of process traces and which incremental learning strategies are particularly suited for predictive monitoring of continuously evolving processes. © 2017 IEEE.","Concept Drift; Incremental Learning; Predictive Monitoring","Data mining; Encoding (symbols); Forecasting; Process control; Signal encoding; Business Process; Concept drifts; Incremental learning; Prediction model; Predictive monitoring; Predictive process; Process execution; Real-world process; Process monitoring",2-s2.0-85032375987
"Fang H., Chen C., Lin J., Liu X., Fang D.","Association rule analysis for tour route recommendation and application to WCTSNOP",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030990312&doi=10.5194%2fisprs-archives-XLII-2-W7-1121-2017&partnerID=40&md5=209e6cfcab26afbfc167cc32f8ce112d","The increasing E-tourism systems provide intelligent tour recommendation for tourists. In this sense, recommender system can make personalized suggestions and provide satisfied information associated with their tour cycle. Data mining is a proper tool that extracting potential information from large database for making strategic decisions. In the study, association rule analysis based on FP-growth algorithm is applied to find the association relationship among scenic spots in different cities as tour route recommendation. In order to figure out valuable rules, Kulczynski interestingness measure is adopted and imbalance ratio is computed. The proposed scheme was evaluated on Wangluzhe cultural tourism service network operation platform (WCTSNOP), where it could verify that it is able to quick recommend tour route and to rapidly enhance the recommendation quality. © Authors 2017. CC BY 4.0 License.","Association rule analysis; Cultural tourism service; E-tourism; FP-growth; Route recommendation","Association rules; Data mining; Association rule analysis; eTourism; FP growths; Route recommendation; Tourism services; Quality control",2-s2.0-85030990312
"Fu J.Y., Jing C.F., Du M.Y., Fu Y.L., Dai P.P.","Study on adaptive parameter determination of cluster analysis in urban management cases",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031022761&doi=10.5194%2fisprs-archives-XLII-2-W7-1143-2017&partnerID=40&md5=58d016fe3900091d840c09df1c2b674c","The fine management for cities is the important way to realize the smart city. The data mining which uses spatial clustering analysis for urban management cases can be used in the evaluation of urban public facilities deployment, and support the policy decisions, and also provides technical support for the fine management of the city. Aiming at the problem that DBSCAN algorithm which is based on the density-clustering can not realize parameter adaptive determination, this paper proposed the optimizing method of parameter adaptive determination based on the spatial analysis. Firstly, making analysis of the function Ripley's K for the data set to realize adaptive determination of global parameter MinPts, which means setting the maximum aggregation scale as the range of data clustering. Calculating every point object's highest frequency K value in the range of Eps which uses K-D tree and setting it as the value of clustering density to realize the adaptive determination of global parameter MinPts. Then, the R language was used to optimize the above process to accomplish the precise clustering of typical urban management cases. The experimental results based on the typical case of urban management in XiCheng district of Beijing shows that: The new DBSCAN clustering algorithm this paper presents takes full account of the data's spatial and statistical characteristic which has obvious clustering feature, and has a better applicability and high quality. The results of the study are not only helpful for the formulation of urban management policies and the allocation of urban management supervisors in XiCheng District of Beijing, but also to other cities and related fields. © Authors 2017. CC BY 4.0 License.","Cluster analysis; Data mining; DBSCAN algorithm; Urban management cases","Cluster analysis; Data mining; Decision making; Smart city; Spatial variables measurement; Adaptive parameters; Clustering densities; DBSCAN algorithm; Density clustering; Parameter adaptive; Statistical characteristics; Urban management; Urban public facilities; Clustering algorithms",2-s2.0-85031022761
"Nooshery N.R., Taleai M., Kazemi R., Ebadi K.","Developing a web-based ppgis, as an environmental reporting service",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031027923&doi=10.5194%2fisprs-archives-XLII-2-W7-115-2017&partnerID=40&md5=4c6457f0ae00bdd68be07b6f71613851","Today municipalities are searching for new tools to empower locals for changing the future of their own areas by increasing their participation in different levels of urban planning. These tools should involve the community in planning process using participatory approaches instead of long traditional top-down planning models and help municipalities to obtain proper insight about major problems of urban neighborhoods from the residents' point of view. In this matter, public participation GIS (PPGIS) which enables citizens to record and following up their feeling and spatial knowledge regarding problems of the city in the form of maps have been introduced. In this research, a tool entitled CAER (Collecting and Analyzing of Environmental Reports) is developed. In the first step, a software framework based on Web-GIS tool, called EPGIS (Environmental Participatory GIS) has been designed to support public participation in reporting urban environmental problems and to facilitate data flow between citizens and municipality. A web-based cartography tool was employed for geo-visualization and dissemination of map-based reports. In the second step of CAER, a subsystem is developed based on SOLAP (Spatial On-Line Analytical Processing), as a data mining tools to elicit the local knowledge facilitating bottom-up urban planning practices and to help urban managers to find hidden relations among the recorded reports. This system is implemented in a case study area in Boston, Massachusetts and its usability was evaluated. The CAER should be considered as bottom-up planning tools to collect people's problems and views about their neighborhood and transmits them to the city officials. It also helps urban planners to find solutions for better management from citizen's viewpoint and gives them this chance to develop good plans to the neighborhoods that should be satisfied the citizens. © Authors 2017. CC BY 4.0 License.","Bottom-up urban planning; Environmental reporting service; PPGIS; SOLAP; Web-GIS","Computer programming; Data mining; Geographic information systems; Maps; Websites; Bottom up; Environmental reporting; PPGIS; SOLAP; Web-GIS; Urban planning",2-s2.0-85031027923
"Juniati E., Arrofiqoh E.N.","Comparison of pixel-based and object-based classification using parameters and non-parameters approach for the pattern consistency of multi scale landcover",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031033420&doi=10.5194%2fisprs-archives-XLII-2-W7-765-2017&partnerID=40&md5=d63f7a05971e1f5fb340bbaf2c93e350","Information extraction from remote sensing data especially land cover can be obtained by digital classification. In practical some people are more comfortable using visual interpretation to retrieve land cover information. However, it is highly influenced by subjectivity and knowledge of interpreter, also takes time in the process. Digital classification can be done in several ways, depend on the defined mapping approach and assumptions on data distribution. The study compared several classifiers method for some data type at the same location. The data used Landsat 8 satellite imagery, SPOT 6 and Orthophotos. In practical, the data used to produce land cover map in 1:50,000 map scale for Landsat, 1:25,000 map scale for SPOT and 1:5,000 map scale for Orthophotos, but using visual interpretation to retrieve information. Maximum likelihood Classifiers (MLC) which use pixel-based and parameters approach applied to such data, and also Artificial Neural Network classifiers which use pixel-based and non-parameters approach applied too. Moreover, this study applied object-based classifiers to the data. The classification system implemented is land cover classification on Indonesia topographic map. The classification applied to data source, which is expected to recognize the pattern and to assess consistency of the land cover map produced by each data. Furthermore, the study analyse benefits and limitations the use of methods.","Image classification; Multi-scale; Multi-source; Object-based; Pattern consistency; Pixel-based","Data mining; Image classification; Maps; Maximum likelihood; Neural networks; Pixels; Remote sensing; Satellite imagery; Artificial neural network classifiers; Land cover classification; Maximum likelihood classifiers; Multi-scale; Multi-Sources; Object based; Object-based classifications; Pattern consistency; Classification (of information)",2-s2.0-85031033420
"Folino F., Folino G., Pontieri L., Sabatino P.","A peer-to-peer architecture for detecting attacks from network traffic and log data",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032348428&doi=10.1109%2fHPCS.2017.116&partnerID=40&md5=9ef109697dabbbd8536bfa737c12c1df","Intrusion detection systems (IDS) support the recognition of attacks, based on the analysis of either network traffic data (Network-based IDS) or application/system logs stored in a host (Host-based IDS). Exploiting heterogeneous data coming from both kinds of sources could be useful to detect coordinated attacks and to reduce the number of false alarms, but poses challenges in terms of both information integration and scalability. In order to foster the development of such a hybrid IDS, we here propose a p2p intrusion detection architecture, which combines different data manipulation/mining techniques and a collaborative ensemble-based learning approach, and allows to incrementally classify attacks by integrating information extracted from both network traffic data and host logs. Preliminary experiments, conducted on real-life dataset, show that the approach is promising. © 2017 IEEE.","Ensemble-based Intrusion Detection Systems; Intrusion Detection Systems; Network Intrusion Detection Systems","Classification (of information); Computer crime; Computer networks; Data integration; Distributed computer systems; Information analysis; Mercury (metal); Network architecture; Network security; Peer to peer networks; Coordinated attack; Data manipulations; Information integration; Integrating information; Intrusion Detection Systems; Network intrusion detection systems; Number of false alarms; Peer-to-peer architectures; Intrusion detection",2-s2.0-85032348428
"Liang H.J., Wang H., Cui T.J., Guo J.F.","Expression and organization of geographic spatial relations based on topic maps",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031029031&doi=10.5194%2fisprs-archives-XLII-2-W7-1257-2017&partnerID=40&md5=086cc83d398e344a58cf7807963703f8","Spatial Relation is one of the important components of Geographical Information Science and Spatial Database. There have been lots of researches on Spatial Relation and many different spatial relations have been proposed. The relationships among these spatial relations such as hierarchy and so on are complex and this brings some difficulties to the applications and teaching of these spatial relations. This paper summaries some common spatial relations, extracts the topic types, association types, resource types of these spatial relations using the technology of Topic Maps, and builds many different relationships among these spatial relations. Finally, this paper utilizes Java and Ontopia to build a topic map among these common spatial relations, forms a complex knowledge network of spatial relations, and realizes the effective management and retrieval of spatial relations. © Authors 2017. CC BY 4.0 License.","Association; Occurrence; Spatial relation; Topic maps; Topic types; Topological relation","Association reactions; Complex networks; Query languages; Occurrence; Spatial relations; Topic Maps; Topic types; Topological relations; Data mining",2-s2.0-85031029031
"Preethi G., Krishna P.V., Obaidat M.S., Saritha V., Yenduri S.","Application of Deep Learning to Sentiment Analysis for recommender system on cloud",2017,"IEEE CITS 2017 - 2017 International Conference on Computer, Information and Telecommunication Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032449494&doi=10.1109%2fCITS.2017.8035341&partnerID=40&md5=f5afbc36ba67f73988020c6bf8c2b85f","Sentiment analysis of short texts like single sentences and reviews available on different social networking sites is challenging because of the limited contextual information. Based on the sentiments and opinions available, developing a recommendation system is an interesting concept, which includes strategies that combine the small text content with prior knowledge. In this paper, we explore a new application of Recursive Neural Networks (RNN) with deep learning system for sentiment analysis of reviews. The proposed RNN-based Deep-learning Sentiment Analysis (RDSA) recommends the places that are near to the user's current location by analyzing the different reviews and consequently computing the score grounded on it. Deep Learning is used to optimize the recommendations depending on the sentiment analysis performed on the different reviews, which are taken from different social networking sites. The Experiments performed indicate that the RNN based Deep-learning Sentiment Analysis (RDSA) improvises the behavior by increasing the accuracy of the sentiment analysis, which in turn yields better recommendations to the user and thus helps to identify a particular position as per the requirement of the user need. © 2017 IEEE.","Cloud; Deep learning; Performance Evaluation; Recursive Neural Networks; Sentiment Analysis","Clouds; Data mining; Deep learning; Electric grounding; Neural networks; Social networking (online); Contextual information; New applications; Performance Evaluation; Prior knowledge; Recursive neural networks; Sentiment analysis; Social networking sites; Text content; Recommender systems",2-s2.0-85032449494
"Chen L.-N., Si H.-P., Fang W., Chen Y.-Q., Cao Y.-S.","Visualization study for investigation data of crop germplasm resources in Guizhou Province",2017,"Acta Agronomica Sinica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027442856&doi=10.3724%2fSP.J.1006.2017.01300&partnerID=40&md5=3469b384c926f58de39d525800a71974","The large amounts of data derived from the project of ""Agro-biological Resources Investigation in Guizhou Province"" require analyzing in effective methods and providing comprehensive information on conservation and utilization of biological resources in this province to support local policy makers. Visualized and intuitive demonstration of data analysis is a well-liked but ignored in previous systems regarding resource investigation and data excavation. In this study, we compared several visualization methods, including Microsoft Excel, GIS, and R software, and proposed an effective strategy. We found that the spreadsheet was suitable for simple data analysis, spatial data visualization method was suitable for validating data and showing richness of resources, and statistical analysis visualization method had the advantage of mining hiding information because it could integrate different types of data. Using several visualization methods can make the intuitive results of data analysis, from which we are able to better understand germplasm resources and promote utilization efficiency of germplasm resources. This article also discussed the existing problems and data-standardizing proposals in the investigation of crop germplasm resources.","Crop germplasm resources; Guizhou; Visualization",,2-s2.0-85027442856
"Tao C.-S., Chen S.-W., Li Y.-Z., Xiao S.-P.","POLSAR LAND COVER CLASSIFICATION BASED on HIDDEN POLARIMETRIC FEATURES in ROTATION DOMAIN and SVM CLASSIFIER",2017,"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031011227&doi=10.5194%2fisprs-annals-IV-2-W4-485-2017&partnerID=40&md5=6b84d575be284361b577c1c5e97397ad","Land cover classification is an important application for polarimetric synthetic aperture radar (PolSAR) data utilization. Rollinvariant polarimetric features such as iH/Ani/<span styleCombining double low line""text-decoration: overline"">α</span>/Span are commonly adopted in PolSAR land cover classification. However, target orientation diversity effect makes PolSAR images understanding and interpretation difficult. Only using the roll-invariant polarimetric features may introduce ambiguity in the interpretation of targets' scattering mechanisms and limit the followed classification accuracy. To address this problem, this work firstly focuses on hidden polarimetric feature mining in the rotation domain along the radar line of sight using the recently reported uniform polarimetric matrix rotation theory and the visualization and characterization tool of polarimetric coherence pattern. The former rotates the acquired polarimetric matrix along the radar line of sight and fully describes the rotation characteristics of each entry of the matrix. Sets of new polarimetric features are derived to describe the hidden scattering information of the target in the rotation domain. The latter extends the traditional polarimetric coherence at a given rotation angle to the rotation domain for complete interpretation. A visualization and characterization tool is established to derive new polarimetric features for hidden information exploration. Then, a classification scheme is developed combing both the selected new hidden polarimetric features in rotation domain and the commonly used roll-invariant polarimetric features with a support vector machine (SVM) classifier. Comparison experiments based on AIRSAR and multi-temporal UAVSAR data demonstrate that compared with the conventional classification scheme which only uses the roll-invariant polarimetric features, the proposed classification scheme achieves both higher classification accuracy and better robustness. For AIRSAR data, the overall classification accuracy with the proposed classification scheme is 94.91 %, while that with the conventional classification scheme is 93.70 %. Moreover, for multi-temporal UAVSAR data, the averaged overall classification accuracy with the proposed classification scheme is up to 97.08 %, which is much higher than the 87.79 % from the conventional classification scheme. Furthermore, for multitemporal PolSAR data, the proposed classification scheme can achieve better robustness. The comparison studies also clearly demonstrate that mining and utilization of hidden polarimetric features and information in the rotation domain can gain the added benefits for PolSAR land cover classification and provide a new vision for PolSAR image interpretation and application. © Authors 2017.","Land Cover Classification; Polarimetric Feature Mining; Polarimetric Synthetic Aperture Radar (PolSAR); Rotation Domain; Support Vector Machine (SVM)",,2-s2.0-85031011227
"Mfula H., Nurminen J.K.","Adaptive root cause analysis for self-healing in 5G networks",2017,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032334496&doi=10.1109%2fHPCS.2017.31&partnerID=40&md5=2ad94f45361d341648ad252fa8532fe5","Root cause analysis (RCA) is a common and recurring task performed by operators of cellular networks. It is done mainly to keep customers satisfied with the quality of offered services and to maximize return on investment (ROI) by minimizing and where possible eliminating the root causes of faults in cellular networks. Currently, the actual detection and diagnosis of faults or potential faults is still a manual and slow process often carried out by network experts who manually analyze and correlate various pieces of network data such as, alarms, call traces, configuration management (CM) and key performance indicator (KPI) data in order to come up with the most probable root cause of a given network fault. In this paper, we propose an automated fault detection and diagnosis solution called adaptive root cause analysis (ARCA). The solution uses measurements and other network data together with Bayesian network theory to perform automated evidence based RCA. Compared to the current common practice, our solution is faster due to automation of the entire RCA process. The solution is also cheaper because it needs fewer or no personnel in order to operate and it improves efficiency through domain knowledge reuse during adaptive learning. As it uses a probabilistic Bayesian classifier, it can work with incomplete data and it can handle large datasets with complex probability combinations. Experimental results from stratified synthesized data affirmatively validate the feasibility of using such a solution as a key part of self-healing (SH) especially in emerging self-organizing network (SON) based solutions in LTE Advanced (LTE-A) and 5G. © 2017 IEEE.","5G; LTE-A; Root cause analysis; Self-healing","4G mobile communication systems; 5G mobile communication systems; Automation; Bayesian networks; Benchmarking; Classification (of information); Computation theory; Information management; Mobile telecommunication systems; Queueing networks; Solution mining; Standards; Wireless networks; Wireless telecommunication systems; Automated fault detection; Configuration management; Detection and diagnosis; Key performance indicators; LTE-A; Root cause analysis; Self Organizing Network (SON); Self-healing; Fault detection",2-s2.0-85032334496
"Ulloa R.S., Mac Cawley A.F., Santelices G.A., Pascual R.","Technology investment effects in performance-based maintenance contracts",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029413803&doi=10.1080%2f00207543.2017.1374573&partnerID=40&md5=9593e129216067c8c250096b38b9f935","This work analyses the effects of a fixed initial investment in technology or infrastructure on performance-based maintenance contracts. We present a mathematical expression which reflects the trade-off between an upfront technology investment by the vendor and the cost of required future interventions. We develop a mathematical model of a performance-based maintenance contract that uses this technology trade-off expression and determines the value of the contract for each party. Contractually, the client indicates the duration of the contract and the optimal number of maintenance interventions to maximise asset availability; the vendor quotes the cost for the requested interventions. We study how the initial investment and the contract parameters affect the net present value for each party and the supply chain, demonstrating the existence of an optimal relation between the number of preventive maintenance interventions and level of investment. We derive the optimal contract parameters for the client, vendor and chain and show lack of coordination between the parties. To achieve coordination, we present a revenue sharing mechanism which maximises the value for the chain. Finally, an industry study case with data from the mining sector is presented. Results indicate that by investing and coordinating, the entire supply chain can improve the contract NPV by 149.7%. © 2017 Informa UK Limited, trading as Taylor & Francis Group","fixed term contracts; net present value analysis; outsourcing; preventive maintenance; supply chain coordination; technology investment","Economic and social effects; Economics; Investments; Maintenance; Outsourcing; Supply chains; Asset availability; Fixed term contracts; Investment in technology; Mathematical expressions; Net present value analysis; Performance based maintenance; Supply chain coordination; Technology investments; Preventive maintenance",2-s2.0-85029413803
"Zou F., Zhan Q., Zhang W.","Quantifying the impact of human activities on geological hazards in mountainous areas: evidence from Shennongjia, China",2017,"Natural Hazards",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029168814&doi=10.1007%2fs11069-017-3039-4&partnerID=40&md5=09ef00058b76c21e4464d16667f455a2","An increasing number of geological hazards threaten human life and property in mountainous areas, especially in China. Existing studies on the prevention of geological hazards mainly focus on natural factors and ignore the impact of human activities on geological hazards. This study aims to enrich our knowledge of the impact of human activities through a case study from the Shennongjia mountainous area, China. Spatial regression models were used to quantify the impact of different construction activities on geological hazards based on remote sensing images, local statistical data, land-use data and geological hazards distribution data. The Shennongjia case revealed the following: (1) The global Moran’s I index of the distribution of geological hazards was 0.35, which showed obvious spatial autocorrelation characteristics. (2) From the multiple model comparison, the spatial lag model was more suitable for quantifying the impact of human activities on geological hazards than the least squares regression model and the spatial error model. (3) Road construction and building construction were the main causes of geological hazards, whereas agricultural activities and mining activities had only a limited effect. The evidence reported here could enable governments to constrain human activities and to reduce the geological hazards in mountainous areas across China and beyond. © 2017 Springer Science+Business Media B.V.","Geological hazards; Human activities; Spatial autocorrelation; Spatial regression models",,2-s2.0-85029168814
"Idri A., Kadi I.","A Data Mining-Based Approach for Cardiovascular Dysautonomias Diagnosis and Treatment",2017,"IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340028&doi=10.1109%2fCIT.2017.28&partnerID=40&md5=8c8a9ba277b01051d8429283af6bb318","Autonomic nervous system (ANS) is a control system that acts largely unconsciously and regulates bodily functions. An autonomic malfunction can lead to serious problems related to blood pressure, heart, swallowing, breathing and others. A set of dynamic tests are therefore adopted in ANS units to diagnose and treat patients with cardiovascular dysautonomias. These tests generate big amount of data which are very well suited to be processed using data mining techniques. The purpose of this study is to develop a cardiovascular dysautonomias prediction system to identify the appropriate diagnosis and treatment for patients with cardiovascular dysautonomias using a dataset extracted from the ANS unit of the university hospital Avicenne in Morocco. Classification techniques and association rules were used for the diagnosis and treatment stages respectively. In fact, K-nearest neighbors, C4.5 decision tree algorithm, Random forest, Naïve bayes and Support vector machine were applied to generate the diagnosis classification models and Apriori algorithm was used for generating the association rules. The results obtained for each classifier were analyzed and compared to identify the most efficient one. © 2017 IEEE.","Association techniques; Autonomic nervous system; Classification techniques; Data mining","Association rules; Blood pressure; Computer aided diagnosis; Decision trees; Diagnosis; Nearest neighbor search; Patient treatment; Apriori algorithms; Autonomic nervous system; Bodily functions; C4.5 decision tree algorithm; Classification models; Classification technique; K-nearest neighbors; Prediction systems; Data mining",2-s2.0-85032340028
"Sarker I.H., Colman A., Kabir M.A., Han J.","An effective call prediction model based on noisy mobile phone data",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030848338&doi=10.1145%2f3123024.3123088&partnerID=40&md5=58c725680b8561439c65e0c6553d841e","Noisy instance in mobile phone data is an important issue for modeling user phone call behavior, with many potential negative consequences. The accuracy of prediction may decrease, thereby increasing the complexity of inferred models and the number of training samples needed. In this paper, we present an effective phone call prediction model based on noisy mobile phone data in order to improve the prediction accuracy for individual mobile phone users. Experimental results on the real phone call log datasets show the effectiveness of our prediction model for individual mobile phone users. © 2017 Association for Computing Machinery.","Decision tree; Mobile data mining; Naive bayes classifier; Noisy data; Phone call behavior","Behavioral research; Cellular telephones; Classification (of information); Classifiers; Data mining; Decision trees; Forecasting; Mobile phones; Trees (mathematics); Ubiquitous computing; Wearable computers; Wearable technology; Mobile data mining; Mobile phone datum; Mobile-phone users; Naive Bayes classifiers; Noisy data; Phone calls; Prediction accuracy; Prediction model; Telephone sets",2-s2.0-85030848338
"Sarker I.H., Kabir M.A., Colman A., Han J.","Understanding recency-based behavior model for individual Mobile phone users",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030859733&doi=10.1145%2f3123024.3124570&partnerID=40&md5=9c385d7fbe25603042d1c91a6bd40b8d","Mobile phone log data is not static as it is progressively added to day-by-day according to individual's behavior. The goal of this position paper is to highlight the issues of traditional behavior modeling utilizing phone log data and to describe the key aspects that constitute the foundation of our recency-based behavior modeling for individual mobile phone users to overcome such issues. Copyright © 2017 ACM.","Incremental rule mining; Mobile data mining; Recency; User behavior modeling","Cellular telephones; Data mining; Mobile phones; Telephone sets; Ubiquitous computing; Wearable computers; Wearable technology; Behavior model; Log data; Mobile data mining; Mobile-phone users; Position papers; Recency; Rule mining; User behavior modeling; Behavioral research",2-s2.0-85030859733
"Ruohonen J., Leppanen V.","Investigating the Agility Bias in DNS Graph Mining",2017,"IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376794&doi=10.1109%2fCIT.2017.55&partnerID=40&md5=2f50a26338369287f529ba2983ee3f9e","The concept of agile domain name system (DNS) refers to dynamic and rapidly changing mappings between domain names and their Internet protocol (IP) addresses. This empirical paper evaluates the bias from this kind of agility for DNS-based graph theoretical data mining applications. By building on two conventional metrics for observing malicious DNS agility, the agility bias is observed by comparing bipartite DNS graphs to different subgraphs from which vertices and edges are removed according to two criteria. According to an empirical experiment with two longitudinal DNS datasets, irrespective of the criterion, the agility bias is observed to be severe particularly regarding the effect of outlying domains hosted and delivered via content delivery networks and cloud computing services. With these observations, the paper contributes to the research domains of cyber security and DNS mining. In a larger context of applied graph mining, the paper further elaborates the practical concerns related to the learning of large and dynamic bipartite graphs. © 2017 IEEE.","Bipartite graph; Botnet; CDN; Content delivery network; DNS; Dynamic network; Fast flux; Fluxiness; IPv6","Botnet; Data mining; Distributed computer systems; Graph theory; Bipartite graphs; Content delivery network; Dynamic network; Fast flux; Fluxiness; IPv6; Internet protocols",2-s2.0-85032376794
"Wu H.-Y., Niibe Y., Watanabe K., Takahashi S., Uemura M., Fujishiro I.","Making many-to-many parallel coordinate plots scalable by asymmetric biclustering",2017,"IEEE Pacific Visualization Symposium",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032025855&doi=10.1109%2fPACIFICVIS.2017.8031609&partnerID=40&md5=4a73f5dbb34948c78a5e40ead0682617","Datasets obtained through recently advanced measurement techniques tend to possess a large number of dimensions. This leads to explosively increasing computation costs for analyzing such datasets, thus making formulation and verification of scientific hypotheses very difficult. Therefore, an efficient approach to identifying feature subspaces of target datasets, that is, the subspaces of dimension variables or subsets of the data samples, is required to describe the essence hidden in the original dataset. This paper proposes a visual data mining framework for supporting semiautomatic data analysis that builds upon asymmetric biclustering to explore highly correlated feature subspaces. For this purpose, a variant of parallel coordinate plots, many-to-many parallel coordinate plots, is extended to visually assist appropriate selections of feature subspaces as well as to avoid intrinsic visual clutter. In this framework, biclustering is applied to dimension variables and data samples of the dataset simultaneously and asymmetrically. A set of variable axes are projected to a single composite axis while data samples between two consecutive variable axes are bundled using polygonal strips. This makes the visualization method scalable and enables it to play a key role in the framework. The effectiveness of the proposed framework has been empirically proven, and it is remarkably useful for many-to-many parallel coordinate plots. © 2017 IEEE.",,"Data mining; Visualization; Advanced measurement techniques; Computation costs; Highly-correlated; Parallel coordinate plots; Scientific Hypothesis; Single composites; Visual data mining; Visualization method; Network function virtualization",2-s2.0-85032025855
"Sarker I.H., Kabir M.A., Colman A., Han J.","Designing architecture of a rule-based system for managing phone call interruptions",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030842842&doi=10.1145%2f3123024.3124562&partnerID=40&md5=f4cd03c8f83cddeb3475c0006b80288d","Now-a-days, mobile phones are considered to be ""always on, always connected"" but mobile phone users are not always attentive and responsive to incoming phone calls. Incoming call notifications such as ringing at an inopportune moment (e.g., meeting) can cause interruptions for both the users and the surrounding people. In this paper, we present a system architecture for managing call interruptions according to individual's call response behavioral rules. Copyright © 2017 ACM.","Call response behavior; Context-aware; Mobile data mining; Personalization; Phone call interruptions; Rule discovery; System architecture","Cellular telephones; Computer architecture; Data mining; Mobile phones; Telephone sets; Ubiquitous computing; Wearable computers; Wearable technology; Context-Aware; Mobile data mining; Personalizations; Phone calls; Response behavior; Rule discovery; System architectures; Cellular telephone systems",2-s2.0-85030842842
"Fang D., Kahng M., Hohman F., Sharmin M., Polack P., Al'Absi M., Sarker H., Chau D.H.","Mhealth visual Discovery Dashboard",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030836909&doi=10.1145%2f3123024.3123170&partnerID=40&md5=00d8627c445f4757c74a176c6feeadc0","We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do - in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking. Copyright © 2017 ACM.","Health informatics; Motif discovery; Time series data; Visual analytics","Data mining; Health care; mHealth; Time series; Visualization; Wearable computers; Wearable technology; Data exploration; Health informatics; Interactive exploration; Motif discovery; Time-series data; University of Minnesota; Visual analytics; Visual analytics systems; Ubiquitous computing",2-s2.0-85030836909
"Li Y., Zhao Y., Wang G., Wang Z., Gao M.","ELM-Based Large-Scale Genetic Association Study via Statistically Significant Pattern",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030309635&doi=10.1109%2fTSMC.2017.2720702&partnerID=40&md5=7a4bf6eae2169bb4dc2615f239cbfb4b","Genetic association study (GAS) is a promising tool for detecting and analyzing the cause of complex diseases. The extreme learning machine (ELM) has been successfully applied in a variety of research fields. Yet, as a black box method, it could not measure up to the task of GAS by itself, because it cannot tell us what causes the diseases, which is very crucial to the biologists. In this paper, we propose an ELM-based statistically significant pattern classification framework, which combines ELM with feature vector-based methods to solve the GAS problem efficiently and effectively. In particular: 1) a statistically significant pattern considering in terms of both family wise error rate (FWER) and false discovery rate (FDR) is proposed to control false positives in multiple hypothesis tests, which is necessary in GAS, but ignored by most of the existing methods; 2) an upper bound of the significance of a pattern is deduced to speed up FWER-constrained statistically significant pattern mining in a row enumeration way. Further, a space-effective grid index is devised to dramatically improves the efficiency of FDR-constrained pattern discovery; and 3) an ELM classifier is constructed based on the significant patterns. Comprehensive empirical studies on four real genotype datasets demonstrate much higher efficiency and effectiveness of our proposed framework with respect to the compared methods. IEEE","Biological statistics; classification; Cybernetics; Diseases; extreme learning machine; Genetic algorithms; Genetics; pattern mining; Probability; Testing","Bioinformatics; Classification (of information); Cybernetics; Data mining; Diseases; Efficiency; Genetic algorithms; Knowledge acquisition; Learning systems; Probability; Statistical tests; Testing; Classification framework; Empirical studies; Extreme learning machine; False discovery rate; Genetics; Multiple hypothesis tests; Pattern mining; Significant patterns; Pathology",2-s2.0-85030309635
"Onoue Y., Koyamada K.","Quasi-biclique edge concentration: A visual analytics method for biclustering",2017,"IEEE Pacific Visualization Symposium",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032010885&doi=10.1109%2fPACIFICVIS.2017.8031597&partnerID=40&md5=abc8c39214da6b5182206ec147029ad7","Biclustering is a well-known approach for data mining, and it is applied in many fields, such as genome analyses, security services, and social network analyses. Biclustering finds bicliques contained in a bipartite graph. However, in real data, a biclique may lack several edges because of various reasons, such as errors. In this situation, traditional biclustering methods cannot find correct biclusters. A novel biclustering method that can analyze real data under uncertainty is needed. Quasi-biclique is a mathematical concept that represents incomplete bicliques. We propose the quasi-biclique edge concentration (QBEC) method, which is a visual analysis method for biclustering using quasi-biclique mining. QBEC includes visual representations and user interactions for quasi-bicliques. Quasi-bicliques contained in a bipartite graph are represented based on edge concentration. The incompleteness of a quasi-biclique is reflected in edge opacity. Users can interactively explore data by adjusting the incompleteness parameter of the quasi-biclique. We demonstrate the effectiveness of QBEC using real-world data. © 2017 IEEE.","G.2.2 [Discrete Mathematics]: Graph Theory - Graph Algorithms; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Clustering","Data mining; Digital storage; Trees (mathematics); Uncertainty analysis; Visualization; Edge concentrations; Graph algorithms; H.3.3 [information storage and retrieval]: information search and retrievals; Mathematical concepts; Security services; User interaction; Visual analytics; Visual representations; Graph theory",2-s2.0-85032010885
"Uysal A.K., Murphey Y.L.","Sentiment Classification: Feature Selection Based Approaches Versus Deep Learning",2017,"IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340653&doi=10.1109%2fCIT.2017.53&partnerID=40&md5=69a88d631f7599f34d54f912d833d1b0","Classification of text documents is commonly carried out using various models of bag-of-words that are generated using feature selection methods. In these models, selected features are used as input to well-known classifiers such as Support Vector Machines (SVM) and neural networks. In recent years, a technique called word embeddings has been developed for text mining and, deep learning models using word embeddings have become popular for sentiment classification. However, there is no extensive study has been conducted to compare these approaches for sentiment classification. In this paper, we present an in-depth comparative study on these two types of approaches, feature selection based approaches and and deep learning models for document-level sentiment classification. Experiments were conducted using four datasets with varying characteristics. In order to investigate the effectiveness of word embeddings features, feature sets including combination of selected bag-of-words features and averaged word embedding features were used in sentiment classification. For analyzing deep learning models, we implemented three different deep learning architecture, convolutional neural network, long short-term memory network, and long-term recurrent convolutional network. Our experimental results show that that deep learning models performed better on three out of the four datasets, a combination of selected bag-of-words features and averaged word embedding features gave the best performance on one dataset. In addition, we will show that a deep learning model initialized with either one-hot vectors or fine-tuned word embeddings performed better than the model initialized using than word embeddings without tuning. © 2017 IEEE.","Deep learning; Feature selection; Sentiment classification; Word embeddings","Convolution; Data mining; Deep learning; Feature extraction; Information retrieval systems; Long short-term memory; Neural networks; Support vector machines; Text processing; Comparative studies; Convolutional networks; Convolutional neural network; Embeddings; Feature selection methods; Learning architectures; Sentiment classification; Short term memory; Classification (of information)",2-s2.0-85032340653
"Prasertsung P., Horanont T.","How does coffee shop get crowded?: Using WiFi footprints to deliver insights into the success of promotion",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030855250&doi=10.1145%2f3123024.3124418&partnerID=40&md5=81823cd85abf684734c68193f51b4398","Real time people density estimation is one of the biggest challenges of today research. This information can be applied to various urban applications such as advertising,traffic planning, and resource management. The recent researches have demonstrated on crowded estimation using various cutting-edge technologies to solve this problem. Since WiFi access points already exist in the major buildings and shops. They become a great tool to estimate the density of people. This research explores the opportunities to use existing access points in order to estimate density of people in the real world environment. WiFi probe request monitoring technique is used to identify the number of the customer's visit to a coffee shop. The result shows that during weekdays, the number of customers in promotion period is 30.43% greater than non-promotion period. © 2017 Association for Computing Machinery.","Geolocation; Knowledge discovery; Mobile devices; Retail analytics; WiFi footprint","Data mining; Information management; Mobile devices; Mobile telecommunication systems; Sales; Ubiquitous computing; Wearable computers; Wearable technology; Cutting edge technology; Geolocations; Monitoring techniques; Real world environments; Resource management; Retail analytics; Urban applications; Wi-fi access points; Wireless local area networks (WLAN)",2-s2.0-85030855250
"Phithakkitnukoon S., Horanont T., Bhattacharya S., Sekimoto Y.","PURBA 2017: The 6th Workshop on Pervasive Urban Applications",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030830443&doi=10.1145%2f3123024.3124453&partnerID=40&md5=52350b742a0013626dc964da2d2c035f","The 6th Workshop on Pervasive Urban Applications (PURBA 2017) aims to build on the success of the previous workshops organized in conjunction with the Pervasive (2011, 2012) and UbiComp (2013, 2015, 2016) to continue to disseminate the results of the latest research outcomes and developments of ubiquitous computing technologies for urban applications. All workshop contributions are published in supplemental proceedings of the UbiComp 2017 conference and included in the ACM Digital Library. © 2017 Association for Computing Machinery.","Data mining; Smart cities; Ubiquitous computing; Urban computing; Urban informatics","Data mining; Digital libraries; Smart city; Wearable computers; Wearable technology; Research outcome; Ubicomp; Ubiquitous computing technology; Urban applications; Urban computing; Urban Informatics; Ubiquitous computing",2-s2.0-85030830443
"He M., Kong Y., Gu W.","Group recommendation: By mining users' check-in behaviors",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030847018&doi=10.1145%2f3123024.3123099&partnerID=40&md5=b4adb8f629facc711ec7ff9bff5f3fd8","In the field of Point-of-interest(POI) recommendation, most existing work focus on individual recommendation. As to the scenario for group, there is still no consensus. In this paper, we propose a novel model to deal with the issue of group recommendation. Specifically, we take the interaction among group members into consideration and reconstruct the dynamic decision-making process within a group based on the inspiration of Pareto Improvement. Besides, we employ both short-term and long-term temporal factors, as well as content factors in different stages within the generation of recommendation list. Finally, the results of extensive experiments on two real datasets validate the effectiveness and efficiency of it, compared with other baseline approaches. © 2017 Association for Computing Machinery.",,"Decision making; Wearable computers; Wearable technology; Different stages; Dynamic decision making; Effectiveness and efficiencies; Group members; Group recommendations; Pareto improvements; Point of interest; Real data sets; Ubiquitous computing",2-s2.0-85030847018
"Zhou M., Sui K., Pei D., Moscibroda T.","Mining crowd mobility and WiFi hotspots on a densely-populated campus",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030870597&doi=10.1145%2f3123024.3124419&partnerID=40&md5=66b31cb17efceb70a407457551036210","Understanding crowd activities at large-scale and diagnosing existing problems of planning on densely-populated campus are fundamentally hard through traditional ways of measurement and management. In this paper, we demonstrate how to collect data from ubiquitous WiFi networks (WLAN), and further to characterize the mobility of campus residents by exploring time-frequency patterns with spatial context. On the campus of Tsinghua University (where everyday nearly 60, 000 mobile devices appear in the public areas of more than 110 buildings), we obtain large-scale observations on physical activities, and provide insights for better diagnosing of WiFi hotspots. © 2017 Association for Computing Machinery.",,"Wearable computers; Wearable technology; Wi-Fi; Wireless local area networks (WLAN); Existing problems; Physical activity; Public areas; Spatial context; Time frequency; Tsinghua University; Wi Fi networks; WiFi hotspots; Ubiquitous computing",2-s2.0-85030870597
"Chaturvedi R., Ezeife C.I.","Predicting Student Performance in an ITS Using Task-Driven Features",2017,"IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032359609&doi=10.1109%2fCIT.2017.34&partnerID=40&md5=a5cc13a55637a83febc16744d389f9d2","Intelligent Tutoring Systems (ITS) are typically designed to offer one-on-one tutoring on a subject to students in an adaptive way so that students can learn the subject at their own pace. The ability to predict student performance enables an ITS to make informed decisions towards meeting the individual needs of students. It is also useful for ITS designers to validate if students are actually able to succeed in learning the subject. Predicting student performance is a function of two complex and dynamic factors: (f1) student learning behavior and (f2) their current knowledge in the subject. Learning behavior is captured from student interaction with the ITS (e.g. Time spent on an assigned task) and is stored in the form of web logs. Student knowledge in the subject is represented by the marks they score in assigned tasks and is stored in a specific component of the ITS called student model. In order to build an accurate prediction model, this raw data from student model and web logs must be engineered carefully and transformed into meaningful features. Existing systems such as LON-CAPA predict students performance using their learning behavior alone, without considering their (current) knowledge on the subject. Lack of proper feature engineering is evident from the low values of accuracy of their prediction models. This research proposes a highly accurate model that predicts student success in assigned tasks with a 96% accuracy by using features that are better informed not only about students in terms of the two factors f1 and f2 mentioned above, but also on the assigned task itself (e.g. Task's difficulty level). In order to accomplish this, an Example Recommendation System (ERS) is designed with a fine-grained student model (to represent student data) and a fine-grained domain model (to represent domain resources such as tasks). © 2017 IEEE.","Domain model; Feature engineering; ITS; Performance; Predictive mining; Student model; Task-driven","Blogs; Computer aided instruction; Education; Forecasting; Intelligent vehicle highway systems; Domain model; Feature engineerings; Performance; Predictive minings; Student Modeling; Task-driven; Students",2-s2.0-85032359609
"Wang B., Zhang T., Chang Z., Ristaniemi T., Liu G.","3D Matrix-Based Visualization System of Association Rules",2017,"IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032394793&doi=10.1109%2fCIT.2017.39&partnerID=40&md5=1ed069632bed48f13654b89b61ceb560","With the growing number of mining datasets, it becomes increasingly difficult to explore interesting rules because of the large number of resultant and its nature complexity. Studies on human perception and intuition show that graphical representation could be a better illustration of how to seek information from the data using the capabilities of human visual system. In this work, we present and implement a 3D matrix-based approach visualization system of association rules. The main visual representation applies the extended matrix-based approach with rule-to-items mapping to general transaction data set. A novel method merging rules and assigning weight is proposed in order to reduce the dimension of the association rules, which will help users to find more important items in the new rule. Furthermore, several interactions such as sorting, filtering, zoom and rotation, facilitate decision-makers to explore the rules which are of interest in various aspects. Finally, extensive evaluations have been conducted to assess the system from a logical reasoning point of view. © 2017 IEEE.","Association rule; Matrix-based apporach; Reduction; Visualization system","Association rules; Decision making; Reduction; Three dimensional computer graphics; Visualization; Graphical representations; Human Visual System; Interesting rules; Logical reasoning; Matrix based approach; Transaction data; Visual representations; Visualization system; Matrix algebra",2-s2.0-85032394793
"Marchetti T., Rossi E.M., Kordopatis G., Brown A.G.A., Rimoldi A., Starkenburg E., Youakim K., Ashley R.","An artificial neural network to discover hypervelocity stars: Candidates in Gaia DR1/TGAS",2017,"Monthly Notices of the Royal Astronomical Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023777503&doi=10.1093%2fmnras%2fstx1304&partnerID=40&md5=23e4abf98b0e02fbff68f98f87839f9c","The paucity of hypervelocity stars (HVSs) known to date has severely hampered their potential to investigate the stellar population of the Galactic Centre and the Galactic potential. The first Gaia data release (DR1, 2016 September 14) gives an opportunity to increase the current sample. The challenge is the disparity between the expected number of HVSs and that of bound background stars. We have applied a novel data mining algorithm based on machine learning techniques, an artificial neural network, to the Tycho-Gaia astrometric solution catalogue. With no pre-selection of data, we could exclude immediately ~99 per cent of the stars in the catalogue and find 80 candidates with more than 90 per cent predicted probability to be HVSs, based only on their position, proper motions and parallax. We have crosschecked our findings with other spectroscopic surveys, determining radial velocities for 30 and spectroscopic distances for five candidates. In addition, follow-up observations have been carried out at the Isaac Newton Telescope for 22 stars, for which we obtained radial velocities and distance estimates. We discover 14 stars with a total velocity in the Galactic rest frame &gt;400 km s-1, and five of these have a probability of &gt;50 per cent of being unbound from the Milky Way. Tracing back their orbits in different Galactic potential models, we find one possible unbound HVS with ν ~ 520 km s-1, five bound HVSs and, notably, five runaway stars with median velocity between 400 and 780 km s-1. At the moment, uncertainties in the distance estimates and ages are too large to confirm the nature of our candidates by narrowing down their ejection location, and we wait for future Gaia releases to validate the quality of our sample. This test successfully demonstrates the feasibility of our new data-mining routine. © 2017 The Authors. Published by Oxford University Press on behalf of the Royal Astronomical Society.","Galaxy: centre; Galaxy: kinematics and dynamics; Galaxy: stellar content",,2-s2.0-85023777503
"Qiu H., Zhu B., Ni S.","Identification of genes associated with primary open-angle glaucoma by bioinformatics approach",2017,"International Ophthalmology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029024692&doi=10.1007%2fs10792-017-0704-2&partnerID=40&md5=c0a11ac0008fa2cb3eac79ca5cb3d645","Purpose: This study aimed to identify associated genes with primary open-angle glaucoma (POAG) and explore the potentially modular mechanism underlying POAG. Methods: We downloaded gene expression profiles data GSE27276 from gene expression omnibus and identified differentially expressed genes between POAG patients and normal controls. Then, gene ontology analysis and kyoto encyclopedia of genes and genomes pathway enrichment were performed to predict the DEGs functions, followed with the construction, centrality analysis, and module mining of protein–protein interaction network. Results: A total of 552 DEGs including 249 up-regulated and 303 down-regulated genes were identified. The up-regulated DEGs were significantly involved in cell adhesion molecule, while the down-regulated DEGs were significantly involved in complement and coagulation cascades. Centrality analysis screened out 20 genes, among which COL4A4, COL3A1, COL1A2, ITGB5, COL5A2, and COL5A1 were shared in ECM–receptor interaction and focal adhesion pathways. In the sub-network, COL5A2, COL8A2, and COL5A1 were significantly enriched in biological function of eye morphogenesis and eye development, while LAMA5, COL3A1, COL1A2, and COL5A1 were significantly enriched in vasculature development and blood vessel development. Conclusions: Six genes, including COL4A4, COL3A1, COL1A2, ITGB5, COL5A2, and COL5A1, ECM–receptor interaction and focal adhesion pathway, are potentially involved in the pathogenesis of POAG via participating in pathways of ECM–receptor interaction and focal adhesion. © 2017 Springer Science+Business Media B.V.","Bioinformatics; Collagen; Primary open-angle glaucoma; Protein–protein interaction network",,2-s2.0-85029024692
"Xue Z., Hu H., Song Y., Wu Z., Liu D., Feng H., Song H.","Comprehensive evaluation based on big data analysis for county electric power company",2017,"Dianli Zidonghua Shebei/Electric Power Automation Equipment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029650588&doi=10.16081%2fj.issn.1006-6047.2017.09.028&partnerID=40&md5=ea24ec5da54264acd6e69988b89794d1","A strategy of comprehensive evaluation based on big data analysis is proposed for county electric power company. With the big data technologies, including statistical analysis, decision-tree analysis, data envelopment analysis, etc., a strategy of index mining is developed based on the data of 1780 county electric power companies and a comprehensive evaluation index system for county electric power company is established by the information mining. With the economic and business factors as the rules, the country electric power companies are clustered and a method of differentiated weight calculation is derived to jointly identify the endogenous and exogenous factors of differentiated county company developments for avoiding the simplified explanation of comprehensive evaluation results. The application of the proposed strategy in Zhejiang Province verifies its effectiveness and feasibility. © 2017, Electric Power Automation Equipment Press. All right reserved.","Big data analysis; Comprehensive evaluation; County electric power company; Data mining; Index system","Data envelopment analysis; Data handling; Data mining; Decision trees; Electric utilities; Information analysis; Trees (mathematics); Company development; Comprehensive evaluation; Comprehensive evaluation index system; Decision tree analysis; Differentiated weights; Electric power company; Index systems; Information mining; Big data",2-s2.0-85029650588
"Mehrdad A., Parvini E.","Interactions of sodium polystyrene sulfonate with 1-hexyl-3-methylimidazolium bromide in aqueous solution: conductometry and density functional theory studies",2017,"Physics and Chemistry of Liquids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029529490&doi=10.1080%2f00319104.2017.1367393&partnerID=40&md5=9aa73d73fd770ab6db2794da45e30584","Conductivity measurements of sodium polystyrene sulfonate (NaPSS) were carried out in aqueous solutions of 1-hexyl-3-methylimidazolium bromide ([HMIm]Br) at three different temperatures. The scaling theory is used for the description of electrical conductance of polyelectrolytes. The fraction of uncondensed counterions is decreased by increasing temperature and concentration of [HMIm]Br. Conductivity measurements of ([HMIm]Br were accomplished in aqueous solutions of sodium polystyrene sulfonate. Data analysis was performed by the Quint–Viallard conductivity equation and low concentration chemical model. Molar conductivity of [HMIm]Br in aqueous solutions of NaPSS was increased with increasing temperature. The values of activation energy for viscous flow are higher than the values of activation enthalpy of charge transport; therefore, it can be concluded that in addition of ion transfer, the formation and breaking hydrogen has a portion in charge transfer. The results of Quantum chemical calculations confirm the existence of hydrogen bonding between [HMIm]+ and [PSS]–. © 2017 Informa UK Limited, trading as Taylor & Francis Group","1-Hexyl-3-methylimidazolium bromide; conductivity; density functional theory; sodium polystyrene sulfonate","Activation energy; Charge transfer; Chemical activation; Chemical analysis; Chemical bonds; Electric conductivity; Electric conductivity measurement; Hydrogen bonds; Polyelectrolytes; Polystyrenes; Quantum chemistry; Solution mining; Solutions; Temperature; 1-Hexyl-3-methylimidazolium bromide; Conductivity equations; Conductivity measurements; Density functional theory studies; Electrical conductance; Increasing temperatures; Quantum chemical calculations; Sodium polystyrene sulfonate; Density functional theory",2-s2.0-85029529490
"Jeon H., Lee W., Park H., Lee H.J., Kim S.K., Kim H.B., Jeon B., Park K.S.","Automatic classification of tremor severity in Parkinson’s disease using awearable device",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029364479&doi=10.3390%2fs17092067&partnerID=40&md5=3ac15317877a5e8a9337a1367ebd9cbb","Although there is clinical demand for new technology that can accurately measure Parkinsonian tremors, automatic scoring of Parkinsonian tremors using machine-learning approaches has not yet been employed. This study aims to fill this gap by proposing machine-learning algorithms as a way to predict the Unified Parkinson’s Disease Rating Scale (UPDRS), which are similar to how neurologists rate scores in actual clinical practice. In this study, the tremor signals of 85 patients with Parkinson’s disease (PD) were measured using a wrist-watch-type wearable device consisting of an accelerometer and a gyroscope. The displacement and angle signals were calculated from the measured acceleration and angular velocity, and the acceleration, angular velocity, displacement, and angle signals were used for analysis. Nineteen features were extracted from each signal, and the pairwise correlation strategy was used to reduce the number of feature dimensions. With the selected features, a decision tree (DT), support vector machine (SVM), discriminant analysis (DA), random forest (RF), and k-nearest-neighbor (kNN) algorithm were explored for automatic scoring of the Parkinsonian tremor severity. The performance of the employed classifiers was analyzed using accuracy, recall, and precision, and compared to other findings in similar studies. Finally, the limitations and plans for further study are discussed. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Automatic scoring; Machine learning algorithm; Parkinson’s disease; Tremor; UPDRS; Wearable device","Angular velocity; Artificial intelligence; Data mining; Decision trees; Discriminant analysis; Learning systems; Nearest neighbor search; Support vector machines; Wearable technology; Automatic classification; Automatic scoring; K nearest neighbor algorithm; Machine learning approaches; Pairwise correlation; Tremor; UPDRS; Wearable devices; Learning algorithms",2-s2.0-85029364479
"Yang J., Zhao J., Wen F., Dong Z.","A Framework of Customizing Electricity Retail Prices",2017,"IEEE Transactions on Power Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030317010&doi=10.1109%2fTPWRS.2017.2751043&partnerID=40&md5=05391a25a2bb799b88fedbab43da8fc1","The problem of designing customized pricing strategies for different residential users is investigated based on the identification results of residential electric appliances and classifications of end-users according to their consumption behaviours. This study is based on following assumptions: 1) each retailer purchases electricity from the forward contract market, day-ahead spot market and real time market; 2) the competition among retailers is modelled by a market share function; 3) each retailer adopts fixed time-of-use (TOU) prices for end-users; 4) the price fluctuations in day-ahead and real time spot markets as well as uncertainty of electricity consumption behaviours are considered as main sources of risk. Under these assumptions, a pricing framework for retailers is established based on the bi-level programming framework and the optimal clustering in a time sequence. Meanwhile, profit risk is considered by taking conditional value at risk (CVaR) as the risk measure. The proposed bi-level optimization model is finally reformulated into a mixed-integer non-linear programming problem (MINLP) by solving Karush-Kuhn-Tucker (KKT) conditions. The online optimization solvers provided by the NEOS (Network-Enabled Optimization System) server and the commercial solver AMPL/GUROBI are used to solve the developed models, respectively. Finally, a case study is employed to demonstrate the feasibility and efficiency of the developed models and algorithms. IEEE","appliance identification; Classification algorithms; Clustering algorithms; custom-ized retail price; Data mining; Electricity retailing; end-user classification; Home appliances; Load modeling; optimal structure of TOU price; Pricing; Real-time systems","Commerce; Competition; Costs; Data mining; Domestic appliances; Electric load management; Housing; Integer programming; Interactive computer systems; Nonlinear programming; Optimization; Real time systems; Risk assessment; Risk perception; Sales; Structural optimization; Value engineering; Classification algorithm; Electricity retailing; End users; Load modeling; Optimal structures; Retail price; Clustering algorithms",2-s2.0-85030317010
"He Q., Zhu X., Li D., Wang S., Shen J., Yang Y.","Cost-Effective Big Data Mining in the Cloud: A Case Study with K-means",2017,"IEEE International Conference on Cloud Computing, CLOUD",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032196042&doi=10.1109%2fCLOUD.2017.124&partnerID=40&md5=49a8cfc9f610867284e457808505b829","Mining big data often requires tremendous computational resources. This has become a major obstacle to broad applications of big data analytics. Cloud computing allows data scientists to access computational resources on-demand for building their big data analytics solutions in the cloud. However, the monetary cost of mining big data in the cloud can still be unexpectedly high. For example, running 100 m4-xlarge Amazon EC2 instances for a month costs approximately 17,495.00. On this ground, it is a critical issue to analyze the cost effectiveness of big data mining in the cloud, i.e., how to achieve a sufficiently satisfactory result at the lowest possible computation cost. In certain big data mining scenarios, 100% accuracy is unnecessary. Instead, it is often more preferable to achieve a sufficient accuracy, e.g., 99%, at a much lower cost, e.g., 10%, than the cost of achieving the 100% accuracy. In this paper, we explore and demonstrate the cost effectiveness of big data mining with a case study using well known k-means. With the case study, we find that achieving 99% accuracy only needs 0.32%-46.17% computation cost of 100% accuracy. This finding lays the cornerstone for cost-effective big data mining in a variety of domains. © 2017 IEEE.","Big Data; Cloud Computing; Cost-Effective; Data Mining; K-Means","Cloud computing; Cost effectiveness; Costs; Data mining; Network function virtualization; Broad application; Computation costs; Computational resources; Cost effective; Critical issues; Data analytics; K-means; Monetary costs; Big data",2-s2.0-85032196042
"Dong B., Lin M.M., Park H.","Integer Matrix Approximation and Data Mining",2017,"Journal of Scientific Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028974202&doi=10.1007%2fs10915-017-0531-7&partnerID=40&md5=7d5c8b648c9982d54a0a97b59a322fe3","Integer datasets frequently appear in many applications in science and engineering. To analyze these datasets, we consider an integer matrix approximation technique that can preserve the original dataset characteristics. Because integers are discrete in nature, to the best of our knowledge, no previously proposed technique developed for real numbers can be successfully applied. In this study, we first conduct a thorough review of current algorithms that can solve integer least squares problems, and then we develop an alternative least square method based on an integer least squares estimation to obtain the integer approximation of the integer matrices. We discuss numerical applications for the approximation of randomly generated integer matrices as well as studies of association rule mining, cluster analysis, and pattern extraction. Our computed results suggest that our proposed method can calculate a more accurate solution for discrete datasets than other existing methods. © 2017 Springer Science+Business Media, LLC","Association rule; Clustering; Data mining; Integer least squares problem; Matrix factorization; Pattern extraction","Approximation algorithms; Association rules; Cluster analysis; Data mining; Extraction; Factorization; Least squares approximations; Clustering; Integer approximation; Integer least squares; Integer least-squares problems; Matrix factorizations; Numerical applications; Pattern extraction; Science and engineering; Matrix algebra",2-s2.0-85028974202
"Mantovani M.","Approximate Temporal Functional Dependencies on Clinical Data",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032390735&doi=10.1109%2fICHI.2017.30&partnerID=40&md5=77a03b9b66a29f11dbafddc2985b476b","The aim of this PhD project is to provide a framework for temporal data mining. © 2017 IEEE.","Functional dependency; Olap; Quantitative functional dependency; Temporal data mining; Temporal functional dependency","Health care; Clinical data; Functional dependency; Olap; PhD project; Temporal data mining; Temporal functional dependencies; Data mining",2-s2.0-85032390735
"Choi Y., McGregor C.","A Flexible Parental Engaged Consent Model for the Secondary Use of Their Infant's Physiological Data in the Neonatal Intensive Care Context",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364705&doi=10.1109%2fICHI.2017.88&partnerID=40&md5=fbfdb506bd79537e88b51855a1ba35a4","The secondary use of health data, especially the use of physiological data for research holds many opportunities for improving the current understanding of neonatal conditions. As a neonate is unable to provide their consent regarding participation in research studies, a substitute decision maker (SDM) must provide parental or legal guardian consent. However it has been well documented that there are many emotional, mental and physical challenges associated with the parental consent process in the neonatal intensive care unit (NICU). It is proposed that a flexible parental engaged consent model could help alleviate some of these issues by providing parents with the ability to choose and change their clinical engagement level preference for their infant's participation in research at their convenience at any point in time. In this paper, an extension to Service based Multidimensional Temporal Data Mining Framework (STDMn0) to allow for the functionality of flexible patient or surrogate consent is presented based on the use of a flexible consent model initially proposed by Heath [1]. This functionality is demonstrated via an example implementation for a generic retrospective research study in the NICU setting. © 2017 IEEE.","Big Data; Consent; Neonatal Intensive Care Unit; Patient Engagement","Data mining; Decision making; Health care; Intensive care units; Physiological models; Physiology; Consent; Engagement levels; Neonatal intensive care; Neonatal intensive care units; Patient Engagement; Physiological data; Research studies; Temporal data mining; Big data",2-s2.0-85032364705
"Alodadi M.","Radiology Clinical Notes Mining Using Weighted Association Rules",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032355901&doi=10.1109%2fICHI.2017.23&partnerID=40&md5=03dc9fe036244f3db2bb47f7fa673241","Electronic health record (EHR) serves to capture the patients' medical conditions and detailed visits information. The structured part of EHR can be used to serve administrative and financial management of patients. However, the unstructured part, that contains the interventions applied to the patient and transcribed in textual format, have unexplored knowledge due to the narrative format. Unlike conventional data mining techniques that can be applied to structured databases, applying large and automated analysis on clinical notes can potentially provide better support for medical decision making for an individual patient or for collective of patients. It can also allow for discovery of emerging associations to explore relationships among the patients using the data stored in the unstructured text of the EHR. © 2017 IEEE.","Clincal notes; Text mining; Weighted association rules mining","Association rules; Decision making; Health care; Medical computing; Patient monitoring; Patient treatment; Clincal notes; Conventional data mining; Electronic health record; Financial managements; Medical decision making; Structured database; Text mining; Weighted association rules; Data mining",2-s2.0-85032355901
"Wei Z., Li X., Li X., Hu Q., Zhang H., Cui P.","Medium- and long-term electric power demand forecasting based on the big data of smart city",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640909&doi=10.1088%2f1742-6596%2f887%2f1%2f012025&partnerID=40&md5=749e5f14f54027793659d57af7789715","Based on the smart city, this paper proposed a new electric power demand forecasting model, which integrates external data such as meteorological information, geographic information, population information, enterprise information and economic information into the big database, and uses an improved algorithm to analyse the electric power demand and provide decision support for decision makers. The data mining technology is used to synthesize kinds of information, and the information of electric power customers is analysed optimally. The scientific forecasting is made based on the trend of electricity demand, and a smart city in north-eastern China is taken as a sample. © Published under licence by IOP Publishing Ltd.",,"Artificial intelligence; Big data; Decision making; Decision support systems; Electric power utilization; Forecasting; Information systems; Population statistics; Smart city; Data mining technology; Decision makers; Decision supports; Economic information; Electric power demands; Electricity demands; Geographic information; Meteorological information; Data mining",2-s2.0-85030640909
"Ho P.H., Vo T.H., Nguyen N.A.T.","Data warehouse designing for Vietnamese textual document-based plagiarism detection system",2017,"Proceedings - 2017 International Conference on System Science and Engineering, ICSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364177&doi=10.1109%2fICSSE.2017.8030873&partnerID=40&md5=8aa5a3a11e42b1833d242f51dd9d585d","In this paper, the significance role of data warehouse designing for textual anti-plagiarism system is investigated. The paper covers the central issues of data warehousing modeling including: (1) formulating the data representation, (2) establishing the foundations of storage structure, (3) proposing corresponding architecture allowing to store, update and manage data. Consequently, two levels are considered in this paper to address the above mentioned research axes. First, at a theoretical level, the objective is to introduce novel and practical contributions in the area of textual document-based plagiarism system. The chosen approach is proposed to collect, analysis and store textual dataset. Secondly, at an implementation level, the paper focuses on the platform for processing the data, calling to modeling exhibits promising capabilities such as support for real-time, new sources of data, and self-service capabilities. The real application is performed in Vietnamese text-based document by conducting documents containing final reports/assignments, dissertations of master/Ph.D and research scientific papers applied for the University of Danang. The contribution of the paper is not only provide values to all researchers, educators and students in the university of Danang systems but also be considered as seminal work to develop plagiarism in our further next investigation of building a big-data warehouse severing for a automatic duplicate system. © 2017 IEEE.","Data mining; Data warehouse; DSpace; Plagiarism Dection System; Vietnamese Text","Big data; Data handling; Data mining; Data warehouses; Digital storage; Intellectual property; D-space; Data representations; Plagiarism Dection System; Plagiarism detection; Service capability; Storage structures; Text-based documents; Vietnamese; Information management",2-s2.0-85032364177
"Combi C., Mantovani M., Sala P.","Discovering Quantitative Temporal Functional Dependencies on Clinical Data",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032334222&doi=10.1109%2fICHI.2017.80&partnerID=40&md5=86257879e5eb92285d60d0e7e57e8439","Approximate functional dependencies, even with suitable temporal extensions, have been recently proposed as a methodological tool for mining clinical data. It allows healthcare stakeholders to derive new knowledge from overwhelming amount of healthcare and clinical data. Some examples of the kind of knowledge derivable from data through dependencies may be 'month by month, patients with the same symptoms get the same type of therapy' or 'within 15 days, patients with the same diagnosis and the same therapy receive the same daily amount of drug'. The main limitation of such kind of dependencies is that they cannot deal with quantitative data, when some tolerance can be allowed for numerical values. In particular, such limitation arises in clinical data warehouses, where analysis and mining have to consider one or more measures (related to quantitative data as lab test results, vital signs as blood pressures, temperature and so on), with respect to many dimensional (alphanumeric) attributes (as patient, hospital, physician, diagnosis) and to some time dimensions (as the day since hospitalization, the calendar date, and so on). According to this scenario, we introduce here a new kind of approximate temporal functional dependency, named multi approximate temporal functional dependency (MATFD), which consider dependencies between dimensions and quantitative measures from temporal clinical data. Such new dependencies may provide new knowledge as 'within 15 days, patients with the same diagnosis and the same therapy receive a daily amount of drug within a fixed range'. Moreover, we provide an original algorithm to mine such kind of dependencies and to derive some core dependencies, both for the discovered temporal window and for the involved dimensional attributes. Finally, we discuss some first results we obtained by pre-processing and mining ICU data from MIMIC III database. © 2017 IEEE.","Clinical Database; Data Mining; Functional Dependencies; OLAP; Temporal Database","Blood pressure; Data mining; Data warehouses; Diagnosis; Drug therapy; Health care; Hospitals; Clinical data warehouse; Clinical database; Functional dependency; OLAP; Quantitative measures; Temporal clinical datum; Temporal Database; Temporal functional dependencies; Patient treatment",2-s2.0-85032334222
"Johnson O.A.","Process Mining and Analytics for Care Pathways: Proposal for a Tutorial at ICHI2017",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032354125&doi=10.1109%2fICHI.2017.104&partnerID=40&md5=1bdf3cebf92686af223d7bcd169c0db9","This tutorial demonstrates the application of a range of process analytics techniques to the study and improvement of clinical care pathways. The past decade has seen increasing interest in care pathway design, documentation and dissemination but formal methods for describing, monitoring and assessing pathways have yet to be established. Outside of healthcare other industries have well established techniques for business processes and there is much scope for translating these to fit the unique nature of healthcare. In particular data analytics, data mining, and machine learning have converged on a set of technologies called process mining which has the potential to lead to a step-change in using e-health record data to mine and manage care pathways. The tutorial presents an iterative method developed with the UK NHS and the Connected Health Cities programme that combines process mining with other process analytics methods including process modeling, process simulation and business process improvement. The session is highly interactive and based on a series of hands-on exercises around a worked example supplemented by case studies of completed work. Links to further study are provided and the aim is encourage further research and build a global community of practice. © 2017 IEEE.","Business process improvement; Care pathways; Data analytics; E-health records; Process mining","Formal methods; Health; Health care; Iterative methods; Learning systems; Process engineering; Business process improvement; Care pathways; Data analytics; E-health records; Process mining; Data mining",2-s2.0-85032354125
"Phuong N., Duy N.D.","Constructing a new algorithm for high average utility Itemsets mining",2017,"Proceedings - 2017 International Conference on System Science and Engineering, ICSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032379476&doi=10.1109%2fICSSE.2017.8030880&partnerID=40&md5=ff2bf0bfe5269f8424b5de38e07d7538","In this paper, we propose a mining algorithm for average-utility itemsets (EHAUI-Tree) based on improving HUUI-Tree algorithm to apply for adding new database transactions without restart. At first, the value of updated data is calculated. Then, itemsets which make changes will be calculated and updated depending upon the updated data value and the previous High Average-utility Upper-bound (HAUUB). This algorithm uses the downward closure property of an average-utility itemset and an index table structure. In addition, a data structure for itemsets is proposed to minimize memory usage and maximize calculating efficiency. The experimental result shows that EHAUI-Tree is more effective than HAUI-Tree when adding new transactions for the previous database. The method applies the downward closure properties of HAUUB Itemset and Index Table. Furthermore, the Bit-Array-structure itemset is also proposed to reduce using memory and calculate more effectively. The result of this algorithm is better than HAUI-Tree on updating new transactions. © 2017 IEEE.","average-utility; data mining; dynamic databases; HAUI-Tree; high average-utility itemsets; itemset mining; mining algorithm","Data mining; Database systems; Forestry; Average utilities; Dynamic database; HAUI-Tree; Item sets; Itemset mining; Mining algorithms; Trees (mathematics)",2-s2.0-85032379476
"Liu M., Li G., Liu B.","Real-Time knowledge discovery from public data of Internet to improve decision-making of Autonomous Vehicles",2017,"2017 2nd International Conference on Reliability Systems Engineering, ICRSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032256583&doi=10.1109%2fICRSE.2017.8030771&partnerID=40&md5=fa977fdcdbc1df9826970b45528db535","with the help of the base installations of network services, mobile networks and pervasive computing, numerous real-Time data information of external environment is available from the Internet and is valuable for autonomous vehicles to improve decision-making. Accordingly, in this paper, an approach is provided to collect useful data from the Internet and discover knowledge from these data. The paper introduces the principles and procedures of the approach detailedly. The approach is featured with low cost, easy deployment and can be widely used in many kinds of autonomous vehicles. © 2017 IEEE.","Autonomous vehicles; Decision-making; Embedded framework; Knowledge discovery","Data mining; Internet; Systems engineering; Ubiquitous computing; Vehicles; Autonomous Vehicles; Embedded framework; External environments; Low costs; Network services; Public data; Real time; Real-time data; Decision making",2-s2.0-85032256583
"Gavai G., Nabi M., Bobrow D., Shahraz S.","Heterogenous Knowledge Discovery from Medical Data Ontologies",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032329980&doi=10.1109%2fICHI.2017.60&partnerID=40&md5=abdaaa2e391446bf6db9d4975e5f1cbd","A variety of knowledge discovery applications on healthcare big data require effective medical ontologies of diseases that can abstract the healthcare record data in order to support formal reasoning. Domain specific ontologies are often created by teams of clinicians manually, partitioning the conditions present in that domain on pre-defined boundaries. However, it is often hard to determine the underlying patterns of diseases that partitioning such data. We use exploratory and confirmatory factor analyses to describe the variability in the observed patterns or groupings of two such ontologies in terms of a potentially lower number of latent factors. This gives valuable preliminary insights into the multimorbidity of conditions prevalent in these populations which can be used to better inform diagnoses and recommend preventative measures for the same. © 2017 IEEE.","Factor Analysis; Healthcare; Multimorbidity","Big data; Data mining; Diagnosis; Factor analysis; Health care; Multivariant analysis; Ontology; Confirmatory factor analysis; Domain-specific ontologies; Formal reasoning; Healthcare record; Knowledge-discovery application; Latent factor; Medical ontology; Multimorbidity; Medical education",2-s2.0-85032329980
"Zhou M., Yang S., Li X., Lv S., Chen S., Marsic I., Farneth R.A., Burd R.S.","Evaluation of Trace Alignment Quality and its Application in Medical Process Mining",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364635&doi=10.1109%2fICHI.2017.57&partnerID=40&md5=78532081abcf441a19c0ee1a328c779d","Trace alignment algorithms have been used in process mining for discovering the consensus treatment procedures and process deviations. Different alignment algorithms, however, may produce very different results. No widely-adopted method exists for evaluating the results of trace alignment. Existing reference-free evaluation methods cannot adequately and comprehensively assess the alignment quality. We analyzed and compared the existing evaluation methods, identifying their limitations, and introduced improvements in two reference-free evaluation methods. Our approach assesses the alignment result globally instead of locally, and therefore helps the algorithm to optimize overall alignment quality. We also introduced a novel metric to measure the alignment complexity, which can be used as a constraint on alignment algorithm optimization. We tested our evaluation methods on a trauma resuscitation dataset and provided the medical explanation of the activities and patterns identified as deviations using our proposed evaluation methods. © 2017 IEEE.","Evaluation; Process mining; Trace alignment; Trauma resuscitation","Alignment; Data mining; Health care; Optimization; Resuscitation; Alignment algorithms; Alignment complexity; Evaluation; Evaluation methods; ITS applications; Process deviations; Process mining; Trauma resuscitations; Quality control",2-s2.0-85032364635
"Kumar D., Raj A., Dharanipragada J.","GraphSteal: Dynamic Re-Partitioning for Efficient Graph Processing in Heterogeneous Clusters",2017,"IEEE International Conference on Cloud Computing, CLOUD",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032219781&doi=10.1109%2fCLOUD.2017.63&partnerID=40&md5=a85d258827ba08f5a1218cd2fd189af0","With continuously growing data, clusters also need to grow periodically to accommodate the increased demand of data processing. This is usually done by addition of newer hardware, whose configuration might differ from the existing nodes. As a result, clusters are becoming heterogeneous in nature. For many real world machine learning and data mining applications, data is represented in the form of graphs. Most of the existing distributed graph processing frameworks such as Pregel and Graphlab assume that the computational nodes are homogeneous. These frameworks split the graph into approximately equal subgraphs, which is appropriate for homogeneous clusters. In heterogeneous clusters, these frameworks perform poorly in most of the scenarios. To the best of our knowledge, GraphIVE is the only heterogeneity-aware graph processing framework. It learns the relative capabilities of the nodes based on runtime metrics of previous jobs and partitions the graph proportionally. However, it may not perform well if a new job differs drastically in terms of resource requirements when compared to previous jobs executed on the cluster. To overcome this limitation, we propose GraphSteal, a dynamic graph re-partitioning policy for vertex-cut based graph processing frameworks on heterogeneous clusters. GraphSteal dynamically re-partitions the graph based on the runtime characteristics of the job. To avoid computational skew in the cluster, it migrates edges from slow nodes to fast nodes. To demonstrate our approach, we modify the source code of Graphlab to incorporate dynamic graph re-partitioning strategy. Experimental results show that GraphSteal significantly improves the performance over Graphlab. © 2017 IEEE.","Adaptive scheduling; Distributed Computing; Heterogeneity; Partitioning algorithms; Vertex-cut","Cloud computing; Data handling; Data mining; Distributed computer systems; Graphic methods; Learning systems; Adaptive scheduling; Data mining applications; Heterogeneity; Heterogeneous clusters; Partitioning algorithms; Partitioning strategies; Resource requirements; Vertex-cut; Graph theory",2-s2.0-85032219781
"Tu H.T., Phan T.T., Nguyen K.P.","An adaptive Latent Semantic Analysis for text mining",2017,"Proceedings - 2017 International Conference on System Science and Engineering, ICSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032372737&doi=10.1109%2fICSSE.2017.8030943&partnerID=40&md5=ab85375326d27f11774400c3535b6e9f","Latent Semantic Analysis or LSA uses a method of singular value decomposition of co-occurrence document-term matrix to derive a latent class model. Despite its success, there are some shortcomings in this technique. Recent works have improved the standard LSA using method of probability distribution, regularization, sparseness constraint. But there are still some other deficiencies. It is dealt with this paper, an adapted technique called hk-LSA based on reducing dimension of vector space and like-probabilistic relationships between document and latent-topic space is proposed. The adaptive technique overcomes some weak points of LSA such as processing density of orthogonal matrices, complexity in matrix decomposition, facing with alternative iteration algorithms, etc. The experiments show consistent and substantial improvements of the hk-LSA over LSA. © 2017 IEEE.","convex optimization; coordinate descent; Latent semantic analysis; matrix decomposition; regularization","Convex optimization; Data mining; Functional analysis; Iterative methods; Matrix algebra; Probability distributions; Singular value decomposition; Vector spaces; Adaptive technique; Coordinate descent; Iteration algorithms; Latent class model; Latent Semantic Analysis; Matrix decomposition; regularization; Sparseness constraints; Semantics",2-s2.0-85032372737
"Yang S., Zhou Y., Guo Y., Farneth R.A., Marsic I., Randall B.S.","Semi-Synthetic Trauma Resuscitation Process Data Generator",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032374821&doi=10.1109%2fICHI.2017.67&partnerID=40&md5=1326ee3a0edd0d814a88dfc228c55859","Process mining techniques have been applied to the visualization, interpretation, and analysis of medical processes. However, only a very limited amount of process data necessary for these analyses is publicly available, especially in the medical field because of patients' privacy. This limits novel medical process research to using insufficiently large or randomly-generated synthetic datasets. Our goal in this study is to train a model (using a limited amount of observed process data) that can generate large amounts of semi-synthetic process data. This generated data has characteristics similar to those of real-world process data, and could potentially be observed in reality. © 2017 IEEE.","Hidden Markov Model; Medical Process; Synthetic Data Generation","Hidden Markov models; Markov processes; Resuscitation; Large amounts; Medical fields; Process mining; Real-world process; Synthetic data generations; Synthetic datasets; Synthetic process; Trauma resuscitations; Health care",2-s2.0-85032374821
"Lee E.S.","Exploring the Performance of Stacking Classifier to Predict Depression among the Elderly",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032355155&doi=10.1109%2fICHI.2017.95&partnerID=40&md5=0356e13f90c041fa9f7e16532534a833","Geriatric depression is a disease prevailing in the elderly. It is characterized by typical symptoms of lower functioning, diminished interest in activities, insomnia or hypersomnia, fatigue or loss of energy and observable psycho motor agitation or retardation. Many studies exist with an aim to predict the geriatric depression from the perspective of healthcare informatics based on data mining analytics. However, there is no study emphasizing on the performance of stacking mechanism, which is one of ensemble classifiers. Therefore, this study is concerned with investigating the performance of stacking approach to predicting the geriatric depression-related dataset from the Korea National Health and Nutrition Examination Survey (KNHANES) ranging from 2010 to 2015. The KNHANES is a publicly available big dataset out of a national surveillance system aimed at assessing the health and nutritional status of Koreans since 1998. It is a nationally representative cross-sectional survey including approximately 10,000 individuals each year as a survey sample. By using 9,089 dataset regarding the geriatric depression in the Korean elderly (2010 ~2015), this study analyzed the changes in performance of the stacking mechanism when combining five classifiers (i.e., LR, DT, NN, SVM, NBN) in the base-level learner and meta-level learner. The performance of stacking mechanism measured in accuracy and AUC shows more robust pattern when the base-level learner is relatively simple (like LR, DT), and the meta-level learner is rather complex (like NBN, NN, SVM). To be specific, before the feature selection, the stacking performance was very competitive with accuracy 0.8624 when LR(SVM) indicating that the base-level learner is LR, and the meta-level learner is SVM. After the feature selection, the stacking performance was best with accuracy 0.8643 when DT (NN). With AUC, the similar results were obtained- i.e., LR(NN) with 0.8182 before the feature selection, and LR(NBN) with 0.8147 after the feature selection. © 2017 IEEE.","Accuracy; AUC; Base-level learner; KNHANES; Meta-level learner; Stacking","Big data; Classification (of information); Data mining; Forecasting; Geriatrics; Health care; Nutrition; Surveys; Accuracy; Base-level learner; KNHANES; Meta levels; Stacking; Feature extraction",2-s2.0-85032355155
"Yang S., Zhou M., Chen S., Dong X., Ahmed O., Burd R.S., Marsic I.","Medical Workflow Modeling Using Alignment-Guided State-Splitting HMM",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340713&doi=10.1109%2fICHI.2017.66&partnerID=40&md5=4e0ac33361881da5cfe93c54c7cf77df","Process mining techniques have been used to discover and analyze workflows in various fields, ranging from business management to healthcare. Much of this research, however, has overlooked the potential of hidden Markov models (HMMs) for workflow discovery. We present a novel alignment-guided state-splitting HMM inference algorithm (AGSS) for discovering workflow models based on observed traces of process executions. We compared the AGSS to existing methods using four real-world medical workflow datasets and a more detailed case study on one of them. Our numerical results show that AGSS not only generates more accurate workflow models, but also better represents the underlying process. In addition, with trace alignment to guide state splitting, AGSS is significantly more efficient (by a factor of O(n)) than previous HMM inference algorithms. Our case study results show that our approach produces a more readable and accurate workflow model that existing algorithms. Comparing the discovered model to the hand-made expert model of the same process, we found three discrepancies. These three discrepancies were reconsidered by medical experts and used for enhancing the expert model. © 2017 IEEE.","Hidden Markov Model; Medical Workflow; Process Mining; State-splitting Algorithm; Trace Alignment","Data mining; Health care; Inference engines; Markov processes; Trellis codes; Business management; Hidden markov models (HMMs); Inference algorithm; Medical Workflow; Numerical results; Process mining; State splitting algorithm; Workflow modeling; Hidden Markov models",2-s2.0-85032340713
"Tsumoto S., Kimura T., Iwata H., Hirano S.","Construction of Discharge Summaries Classifier",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032373029&doi=10.1109%2fICHI.2017.92&partnerID=40&md5=5839a7a5bd622c5c3df72d680c3e131f","This paper proposes a method for construction of classifiers for discharge summaries. First, morphological analysis is applied to a set of summaries and a term matrix is generated. Second, correspond analysis is applied to the classification labels and the term matrixand generates two dimensional coordinates. By measuring thedistance between categories and the assigned points, ranking of key wordswill be generated. Then, keywords are selected as attributes according to the rank, andtraining example for classifiers will be generated. Finally learning methodsare applied to the training examples. Experimental validation shows that random forest achieved the best performance and the second best was the deep learner with a small difference, but decision tree methods with many keywords performed only a little worse than neural network or deep learning methods. © 2017 IEEE.","Classification; Correspondence analysis; Decision tree; Deep learning; Discharge summary; Random forest; SVM; Text mining","Classification (of information); Data mining; Decision trees; Deep learning; Health care; Text processing; Classification labels; Correspondence analysis; Decision tree method; Discharge summary; Experimental validations; Morphological analysis; Random forests; Text mining; Learning systems",2-s2.0-85032373029
"Tayal A., Coleman T.F., Li Y.","Bounding the difference between RankRC and RankSVM and application to multi-level rare class kernel ranking",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028968904&doi=10.1007%2fs10618-017-0540-z&partnerID=40&md5=34fcd62be1743bf590a1b1ebb2bc3de0","Rapid explosion in data accumulation has yielded large scale data mining problems, many of which have intrinsically unbalanced or rare class distributions. Standard classification algorithms, which focus on overall classification accuracy, often perform poorly in these cases. Recently, Tayal et al. (IEEE Trans Knowl Data Eng 27(12):3347–3359, 2015) proposed a kernel method called RankRC for large-scale unbalanced learning. RankRC uses a ranking loss to overcome biases inherent in standard classification based loss functions, while achieving computational efficiency by enforcing a rare class hypothesis representation. In this paper we establish a theoretical bound for RankRC by establishing equivalence between instantiating a hypothesis using a subset of training points and instantiating a hypothesis using the full training set but with the feature mapping equal to the orthogonal projection of the original mapping. This bound suggests that it is optimal to select points from the rare class first when choosing the subset of data points for a hypothesis representation. In addition, we show that for an arbitrary loss function, the Nyström kernel matrix approximation is equivalent to instantiating a hypothesis using a subset of data points. Consequently, a theoretical bound for the Nyström kernel SVM can be established based on the perturbation analysis of the orthogonal projection in the feature mapping. This generally leads to a tighter bound in comparison to perturbation analysis based on kernel matrix approximation. To further illustrate computational effectiveness of RankRC, we apply a multi-level rare class kernel ranking method to the Heritage Health Provider Network’s health prize competition problem and compare the performance of RankRC to other existing methods. © 2017 The Author(s)","AUC; Nonlinear kernel; Ranking loss; Ranking svm; Rare class; Receiver operator characteristic; Scalable computing","Computational efficiency; Data mining; Mapping; Nonlinear kernels; Ranking SVM; Rare class; Receiver operator characteristics; Scalable computing; Matrix algebra",2-s2.0-85028968904
"Jia T., Yang L., Chen P., Li Y., Meng F., Xu J.","LogSed: Anomaly Diagnosis through Mining Time-Weighted Control Flow Graph in Logs",2017,"IEEE International Conference on Cloud Computing, CLOUD",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032214508&doi=10.1109%2fCLOUD.2017.64&partnerID=40&md5=eb93b4baf42a76ae4c4727fb5783b32d","Detecting execution anomalies is very important to monitoring and maintenance of cloud systems. People often use execution logs for troubleshooting and problem diagnosis, which is time consuming and error-prone. There is great demand for automatic anomaly detection based on logs. In this paper, we mine a time-weighted control flow graph (TCFG) that captures healthy execution flows of each component in cloud, and automatically raise anomaly alerts on observing deviations from TCFG. We outlined three challenges that are solved in this paper, including how to deal with the interleaving of multiple threads in logs, how to identify operational logs that do not contain any transactional information, and how to split the border of each transaction flow in the TCFG. We evaluate the effectiveness of our approach by leveraging logs from an IBM public cloud production platform and two simulated systems in the lab environment. The evaluation results show that our TCFG mining and anomaly diagnosis both perform over 80% precision and recall on average. © 2017 IEEE.","Anomaly diagnosis; interleaved logs; time-weighted control flow graph mining","Cloud computing; Codes (symbols); Data flow analysis; Graphic methods; Production platforms; Anomaly detection; Anomaly diagnosis; Evaluation results; interleaved logs; Multiple threads; Precision and recall; Simulated system; Weighted controls; Flow graphs",2-s2.0-85032214508
"Henderson J., Ho J.C., Kho A.N., Denny J.C., Malin B.A., Sun J., Ghosh J.","Granite: Diversified, Sparse Tensor Factorization for Electronic Health Record-Based Phenotyping",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032389331&doi=10.1109%2fICHI.2017.61&partnerID=40&md5=d11b5d58448b3c92e2fe5191d9860789","One of the most formidable challenges electronic health records (EHRs) pose for traditional analytics is the inability to map directly (or reliably) to medical concepts or phenotypes. Among other things, EHR-based phenotyping can help identify and target patients for interventions and improve real-time clinical decisions. Existing phenotyping approaches often require labor-intensive supervision from medical experts or do not focus on generating concise and diverse phenotypes. Sparsity in phenotypes is key to making them interpretable and useful to clinicians, while diversity allows clinicians to grasp the main features of a patient population quickly.In this paper, we introduce Granite, a diversified, sparse nonnegative tensor factorization method to derive phenotypes with limited human supervision. Compared to existing high-throughput phenotyping techniques, Granite yields phenotypes with much more distinct (non-overlapping) elements that can, as an artifact, capture rare phenotypes. Moreover, the resulting concise phenotypes retain predictive powers comparable to or surpassing existing dimensionality reduction techniques. We evaluate Granite by comparing its resulting phenotypes with those generated using state-of-the-art, high-throughput methods on simulated as well as real EHR data. Our algorithm offers a promising and novel data-driven solution to rapidly characterize, predict, and manage a wide range of diseases. © 2017 IEEE.","Computational phenotyping; Data mining; Electronic health records; Feature extraction; Health information management; Tensor factorization","Factorization; Feature extraction; Granite; Health; Health care; Information management; Patient treatment; Records management; Tensors; Throughput; Dimensionality reduction techniques; Electronic health record; Electronic health record (EHRs); Health information management; High-throughput phenotyping; Nonnegative tensor factorizations; Phenotyping; Tensor factorization; Data mining",2-s2.0-85032389331
"Carvalho L.F.M., Teixeira C.H.C., Meira W., Ester M., Carvalho O., Brandao M.H.","Provider-Consumer Anomaly Detection for Healthcare Systems",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032344942&doi=10.1109%2fICHI.2017.75&partnerID=40&md5=91487f78b737bcace5ef5b24365dd94b","Anomaly detection is an important task that has been widely applied to different scenarios. In particular, its application in public healthcare is a crucial management task that can improve the quality of the health services and avoid loss of huge amounts of money. In this work we propose and evaluate, in a real scenario, a method for anomaly detection in healthcare based on a provider-consumer model. Our method is divided into two phases. In the first phase it assigns anomaly scores to the cities (consumers) as a function of their demand, then, in the second phase, it transfers the scores from cities to hospitals (providers). We applied the method to a real database from the Brazilian public healthcare that records medical procedures which cost more than $8.5 billion from 2008 to 2012, and demonstrated our method's ability to find potentially fraudulent hospitals. The method is being adopted by the Brazilian government for selecting anomalous hospitals to be investigated. Our main contributions are (i) a simple and effective method for anomaly detection in healthcare; (ii) our method does not require information about the providers nor medical rules; (iii) the analysis from the consumer perspective allows the detection of anomalies that could not be detected with traditional methods; and (iv) we applied the method to a real database and performed a detailed validation. © 2017 IEEE.","Anomaly detection; Data mining; Healthcare management","Data mining; Hospitals; Anomaly detection; Health services; Health-care managements; Health-care system; ITS applications; Management tasks; Medical procedures; Public healthcares; Health care",2-s2.0-85032344942
"Hwang K.-S., Li C.-W., Jiang W.-C.","Adaptive exploration strategies for reinforcement learning",2017,"Proceedings - 2017 International Conference on System Science and Engineering, ICSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032376837&doi=10.1109%2fICSSE.2017.8030828&partnerID=40&md5=cbed4361329ba0065580fbcbe224c2bb","Reinforcement learning through an agent to learn policy use trial and error method to achieve the goal, but when we want to apply it in a real environment, how to dividing state space becomes difficult to decide, another problem in reinforcement learning, agent takes an action in the learning process according to the policy, we will encounter how to balance exploitation and exploration, to explore a new areas in order to gain experience, or to get the maximum reward on existing knowledge. To solve problems, we proposed the decision tree-based adaptive state space segmentation algorithm and then use decreasing Tabu search and adaptive exploration strategies to solve the problem of exploitation and exploration on this method. Decreasing Tabu search will put the action into the Tabu list, after agent take an action. If the Tabu list is full, release the action, but the size of Tabu list will decreasing according to the number of successful reaching goals. Adaptive exploration strategy is based on information entropy, not tuning exploration rate by manually. Finally, a maze environment simulation is used to validate the proposed method, further to decrease the learning time. © 2017 IEEE.","decision tree; Exploitation; Exploration; Reinforcement learning","Data mining; Decision trees; Natural resources exploration; Problem solving; Tabu search; Trees (mathematics); Adaptive explorations; Environment simulation; Exploitation; Exploitation and explorations; Information entropy; Real environments; State space segmentation; Trial-and-error method; Reinforcement learning",2-s2.0-85032376837
"Zuo X., Zhang S., Xia J.","The enhancement of TextRank algorithm by using word2vec and its application on topic extraction",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030633149&doi=10.1088%2f1742-6596%2f887%2f1%2f012028&partnerID=40&md5=2f91527a68d21511667ad444f459b12e","TextRank is a traditional method for keyword matching and topic extraction, while its drawback stems from the ignoring of the semantic similarity among texts. By using word embedding technique, Word2Vec was incorporated into traditional TextRank and four simulation tests were carried on for model comparison. The results showed that the hybrid combination of Word2Vec and TextRank algorithms achieved better keyword/topic extraction towards our testing text dataset. © Published under licence by IOP Publishing Ltd.",,"Artificial intelligence; Information systems; Semantics; Statistical tests; Embedding technique; ITS applications; Key word matching; Model comparison; Semantic similarity; Simulation tests; Topic extraction; Data mining",2-s2.0-85030633149
"Anupindi T.R., Srinivasan P.","Disease Comorbidity Linkages between MEDLINE and Patient Data",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032363083&doi=10.1109%2fICHI.2017.48&partnerID=40&md5=d32620e5a324fc1ebac3800434ebac56","This paper presents an analysis of a class of inferred links between MEDLINE and patient data. Records in the two datasets are linked via pairs of disease associations with a view to emphasizing disease comorbidities. In MEDLINE disease pairs are extracted by mining specific patterns such as MeSH disease term 1/etiology and MeSH disease term 2/complications. 701,780 pairs are extracted by our pattern set from a 2017 download of MEDLINE with close to 27 million records. The patient data, obtained from another study, has 6,088,553 disease cooccurrence pairs. Our methodology to infer connections involves mapping ICD9 codes and MeSH terms to UMLS concept ids followed by both exact and approximate matching strategies. The approximate matching strategy involves semantic relations present in the UMLS. We are able to connect 2,478,366 patient disease pairs encoded using 5 digit ICD9 codes to MEDLINE pairs (and therefore to the corresponding documents) and 536,685 MEDLINE disease pairs onto the patient disease pairs (and therefore implicitly to the corresponding patient records). While these numbers are large the percentages are between 43% and 77%. This indicates that other approaches for linking the two datasets would be of interest. Moreover, comorbidity is a particular viewpoint among many options. We suggest that the study of inferred links between biomedical datasets - especially between core datasets - is of great value in terms of enriching the biomedical web of knowledge. © 2017 IEEE.",,"Epidemiology; Health care; Mesh generation; Semantics; Approximate matching; Co morbidities; Co-occurrence; Disease associations; Patient data; Patient record; Semantic relations; Web of knowledge; Hospital data processing",2-s2.0-85032363083
"Zhao M.","Off-Label Drug Use Detection Based on Heterogeneous Network Mining",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032358876&doi=10.1109%2fICHI.2017.33&partnerID=40&md5=d7934c3638afc42df79000517b62cab1","Off-label drug use refers to prescribing marketed medications for the indications that are not included in their FDA-approved labeling information. Off-label drug use is quite common in clinical practice and inevitable to some extent. Considering the increasing discussions in online health communities (OHCs) among the health consumers, we proposed to harness the large volume of timely information in OHCs to develop an automated method for detecting off-label drug uses from health consumer generated data. From the text corpus, we extracted medical with lexicon-based approaches and measured their interactions with word embedding models, based on which, we constructed a heterogeneous healthcare network. We defined several meta-path-based indicators to describe the drug-disease associations in the heterogeneous network and used them as features to train classifiers to recognize the known drug-disease associations. Lastly, we identified the off-label drug uses from the false-positive results. © 2017 IEEE.",,"Health; Health care; Heterogeneous networks; Automated methods; Clinical practices; Disease associations; False positive; Large volumes; Lexicon-based; Online health communities; Text corpora; Cesium compounds",2-s2.0-85032358876
"Manjula R., Chilambuchelvan A.","An novel approach to extract the content retrieval with the image perception using collaborative community oriented sifting (CCOS)",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028981933&doi=10.1007%2fs10586-017-1125-8&partnerID=40&md5=d2971704bffb44e89b371858ffa4e672","Today it is practically difficult to recover data with a catchphrase pursuit when the data is spread more than a few pages. Semantic Web is a growth of the present web in which data is given well characterized meaning and web personalization is the one of the utilization of the semantic web. In this we have investigated correlation of different community oriented sifting procedures and filtering methodologies were induced to get the exact result for the given queries. In our work we have proposed an image based content retrieval for the given search query from the enduser. The user identification are taken in to consideration to frame the search using the community oriented sifting mining for their desires and the same will be mapped with the image to give out the exact content in terms of text and images. Those systems are memory based, model based and cross breed communitarian sifting. Our review demonstrates that the execution of half and half communitarian separating method is superior to anything memory based and show based collective sifting strategy. We have introduced standardization step, which will enhance precision of conventional community oriented sifting procedures. One of the intense personalization advances controlling the versatile web is synergistic separating. Collaborative community oriented sifting (CCOS) is the way toward separating or assessing things through the sentiments of other individuals. CCOS innovation unites the suppositions of huge interconnected groups on the web, supporting separating of significant amounts of information. © 2017 Springer Science+Business Media, LLC","Collaborative community oriented sifting; Collaborative filtering; Content mining; Image mining","Collaborative filtering; Information retrieval; Semantic Web; Separation; Collaborative community; Content mining; Content retrieval; Image mining; Image perception; Separating methods; User identification; Web personalization; Image processing",2-s2.0-85028981933
[No author name available],"IEEE International Conference on Cloud Computing, CLOUD",2017,"IEEE International Conference on Cloud Computing, CLOUD",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032190931&partnerID=40&md5=fc5a95ae2931cd147fa08ac09ce4cd9b","The proceedings contain 114 papers. The topics discussed include: emerge: self-emerging data release using cloud data storage; arion: a model-driven middleware for minimizing data loss in stream data storage; taming performance hotspots in cloud storage with dynamic load redistribution; cloud standards in comparison: are new security frameworks improving cloud security?; end-to-end policy monitoring and enforcement for service-oriented architecture; privacy-preserving data deduplication on trusted processors; cost-effective big data mining in the cloud: a case study with K-means; hierarchical spark: a multi-cluster big data computing framework; and cloudroid: a cloud framework for transparent and QoS-aware robotic computation outsourcing.",,,2-s2.0-85032190931
"Gao W., Shi L., Han J., Zhai P.","Dynamic Monitoring of Water in a Working Face Floor Using 2D Electrical Resistivity Tomography (ERT) [利用二维电阻率层析成像动态监测工作面底板水] [Die Verwendung elektrischer 2D-Widerstands-Tomografie (ERT) im dynamischen Wassermonitoring am Boden von Ortsbrustbereichen] [Monitoreo dinámico de agua en un piso de trabajo utilizando tomografía de resistividad eléctrica 2D (ERT)]",2017,"Mine Water and the Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028994520&doi=10.1007%2fs10230-017-0483-z&partnerID=40&md5=fc2e44fdddece311324517eec41f652b","The roof and floor of a water-rich area can be explored using traditional two-dimensional (2D) electrical resistivity tomography (ERT), but this method cannot be applied to a water-rich internal working face. In this paper, the resistivity of an internal working face was determined using 2D ERT by laying electrodes and cables in two adjacent roadways and using an equatorial dipole device. The water inrush process of the goaf floor in the study area was monitored with pre-implanted electrodes and cables at the front of the mining working face and was analyzed using the processed results of four acquired data sets. The location of the water inrush from the floor was determined based on the change in the resistivity of the floor and the hydrological variations at the observational hole, which provided the basis for how to stop the water inrush. © 2017 Springer-Verlag GmbH Germany","Dipole equatorial array; Liangzhuang coal mine; Location of water-inrush from floor; Water inrush of the goaf floor",,2-s2.0-85028994520
"Moro L., Srnec Novak J., Benasciutti D., De Bona F.","Thermal distortion in copper moulds for continuous casting of steel: numerical study on creep and plasticity effect",2017,"Ironmaking and Steelmaking",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029427977&doi=10.1080%2f03019233.2017.1371425&partnerID=40&md5=2296f9ecf981f96b2cb5c9646cbcc051","In this work, the thermal distortion of a copper mould for continuous casting of steel is investigated through numerical models based on the finite element method. Special attention is devoted to the accuracy of the adopted material properties: several elasto-plastic models, with or without creep effects, are considered and compared into the analysis. The early formation of a bulge close to the meniscus is correctly simulated and results are in good agreement with experimental data from the literature. © 2017 Institute of Materials, Minerals and Mining","Copper mould; creep; cyclic plasticity; material models; thermo-mechanical analysis","Continuous casting; Copper; Creep; Finite element method; Molds; Numerical methods; Plasticity; Continuous casting of steels; Copper moulds; Creep effects; Cyclic plasticity; Elasto-plastic models; Material models; Thermal distortions; Thermo-mechanical analysis; Steel castings",2-s2.0-85029427977
"Yin Y., Yu F., Xu Y., Yu L., Mu J.","Network location-aware service recommendation with random walk in cyber-physical systems",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029171641&doi=10.3390%2fs17092059&partnerID=40&md5=6a0de0c962e378f2979f4c448c22098f","Cyber-physical systems (CPS) have received much attention from both academia and industry. An increasing number of functions in CPS are provided in the way of services, which gives rise to an urgent task, that is, how to recommend the suitable services in a huge number of available services in CPS. In traditional service recommendation, collaborative filtering (CF) has been studied in academia, and used in industry. However, there exist several defects that limit the application of CF-based methods in CPS. One is that under the case of high data sparsity, CF-based methods are likely to generate inaccurate prediction results. In this paper, we discover that mining the potential similarity relations among users or services in CPS is really helpful to improve the prediction accuracy. Besides, most of traditional CF-based methods are only capable of using the service invocation records, but ignore the context information, such as network location, which is a typical context in CPS. In this paper, we propose a novel service recommendation method for CPS, which utilizes network location as context information and contains three prediction models using random walking. We conduct sufficient experiments on two real-world datasets, and the results demonstrate the effectiveness of our proposed methods and verify that the network location is indeed useful in QoS prediction. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Cyber-physical systems; Network location; QoS prediction; Random walk; Service recommendation","Collaborative filtering; Cyber Physical System; Embedded systems; Forecasting; Location; Random processes; Semantics; Cyber-physical systems (CPS); Network location; Qos predictions; Random Walk; Real-world datasets; Service recommendations; Similarity relations; Traditional services; Location based services",2-s2.0-85029171641
[No author name available],"Journal of Physics: Conference Series",2017,"Journal of Physics: Conference Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030626393&partnerID=40&md5=642459333c570d90e658d6baf0a5a3f6","The proceedings contain 96 papers. The topics discussed include: research and application of mobile teaching platform; study to the simulation for pedestrian-vehicle collision accident of the second crushing by PC-CRASH; requirement analysis for the one-stop logistics management of fresh agricultural products; component reliability parameters estimation considering weather factors; geomagnetic matching navigation algorithm based on robust estimation; design and realization of high voltage disconnector condition monitoring system; features research of ideal ahead-detecting method used in mining roadway and exploitation of a new technology - DFIP; agricultural SWOT analysis and wisdom agriculture design of Chengdu; combing VFH with Bezier for motion planning of an autonomous vehicle; an aggregation-decomposition Bayesian stochastic optimization model for cascade hydropower reservoirs using medium-range precipitation forecasts; development of remote data acquisition system based on OPC for brake test bench; profitability analysis of KINGLONG nearly 5 years; Word2vec and dictionary based approach for Uyghur text filtering; and research on optimal driver steering model based on multi-point preview.",,,2-s2.0-85030626393
"Yang C.C., Zhao M.","Determining Associations with Word Embedding in Heterogeneous Network for Detecting Off-Label Drug Uses",2017,"Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032344642&doi=10.1109%2fICHI.2017.78&partnerID=40&md5=96d21a91256f19f89edc438844d42502","Off-label drug use is quite common in clinical practice and inevitable to some extent. Such uses might deliver effective treatment and suggest clinical innovation sometimes, however, they have the unknown risk to cause serious outcomes due to lacking scientific support. As gaining information about off-label drug use could present a clue to the stakeholders such as healthcare professionals and medication manufacturers to further the investigation on drug efficacy and safety, it raises the need to develop a systematic way to detect off-label drug uses. Considering the increasing discussions in online health communities (OHCs) among the health consumers, we proposed to harness the large volume of timely information in OHCs to develop an automated method for detecting off-label drug uses from health consumer generated data. From the text corpus, we extracted medical entities (diseases, drugs, and adverse drug reactions) with lexicon-based approaches and measured their interactions with word embedding models, based on which, we constructed a heterogeneous healthcare network. We defined several meta-path-based indicators to describe the drug-disease associations in the heterogeneous network and used them as features to train a binary classifier built on Random Forest algorithm, to recognize the known drug-disease associations. The classification model obtained better results when incorporating word embedding features and achieved the best performance when using both association rule mining features and word embedding features, with F1-score reaching 0.939, based on which, we identified 2,125 possible off-label drug uses and checked their potential by searching evidence in PubMed and FAERS. © 2017 IEEE.","Heterogeneous network; Off-label drug use; Online health community; Word embedding","Decision trees; Health; Health care; Heterogeneous networks; Adverse drug reactions; Classification models; Disease associations; Health care professionals; Off-label drug use; Online health communities; Random forest algorithm; Word embedding; Cesium compounds",2-s2.0-85032344642
"Boddula V., Ramaswamy L., Mishra D.","A Spatio-Temporal Mining Approach for Enhancing Satellite Data Availability: A Case Study on Blue Green Algae",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032378802&doi=10.1109%2fBigDataCongress.2017.37&partnerID=40&md5=4e5f78879baa2da4a37cec96d1c57fdc","Satellite imagery provides geospatial data, generally terabytes in size. Due to their cost effectiveness and scalability, they are used in various large scale applications related to ecological monitoring. However, satellite data is prone to problems of data incompleteness owing to a number of issues such as cloud cover, fog, etc. In this paper, we conduct a detailed study on the 10 years of satellite data using Blue-Green Algae as a case study. Blue-Green Algae (BGA) are a toxic phytoplankton which are now a worldwide phenomena and a concern to various public authorities. We first illustrate the data incompleteness problem in our dataset with respect to BGA, and then formulate this problem using time-series spatio-temporal data mining approach that harnesses similarities among lakes with respect to BGA manifestation. We evaluate our approach through experimental studies from 99 lakes in the southeastern United States. Our experiment shows that our approach is an effective method to address data incompleteness in the BGA domain. © 2017 IEEE.","ecological surveillance; satellite data incompleteness; spatio-temporal data; time series","Algae; Ball grid arrays; Cost effectiveness; Data mining; Ecology; Green computing; Lakes; Satellite imagery; Satellites; Time series; Ecological monitoring; Large-scale applications; Public authorities; Satellite data; Spatio-temporal data; Spatio-temporal data mining; Spatio-temporal minings; Toxic phytoplanktons; Big data",2-s2.0-85032378802
"Zhu M., Liu C., Wang J., Su S., Han Y.","Service Hyperlink: Modeling and Reusing Partial Process Knowledge by Mining Event Dependencies among Sensor Data Services",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364946&doi=10.1109%2fICWS.2017.117&partnerID=40&md5=1eec08571d603303a4e287aecbedbe54","In an IoT environment, process analysis becomes more difficult as a process usually spans over a set of autonomous and distributed sensors. This paper consummates our previous service hyperlink model, to encapsulate dependencies among events generated from services. To effectively discover service hyperlinks, we transform the service hyperlink discovery problem into a frequent sequence mining problem. Existing frequent sequence mining algorithms cannot be directly used because they do not take the temporal constraints in event dependencies into consideration. Based on the dataset from a real power plant as well as several synthetic datasets, we do lots of experiments to verify the effectiveness and efficiency of our algorithm. © 2017 IEEE.","IoT service; partial process knowledge; proactive stream data service; sensor data; service hyperlinks","Data mining; Hypertext systems; Internet of things; Microcellular radio systems; Websites; Hyperlinks; Iot services; Process knowledge; Sensor data; Stream data; Web services",2-s2.0-85032364946
"Li Y., Jiang Z.L., Wang X., Yiu S.M., Zhang P.","Outsourcing privacy preserving ID3 decision tree algorithm over encrypted data-sets for two-parties",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032355698&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.354&partnerID=40&md5=555a28f0cf9477a58a16a8bda891e7b4","ID3 decision tree data mining is a popular and widely studied data analysis technique for a range of applications. In this paper, we focus on the privacy-preserving ID3 decision tree algorithm on horizontally partitioned datasets. In such a scenario, data owners wish to learn the decision tree result from a collective data set but disclose minimal information about their own sensitive data. In this paper, we consider a scenario in which multiple parties with weak computational power need to run an ID3 algorithm on their databases jointly while simultaneously outsourcing most of the computation of the protocol and databases to the cloud. In such a scenario, each party can have the correct result calculated on the data from all the parties with most of the computation outsourced to the cloud. Concerning privacy, the data owned by each party should be kept confidential from both the other parties and the cloud. To ensure data privacy, we modify the Secure Equivalent Testing Protocol (SET) and design the Outsourced Secure Shared xlnx Protocol (OSSx ln x) and other sub-protocols. We then propose a cloud-aided ID3 solution based on these protocols, which is used to build an outsourced privacy-preserving ID3 data mining solution. © 2017 IEEE.","Encrypted data sets; ID3 decision tree; Outsourced computation; Privacy preserving data mining; Secure multiparty computation","Big data; Cryptography; Data communication systems; Data mining; Decision trees; Embedded software; Embedded systems; Outsourcing; Trees (mathematics); Computational power; Data analysis techniques; Decision-tree algorithm; Encrypted data; Minimal information; Privacy preserving; Privacy preserving data mining; Secure multi-party computation; Data privacy",2-s2.0-85032355698
"Dierckens K.E., Harrison A.B., Leung C.K., Pind A.V.","A data science and engineering solution for fast k-means clustering of big data",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350654&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.332&partnerID=40&md5=38c358967fdcfeefecb8a7b46442f7a5","With advances in technology, high volumes of a wide variety of valuable data of different veracity can be easily collected or generated at a high velocity in the current era of big data. Embedded in these big data are implicit, previously unknown and potentially useful information. Hence, fast and scalable big data science and engineering solutions that mine and discover knowledge from these big data are in demand. A popular and practical data mining task is to group similar data into clusters (i.e., clustering). To cluster very large data or big data, k-means based algorithms have been widely used. Although many existing k-means algorithms give quality results, they also suffer from some problems. For instance, there are risks associated with randomly selecting the k centroids, there is a tendency to produce roughly equal circular clusters, and the runtime complexity is very high. To deal with these problems, we present in this paper a big data science and engineering solution that applies heuristic prototype-based algorithm. Evaluation results show the efficiency and scalability of this solution. © 2017 IEEE.","Big data; Clustering; Data mining; K-means","Clustering algorithms; Data communication systems; Data mining; Data privacy; Embedded software; Embedded systems; Clustering; Data mining tasks; Evaluation results; K-means; k-Means algorithm; K-means clustering; Run time complexity; Science and engineering; Big data",2-s2.0-85032350654
"Taleb I., Serhani M.A.","Big Data Pre-Processing: Closing the Data Quality Enforcement Loop",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032359531&doi=10.1109%2fBigDataCongress.2017.73&partnerID=40&md5=7ae85b2e3246bb96e0b3dd907f08d2d0","In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data. © 2017 IEEE.","Big Data; Big Data Pre-Processing; Data Quality Evaluation; Data Quality Rules Discovery","Data handling; Data mining; Data reduction; Quality control; Data preprocessing; Data quality; Data quality dimensions; Generation process; Low quality datum; Private organizations; Quality requirements; Rules discovery; Big data",2-s2.0-85032359531
"Bewong M., Liu J., Liu L., Li J., Choo K.-K.R.","A relative privacy model for effective privacy preservation in transactional data",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032349356&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.263&partnerID=40&md5=fe7597e33dcb1036e38d519cc911a0c3","Transactional data such as shopping logs, web search queries and medical notes present enormous opportunities for knowledge discovery through data mining. When such data is published for knowledge discovery, privacy disclosure risks arise, making privacy preserving publication a fundamental requirement. However, existing publication mechanisms do not fully prevent an adversary from making an inference about the intended victim. While some solutions to this problem exist for the publication of relational data, they are not transferable to the publication of transactional data due to the difference in data models. This work aims to prevent inference attacks in the publication of transactional data by proposing a relative privacy metric that ensures that the knowledge gain of an adversary about any individual from the published data is bound to the general public knowledge. We then propose a publication mechanism Anony, which satisfies the proposed privacy metric without having to use excessively large cluster sizes. Finally, we evaluate our publication mechanism using two benchmark datasets and the results demonstrate that the proposed mechanism is effective and efficient. © 2017 IEEE.","Data Privacy; Data Publishing; Privacy Preservation; Transactional Data","Big data; Data communication systems; Data mining; Embedded software; Embedded systems; Medical computing; Medical education; Publishing; Benchmark datasets; Data publishing; Privacy disclosures; Privacy preservation; Privacy preserving; Publication mechanisms; Transactional data; Web search queries; Data privacy",2-s2.0-85032349356
"Sato H., Seino Y., Ogata T., Shirakawa T.","Detection of Irregularities and Rips by Finding Critical Points of Morse Theory - Preliminary Results on Analysis on Sales Data",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032874314&doi=10.1109%2fCOMPSAC.2017.106&partnerID=40&md5=1ec16aea205981e055cdaeb681c9206d","As data mining grows its significance in real world information extraction, we need a robust methodology without using rich assumptions. In detection of irregularities, statistical methods are very successful when we assume some distribution. However, even when we encounter data with unknown distribution, we must first assume some distribution, which may cause incorrect inference. This paper applies topological data analysis (TDA) methodologies to detection of irregularities and rips. We also apply Morse theory together with persistent homology to the analysis of dynamical system of big data set such as the surveillance video data and the sales dynamics. The idea is that data tendency changes can be observed as a change of data connectivity. In this paper, we explore the theory of critical points by using Morse theory. Furthermore, we compute the persistent homology groups over daily sales data in a store, a typical dynamical system that we know little about. We show the computed critical points of dimension 1 through 3 in the experiment. The growth of dimensions shows that some irregular event may have happened on the corresponding date point. Although we need refinement on the interpretation of the result, anyway we can collect the candidates of irregularities. © 2017 IEEE.","anomaly detection; critical points; Morse theory; topological data analysis","Application programs; Computer software; Data handling; Data mining; Dynamical systems; Information analysis; Sales; Security systems; Topology; Anomaly detection; critical points; Data connectivity; Morse the-ory; Persistent homology; Real-world information; Surveillance video; Topological data analysis; Big data",2-s2.0-85032874314
"Siriweera T.H.A.S., Paik I., Kumara B.T.G.S.","Constraint-Driven Dynamic Workflow for Automation of Big Data Analytics Based on GraphPlan",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032354849&doi=10.1109%2fICWS.2017.120&partnerID=40&md5=62b603f38ac88456c2bb5fc21088caa9","The use of the big data analytics (BDA) platform is increasingly becoming prevalent in the data sciences. However, BDA processes consume resources and time excessively. Automating BDA processes is a cognitive approach to the BDA domain, which is most impaired by its heavy consumption of time and resources. However, the BDA workflow is highly dependent on diversified constraints because of the high variability, veracity, and volume of data processing to accomplish highly influential and sophisticated requirements. The workflow has to pass rigorous and diverse data mining steps, each step contains several tasks and these tasks made by many sub tasks, that it must be accomplished to progress to the next step in the workflow. This increases the available solution space for BDA processes. The intelligent heuristic approach is needed to address the domain-specific concerns which are large solution space, and awareness of constraints are caused the BDA planning. Therefore, we propose to use GraphPlan-based dynamic workflow generation for the BDA domain. Our empirical studies prove that the proposed sophisticated method satisfied planning requirements and outperformed a related planning technique. © 2017 IEEE.","Artificial Intelligence; Automatic Service Composition; Big Data Analytics; Constraint; GraphPlan","Artificial intelligence; Data handling; Data mining; Heuristic methods; Web services; Websites; Automatic service composition; Cognitive approaches; Constraint; Data analytics; Empirical studies; Graphplan; Heuristic approach; Planning techniques; Big data",2-s2.0-85032354849
"Prasad S.K., Aghajarian D., McDermott M., Shah D., Mokbel M., Puri S., Rey S.J., Shekhar S., Xe Y., Vatsavai R.R., Wang F., Liang Y., Vo H., Wang S.","Parallel Processing over Spatial-Temporal Datasets from Geo, Bio, Climate and Social Science Communities: A Research Roadmap",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032384688&doi=10.1109%2fBigDataCongress.2017.39&partnerID=40&md5=67f9daac519c56f2659e783456b3d890","This vision paper reviews the current state-ofart and lays out emerging research challenges in parallel processing of spatial-temporal large datasets relevant to a variety of scientific communities. The spatio-temporal data, whether captured through remote sensors (global earth observations), ground and ocean sensors (e.g., soil moisture sensors, buoys), social media and hand-held, traffic-related sensors and cameras, medical imaging (e.g., MRI), or large scale simulations (e.g., climate) have always been 'big.' A common thread among all these big collections of datasets is that they are spatial and temporal. Processing and analyzing these datasets requires high-performance computing (HPC) infrastructures. Various agencies, scientific communities and increasingly the society at large rely on spatial data management, analysis, and spatial data mining to gain insights and produce actionable plans. Therefore, an ecosystem of integrated and reliable software infrastructure is required for spatialtemporal big data management and analysis that will serve as crucial tools for solving a wide set of research problems from different scientific and engineering areas and to empower users with next-generation tools. This vision requires a multidisciplinary effort to significantly advance domain research and have a broad impact on the society. The areas of research discussed in this paper include (i) spatial data mining, (ii) data analytics over remote sensing data, (iii) processing medical images, (iv) spatial econometrics analyses, (v) Map-Reducebased systems for spatial computation and visualization, (vi) CyberGIS systems, and (vii) foundational parallel algorithms and data structures for polygonal datasets, and why HPC infrastructures, including harnessing graphics accelerators, are needed for time-critical applications. © 2017 IEEE.","CyberGIS; High performance computing; Map-reduce systems; Medical images; Parallel algorithms and data structures; Remote sensing data; Spatial data mining; Spatial econometrics","Data mining; Data structures; Data visualization; Earth (planet); Economics; Information management; Magnetic resonance imaging; Medical computing; Medical imaging; Moisture control; Parallel algorithms; Parallel processing systems; Remote sensing; Soil moisture; Spatial variables measurement; Statistics; Algorithms and data structures; CyberGIS; High performance computing; Map-reduce; Remote sensing data; Spatial data mining; Spatial econometrics; Big data",2-s2.0-85032384688
"Gottipati S., Shankararaman V., Goh M.","Mining Capstone Project Wikis for Knowledge Discovery",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031896967&doi=10.1109%2fCOMPSAC.2017.169&partnerID=40&md5=c5b4f34959c1fcdb45d943d4091590cc","Wikis are widely used collaborative environments as sources of information and knowledge. The facilitate students to engage in collaboration and share information among members and enable collaborative learning. In particular, Wikis play an important role in capstone projects. Wikis aid in various project related tasks and aid to organize information and share. Mining project Wikis is critical to understand the students learning and latest trends in industry. Mining Wikis is useful to educationists and academicians for decision-making about how to modify the educational environment to improve student's learning. The main challenge is that the content or data in project Wikis is unstructured in nature. The data formats are in both text and images. In this work, we propose an automated project Wiki mining solution that leverages data mining, text mining and optical character recognition techniques for discovering insights from Project Wikis. The results of mining process are presented as visual summaries which can be useful for the capstone project coordinators and academicians for education pedagogy decisions. We use dataset from Singapore Management University, School of Information Systems' undergraduate capstone projects for our solution evaluation. We evaluated our model on 314 capstone projects over a period of 8 years. © 2017 IEEE.","Capstone Projects; Data Mning; OCR; Project Wikis; Text Mining","Application programs; Character recognition; Computer software; Data mining; Decision making; Education computing; Information dissemination; Information management; Mining; Optical character recognition; Students; Capstone projects; Collaborative environments; Collaborative learning; Data Mning; Educational environment; Project Wikis; Sources of informations; Text mining; Education",2-s2.0-85031896967
"Manica E., Dorneles C.F., Galante R.","R-Extractor: A Method for Data Extraction from Template-Based Entity-Pages",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031938570&doi=10.1109%2fCOMPSAC.2017.202&partnerID=40&md5=7c4a68599cba397ad110c715df7eca52","The challenges in Big Data start during the data acquisition, where it is necessary to transform non-structured data into a structured format. The Big Data Era brought a new challenge to data acquisition: The need for eliminating user intervention. In this context, the domain-centric data extraction (DCDE) methods arose through replacing user intervention with content redundancy. The DCDE methods extract the attribute values of entities of the web that are restricted to a specific application domain. A research gap was identified from various analyzed methods, i.e., the DCDE methods are not effective in extracting attribute values that are differently presented within the same website. We have responded to this research gap by proposing the R-Extractor method, which extends a state-of-the-art DCDE method by adding a reinforcement stage for the treatment of attribute values that are differently presented within the same website. The R-Extractor method re-analyzes the extraction rules to identify those that can be combined to extract the values of a given attribute from all the pages that describe entities on a website. This identification is based on a novel score function that takes into account different features of the extraction rules. We carried out experiments on a dataset with more than 50k web pages from different real-world websites of a wide range of application domains. The R-Extractor method reached 98% of precision. Our method was compared with two baselines (an XPath-based method and a tree-based method) and outperformed them with an increase in precision up to 14%. © 2017 IEEE.","Big Data; Data Extraction; Entity-Page","Application programs; Big data; Computer software; Data acquisition; Extraction; Websites; Attribute values; Data extraction; Entity-Page; Extraction rule; State of the art; Structured data; Tree-based methods; User intervention; Data mining",2-s2.0-85031938570
"Ankele R., Simpson A.","On the performance of a trustworthy remote entity in comparison to secure multi-party computation",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032339903&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.361&partnerID=40&md5=12053c2db271f6182e11ca380d27e77e","Novel trusted hardware extensions such as Intel's SGX enable user-space applications to be protected against potentially malicious operating systems. Moreover, SGX supports strong attestation guarantees, whereby remote parties can be convinced of the trustworthy nature of the executing user-space application. These developments are particularly interesting in the context of large-scale privacy-preserving data mining. In a typical data mining scenario, mutually distrustful parties have to share potentially sensitive data with an untrusted server, which in turn computes a data mining operation and returns the result to the clients. Generally, such collaborative tasks are referred to as secure multi-party computation (MPC) problems. Privacy-preserving distributed data mining has the additional requirement of (output) privacy preservation (which typically is achieved by the addition of random noise to the function output); additionally, it limits the general purpose functionality to distinct data mining operations. To solve these problems in a scalable and efficient manner, the concept of a Trustworthy Remote Entity (TRE) was recently introduced. We report upon the performance of a SGX-based TRE and compare our results to popular secure MPC frameworks. Due to limitations of the MPC frameworks, we benchmarked only simple operations (and argue that more complex data mining operations can be established by composing several basic operations). We consider both a two-party setting (where we iterate over the number of operations) and a multi-party setting (where we iterate over the number of participants). © 2017 IEEE.","Intel SGX; Large Scale; Performance Evaluation; Privacy Preserving Data Mining; Secure Multiparty Computation; Software Attestation; Trustworthy Remote Entity","Big data; Data communication systems; Data mining; Embedded software; Embedded systems; Space applications; Intel SGX; Large Scale; Performance Evaluation; Privacy preserving data mining; Remote entity; Secure multi-party computation; Data privacy",2-s2.0-85032339903
"Grant-Muller S., Hodgson F., Malleson N., Snowball R.","Enhancing Energy, Health and Security Policy by Extracting, Enriching and Interfacing Next Generation Data in the Transport Domain (A Study on the Use of Big Data in Cross-Sectoral Policy Development)",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032356954&doi=10.1109%2fBigDataCongress.2017.77&partnerID=40&md5=eae5e510c581f8f10f293f601d574585","Improved understanding of the interfaces between travel choices and the consequential energy, health and security impacts would support the development of policies that deliver positive benefits across the sectors. This would represent a step change from the common sectoral based approach, whereby positive impacts for one sector may be negated by dis-benefits in another. This concept paper presents a precis of on-going research to harness the potential of new generation data ('Big Data') arising from innovative policies (based on persuasive technologies) in the transport sector to model, then analyse, cross sectoral impacts. The key issues for exploration include 1) the scope and veracity of both new generation data and established data, and 2) the developments needed to existing model interfaces and internal algorithms (transport and cross-sectoral) to accommodate new generation data. The research is based on data generated as the result of a large scale implementation of positive incentives to encourage changes to travel choices using mobile phone apps in Newcastle (UK). The broader research program involves ten other cities across Europe. © 2017 IEEE.","cross-sectoral policies; Energy policy; Health policy; mobile phone; new generation location data; persuasive technologies; Security policy; Transport policy","Cellular telephone systems; Cellular telephones; Data mining; Energy policy; Health; Mobile phones; Mobile security; Public policy; Security systems; Telephone sets; Health policy; Location data; Persuasive technology; Sectoral policies; Security policy; Transport policy; Big data",2-s2.0-85032356954
"Leemaqz K.L., Lee S.X., McLachlan G.J.","Corruption-resistant privacy preserving distributed em algorithm for model-based clustering",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032336282&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.356&partnerID=40&md5=e335074645e8cc0d6ff1b6d49e7b88a7","Statistical clustering plays an important role in data analysis and is one of the most widely used data mining methods. Concerns about the security and privacy of analyzing modernday massive data across distributed networks have prompted the development of privacy preserving data mining algorithms. This paper proposes a scheme for model-based clustering and classification through a privacy-preserving EM-based learning of a mixture model. We focus on cooperative learning in a multiparty scenario where the parameters of the mixture model can be estimated on the entire data and learnt by all parties without disclosing any private data. In contrast to most existing works which assumed the adversary is Honest-but-Curious, we consider the seldom studied and much stronger and more realistic case of Malicious adversary with unlimited corruption capabilities. The proposed scheme adopts a cyclic communication topology and utilizes cryptographic techniques to encrypt communicated messages, rendering it resistant to multiple corrupted parities. By enforcing one way communication across a ring topology, no trust level hierarchy is required. Upon completion of the training algorithm, each party obtains a clustering of its own private data and is able service a third party by providing predicted cluster labels for new data. For illustration, the Gaussian mixture model is used to present our scheme. © 2017 IEEE.","Distributed Computing; Mixtrure Models; Privacy Preserving","Big data; Clustering algorithms; Crime; Cryptography; Data communication systems; Data mining; Distributed computer systems; Embedded software; Embedded systems; Gaussian distribution; Topology; Communication topologies; Cryptographic techniques; Gaussian Mixture Model; Malicious adversaries; Model-based clustering; Privacy preserving; Privacy-preserving data mining algorithms; Statistical clustering; Data privacy",2-s2.0-85032336282
"Ding J., Wang J., Kang X., Hu X.-H.","Building an SVM Classifier for Automated Selection of Big Data",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032340233&doi=10.1109%2fBigDataCongress.2017.12&partnerID=40&md5=31437f132c0c6eefd7c4a290e32524e5","The quality of big data could great impact the value extracted from the data. Automated filtering of noisy data from big data is an ideal approach for improving the quality of big data. However, due to large volume and variety of big data, automated filtering of noisy data from big data is a grand challenging task. In this paper, we propose a support vector machine based approach for automated classification of big data so that the noisy data are classified as separated categories from the regular data. In order to improve the classification accuracy and training performance, we design an experiment for improving the classification model through finding the optimized learning feature set and an approach for iteratively improving the quality of the training data set. We conducted a thorough experimental study of automated classification of massive image data of biology cells to explain the approach of automated selection of big data and demonstrate its effectiveness. Finally, we compare the performance of the SVM based classification and a deep learning based classification of the same data set. The proposed approach and experience collected from the experimental study can help big data researchers and practitioners to design strategies for improving the quality of big data, designing high performance classifier, and building tools for automated selection of big data. © 2017 IEEE.","big data; deep learning; diffraction image; feature selection; GLCM; machine learning; support vector machine","Automation; Classification (of information); Classifiers; Data mining; Deep learning; Feature extraction; Learning systems; Structural design; Support vector machines; Automated classification; Automated selection; Classification accuracy; Classification models; Design strategies; Diffraction images; GLCM; Training data sets; Big data",2-s2.0-85032340233
"Thao T.P., Yamada A., Murakami K., Urakawa J., Sawaya Y., Kubota A.","Classification of landing and distribution domains using whois' text mining",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032370692&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.213&partnerID=40&md5=b4b965859fb0fa1485bc5979235da073","Detection of drive-by-download attack has gained a focus in security research since the attack has turned into the most popular and serious threat to web infrastructure. The attack exploits vulnerabilities in web browsers and their extensions for unnoticeably downloading malicious software. Often, the victim is sent through a long chain of redirection operations in order to take down the offending pages. Concretely, the attack is triggered when a user visits a benign webpage that is compromised by the attacker (called landing page) and is inserted some malicious code inside. The user is then automatically redirected to an actual page that installs malware on the user's computer (called distribution page) without his/her consent or knowledge. While there is a large body of works targeting on detection of drive-by download attack, there is little attention on the redirection which is a crucial characteristic of the attack. In this paper, for the first time, we propose an approach to the classification of landing and distribution domains which are important components forming the head and tail of a redirection chain in the attack. The methodology in our approach is to use machine learning for text mining on the registered information of the domains called whois. We intensively implemented our approach with six popular supervised learning algorithms, compared the results and concluded that Linear-based Support Vector Machine and CART algorithm-based Decision Tree are the best models for our dataset which respectively give 98.55% and 99.28% of accuracy, 97.78% and 98.95% of F1 score, 98.35% and 99.45% of average precision. © 2017 IEEE.","Automated Redirection Chain; Drive-by-download; Supervised Machine Learning; Web Security; Whois","Artificial intelligence; Big data; Chains; Computer crime; Data communication systems; Data privacy; Decision trees; Digital storage; Embedded software; Embedded systems; Learning algorithms; Learning systems; Malware; Supervised learning; Text processing; Trees (mathematics); Web browsers; CART algorithms; Drive-by-download; Malicious codes; Security research; Supervised machine learning; Web infrastructure; WEB security; Whois; Data mining",2-s2.0-85032370692
"Li C., Palanisamy B., Joshi J.","Differentially Private Trajectory Analysis for Points-of-Interest Recommendation",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032383295&doi=10.1109%2fBigDataCongress.2017.16&partnerID=40&md5=1f119542fac4a220daf5b64dbcc2ce16","Ubiquitous deployment of low-cost mobile positioning devices and the widespread use of high-speed wireless networks enable massive collection of large-scale trajectory data of individuals moving on road networks. Trajectory data mining finds numerous applications including understanding users' historical travel preferences and recommending places of interest to new visitors. Privacy-preserving trajectory mining is an important and challenging problem as exposure of sensitive location information in the trajectories can directly invade the location privacy of the users associated with the trajectories. In this paper, we propose a differentially private trajectory analysis algorithm for points-of-interest recommendation to users that aims at maximizing the accuracy of the recommendation results while protecting the privacy of the exposed trajectories with differential privacy guarantees. Our algorithm first transforms the raw trajectory dataset into a bipartite graph with nodes representing the users and the points-of-interest and the edges representing the visits made by the users to the locations, and then extracts the association matrix representing the bipartite graph to inject carefully calibrated noise to meet ϵ-differential privacy guarantees. A post-processing of the perturbed association matrix is performed to suppress noise prior to performing a Hyperlink-Induced Topic Search (HITS) on the transformed data that generates an ordered list of recommended points-of-interest. Extensive experiments on a real trajectory dataset show that our algorithm is efficient, scalable and demonstrates high recommendation accuracy while meeting the required differential privacy guarantees. © 2017 IEEE.","differential privacy; location privacy; recommendation; trajectory","Data mining; Data privacy; Graph theory; Hypertext systems; Location; Matrix algebra; Trajectories; Differential privacies; High speed wireless networks; Location information; Location privacy; Mobile positioning devices; recommendation; Recommendation accuracy; Trajectory data minings; Big data",2-s2.0-85032383295
"Amroun H., Temkit M., Ammi M.","DNN-Based Approach for Recognition of Human Activity Raw Data in Non-Controlled Environment",2017,"Proceedings - 2017 IEEE 6th International Conference on AI and Mobile Services, AIMS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032300119&doi=10.1109%2fAIMS.2017.26_pubyear2017&partnerID=40&md5=6e00e9c5074a64af15c7ebf6bd4c1012","In this paper, we ask whether accurate recognition of activity can be obtained by using a network of smart objects. The approach consists in the classification of certain activities of the subjects: walking, standing, sitting and lying down. The study uses a network of commonly connected objects: a smart watch, a smartphone and a remote control and transported by the participants during an uncontrolled experiment. The sensor data of the three devices were classified by a deep neural networks (DNN) algorithm without prior pre-processing of the data. We show that (DNN) provides better results compared to Decision Tree (DT) and Support Vector Machine (SVM) algorithms. The results also show that the activities of the participants were classified with an accuracy of more than 98.53%, on average. © 2017 IEEE.","Activity recognition; Automatic classification; Iot; Unconstrained environment","Data handling; Data mining; Decision trees; Mobile telecommunication systems; Remote control; Support vector machines; Activity recognition; Automatic classification; Controlled environment; Human activities; Pre-processing; Smart objects; Support vector machine algorithm; Unconstrained environments; Deep neural networks",2-s2.0-85032300119
"Lei Y., Shiping C., Yu P.S.","Heterogeneous Service Information Mining Based on Parallel Computing",2017,"Proceedings - 2017 IEEE 1st International Conference on Cognitive Computing, ICCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032264752&doi=10.1109%2fIEEE.ICCC.2017.27&partnerID=40&md5=5f075779d008f823b4d1d61ebad2c91a","Cloud computing and service computing are merging together rapidly. This process bring large challenges to data mining and analysis. Our aim is to develop a system for service deployment, analysis, and recommendation to boost API economy. In this paper, we show our partial outcomes. © 2017 IEEE.","Big graph; Cluster; Community discovery; Mashup; Parallel computing","Cluster computing; Parallel processing systems; Big graph; Cluster; Community discoveries; Heterogeneous services; Mash-up; Service computing; Service deployment; Data mining",2-s2.0-85032264752
"He P., Zhu J., Zheng Z., Lyu M.R.","Drain: An Online Log Parsing Approach with Fixed Depth Tree",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032368743&doi=10.1109%2fICWS.2017.13&partnerID=40&md5=8c7685a02dd9138834eb877722bbb203","Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format, and then apply data mining models to extract critical system behavior information, which can assist Web service management. Most of the existing log parsing methods focus on offline, batch processing of logs. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis. © 2017 IEEE.","Log analysis; Log parsing; Online algorithm; Web service management","Batch data processing; Data mining; Forestry; Information management; Websites; Anomaly detection; Data mining models; Log analysis; Log parsing; On-line algorithms; Run-time information; Service provider; Web service management; Web services",2-s2.0-85032368743
"Gao Y., Zhang Q., Chen L., Wang K., Chen N., Liu H.","Research on application of FP-tree based association rule mining on test sequence in train control system",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032203145&doi=10.23919%2fChiCC.2017.8028952&partnerID=40&md5=f7ce38e7b848bc6c7abb2d9a9591427a","Based on the development and application of on-board subsystem test bench for current CTCS-3 system, this paper focuses on the approach of automatically generation of test sequence, takes the existing test sequences of ETCS-2 (European Train Control system level 2) as the train set existing relatively mature test sequence as the training set, to execute association rule mining. The whole data mining process involves data preparation (including data cleaning and data selection) firstly, providing basement for association rule, then establishes FP-tree and seeks test cases with frequent pattern through implementing FP-growth algorithm for the target database. Comparing the analysis results and experience, it shows that the association rule based on FP-tree could play an important role on the efficiency and verification of automatically generation of test sequence. © 2017 Technical Committee on Control Theory, CAA.","association rule; CTCS-3; data mining; FP-tree; test case; test sequence",,2-s2.0-85032203145
"Vasilievich B.S., Vladimirovich M.A., Nikolaevna C.S., Anatolievna K.L.","The Method of Immersion the Problem of Comparing Technical Objects in an Expert Shell in the Class of Artificial Intelligence Algorithms",2017,"IOP Conference Series: Materials Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030313569&doi=10.1088%2f1757-899X%2f225%2f1%2f012208&partnerID=40&md5=e50ca600b754a76178a74b4bf55a5800","The method of dip of the underlying computational problem of comparing technical object in an expert shell in the class of data mining methods is examined. An example of using the proposed method is given. © Published under licence by IOP Publishing Ltd.","composite index of quality, expert shell, neural network, perceptron.","Condensed matter physics; Engineering; Industrial engineering; Materials science; Artificial intelligence algorithms; Composite index; Computational problem; Data mining methods; Technical objects; Data mining",2-s2.0-85030313569
"Lu W., Chen B., Jin Z., Waxman D., Feng J.","Establishing the community structure of signed interconnected graph in data",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032184209&doi=10.23919%2fChiCC.2017.8029133&partnerID=40&md5=09fb9bc1381f0ffae43dcb1e858ff1a7","Graph communities can be an efficient and robust way to identify the clustering structure of data. A common scenario is the case that data matrix can be represented by graphs from instance and attribute. How to associate the consistent community structure in the two data matrices from instance and attribute respectively are important for understanding the data. We propose a novel and simple method to construct a interconnected network, by combining the instance and attribute graphs of the data as the interconnections between them, which unlike the bipartite graph cluster or the bicluster approaches. The community structure can be detected with an extended Q-modularity algorithm that also takes negative weighted edges into consideration, where each community can contain both instances and attributes that provide the association. Toy models are provided to illustrate the efficiency of the method. Real data of waste water treatment plants are treated by this method and the consistent community structure associated with day/measurements can be found. © 2017 Technical Committee on Control Theory, CAA.","Bi-clustering; Community Structure; Data mining; Interconnected graph",,2-s2.0-85032184209
"Dimitriadis S.I., Salis C.I.","Mining time-resolved functional brain graphs to an EEG-based Chronnectomic brain aged index (CBAI)",2017,"Frontiers in Human Neuroscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031995262&doi=10.3389%2ffnhum.2017.00423&partnerID=40&md5=9cf7cd10ef1d738b6faf36cae60f799a","The brain at rest consists of spatially and temporal distributed but functionally connected regions that called intrinsic connectivity networks (ICNs). Resting state electroencephalography (rs-EEG) is a way to characterize brain networks without confounds associated with task EEG such as task difficulty and performance. A novel framework of how to study dynamic functional connectivity under the notion of functional connectivity microstates (FCμstates) and symbolic dynamics is further discussed. Furthermore, we introduced a way to construct a single integrated dynamic functional connectivity graph (IDFCG) that preserves both the strength of the connections between every pair of sensors but also the type of dominant intrinsic coupling modes (DICM). The whole methodology is demonstrated in a significant and unexplored task for EEG which is the definition of an objective Chronnectomic Brain Aged index (CBAI) extracted from resting-state data (N = 94 subjects) with both eyes-open and eyes-closed conditions. Novel features have been defined based on symbolic dynamics and the notion of DICM and FCμstates. The transition rate of FCμstates, the symbolic dynamics based on the evolution of FCμstates (the Markovian Entropy, the complexity index), the probability distribution of DICM, the novel Flexibility Index that captures the dynamic reconfiguration of DICM per pair of EEG sensors and the relative signal power constitute a valuable pool of features that can build the proposed CBAI. Here we applied a feature selection technique and Extreme Learning Machine (ELM) classifier to discriminate young adults from middle-aged and a Support Vector Regressor to build a linear model of the actual age based on EEG-based spatio-temporal features. The most significant type of features for both prediction of age and discrimination of young vs. adults age groups was the dynamic reconfiguration of dominant coupling modes derived from a subset of EEG sensor pairs. Specifically, our results revealed a very high prediction of age for eyes-open (R2 = 0.60; y = 0.79x + 8.03) and lower for eyes-closed (R2 = 0.48; y = 0.71x + 10.91) while we succeeded to correctly classify young vs. middle-age group with 97.8% accuracy in eyes-open and 87.2% for eyes-closed. Our results were reproduced also in a second dataset for further external validation of the whole analysis. The proposed methodology proved valuable for the characterization of the intrinsic properties of dynamic functional connectivity through the age untangling developmental differences using EEG resting-state recordings. © 2017 Dimitriadis and Salis.","Chronnectomics; Cross-frequency coupling; Dominant coupling modes; EEG; Maturation index; Signal processing; Symbolic analysis; Time-varying network analysis","adult; age distribution; Article; brain function; brain maturation; Chronnectomic Brain Aged index; controlled study; data mining; electroencephalography; entropy; female; functional connectivity; human; human experiment; male; mental task; nerve cell network; normal human; resting state network; support vector machine; task performance",2-s2.0-85031995262
"Wang L., Cao S., Wan L., Wang F.","Web anomaly detection based on frequent closed episode rules",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032331350&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.338&partnerID=40&md5=5ffd4730c6ed7ab4470487a0e61a0123","Due to the fact that web services spread around the world, new threats are increasing. The misuse intrusion detection system is not able to provide enough protection for the security of Web Services, because it only detects formerly known attacks and cannot detect new unknown attacks. Web logs contain a lot of valuable information that is useful in preventing intrusion. In this paper, we present a new web anomaly detection method which uses FCERMining(Frequent Closed Episode Rules Mining) algorithm to analyze web logs and detect new unknown web attacks. The novel FCERMining algorithm parallelly mines the frequent closed episode rules on Spark, which handles massive data rapidly. Meanwhile, it reduces a part of rules which are redundant for anomaly detection to improve the matching efficiency. Then we also propose a grouping scheme to improve the parallel efficiency of FCERMining algorithm. Finally, we use SQLMAP and WebCruiser to simulate some web attacks, our method has a detection rate of 96.67% and a false alarm rate of 3.33% for detecting abnormal users. Our experimental results also demonstrate the reduction of redundant rules improve the matching efficiency. Furthermore, we compare the efficiency of our FCERMining algorithm with other pattern mining algorithms, experimental results indicate that our FCERMining algorithm outperforms other pattern mining algorithms. © 2017 IEEE.","Frequent closed episode rules; Spark; Web anomaly detection; Web logs","Big data; Blogs; Data communication systems; Data mining; Data privacy; Efficiency; Electric sparks; Embedded software; Embedded systems; Intrusion detection; Mercury (metal); Websites; Anomaly detection; Anomaly detection methods; False alarm rate; Frequent closed episodes; Misuse intrusion detections; Parallel efficiency; Pattern mining algorithms; Weblogs; Web services",2-s2.0-85032331350
"Sun D., Yang K., Shi Z., Wang Y.","A distinction method of flooding DDoS and flash crowds based on user traffic behavior",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032368731&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.221&partnerID=40&md5=c856dc9a6419e6f17297ca9ffca36d88","Discriminating Distributed Denial of Service (DDoS) from Flash Crowds (FC) is a tough and challenging problem, because there are many similarities between each other existed in network layer. In this paper, according to an extensive analysis of user traffic behavior of DDoS and FC, it can be found that some traffic abnormalities are existed between Bots and legitimate users. So a behavior-based method employed Data Mining isproposed to distinguish each other, and two public real-world datasets are used to evaluate the method. What's more, simulated traffic are produced to evaluate the method further, which is based on statistical parameters took from the two datasets and combined with two popular and common distributions together, Gaussian Distribution and Pareto Distribution. And two types of simulations are considered: Novice Simulation and Veteran Simulation. The result in Novice Simulation has almost 100% accuracy, while in Veteran Simulation, the result has a more than 98% accuracy, less than 15% FRP and 3% FNR, all of them show the proposed method could have a good accuracy and robustness. In addition, compared it with traditional methods-Entropy and Threshold methods in Veteran Simulation, the results indicate that both of them could hardly distinguish DDoS and FC, whilethe proposed method could achieve a better distinguished effect. © 2017 IEEE.","Flash Crowds; Flooding DDoS; Ran-dom Forest; Simulation; User Behavior","Big data; Data communication systems; Data mining; Data privacy; Denial-of-service attack; Embedded software; Embedded systems; Floods; Human computer interaction; Network layers; Network security; Pareto principle; Behavior-based methods; Distributed denial of service; Flash crowd; Pareto distributions; Ran-dom Forest; Simulation; Statistical parameters; User behaviors; Behavioral research",2-s2.0-85032368731
"Truex S., Liu L., Gursoy M.E., Yu L.","Privacy-Preserving Inductive Learning with Decision Trees",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032352887&doi=10.1109%2fBigDataCongress.2017.17&partnerID=40&md5=bc4ebb20d8b25ede185035b837453e32","With the continued explosion of digitized data, data mining and data collection have become more prevalent. With this growth, we have also seen increased concern over data privacy and intellectual property. Within this environment, an important question has emerged: Can machine learning and data mining techniques be leveraged without compromising privacy? This paper revisits the concepts and techniques of privacy-preserving decision tree learning, a fundamental model of inductive learning. We first examine different privacy risks during decision tree based inductive learning processes, including the sensitivity of private input data and potential privacy risks induced by inference over both the learning output and the intermediate results of inductive learning iterations. We then review and compare the privacy notions and properties of three orthogonal and yet complimentary technical frameworks: randomization-based data obfuscation, differential privacy, and secure multiparty computation. We analyze their effectiveness and review representative approaches in each of these three frameworks. Finally, we highlight some of the open challenges to privacy-preserving solutions for decision tree learning. © 2017 IEEE.",,"Big data; Cryptography; Data mining; Decision trees; Learning systems; Trees (mathematics); Decision tree learning; Differential privacies; Fundamental models; Intermediate results; Privacy preserving; Privacy preserving solutions; Secure multi-party computation; Technical frameworks; Data privacy",2-s2.0-85032352887
"Gao X., Yu W., Rong Y., Zhang S.","Ontology-Based Social Media Analysis for Urban Planning",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031893876&doi=10.1109%2fCOMPSAC.2017.4&partnerID=40&md5=04dfadc3e3629c15f3af83f9299379b1","This paper reports a social media analysis studyproposed by the Beijing Municipal Institute of Urban Planningand Design. The purpose is to explore techniques that can help theurban planning administrations to improve the social sensing andsocial perception abilities under the evolving data and technologyenvironments. A framework integrating a comprehensive set oftext mining algorithms is presented to conduct topic modeling, text clustering, event evolution detection, sentiment analysis, opinion mining, and information extraction on user-generatedcontents in Chinese social media. A domain ontology of Beijingurban planning is constructed to facilitate the text mining processes. Evaluations on two large, real-world datasets composed ofmicroblogs and WeChat articles about the residential communityand school education in Beijing demonstrate the effectiveness ofour framework. The study illustrates the power of combiningmachine learning with knowledge-based, semantic approaches inanalyzing social media for the domain of interest. © 2017 IEEE.","ontology; social media analysis; text mining; urban planning","Application programs; Computer software; Education; Knowledge based systems; Natural language processing systems; Ontology; Semantics; Social networking (online); Urban planning; Domain ontologies; Mining algorithms; Real-world datasets; School education; Semantic approach; Sentiment analysis; Social media analysis; Text mining; Data mining",2-s2.0-85031893876
"Hu C., Miao J., Su Z., Shi X., Chen Q., Luo X.","Precision-enhanced image attribute prediction model",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364781&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.324&partnerID=40&md5=1000bffc2c78b089251a1227ba508497","High-precision attribute prediction is a challenging issue due to the complex object and scene variations. Targeting on enhancing attribute prediction precision, we propose an Enhanced Attribute Prediction-Latent Dirichlet Allocation (EAP-LDA) model to address this issue. EAP-LDA model enhances the attribute prediction precision in two steps: classification adaptation and prediction enhancement. In classification adaptation, we transfer image low-level features to mid-level features (attributes) by the SVM classifiers, which are trained using the low-level features extracted from images. In prediction enhancement, we first exploit its advantages in extracting and analyzing the topic information between image samples and attributes by the LDA topic model. We then use a strategy to search the nearest neighbor image collection from test datasets by KNN. Finally, we evaluate the accuracy onHAT datasets and demonstrate significant improvement over the baseline algorithm. © 2017 IEEE.","Attribute Prediction; Classification Adaptation; Prediction Enhancement","Data communication systems; Data mining; Data privacy; Embedded software; Embedded systems; Forecasting; Image enhancement; Image processing; Nearest neighbor search; Statistics; Image attributes; Image collections; Latent Dirichlet allocation; Low-level features; Mid-level features; Nearest neighbors; Prediction model; Prediction precision; Big data",2-s2.0-85032364781
"Zhang P., Zhang L., Leung H., Wang J.","A Deep-Learning Based Precipitation Forecasting Approach Using Multiple Environmental Factors",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032389049&doi=10.1109%2fBigDataCongress.2017.34&partnerID=40&md5=4e36a9f72ee4068f352f1d13032e252f","Precise precipitation forecasting can better reflect the changing trend of climate and also provide timely and efficient environmental information for management decision, as well as prevent the occurrence of floods or droughts. In the era of big data, this paper proposes a novel approach for precipitation forecasting based on deep belief nets, called DBNPF (Deep Belief Network for Precipitation Forecast). Through simulating neural connecting structure of human brain, Gaussian kernel function for data conversion, and back-propagation network for fine-tuning the entire network, the features of the data in the original space are mapped into the new feature space with semantic feature through the dimensionality reduction. The proposed approach can not only learn the hierarchical representation of raw data using a highly generalized way, fully mining the information hidden in the original data, but also make a more accurate description of the rule underlying the different kind of time series of big data. Seven kinds of environmental factors, which are very closely related to precipitation, are used as input vector, and the next 24 hours precipitation is used as the output vector. A set of dedicated experiments with data from Zunyi area of Guizhou Province is conducted to validate the feasibility and useability of the model. We also compare the deep-learning based approach with other traditional machine learning approaches. The experimental results show that the proposed approach can improve the precision of precipitation forecasting. © 2017 IEEE.","Data mining; Deep-learning; multiple time series; Precipitation forecasting","Backpropagation; Data handling; Data mining; Deep learning; Forecasting; Learning algorithms; Learning systems; Semantics; Time series; Weather forecasting; Dimensionality reduction; Environmental information; Gaussian kernel functions; Hierarchical representation; Learning-based approach; Machine learning approaches; Multiple time series; Precipitation forecasting; Big data",2-s2.0-85032389049
"Yamamori A., Hagward A.M., Kobayashi T.","Can Developers' Interaction Data Improve Change Recommendation?",2017,"Proceedings - International Computer Software and Applications Conference",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029840337&doi=10.1109%2fCOMPSAC.2017.79&partnerID=40&md5=d71b48ed13627c82fe4a85574a521862","One of the most common causes of bugs is overlooking changes. To prevent bugs and improve the quality of the products, numerous studies have been undertaken on change guides based on logical couplings extracted from developers' past process histories, such as change history. While valuable change rules based on logical couplings can be gleaned found from the change history, these rules often fail to find appropriate candidates because the change histories in repositories only preserve a summary of changes between commits. We recently analyzed the interaction data produced by a developer in an integrated development environment. Such interaction data contains not only a detailed change history but also reference activities between commits. In this paper, we investigate whether logical couplings extracted from interaction data could improve change recommendation performance. We used the interaction data from actual open source development, not from the project only for this study. Experimental results obtained using the interaction data from actual open source development showed a significant improvement in the efficiency of the change recommendation process. The results also indicated improvement in the number of detected artifacts that the developer had forgot to change. © 2017 IEEE.","Change Guide; Change Impact Analysis; Interaction Data; Mining Software Repository; Software Maintenance","Application programs; Computer software maintenance; Couplings; Open source software; Change Guide; Change impact analysis; Integrated development environment; Interaction Data; Mining software repositories; Open source development; Process history; Recommendation performance; Computer software",2-s2.0-85029840337
"Chen N.C., Xie W., Welsch R.E., Larson K., Xie J.","Comprehensive Predictions of Tourists' Next Visit Location Based on Call Detail Records Using Machine Learning and Deep Learning Methods",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032387400&doi=10.1109%2fBigDataCongress.2017.10&partnerID=40&md5=41288d6c37a98608570d394cb9013f21","Recent developments in data mining and machine learning have helped to solve many issues in prediction and recommendation. In this project, we run a comprehensive study on individual behavior patterns from call detail records (CDR) data to predict tourists' future stops. Multiple classification algorithms are employed, including Decision Tree, Random Forest, Neural Network, Naïve Bayes and SVM. In addition, a Recurrent Neural Network-Long Short Term Memory (LSTM) that is ordinarily applied to language inference problems is tested. Surprisingly, we find that LSTM provides us with the best prediction (94.8%), while Random Forest/Neural Network give the second best (85%). Our investigation suggests that the memory-dependence property of LSTM architecture gives it great expressive power to model our time-series location data, making it an outstanding classifier. © 2017 IEEE.","Call Detail Record; Data Mining; Deep Learning; Next Location Prediction","Artificial intelligence; Data mining; Decision trees; Deep learning; Forecasting; Learning systems; Location; Long short-term memory; Recurrent neural networks; Call detail records; Comprehensive prediction; Dependence properties; Expressive power; Individual behavior; Language inference; Multiple Classification; Next location predictions; Big data",2-s2.0-85032387400
"Bikku T., Sambasiva Rao N., Ananda Rao A.","A Novel Multi-Class Ensemble Model for Classifying Imbalanced Biomedical Datasets",2017,"IOP Conference Series: Materials Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030320220&doi=10.1088%2f1757-899X%2f225%2f1%2f012161&partnerID=40&md5=3cb9ceef389a204c279868083e8364d5","This paper mainly focuseson developing aHadoop based framework for feature selection and classification models to classify high dimensionality data in heterogeneous biomedical databases. Wide research has been performing in the fields of Machine learning, Big data and Data mining for identifying patterns. The main challenge is extracting useful features generated from diverse biological systems. The proposed model can be used for predicting diseases in various applications and identifying the features relevant to particular diseases. There is an exponential growth of biomedical repositories such as PubMed and Medline, an accurate predictive model is essential for knowledge discovery in Hadoop environment. Extracting key features from unstructured documents often lead to uncertain results due to outliers and missing values. In this paper, we proposed a two phase map-reduce framework with text preprocessor and classification model. In the first phase, mapper based preprocessing method was designed to eliminate irrelevant features, missing values and outliers from the biomedical data. In the second phase, a Map-Reduce based multi-class ensemble decision tree model was designed and implemented in the preprocessed mapper data to improve the true positive rate and computational time. The experimental results on the complex biomedical datasets show that the performance of our proposed Hadoop based multi-class ensemble model significantly outperforms state-of-the-art baselines. © Published under licence by IOP Publishing Ltd.","Bioinformatics; Ensemble model; Map-Reduce; Medical databases; Textual Decision Patterns","Big data; Bioinformatics; Data mining; Decision trees; Learning systems; Medical computing; Statistics; Text processing; Trees (mathematics); Decision tree modeling; Ensemble modeling; Feature selection and classification; Map-reduce; Medical database; Pre-processing method; Textual Decision Patterns; Unstructured documents; Classification (of information)",2-s2.0-85030320220
"Asthana S., Megahed A., Strong R.","A Recommendation System for Proactive Health Monitoring Using IoT and Wearable Technologies",2017,"Proceedings - 2017 IEEE 6th International Conference on AI and Mobile Services, AIMS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032299242&doi=10.1109%2fAIMS.2017.11&partnerID=40&md5=fad58eb1689c2cfe472908bfa2b56be8","Proactive monitoring of one's health could avoid serious diseases as well as better maintain the individual's well-being. In today's Internet of Things (IoT) world, there has been numerous wearable technological devices to monitor/measure different health attributes. With the increasing number of attributes and wearables, it becomes unclear to individuals which ones they should be using. The aim of this paper is to provide a novel recommendation engine for personalized advised wearables and IoT solutions for any given individual. The way the engine works is through first identifying the diseases that this person is at risk of, given his/her attributes and medical history. This is done via analyzing the individual's unstructured medical history using text mining, adding it to his/her structured demographic attributes, and then feeding this data to a machine learning classification model that predicts eventual diseases. Then, we map these diseases to the attributes that need to be measured in order to monitor them. Lastly, we use a mathematical optimization model that we developed to recommend the optimal wearable devices and IoT solutions for the individual. Thus, our solution enables proactive health monitoring and can thus provide a significant human benefit. © 2017 IEEE.","Analytics; Healthcare; IoT; Monitoring; Optimization; Text Mining; Wearable Technologies","Data mining; Engines; Health; Health care; Internet of things; Learning systems; Medical computing; Mobile telecommunication systems; Monitoring; Optimization; Recommender systems; Text processing; Analytics; Health monitoring; Internet of Things (IOT); Machine learning classification; Mathematical optimization model; Proactive Monitoring; Text mining; Wearable devices; Wearable technology",2-s2.0-85032299242
"Peng H., Jin Z., Miller J.A.","Bayesian Networks with Structural Restrictions: Parallelization, Performance, and Efficient Cross-Validation",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032384652&doi=10.1109%2fBigDataCongress.2017.11&partnerID=40&md5=146db1908e341d47fcd0712db00474e4","Bayesian Network algorithms are widely applied in the fields of bioinformatics, document classification, big data, and marketing informatics. In this paper, several Bayesian Network algorithms are evaluated, including Naive Bayes, Tree Augmented Naive Bayes, k-BAN, and k-BAN with Order Swapping. The algorithms are implemented using Scala and compared with the bnlearn library in R and Weka. Several datasets with varying numbers of attributes and instances are used to test the accuracy and efficiency of the implementations of the algorithms provided by the three packages. When handling huge datasets, issues involving accuracy, efficiency, and serial vs. parallel execution become more critical and should be addressed. We implemented several parallel algorithms as well as an efficient way to perform cross-validations, resulting in significant speedups. © 2017 IEEE.","Analytics; Bayesian networks; Big data; Classification; Data mining; Parallel programming","Bayesian networks; Classification (of information); Classifiers; Computer aided diagnosis; Data mining; Efficiency; Information retrieval systems; Parallel programming; Analytics; Cross validation; Document Classification; Network algorithms; Parallel executions; Parallelizations; Structural restrictions; Tree augmented Naive Bayes; Big data",2-s2.0-85032384652
"Yao J., Wencheng W., Prabhakara J., Watts-Englert J., Simmons I., Mongeon M., Tharayil M.","Crowdsourcing Workflow Optimization to Internal Worker Crowds",2017,"Proceedings - 2017 IEEE 1st International Conference on Cognitive Computing, ICCC 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032303193&doi=10.1109%2fIEEE.ICCC.2017.15&partnerID=40&md5=350870a2e1f067a98b15d37216e9eac5","Business service companies generate revenue by delivering high-quality services to their clients. Such business services could be customer support, transaction processing, information technology, etc. Given the large service volume, meeting strict service level agreements while maintaining a reasonable profit margin can be challenging. Naturally, the worker crowd is trained by the enterprise using carefully designed standard operations and workflows aimed at maximizing its performance and thus the output. Service delivering workflows are never perfect, continuous effort must be invested into improving and optimizing the workflows. However, such effort is usually difficult to sustain. Despite considerable advances in the workflow optimization technologies, a business analyst may still spend countless hours studying the workflow in order to identify the opportunities for improvement. This issue motivated us to develop a novel crowd-based framework to utilize the worker crowd for workflow optimization. In this paper, we illustrate our approach to effectively leverage the internal worker crowd to improve the workflows for the enterprise. A modeling is provided to elaborate the different stages in the framework. And a reference implementation is detailed to demonstrate the practical use of the framework. © 2017 IEEE.","Business process optimization; Crowdsourcing; Information extraction; Process mining","Crowdsourcing; Information retrieval; Optimization; Business process optimization; High quality service; Process mining; Reasonable profit; Reference implementation; Service Level Agreements; Transaction processing; Workflow optimization; Data mining",2-s2.0-85032303193
"Zhang J., Yu P.S., Aggarwal C.C., Cui L.","Real Time Social Attitude Expression Prediction",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032357006&doi=10.1109%2fBigDataCongress.2017.69&partnerID=40&md5=6e259e714b89a23991e621e63ac73413","Via online social interactions, users in social networks can form their personal attitudes toward other users. Some of the personal social attitudes will be expressed explicitly, which are represented as the signed social links from the initiators to the recipients. In this paper, we will study the 'social Attitude exPression prEdiction' (APE) problem, which aims at inferring both the expression activities and the expressed social attitudes simultaneously. The APE problem is very challenging to address due to two reasons: (1) extraction of useful features for predicting social attitude expression activities and the expressed social attitudes, and (2) the prediction model needs to incorporates the correlations between these two tasks covered in the APE problem. To address the APE problem, a novel real-time 'Bayesian network based Integrated Social Attitude exPression inference' framework BI-SAP is introduced in this paper. Framework BI-SAP extracts a set of features for the expression activities and social attitudes from the networks based on various social closeness measures and social balance theory respectively. In addition, with the extracted features, the integrated social expression prediction framework BI-SAP is built based on the Bayesian network model, in which the dependence relationships between these two tasks covered in APE can be effectively represented and the parameters can be updated in real time. Extensive experiments conducted on real-world signed network datasets have demonstrated the effectiveness of BI-SAP in addressing the APE problem. © 2017 IEEE.","Data Mining; Signed Networks; Social Attitude Inference","Bayesian networks; Data mining; Forecasting; Bayesian network models; Dependence relationships; Prediction model; Real-world; Signed networks; Social Attitude Inference; Social balances; Social interactions; Big data",2-s2.0-85032357006
"Zhang L.-J., Chen H.","PAS4SCF - A Publication Analytics System for Services Conference Federation (SCF 2004-2016)",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032352309&doi=10.1109%2fBigDataCongress.2017.52&partnerID=40&md5=9ef038f8f89c659a3b7cc2503e872de9","As a practical and valuable method, bibliometric analysis has been applied to many research fields. In this paper, we propose a Publication Analytics System for Services Conference Federation (PAS4SCF). With SCF paper published from 2004 to 2016, the paper illustrates feature extraction methods used in the PAS4SCF and conducts citation analysis and topic analytics to explore the panorama of PAS4SCF as well as SCF publications. The PAS4SCF offers many interesting results which can help paper reviewers check plagiarism, build author credit and predict technology trends. We expect more researchers and engineers to participate in the discipline of services computing and discover more technologies, trends and methodologies based on or related to services computing, thus improving the overall scientific and social impact. © 2017 IEEE.","Bibliometric; Big Data; Gartner; SCF","Data mining; Publishing; Analytics systems; Bibliometric; Bibliometric analysis; Citation analysis; Feature extraction methods; Gartner; Services computing; Technology trends; Big data",2-s2.0-85032352309
"Pallaprolu S.C., Sankineni R., Thevar M., Karabatis G., Wang J.","Zero-Day Attack Identification in Streaming Data Using Semantics and Spark",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032368863&doi=10.1109%2fBigDataCongress.2017.25&partnerID=40&md5=4456e7f8de3a41369fe985e09ae364e0","Intrusion Detection Systems (IDS) have been in existence for many years now, but they fall short in efficiently detecting zero-day attacks. This paper presents an organic combination of Semantic Link Networks (SLN) and dynamic semantic graph generation for the on the fly discovery of zero-day attacks using the Spark Streaming platform for parallel detection. In addition, a minimum redundancy maximum relevance (MRMR) feature selection algorithm is deployed to determine the most discriminating features of the dataset. Compared to previous studies on zero-day attack identification, the described method yields better results due to the semantic learning and reasoning on top of the training data and due to the use of collaborative classification methods. We also verified the scalability of our method in a distributed environment. © 2017 IEEE.","Collaborative mining; Flow Creation; IDS; Semantic learning and reasoning; Spark Streaming; Zero-day Attack Identification","Classification (of information); Computer worms; Intrusion detection; Network security; Collaborative classifications; Distributed environments; Feature selection algorithm; Flow Creation; Intrusion Detection Systems; Minimum redundancy-maximum relevances; Semantic learning; Zero day attack; Big data",2-s2.0-85032368863
"Yue X., Ding Y., Hu H., Zhao D.","Analysis on the association between driver's macroscopic characteristics and accident type of urban traffic accidents",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032203793&doi=10.23919%2fChiCC.2017.8028257&partnerID=40&md5=c022882a591b42dd80575bb5e5af5977","Data mining technology is used to analyze the association between driver's macroscopic characteristics and accident types of drivers of urban traffic accidents. Based on the correlation between microscopic characteristics (such as physiology and psychology of the driver and macro characteristics (such as sex, age and driving age), the statistic values of driver sex, age and driving characteristics of road in traffic accidents are taken as the comparison sequences, and the types of traffic accidents as the reference sequence. Simultaneously, the gray relational model of the driver's accident and the accident type is established. By analyzing the quantified results of the gray incidence matrix, evaluated the different characteristics of the transport drivers for the influence of the accident types, and a reference for prevention of traffic accidents is provided effectively. © 2017 Technical Committee on Control Theory, CAA.","Accident type; Data mining technology; Driver's macroscopic characteristics; Frequency; Grey correlation method",,2-s2.0-85032203793
"Baralis E., Cagliero L., Farinetti L., Mezzalama M., Venuto E.","Experimental Validation of a Massive Educational Service in a Blended Learning Environment",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031904460&doi=10.1109%2fCOMPSAC.2017.123&partnerID=40&md5=e921923f44a8f3e9d06b5f0f19ef449b","New information and communication technologies offer today many opportunities to improve the quality of educational services in universities and in particular they allow to design and implement innovative learning models. This paper describes and validates our university blended learning model, and specifically the massive educational video service that we offer to our students since 2010. In these years, we have gathered a huge amount of detailed data about the students' access to the service, and the paper describes a number of analyses that we carried out with these data. The common goal was to find out experimentally whether the main objectives of the educational video service we had in our mind when we designed it, namely appreciation, effectiveness and flexibility, were reflected by the users' behavior. We analyzed how many students used the service, for how many courses, and how many videos they accessed within a course (appreciation of the service). We analyzed the correlation between the use of the service and the performance of the students in terms of successful examination rate and average mark (effectiveness of the service). Finally, by using data mining techniques we profiled users according to their behavior while accessing the educational video service. We found out six different patterns that reflect different uses of the services matching different learning goals (flexibility of the service). The results of these analyses show the quality of the proposed blended learning model and the coherency of its implementation with respect to the design goals. © 2017 IEEE.","educational video; learning analytics; learning methodologies; learning technologies; massive online courses","Application programs; Computer aided instruction; Computer software; Data mining; Students; Educational videos; learning analytics; learning methodologies; Learning technology; Online course; Education",2-s2.0-85031904460
"Jena M.K., Samantaray S.R., Panigrahi B.K.","Variational mode decomposition-based power system disturbance assessment to enhance WA situational awareness and post-mortem analysis",2017,"IET Generation, Transmission and Distribution",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031033570&doi=10.1049%2fiet-gtd.2016.1827&partnerID=40&md5=cffa558e9429248cc5bad2411fcb627f","This study presents a variational mode decomposition (VMD)-based approach to analyse wide-area (WA) measurements based signals. The commonly used empirical mode decomposition (EMD) has limitations such as sensitivity to noise and sampling rate. However, VMD is an entirely non-recursive algorithm, where the modes are extracted concurrently. These modes help in extracting dynamic patterns of different power system disturbances. The performance of the proposed scheme is extensively validated on the IEEE-39 bus New England test system. The modes generated and the frequency deviation contours of the disturbances including generation loss, fault and line outage are assessed using VMD and the results provide improved performance in terms of decomposition quality compared with the existing EMD technique. Furthermore, a data-mining model known as decision tree is used to classify different power system disturbances based on intrinsic mode functions generated through VMD. The suggested method shows improved decomposition quality and classification accuracy. Thus, the proposed scheme is a potential candidate for improving WA situational awareness along with a post-mortem analysis of real events occurring in a power system. © The Institution of Engineering and Technology.",,"Decision trees; Signal processing; Transients; Trees (mathematics); Classification accuracy; Empirical Mode Decomposition; Intrinsic Mode functions; New England test system; Post mortem analysis; Power system disturbances; Recursive algorithms; Situational awareness; Data mining",2-s2.0-85031033570
"Luu M.-D., Lim E.-P.","Do your friends make you buy this brand?: Modeling social recommendation with topics and brands",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028981794&doi=10.1007%2fs10618-017-0535-9&partnerID=40&md5=3202ae5e81890f4777e06dc2ef50efd7","Consumer behavior and marketing research have shown that brand has significant influence on product reviews and product purchase decisions. However, there is very little work on incorporating brand related factors into product recommender systems. Meanwhile, the similarity in brand preference between a user and other socially connected users also affects her adoption decisions. To integrate seamlessly the individual and social brand related factors into the recommendation process, we propose a novel model called Social Brand–Item–Topic (SocBIT). As the original SocBIT model does not enforce non-negativity, which poses some difficulty in result interpretation, we also propose a non-negative version, called SocBIT(Formula presented.). Both SocBIT and (Formula presented.) return not only user topic interest, but also brand-related user factors, namely user brand preference and user brand-consciousness. The former refers to user preference for each brand, the latter refers to the extent to which a user relies on brand to make her adoption decisions. Our experiments on real-world datasets demonstrate that SocBIT and (Formula presented.) significantly improve rating prediction accuracy over state-of-the-art models such as Social Regularization Ma et al. (in: ACM conference on web search and data mining (WSDM), 2011), Recommendation by Social Trust Ensemble Ma et al. (in: ACM conference on research and development in information retrieval (SIGIR), 2009a) and Social Recommendation Ma et al. (in: ACM conference on information and knowledge management (CIKM), 2008), which incorporate only the social factors. Specifically, both SocBIT and (Formula presented.) offer an improvement of at least 22% over these state-of-the-art models in rating prediction for various real-world datasets. Last but not least, our models also outperform the mentioned models in adoption prediction, e.g., they provide higher precision-at-N and recall-at-N. © 2017 The Author(s)","Adoption; Brand effect; Latent factors; Probabilistic matrix factorization; Social recommendation","Aluminum; Consumer behavior; Data mining; Factorization; Forecasting; Knowledge management; Marketing; Recommender systems; Adoption; Brand effect; Latent factor; Probabilistic matrix factorizations; Social recommendation; Behavioral research",2-s2.0-85028981794
"Wang H., Chi X., Wang Z., Xu X., Chen S.","Extracting Fine-Grained Service Value Features and Distributions for Accurate Service Recommendation",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032369962&doi=10.1109%2fICWS.2017.43&partnerID=40&md5=dba5688e631c79d68bd018cee4f13598","With more proliferation of services and higher degree of personalization, higher accurate approaches to service recommendation are becoming more and more pivotal. Performance of existing service recommendation approaches is not satisfactory due to the sparseness of available data set or the incomplete information of the global service market, which make it difficult to identify a customer's potential preferences on available services. In this paper, we extract finegrained value features from customer reviews, and identify the personalized distribution of each value features to demonstrate the value preference of a specific customer. Then, a novel recommendation algorithm (VFDSR) is proposed. An algorithm VFMine based on text mining is presented to effectively extract value features from customer reviews. A VFDAnalysis algorithm based on sentiment analysis is employed to identify the value feature distributions. Based on it, VFDSR recommends top-satisfying services to customers. In addition, the value feature distributions are visualized in the form of 'heatmaps'. Comprehensive experiments are conducted on a Yelp dataset and the experimental results show the superiority of our approach. © 2017 IEEE.","natural language process; service recommendation; value feature; value feature distribution; visualization","Data mining; Flow visualization; Natural language processing systems; Sales; Websites; Feature distribution; Incomplete information; Natural language process; Personalizations; Recommendation algorithms; Sentiment analysis; Service recommendations; value feature; Web services",2-s2.0-85032369962
"Megahed A., Asthana S., Becker V., Nakamura T., Gajananan K.","A Method for Selecting Peer Deals in IT Service Contracts",2017,"Proceedings - 2017 IEEE 6th International Conference on AI and Mobile Services, AIMS 2017",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032302034&doi=10.1109%2fAIMS.2017.10&partnerID=40&md5=3b7d9ad261484ca34658c0f77e4a7371","To respond to requests for proposals from clients requiring complex Information technology (IT) services, IT service providers have to prepare a solution composed of the multiple services requested by the clients and price that solution. Then, each provider competes in a tender-kind of process trying to convince the client with their solution. Pricing these solutions/deals, using historical and market data, is a complex task that we studied in our previous works. In our prior pricing approaches, we used a simple algorithm for selecting similar historical and market deals to the one we are trying to price, before we mine the data of these deals to estimate the costs of that latter deal. However, there are multiple limitations to that algorithm that we overcome in the novel approach that we present in this paper. These limitations include missing on some similar deals due to the way we chose them. Our new approach involves an iterative algorithm that selects peer deals at different levels until a pre-specified number of deals is determined. We present a proof-of-concept implementation of our approach, using real-world data, to illustrate its efficiency. © 2017 IEEE.","Analytics; Data Mining; Peer Deals; Predictive Analytics; Pricing Services; Services","Commerce; Data mining; Iterative methods; Mobile telecommunication systems; Predictive analytics; Analytics; IT service providers; Iterative algorithm; Multiple services; Peer Deals; Pricing services; Requests for proposals; Services; Costs",2-s2.0-85032302034
"Zhang W., Wang J.","A Hybrid Learning Framework for Imbalanced Stream Classification",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032350372&doi=10.1109%2fBigDataCongress.2017.70&partnerID=40&md5=33a256fbe13789eb64c3b409db5e372b","The pervasive imbalanced class distribution occurring in real-world stream applications, such as surveillance, security and finance, in which data arrive continuously has sparked extensive interest in the study of imbalanced stream classification. In such applications, the evolution of unstable class concepts is always accompanied and complicated by the skewed class distribution. However, most of the existing methods focus on either class imbalance problem or non-stationary learning problem, the combined approach of addressing both issues has enjoyed relatively little research. In this paper, we propose a hybrid framework for imbalanced stream learning that consists of three components: classifier updating, resampling and cost sensitive classifier. Based on the framework, we propose a hybrid learning algorithm to combine data-level and algorithm-level methods as well as classifier retraining mechanics to tackle class imbalance in data streams. Our experiments using real-world datasets and synthetic datasets show that our proposed hybrid learning algorithm can have better effectiveness and efficiency. © 2017 IEEE.","Class imbalance; concept drift; data stream mining; hybrid learning","Big data; Class imbalance; Class imbalance problems; Concept drifts; Data stream mining; Effectiveness and efficiencies; Hybrid learning; Hybrid learning algorithm; Skewed class distributions; Learning algorithms",2-s2.0-85032350372
"Hajjej F., Hlaoui Y.B., Ayed L.J.B.","Cloud Adapted Workflow e-Assessment System: Cloud-AWAS",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031944466&doi=10.1109%2fCOMPSAC.2017.86&partnerID=40&md5=11f13a3b565e1d3ca53285590fe69c7e","In this paper, we present an approach to adapt the e-assessment workflow by considering learner's profiles. We have started by creating a learner profile ontology based on extraction data from e-assessment activities, file log and personal information. Then, we have defined three adaptation actions: Add Activity, Edit Activity and Delete Activity, applied on the workflow assessment and using information extracted from learner profile ontology instances. Each action is applied according to conditions. After that, we present some results of the empirical evaluation of our system. © 2017 IEEE.","adaptation; cloud computing; e-assessment; wokflow","Application programs; Cloud computing; Computer software; Ontology; adaptation; E assessments; Empirical evaluations; Learner profiles; Learner's profile; Personal information; wokflow; Workflow assessment; Data mining",2-s2.0-85031944466
"Carson-Chahhoud K.V., Wakai A., van Agteren J.E.M., Smith B.J., Mccabe G., Brinn M.P., O'Sullivan R.","Simple aspiration versus intercostal tube drainage for primary spontaneous pneumothorax in adults",2017,"Cochrane Database of Systematic Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028958760&doi=10.1002%2f14651858.CD004479.pub3&partnerID=40&md5=cc6a129c015c3cb3760c697c40702eef","Background: For management of pneumothorax that occurs without underlying lung disease, also referred to as primary spontaneous pneumothorax, simple aspiration is technically easier to perform than intercostal tube drainage. In this systematic review, we seek to compare the clinical efficacy and safety of simple aspiration versus intercostal tube drainage for management of primary spontaneous pneumothorax. This review was first published in 2007 and was updated in 2017. Objectives: To compare the clinical efficacy and safety of simple aspiration versus intercostal tube drainage for management of primary spontaneous pneumothorax. Search methods: We searched the Cochrane Central Register of Controlled Trials (CENTRAL; 2017, Issue 1) in the Cochrane Library; MEDLINE (1966 to January 2017); and Embase (1980 to January 2017). We searched the World Health Organization (WHO) International Clinical Trials Registry for ongoing trials (August 2017). We checked the reference lists of included trials and contacted trial authors. We imposed no language restrictions. Selection criteria: We included randomized controlled trials (RCTs) of adults 18 years of age and older with primary spontaneous pneumothorax that compared simple aspiration versus intercostal tube drainage. Data collection and analysis: Two review authors independently selected studies for inclusion, assessed trial quality, and extracted data. We combined studies using the random-effects model. Main results: Of 2332 publications obtained through the search strategy, seven studies met the inclusion criteria; one study was ongoing and six studies of 435 participants were eligible for inclusion in the updated review. Data show a significant difference in immediate success rates of procedures favouring tube drainage over simple aspiration for management of primary spontaneous pneumothorax (risk ratio (RR) 0.78, 95% confidence interval (CI) 0.69 to 0.89; 435 participants, 6 studies; moderate-quality evidence). Duration of hospitalization however was significantly less for patients treated by simple aspiration (mean difference (MD) -1.66, 95% CI -2.28 to -1.04; 387 participants, 5 studies; moderate-quality evidence). A narrative synthesis of evidence revealed that simple aspiration led to fewer adverse events (245 participants, 3 studies; low-quality evidence), but data suggest no differences between groups in terms of one-year success rate (RR 1.07, 95% CI 0.96 to 1.18; 318 participants, 4 studies; moderate-quality evidence), hospitalization rate (RR 0.60, 95% CI 0.25 to 1.47; 245 participants, 3 studies; very low-quality evidence), and patient satisfaction (median between-group difference of 0.5 on a scale from 1 to 10; 48 participants, 1 study; low-quality evidence). No studies provided data on cost-effectiveness. Authors' conclusions: Available trials showed low to moderate-quality evidence that intercostal tube drainage produced higher rates of immediate success, while simple aspiration resulted in a shorter duration of hospitalization. Although adverse events were reported more commonly for patients treated with tube drainage, the low quality of the evidence warrants caution in interpreting these findings. Similarly, although this review observed no differences between groups when early failure rate, one-year success rate, or hospital admission rate was evaluated, this too needs to be put into the perspective of the quality of evidence, specifically, for evidence of very low and low quality for hospitalization rate and patient satisfaction, respectively. Future adequately powered research is needed to strengthen the evidence presented in this review. © 2017 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.",,"adverse event; clinical effectiveness; data mining; evidence based medicine; follow up; hospital readmission; hospital utilization; human; intermethod comparison; length of stay; morality; patient satisfaction; pleurectomy; pleurodesis; postoperative pain; priority journal; process design; randomized controlled trial (topic); Review; risk assessment; spontaneous pneumothorax; thoracocentesis; thorax drainage; treatment failure",2-s2.0-85028958760
"Huang C., Yao L., Wang X., Benatallah B., Sheng Q.Z.","Expert as a Service: Software Expert Recommendation via Knowledge Domain Embeddings in Stack Overflow",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032369371&doi=10.1109%2fICWS.2017.122&partnerID=40&md5=6de21b249c2bbb685fda38c5cfe78c35","Question answering (Q&A) communities have gained momentum recently as an effective means of knowledge sharing over the crowds, where many users are experts in the real-world and can make quality contributions in certain domains or technologies. Although the massive user-generated Q&A data present a valuable source of human knowledge, a related challenging issue is how to find those expert users effectively. In this paper, we propose a framework for finding such experts in a collaborative network. Accredited with recent works on distributed word representations, we are able to summarize text chunks from the semantics perspective and infer knowledge domains by clustering pre-trained word vectors. In particular, we exploit a graph-based clustering method for knowledge domain extraction and discern the shared latent factors using matrix factorization techniques. The proposed clustering method features requiring no post-processing of clustering indicators and the matrix factorization method is combined with the semantic similarity of the historical answers to conduct expertise ranking of users given a query. We use Stack Overflow, a website with a large group of users and a large number of posts on topics related to computer programming, to evaluate the proposed approach and conduct extensively experiments to show the effectiveness of our approach. © 2017 IEEE.","Expert as a Service; Expertise finding; Knowledge discovery; Question answering; Stack Overflow","Cluster analysis; Computer programming; Data mining; Factorization; Graphic methods; Matrix algebra; Natural language processing systems; Semantics; Websites; Collaborative network; Expert as a Service; Expert recommendations; Expertise finding; Graph-based clustering; Matrix factorizations; Question Answering; Stack overflow; Web services",2-s2.0-85032369371
"Ali K., Dong H., Bouguettaya A., Erradi A., Hadjidj R.","Sentiment Analysis as a Service: A Social Media Based Sentiment Analysis Framework",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364127&doi=10.1109%2fICWS.2017.79&partnerID=40&md5=376d2b71a5ae2528b15ccde9a09aa8dd","We propose a 'Sentiment Analysis as a Service' (SAaaS) framework that abstracts sentiments from social information services, analyses and transforms into useful information. We propose a dynamic service composition mechanism for sentiment analysis based on the social information service classification. We also propose a new quality model to assess the quality of social information services. We use social media based public health surveillance as a motivating scenario. In particular, we focus on the spatio-temporal properties of social media users' sentiments to identify the locations of disease outbreaks. Experiments are conducted on the real-world datasets. Analytical results preliminarily show the performance of our proposed approach. © 2017 IEEE.","sentiment analysis; service composition; service quality; Social information services; spatio-temporal","Classification (of information); Data mining; Information services; Quality of service; Social aspects; Social networking (online); Web services; Websites; Dynamic service composition; Public health surveillances; Real-world datasets; Sentiment analysis; Service compositions; Service Quality; Spatio temporal; Spatio-temporal properties; Quality control",2-s2.0-85032364127
"Zhang L.J., Li N., Zhang J.J., Tian X.Y.","Application of neural network in modeling of activated sludge wastewater treatment process",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032199793&doi=10.23919%2fChiCC.2017.8028074&partnerID=40&md5=ccfe6e093b73e145a2f5ce0b6d5a19e6","This paper present a prediction model for three different objection (the airflow rate, the carbonaceous biochemical oxygen demand (CBOD) of the effluent, and the total suspend solids (TSS) of the effluent. The model is built by the MLP neural network. The accurancy of the prediction result of MLP neural network is compared with the accurancy of the result of trational stational autoregressive model(AR). The conclution is that the percentage error prediction model of the MLP neural network (PE), the fractional deviation (FB), normalized mean square error (NMSE), the mean absolute error (MAE) and mean square error (MSE) prediction model of evaluation index is better than the AR model. In other words, the prediction model based on MLP neural network provides a reliable basis for reducing the energy consumption in the activated sludge process of industrial waste water treatment and further improving its effect on the treatment of industrial waste water. © 2017 Technical Committee on Control Theory, CAA.","Activated sludge process; Data mining; Industrial waste water treatment; MLP neural network",,2-s2.0-85032199793
"Shi M., Liu J., Zhou D., Tang M., Cao B.","WE-LDA: A Word Embeddings Augmented LDA Model for Web Services Clustering",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032359809&doi=10.1109%2fICWS.2017.9&partnerID=40&md5=a69ecbe5815d7614ce4d4555565c1452","Due to the rapid growth in both the number and diversity of Web services on the web, it becomes increasingly difficult for us to find the desired and appropriate Web services nowadays. Clustering Web services according to their functionalities becomes an efficient way to facilitate the Web services discovery as well as the services management. Existing methods for Web services clustering mostly focus on utilizing directly key features from WSDL documents, e.g., input/output parameters and keywords from description text. Probabilistic topic model Latent Dirichlet Allocation (LDA) is also adopted, which extracts latent topic features of WSDL documents to represent Web services, to improve the accuracy of Web services clustering. However, the power of the basic LDA model for clustering is limited to some extent. Some auxiliary features can be exploited to enhance the ability of LDA. Since the word vectors obtained by Word2vec is with higher quality than those obtained by LDA model, we propose, in this paper, an augmented LDA model (named WE-LDA) which leverages the high-quality word vectors to improve the performance of Web services clustering. In WE-LDA, the word vectors obtained by Word2vec are clustered into word clusters by K-means++ algorithm and these word clusters are incorporated to semi-supervise the LDA training process, which can elicit better distributed representations of Web services. A comprehensive experiment is conducted to validate the performance of the proposed method based on a ground truth dataset crawled from ProgrammableWeb. Compared with the state-of-the-art, our approach has an average improvement of 5.3% of the clustering accuracy with various metrics. © 2017 IEEE.","clustering; K-means++; LDA; Web services; Word2vec","Data mining; Statistics; Websites; clustering; Distributed representation; Ground-truth dataset; K-means; Latent dirichlet allocations; Probabilistic topic models; Web Services discovery; Word2vec; Web services",2-s2.0-85032359809
"He G., Yu B., Li S., Zhu Y.","Comprehensive evaluation of ecological security in mining area based on PSR–ANP–GRAY",2017,"Environmental Technology (United Kingdom)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029447110&doi=10.1080%2f09593330.2017.1371250&partnerID=40&md5=9d84db95a48d260fb6bd61d43888c4c5","With the large exploitation of mineral resources, a series of problems have appeared in the ecological environment of the mining area. Therefore, evaluating the ecological security of mining area is of great significance to promote its healthy development. In this paper, the evaluation index system of ecological security in mining area was constructed from three dimensions of nature, society and economy, combined with Pressure–State–Response framework model. Then network analytic hierarchy process and GRAY relational analysis method were used to evaluate the ecological security of the region, and the weighted correlation degree of ecological security was calculated through the index data of a coal mine from 2012 to 2016 in China. The results show that the ecological security in the coal mine area is on the rise as a whole, though it alternatively rose and dropped from 2012 to 2016. Among them, the ecological security of the study mining area is at the general security level from 2012 to 2015, and at a relatively safe level in 2016. It shows that the ecological environment of the study mining area can basically meet the requirement of the survival and development of the enterprises. © 2017 Informa UK Limited, trading as Taylor & Francis Group","ANP; Ecological security; GRAY relational analysis; mining area; PSR model","China; coal mining; economic aspect; human",2-s2.0-85029447110
"Li J., Chen J., Huang M., Zhou M., Zhang L., Xie W.","An integration testing platform for software vulnerability detection method",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032342067&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.341&partnerID=40&md5=938be574f9a0d14ba7988c18e2a65a73","Software vulnerability detecting is an important way of discovering the existing loopholes in software in order to ensure the information security. With the rapid development of the information technology in our society, a large variety of application software with various potentially vulnerabilities has emerged. Therefore, a timely discovery and repair of these loopholes before they are exploited by attackers can effectively reduce the threat in the information system. It is of great significance for us to take the initiative to explore and analyze the system security loopholes, so that the danger or threat to the system will be effectively reduced. From the previous research on the software vulnerability detection we have found that each of the existing vulnerability detection methods or tools can only perform well in some particular occasions. In order to overcome such shortcoming and improve these existing detection methods, we present a more accurate and complete analysis of current mainstream detection methods as well as design a set of evaluation criteria for different detection methods in this paper. Meanwhile, we also propose and design an integrated test framework, on which we can test the typical static analysis methods and dynamic mining methods as well as make the comparison, so that we can obtain an intuitive comparative analysis of the results. Finally, we report the experimental analysis to verify the feasibility and effectiveness of the proposed evaluation method and the testing framework, with the results showing that the final test results will serve as a form of guidance to aid the selection of the most appropriate and effective method or tools in vulnerability detection activity. © 2017 IEEE.","Evaluation method; Information security; Loopholes; Software vulnerability detection; Test platform","Application programs; Big data; Data communication systems; Data privacy; Embedded software; Embedded systems; Integration testing; Mining; Security of data; Static analysis; Comparative analysis; Evaluation method; Experimental analysis; Loopholes; Software vulnerabilities; Static analysis method; Test platforms; Vulnerability detection; Software testing",2-s2.0-85032342067
"Wang W., Li C., Wang S.","Big data used in energy efficiency and load forecasting of heating boilers",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032186974&doi=10.23919%2fChiCC.2017.8028253&partnerID=40&md5=7aa71ce2a351c782ede1e14b34b0b16b","Boilers are large energy conversion equipment in the Heating industry, it has the characteristics of extensive management, higher energy consumption and so on. There is the lack of effective 'handle' and difficult to carry out targeted operational guidance in the actual operation management. This paper proposed methods of operation optimization and load forecasting based on big data thinking. The performance management system of boiler is big data platform, using clustering theory by big data to analysis data for the boilers, to find main factors affecting efficiency of boilers, and operational guidance and optimization. Meanwhile, the theory of information mining is used to forecasting load of boilers. Obtain remarkable benefit of energy saving 9%, and to achieve the boiler load guidance, the error of 1%. © 2017 Technical Committee on Control Theory, CAA.","Big Data; Heating Boilers; Incidence Relation; Load Forecasting",,2-s2.0-85032186974
"Tian F., Shen X., Liu X.","Multimedia automatic annotation by mining label set correlation",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028974754&doi=10.1007%2fs11042-017-5170-3&partnerID=40&md5=adf27cd24106b00d2c9dabc60e050f83","Organizing and retrieving multimedia data heavily rely on the relevant textual descriptions. Multimedia automatic annotation, which assigns text labels to multimedia samples, has been widely studied. Among others, search-based annotation methods are well suited for annotation tasks on large-scale datasets and are studied in depth because of their simplicity and scalability. However, classical search based annotation methods address this problem by treating each label independently, which ignores the correlation between different labels in the assigned label set. This paper aims to integrate the relevant information of the label set with respect to the multimedia content and the inner correlated information of the label set into a joint learning framework. We evaluate the performance of the proposed method on MIRFLICKR-25000 and NUS-WIDE datasets. Experimental results show that the proposed annotation method achieves excellent performance. © 2017 Springer Science+Business Media, LLC","Automatic annotation; Label set correlation mining; Multimedia annotation","Hardware; Annotation methods; Automatic annotation; Large-scale datasets; Multimedia annotations; Multimedia contents; Multimedia samples; Set correlation; Textual description; Multimedia systems",2-s2.0-85028974754
"Wang S., Moise I., Helbing D., Terano T.","Early Signals of Trending Rumor Event in Streaming Social Media",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032876946&doi=10.1109%2fCOMPSAC.2017.115&partnerID=40&md5=3340e176487069c273ffdbb237c703ba","In this study, we propose a mechanism for identifying early signals of trending rumor events (i.e. controversial emerging topics) in streaming social media. The pattern, combining features of both user's attitude and information diffusion, is applied in the sliding windows of social media data streams. By capturing and analyzing frequent patterns within early windows, we found signal patterns appearing at very early stages of trending rumor events (in average, months before their peak time). Our preliminary empirical analysis is applied in two different Twitter datasets. The obtained results indicate the potential of our approach to detect trending rumor event candidates (with high probability of being false) as early as possible in real-time environments. © 2017 IEEE.","data stream; early signals; frequent pattern mining; rumor detection; social media","Application programs; Computer software; Social networking (online); Data stream; Empirical analysis; Frequent pattern mining; High probability; Information diffusion; Real-time environment; Social media; Social media datum; Media streaming",2-s2.0-85032876946
"Schultze U., Mason R.O.","Managing the risks of big data at MyTelco: taking ethics seriously",2017,"Journal of Information Technology Teaching Cases",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028808775&doi=10.1057%2fs41266-017-0025-3&partnerID=40&md5=4b887dc36dc33c7af118358f87cf2d40","In 2014, MyTelco established an independent Business Analytics (MyBA) organization that provided data analytics services to both MyTelco and third party customers. It was expected that this organization would be able to fund itself through the revenues generated from the commercialization of insights derived from MyTelco’s data. To address the ethical risks associated with mining and commercializing big data, MyBA relied on an Ethics and Compliance Committee (ECC) to review project proposals that involved customer data. Situated in November 2016, a week after the Federal Communications Commission (FCC) passed new legislation to increase consumer’s choice over their personal information, the case describes three proposals that the ECC needed to evaluate. Students deliberate each proposal’s legality and ethicality from the point of view of MyTelco’s Ethics and Compliance Committee. © 2017 Association for Information Technology Trust","Business analytics; Ethical decision making; Privacy; Surveillance; Telecom industry",,2-s2.0-85028808775
"Ye A., Li Y., Xu L., Li Q., Lin H.","A trajectory privacy-preserving algorithm based on road networks in continuous location-based services",2017,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032383538&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.278&partnerID=40&md5=31f16ccfbb1caaf93ce150912fff425d","A major concern of the large-scale deployment of location based services (LBSs) is the safeguards of the user's location data collected by service providers, since person's location information may imply sensitive private information. Most existing techniques have addressed privacy protection mainly for snapshot queries. However, providing anonymity for continuous queries is important, since users' privacy information such as habits and customs is easy to be inferred by observing and mining a time-series sequence of query. In this paper, we present a novel l-diversity algorithm based on road networks for trajectory privacy protection, which preprocesses a set of similar trajectories to blur the actual trajectory of a user. Furthermore, we depersonalize a user's trajectory by ensuring that each location reported to LBS server is a cloaking region that contains other l-1 different trajectories, which are generated in advance from road maps. Therefore, both identity and location of the user remain anonymous from service providers in continuous queries. We evaluate our techniques under various conditions using location data synthetically generated based on real road maps, and the results show that our techniques can provide trajectory l-diversity protection using a minimum cloaking region. © 2017 IEEE.","Continuous location-based services; Security and privacy; Trajectory l-diversity","Big data; Data communication systems; Data privacy; Embedded software; Embedded systems; Location; Mobile telecommunication systems; Roads and streets; Telecommunication services; Trajectories; Transfer cases (vehicles); Transportation; Continuous locations; L diversities; Large-scale deployment; Location information; Privacy information; Privacy preserving; Private information; Security and privacy; Location based services",2-s2.0-85032383538
"Yang Z., Liu X., Mu Y., Liu X.","A parallel fuzzy rule discovery algorithm and future goods automated trading system",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032171912&doi=10.23919%2fChiCC.2017.8028256&partnerID=40&md5=90ba1b8f4200bc46d718275b60437e40","Nowadays, with the rapid development of information technology, many business fields gradually usher in the era of big data. Extracting valuable information from the large data is very urgent. However, because of some limitations such as memory restrictions, data complexity or time complexity, the majority of traditional data mining methods are time consuming and low efficiency working on big data. In contrast, parallel computing is a common and reliable choice to solve that problem. In this paper, we propose a fuzzy rule discovery algorithm - fuzzy rule based classification system in the framework of MapReduce (FRBCS-MR). The proposed FRBCS-MR algorithm applies parallel computing technology to extract fuzzy rules and build fuzzy rule-base, which combines the advantage of dealing with uncertainty of fuzzy system and the ability of MapReduce in parallel computing. Future goods trading has been gradually transformed from the original manual trading to stylized trading and the stylized trading is considered to be the forefront and the most scientific mode of investment. In the experimental studies, the rules extracted by FRBCS-MR algorithm using future goods data are turned into trading strategies which are to be used in the real automated trading platform TradeBlazer (TB) to compute the profit and loss situation. The result shows that the FRBCS-MR algorithm has good profit ability and possesses a certain guiding significance for investors. © 2017 Technical Committee on Control Theory, CAA.","Big data; Future goods data; Fuzzy rule based classification system; MapReduce",,2-s2.0-85032171912
"Volk M., Shareef A.E., Jamous N., Turowski K.","New E-Commerce User Interest Patterns",2017,"Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032331123&doi=10.1109%2fBigDataCongress.2017.60&partnerID=40&md5=5efcf371325bbaf9571143402f6bb91d","The number of online purchases is increasing constantly. Companies have recognized the related opportunities and they are using online channels progressively. In order to acquire potential customers, companies often try to gain a better understanding through the use of web analytics. One of the most useful sources are web log files. Basically, these provide an abundance of important information about the user behavior on a website, such as the path or access time. Mining this so-called clickstream data in the most comprehensive way has become an important task in order to predict the behavior of online customers, optimize webpages, and give personalized recommendations. As the number of customers constantly rises, the volume of the generated data log files also increases, both in terms of size and quantity. Thus, for certain companies, the currently used technologies are no longer sufficient. In this work, a comprehensive workflow will be proposed using a clustering algorithm in a Hadoop ecosystem to investigate user interest patterns. The complete workflow will be demonstrated on an application scenario of one of the largest business-to-business (B2B) electronic commerce websites in Germany. Furthermore, an experimental evaluation method will be applied to verify the applicability and efficiency of the used algorithm, along with the associated framework. © 2017 IEEE.","big data; clickstream data; clustering algorithm; hadoop ecosystem","Behavioral research; Big data; Commerce; Ecology; Ecosystems; Electronic commerce; Sales; Websites; Application scenario; Business to business; Clickstream data; Experimental evaluation; Online channels; Online customers; Personalized recommendation; Potential customers; Clustering algorithms",2-s2.0-85032331123
"Shibahara T., Takata Y., Akiyama M., Yagi T., Yada T.","Detecting Malicious Websites by Integrating Malicious, Benign, and Compromised Redirection Subgraph Similarities",2017,"Proceedings - International Computer Software and Applications Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031903476&doi=10.1109%2fCOMPSAC.2017.105&partnerID=40&md5=d43df156ce33d4ec2d29dbb1c9f69b91","To expose more users to threats of drive-by download attacks, attackers compromise vulnerable websites discovered by search engines and redirect clients to malicious websites created with exploit kits. Security researchers and vendors have tried to prevent the attacks by detecting malicious data, i.e., malicious URLs, web content, and redirections. However, attackers conceal a part of malicious data with evasion techniques to circumvent detection systems. In this paper, we propose a system for detecting malicious websites without collecting all malicious data. Even if we cannot observe a part of malicious data, we can always observe compromised websites. Since vulnerable websites are discovered by search engines, compromised websites have similar traits. Therefore, we built a classifier by leveraging not only malicious websites but also compromised websites. More precisely, we convert all websites observed at the time of access into a redirection graph and classify it by integrating similarities between its subgraphs and redirection subgraphs shared across malicious, benign, and compromised websites. As a result of evaluating our system with crawling data of 455,860 websites, we found that the system achieved a 91.7% true positive rate for malicious websites containing exploit URLs at a low false positive rate of 0.1%. Moreover, it detected 143 more evasive malicious websites than conventional systems. © 2017 IEEE.","drive-by download attacks; evasion techniques; graph mining","Application programs; Computer software; Digital storage; Search engines; Conventional systems; Detection system; Drive-by-download; evasion techniques; False positive rates; Graph mining; True positive rates; Web content; Websites",2-s2.0-85031903476
"Zhang S., Huang C., Fan Z., Liu M.","Silk design expert system using association rules extracted from silk relics",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032215173&doi=10.23919%2fChiCC.2017.8028969&partnerID=40&md5=c1778ca73629e9b8c70be257fff03c2e","The traditional silk pattern design process depends largely on designer's knowledge and experience which makes the new creative design very difficult to produce. The research applies expert system to the silk design field and uses association data mining to extract association rules from ancient silk relics, the expert system recommends users with appropriate design through description rules and association rules, it helps bring the information of ancient silk relics to the modern design, and solve the problem of creativity generation. © 2017 Technical Committee on Control Theory, CAA.","Association rule; Expert system; Silk relics",,2-s2.0-85032215173
"Liu J.-W., Li S.-C., Cui L.-P., Luo X.-L.","Simultaneous classification and feature selection via LOG SVM and Elastic LOG SVM",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032171709&doi=10.23919%2fChiCC.2017.8029116&partnerID=40&md5=cef35fcb408e30678686a2bbcf8e4faf","In data mining and machine learning, classifying the class labels and selecting features simultaneously are important. This study proposes two new sparse support vector machines (SVMs), namely, LOG SVM and Elastic LOG SVM. The LOG SVM uses the LOG penalty, and the Elastic LOG SVM combines the non-convex LOG penalty and the L2 norm penalty. The LOG SVM and Elastic LOG SVM can achieve classification and feature selection simultaneously. Local quadratic approximation is used to solve both SVMs. Experiments are also conducted to show that the proposed SVMs perform well in the aspects of classification and feature selection. © 2017 Technical Committee on Control Theory, CAA.","Classification; Elastic LOG SVM; Feature selection; LOG SVM; Non-convex LOG penalty; Sparse",,2-s2.0-85032171709
"Shi Y., Du S.","Manifold regularized robust unsupervised feature selection for image clustering",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032172374&doi=10.23919%2fChiCC.2017.8029138&partnerID=40&md5=1e38976319deabab84caa8bfdead594e","Dimensionality reduction is a challenging task for high dimensional data processing in machine learning and data mining. As an effective dimension reduction technique, unsupervised feature selection aims at finding a subset of features to retain the most relevant information. In this paper, we propose a novel unsupervised feature selection method, called Manifold Regularized Robust Unsupervised Feature Selection (MRUFS) for image clustering. MRUFS performs robust discriminative feature selection and robust clustering simultaneously under ℓ-2,1-norm while preserves the local manifold structures of original data. Compared with several unsupervised feature selection methods, the proposed algorithm comes with better clustering performance for two datasets: FERET and COIL20 which we have experimented with here. © 2017 Technical Committee on Control Theory, CAA.","dimensionality reduction; manifold regularization; matrix factorization; unsupervised feature selection",,2-s2.0-85032172374
"Li J., Jing J., Cao Y., Xiao H.","Weighted least squares twin support vector machine for regression with noise",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032177700&doi=10.23919%2fChiCC.2017.8028934&partnerID=40&md5=9bdb5517da06a14a4760f60b68c9a9b4","Twin support vector regression and its extensions have been widely applied in machine learning and data mining. However, most of them can not achieve the satisfactory performances when the noise is involved. To this end, this paper presents a weighted least squares twin support vector regression (WLSTSVR) which can reduce the influence of the noise on prediction accuracy by using the information of the responses of the samples. Furthermore, both offline and online learning algorithms are developed. The experimental results on artificial and benchmark datasets with noise indicate that the both online and offline learning WLSTSVR achieve better prediction accuracy and agreement between estimations and real-values compared with least squares twin support vector regression. © 2017 Technical Committee on Control Theory, CAA.","regression with noise; Twin support vector regression; weighted mechanism",,2-s2.0-85032177700
"Zhong Y., Jianhua Z.","Subject-generic EEG feature selection for emotion classification via transfer recursive feature elimination",2017,"Chinese Control Conference, CCC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032175359&doi=10.23919%2fChiCC.2017.8029114&partnerID=40&md5=86b58034e556b4c4825eb59242572de3","The machine-learning based data-mining approaches become increasingly attractive in EEG-data-based human emotion recognition since the physiological data possesses the objectivity. In particular, those learning principles can effectively model emotion classifiers via heterogeneous features. By exploring the exciting literature, the subject-specific emotion estimator has a significant disadvantage, i.e., it induces additional burdens to a single user because of the preparation of long-time, multiple-session EEG data as reliable training sets. In this paper, we propose a new subject generic EEG feature selection method called transfer recursive feature elimination (T-RFE), to find the optimal feature subset that consists of the most robust EEG markers of stable distributions among multiple training subjects and a single testing subject. We adopt the DEAP database to validate the effectiveness of the T-RFE algorithm. The subject-generic emotion classification paradigm is also constructed and investigated. By implementing a linear least square support vector machine model, the T-RFE performance is compared with several conventional feature selection methods. We also found the statistical significance in the improvement of the classification accuracy. The classification rate and F-score outperform several recent reported works on the same database. © 2017 Technical Committee on Control Theory, CAA.","affective computing; EEG; Emotion recognition; physiological signals; recursive feature elimination",,2-s2.0-85032175359
"Zhang F., Tang H., Jiang Y., Mao Z.","The transcription factor GATA3 is required for homologous recombination repair by regulating CtIP expression",2017,"Oncogene",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029186792&doi=10.1038%2fonc.2017.127&partnerID=40&md5=fcd8e72f30da298d327367a893b217b3","GATA3, a critical transcription factor involved in the development of the mammary gland, also plays important roles in mammary tumorigenesis by regulating transcription in coordination with two essential DNA repair factors, PARP1 and BRCA1. However, whether and how GATA3 participates in the process of DNA repair, which is often associated with tumorigenesis, has not been investigated. Here we demonstrate that GATA3 is required for the repair of DNA double-strand breaks (DSBs) by homologous recominbation (HR). Mechanistic studies indicate that at both the protein and the mRNA level, depleting GATA3 leads to reduced expression of CtIP, an essential HR factor involved in end resection, thereby suppressing the repair of DSBs by HR and sensitizing cells to etoposide induced DNA DSBs. Further studies indicate that upon the occurrence of DNA DSBs GATA3 directly binds to the CtIP promoter at the region of -2119 to -2130 and -2274 to -2285, and promotes the transcription of CtIP. Overexpression of CtIP in GATA3 depleted cells rescues the decline of HR, and cell survival in the presence of etoposide. In addition, through data mining analysis, we observed an extremely strong correlation between the expression levels of GATA3 and CtIP in paratumors, but the correlation turned insignificant in mammary tumors. Using vectors encoding GATA3 with mutations frequently occurring in mammary tumors, we found that several mutations on GATA3 led to a dysregulation of CtIP, and therefore HR repair. In summary, our data delineates the regulatory mechanisms of GATA3 in DNA DSB repair and strongly suggests that it might act as a tumor suppressor by promoting CtIP expression and HR to stabilize genomes. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,"etoposide; messenger RNA; protein CtIP; protein derivative; transcription factor GATA 3; unclassified drug; carrier protein; DNA binding protein; GATA3 protein, human; nuclear protein; RBBP8 protein, human; transcription factor GATA 3; Article; breast tumor; cell survival; controlled study; correlation analysis; DNA strand breakage; gene overexpression; genetic transcription; genomic instability; human; human cell; priority journal; promoter region; protein binding; protein function; recombination repair; regulatory mechanism; double stranded DNA break; gene expression regulation; genetics; homologous recombination; MCF-7 cell line; metabolism; physiology; recombination repair; Carrier Proteins; DNA Breaks, Double-Stranded; DNA-Binding Proteins; GATA3 Transcription Factor; Gene Expression Regulation, Neoplastic; Genomic Instability; Homologous Recombination; Humans; MCF-7 Cells; Nuclear Proteins; Recombinational DNA Repair",2-s2.0-85029186792
"Bravo S., García-Ordiales E., García-Navarro F.J., Amorós J.Á., Pérez-de-los-Reyes C., Jiménez-Ballesta R., Esbrí J.M., García-Noguero E.M., Higueras P.","Geochemical distribution of major and trace elements in agricultural soils of Castilla-La Mancha (central Spain): finding criteria for baselines and delimiting regional anomalies",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028995669&doi=10.1007%2fs11356-017-0010-6&partnerID=40&md5=c69cc39d0afd334fa66c24cdda873cc0","Castilla-La Mancha (central Spain) is a region characterized by significant agricultural production aimed at high-quality food products such as wine and olive oil. The quality of agricultural products depends directly on the soil quality. Soil geochemistry, including dispersion maps and the recognition of baselines and anomalies of various origins, is the most important tool to assess soil quality. With this objective, 200 soil samples were taken from agricultural areas distributed among the different geological domains present in the region. Analysis of these samples included evaluation of edaphological parameters (reactivity, electrical conductivity, organic matter content) and the geochemistry of major and trace elements by X-ray fluorescence. The dataset obtained was statistically analyzed for major elements and, in the case of trace elements, was normalized with respect to Al and analyzed using the relative cumulative frequency (RCF) distribution method. Furthermore, the geographic distribution of analytical data was characterized and analyzed using the kriging technique, with a correspondence found between major and trace elements in the different geologic domains of the region as well as with the most important mining areas. The results show an influence of the clay fraction present in the soil, which acts as a repository for trace elements. On the basis of the results, of the possible elements related with clay that could be used for normalization, Al was selected as the most suitable, followed by Fe, Mn, and Ti. Reference values estimated using this methodology were lower than those estimated in previous studies. © 2017 Springer-Verlag GmbH Germany","Agricultural soils; Geochemistry; Pedogeochemical maps; Physico-chemical properties",,2-s2.0-85028995669
"Duan X., Zhang J., Bao Q., Ramachandran R., Lee T.J., Lee S., Pan L.","Linking Design-Time and Run-Time: A Graph-Based Uniform Workflow Provenance Model",2017,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032337757&doi=10.1109%2fICWS.2017.21&partnerID=40&md5=44dd51b797da6e1e77e23cfebdfbae0d","Workflow is an important way to mashup reusable software services to create value-added data analytics services. Workflow provenance is core to understand how services and workflows behaved in the past, which knowledge can be used to provide a better recommendation. Existing workflow provenance management systems handle various types of provenance separately. A typical data science exploration scenario, however, calls for an integrated view of provenance and seamless transition among different types of provenance. In this paper, a graph-based, uniform provenance model is proposed to link together design-time and run-time provenance, by combining retrospective provenance, prospective provenance, and evolution provenance. Such a unified provenance model will not only facilitate workflow mining and exploration, but also facilitate workflow interoperability. The model is formalized into colored Petri nets for verification and monitoring management. A SQL-like query language is developed, which supports basic queries, recursive queries, and cross-provenance queries. To verify the effectiveness of our model, A web-based, collaborative workflow prototyping system is developed as a proof-of-concept. Experiments have been conducted to evaluate the effectiveness of the proposed SQL-like graph query against SQL query. © 2017 IEEE.","design-time provenance; integrated provenance model; run-time provenance; scientific workflow","Computer software reusability; Graphic methods; Petri nets; Query languages; Websites; Collaborative workflow; Design time; Provenance models; Runtimes; Science exploration; Scientific workflows; Seamless transition; Workflow provenances; Web services",2-s2.0-85032337757
"Sun H., Ji G., Zhao B., Liu X.","A Parallel Algorithm for Mining Time Relaxed Gradual Clustering Pattern Based on Spatio-Temporal Trajectories",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031744668&doi=10.1109%2fCBD.2017.60&partnerID=40&md5=0295e521e791e1e4a7daa30e80be2531","As an important area of spatio-Temporal data mining, time relaxed gradual clustering pattern has been attracting broad attention in recent years. Algorithm PTRGSP is proposed to mining time relaxed gradual clustering pattern from spatio-Temporal trajectory, which is implemented under spark framework. All trajectories are divided into point sets with different timestamps, and such point sets are parallel clustered. After that, clusters of different time intervals are joined in parallel to achieve pattern candidates. Candidates are combined to obtain interesting maximal time relaxed gradual clustering pattern. To improve the efficiency of the algorithm PTRGSP, algorithm PTRGSP-G is presented based on grid index for mining time relaxed gradual clustering pattern. The experiment results on real dataset and synthetic trajectory dataset demonstrate the effectiveness and efficiency of the two algorithms. © 2017 IEEE.","Parallel data mining; Spatio-Temporal trajectory; Time relaxed gradual clustering pattern","Big data; Data mining; Efficiency; Geometry; Trajectories; Effectiveness and efficiencies; Parallel data mining; Point set; Spatio-temporal data mining; Spatio-temporal trajectories; Time interval; Time relaxed gradual clustering pattern; Time stamps; Clustering algorithms",2-s2.0-85031744668
"Hao Q., Li Y., Wang L.M., Wang M.","An Ontology-Based Data Organization Method",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031737621&doi=10.1109%2fCBD.2017.31&partnerID=40&md5=483cb32989a5fc4c149ef93ed5098dbd","Data open sharing becomes the basis for mining the potential value of big data. Due to the different structure of different shared data sources, the upper layer application is restricted. Therefore, how to establish a unified and user friendly logical organization form is an urgent problem to be solved in the open sharing of data. This paper presents a method of data organization based on ontology knowledge base. Firstly, we design rules to extract the concepts and relationships in the original data set. Then we implement three standardized human-computer interaction interfaces to perfect the ontology model. Finally, according to the ontology knowledge base model, original data set is reorganized, which makes the logic structure of the new data set more suitable to the upper level application, and improves the utilization ratio of the open data. © 2017 IEEE.","Data Model; Data Open; Knowledge Base; Ontology","Data mining; Data structures; Human computer interaction; Knowledge based systems; Ontology; Data Open; Data organization; Data organization method; Different structure; Human computer interaction interface; Knowledge base; Knowledge base modeling; Utilization ratios; Big data",2-s2.0-85031737621
"Li X., Chen J., Lin Z., Zhang L., Wang Z., Zhou M., Xie W.","A Mining Approach to Obtain the Software Vulnerability Characteristics",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031724904&doi=10.1109%2fCBD.2017.58&partnerID=40&md5=8638642cf827628385c6ae76a3b7a1a1","Software vulnerability remains a serious challenge to the software engineering domain over the past decade as a result of the recent technological advancement in information systems. The rapid development in software applications and failure on the part of system developers to properly analyze program codes before been released to the market increases the chance for data breaches. It is a known fact that most system failures are as a result of bugs and errors detected in software applications. Although code errors significantly affect software quality, there is no effective method that can be used in eliminating software errors to improve its reliability. Data Mining and its related algorithms are an active area which can successfully be applied in analyzing software vulnerability. However the concept of applying data mining techniques has not been empirically proven as an effective method for obtaining the essential characteristics of software vulnerability. To investigate this effect, we propose a vulnerability mining algorithm to analyze and obtain the essential characteristics of Software vulnerability based data mining techniques. We first extracted and preprocessed the software vulnerabilities using data mining techniques and common vulnerability database. We evaluate the proposed technique using the Common Vulnerability and Exposure (CVE) Database, Common Weakness Enumeration (CWE) Database, National Vulnerability Database (NVD) datasets. Empirical results show that the proposed vulnerability mining algorithm has a remarkable improvement in the vulnerability mining process. The most interesting finding is that, we observed that across all the three projects, recall was around 70% and precision was approximately 60%. © 2017 IEEE.","Data mining; Essential characteristics; Extraction algorithm; Information security; Software vulnerability","Application programs; Big data; Computer software selection and evaluation; Database systems; Errors; Program debugging; Security of data; Software engineering; Software reliability; Systems engineering; Essential characteristic; Extraction algorithms; National vulnerability database; Software engineering domain; Software vulnerabilities; Software vulnerability characteristics; Technological advancement; Vulnerability database; Data mining",2-s2.0-85031724904
"Jiang H., He X.","An Improved Algorithm for Frequent Itemsets Mining",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760031&doi=10.1109%2fCBD.2017.61&partnerID=40&md5=9f75f42e046f0f51b35966f998988845","Based on the classical FP-growth algorithm about frequent itemsets mining, this paper proposes a more efficient non-recursive FPNR-growth algorithm and corresponding data structure. The experimental results show that the FPNR-growth algorithm is superior to the FP-growth algorithm, both in mining time and in storage space. © 2017 IEEE.","Data mining; FP-growth; Frequent itemsets","Data mining; Digital storage; FP growths; FP-growth algorithm; Frequent itemsets minings; Growth algorithms; Item sets; Storage spaces; Big data",2-s2.0-85031760031
"Dong L., Liu G.","Finding Compositional Skyline Based on Sharing Strategy",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031754865&doi=10.1109%2fCBD.2017.36&partnerID=40&md5=514fadb0ce5003c1ac2c6710271cfb20","Traditional skyline queries are mainly to return the best points from a large data set. However, it only identifies the individual points. In this paper, we perfectly present the concept of compositional skyline(C-Skyline), which returns the best results from all the compositions with k points. It is more useful in big data mining and cloud computing. In order to compute C-Skyline efficiently, we present two sharing strategies which show how to find candidate compositions, and we build a dominance graph to reflect dominance relations among the points in adjacent levels. Then we propose two heuristic algorithms to query C-Skyline compositions: The ordinary algorithm and the improved algorithm. Based on the synthetic dataset and the real NBA dataset, the experiments manifest the abundant information of C-Skyline, and the efficiency of our algorithms. © 2017 IEEE.","Big data mining; Compositional skyline; Dominance graph","Data mining; Heuristic algorithms; Compositional skyline; Dominance graph; Dominance relation; K points; Large datasets; Sharing strategies; Skyline query; Big data",2-s2.0-85031754865
"Lv L., Zhao J.","The Firefly Algorithm with Gaussian Disturbance and Local Search",2017,"Journal of Signal Processing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028856805&doi=10.1007%2fs11265-017-1278-y&partnerID=40&md5=26a7a9e617f373d56a3fca2d1eac509f","Along with the rapid development of mobile Internet, Internet of things and cloud computing technology, the data volume has shown an explosive growth in different industries. Big data technology, which provides new solutions to data-related problems, draws an increasing attention, especially in the field of artificial intelligence. Swarm intelligence is an important tool for solving complex problems in both scientific research and engineering practice. Representing a major development trend in artificial intelligence and information science, swarm intelligence has displayed great application potentials in big data analysis and data mining. Firefly algorithm (FA), an optimization technique based on swarm intelligence, has been successfully applied to a diversity of complex engineering optimization problems. In a standard FA, particles migrate blindly towards those better ones, without considering the status of the object of learning. However, this type of particle regeneration may result in a solution being trapped into local optima, with fast convergence speed but low convergence precision. We propose an FA with Gaussian disturbance and local search. The swarm is updated using random attraction model. The current position of the particle is compared with particle’s historical optimal position. If the current position is inferior to the historical optimal position, the particle is updated by Gaussian disturbance and local search strategy. The optimal particle will be selected for the next round of learning. This method not only enhances population diversity, but also increases optimizing precision. Simulations were performed on 12 benchmark functions under the same parameters. The results indicate that the optimizing performance of the proposed algorithm is superior to the other 5 recently provided FA methods. Local search strategy, as compared with random attraction model and Gaussian disturbance, can dramatically improve the optimizing performance. © 2017 Springer Science+Business Media, LLC","Firefly algorithm; Gaussian disturbance; Local search; Random model","Artificial intelligence; Big data; Bioluminescence; Data mining; Gaussian distribution; Local search (optimization); Problem solving; Swarm intelligence; Cloud computing technologies; Fast convergence speed; Firefly algorithms; Gaussian disturbances; Local search; Optimization techniques; Optimizing performance; Random Model; Optimization",2-s2.0-85028856805
"Zhang L., Yuan Y., Wu Z., Cao J.","Semi-SGD: Semi-Supervised Learning Based Spammer Group Detection in Product Reviews",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031736169&doi=10.1109%2fCBD.2017.70&partnerID=40&md5=601dcf69aa44cb3d99d1a70b4f058363","The purchase decision of customers in e-commerce platforms is strongly influenced by product ratings and reviews. Driven by the profits, review spammers post fake reviews to promote their products or demote their competitors' products. Differ from individual spammers, the spammer groups manipulate reviews together and can be more damaging. Existing work for spammer group detection extract candidate groups from review data and identify the spammer groups using unsupervised spamicity ranking methods. However, the labeled and unlabeled data are existing simultaneously in practice and no method makes good use of both these data in spammer group detection. In this paper, we propose a semi-supervised learning based spammer group detection method (Semi-SGD), which trains a Naive Bayes classifier on a small set of labeled data as an initial classifier, and then incorporates unlabeled data with Expectation Maximization (EM) algorithm to improve the initial classifier iteratively. Experiments on Amazon.cn datasets show that our proposed Semi-SGD is efficient and effective. © 2017 IEEE.","Amazon.cn; EM Algorithm; Naive Bayes Classifier; Semi-supervised Learning; Spammer Group Detection","Classification (of information); Classifiers; Data mining; Iterative methods; Learning algorithms; Learning systems; Maximum principle; Supervised learning; Amazon.cn; EM algorithms; Group detection; Naive Bayes classifiers; Semi- supervised learning; Big data",2-s2.0-85031736169
"Song Y., Gu K., Li H., Sun G.","A Lexical Updating Algorithm for Sentiment Analysis on Chinese Movie Reviews",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031745039&doi=10.1109%2fCBD.2017.40&partnerID=40&md5=3b12d3433784ed79ba0bd51efe47b53a","With the prevalence of Internet, sentiment analysis gets popularity among the world. Researchers have made use of kinds of online documents like commodities reivews and movie reviews as training samples to train their models and classfiers, by which they could speculate the underlying emotion in new ones. Douban is a Chinese online community where users share their personal reviews to express their feelings about movies. Those Chinese movie reviews were utilized by us to train our lexicon-based model. Yet multiple words in a ready-made lexicon do not agree with the movie reviews in a specific domain, which means the original lexicon acquires being updated to gain higher accuracy. In this paper we introduce a lexical updating algorithm based on a widely used lexicon. After turns of training of updating, this lexicon is capable of classifying sentiment among movie reviews. The experimental result shows our model using the updated lexicon could get a better performance than the primitive lexicon-based model. © 2017 IEEE.","Lexical updating algorithm; Movie review; Sentiment Analysis","Data mining; Motion pictures; Lexicon-based; Movie reviews; On-line communities; On-line documents; Sentiment analysis; Training sample; Updating algorithm; Big data",2-s2.0-85031745039
"Liang W., Zhou X., Huang S., Hu C., Jin Q.","Recommendation for Cross-Disciplinary Collaboration Based on Potential Research Field Discovery",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031713986&doi=10.1109%2fCBD.2017.67&partnerID=40&md5=f72a1381d69359885230f8fafa581e97","In recent years, cross-disciplinary scientific collaboration has been proved to be promising for both research practice and innovation. Lots of efforts have been spent in collaboration recommendation. However, the cross-disciplinary information is hidden in tons of publications, and the relationships between different fields are complicated, which make it challengeable recommending cross-disciplinary collaboration for a specific researcher. In this paper, a novel cross-disciplinary collaboration recommendation method (CDCR) that unearths the common cross-disciplinary collaboration patterns and historical scientific field preferences of authors is proposed to recommend potential cross-disciplinary research collaboration. In CDCR, a research field discovery algorithm is designed to classify scientific topics obtained from the publications into the correct field automatically. Then, the collaborative patterns are studied through analyzing the composition fields and the corresponding percentage of all publications. Furthermore, we investigate the common correlation of different research fields. Based on the common correlation and the researcher's specific pattern, the most valuable fields will be listed by CDCR. The effectiveness of our approach is evaluated based on a real academic dataset. © 2017 IEEE.","Cross-disciplinary; Data mining; Research field discovery; Scientific collaboration","Big data; Collaborative patterns; Cross-disciplinary; Cross-disciplinary collaborations; Cross-disciplinary research; Practice and innovations; Recommendation methods; Research fields; Scientific collaboration; Data mining",2-s2.0-85031713986
"Rosero R.H., Gomez O.S., Rodriguez G.","Regression Testing of Database Applications under an Incremental Software Development Setting",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029181331&doi=10.1109%2fACCESS.2017.2749502&partnerID=40&md5=693f481591754bedd7a7a5adf62c2562","Software regression testing verifies previous features on a software product when it is modified or new features are added to it. Because of the nature of regression testing it is a costly process. Different approaches have been proposed to reduce the costs of this activity, among which are: Minimization, prioritization, and selection of test cases. Recently, soft computing techniques, such as data mining, machine learning, and others have been used to make regression testing more efficient and effective. Currently, in different contexts, to a greater or lesser extent, software products have access to databases (DBs). Given this situation, it is necessary to consider regression testing also for software products such as information systems that are usually integrated with or connected to DBs. In this paper, we present a selection regression testing approach that utilizes a combination of unsupervised clustering with random values, unit tests, and the DB schema to determine the test cases related to modifications or new features added to software products connected to DBs. Our proposed approach is empirically evaluated with two database software applications in a production context. Effectiveness metrics, such as test suite reduction, fault detection capability, recall, precision, and the F-measure are examined. Our results suggest that the proposed approach is enough effective with the resulting clusters of test cases. © 2013 IEEE.","fault detection capability; software engineering; Software regression testing; software verification; test suite reduction","Application programs; Data mining; Database systems; Fault detection; Learning systems; Reduction; Regression analysis; Soft computing; Software design; Software engineering; Verification; Detection capability; Effectiveness metrics; Incremental software development; Regression testing; Softcomputing techniques; Software verification; Test suite reduction; Unsupervised clustering; Software testing",2-s2.0-85029181331
"Xie H., Wang F.L., Mao X., Li K., Li Q., Wang H.","Recent advances in semantic computing and personalization",2017,"Neurocomputing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015309269&doi=10.1016%2fj.neucom.2017.02.073&partnerID=40&md5=e5df9cfd269c4016e0e01879aa2ed792",[No abstract available],,"algorithm; classifier; data mining; Editorial; electronic commerce; information system; Internet; kernel method; mathematical computing; prediction; priority journal; semantics; social media; social network; support vector machine; virtual reality",2-s2.0-85015309269
"Jing D., Zhu H.","VOut: Mining Sampled GPS Trace for Taximeter Fraud",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031763669&doi=10.1109%2fCBD.2017.52&partnerID=40&md5=7e707b7471c72a74bd9cc1d7d105bb28","Nowadays Taxies play an important role in urban transportation. One practical issue in taxi operation is charging fraud, where the taximeter of a malicious taxi is altered to fake the real travel distance. One labor-intensive scheme to detect fraudulent taxies is to check the taximeters of a great number of randomly selected taxies or those taxies been accused of cheating. Alternatively, with GPS (Global Positioning System) receiver onboard, another potential scheme is to calculate the travel distance of a taxi according to the GPS reports and a digital map. However, it is hard to retrieve accurate trajectories of taxies due to the sparseness of GPS reports, outlier reports, and the distortion of the digital map. In this paper, we propose a speed-based scheme, called VOut, which relies on one key insight that the average speed derived from a faked travel distance should be higher than the majority of GPS velocity reports. Given the operation records and GPS reports of taxies, VOut judges the suspicion of each delivery task for each taxi. We conduct extensive trace-driven simulations and the results demonstrate the efficacy of VOut. © 2017 IEEE.","Data analysis; Operation records; Sparse GPS reports; Taximeter fraud","Crime; Data reduction; Global positioning system; Taxi meters; Taxicabs; Urban transportation; Gps (global positioning system); GPS velocities; Labor intensive; Operation records; Practical issues; Sparse GPS reports; Trace driven simulation; Travel distance; Big data",2-s2.0-85031763669
"Su J., Tang Y.","Business Intelligence Revisited",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760696&doi=10.1109%2fCBD.2017.9&partnerID=40&md5=93dbd4a10fa55cb933ae0d25b19f39c8","Combining big data with machine learning is a powerful tool for business intelligence (BI). Developed in the database community, The traditional ETL-data warehouse-OLAP approach to BI is effective to deal with multi-dimensional data (i.e. data cubes) but not suitable for flexible analytics such as exploration with ad hoc queries and process/data changes. Process mining techniques developed in the BPM community focus on activities and control flow but ignore data. In this paper, we propose a new framework for business analytics based on workflow logs. We introduce the key notions, illustrate querying logs as one useful aspect, and then discuss a range of interesting technical problems to be studied further. © 2017 IEEE.",,"Data warehouses; Information analysis; Learning systems; Query languages; Query processing; Ad-hoc queries; Business analytics; Control flows; Data cube; Database community; Multidimensional data; Process mining; Big data",2-s2.0-85031760696
"Wang J., Wang Y., Jin Y.","RBTA: A Multivariate Time-Series Method for City Incidents Mining and Forecasting",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031711709&doi=10.1109%2fCBD.2017.66&partnerID=40&md5=17285d5b020862cfd971a8ad1795aea1","Mining and forecasting time-series incidents in large cities is very useful for the administration. However, most of the existing time-series prediction methods use univariate models which ignore the relationship among different city incidents. This paper proposes RBTA, a multivariate time-series model, to find the patterns including basic trend, seasonality, irregular components and relationship among different incidents. We evaluate our model on the real dataset from the downtown area of Shanghai, one the biggest metropolitan of the world. The average forecasting root mean squared error(RMSE) is 0.15, which decreases 4.9% comparing to the best one of the existing methods. © 2017 IEEE.","Forecast; Time-series; Urban incident","Forecasting; Mean square error; Time series; Forecasting time series; Large cities; Multivariate time series; Multivariate time series models; Root mean squared errors; Time series prediction; Univariate models; Urban incident; Big data",2-s2.0-85031711709
"Zhang Z., Zhu W.","Location and Motion Prediction of Consumers in a Large Shopping Mall",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031760727&doi=10.1109%2fCBD.2017.50&partnerID=40&md5=016934c2116e27ae1d261faa4f093368","It is important to predict consumers' location and motion in a large shopping mall to provide them better service. When a consumer passes regions of a shopping mall, his/her moving trace can be recorded for prediction. Existing approaches cannot be directly used to fulfill such task because handling the ordered region sequences is quite challenging. In this paper, we propose an improved Apriori algorithm called AprioriOS (Apriori for Ordered Sequences) to solve this problem. Using this method, association rules are mined out from ordered region sequences and then used to predict future locations of consumers. We can predict more than one regions that a consumer may pass in future. We also design an association rule querying method and a tree storage structure for location prediction. And we propose a motion prediction method based on 3-Axis accelerometer or RFID to predict motions of consumers. Based on the proposed we develop an location and motion prediction system for shopping malls. Our simulation results show that the system is effective in terms of the accuracy of prediction. © 2017 IEEE.","Apriori; Location prediction; Motion prediction; Sequential pattern mining; Shopping mall","Association rules; Big data; Digital storage; Forecasting; Location; Shopping centers; 3-axis accelerometer; Apriori; Improved apriori algorithms; Location prediction; Motion prediction; Sequential-pattern mining; Storage structures; Motion estimation",2-s2.0-85031760727
"Zhu B., Zhao J., Li D.-A., Wang H., Bai R., Li Y., Yang B.","Dynamic Cloud Access Control via Accelerometers Using Customized Actions of Occupants",2017,"Proceedings - 5th International Conference on Advanced Cloud and Big Data, CBD 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031713972&doi=10.1109%2fCBD.2017.53&partnerID=40&md5=23efa58245e1979e66a8d6f12ff79c6a","Despite the widespread installation of access control systems in all places where authorized users to enter, access control systems are still not immature due to the limited technology of convenience, security and low latency. Existing biological method based systems are relatively reliable, such as fingerprint identification, face perception and iris recognition. However, these techniques require excessive installation and maintenance. Thus, we propose a new dynamic cloud access control, which leverages sensing capability of Wireless Identification and Sensing Platform (WISP) tags, to combine user motion and to realize abnormal alarm. Users can be granted to get into the access control system when they perform user-defined action. The intelligent security system is not only convenient for users to customize the action, but also improves the security of access control system. By measuring and mining data from the WISP tags, we implement the proposed system and conduct extension experiments. The experimental data demonstrates the effectiveness of the proposed system. © 2017 IEEE.","Cloud access control system; Dynamic authentication; Sensing data processing; User-defined actions","Big data; Biometrics; Control systems; Data handling; Authorized users; Biological methods; Fingerprint identification; Intelligent security systems; Iris recognition; Low latency; User-defined actions; Wireless identification and sensing platforms; Access control",2-s2.0-85031713972
"Cristofolini I., Molinari A., Pederzini G., Rambelli A.","From experimental data, the mechanics relationships describing the behaviour of four different low alloyed steel powders during uniaxial cold compaction",2017,"Powder Metallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029422911&doi=10.1080%2f00325899.2017.1361507&partnerID=40&md5=d7940be4f90ec8db049c48ccfb96c44c","This work aims at determining the constitutive model of four commercial water atomised low alloyed steel powders during cold compaction. Single-action experiments were performed, obtaining cylindrical specimens with different H/D ratios. The distribution of axial and radial stresses was investigated, and the relationships describing both the radial stress transmission coefficient and the flow stress as functions of the relative density were determined. The radial stress transmission coefficient also confirmed the hypothesised value of Poisson’s coefficient. The friction coefficient between the powder column and the die wall was determined, also highlighting the influence of the H/D ratio. Measuring the axial and radial strains due to spring-back, the axial and radial elastic moduli were determined, as functions of the relative density. The results obtained for the four materials were compared, also highlighting both differences and similarities. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute","Cold compaction; constitutive model; low alloyed steel powders","Compaction; Constitutive models; Friction; Cold compaction; Commercial waters; Cylindrical specimens; Friction coefficients; Low alloyed steels; Radial strains; Radial stress; Relative density; Powders",2-s2.0-85029422911
"Silva F., Moran L., Torres M., Weishaupt C.","A Method to Evaluate Cycloconverters Commutation Robustness under Voltage and Frequency Variations in Mining Distribution Systems",2017,"IEEE Transactions on Industry Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029153278&doi=10.1109%2fTIA.2017.2750115&partnerID=40&md5=66ec48f6ce14e6f7287fe1099c574d87","This paper analyzes the influence of frequency and voltage variation over the commutation of thyristors in high power cycloconverters. The analysis demonstrates that frequency and voltage variations can cause commutation failures generating significant damages in cycloconverters. In addition, the paper shows how to determine the maximum frequency and voltage variations that will not affect commutation between thyristors, information that can be later used for the correct setting of protection relays. The analysis is complemented with simulated results using data obtained from high power thyristors used in commercially available cycloconverters. Finally, a commutation failure in a 15 MW grinding mill cycloconverter drive is presented and analyzed. IEEE","Cycloconverters; frequency and voltage variations; thyristor commutation",,2-s2.0-85029153278
"Charalampos P., Emmanouil Z., Dimitrios I., Lappa E.","Design and implementation of monitoring and evaluation of healthcare organization management",2017,"AIP Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029840129&doi=10.1063%2f1.4996685&partnerID=40&md5=ff81669941930e36b838fdcdb4735747","The management of a healthcare organization is monitored using a suitably designed questionnaire to 271 nurses operating in Greek hospital. The data are fed to an automatic data mining system to obtain a suitable series of models to analyse, visualise and study the obtained information. Hidden patterns, correlations and interdependencies are investigated and the results are analytically presented. © 2017 Author(s).",,,2-s2.0-85029840129
"Poonsirivong K., Jittawiriyanukoon C.","A rapid anomaly detection technique for big data curation",2017,"Proceedings of the 2017 14th International Joint Conference on Computer Science and Software Engineering, JCSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732604&doi=10.1109%2fJCSSE.2017.8025900&partnerID=40&md5=b732d3b2f5c093b39d222213ae05cbf1","Anomaly detection (outlier) using simulation helps us analyze the anomaly instances from big data source. As the hasty explosion of today's data stream, outlier detection technique will be an analytical tool to be employed for evaluating massive unstructured datasets. In order to speed-up the processing time to handle enormous datasets, this research will conduct experiments of advanced distant-based outlier detection algorithms to investigate the most effective algorithms using MOA. The algorithms used in this study are Continuous Outlie Detection (COD), Micro-Cluster based COD or MCOD, and STream OutlierR Miner (STORM). The results demonstrate MCOD algorithm can outperform other two algorithms in terms of processing time and accurate anomalies. © 2017 IEEE.","anomaly detection; big data; MOA; outlier detection; performance evaluation","Birds; Clustering algorithms; Data handling; Data mining; Software engineering; Statistics; Analytical tool; Anomaly detection; Anomaly detections (Outlier); Effective algorithms; Outlier Detection; Outlier detection algorithm; performance evaluation; Processing time; Big data",2-s2.0-85031732604
"Sapul M.S.C., Aung T.H., Jiamthapthaksin R.","Trending topic discovery of Twitter Tweets using clustering and topic modeling algorithms",2017,"Proceedings of the 2017 14th International Joint Conference on Computer Science and Software Engineering, JCSSE 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031762233&doi=10.1109%2fJCSSE.2017.8025911&partnerID=40&md5=838199371d9379c362378868d67d7c2a","There is no previous research that compares the results of k-means, CLOPE clustering and Latent Dirichlet Allocation (LDA) topic modeling algorithms for detecting trending topics on tweets. Since not all tweets contain hashtags, we considered three training data feature sets: hashtags, keywords and keywords + hashtags in this study. Our proposed methodology proved that CLOPE can also be used in a non-Transactional database like Twitter data set to answer the trending topic discovery and could provide more topic patterns than k-means and LDA. Using additional feature sets has improved the results of k-means and LDA, thus, keywords + hashtags can identify more meaningful topics. © 2017 IEEE.","CLOPE; k-means; Latent Dirichlet Allocation; Trending Analysis; Tweets; Twitter Data Mining","Clustering algorithms; Indexing (of information); Social networking (online); Software engineering; Statistics; CLOPE; K-means; Latent Dirichlet allocation; Trending Analysis; Tweets; Data mining",2-s2.0-85031762233
"Asghar M.Z., Khan A., Bibi A., Kundi F.M., Ahmad H.","Sentence-Level Emotion Detection Framework Using Rule-Based Classification",2017,"Cognitive Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028811705&doi=10.1007%2fs12559-017-9503-3&partnerID=40&md5=08f13989f83fb508adf35e16922731f2","Emotion detection and analysis aims at developing applications that can detect and analyse emotions expressed by the users in a given text. Such applications have received considerable attention from experts in computer science, psychology, communications and health care. Emotion-based sentiment analysis can be performed using supervised and unsupervised techniques. The existing studies using supervised and unsupervised emotion-based sentiment analysis are based on Ekman’s basic emotion model; have limited coverage of emotion-words, polarity shifters and negations; and lack emoticons and slang. The problems associated with existing approaches can be overcome by the development of an effective, sentence-level emotion-detection sentiment analysis system under a rule-based classification scheme with extended lexicon support and an enhanced model of emotion signals: emotion words, polarity shifters, negations, emoticons and slang. In this work, we propose a rule-based framework for emotion-based sentiment classification at the sentence level obtained from user reviews. The main contribution of this work is to integrate cognitive-based emotion theory (e.g. Ekman’s model) with sentiment analysis-based computational techniques (e.g. detection of emotion words, emoticons and slang) to detect and classify emotions from natural language text. The main focus is to improve the performance of state-of-the-art methods by including additional emotion-related signals, such as emotion words, emoticons, slang, polarity shifters and negations, to efficiently detect and classify emotions in user reviews. The improved results in terms of accuracy, precision, recall and F-measure demonstrate the superiority of the proposed method’s classification results compared with baseline methods. The framework is generalized and capable of classifying emotions in any domain. © 2017 Springer Science+Business Media, LLC","Emoticons; Emotion detection; Opinion mining; Sentiment analysis; Slang","Computation theory; Data mining; Medical computing; Emoticons; Emotion detection; Opinion mining; Sentiment analysis; Slang; Natural language processing systems",2-s2.0-85028811705
"Nouroz F., Noreen S., Khan M.F., Ahmed S., Heslop-Harrison J.S.P.","Identification and characterization of mobile genetic elements LINEs from Brassica genome",2017,"Gene",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020741963&doi=10.1016%2fj.gene.2017.06.015&partnerID=40&md5=f568638373571beaa80a222aafdae9a9","Among transposable elements (TEs), the LTR retrotransposons are abundant followed by non-LTR retrotransposons in plant genomes, the lateral being represented by LINEs and SINEs. Computational and molecular approaches were used for the characterization of Brassica LINEs, their diversity and phylogenetic relationships. Four autonomous and four non-autonomous LINE families were identified and characterized from Brassica. Most of the autonomous LINEs displayed two open reading frames, ORF1 and ORF2, where ORF1 is a gag protein domain, while ORF2 encodes endonuclease (EN) and a reverse transcriptase (RT). Three of four families encoded an additional RNase H (RH) domain in pol gene common to ‘R’ and ‘I’ type of LINEs. The PCR analyses based on LINEs RT fragments indicate their high diversity and widespread occurrence in tested 40 Brassica cultivars. Database searches revealed the homology in LINE sequences in closely related genera Arabidopsis indicating their origin from common ancestors predating their separation. The alignment of 58 LINEs RT sequences from Brassica, Arabidopsis and other plants depicted 4 conserved domains (domain II–V) showing similarity to previously detected domains. Based on RT alignment of Brassica and 3 known LINEs from monocots, Brassicaceae LINEs clustered in separate clade, further resolving 4 Brassica-Arabidopsis specific families in 2 sub-clades. High similarities were observed in RT sequences in the members of same family, while low homology was detected in members across the families. The investigation led to the characterization of Brassica specific LINE families and their diversity across Brassica species and their cultivars. © 2017 Elsevier B.V.","Brassica; Diversity; LINEs; Phylogeny; Retrotransposons; Reverse transcriptase","ribonuclease H; RNA directed DNA polymerase; Arabidopsis; Article; Brassica; Brassicaceae; cladistics; cultivar; gene identification; genetic conservation; last common ancestor; long interspersed repeat; monocot; multigene family; nonhuman; plant genome; polymerase chain reaction; priority journal; sequence alignment; sequence homology; species; structural gene; amino acid sequence; Brassica; chemistry; classification; data mining; genetics; open reading frame; phylogeny; Amino Acid Sequence; Brassica; Data Mining; Genome, Plant; Long Interspersed Nucleotide Elements; Open Reading Frames; Phylogeny; RNA-Directed DNA Polymerase; Sequence Alignment",2-s2.0-85020741963
"Baroni A., Ruggieri S.","Segregation discovery in a social network of companies",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028990445&doi=10.1007%2fs10844-017-0485-0&partnerID=40&md5=075829f12c76acb237672d6cc91cec60","We introduce a framework for the data-driven analysis of social segregation of minority groups, and challenge it on a complex scenario. The framework builds on quantitative measures of segregation, called segregation indexes, proposed in the social science literature. The segregation discovery problem is introduced, which consists of searching sub-groups of population and minorities for which a segregation index is above a minimum threshold. A search algorithm is devised that solves the segregation problem by computing a multi-dimensional data cube that can be explored by the analyst. The machinery underlying the search algorithm relies on frequent itemset mining concepts and tools. The framework is challenged on a cases study in the context of company networks. We analyse segregation on the grounds of sex and age for directors in the boards of the Italian companies. The network includes 2.15M companies and 3.63M directors. © 2017 Springer Science+Business Media, LLC","Frequent itemset mining; Network of company board directors; Segregation discovery; Segregation indexes","Learning algorithms; Machinery; Company network; Data-driven analysis; Frequent itemset mining; Italian companies; Multidimensional data; Quantitative measures; Search Algorithms; Segregation index; Social networking (online)",2-s2.0-85028990445
"Westhues A., Hanchar J.M., Voisey C.R., Whitehouse M.J., Rossman G.R., Wirth R.","Tracing the fluid evolution of the Kiruna iron oxide apatite deposits using zircon, monazite, and whole rock trace elements and isotopic studies",2017,"Chemical Geology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021378574&doi=10.1016%2fj.chemgeo.2017.06.020&partnerID=40&md5=4b0bc5e4ff76423d6a3a4900674418e3","The ore genesis of the Paleoproterozoic iron oxide apatite deposits in the vicinity of Kiruna in northern Sweden is poorly understood, despite a century-long mining history and 2500 Mt of iron ore with grades of 30 to 70 wt% Fe produced in the region to date. Zircon grains from the ore, recently dated at ca. 1874 Ma, show very different appearances compared to zircon from surrounding host rocks (ca. 1880 Ma) and related intrusions (ca. 1880 and ca. 1874 Ma), particularly an inclusion-rich rim. In contrast, zircon from the host rocks, and a proximal granite intrusion, exhibit typical igneous growth zoning. Electron microprobe results show near stoichiometric composition for Zr, Si, and Hf in the host rock zircon grains. The ore zircon crystals have low analytical totals with significant concentrations of Ca, Fe, Y, and P and infrared spectroscopy showed several weight percent of water. These ore zircon grains further show Fe-rich inclusions, zones and/or veins in elemental X-ray maps, and light rare earth elements (LREE) enrichment. Transmission electron microscopy (TEM) shows that the LREE are not due to micro- or nano-inclusions in the zircon, but are likely hosted as LREE oxides in amorphous regions of the grains. Based on these characteristics, the rims on ore zircon grains are interpreted to be of hydrothermal origin. Uranium-Pb in monazite from the ore, measured by SIMS, suggests a secondary event influencing the area at ca. 1624 Ma, a period of known geologic activity in Fennoscandia. Electron microprobe X-ray mapping of these monazite grains shows no zoning and relatively low U and Th concentrations. Stark contrasts are visible between the ore (depleted mantle influence) and host rocks (crustal influence) in the whole rock Lu-Hf and Sm-Nd data. The depleted mantle signature of the ore could be related to the Kiruna greenstone group as a potential source region for the iron. The Sm-Nd isotopic composition of monazite from the ore shows a crustal influence, and indicates that the younger event has not disturbed the whole rock Sm-Nd signature of the ore. The hydrothermal nature of the ore zircon grains and the isotopic signatures point to a hydrothermal influence on the ore formation, with a high temperature magmatic fluid related to the intrusions as most likely heat and fluid source. © 2017 Elsevier B.V.","FTIR spectroscopy; Hydrothermal alteration; Iron oxide apatite; Kiruna; LA-ICPMS; Monazite; Norrbotten; Paleoproterozoic; Sm-Nd; TEM; U-Pb; Zircon","Apatite; Binary alloys; Clay alteration; Deposits; Electron probe microanalysis; Fourier transform infrared spectroscopy; Geochronology; Hafnium; High resolution transmission electron microscopy; Igneous rocks; Infrared spectroscopy; Iron; Iron deposits; Iron oxides; Isotopes; Lead; Monazite; Monazite deposits; Ore analysis; Ores; Phosphate minerals; Rocks; Silicate minerals; Trace elements; Transmission electron microscopy; Zircon; Zoning; FTIR spectroscopy; Hydrothermal alterations; Kiruna; La-icpms; Norrbotten; Paleoproterozoic; Zircon deposits; apatite; FTIR spectroscopy; hydrothermal alteration; inductively coupled plasma method; iron oxide; isotopic composition; lead; mass spectrometry; monazite; neodymium; ore deposit; Proterozoic; samarium; trace element; transmission electron microscopy; uranium; zircon; Norrbotten; Sweden",2-s2.0-85021378574
"Dhanalakshmi R., Sethukarasi T.","A hybrid approach for mismatch data reduction in datasets and guide data mining",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028847576&doi=10.1007%2fs10586-017-1137-4&partnerID=40&md5=c63f799d42d5b9b3b6df348cefab6a61","An outlier is a set of data that distinctly differ from rest of the data in a dataset defined as normal. Detection of outlier is an active area of research in data mining. If clustering methods are used, the elements that are lying outside the clusters are focused and detected as outliers. But it is not true few unknown elements will become a part of the cluster. So to ignore the irrelevant data completely from the data set, it becomes necessary to identify and eliminate these data merged with the clusters. An efficient hybrid approach is proposed to reduce the number of outliers. Two algorithms namely multilayer neural networks (MLN) and weighted-K means adopted for datamining are employed in proposed approach to identify outliers in a data group. This approach guides and results in better cluster formation. Each element in the dataset provided as input to MLN after assigning weights by weighted K-means. MLN is trained to reproduce the normal input data (inliers) and ensures that groups formed by weighted K-means are consisting of inliers only. Among the outlier detection methods presented in literature for outlier detection in data mining, the proposed method is based on Integrating Semantic Knowledge. This method relates the data point is an outlier by identifying the behaviour of the data elements that differ from other data elements belonging to the same cluster or class. The principle intention of this research work is to reduce the amount of outliers by enhancing the performance of clustering or classification techniques that guides to improve accuracy and reduce the mean square error. The test results provides evident to supremacy of the proposed strategy in reducing the outlier. © 2017 Springer Science+Business Media, LLC","Classification; Clustering; Mismatch data; Multilayer neural network","Classification (of information); Data handling; Data mining; Mean square error; Multilayer neural networks; Multilayers; Semantics; Statistics; Classification technique; Cluster formations; Clustering; Clustering methods; Mismatch data; Outlier Detection; Semantic knowledge; Weighted k-means; Data reduction",2-s2.0-85028847576
"Timonin A.Y., Bozhday A.S., Bershadsky A.M.","Analysis of unstructured text data for a person social profile",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030115603&doi=10.1145%2f3129757.3129758&partnerID=40&md5=52b0400f9700ece3c13a9e7a1dd99893","The greatest scientific interest for analysts are Internet open social data, because it has a direct link with all kinds of human activity. However, these data are not suitable for the application in its original form. Information should be presented in a structured, convenient, human-readable form which is called a social profile. The social profile building is carried out through the analysis of the filtered Internet open source data. Analysis of personal profile data is achieved through the use of mathematical set theory, Big Data software, NoSQL data stores and analytic tools for social media. This article discusses methods of unstructured textual data analysis in relation to a social profile. Special attention is given to the search of implicit dependences in texts using visual analysis and natural language processing means. Phase of the textual data analysis is the most important in terms of results and complicated to implement. There is the possibility to partially automate the process of information analyzing through the use of visual analysis, natural language processing (NLP), neural networks and specialized algorithms. Resulted data provide a detailed in-depth review of the social profile entities and relations. It can be used in further deeper social researches. © 2017 Association for Computing Machinery.","Big data; Data analysis; Data mining; Natural language processing; Personal social profile; Public data sources; Social media; Text analysis; Unstructured data; Visual analysis","Data handling; Data mining; Data reduction; Government data processing; Information analysis; Natural language processing systems; Open source software; Set theory; Social networking (online); Visual languages; Public data source; Social media; Social profiles; Text analysis; Unstructured data; Visual analysis; Big data",2-s2.0-85030115603
"Corcoran P., Jones C.B.","Modelling Topological Features of Swarm Behaviour in Space and Time with Persistence Landscapes",2017,"IEEE Access",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029178386&doi=10.1109%2fACCESS.2017.2749319&partnerID=40&md5=a6f01d5f224ff7129e4b5934e29995de","This paper presents a model of swarm behavior that encodes the spatial-temporal characteristics of topological features, such as holes and connected components. Specifically, the persistence of topological features with respect to time is computed using zig-zag persistent homology. This information is in turn modelled as a persistence landscape, which forms a normed vector space and facilitates the application of statistical and data mining techniques. Validation of the proposed model is performed using a real data set corresponding to a swarm of fish. It is demonstrated that the proposed model may be used to perform retrieval and clustering of swarm behavior in terms of topological features. In fact, it is discovered that clustering returns clusters corresponding to the swarm behaviors of flock, torus, and disordered. These are the most frequently occurring types of behavior exhibited by swarms in general. © 2013 IEEE.","persistence landscape; spatial-temporal; Swarm behaviour; topology","Data mining; Data structures; Estimation; Feature extraction; Vector spaces; Computational model; Kernel; Persistence Landscape; Spatial temporals; Swarm Behaviour; Topology",2-s2.0-85029178386
"Bhavnani Y., Rodden K., Guarnotta L.C., Lynn M.T., Chizari S., Granka L.","Understanding mobile phone activities via retrospective review of visualizations of usage data",2017,"Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030329280&doi=10.1145%2f3098279.3119841&partnerID=40&md5=1e3f2c35e4076c625cc89b7f0af11641","It can be very challenging to get an accurate understanding of mobile phone usage because of the difficulty of observing phone activity in a natural setting. We describe a retrospective methodology where participants review visualizations of their logged activity in an interview setting, and our lessons learned in applying this methodology in a study of user goals and journeys on mobile devices across apps.","Ethno-mining; Mobile phone usage; Retrospective cued recall; Visualization","Cellular telephones; Flow visualization; Mobile phones; Telephone sets; Visualization; Cued recalls; Mobile phone usages; Usage data; User goals; Human computer interaction",2-s2.0-85030329280
"Velliangiri S., Premalatha J.","Intrusion detection of distributed denial of service attack in cloud",2017,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028811149&doi=10.1007%2fs10586-017-1149-0&partnerID=40&md5=9203448161ec4110bc48114bc8b93574","Security issue in cloud environment is one of the major obstacle in cloud implementation. Network attacks make use of the vulnerability in the network and the protocol to damage the data and application. Cloud follows distributed technology; hence it is vulnerable for intrusions by malicious entities. Intrusion detection systems (IDS) has become a basic component in network protection infrastructure and a necessary method to defend systems from various attacks. Distributed denial of service (DDoS) attacks are a great problem for a user of computers linked to the Internet. Data mining techniques are widely used in IDS to identify attacks using the network traffic. This paper presents and evaluates a Radial basis function neural network (RBF-NN) detector to identify DDoS attacks. Many of the training algorithms for RBF-NNs start with a predetermined structure of the network that is selected either by means of a priori knowledge or depending on prior experience. The resultant network is frequently inadequate or needlessly intricate and a suitable network structure could be configured only by trial and error method. This paper proposes Bat algorithm (BA) to configure RBF-NN automatically. Simulation results demonstrate the effectiveness of the proposed method. © 2017 Springer Science+Business Media, LLC","Bat algorithm (BA); Distributed denial of service (DDoS); Intrusion detection systems (IDS); Neural network (NN); Radial basis function (RBF)","Computer crime; Data mining; Denial-of-service attack; Functions; Image segmentation; Intrusion detection; Mercury (metal); Radial basis function networks; Bat algorithms; Distributed denial of service; Intrusion Detection Systems; Neural network (nn); Radial Basis Function(RBF); Network security",2-s2.0-85028811149
"Kang M., Li Y., Zhao Y., He S., Shi J.","miR-33a inhibits cell proliferation and invasion by targeting CAND1 in lung cancer",2017,"Clinical and Translational Oncology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028852718&doi=10.1007%2fs12094-017-1730-2&partnerID=40&md5=71814e11895cddc4bfa7b60548cde89b","Background: Lung cancer continues to be one of the top five causes of cancer-related mortality. This study aims to identify down- and upregulated miRNAs and mRNA which can be used as potential biomarkers and/or therapeutic targets for lung cancer. Methods: Integrated analysis of differential expression profiles of miRNA and mRNA in lung cancer was performed by searching Gene Expression Omnibus datasets. Based on miRNA expression profiles, direct mRNA targets of miRNAs with experimental support were identified through miRTarBase. The levels of representative miRNAs and mRNAs were confirmed through qualitative real-time reverse transcriptase polymerase chain reaction (qRT-PCR). Results: The miR-33a was decreased in non-small cell lung cancer (NSCLC) tissues compared with the para-carcinoma tissues, whereas its target mRNA of cullin-associated NEDD8-dissociated protein 1 (CAND1) was increased in NSCLC tissues. Further research has shown that miR-33a can inhibit lung cancer cell proliferation, cell cycle progression, and migration by targeting CAND1. Moreover, the CAND1 knockout lung cancer cells showed similar results as cells transfected with miR-33a mimic. Conclusions: These results suggested that the data mining based on online databases was an effective method in finding novel target in cancer research, and the miR-33a and CAND1 played an important role in lung cancer proliferation and cell migration. © 2017 Federación de Sociedades Españolas de Oncología (FESEO)","CAND1; Data mining; Lung cancer; miR-33a",,2-s2.0-85028852718
"Matthies D.J.C., Roumen T., Kuijper A., Urban B.","CapSoles: Who is walking on what kind of floor?",2017,"Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030307967&doi=10.1145%2f3098279.3098545&partnerID=40&md5=72c777c2cc287d6fb7b6c52ee4f10708","Foot interfaces, such as pressure-sensitive insoles, still yield unused potential such as for implicit interaction. In this paper, we introduce CapSoles, enabling smart insoles to implicitly identify who is walking on what kind of floor. Our insole prototype relies on capacitive sensing and is able to sense plantar pressure distribution underneath the foot, plus a capacitive ground coupling effect. By using machine-learning algorithms, we evaluated the identification of 13 users, while walking, with a confidence of ∼95% after a recognition delay of ∼1s. Once the user's gait is known, again we can discover irregularities in gait plus a varying ground coupling. While both effects in combination are usually unique for several ground surfaces, we demonstrate to distinguish six kinds of floors, which are sand, lawn, paving stone, carpet, linoleum, and tartan with an average accuracy of ∼82%. Moreover, we demonstrate the unique effects of wet and electrostatically charged surfaces.","Capacitive sensing; Data mining; Floor detection; Foot interaction; Ground surface detection; Implicit input; Machine learning; Shoe interface; Smart insole; User identification; Wearable computing","Artificial intelligence; Couplings; Data mining; Floors; Learning algorithms; Learning systems; Capacitive sensing; Foot interaction; Ground surfaces; Implicit input; Smart insole; User identification; Wearable computing; Human computer interaction",2-s2.0-85030307967
"Atsumi T., Ando Y., Matsuda S., Tomizawa S., Tanaka R., Takagi N., Nakasone A.","Prodromal signs and symptoms of serious infections with tocilizumab treatment for rheumatoid arthritis: Text mining of the Japanese postmarketing adverse event-reporting database",2017,"Modern Rheumatology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029404451&doi=10.1080%2f14397595.2017.1366007&partnerID=40&md5=07223bf3b0ac856268a7d0ea508584ca","Objective: To search for signs and symptoms before serious infection (SI) occurs in tocilizumab (TCZ)-treated rheumatoid arthritis (RA) patients. Methods: Individual case safety reports, including structured (age, sex, adverse event [AE]) and unstructured (clinical narratives) data, were analyzed by automated text mining from a Japanese post-marketing AE-reporting database (16 April 2008–10 April 2015) assuming the following: treated in Japan; TCZ RA treatment; ≥1 SI; unable to exclude causality between TCZ and SIs. Results: The database included 7653 RA patients; 1221 reports met four criteria, encompassing 1591 SIs. Frequent SIs were pneumonia (15.9%), cellulitis (9.9%), and sepsis (5.0%). Reports for 782 patients included SI onset date; 60.7% of patients had signs/symptoms ≤28 days before SI diagnosis, 32.7% had signs/symptoms with date unidentified, 1.7% were asymptomatic, and 4.9% had unknown signs/symptoms. The most frequent signs/symptoms were for skin (swelling and pain) and respiratory (cough and pyrexia) infections. Among 68 patients who had normal laboratory results for C-reactive protein, body temperature, and white blood cell count, 94.1% had signs or symptoms of infection. Conclusion: This study identified prodromal signs and symptoms of SIs in RA patients receiving TCZ. Data mining clinical narratives from post-marketing AE databases may be beneficial in characterizing SIs. © 2017 Japan College of Rheumatology","Adverse events; infections; rheumatoid arthritis; text mining; tocilizumab",,2-s2.0-85029404451
"Kairaliyeva T., Aksenenko E.V., Mucic N., Makievski A.V., Fainerman V.B., Miller R.","Surface Tension and Adsorption Studies by Drop Profile Analysis Tensiometry",2017,"Journal of Surfactants and Detergents",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028860407&doi=10.1007%2fs11743-017-2016-y&partnerID=40&md5=a4223d98963f00da772b3853bf9c7247","Surface tension and dilational viscoelasticity of solutions of various surfactants measured with bubble and drop profile analysis tensiometry are discussed. The study also includes experiments on the co-adsorption of surfactant molecules from a solution drop and alkane molecules from saturated alkane vapor phase. Using experimental data for 12 surfactants with different surface activities, it is shown that depletion due to adsorption of surfactant from the drop bulk can be significant. An algorithm is proposed quantitatively to take into consideration the depletion effect which is required for a correct description of the co-adsorption of alkanes on the solution drop surface and the correct analysis of experimental dynamic surface tension data to determine the adsorption mechanism. Bubble and drop profile analysis tensiometry is also the method of choice for measuring the dilational viscoelasticity of the adsorbed interfacial layer. The same elasticity moduli are obtained with the bubble and drop method only when the equilibrium surface pressures are sufficiently small (Π &lt; 15 mN m−1). When the surface pressure for a surfactant solution is larger than this value, the viscoelasticity moduli determined from drop profile experiments become significantly larger than those obtained from bubble profile measurements. © 2017 The Author(s)","Bubble and drop profile analysis tensiometry; Surfactant adsorption layers; Surfactant depletion due to adsorption","Adsorption; Molecules; Paraffins; Solution mining; Surface active agents; Surface tension; Viscoelasticity; Adsorption mechanism; Dilational viscoelasticity; Drop profile; Experimental dynamics; Profile measurement; Surfactant adsorption layers; Surfactant molecules; Surfactant solution; Drops",2-s2.0-85028860407
"Chen Y., Xu P., Ren L.","Sequence Synopsis: Optimize Visual Summary of Temporal Event Data",2017,"IEEE Transactions on Visualization and Computer Graphics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029166462&doi=10.1109%2fTVCG.2017.2745083&partnerID=40&md5=7db81d3af80d5cc94757b026422b36f4","Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback. IEEE","Algorithm design and analysis; Data mining; Data models; Data Transformation and Representation; Data visualization; Noise measurement; Time Series Data; Visual analytics; Visual Analytics; Visual Knowledge Representation",,2-s2.0-85029166462
"Tu E., Zhang G., Rachmawati L., Rajabally E., Huang G.","Exploiting AIS Data for Intelligent Maritime Navigation: A Comprehensive Survey From Data to Methodology",2017,"IEEE Transactions on Intelligent Transportation Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029150168&doi=10.1109%2fTITS.2017.2724551&partnerID=40&md5=3396b7abb27d069817b4e724b4694cab","The automatic identification system (AIS) tracks vessel movement by means of electronic exchange of navigation data between vessels, with onboard transceiver, terrestrial, and/or satellite base stations. The gathered data contain a wealth of information useful for maritime safety, security, and efficiency. Because of the close relationship between data and methodology in marine data mining and the importance of both of them in marine intelligence research, this paper surveys AIS data sources and relevant aspects of navigation in which such data are or could be exploited for safety of seafaring, namely traffic anomaly detection, route estimation, collision prediction, and path planning. IEEE","AIS data survey; anomaly detection; Artificial intelligence; collision prediction; Data mining; Estimation; Intelligent maritime navigation; Marine vehicles; Navigation; path planning; route estimation; Safety",,2-s2.0-85029150168
"Mahdianpari M., Salehi B., Mohammadimanesh F., Brisco B.","An Assessment of Simulated Compact Polarimetric SAR Data for Wetland Classification Using Random Forest Algorithm",2017,"Canadian Journal of Remote Sensing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032272752&doi=10.1080%2f07038992.2017.1381550&partnerID=40&md5=722bdb3e8a3fe5366eac05bc79559cfe","Synthetic aperture radar (SAR) compact polarimetry (CP) systems are of great interest for large area monitoring because of their ability to acquire data in a wider swath compared to full polarimetry (FP) systems and a significant improvement in information content compared to single or dual polarimetry (DP) sensors. In this study, we compared the potential of DP, FP, and CP SAR data for wetland classification in a case study located in Newfoundland, Canada. The DP and CP data were simulated using full polarimetric RADARSAT-2 data. We compared the classification results for different input features using an object-based random forest classification. The results demonstrated the superiority of FP imagery relative to both DP and CP data. However, CP indicated significant improvements in classification accuracy compared to DP data. An overall classification accuracy of approximately 76% and 84% was achieved with the inclusion of all polarimetric features extracted from CP and FP data, respectively. In summary, although full polarimetric SAR data provide the best classification accuracy, the results demonstrate the potential of RADARSAT Constellation Mission for mapping wetlands in a large landscape. © 2017, Copyright © Crown copyright.",,"Data mining; Decision trees; Ellipsometry; Polarimeters; Radar imaging; Synthetic aperture radar; Wetlands; Classification accuracy; Classification results; Constellation missions; Newfoundland , Canada; Polarimetric features; Random forest algorithm; Random forest classification; Wetland classification; Classification (of information)",2-s2.0-85032272752
"Liao S.-H., Chen Y.-J.","A rough set-based association rule approach implemented on a brand trust evaluation model",2017,"Journal of Experimental and Theoretical Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007109454&doi=10.1080%2f0952813X.2016.1264089&partnerID=40&md5=2aec0a14dd4d3ed2aef654af572a69a8","In commerce, businesses use branding to differentiate their product and service offerings from those of their competitors. The brand incorporates a set of product or service features that are associated with that particular brand name and identifies the product/service segmentation in the market. This study proposes a new data mining approach, a rough set-based association rule induction, implemented on a brand trust evaluation model. In addition, it presents as one way to deal with data uncertainty to analyse ratio scale data, while creating predictive if–then rules that generalise data values to the retail region. As such, this study uses the analysis of algorithms to find alcoholic beverages brand trust recall. Finally, discussions and conclusion are presented for further managerial implications. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","association rule; brand trust evaluation model; Data mining; ratio scale data processing; rough set theory","Association rules; Beverages; Data handling; Data mining; Uncertainty analysis; Alcoholic beverages; Analysis of algorithms; Data uncertainty; Managerial implications; Product and services; Ratio scale; Rough-set based; Trust evaluation; Rough set theory",2-s2.0-85007109454
"Hadjidimitriou N.S., Mamei M., Dell'Amico M., Kaparias I.","Classification of Livebus arrivals user behavior",2017,"Journal of Intelligent Transportation Systems: Technology, Planning, and Operations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032864506&doi=10.1080%2f15472450.2016.1265890&partnerID=40&md5=dcab65cf0ada6c3a1a0766e1769a68f5","With the increasing use of Intelligent Transport Systems, large amounts of data are created. Innovative information services are introduced and new forms of data are available, which could be used to understand the behavior of travelers and the dynamics of people flows. This work analyzes the requests for real-time arrivals of bus routes at stops in London made by travelers using Transport for London's LiveBus Arrivals system. The available dataset consists of about one million requests for real-time arrivals for each of the 28 days under observation. These data are analyzed for different purposes. LiveBus Arrivals users are classified based on a set of features and using K-Means, Expectation Maximization, Logistic regression, One-level decision tree, Decision Tree, Random Forest, and Support Vector Machine (SVM) by Sequential Minimal Optimization (SMO). The results of the study indicate that the LiveBus Arrivals requests can be classified into six main behaviors. It was found that the classification-based approaches produce better results than the clustering-based ones. The most accurate results were obtained with the SVM-SMO methodology (Precision of 97%). Furthermore, the behavior within the six classes of users is analyzed to better understand how users take advantage of the LiveBus Arrivals service. It was found that the 37% of users can be classified as interchange users. This classification could form the basis of a more personalized LiveBus Arrivals application in future, which could support management and planning by revealing how public transport and related services are actually used or update information on commuters. © 2017 Taylor & Francis.","Clustering algorithms; data mining; data-driven behavior; intelligent transport system; public transport; real time information; supervised classification","Advanced public transportation systems; Behavioral research; Classification (of information); Clustering algorithms; Decision trees; Information services; Intelligent systems; Intelligent vehicle highway systems; Maximum principle; Multicasting; Optimization; Support vector machines; Traffic control; Transportation; Transportation routes; Data driven; Intelligent transport systems; Public transport; Real-time information; Supervised classification; Data mining; algorithm; data mining; data set; image classification; innovation; intelligent transportation system; optimization; public transport; real time; support vector machine; England; London [England]; United Kingdom",2-s2.0-85032864506
"Moreno-Montiel B., MacKinney-Romero R., Moreno-Montiel C.H.","Detection of Genes in Individual Associated with Laryngeal Cancer using ParalTabs",2017,"IETE Journal of Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018321436&doi=10.1080%2f03772063.2017.1315964&partnerID=40&md5=f5adf8cabf2968a26ad0a1613fdd9a2b","This paper shows the experimental results obtained by applying the classifier parallel scheme of decision tables (ParalTabs) to detection of genes in individuals associated with laryngeal cancer1 1 This paper is an extended version of two conference paper that appeared in [1] and [2]. The key additions of this journal version are as follows. First, in Section 3 we use a different way of how to implement the ParalTabs algorithm, using a set of processes instead of threads. Second, in Section 4 we use the same transformation of data used in previous work [2], but the size of data in this paper is different. Finally, this paper contains new experimental results in Section 4, one of this is using ParalTabs for searching genetic Q3 Q2 markers of laryngeal cancer, which is different from those shown in the paper [2]. The experiments are performed by analyzing a chromosomal database of laryngeal cancer which has the International System for Human Cytogenetic Nomenclature format. ParalTabs is an implementation of decision tables classifiers using the parallel model of multiple instruction and multiple data streams. This classifier uses a set of process that communicates via messages passing and use a parallel scheme that follows the strategy of divide and conquers. We found ParalTabs a useful algorithm to perform classification on genetic databases, obtaining improvements in execution times and performance measures. Using this system, we can determine genes associated with proliferation or non-proliferation of laryngeal cancer, only by taking samples of individuals and analyzing their DNA. © 2017 IETE.","Classification; Classifiers; Data base; Decision tables; Genetic markers; Laryngeal cancer; Machine learning; Parallel computing","Classifiers; Data mining; Decision tables; Diseases; Genes; Learning systems; Metadata; Parallel processing systems; Conference papers; Extended versions; Genetic markers; International system; Laryngeal cancer; Multiple data streams; Multiple instructions; Performance measure; Classification (of information)",2-s2.0-85018321436
"Mohammadi S., Mirvaziri H., Ghazizadeh-Ahsaee M.","Multivariate correlation coefficient and mutual information-based feature selection in intrusion detection",2017,"Information Security Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029925891&doi=10.1080%2f19393555.2017.1358779&partnerID=40&md5=34a5b22c885f0ff44850b1cd9e3d179f","Feature selection is one of the major problems in an intrusion detection system (IDS) since there are additional and irrelevant features. This problem causes incorrect classification and low detection rate in those systems. In this article, four feature selection algorithms, named multivariate linear correlation coefficient (MLCFS), feature grouping based on multivariate mutual information (FGMMI), feature grouping based on linear correlation coefficient (FGLCC), and feature grouping based on pairwise MI, are proposed to solve this problem. These algorithms are implementable in any IDS. Both linear and nonlinear measures are used in the sense that the correlation coefficient and the multivariate correlation coefficient are linear, whereas the MI and the multivariate MI are nonlinear. Least Square Support Vector Machine (LS-SVM) as an intrusion classifier is used to evaluate the selected features. Experimental results on the KDDcup99 and Network Security Laboratory-Knowledge Discovery and Data Mining (NSL) datasets showed that the proposed feature selection methods have a higher detection and accuracy and lower false-positive rate compared with the pairwise linear correlation coefficient and the pairwise MI employed in several previous algorithms. © 2017 Taylor & Francis.","Feature grouping; feature selection; intrusion detection system; linear correlation coefficient; multivariate correlation coefficient; multivariate mutual information","Computer crime; Data mining; Intrusion detection; Mercury (metal); Network security; Problem solving; Support vector machines; Feature grouping; Intrusion Detection Systems; Linear correlation coefficient; Multivariate correlation; Multivariate mutual information; Feature extraction",2-s2.0-85029925891
"Munk M., Pilkova A., Benko L., Blažeková P.","Pillar 3: market discipline of the key stakeholders in CEE commercial bank and turbulent times",2017,"Journal of Business Economics and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032485414&doi=10.3846%2f16111699.2017.1360388&partnerID=40&md5=99e5a2c287868847a4a65880d4aaa606","The study presented in the paper contributes to covering the gap in the area of sufficient information disclosure that also increases the interests of relevant stakeholders in contributing to depository market discipline and in being relevant to their interest within Pillar 3 framework. This paper is focused on an analysis of website data dedicated to Pillar 3 disclosures of commercial banks and on studying the behaviour of stakeholders in relation to the timing of serious market turbulence. The examined data consists of log files that were pre-processed using web mining techniques and from which were extracted frequent itemsets by quarters and evaluated in terms of quantity. The authors have proposed a methodology to evaluate frequent itemsets of web parts over a dedicated time period. The results show that stakeholders’ interest in disclosures is lower after turbulent times in 2009, higher in the first quarter, also higher together with annual reports (lower for Pillar 3 solo information). The paper's results suggest that further changes in commercial banks´ information disclosure are inevitable in order to achieve an effective market discipline mechanism and meaningful disclosures according to the regulator´s expectations. © 2017 Vilnius Gediminas Technical University (VGTU) Press.","data pre-processing; financial regulation; market discipline; Pillar 3; risk management; web usage mining",,2-s2.0-85032485414
"Ewenighi C.O., Dimkpa U., Onyeanusi J.C., Babtunde A., Onoh L.U.M., Onoh G.O., Ezeugwu U.","Prostate-specific antigen and its derivatives in young adults occupationally exposed to quarry pollutants in southeastern Nigeria",2017,"Archives of Environmental and Occupational Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979971364&doi=10.1080%2f19338244.2016.1207593&partnerID=40&md5=f57d4df85d6182720f454badca2fb68a","To evaluate the level of total prostate-specific antigen (PSA) and effect of duration of exposure to pollutants in quarry workers, 5 mls of blood sample was collected from participants: 72 male quarry workers exposed to quarry pollutants and 72 unexposed controls. PSA estimations were done using the principle of ELISA. Mean total PSA, free PSA, and free-total PSA ratio levels of quarry workers did not differ from those of controls and was below the cutoff for the risk of prostate diseases. Higher mean total PSA and free PSA were observed in workers exposed for > 3 years compared to the unexposed control and workers exposed for ≤ 3 years. Age-adjusted linear regression indicated significant association (R = 0.515; p <.001) between the duration of exposure and total PSA level in quarry workers. This study suggests that longer duration of exposure to the quarry pollutants may elevate PSA level if precautions are not taken to minimize dose of exposure. © 2017 Taylor & Francis.","Ebonyi State; pollutants; prostate dysfunction; prostate-specific antigen; quarry; southeastern Nigeria","prostate specific antigen; air pollutant; prostate specific antigen; adult; Article; blood sampling; controlled study; cross-sectional study; enzyme linked immunosorbent assay; human; industrial worker; major clinical study; male; Nigeria; occupational exposure; pollutant; priority journal; prostate disease; young adult; adverse effects; air pollutant; blood; case control study; epidemiology; mining; occupational exposure; statistics and numerical data; time factor; Adult; Air Pollutants; Case-Control Studies; Humans; Male; Mining; Nigeria; Occupational Exposure; Prostate-Specific Antigen; Time Factors; Young Adult",2-s2.0-84979971364
"Westfall P.H., Arias A.L., Fulton L.V.","Teaching Principal Components Using Correlations",2017,"Multivariate Behavioral Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024399150&doi=10.1080%2f00273171.2017.1340824&partnerID=40&md5=2a040797dca0f642237e557b828f6e2d","Introducing principal components (PCs) to students is difficult. First, the matrix algebra and mathematical maximization lemmas are daunting, especially for students in the social and behavioral sciences. Second, the standard motivation involving variance maximization subject to unit length constraint does not directly connect to the “variance explained” interpretation. Third, the unit length and uncorrelatedness constraints of the standard motivation do not allow re-scaling or oblique rotations, which are common in practice. Instead, we propose to motivate the subject in terms of optimizing (weighted) average proportions of variance explained in the original variables; this approach may be more intuitive, and hence easier to understand because it links directly to the familiar “R-squared” statistic. It also removes the need for unit length and uncorrelatedness constraints, provides a direct interpretation of “variance explained,” and provides a direct answer to the question of whether to use covariance-based or correlation-based PCs. Furthermore, the presentation can be made without matrix algebra or optimization proofs. Modern tools from data science, including heat maps and text mining, provide further help in the interpretation and application of PCs; examples are given. Together, these techniques may be used to revise currently used methods for teaching and learning PCs in the behavioral sciences. © 2017 Taylor & Francis Group, LLC.","Factor analysis; heat map; optimality; rotation; variance explained","behavioral science; covariance; factor analysis; heat; human; human experiment; learning; mathematics; mining; motivation; oblique rotation; student; teaching",2-s2.0-85024399150
"Zhou F., Qu Q., Toivonen H.","Summarisation of weighted networks",2017,"Journal of Experimental and Theoretical Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012082486&doi=10.1080%2f0952813X.2017.1280089&partnerID=40&md5=fdcb2c75b762847eacee7a8ca5d0367f","Networks often contain implicit structure. We introduce novel problems and methods that look for structure in networks, by grouping nodes into supernodes and edges to superedges, and then make this structure visible to the user in a smaller generalised network. This task of finding generalisations of nodes and edges is formulated as ‘network Summarisation’. We propose models and algorithms for networks that have weights on edges, on nodes or on both, and study three new variants of the network summarisation problem. In edge-based weighted network summarisation, the summarised network should preserve edge weights as well as possible. A wider class of settings is considered in path-based weighted network summarisation, where the resulting summarised network should preserve longer range connectivities between nodes. Node-based weighted network summarisation in turn allows weights also on nodes and summarisation aims to preserve more information related to high weight nodes. We study theoretical properties of these problems and show them to be NP-hard. We propose a range of heuristic generalisation algorithms with different trade-offs between complexity and quality of the result. Comprehensive experiments on real data show that weighted networks can be summarised efficiently with relatively little error. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","generalisation; network mining; network summarisation; Weighted networks","Economic and social effects; Heuristic algorithms; Edge weights; Generalisation; In networks; Models and algorithms; Node-based; Path-based; Supernodes; Weighted networks; Complex networks",2-s2.0-85012082486
"Fu X., Ma M., Jiang P., Quan Y.","Spatiotemporal vegetation dynamics and their influence factors at a large coal-fired power plant in Xilinhot, Inner Mongolia",2017,"International Journal of Sustainable Development and World Ecology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007319691&doi=10.1080%2f13504509.2016.1273265&partnerID=40&md5=66130b2d9ab96bd98685c8c73869672d","We monitored the vegetation dynamics in a large coal-fired power plant and investigated the factors influencing these dynamics. The findings improve the understanding of the impact of climate change and human activities on vulnerable ecosystems and contribute to the sustainable development of these regions. We used normalized difference vegetation index (NDVI) images derived from a 16-day maximum-value composite of the Moderate Resolution Imaging Spectroradiometer data and employed the rate of change in greenness and coefficient of variation as indicators of the vegetation dynamics in Xilinhot, Inner Mongolia, from 2001 to 2013. We investigated the driving factors of vegetation dynamics in regions with different vegetation variations based on correlation and stepwise regression analyses. The results show that the vegetation dynamics of the study area have improved over the past 13 years. The vegetation dynamics were highly correlated with the average relative humidity of the current month. That is, the vegetation dynamics in the study area were mainly influenced by meteorological factors. Some regions suffered from vegetation degradation, accounting for 15.27% of the total area. The NDVI values of regions displaying vegetation degradation were jointly influenced by meteorological and human factors. The degree of vegetation degradation was negatively correlated with the population engaged in secondary industry. This indicates that industrial and mining development was the primary cause of vegetation degradation. However, the development of animal husbandry (characterized by the number of livestock on hand at the end of the year) had no significant influence on the vegetation dynamics. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","dynamic monitoring; large coal-fired power plant; normalized difference vegetation index (NDVI); Vegetation degradation","animal husbandry; anthropogenic effect; climate change; coal-fired power plant; environmental degradation; environmental factor; environmental monitoring; human activity; MODIS; NDVI; relative humidity; spatiotemporal analysis; vegetation dynamics; China; Nei Monggol; Xilinhaote; Animalia",2-s2.0-85007319691
"Tan K.H., Ji G., Lim C.P., Tseng M.-L.","Using big data to make better decisions in the digital economy",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019630673&doi=10.1080%2f00207543.2017.1331051&partnerID=40&md5=98e7fe5a6e22713c9018dfa9762a72e0","The question this special issue would like to address is how to harvest big data to help decision-makers to deliver better fact-based decisions aimed at improving performance or to create better strategy? This special issue focuses on the big data applications in supporting operations decisions, including advanced research on decision models and tools for the digital economy. Responds to this special issue was great and we have included many high-quality papers. We are pleased to present 13 of the best papers. The techniques presented include data mining, simulation and expert system with applications span across online reviews, food retail chain to e-health. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","big data; business analytics; data mining; decision support systems; digital economy","Artificial intelligence; Data mining; Decision making; Decision support systems; Expert systems; Advanced researches; Big data applications; Business analytics; Decision makers; Decision models; Digital economy; High quality papers; Improving performance; Big data",2-s2.0-85019630673
"Chien C.-F., Liu C.-W., Chuang S.-C.","Analysing semiconductor manufacturing big data for root cause detection of excursion for yield enhancement",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947263268&doi=10.1080%2f00207543.2015.1109153&partnerID=40&md5=ce20a4675e3f368917359def0f0b742a","With the shrinking feature size of integrated circuits driven by continuous technology migrations for wafer fabrication, the control of tightening critical dimensions is critical for yield enhancement, while physical failure analysis is increasingly difficult. In particular, the yield ramp up stage for implementing new technology node involves new production processes, unstable machine configurations, big data with multiple co-linearity and high dimensionality that can hardly rely on previous experience for detecting root causes. This research aims to propose a novel data-driven approach for Analysing semiconductor manufacturing big data for low yield (namely, excursions) diagnosis to detect process root causes for yield enhancement. The proposed approach has shown practical viability to efficiently detect possible root causes of excursion to reduce the trouble shooting time and improve the production yield effectively. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","big data; co-linearity; data mining; excursion; outlier detection; semiconductor manufacturing","Data mining; Failure analysis; Industrial research; Manufacture; Semiconductor device manufacture; Data-driven approach; excursion; High dimensionality; Outlier Detection; Physical failure analysis; Semiconductor manufacturing; Shrinking feature sizes; Technology migration; Big data",2-s2.0-84947263268
"Chan H.K., Lacka E., Yee R.W.Y., Lim M.K.","The role of social media data in operations and production management",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930989072&doi=10.1080%2f00207543.2015.1053998&partnerID=40&md5=ea18ecb675d56ce77b380096d553bcce","Social media data contain rich information in posts or comments written by customers. If those data can be extracted and analysed properly, companies can fully utilise this rich source of information. They can then convert the data to useful information or knowledge, which can help to formulate their business strategy. This cannot only facilitate marketing research in view of customer behaviour, but can also aid other management disciplines. Operations management (OM) research and practice with the objective to make decisions on product and process design is a fine example. Nevertheless, this line of thought is under-researched. In this connection, this paper explores the role of social media data in OM research. A structured approach is proposed, which involves the analysis of social media comments and a statistical cluster analysis to identify the interrelationships amongst important factors. A real-life example is employed to demonstrate the concept. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","cluster analysis; content analysis; operations management; social media","Cluster analysis; Data mining; Marketing; Product design; Social networking (online); Content analysis; Customer behaviour; Marketing research; Operations and production management; Operations management; Social media; Social media datum; Structured approach; Data handling",2-s2.0-84930989072
"Menon A., Gaglani S., Haynes M.R., Tackett S.","Using “big data” to guide implementation of a web and mobile adaptive learning platform for medical students",2017,"Medical Teacher",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019193675&doi=10.1080%2f0142159X.2017.1324949&partnerID=40&md5=79b9e9a8e07f97e0401b0f37e6057164","Background: Adaptive learning platforms (ALPs) can revolutionize medical education by making learning more efficient, but their potential has not been realized because students do not use them persistently. Methods: We applied educational data mining methods to study United States medical students who used an ALP called Osmosis (www.osmosis.org) from 1 August 2014 to 31 July 2015. Multivariate logistic regressions modeled persistence on Osmosis as the dependent variable and Osmosis-collected variables as predictors. Results: The 6787 students included in our analysis responded to a total of 887,193 items, with 2138 (31.5%) using Osmosis persistently. Number of items per student, mobile device use, subscription payment, and group membership were independently associated with persisting (p < 0.001 in all models). Persistent users rated quality more favorably (p < 0.01) but were not more confident in answer selections (p = 0.80). While persisters were more accurate than non-persisters (55% (SD 18%) vs 52% (SD 22%), p < 0.001), after adjusting for number of items, lower accuracy was associated with persistent use (OR 0.93 [95% CI 0.90–0.97], p < 0.01). Conclusions: Our study of a large sample of U.S. medical students illustrates big data medical education research and provides guidance for improving implementation of ALPs and further investigation. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"data mining; dependent variable; human; learning; logistic regression analysis; major clinical study; medical education; medical student; model; osmosis; United States",2-s2.0-85019193675
"Chiang W.-Y.","Discovering customer value for marketing systems: an empirical case study",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987861938&doi=10.1080%2f00207543.2016.1231429&partnerID=40&md5=4683c3879f3cbe5aa84e7d66590a0a5e","Data mining technologies have been employed in a variety of business managements for discovering useful commercial knowledge or marketing model for many years. Hence, the major marketing issue for airlines is to identify and analyse valuable air travellers recently, so that airlines can attract them for enhancing the profits and growth rates. However, growth rates are always an important issue for airline industries. An empirical case of air travellers’ markets in Taiwan is implemented in this research. This research proposes a model (FSLC model, RFM model based) via the data mining technologies to discover valuable travellers for airlines. This study partitions the market of air travellers in Taiwan, and the paper generates useful association rules to find an optimised target market for dynamic marketing or CRM systems. Nevertheless, the results of this research can be applied on marketing or CRM systems of the airline industry for identifying valuable travellers. Finally, the purpose of this research is to find high-value markets for marketing or CRM systems of airlines in Taiwan, and the framework can be applied to other industries as well. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","association rules; CRM systems; data mining; FSLC model; marketing systems; RFM model","Air transportation; Association rules; Data mining; Marketing; Airline industry; Business management; CRM systems; Customer values; Data mining technology; Empirical case studies; Marketing models; Rfm models; Commerce",2-s2.0-84987861938
"Deng M., Cai J., Liu Q., He Z., Tang J.","Multi-level method for discovery of regional co-location patterns",2017,"International Journal of Geographical Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020661034&doi=10.1080%2f13658816.2017.1334890&partnerID=40&md5=ddd2e53584808c48edf1e79e8aa73864","Regional co-location patterns represent subsets of feature types that are frequently located together in sub-regions in a study area. These sub-regions are unknown a priori, and instances of these co-location patterns are usually unevenly distributed across a study area. Regional co-location patterns remain challenging to discover. This study developed a multi-level method to identify regional co-location patterns in two steps. First, global co-location patterns were detected, and other non-prevalent co-location patterns were identified as candidates for regional co-location patterns. Second, an adaptive spatial clustering method was applied to detect the sub-regions where regional co-location patterns are prevalent. To improve computational efficiency, an overlap method was developed to deduce the sub-regions of (k + 1)-size co-location patterns from the sub-regions of k-size co-location patterns. Experiments based on both synthetic and ecological data sets showed that the proposed method is effective in the detection of regional co-location patterns. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","adaptive spatial clustering; multi-level; regional co-location patterns; spatial data mining; Spatial heterogeneity","data mining; data set; efficiency measurement; experimental study; heterogeneity; location-allocation model; spatial analysis",2-s2.0-85020661034
"Kim K.-J., Ahn H.","Recommender systems using cluster-indexing collaborative filtering and social data analytics",2017,"International Journal of Production Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011827183&doi=10.1080%2f00207543.2017.1287443&partnerID=40&md5=b817963387fa7ea7ad15db9e8b427be2","As a result of the extensive variety of products available in e-commerce settings during the last decade, recommender systems have been highlighted as a means of mitigating the problem of information overload. Collaborative filtering (CF) is the most widely used algorithm to build such systems, and improving the predictive accuracy of CF-based recommender systems has been a major research challenge. This research aims to improve the prediction accuracy of CF by incorporating social network analysis (SNA) and clustering techniques. Our proposed model identifies the most influential people in an online social network by SNA and then conducts clustering analysis using these people as initial centroids (cluster centres). Finally, the model makes recommendations using cluster-indexing CF based on the clustering outcomes. In this step, our model adjusts the effect of neighbours in the same cluster as the target user to improve prediction accuracy by reflecting hidden information about his or her social community. The experimental results indicate that the proposed model outperforms other comparison models, including conventional CF, with statistical significance. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","business analytics; cluster-indexing collaborative filtering; data mining; recommender system; social network","Data mining; Electronic commerce; Indexing (of information); Recommender systems; Social networking (online); Business analytics; Clustering techniques; Information overloads; On-line social networks; Prediction accuracy; Predictive accuracy; Research challenges; Statistical significance; Collaborative filtering",2-s2.0-85011827183
"van der Spoel S., Amrit C., van Hillegersberg J.","Predictive analytics for truck arrival time estimation: a field study at a European distribution centre",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937783388&doi=10.1080%2f00207543.2015.1064183&partnerID=40&md5=d29031e2dfd67284f5c516badbd6dbe5","Distribution centres (DCs) are the hubs connecting transport streams in the supply chain. The synchronisation of coming and going cargo at a DC requires reliable arrival times. To achieve this, a reliable method to predict arrival times is needed. A literature review was performed to find the factors that are reported to predict arrival time: congestion, weather, time of day and incidents. While travel time receives considerable attention, there is a gap in literature concerning arrival vs. travel/journey time prediction. None of the reviewed papers investigate arrival time: all the papers found investigate travel time. Arrival time is the consequence of travel time in combination with departure time, so though the travel time literature is applicable, the human factor involved in planning the time of departure can affect the arrival time (especially for truck drivers who have travelled the same route before). To validate the factors that influence arrival time, the authors conducted a detailed case study that includes a survey of 230 truckers, a data analysis and a data mining experiment, using real traffic and weather data. These show that although a ‘big data’ approach delivers valuable insights, the predictive power is not as high as expected; other factors, such as human or organisational factors, could influence arrival time, and it is concluded that such organisational factors should be considered in future predictive models. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","arrival time; case study; distribution centre; predictive analytics","Balloons; Data mining; Forecasting; Mine trucks; Supply chains; Traffic congestion; Travel time; Truck drivers; Trucks; Warehouses; Arrival time; Distribution centers; Distribution centres; Literature reviews; Organisational factors; Predictive analytics; Predictive models; Transport streams; Big data",2-s2.0-84937783388
"Mao Y., Zhong H., Qi H., Ping P., Li X.","An adaptive trajectory clustering method based on grid and density in mobile pattern analysis",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028693488&doi=10.3390%2fs17092013&partnerID=40&md5=ccb7e4918f0ca86426404f4935552e88","Clustering analysis is one of the most important issues in trajectory data mining. Trajectory clustering can be widely applied in the detection of hotspots, mobile pattern analysis, urban transportation control, and hurricane prediction, etc. To obtain good clustering performance, the existing trajectory clustering approaches need to input one or more parameters to calibrate the optimal values, which results in a heavy workload and computational complexity. To realize adaptive parameter calibration and reduce the workload of trajectory clustering, an adaptive trajectory clustering approach based on the grid and density (ATCGD) is proposed in this paper. The proposed ATCGD approach includes three parts: partition, mapping, and clustering. In the partition phase, ATCGD applies the average angular difference-based MDL (AD-MDL) partition method to ensure the partition accuracy on the premise that it decreases the number of the segments after the partition. During the mapping procedure, the partitioned segments are mapped into the corresponding cells, and the mapping relationship between the segments and the cells are stored. In the clustering phase, adopting the DBSCAN-based method, the segments in the cells are clustered on the basis of the calibrated values of parameters from the mapping procedure. The extensive experiments indicate that although the results of the adaptive parameter calibration are not optimal, in most cases, the difference between the adaptive calibration and the optimal is less than 5%, while the run time of clustering can reduce about 95%, compared with the TRACLUS algorithm. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Adaptive parameter calibration; Grid; Mobile pattern analysis; Spatio-temporal data; Trajectory clustering","Calibration; Data mining; Mapping; Trajectories; Urban transportation; Adaptive parameters; Grid; Mobile patterns; Spatio-temporal data; Trajectory clustering; Cluster analysis",2-s2.0-85028693488
"Malgieri G., Comandé G.","Sensitive-by-distance: quasi-health data in the algorithmic era",2017,"Information and Communications Technology Law",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020272912&doi=10.1080%2f13600834.2017.1335468&partnerID=40&md5=1435ef0eccca9ecdcabcfc1655fd4ace","This article offers a new perspective on the boundaries between health and non-health data in the age of ‘Quantified-Self’ apps: the ‘data-sensitiveness-by-computational-distance’ approach-or, more simply, the ‘sensitive-by-distance’ approach. This approach takes into account two variables: the intrinsic sensitiveness (a static variable) of personal data and the computational distance (a dynamic variable) between some kinds of personal data and pure health (or sensitive) data, which depends upon computational capacity. From an objective perspective, computational capacity depends on the level of development of data retrieval technologies at a certain moment, the availability of ‘accessory data’, and the applicable legal restraints on processing data. From a subjective perspective, computational capacity depends on the specific data mining efforts (or the ability to invest in them) taken by a given data controller: economic resources, human resources, and the use of accessory data. A direct consequence of the expansion of augmented humanity in collecting and inferring personal data is the increasing loss of health data processing ‘legibility’ for data subjects. In order to address this issue, we propose exploiting the existing legal tools in the General Data Protection Regulation to empower data subjects (the right to data access, the right to know the logic involved in automated decision-making, data portability, etc.). © 2017 Informa UK Limited, trading as Taylor & Francis Group.","algorithm; data protection; health data; legibility; Privacy; sensitive data",,2-s2.0-85020272912
"Tsai F.-M., Huang L.J.W.","Using artificial neural networks to predict container flows between the major ports of Asia",2017,"International Journal of Production Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949752872&doi=10.1080%2f00207543.2015.1112046&partnerID=40&md5=05b2d811c80e88fe164cc9e233f2bd7b","Container flow information is a critical issue for port operators and liners to support their strategic planning and decision-making. This study uses artificial neural networks (ANNs) to predict container flows by considering GDP, interest rates, the value of export and import trade, the numbers of export and import containers and the number of quay cranes. ANNs are developed for data mining purposes, and the developed model can simultaneously predict container flows between the major ports of Asia. The forecasting results indicate that the prediction errors are relatively small in most selected ports, and thus shipping companies can use the container flow prediction model to make decisions concerning operations. The results can be further applied to the trend analysis of container flows among the major ports of Asia, and a community analysis of the containers was conducted for the purpose of supply chain management. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","artificial neural networks; container flows; decision-making; ports of Asia; supply chain management","Containers; Data mining; Forecasting; Information dissemination; Neural networks; Supply chain management; Community analysis; Container flow; Critical issues; Developed model; Interest rates; ports of Asia; Prediction errors; Shipping companies; Decision making",2-s2.0-84949752872
"Bajorath J.","Representation and identification of activity cliffs",2017,"Expert Opinion on Drug Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023775488&doi=10.1080%2f17460441.2017.1353494&partnerID=40&md5=1beeb2a8c31df0c764f74b242253b198",[No abstract available],"Active compounds; activity cliffs; compound data mining; molecular similarity; potency differences; structure-activity relationships","thrombin inhibitor; activity cliffs; biological activity; chemical modification; chemical structure; drug potency; Editorial; medicinal chemistry; metabolic stability; pharmacological parameters; priority journal; structure activity relation",2-s2.0-85023775488
"Xie P., Li W., Yang D., Jiao J.","Hydrogeological Model for Groundwater Prediction in the Shennan Mining Area, China [神南矿区(中国)水文地质模型] [Hydrogeologisches Modell zur zukünftigen Grundwasserabsenkung infolge der Kohlegewinnung im Shennan Bergbaudistrikt, nördliches Shaanxi, China] [Modelo hidrogeológico para la predicción de agua subterránea en la región minera Shennan, China]",2017,"Mine Water and the Environment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028759107&doi=10.1007%2fs10230-017-0490-0&partnerID=40&md5=6f7c1e8f1590a25776b54b50c0419abe","Coal mining can seriously affect groundwater systems in aquifers that overlie the coal seam, especially in dry and water-stressed areas where protection of groundwater resources is very important. Through generalization of the hydrogeological conditions and analysis of the actual groundwater flow field in a confined weathered bedrock aquifer overlying the Shennan mining area in northern Shaanxi, a hydrogeological conceptual model and numerical groundwater flow model were established. FEFLOW finite element software was used to solve the model and dynamic groundwater data were used to validate it. The study area was hydrogeologically modeled by repeatedly adjusting the parameters. The model was then used to simulate the effect of mining on the overlying aquifer based on the mining plan for the next 5 years by adjusting the quantity of water discharged, the hydrogeological parameters of the upper water-bearing zone, the characteristics of the groundwater flow field, and the predicted water balance after 5 years. The results show that the maximum drawdown could be as high as 50 m (northeast of the Zhangjiamao Mine). A cone of depression centered on the Ningtiaota, Zhangjiamao, and Hongliulin mines will be formed that will influence more than 75% of the simulation area. © 2017 Springer-Verlag GmbH Germany","Coal mining; FEFLOW; Finite element; Groundwater; Mine water",,2-s2.0-85028759107
"Deonandan K., Tatham R., Field B.","Indigenous women’s anti-mining activism: a gendered analysis of the El Estor struggle in Guatemala",2017,"Gender and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032823647&doi=10.1080%2f13552074.2017.1379779&partnerID=40&md5=e0cea4be091eee38a89d3c74f202f435","Focusing on the struggle against the Fenix mine in El Estor Guatemala, this article argues that women are disproportionately affected by resource development; and that women’s activism against mining is also gendered, in the sense that they are often distinct from men’s strategies and are rooted in women’s experiences as women, and as indigenous women within a particular socioeconomic and historical context. We draw on original data gathered from interviews with indigenous women activists in the El Estor communities in Guatemala. © 2017 Oxfam GB.","activism; effects; gender; Guatemala; mining; Women",,2-s2.0-85032823647
"Ren L., Wei Y., Cui J., Du Y.","A sliding window-based multi-stage clustering and probabilistic forecasting approach for large multivariate time series data",2017,"Journal of Statistical Computation and Simulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014567420&doi=10.1080%2f00949655.2017.1299151&partnerID=40&md5=a6c880a42cfbdf02dbf779705d39a883","Time series data analysis, such as temporal pattern recognition and trend forecasting, plays an increasingly significant part in temporal data statistics and analytics. Yet challenges still exist in the efficiency of pattern extracting and trend prediction for large multivariate time series. The paper proposes a multi-stage clustering approach towards multivariate time series by using dynamic sliding time windows. The segmented multivariate time series are clustered separately in each time window to product first-stage clustering centres, and which are used to generate second-stage clustering results involving all time windows. The method can simplify large scale multivariate time series mining problems through multi-stage clustering on multiple sliding time windows thus achieve improved efficiency. Based on the clustering outcomes, a correlation rules mining method is given to discover frequent patterns in the time series and generate self-correlation rules. Then, the paper presents a probabilistic forecasting model that leverages the extracted rules to make short-term predictions. Finally, experiments are presented to show the efficiency and effectiveness of the proposed approach. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","dynamic sliding time window; multi-stage clustering; statistics; Time series; time series forecasting model",,2-s2.0-85014567420
"Li F., Chang X., Yang H., Xu Z.","Study on the Electrospinnability of Polyvinyl Alcohol Solutions by Using Water/N, N-dimethylacetamide or Water/N, N-dimethylformamide as Solvents",2017,"Journal of Macromolecular Science, Part B: Physics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029420423&doi=10.1080%2f00222348.2017.1361284&partnerID=40&md5=feefb485d6673fcd5368ed986eda45d6","The electrospinning of poly(vinyl alcohol) (PVA) (99% hydrolysis degree) aqueous solution with added organic solvents N, N-dimethylacetamide (DMAc) or N, N-Dimethylformamide (DMF) was investigated. After the addition of the organic solvents to the PVA aqueous solutions, the surface tension and conductivity decreased and the viscosity significantly increased, which caused an improved electrospinnability of the PVA solutions. The micro-structures of the three solutions were investigated by dynamic light scattering (DLS), differential scanning calorimetry (DSC) and dynamic viscoelastic measurements. The DLS data revealed that the swelling of the PVA coils was slightly increased but the overlaps of PVA coils decreased greatly after one of the organic solvents was added to the aqueous solution. The DSC data showed both the water-rich phase and PVA-rich phase were destroyed and the solution became more homogenous after the addition due to the interaction between the organic solvent and water. Viscoelastic data showed there was an obvious difference in the storage modulus behavior between the aqueous solutions and the water/solvents solutions. These changes in the micro-structure and properties were the reason for the improved electrospinnability of the PVA solution. According to scanning electron microscopy (SEM), the average diameter of the electrospun PVA nanofibers was about 308 nm for the DMF/water system, and 255 nm for the DMAC/water system, as compared with uneven diameter nanofibers for the water system. © 2017 Taylor & Francis Group, LLC.","electrospinning; micro-structure; nanofibers; poly(vinyl alcohol); water/organic solvent","Differential scanning calorimetry; Digital storage; Dimethylformamide; Dynamic light scattering; Electrospinning; Light scattering; Microstructure; Nanofibers; Organic solvents; Polyvinyl alcohols; Scanning electron microscopy; Solutions; Solvents; Spinning (fibers); Viscoelasticity; Electrospinnability; Hydrolysis degree; N ,N-Dimethylacetamide; N ,N-Dimethylformamide; N-dimethyl formamide; Poly (vinyl alcohol) (PVA); Polyvinyl alcohol solution; Viscoelastic measurements; Solution mining",2-s2.0-85029420423
"Ryu J., Lee D., Shin K.G., Kang K.","ClusterFetch: A Lightweight Prefetcher for Intensive Disk Reads",2017,"IEEE Transactions on Computers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029175638&doi=10.1109%2fTC.2017.2748939&partnerID=40&md5=f7d7d79f5fa4675722afb082b1a796c6","By overlapping disk accesses with computation-intensive operations, prefetching can reduce delays in launching an application and in loading significant amounts of data while the application is running. The key to effective prefetching is making the tradeoff between the mining accuracy of selecting relevant blocks, and the time to decide those blocks. To address this problem, we propose a new prefetcher called ClusterFetch. In its learning mode, ClusterFetch detects periods of intensive disk accesses by monitoring the speed at which read requests are queued; it re-organizes these reads and locates the file opened by the application just before each such period. During subsequent runs of the same application, ClusterFetch prefetches the data associated with the opening of a &#x201C;trigger&#x201D; file. Our experimental results show that ClusterFetch implemented in Linux can reduce the application launch time by up to 41.3% and the loading time by up to 38.2%, while taking up less than 200KB of main memory. IEEE","Clustering algorithms; Correlation; Libraries; Linux; Loading; Memory management; Prefetching",,2-s2.0-85029175638
"Lago J., Guagliano M., Bokůvka O., Trško L., Řidký O., Nový F., Závodská D.","Improvement of fatigue endurance of welded S355 J2 structural steel by severe shot peening",2017,"Surface Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011271396&doi=10.1080%2f02670844.2017.1279858&partnerID=40&md5=7e95832215b6c058749e6da581842cad","This work deals with the fatigue resistance of welded joints manufactured from EN S355 J2 structural steel and the possibility of improving fatigue characteristics by the severe shot peening (SSP) surface treatment. Fatigue testing was carried out by the rotating bending test on specimens manufactured from the base material (BM), welded material and welded material treated by SSP. Results have shown big scatter in obtained results from as-welded material and overall reduction of fatigue endurance when compared to the BM. SSP applied on the welded joints has reduced the scatter of experimental data and increased the fatigue strength. Obtained results of fatigue tests are compared, discussed and supported by correlation with results of additional experiments, e.g. metallographic analyses, microhardness tests, residual stresses measurements and surface roughness measurement. © 2017 Institute of Materials, Minerals and Mining Published by Taylor & Francis on behalf of the Institute.","endurance; fatigue; improving; severe-shot-peening; Steel; weld","Building materials; Durability; Fatigue of materials; Fatigue testing; Metallography; Shot peening; Steel; Steel construction; Surface roughness; Surface treatment; Welded steel structures; Welds; Additional experiments; Fatigue characteristics; Fatigue endurances; improving; Metallographic analysis; Microhardness tests; Stresses measurements; Structural steels; Welding",2-s2.0-85011271396
"Kan B., Yang Z.X., Wang Z., Li J.X., Zhou Q.J., Su Y.J., Qiao L.J., Volinsky A.A.","Hydrogen redistribution under stress-induced diffusion and corresponding fracture behaviour of a structural steel",2017,"Materials Science and Technology (United Kingdom)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019541598&doi=10.1080%2f02670836.2017.1325562&partnerID=40&md5=0ae66bc96770a96dca677a9cac490451","Hydrogen redistribution under stress-induced hydrogen diffusion and corresponding fracture behaviour of a 960 MPa grade martensitic steel were studied. Slow strain rate tensile (SSRT) tests after hydrogen pre-charging were performed and the fracture surface was observed and analysed. The strain rate ranged from 10−6 to 10−4 s−1. In the pre-charged sample with a certain hydrogen content of 0.62 ppm, hydrogen distribution was homogeneous before the SSRT test. After tensile testing, brittle fracture features appeared in the centre of the fracture surface, while ductile features appeared in the surrounding area. Brittle region size increased with the strain rate slowing down in the range from 10−4 to 5 × 10−6 s−1, while it stabilised at the strain rate slower than 5 × 10−6 s−1. Relationship between the strain rate and the brittle region size was established and discussed based on the present data of hydrogen content in the material. This paper is part of a thematic issue on Hydrogen in Metallic Alloys. © 2017 Institute of Materials, Minerals and Mining.","brittle region size; diffusion; hydrogen pre-charging; hydrogen redistribution; Slow strain rate test; structural steel","Brittle fracture; Building materials; Diffusion; Ductile fracture; Fracture; Fracture mechanics; Fracture testing; Hydrogen; Martensitic steel; Steel construction; Tensile strength; Tensile testing; Brittle region; Fracture behaviour; Hydrogen diffusion; Hydrogen distribution; Slow strain rate tensile test; Slow strain rate tests; Stress-induced diffusion; Structural steels; Strain rate",2-s2.0-85019541598
"Parkhimenka U., Tatur M., Zhvakina A.","Heuristic approach to online purchase prediction based on internet store visitors classification using data mining methods",2017,"Proceedings of the International Conference on Information and Digital Technologies, IDT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030100320&doi=10.1109%2fDT.2017.8024313&partnerID=40&md5=b9daf18d39379c8d1bb8382c51a99fe6","Last years research gave some preliminary results in approaches to customer online purchase prediction. However, it still remains unclear what exact set of features of data instances should be incorporated in a model and is enough for prediction, what is the best data mining method (algorithm) to use, how stable over time could be such a model, whether a model is transferable from one online store to another. This study is focused on a heuristic approach to dealing with the problem under conditions of such theoretical and methodological diversity in order to find a quick and inexpensive first approximation to the solution or at least to find useful patterns and facts in the data. © 2017 IEEE.","automatic marketing decision-making; data mining & knowledge discovery; feature selection and design; online purchase prediction; statistical classification","Classification (of information); Data mining; Decision making; Electronic commerce; Forecasting; Sales; Data mining methods; Heuristic approach; Marketing decision; Online store; Prediction-based; Statistical classification; Useful patterns; Heuristic methods",2-s2.0-85030100320
"Asian S., Ertek G., Haksoz C., Pakter S., Ulun S.","Wind Turbine Accidents: A Data Mining Study",2017,"IEEE Systems Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973912649&doi=10.1109%2fJSYST.2016.2565818&partnerID=40&md5=e6a88992b07b8adccdc2945c9af06e4b","While the global production of wind energy is increasing, there exists a significant gap in the academic and practice literature regarding the analysis of wind turbine accidents. This paper presents the results obtained from the analysis of 240 wind turbine accidents from around the world. The main focus of this paper is revealing the associations between several factors and deaths and injuries in wind turbine accidents. Specifically, the associations of death and injuries with the stage of the wind turbine's life cycle (transportation, construction, operation, and maintenance) and the main cause factor categories (human, system/equipment, and nature) were studied. To this end, we conducted a detailed investigation that integrates exploratory and statistical data analysis and data mining methods. This paper presents a multitude of insights regarding the accidents and discusses implications for wind turbine manufacturers, engineering and insurance companies, and government organizations. © 2017 IEEE.","Accidents; data analysis; data mining; wind energy; wind power generation","Accidents; Data mining; Insurance; Life cycle; Wind power; Data mining methods; Global production; Government organizations; Insurance companies; Statistical data analysis; Wind turbines",2-s2.0-84973912649
"Alasadi S.A., Bhaya W.S.","Review of data preprocessing techniques in data mining",2017,"Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029819648&doi=10.3923%2fjeasci.2017.4102.4107&partnerID=40&md5=f0d04fb63c552259aeb4f1a3a5a95bba","Data mining is the process of extraction useful patterns and models from a huge dataset. These models and patterns have an effective role in a decision making task. Data mining basically depend on the quality of data. Raw data usually susceptible to missing values, noisy data, incomplete data, inconsistent data and outlier data. So, it is important for these data tobe processed before being mined. Preprocessing data is an essential step to enhance data efficiency. Data preprocessing is one of the most data mining steps which deals with data preparation and transformation of the dataset and seeks at the same time to make knowledge discovery more efficient. Preprocessing include several techniques like cleaning, integration, transformation and reduction. This study shows a detailed description of data preprocessing techniques which are used for data mining. © Medwell Journals, 2017.","Data mining; Data preprocessing; Data set; Dataset; KDD (Knowledge Discovery in Databases); Pattein",,2-s2.0-85029819648
"Senthil Kumar J., Meganathan S., Venkataraman V., Meena V.","A data mining approach to classify higher education sector data using Bayesian classifier",2017,"International Journal of Mechanical Engineering and Technology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029903484&partnerID=40&md5=252726819bb39933efaf9fe5df41d6c5","The Data Mining terminology is a proficient mechanism for the process of academic data intervention. The process of Mining in education sector data is called Educational Data Mining (EDM). EDM is pertained with formulating new methodologies to expose cognition from educational sector data and can utilized for making wise decision in the educational system. The existing software tools of data mining holds commercial pattern identification schemes and is useful for the data classification process in the domain based research centres. The existing data mining terminologies works with large datasets and offers efficient clusters and classification over the datasets. The proposed research work deals the data of educational system and effectively performs the classification process and predicts the class label ""Research Status"" of the educational sector data. For the prediction methodology the naive Bayesian classification procedure is to be incorporated in the proposed research work and the outcomes states that the Bayesian mechanism improves the classification accuracy and effectiveness of the Higher Education Sector data.","Data mining; Education sector data; Mining; Naive Bayesian classification; Predication",,2-s2.0-85029903484
"Chen Z.-W., Zhao Z.-Z., Yao J.-N., Han Y.","Spatial data mining strategy for expensive interval multi-objective optimization",2017,"Kongzhi yu Juece/Control and Decision",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031732810&doi=10.13195%2fj.kzyjc.2016.0873&partnerID=40&md5=545d43b4d14ca53015efcd40da4d1f7e","In this paper, an improved NSGA-II algorithm is proposed based on the principal curve modeling for solving the expensive interval multi-objective optimization with unknown objective function. Firstly, the proposed algorithm builds a K principal curve using the population data of the manifold distribution in decision space. Then, a new offspring is generated through interpolation and extension according to the built K principal curve, and the proposed strategy of offspring generation is more efficient than that of random offspring generation in the genetic algorithm. Finally, because of the absence of the crowding distance in objective space, the closest solutions before and after the candidate solution can be found based on the built K principal curve, so the solutions with same sequence are screened by crowding distance in decision space, thus the NSGA-II is improved. © 2017, Editorial Office of Control and Decision. All right reserved.","Interval programming; Multi-objective optimization; NSGA-II; Principal curve; Spatial data mining","Curve fitting; Data mining; Genetic algorithms; Optimization; Population statistics; Crowding distance; Improved NSGA-II; Interval programming; K principal curves; NSGA-II; Objective functions; Principal curve; Spatial data mining; Multiobjective optimization",2-s2.0-85031732810
"Lu W., Xiao R., Yang J., Li H., Zhang W.","Data mining-aided materials discovery and optimization",2017,"Journal of Materiomics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028878602&doi=10.1016%2fj.jmat.2017.08.003&partnerID=40&md5=ac0950155f69bff56ddb53b275f20c5f","Recent developments in data mining-aided materials discovery and optimization are reviewed in this paper, and an introduction to the materials data mining (MDM) process is provided using case studies. Both qualitative and quantitative methods in machine learning can be adopted in the MDM process to accomplish different tasks in materials discovery, design, and optimization. State-of-the-art techniques in data mining-aided materials discovery and optimization are demonstrated by reviewing the controllable synthesis of dendritic Co3O4 superstructures, materials design of layered double hydroxide, battery materials discovery, and thermoelectric materials design. The results of the case studies indicate that MDM is a powerful approach for use in materials discovery and innovation, and will play an important role in the development of the Materials Genome Initiative and Materials Informatics. © 2017 The Chinese Ceramic Society","Battery materials; Co3O4 superstructures; Data mining; Layered double hydroxide; Materials design; Materials genome initiative; Thermoelectric materials",,2-s2.0-85028878602
"Caballero D., Pérez-Palacios T., Caro A., Amigo J.M., Dahl A.B., ErsbØll B.K., Antequera T.","Prediction of pork quality parameters by applying fractals and data mining on MRI",2017,"Food Research International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021357311&doi=10.1016%2fj.foodres.2017.06.048&partnerID=40&md5=c8fcd4eaf9175cc8ab1d0fb9e88b7405","This work firstly investigates the use of MRI, fractal algorithms and data mining techniques to determine pork quality parameters non-destructively. The main objective was to evaluate the capability of fractal algorithms (Classical Fractal algorithm, CFA; Fractal Texture Algorithm, FTA and One Point Fractal Texture Algorithm, OPFTA) to analyse MRI in order to predict quality parameters of loin. In addition, the effect of the sequence acquisition of MRI (Gradient echo, GE; Spin echo, SE and Turbo 3D, T3D) and the predictive technique of data mining (Isotonic regression, IR and Multiple linear regression, MLR) were analysed. Both fractal algorithm, FTA and OPFTA are appropriate to analyse MRI of loins. The sequence acquisition, the fractal algorithm and the data mining technique seems to influence on the prediction results. For most physico-chemical parameters, prediction equations with moderate to excellent correlation coefficients were achieved by using the following combinations of acquisition sequences of MRI, fractal algorithms and data mining techniques: SE-FTA-MLR, SE-OPFTA-IR, GE-OPFTA-MLR, SE-OPFTA-MLR, with the last one offering the best prediction results. Thus, SE-OPFTA-MLR could be proposed as an alternative technique to determine physico-chemical traits of fresh and dry-cured loins in a non-destructive way with high accuracy. © 2017","Acquisition sequences; Image analysis; Loin; MLR; Non-destructive analysis; Quality traits","Data mining; Forecasting; Fractals; Image analysis; Linear regression; Quality control; Acquisition sequences; Correlation coefficient; Loin; Multiple linear regressions; Non-destructive analysis; Physico - chemical parameters; Physico-chemical traits; Predictive techniques; Parameter estimation",2-s2.0-85021357311
"Saltos G., Cocea M.","An Exploration of Crime Prediction Using Data Mining on Open Data",2017,"International Journal of Information Technology and Decision Making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021841110&doi=10.1142%2fS0219622017500250&partnerID=40&md5=835524feb0b420da6b896aa4b58e5b23","The increase in crime data recording coupled with data analytics resulted in the growth of research approaches aimed at extracting knowledge from crime records to better understand criminal behavior and ultimately prevent future crimes. While many of these approaches make use of clustering and association rule mining techniques, there are fewer approaches focusing on predictive models of crime. In this paper, we explore models for predicting the frequency of several types of crimes by LSOA code (Lower Layer Super Output Areas - an administrative system of areas used by the UK police) and the frequency of anti-social behavior crimes. Three algorithms are used from different categories of approaches: instance-based learning, regression and decision trees. The data are from the UK police and contain over 600,000 records before preprocessing. The results, looking at predictive performance as well as processing time, indicate that decision trees (M5P algorithm) can be used to reliably predict crime frequency in general as well as anti-social behavior frequency. © 2017 World Scientific Publishing Company.","Crime prediction; data mining; decision trees; instance-based learning; open data; regression",,2-s2.0-85021841110
"Rizal M.T., Yusof Y.","Application of data mining in forecasting graduates employment",2017,"Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029818313&doi=10.3923%2fjeasci.2017.4202.4207&partnerID=40&md5=461fc9626126b3dacc2d7570962539a5","Obtaining information on graduate employability is crucial to every higher education institution. This is because such data would provide insight on the effectiveness of the institution curriculum in preparing human capital for the market needs. To date, the MARA Professional College (KPM) in Malaysia relies on graduates to manually provide data on their employment. Such an approach is not reliable as not all graduates provide the information to the institution. This study presents the application of data mining techniques in forecasting the KPM graduates employment type. In data mining, there exist three main tasks; classification, clustering, and association mining. The aim of this study is to forecast whether a particular graduate will be ""employed"", ""unemployed"" or ""further study"" 6 months after the completion of his study. The undertaken experiments include the utilization of five data mining techniques, namely, the Naive Bayes, Logistic regression, multilayer perceptron, K-nearest neighbor and decision tree. Furthermore, the experimental setup-up is based on three types of data proportion (training-testing) 70-30, 80-20 and 90-10. Based on the obtained result, it is learned that the Logistic regression is the best classifier for the in-hand dataset. In particular, the classifier is at its best when the 80-20 proportion is adopted. The produced classification model will benefit the management of the college as it provides insight to the quality of graduates that they produce and how their curriculum can be improved to cater the needs from the industry. © Medwell Journals, 2017.","Data mining; Decision tree; Graduates employment; K-nearest neighbor; Logistic regression; Multilayer perceptron; Naive Bayes",,2-s2.0-85029818313
"Turra C., Dias de Lima M., Fernandes E.A.D.N., Bacchi M.A., Barbosa F., Jr., Barbosa R.","Multielement determination in orange juice by ICP-MS associated with data mining for the classification of organic samples",2017,"Information Processing in Agriculture",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020378710&doi=10.1016%2fj.inpa.2017.05.004&partnerID=40&md5=780acb984e07fddf7a469926c6b45dc2","The aim of this study was to discriminate organic from conventional orange juice based on chemical elements and data mining applications. A comprehensive sampling of organic and conventional oranges was carried out in Borborema, state of São Paulo, Brazil. The fruits of the variety Valencia (Citrus sinensis (L.) Osbeck) budded on Rangpur lime (Citrus limonia Osbeck) were analyzed. Eleven chemical elements were determined in 57 orange samples grown in organic and conventional systems. In order to classify these samples, data mining techniques (Support Vector Machine (SVM) and Multilayer Perceptron (MLP)) were combined with feature selection (F-score and chi-squared). SVM with chi-squared had a better performance compared with the other techniques because it reached 93.00% accuracy using only seven chemical components (Cu, Cs, Zn, Al, Mn, Rb and Sr), and correctly classified 96.73% of the samples grown in an organic system. © 2017 China Agricultural University","Data mining; Inductively coupled plasma mass spectrometry (ICP-MS); Multilayer perceptron; Orange juice; Support vector machines; Trace elements",,2-s2.0-85020378710
"Khakshoor M.M., Pourbadakhshan K., Goshayeshi L.","Prediction of colorectal cancer tumor location using data mining",2017,"Govaresh",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031095868&partnerID=40&md5=3f86af1402e716c73947b272f5edb617","Background: Colorectal cancer is one of the most common cancers in terms of morbidity and mortality worldwide. A lot of research have been done in this field in Iran and worldwide, which have positive results. The aims of this study were firstly doing a statistical study on colorectal cancer in Mashhad, Iran, and finally predicting the colorectal location of cancer based on the clinical data by using data mining science and decision tree model. Materials and Methods: The data of 316 patients with colorectal cancer (including 14 features) were extracted from the archive of Imam Reza Hospital, Mashhad. The instrument used in this research was RapidMiner data mining software that would try to be extract the details of the relevant data by statistical surveys and then would do initial simulations and the use of classification and decision tree method have predicteion the location of cancer. Results: Male to female ratio of 56% to 44%, family history of 37%, more young patients, and relatively more distally located cancers (39%) compared with the proximal (35%), and rectum (26%) were the striking findings of this study. The final and most important stage of research models were presented, which was able to predict the location of the cancerous tumor with 80% accuracy. Conclusion: Similarities with global statistics, such as the ratio of men to women and family history were observed. But there were also differences with global statistics including the Iran's younger patients and relatively more patients with distal cancers. The efficiency of data mining techniques to predict the location of cancer as well as cost reduction was among the most important results of this study.","Colorectal cancer; Cost matrix; Data mining; Decision tree; Location; Predict",,2-s2.0-85031095868
"Ucar E., Ozhan E.","The Analysis of Firewall Policy Through Machine Learning and Data Mining",2017,"Wireless Personal Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019615776&doi=10.1007%2fs11277-017-4330-0&partnerID=40&md5=f76f98b308346204dd70802abbb16c66","Firewalls are primary components for ensuring the network and information security. For this purpose, they are deployed in all commercial, governmental and military networks as well as other large-scale networks. The security policies in an institution are implemented as firewall rules. An anomaly in these rules may lead to serious security gaps. When the network is large and policies are complicated, manual cross-check may be insufficient to detect anomalies. In this paper, an automated model based on machine learning and high performance computing methods is proposed for the detection of anomalies in firewall rule repository. To achieve this, firewall logs are analysed and the extracted features are fed to a set of machine learning classification algorithms including Naive Bayes, kNN, Decision Table and HyperPipes. F-measure, which combines precision and recall, is used for performance evaluation. In the experiments, kNN has shown the best performance. Then, a model based on the F-measure distribution was envisaged. 93 firewall rules were analysed via this model. The model anticipated that 6 firewall rules cause anomaly. These problematic rules were checked against the security reports prepared by experts and each of them are verified to be an anomaly. This paper shows that anomalies in firewall rules can be detected by analysing large scale log files automatically with machine learning methods, which enables avoiding security breaches, saving dramatic amount of expert effort and timely intervention. © 2017, Springer Science+Business Media New York.","Computer security; Firewall logs; Firewall rule; Machine learning","Artificial intelligence; Computer system firewalls; Computer viruses; Data mining; Decision tables; Learning systems; Network function virtualization; Security of data; Security systems; Automated modeling; Firewall logs; Firewall rules; High performance computing; Large-scale network; Machine learning classification; Machine learning methods; Precision and recall; Network security",2-s2.0-85019615776
"Ulaczyk J., Morawiec K., Zabierowski P., Drobiazg T., Barreau N.","Finding Relevant Parameters for the Thin-film Photovoltaic Cells Production Process with the Application of Data Mining Methods",2017,"Molecular Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019628471&doi=10.1002%2fminf.201600161&partnerID=40&md5=30e1f0647de67a8555dbe21e44062a97","A data mining approach is proposed as a useful tool for the control parameters analysis of the 3-stage CIGSe photovoltaic cell production process, in order to find variables that are the most relevant for cell electric parameters and efficiency. The analysed data set consists of stage duration times, heater power values as well as temperatures for the element sources and the substrate – there are 14 variables per sample in total. The most relevant variables of the process have been found based on the so-called random forest analysis with the application of the Boruta algorithm. 118 CIGSe samples, prepared at Institut des Matériaux Jean Rouxel, were analysed. The results are close to experimental knowledge on the CIGSe cells production process. They bring new evidence to production parameters of new cells and further research. © 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim","Data Mining; Fuzzy logic; Photovoltaics; Power sources; Semiconductors","molybdenum; algorithm; Article; artificial intelligence; boruta algorithm; current density; data mining; decision tree; electrical parameters; priority journal; quantum mechanics; random forest; solar cell; temperature",2-s2.0-85019628471
"Regulski K., Wilk-Kołodziejczyk D., Kacprzyk B., Gumienny G., Rojek G., Mrzygłód B.","Approximation of ausferrite content in the compacted graphite iron with the use of combined techniques of data mining",2017,"Archives of Foundry Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032203275&doi=10.1515%2fafe-2017-0102&partnerID=40&md5=8c87e1d809da2f98d6ddefdad22545b9","This article presents the methodology for exploratory analysis of data from microstructural studies of compacted graphite iron to gain knowledge about the factors favouring the formation of ausferrite. The studies led to the development of rules to evaluate the content of ausferrite based on the chemical composition. Data mining methods have been used to generate regression models such as boosted trees, random forest, and piecewise regression models. The development of a stepwise regression modelling process on the iteratively limited sets enabled, on the one hand, the improvement of forecasting precision and, on the other, acquisition of deeper knowledge about the ausferrite formation. Repeated examination of the significance of the effect of various factors in different regression models has allowed identification of the most important variables influencing the ausferrite content in different ranges of the parameters variability. © 2017 K. Regulski et al., published by De Gruyter Open.","Application of information technologies in the field of foundry; Ausferrite; Compacted graphite iron; Data mining; Regression","Decision trees; Graphite; Iron; Iterative methods; Regression analysis; Application of information technologies; Ausferrite; Chemical compositions; Compacted graphite iron; Exploratory analysis; Forecasting precision; Piecewise regression; Regression; Data mining",2-s2.0-85032203275
"Kong B., Yang K., Sun D., Li M., Shi Z.","Distinguishing flooding distributed denial of service from flash crowds using four data mining approaches",2017,"Computer Science and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031279008&doi=10.2298%2fCSIS161230032K&partnerID=40&md5=0a4fb0cb4246eb5910760d7a5aa9c016","Flooding Distributed Denial of Service (DDoS) attacks can cause significant damage to Internet. These attacks have many similarities to Flash Crowds (FCs) and are always difficult to distinguish. To solve this issue, this paper first divides existing methods into two categories to clarify existing researches. Moreover, after conducting an extensive analysis, a new feature set is concluded to profile DDoS and FC. Along with this feature set, this paper proposes a new method that employs Data Mining approaches to discriminate between DDoS attacks and FCs. Experiments are conducted to evaluate the proposed method based on two realworld datasets. The results demonstrate that the proposed method could achieve a high accuracy (more than 98%). Additionally, compared with a traditional entropy method, the proposed method still demonstrates better performance. © 2017, ComSIS Consortium. All rights reserved.","Data mining; Entropy; Flash crowds; Flooding DDoS",,2-s2.0-85031279008
"Chen W.-J., Tang R.-X., He R.-Q., Li D.-Y., Liang L., Zeng J.-H., Hu X.-H., Ma J., Li S.-K., Chen G.","Clinical roles of the aberrantly expressed lncRNAs in lung squamous cell carcinoma: A study based on RNA sequencing and microarray data mining",2017,"Oncotarget",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028772826&doi=10.18632%2foncotarget.18058&partnerID=40&md5=a042b5c8d1e36df68d672a05d5d848d1","Lung squamous cell carcinoma (LUSC) accounts for a significant proportion of lung cancer and there have been few therapeutic alternatives for recurrent LUSC due to the lack of specific driver molecules. To investigate the prospective role of lncRNAs in the tumorigenesis and progression of LUSC, the aberrantly expressed lncRNAs were calculated based on The Cancer Genome Atlas RNA-seq data. Of 7589 lncRNAs with 504 LUSC cases, 884 lncRNAs were identified as being aberrantly expressed (|log2 fold change| > 2 and adjusted P < 0.05) by DESeq R. The top 10 lncRNAs with the highest diagnostic value were SFTA1P,LINC00968, LINC00961, LINC01572,RP1-78O14.1, FENDRR, LINC01314,LINC01272, GATA6-AS1, and MIR3945HG. In addition to the significant roles in the carcinogenesis of LUSC, several lncRNAs also played vital parts in the survival and progression of LUSC. SFTA1P, LINC01272, GATA6-AS1 and MIR3945HG were closely related to the survival time of LUSC. Furthermore, LINC01572 and LINC01314 could distinguish the LUSC at early stage from that at advanced stage. The prospective molecular assessment of key lncRNAs showed that a certain series of genes could be involved in the regulation network. Furthermore, the OncoPrint from cBioPortal indicated that 14% (69/501) LUSC cases with genetic alterations could be obtained, including amplification, deep deletion and mRNA upregulation. More interestingly, the cases with genetic alterations had a poorer survival as compared to those without alterations. Overall, the study propounds a potentiality for interpreting the pathogenesis and development of LUSC with lncRNAs, and provides a novel platform for searching for more capable diagnostic biomarkers for LUSC. © Chen et al.","Biomarker; LncRNAs; LUSC; TCGA; Tumorigenesis","long untranslated RNA; messenger RNA; Article; cancer growth; cancer staging; controlled study; data mining; diagnostic value; FENDRR gene; GATA6 AS1 gene; gene; gene amplification; gene deletion; gene expression; human; human tissue; LINC00961 gene; LINC00968 gene; LINC01272 gene; LINC01314 gene; LINC01572 gene; lung carcinogenesis; microarray analysis; MIR3945HG gene; receiver operating characteristic; RNA sequence; RP178O14.1 gene; SFTA1P gene; squamous cell lung carcinoma; survival time; validation study",2-s2.0-85028772826
"Saleh M., Serbey S., Fahs B.","Data mining approach for estimating cloud-covered areas in MODIS satellite images",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030556415&doi=10.1109%2fISCC.2017.8024682&partnerID=40&md5=3e165a9026b8b298faa4e98fac1a950b","One of the main remote sensors used for monitoring snow covered areas is the Moderate Resolution Imaging Spectroradiometer (MODIS) employed by NASA's Terra and Aqua satellites. Using MODIS-derived snow cover images is limited under cloud-covered regions due to the sensor's capabilities. This paper presents an automated process based on K-Nearest Neighbor (KNN) algorithm using spatiotemporal features, to estimate the pixel cover for cloud-covered regions. The algorithm was tested using MODIS's daily snow-cover datasets obtained from Terra (MOD10A1) and Aqua (MYD10A1) satellite for the Lebanese territories as a study region. Several experiments were implemented to test the accuracy of the proposed algorithm, and the results were highly acceptable (>90%). © 2017 IEEE.","Aqua; Cloud-free snow cover product; K-Nearest Neighbors; Moderate Resolution Imaging Spectroradiometer (MODIS); Spatial-temporal techniques; Terra","Automation; Data mining; Membership functions; Motion compensation; NASA; Nearest neighbor search; Radiometers; Remote sensing; Satellites; Snow; Spectrometers; Text processing; Aqua; K-nearest neighbors; Moderate resolution imaging spectroradiometer; Snow-cover products; Spatial temporals; Terra; Satellite imagery",2-s2.0-85030556415
"Fathian M., Azhdari E.","Extracting customer behavior pattern in a telecom company using temporal fuzzy clustering and data mining",2017,"Journal of Information Technology Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029535569&partnerID=40&md5=6ffd1d2ae6428be7d10041003878621c","One of the most important issues in Customer Relationship Management is customer segmentation and product offer based on their needs. In practice, Customer's behavior will change over the time by changes in technology, increase in the number of new customers and new competitors, and product variety. Traditional segmentation models that are static over time cannot predict these changes in customer's behavior and ignore them. This challenge is especially critical in Telecommunication with high churn rates. In this research, we have used temporal fuzzy clustering to detect significant changes in customers' behavior for a telecom company during a 10-month period. The aim of this study is to find factors that affect structural and gradual changes in clustering model. In addition, we have suggested a method based on Frechet distance to extract similar patterns in customer's usage behavior. Provided that combining the temporal clustering with trajectory analysis is an effective way to recognize customers' behavior among the clusters, the results showed that there are seven distinct customer behavior patterns two of which lead to the customer drop or churn. These patterns can be used to reduce the risk and costs of customers churn and to design optimum services.","Customer behavior; Data mining; Dynamic clustering; Fuzzy clustering; Trajectory analysis",,2-s2.0-85029535569
"Yao L., Zhang Y., Chen Q., Qian H., Wei B., Hu Z.","Mining coherent topics in documents using word embeddings and large-scale text data",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026442519&doi=10.1016%2fj.engappai.2017.06.024&partnerID=40&md5=4a1a04592b206f9ab7a0b02a2478387a","Probabilistic topic models have been extensively used to extract low-dimension aspects from document collections. However, such models without any human knowledge often generate topics that are not interpretable. Recently, a number of knowledge-based topic models have been proposed, which enable users to input prior domain knowledge to produce more meaningful and coherent topics. Word embeddings, on the other hand, can automatically capture both semantic and syntactic information of words from a large amount of documents, and can be used to measure word similarities. In this paper, we incorporate word embeddings obtained from a large number of domains into topic modeling. By combining Latent Dirichlet Allocation, a widely used topic model with Skip-Gram, a well-known framework for learning word vectors, we improve the semantic coherence significantly. Our evaluation results using product review documents from 100 domains will demonstrate the effectiveness of our method. © 2017 Elsevier Ltd","Domain knowledge; Large-scale text data; Topic model; Word embedding","Knowledge based systems; Semantics; Statistics; Document collection; Domain knowledge; Latent Dirichlet allocation; Probabilistic topic models; Syntactic information; Text data; Topic Modeling; Word embedding; Data mining",2-s2.0-85026442519
"Elankavi R., Kalaiprasath R., Udayakumar R.","Data mining with big data revolution hybrid",2017,"International Journal on Smart Sensing and Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028002495&partnerID=40&md5=ce50d3d46db4546e7681123a513bd5fc","Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.","Data storage; Demand-driven; HACE",,2-s2.0-85028002495
"Yarmohammadi S., Pourabolghasem R., Castro-Lacouture D.","Mining implicit 3D modeling patterns from unstructured temporal BIM log text data",2017,"Automation in Construction",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019461492&doi=10.1016%2fj.autcon.2017.04.012&partnerID=40&md5=6ce524313333e6dec5ce9e2b2a559e78","Building information modeling is instrumental in documenting design, enhancing customer experience, and improving product functionality in capital projects. However, good building models do not happen by accident, but rather as a result of a managed process that involves several participants from different disciplines and backgrounds. Effective management of this process requires an ability to closely monitor the modeling process and correctly measure modelers' performance. Nevertheless, existing methods of performance monitoring in building design practices lack an objective measurement system to quantify modeling progress. The widespread utilization of Building Information Modeling (BIM) tools presents a unique opportunity to retrieve granular design process data and conduct accurate performance measurements. As a building's 3D model is gradually developed, model generation software packages, such as Autodesk Revit, automatically create log files that record design activities. This paper investigates what information these log files contain and how one can extract and further analyze the information to provide insight into the design modeling process. The specific objectives of this study were to: (1) investigate the presence of implicit patterns in 3-D design log files; and (2) to empirically characterize the performance of modelers based on the time it takes them to execute similar modeling tasks. To fulfill these objectives, design log files provided by an international architecture and design firm were analyzed. Using a tailored text file parser, user-model interaction data including modeler characteristics, command type, and command time were extracted from the journal files. To identify implicit command execution patterns, a sequence mining algorithm based on Generalized Suffix Trees (GST) was implemented. It was shown that there is a statistically significant difference between the average time it takes modelers to execute each command sequence. This study extends the existing knowledge by proposing a novel methodology to extract meaningful patterns from time-stamped unstructured design log data. This research contributes to the state of practice by providing a better understanding of information embedded in design log files. © 2017 Elsevier B.V.","Building information modeling; Data mining; Design log files; Performance management; Sequential pattern analysis","Buildings; Data mining; Information theory; Product design; Trees (mathematics); Building Information Model - BIM; Log file; Objective measurement; Performance management; Performance monitoring; Sequential patterns; Statistically significant difference; User-model interaction; Architectural design",2-s2.0-85019461492
"Nikoo M.R., Kerachian R.","Wave height prediction using artificial immune recognition systems (AIRS) and some other data mining techniques",2017,"Iranian Journal of Science and Technology - Transactions of Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028597797&doi=10.1007%2fs40996-017-0067-y&partnerID=40&md5=9e4e4edc1a50048d30a615215b71a755","Significant wave height (SWH) prediction is an important task in coastal engineering activities. Due to difficulties and complexities in predicting SWH using traditional numerical and empirical models, in the past decades, a high tendency to use soft computing approaches has been observed. In this paper, an artificial immune recognition system (AIRS), which is a recently developed data mining approach, is utilized for SWH prediction. The results of AIRS model are compared with those of artificial neural networks (ANN), regression tree induction (named M5P), Rough Set Theory, Bayesian networks and support vector regression models. The AIRS has been successfully employed in some engineering applications, but its efficiency and applicability for SWH prediction have not been investigated. In this paper, the AIRS model is utilized for the SWH prediction in the Lake Superior in North America. Comparing the results of this model and five other data mining techniques shows that the AIRS and ANN can outperform other models. © Shiraz University 2017.","Artificial immune recognition systems; Bayesian networks; Lake superior; Regression tree induction; Significant wave height prediction",,2-s2.0-85028597797
"Kessentini M., Menzies T.","A guest editorial: special issue on search based software engineering and data mining",2017,"Automated Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019583881&doi=10.1007%2fs10515-017-0217-2&partnerID=40&md5=7a337ce52bc1f3b35de88cd123aacbeb",[No abstract available],,,2-s2.0-85019583881
"Apiletti D., Baralis E., Cerquitelli T., Garza P., Pulvirenti F., Venturini L.","Frequent Itemsets Mining for Big Data: A Comparative Analysis",2017,"Big Data Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028451362&doi=10.1016%2fj.bdr.2017.06.006&partnerID=40&md5=63647c4e15fa476265e8f67b58e6471b","Itemset mining is a well-known exploratory data mining technique used to discover interesting correlations hidden in a data collection. Since it supports different targeted analyses, it is profitably exploited in a wide range of different domains, ranging from network traffic data to medical records. With the increasing amount of generated data, different scalable algorithms have been developed, exploiting the advantages of distributed computing frameworks, such as Apache Hadoop and Spark. This paper reviews Hadoop- and Spark-based scalable algorithms addressing the frequent itemset mining problem in the Big Data domain through both theoretical and experimental comparative analyses. Since the itemset mining task is computationally expensive, its distribution and parallelization strategies heavily affect memory usage, load balancing, and communication costs. A detailed discussion of the algorithmic choices of the distributed methods for frequent itemset mining is followed by an experimental analysis comparing the performance of state-of-the-art distributed implementations on both synthetic and real datasets. The strengths and weaknesses of the algorithms are thoroughly discussed with respect to the dataset features (e.g., data distribution, average transaction length, number of records), and specific parameter settings. Finally, based on theoretical and experimental analyses, open research directions for the parallelization of the itemset mining problem are presented. © 2017 Elsevier Inc.","Big Data; Frequent itemset mining; Hadoop and Spark platforms",,2-s2.0-85028451362
"Hufsky F., Böcker S.","Mining molecular structure databases: Identification of small molecules based on fragmentation mass spectrometry data",2017,"Mass Spectrometry Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954304545&doi=10.1002%2fmas.21489&partnerID=40&md5=b0f008c89c7d5079ce59cffbdfeb66d3","Mass spectrometry (MS) is a key technology for the analysis of small molecules. For the identification and structural elucidation of novel molecules, new approaches beyond straightforward spectral comparison are required. In this review, we will cover computational methods that help with the identification of small molecules by analyzing fragmentation MS data. We focus on the four main approaches to mine a database of metabolite structures, that is rule-based fragmentation spectrum prediction, combinatorial fragmentation, competitive fragmentation modeling, and molecular fingerprint prediction. © 2016 Wiley Periodicals, Inc. Mass Spec Rev 36:624–633, 2017. © 2016 Wiley Periodicals, Inc.","computational methods; mass spectrometry; mining metabolite structures; structural elucidation","Computational methods; Mass spectrometry; Metabolites; Spectrometry; Fragmentation models; Key technologies; Mass spectrometry data; Molecular fingerprint; New approaches; Novel molecules; Small molecules; Structural elucidation; Molecules",2-s2.0-84954304545
"Kalyani G., Rao M.V.P.C.S., Janakiramaiah B.","Decision tree based data reconstruction for privacy preserving classification rule mining",2017,"Informatica (Slovenia)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031720665&partnerID=40&md5=d20e5699e36a9ad770b983f6b6cd1c94","Data sharing among the organizations is a general activity in several areas like business promotion and marketing. Useful and interesting patterns can be identified with data collaboration. But, some of the sensitive patterns that are supposed to be kept private may be disclosed and such disclosure of sensitive patterns may effects the profits of the organizations that own the data. Hence the rules which are sensitive must be concealed prior to sharing the data. Concealing of sensitive patterns can be handled by modifying or reconstructing the database before sharing with others. However, to make the reconstructed database usable for data analysts the utility or usability of the database is to be maximized. Hence, both privacy and usability are to be balanced. A novel method is proposed to conceal the classification rules which are sensitive by reconstructing a new database. Initially, classification rules identified from the database are made accessible to the owner of the data to spot out the sensitive rules that are to be concealed. In the next, from the non-sensitive rules of the database, a decision tree will be constructed based on the classifying capability of the rules, from which a new database will be reconstructed. Finally, the released reconstructed database to the analysts reveals only non-sensitive classification rules. Empirical studies proved that the proposed algorithm preserves the privacy effectively. In addition to that utility of the classification model on the reconstructed database was also be preserved.","Classification rules; Data reconstruction; Decision tree; Privacy; Sensitive patterns","Data privacy; Database systems; Decision trees; Trees (mathematics); Business promotions; Classification models; Classification rules; Data collaborations; Data reconstruction; Empirical studies; Privacy-preserving classification; Sensitive patterns; Classification (of information)",2-s2.0-85031720665
"Zhu D., Jin H., Yang Y., Wu D., Chen W.","DeepFlow: Deep learning-based malware detection by mining Android application for abnormal usage of sensitive data",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541939&doi=10.1109%2fISCC.2017.8024568&partnerID=40&md5=bd64a6968ee8f1d97c6b36a5b0e8d197","The open nature of Android allows application developers to take full advantage of the system. While the flexibility is brought to developers and users, it may raise significant issues related to malicious applications. Traditional malware detection approaches based on signatures or abnormal behaviors are invalid when dealing with novel malware. To solve the problem, machine learning algorithms are used to learn the distinctions between malware and benign apps automatically. Deep learning, as a new area of machine learning, is developing rapidly as its better characterization of samples. We thus propose DeepFlow, a novel deep learning-based approach for identifying malware directly from the data flows in the Android application. We test DeepFlow on thousands of benignware and malware. The results show that DeepFlow can achieve a high detection F1 score of 95.05%, outperforming traditional machine learning-based approaches, which reveals the advantage of deep learning technique in malware detection. © 2017 IEEE.","Android malware detection; Characterization; Data flows; Deep learning","Artificial intelligence; Characterization; Computer crime; Data transfer; Deep learning; Learning algorithms; Learning systems; Malware; Abnormal behavior; Android applications; Android malware; Application developers; Data flow; Learning techniques; Learning-based approach; Malware detection; Android (operating system)",2-s2.0-85030541939
"Sathya M., Madhan S., Jothilakshmi U.","VWDRE – A vision-based approach for mining data from search engine result pages",2017,"International Journal of Civil Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030237911&partnerID=40&md5=c64f6308e6fcf131cdd03bf86664b823","The data extraction from the dynamically generated web pages is a challenging factor because the result of the search engines are always different for every query submitted. Many techniques were proposed to address this issue but most of them have the common problem of language-dependency. In order to overcome the limitations of previous works, there are few ways which analyze visual features of the web page. In this paper, we proposed a new vision-based approach which is independent of the code used. It broadly utilizes the visual features on the search engine result pages to locate the data region so asto mine the data records from it. We develop a clustering by similarity algorithm to check the similarity of data records. Also, we propose a technique to generate the wrapper for data record extraction by examining the multiple result pages from the same search engine. © IAEME Publication.","Data Extraction; DOM tree; Search Engine Result Pages; Vision-Based; Wrapper",,2-s2.0-85030237911
"Nayahi J.J.V., Kavitha V.","Privacy and utility preserving data clustering for data anonymization and distribution on Hadoop",2017,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006825716&doi=10.1016%2fj.future.2016.10.022&partnerID=40&md5=3f385c2bb2e2946d3bdf8f8e7e5f0afe","Data privacy is a stringent need when sharing and processing data on a distributed environment or in Internet of Things. Collaborative privacy-preserving data mining based on secured multiparty computation incur high communication and computational cost. Data anonymization is a promising technique in the field of privacy-preserving data mining used to protect the data against identity disclosure. Information loss and common attacks possible on the anonymized data are serious challenges of anonymization. Recently, data anonymization using data mining techniques has showed significant improvement in data utility. Still the existing techniques lack in effective handling of attacks. Hence in this paper, an anonymization algorithm based on clustering and resilient to similarity attack and probabilistic inference attack is proposed. The anonymized data is distributed on Hadoop Distributed File System. The method achieves a better trade-off between privacy and utility. In our work the data utility is measured in terms of accuracy and FMeasure with respect to different classifiers. Experiments show that the accuracy, FMeasure and the execution time of the classification algorithms on the privacy-preserved data sets formed by the proposed clustering algorithms are better than the existing algorithms. © 2016 Elsevier B.V.","Classification; Clustering; Data anonymization; Data privacy; Distributed data; Hadoop","Classification (of information); Clustering algorithms; Data handling; Data mining; Economic and social effects; File organization; Inference engines; Classification algorithm; Clustering; Data anonymization; Distributed data; Distributed environments; Hadoop; Hadoop distributed file systems; Privacy preserving data mining; Data privacy",2-s2.0-85006825716
"Dhifli W., Aridhi S., Nguifo E.M.","MR-SimLab: Scalable subgraph selection with label similarity for big data",2017,"Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019726178&doi=10.1016%2fj.is.2017.05.006&partnerID=40&md5=925677596b916c7f112a0f9ee8cac2c5","With the increasing size and complexity of available databases, existing machine learning and data mining algorithms are facing a scalability challenge. In many applications, the number of features describing the data could be extremely high. This hinders or even could make any further exploration infeasible. In fact, many of these features are redundant or simply irrelevant. Hence, feature selection plays a key role in helping to overcome the problem of information overload especially in big data applications. Since many complex datasets could be modeled by graphs of interconnected labeled elements, in this work, we are particularly interested in feature selection for subgraph patterns. In this paper, we propose MR-SIMLAB, a MAPREDUCE-based approach for subgraph selection from large input subgraph sets. In many applications, it is easy to compute pairwise similarities between labels of the graph nodes. Our approach leverages such rich information to measure an approximate subgraph matching by aggregating the elementary label similarities between the matched nodes. Based on the aggregated similarity scores, our approach selects a small subset of informative representative subgraphs. We provide a distributed implementation of our algorithm on top of the MAPREDUCE framework that optimizes the computational efficiency of our approach for big data applications. We experimentally evaluate MR-SIMLAB on real datasets. The obtained results show that our approach is scalable and that the selected subgraphs are informative. © 2017 Elsevier Ltd","Feature selection; Label similarity; MAPREDUCE; Subgraph mining","Computational efficiency; Data mining; Feature extraction; Graph theory; Learning systems; Big data applications; Data mining algorithm; Distributed implementation; Information overloads; Map-reduce; Mapreduce frameworks; Subgraph matching; Subgraph mining; Big data",2-s2.0-85019726178
"Gamasaee R., Zarandi M.H.F.","A new Dirichlet process for mining dynamic patterns in functional data",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017295853&doi=10.1016%2fj.ins.2017.04.008&partnerID=40&md5=d3f26b57bdfe27a338fc79dcf43a2e53","This paper proposes a novel model for mining patterns including dynamic function clustering, segmentation, and forecasting values of dependent variables simultaneously. The proposed Dependent Dirichlet Process Piecewise Regression Mixture (DDPPRM) model is capable of handling dynamic nature of data by detecting evolving clusters at each time step. This evolution manifests dynamically in three states of clusters: newly created, existing, and transient states of clusters. The model is also able to generate clusters over time infinitely. It is capable of learning the optimal number of clusters rather than using the fixed, predefined clusters. However, other clustering methods such as Fuzzy C-Regression Model and Piecewise Regression Mixture technique support none of those capabilities. The proposed model is also capable of showing regime changes and segmenting functions in regression/time series problems. A two-step Gibbs sampling method is utilized for assigning data to clusters. Expectation-Maximization method is used for finding the optimal values of parameters of functions and probability distributions. The model is validated by using some numerical experiments and calculating three validity indexes as well as Mean Square Error of the model. The results indicate that the proposed method outperforms other clustering, segmentation, and forecasting models in literature. © 2017 Elsevier Inc.","Bayesian non-parametric; Dependent Dirichlet process regression mixture; Dynamic clustering; Expectation-Maximization; Forecasting; Gibbs sampling; Segmentation","Channel estimation; Forecasting; Image segmentation; Maximum principle; Mean square error; Mixtures; Regression analysis; Dirichlet process; Dynamic clustering; Expectation - maximizations; Gibbs sampling; Non-parametric; Probability distributions",2-s2.0-85017295853
"Zheng K., Li F., Shim K.","Preface to the special issue on big data search and mining",2017,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015917207&doi=10.1007%2fs11280-017-0452-2&partnerID=40&md5=936b9cf3d892202da6839bb5a23914d8",[No abstract available],,,2-s2.0-85015917207
"Schulze C., Cleaveland R.","Improving invariant mining via static analysis",2017,"ACM Transactions on Embedded Computing Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030672741&doi=10.1145%2f3126504&partnerID=40&md5=90a7859f3145db54d7de039f97e29d99","This paper proposes the use of static analysis to improve the generation of invariants from test data extracted from Simulink models. Previous work has shown the utility of such automatically generated invariants as a means for updating and completing system specifications; they also are useful as a means of understanding model behavior. This work shows how the scalability and accuracy of the data mining process can be dramatically improved by using information from data/control flow analysis to reduce the search space of the invariant mining and to eliminate false positives. Comparative evaluations of the process show that the improvements significantly reduce execution time and memory consumption, thereby supporting the analysis of more complex models, while also improving the accuracy of the generated invariants. © 2017 ACM.","Automated test generation; Invariant mining; Model-based development; Verification and validation","Software testing; Specifications; Static analysis; Automated test generations; Automatically generated; Comparative evaluations; Data mining process; Memory consumption; Model based development; System specification; Verification-and-validation; Data mining",2-s2.0-85030672741
"Machado D., Paiva T., Dutra I., Costa V.S., Brandao P.","Managing diabetes: Pattern discovery and counselling supported by user data in a mobile platform",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030552949&doi=10.1109%2fISCC.2017.8024545&partnerID=40&md5=8b107ce37e9f581236be1871c759157b","Diabetes management is a complex and a sensible problem as each diabetic is a unique case with particular needs. The optimal solution would be a constant monitoring of the diabetic's values and automatically acting accordingly. We propose an approach that guides the user and analyses the data gathered to give individual advice. By using data mining algorithms and methods, we uncover hidden behaviour patterns that may lead to crisis situations. These patterns can then be transformed into logical rules, able to trigger in a particular context, and advise the user. We believe that this solution, is not only beneficial for the diabetic, but also for the doctor accompanying the situation. The advice and rules are useful input that the medical expert can use while prescribing a particular treatment. During the data gathering phase, when the number of records is not enough to attain useful conclusions, a base set of logical rules, defined from medical protocols, directives and/or advice, is responsible for advise and guiding the user. The proposed system will accompany the user at start with generic advice, and with constant learning, advise the user more specifically. We discuss this approach describing the architecture of the system, its base rules and data mining component. The system is to be incorporated in a currently developed diabetes management application for Android. © 2017 IEEE.",,"Communication; Behaviour patterns; Crisis situations; Data mining algorithm; Diabetes management; Medical protocols; Mobile platform; Optimal solutions; Pattern discovery; Data mining",2-s2.0-85030552949
"You Y., Demmel J.","Runtime Data Layout Scheduling for Machine Learning Dataset",2017,"Proceedings of the International Conference on Parallel Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640667&doi=10.1109%2fICPP.2017.54&partnerID=40&md5=59e693ae41fc7fd852c7bc892f9ed128","Machine Learning (ML) approaches are widelyused classification/regression methods for data mining applications. However, the time-consuming training process greatly limits the efficiency of ML approaches. We use the example of SVM (traditional ML algorithm) and DNN (state-of-the-art ML algorithm) to illustrate the idea in this paper. For SVM, a major performance bottleneck of current tools is that they use a unified data storage format because the data formats can have a significant influence on the complexity of storage and computation, memory bandwidth, and the efficiency of parallel processing. To address the problem above, we study the factors influencing the algorithm's performance and conduct auto-tuning to speed up SVM training. DNN training is even slower than SVM. For example, using a 8-core CPUs to train AlexNet model by CIFAR-10 dataset costs 8.2 hours. CIFAR-10 is only 170 MB, which is not efficient for distributed processing. Moreover, due to the algorithm limitation, only a small batch of data can be processed at each iteration. We focus on finding the right algorithmic parameters and using auto-tuning techniques to make the algorithm run faster. For SVM training, our implementation achieves 1:7..16:3 speedup (6:8 on average) against the non-adaptive case (using the worst data format) for various datasets. For DNN training on CIFAR-10 dataset, we reduce the time from 8.2 hours to only roughly 1 minute. We use the benchmark of dollars per speedup to help the users to select the right deep learning hardware. © 2017 IEEE.","Machine learning; Parallel auto-tuning","Artificial intelligence; Data mining; Deep neural networks; Efficiency; Iterative methods; Learning systems; Program processors; Algorithm's performance; Algorithmic parameters; Autotuning; Data mining applications; Distributed processing; Memory bandwidths; Parallel processing; Performance bottlenecks; Digital storage",2-s2.0-85030640667
"Krawczyk B., Minku L.L., Gama J., Stefanowski J., Woźniak M.","Ensemble learning for data stream analysis: A survey",2017,"Information Fusion",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011884671&doi=10.1016%2fj.inffus.2017.02.004&partnerID=40&md5=7fbd60b614cf5ec63ae0fd16bda28573","In many applications of information systems learning algorithms have to act in dynamic environments where data are collected in the form of transient data streams. Compared to static data mining, processing streams imposes new computational requirements for algorithms to incrementally process incoming examples while using limited memory and time. Furthermore, due to the non-stationary characteristics of streaming data, prediction models are often also required to adapt to concept drifts. Out of several new proposed stream algorithms, ensembles play an important role, in particular for non-stationary environments. This paper surveys research on ensembles for data stream classification as well as regression tasks. Besides presenting a comprehensive spectrum of ensemble approaches for data streams, we also discuss advanced learning concepts such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations and structured outputs. The paper concludes with a discussion of open research problems and lines of future research. © 2017","Concept drift; Data streams; Ensemble learning; Non-stationary environments; Online learning","Data communication systems; Data mining; Learning algorithms; Supervised learning; Surveys; Concept drifts; Data stream; Ensemble learning; Non-stationary environment; Online learning; Data handling",2-s2.0-85011884671
"Pancerz K., Grochowalski P.","From unstructured data included in real-estate listings to information systems over ontological graphs",2017,"Proceedings of the International Conference on Information and Digital Technologies, IDT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030120068&doi=10.1109%2fDT.2017.8024312&partnerID=40&md5=56b612471af5cccb5249c2be9c6c4b1c","In the paper, we consider the problem of automatic transformation of unstructured data included in real-estate listings into data arranged in a tabular form, as the so-called information tables. Transformation is an important preprocessing stage enabling us to obtain data in a form accepted by many data mining and machine learning tools. In the presented approach, information tables represent information systems over ontological graphs. In such systems, the domain knowledge covering some data semantics is directly incorporated. In the paper, we describe important aspects of a specialized computer tool created by us to automatize the transformation process. © 2017 IEEE.","data preprocessing; data semantics; ontology; real-estate listings","Data mining; Information systems; Learning systems; Ontology; Semantics; Automatic transformations; Data preprocessing; Data semantics; Domain knowledge; Information table; Real estate; Transformation process; Unstructured data; Metadata",2-s2.0-85030120068
"Olorisade B.K., Brereton P., Andras P.","Reproducibility of studies on text mining for citation screening in systematic reviews: Evaluation and checklist",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025143405&doi=10.1016%2fj.jbi.2017.07.010&partnerID=40&md5=11ac5d14a66a490160b83632a3f8b474","Context Independent validation of published scientific results through study replication is a pre-condition for accepting the validity of such results. In computation research, full replication is often unrealistic for independent results validation, therefore, study reproduction has been justified as the minimum acceptable standard to evaluate the validity of scientific claims. The application of text mining techniques to citation screening in the context of systematic literature reviews is a relatively young and growing computational field with high relevance for software engineering, medical research and other fields. However, there is little work so far on reproduction studies in the field. Objective In this paper, we investigate the reproducibility of studies in this area based on information contained in published articles and we propose reporting guidelines that could improve reproducibility. Methods The study was approached in two ways. Initially we attempted to reproduce results from six studies, which were based on the same raw dataset. Then, based on this experience, we identified steps considered essential to successful reproduction of text mining experiments and characterized them to measure how reproducible is a study given the information provided on these steps. 33 articles were systematically assessed for reproducibility using this approach. Results Our work revealed that it is currently difficult if not impossible to independently reproduce the results published in any of the studies investigated. The lack of information about the datasets used limits reproducibility of about 80% of the studies assessed. Also, information about the machine learning algorithms is inadequate in about 27% of the papers. On the plus side, the third party software tools used are mostly free and available. Conclusions The reproducibility potential of most of the studies can be significantly improved if more attention is paid to information provided on the datasets used, how they were partitioned and utilized, and how any randomization was controlled. We introduce a checklist of information that needs to be provided in order to ensure that a published study can be reproduced. © 2017 Elsevier Inc.","Citation screening; Reproducibility; Reproducible research; Systematic review; Text mining","Application programs; Diagnosis; Learning algorithms; Publishing; Software engineering; Context independent; Reproducibilities; Reproducible research; Systematic literature review; Systematic Review; Text mining; Text mining techniques; Third party software; Data mining; access to information; Article; automation; citation analysis; data analysis; data analysis software; data mining; information processing; machine learning; mathematical analysis; medical research; perceptron; practice guideline; priority journal; reproducibility; support vector machine; systematic review (topic)",2-s2.0-85025143405
"Figueiredo L.N.L., de Assis G.T., Ferreira A.A.","DERIN: A data extraction method based on rendering information and n-gram",2017,"Information Processing and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019118156&doi=10.1016%2fj.ipm.2017.04.007&partnerID=40&md5=b3d27bfa45f759ca14dd68570604e1a5","Extracting data from web pages is an important task for several applications such as comparison shopping and data mining. Ordinarily, the data in web pages represent records from a database and are obtained using a web search. One of the most important steps for extracting records from a web page is identifying out of the different data regions, the one containing the records to be extracted. An incorrect identification of this region may lead to an extraction of incorrect records. This process is followed by the equally important step of detecting and correctly splitting the necessary records and their attributes from the main data region. In this study, we propose a method for data extraction based on rendering information and an n-gram model (DERIN) that aims to improve wrapper performance by automatically selecting the main data region from a search results page and extracting its records and attributes based on rendering information. The proposed DERIN method can detect different record structures using techniques based on an n-gram model. Moreover, DERIN does not require examples to learn how to extract the data, performs a given domain independently and can detect records that are not children of the same parent element in the DOM tree. Experimental results using web pages from several domains show that DERIN is highly effective and performs well when compared with other methods. © 2017 Elsevier Ltd","Data extraction; Main data region; N-gram; Path expression; Rendering information; Visual information; Wrapper","Extraction; Trees (mathematics); Websites; Data extraction; Data regions; N-grams; Path expressions; Rendering information; Visual information; Wrapper; Data mining",2-s2.0-85019118156
"Maskey M., Ramachandran R., Li X., Weigel A., Bugbee K., Gatlin P., Miller J.J.","A relevancy algorithm for curating earth science data around phenomenon",2017,"Computers and Geosciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020849168&doi=10.1016%2fj.cageo.2017.06.007&partnerID=40&md5=f46e19f930ac7fc92d5f09082b163a84","Earth science data are being collected for various science needs and applications, processed using different algorithms at multiple resolutions and coverages, and then archived at different archiving centers for distribution and stewardship causing difficulty in data discovery. Curation, which typically occurs in museums, art galleries, and libraries, is traditionally defined as the process of collecting and organizing information around a common subject matter or a topic of interest. Curating data sets around topics or areas of interest addresses some of the data discovery needs in the field of Earth science, especially for unanticipated users of data. This paper describes a methodology to automate search and selection of data around specific phenomena. Different components of the methodology including the assumptions, the process, and the relevancy ranking algorithm are described. The paper makes two unique contributions to improving data search and discovery capabilities. First, the paper describes a novel methodology developed for automatically curating data around a topic using Earth science metadata records. Second, the methodology has been implemented as a stand-alone web service that is utilized to augment search and usability of data in a variety of tools. © 2017","Data curation; Earth science phenomena; Information retrieval; Relevancy algorithm; Search paradigms","Earth sciences; Information retrieval; Web services; Data curation; Data discovery; Earth science data; Multiple resolutions; Novel methodology; Relevancy ranking; Search paradigms; Subject matters; Data mining; algorithm; data management; Earth science; metadata; World Wide Web",2-s2.0-85020849168
"Mirge V., Verma K., Gupta S.","Dense traffic flow patterns mining in bi-directional road networks using density based trajectory clustering",2017,"Advances in Data Analysis and Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973140884&doi=10.1007%2fs11634-016-0256-8&partnerID=40&md5=0a6ab13a5298f0af26a713e564781859","Due to the rapid growth of wireless communications and positioning technologies, trajectory data have become increasingly popular, posing great challenges to the researchers of data mining and machine learning community. Trajectory data are obtained using GPS devices that capture the position of an object at specific time intervals. These enormous amounts of data necessitates to explore efficient and effective techniques to extract useful information to solve real world problems. Traffic flow pattern mining is one of the challenging issues for many applications. In a literature significant number of approaches are available to cluster the trajectory data, however the clustering has not been explored for trajectories pattern mining in bi-directional road networks. This paper presents a novel technique for excavating heavy traffic flow patterns in bi-directional road network, i.e. identifying divisions of the roads where the traffic flow is very dense. The proposed technique works in two phases: phase I, finds the clusters of trajectory points based on density of trajectory points; phase II, arranges the clusters in sequence based on spatiotemporal values for each route and directions. These sequences represent the traffic flow patterns. All the routes and sections exceeding a user specified minimum traffic threshold are marked as high dense traffic areas. The experiments are performed on synthetic dataset. The proposed algorithm efficiently and accurately finds the dense traffic in bi-directional roads. Proposed clustering method is compared with the standard k-means clustering algorithm for the performance evaluation. © 2016, Springer-Verlag Berlin Heidelberg.","Data mining; Mobility mining; Traffic control; Traffic flow patterns mining; Trajectory patterns","Artificial intelligence; Clustering algorithms; Data mining; Flow patterns; Learning systems; Motor transportation; Roads and streets; Traffic control; Trajectories; Transportation; Wireless telecommunication systems; K-Means clustering algorithm; Machine learning communities; Mobility minings; Positioning technologies; Traffic flow patterns; Trajectory clustering; Trajectory pattern; Wireless communications; Street traffic control",2-s2.0-84973140884
"Oliveira J.N., Macedo H.D.","The data cube as a typed linear algebra operator",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030560039&doi=10.1145%2f3122831.3122834&partnerID=40&md5=d78cdcd524a209dfbc6bfce94d367054","There is a need for a typed notation for linear algebra applicable to the ields of econometrics and data mining. In this paper we show that such a notation exists and can be useful in formalizing and reasoning about data aggregation operations. One such operation - the construction of a data cube - is shown to be easily expressible as a linear algebra operator. The construction is shown to be type-generic and some of its properties are derived from its typed deinition and proved using matrix algebra. Other forms of data aggregation such as eg. rollup and cross tabulation are shown to be algebraically derivable from data cubes. © 2017 Association for Computing Machinery.",,"Algebra; Data mining; Economics; Geometry; Linear algebra; Statistics; Algebra operators; Data aggregation; Data cube; Matrix algebra",2-s2.0-85030560039
"Dam T.-L., Li K., Fournier-Viger P., Duong Q.-H.","An efficient algorithm for mining top-k on-shelf high utility itemsets",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009201661&doi=10.1007%2fs10115-016-1020-2&partnerID=40&md5=49f439060b0db3340d4e4615f1fdc27f","High on-shelf utility itemset (HOU) mining is an emerging data mining task which consists of discovering sets of items generating a high profit in transaction databases. The task of HOU mining is more difficult than traditional high utility itemset (HUI) mining, because it also considers the shelf time of items, and items having negative unit profits. HOU mining can be used to discover more useful and interesting patterns in real-life applications than traditional HUI mining. Several algorithms have been proposed for this task. However, a major drawback of these algorithms is that it is difficult for users to find a suitable value for the minimum utility threshold parameter. If the threshold is set too high, not enough patterns are found. And if the threshold is set too low, too many patterns will be found and the algorithm may use an excessive amount of time and memory. To address this issue, we propose to address the problem of top-k on-shelf high utility itemset mining, where the user directly specifies k, the desired number of patterns to be output instead of specifying a minimum utility threshold value. An efficient algorithm named KOSHU (fast top-K on-shelf high utility itemset miner) is proposed to mine the top-k HOUs efficiently, while considering on-shelf time periods of items, and items having positive and/or negative unit profits. KOSHU introduces three novel strategies, named efficient estimated co-occurrence maximum period rate pruning, period utility pruning and concurrence existing of a pair 2-itemset pruning to reduce the search space. KOSHU also incorporates several novel optimizations and a faster method for constructing utility-lists. An extensive performance study on real-life and synthetic datasets shows that the proposed algorithm is efficient both in terms of runtime and memory consumption and has excellent scalability. © 2017, Springer-Verlag London.","Data mining; High utility mining; On-shelf high utility mining; Top-k on-shelf high utility mining",,2-s2.0-85009201661
"Wei C.","Mining of user behavioral features based on indoor semantic trajectories",2017,"Boletin Tecnico/Technical Bulletin",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765536&partnerID=40&md5=eb33b2dbb0e026d9e215a866a84fb12b","Along with increasingly wide applications of indoor location technologies, sensors have recorded a large number of location data of indoor moving objects. The trajectories formed by these location data can largely reflect users’ daily behavioral patterns. Mining and analysis of user behavioral trajectories collected through the indoor terminal can contribute to intelligent promotion of location services. However, the trajectory data are often heterogeneous, unreliable and incomplete, and the current mining algorithms cannot get adapted to these characteristics. Besides, the current research findings have not yet extracted in-depth knowledge (such as purposes, intentions, living habits, and social relations) hidden in the trajectory data. This results in a low degree of abstraction of the mining results and failure of expressing users’ advanced semantics. Concerning the above problems, this paper proposes a user behavioral feature mining method based on the indoor semantic trajectories. To start with, semantic analysis of indoor semantic trajectories of users is conducted. Next, the users’ temporal semantic trajectories are traversed, and the similarity of these temporal semantic trajectories or of users’ indoor moving trajectories is solved. Finally, based on analysis of semantic trajectories, user features are extracted for the convenience of recommending personalized services or predicting users’ next location according to user types and preferences. To sum up, the user behavioral feature mining method based on indoor semantic trajectories put forward in this paper is reasonable, efficient, and can accurately and effectively find out the semantic behavioral patterns.","Indoor Navigation; Location-based services; Semantic Trajectory; Trajectory Mining; User Behavior.","Behavioral research; Data mining; Location; Mining; Semantics; Telecommunication services; Trajectories; Behavioral features; Behavioral patterns; In-door navigations; Indoor moving objects; Personalized service; Semantic trajectories; Trajectory minings; User behaviors; Location based services",2-s2.0-85031765536
"Westra B.L., Sylvia M., Weinfurter E.F., Pruinelli L., Park J.I., Dodd D., Keenan G.M., Senk P., Richesson R.L., Baukner V., Cruz C., Gao G., Whittenburg L., Delaney C.W.","Big data science: A literature review of nursing research exemplars",2017,"Nursing Outlook",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008339416&doi=10.1016%2fj.outlook.2016.11.021&partnerID=40&md5=b7f7233575d71198889aeda14f8f7e84","Background Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge. Purpose The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals. Methods A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice. Discussion Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods. Conclusion There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science. © 2016 Elsevier Inc.","Big data; Data science; Nurse scientist; Nursing informatics; Nursing research","analytic method; doctor patient relation; human; nurse; nursing informatics; nursing research; scientist; screening; systematic review; university; visibility; Web of Science; data base; data mining; nursing informatics; nursing research; procedures; Data Mining; Databases as Topic; Humans; Nursing Informatics; Nursing Research",2-s2.0-85008339416
"Zheng Z., Jeong H.-Y., Huang T., Shu J.","KDE based outlier detection on distributed data streams in multimedia network",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976307938&doi=10.1007%2fs11042-016-3681-y&partnerID=40&md5=9761700a58c32a9af4f6548697f61ec6","Multimedia networks hold the promise of facilitating large-scale, real-time data processing in complex environments. Their foreseeable applications will help protect and monitor military, environmental, safety-critical, or domestic infrastructures and resources. Cloud infrastructures promise to provide high performance and cost effective solutions to large scale data processing problems. This paper focused on the outlier detection over distributed data stream in real time, proposed kernel density estimation (KDE) based outlier detection algorithm KDEDisStrOut in Storm, firstly formalized the problem of outlier detection using the kernel density estimation technique and update the transported data incrementally between the child node and the coordinator node which reduces the communication cost. Then the paper adopted the exponential decay policy to keep pace with the transient and evolving natures of stream data and changed the weight of different data in the sliding window adaptively made the data analysis more reasonable. Theoretical analysis and experiments on Storm with synthetic and real data show that the KDEDisStrOut algorithm is efficient and effective compared with existing outlier detection algorithms, and more suitable for data streams. © 2016, Springer Science+Business Media New York.","Data stream; Distributed; Exponential decay policy; Kernel density estimation; Stream analysis","Complex networks; Cost effectiveness; Data communication systems; Data mining; Military applications; Signal detection; Statistics; Storms; Data stream; Distributed; Exponential decays; Kernel Density Estimation; Stream analysis; Data handling",2-s2.0-84976307938
"Gargiulo F., Silvestri S., Ciampi M.","A Big Data architecture for knowledge discovery in PubMed articles",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030527435&doi=10.1109%2fISCC.2017.8024509&partnerID=40&md5=1c07f1a4495bc6b2cacd8e6eda6b202a","The need of smart information retrieval systems is in contrast with the difficulties to deal with huge amount of data. In this paper we present a Big Data Analytics architecture used to implement a semantic similarity search tool for natural language texts in biomedical domain. The implemented methodology is based on Word Embeddings (WEs) models obtained using the word2vec algorithm. The system has been assessed with documents extracted from the whole PubMed library. It will be also presented a user friendly web front-end able to assess the methodology on a real context. © 2017 IEEE.","Big Data Analytics; Bio-Medical Literature; Natural Language Processing; PubMed; Semantic Similarity Search; SPARK; Word Embeddings","Data mining; Electric sparks; Information retrieval systems; Natural language processing systems; Search engines; Semantic Web; Semantics; Bio-medical; Data analytics; Embeddings; PubMed; Semantic similarity; Big data",2-s2.0-85030527435
"Mencagli G., Torquati M., Danelutto M., De Matteis T.","Parallel continuous preference queries over out-of-order and bursty data streams",2017,"IEEE Transactions on Parallel and Distributed Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029597012&doi=10.1109%2fTPDS.2017.2679197&partnerID=40&md5=4fdcf6bc070d2312215f79447728b914","Techniques to handle traffic bursts and out-of-order arrivals are of paramount importance to provide real-time sensor data analytics in domains like traffic surveillance, transportation management, healthcare and security applications. In these systems the amount of raw data coming from sensors must be analyzed by continuous queries that extract value-added information used to make informed decisions in real-time. To perform this task with timing constraints, parallelism must be exploited in the query execution in order to enable the real-time processing on parallel architectures. In this paper we focus on continuous preference queries, a representative class of continuous queries for decision making, and we propose a parallel query model targeting the efficient processing over out-of-order and bursty data streams. We study how to integrate punctuation mechanisms in order to enable out-of-order processing. Then, we present advanced scheduling strategies targeting scenarios with different burstiness levels, parameterized using the index of dispersion quantity. Extensive experiments have been performed using synthetic datasets and real-world data streams obtained from an existing real-time locating system. The experimental evaluation demonstrates the efficiency of our parallel solution and its effectiveness in handling the out-of-orderness degrees and burstiness levels of real-world applications. © 1990-2012 IEEE.","burstiness and traffic surges; continuous preference queries; data streams; multicores; out-of-order arrivals; Parallelism; sliding windows","Continuous time systems; Data communication systems; Decision making; Parallel architectures; Real time systems; Burstiness; Data stream; Multi-cores; Out-of-order arrival; Parallelism; Preference queries; Sliding Window; Data mining",2-s2.0-85029597012
"Zhai T., Gao Y., Wang H., Cao L.","Classification of high-dimensional evolving data streams via a resource-efficient online ensemble",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015843496&doi=10.1007%2fs10618-017-0500-7&partnerID=40&md5=79b0e03273911685c279539695fed923","A novel online ensemble strategy, ensemble BPegasos (EBPegasos), is proposed to solve the problems simultaneously caused by concept drifting and the curse of dimensionality in classifying high-dimensional evolving data streams, which has not been addressed in the literature. First, EBPegasos uses BPegasos, an online kernelized SVM-based algorithm, as the component classifier to address the scalability and sparsity of high-dimensional data. Second, EBPegasos takes full advantage of the characteristics of BPegasos to cope with various types of concept drifts. Specifically, EBPegasos constructs diverse component classifiers by controlling the budget size of BPegasos; it also equips each component with a drift detector to monitor and evaluate its performance, and modifies the ensemble structure only when large performance degradation occurs. Such conditional structural modification strategy makes EBPegasos strike a good balance between exploiting and forgetting old knowledge. Lastly, we first prove experimentally that EBPegasos is more effective and resource-efficient than the tree ensembles on high-dimensional data. Then comprehensive experiments on synthetic and real-life datasets also show that EBPegasos can cope with various types of concept drifts significantly better than the state-of-the-art ensemble frameworks when all ensembles use BPegasos as the base learner. © 2017, The Author(s).","Concept drift; Data stream classification; High dimensionality; Online ensemble","Budget control; Clustering algorithms; Data communication systems; Data mining; Trees (mathematics); Component classifiers; Concept drifts; Curse of dimensionality; Data stream classifications; High dimensionality; Online ensemble; Performance degradation; Structural modifications; Classification (of information)",2-s2.0-85015843496
"Chen Z.","A digital metrology process model (MPM) for measuring planning and data analysis and its application with a computer-aided system",2017,"International Journal of Advanced Manufacturing Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015992069&doi=10.1007%2fs00170-017-0294-1&partnerID=40&md5=3d94290a331dbf535ec5024ad906fe4a","With the rapid development of digital and flexible manufacturing technologies in the area of aircraft manufacturing, the digital metrologies and measurement data are applied and used not only in the phrase of product inspection but also in the phrases of product design and assembly. A reasonable plan is always needed for making good use of the digital measurement devices due to their complexities. In order to support the measuring planning and measurement data mining activities, in this paper, the digital measurement process is analyzed to build a digital metrology process model (MPM), which illustrates the contents of a measurement process with a three-dimensional structure, and reveals the measurement datum flow chains (MDFC). Then, the paper discusses the approaches of information management and measuring planning based on the metrology process model. To make them applicable, the digital measurement process platform is developed. The complete digital measurement system can be implemented by the DMPP application together with the digital measurement devices. The proposed model and methods provide a theoretically feasible way to effectively employ the digital metrologies in aircraft assembly. © 2017, Springer-Verlag London.","Digital metrology; Large-scale assembly; Measurement data analysis; Measuring planning; Metrology process model","Assembly; Computer aided analysis; Data handling; Data mining; Digital devices; Information analysis; Information management; Manufacture; Product design; Units of measurement; Aircraft manufacturing; Computer-aided systems; Digital measurement devices; Digital measurement systems; Digital metrology; Measurement data analysis; Process Modeling; Three-dimensional structure; Computer aided process planning",2-s2.0-85015992069
"Dzyuba V., van Leeuwen M., De Raedt L.","Flexible constrained sampling with guarantees for pattern mining",2017,"Data Mining and Knowledge Discovery",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016061080&doi=10.1007%2fs10618-017-0501-6&partnerID=40&md5=b282bd480e7de38f87769edb4979b0ce","Pattern sampling has been proposed as a potential solution to the infamous pattern explosion. Instead of enumerating all patterns that satisfy the constraints, individual patterns are sampled proportional to a given quality measure. Several sampling algorithms have been proposed, but each of them has its limitations when it comes to (1) flexibility in terms of quality measures and constraints that can be used, and/or (2) guarantees with respect to sampling accuracy. We therefore present Flexics, the first flexible pattern sampler that supports a broad class of quality measures and constraints, while providing strong guarantees regarding sampling accuracy. To achieve this, we leverage the perspective on pattern mining as a constraint satisfaction problem and build upon the latest advances in sampling solutions in SAT as well as existing pattern mining algorithms. Furthermore, the proposed algorithm is applicable to a variety of pattern languages, which allows us to introduce and tackle the novel task of sampling sets of patterns. We introduce and empirically evaluate two variants of Flexics: (1) a generic variant that addresses the well-known itemset sampling task and the novel pattern set sampling task as well as a wide range of expressive constraints within these tasks, and (2) a specialized variant that exploits existing frequent itemset techniques to achieve substantial speed-ups. Experiments show that Flexics is both accurate and efficient, making it a useful tool for pattern-based data exploration. © 2017, The Author(s).","Hashing-based sampling; Itemset mining; Pattern sampling; Pattern set mining; Tiling","Data mining; Constrained sampling; Frequent itemset techniques; Itemset mining; Pattern mining algorithms; Pattern samplings; Pattern set; Sampling accuracies; Tiling; Constraint satisfaction problems",2-s2.0-85016061080
"Cheng L., Liu F., Yao D.D.","Enterprise data breach: causes, challenges, prevention, and future directions",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020435155&doi=10.1002%2fwidm.1211&partnerID=40&md5=a6046eebfb11503b33b143c5b480c5c6","A data breach is the intentional or inadvertent exposure of confidential information to unauthorized parties. In the digital era, data has become one of the most critical components of an enterprise. Data leakage poses serious threats to organizations, including significant reputational damage and financial losses. As the volume of data is growing exponentially and data breaches are happening more frequently than ever before, detecting and preventing data loss has become one of the most pressing security concerns for enterprises. Despite a plethora of research efforts on safeguarding sensitive information from being leaked, it remains an active research problem. This review helps interested readers to learn about enterprise data leak threats, recent data leak incidents, various state-of-the-art prevention and detection techniques, new challenges, and promising solutions and exciting opportunities. WIREs Data Mining Knowl Discov 2017, 7:e1211. doi: 10.1002/widm.1211. For further resources related to this article, please visit the WIREs website. © 2017 The Authors. WIREs Data Mining and Knowledge Discovery published by John Wiley & Sons, Ltd.",,"Data mining; Confidential information; Critical component; Enterprise data; Reputational damage; Research efforts; Research problems; Sensitive informations; State of the art; Losses",2-s2.0-85020435155
"Li J., Singh R., Singh R.","A novel large-scale multimedia image data classification algorithm based on mapping assisted deep neural network",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009863270&doi=10.1007%2fs11042-017-4364-z&partnerID=40&md5=78d95f0df1fc0d7b11cd12d3e935e1f0","With the increasing number of the images, how to effectively manage and use these images becomes an urgent problem to be solved. The classification of the images is one of the effective ways to manage and retrieve images. In this paper, we propose a novel large-scale multimedia image data classification algorithm based on deep learning. We firstly select the image characteristics to represent the flag for retrieval, which represents the color, texture and shape characteristics respectively. A feature of color is the most basic image data, mainly including the average brightness, color histogram and dominant color, etc. What the texture refers to is the image data in the anomalous, macroscopic as well as orderly one key character that on partial has. The contour feature extraction of image data needs to rely on the edge detection, edge of the detected edge through the connection or grouping to form a meaningful image event. Secondly, we revise the convolutional neural network model based on the pooling operation optimization, the pooling is in the process of the convolution operation to extract the image characteristics of the different locations to gather statistics. Furthermore, we integrate the parallel and could storage strategy to enhance the efficiency of the proposed methodology. The performance of the algorithm is verified, compared with the other state-of-the-art approaches, the proposed one obtains the better efficiency and accuracy. © 2017, Springer Science+Business Media New York.","Convolutional neural network; Deep learning; Image data classification; Information retrieval; Large-scale system; Multimedia architecture","Classification (of information); Color; Convolution; Data mining; Digital storage; Edge detection; Efficiency; Feature extraction; Image classification; Image processing; Information retrieval; Large scale systems; Neural networks; Optimization; Search engines; Convolutional neural network; Deep learning; Image characteristics; Image data; Multimedia architecture; Operation optimization; Shape characteristics; State-of-the-art approach; Image texture",2-s2.0-85009863270
"Žliobaite I., Puolamäki K., Eronen J.T., Fortelius M.","A survey of computational methods for fossil data analysis",2017,"Evolutionary Ecology Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030976231&partnerID=40&md5=c788fc737511180774c6439611a350a0","Aim: (1) Survey and organize computational approaches to fossil data analysis into a methodological framework. (2) Highlight the kinds of research questions about evolutionary and environmental change that can be answered by applying computational algorithms to mammal fossil data to better understand past ecosystems and climates. Questions: What models have been used for what research questions? What is their scope of application? What are their potential limitations? Search methods: Our search of the literature was based on personal knowledge in combination with keyword-based searches. Papers were considered relevant if data-driven computational methods were used to analyse relationships between organisms and their environments at evolutionary time scales. Conclusions: We demonstrate that different research questions may be answered with the same computational algorithm, and different algorithms may be needed to answer the same question in different contexts. We believe that in order to move forward, we need to match knowledge of methods with knowledge of the fossil record in a research question-driven way. Figure 2 presents a proposed workflow. Following this framework, we survey existing work and highlight what research questions can potentially be answered with which methods, some of which may not have been reported in the evolutionary palaeontology literature to date. The outcome of this survey is a proposal for a research agenda in computational fossil data analysis. © 2017 Indre. Žliobaite.","Big data; Computational fossil data analysis; Data mining; Ecometrics; Evolutionary palaeontology; Machine learning; Mammals",,2-s2.0-85030976231
"Fu X., Chen X., Shi Y.-T., Bose I., Cai S.","User segmentation for retention management in online social games",2017,"Decision Support Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020419611&doi=10.1016%2fj.dss.2017.05.015&partnerID=40&md5=741652033d3c84a1f414db08f3ba1d07","This work proposes an innovative model for segmenting online players based on data related to their in-game behaviours to support player retention management. This kind of analysis is helpful to explore the potential reasons behind why players leave the game, analyse retention trends, design customised strategies for different player segments, and then boost the overall retention rate. In particular, a new similarity metric which is driven by players' stickiness to the game is developed to cluster players. Three feature dimensions, namely engagement features (e.g., log-in frequency and length of log-in time), performance features (e.g., level, the number of completed tasks, coins and achievements), and social interactions features (e.g., the number of in-game friends, whether or not to join a guild, and the guild role), are employed and aggregated to derive the stickiness metric. The applicability and utility of this new segmentation model are illustrated through experiments that are conducted on a realistic MMORPG dataset. The derived results are also discussed and compared against two benchmark models. The results reveal that the new segmentation model not only achieves better clustering performance, but also improves player's lifetime prediction by better distinguishing between loyal customers and churners. The empirical results confirm the effects of social interaction, which is usually underestimated in the current research, on player segmentation. From an operational perspective, the derived results would help game developers better understand the different retention-behaviour patterns of players, establish effective and customised tactics to retain more players, and boost product revenue. © 2017","Clustering analysis; Game data mining; Online social games; Retention management","Data mining; Image segmentation; Social sciences; Clustering analysis; Feature dimensions; Game data minings; Lifetime prediction; Player segmentation; Segmentation models; Social games; Social interactions; Social networking (online)",2-s2.0-85020419611
"Yun U., Lee G., Yoon E.","Efficient High Utility Pattern Mining for Establishing Manufacturing Plans with Sliding Window Control",2017,"IEEE Transactions on Industrial Electronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029514052&doi=10.1109%2fTIE.2017.2682782&partnerID=40&md5=da801e92171b8b60c43820c06e631b04","In industrial areas, understanding the preference of customers is one of the important considerations for establishing profitable product manufacturing plans. As one of the approaches in pattern mining, high utility pattern mining has been employed to find a set of products creating high profits by considering the purchase quantity and price of each product. In this regard, high utility pattern mining can be useful to establish profitable product manufacturing plans that allow a corporation to maximize its revenue. For establishing manufacturing plans, we also need to understand the recent preference of customers from stream data, which are continually generated without limitations. In this paper, we propose a novel algorithm and list structure for finding high utility patterns over data streams on the basis of a sliding window mode. Unlike existing algorithms, the proposed algorithm does not consume huge computational resources for verifying candidate patterns because it can avoid the generation of candidate patterns. Therefore, the algorithm efficiently works in complex dynamic systems. Experimental results obtained from various tests using real-world dataset show that the proposed algorithm outperforms state-of-The-Art methods in terms of runtime, memory usage, and scalability. © 1982-2012 IEEE.","Industrial systems; manufacturing plan; sliding window-based utility pattern mining","Data mining; Profitability; Sales; Statistical tests; Candidate patterns; Complex dynamic systems; Computational resources; Industrial systems; Manufacturing plans; Pattern mining; Product manufacturing; State-of-the-art methods; Manufacture",2-s2.0-85029514052
"Stenzel O., Pecho O., Holzer L., Neumann M., Schmidt V.","Big data for microstructure-property relationships: A case study of predicting effective conductivities",2017,"AIChE Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018325269&doi=10.1002%2faic.15757&partnerID=40&md5=b270a1bd6f0748a9ae3039421e6827f7","The analysis of big data is changing industries, businesses and research as large amounts of data are available nowadays. In the area of microstructures, acquisition of (3-D tomographic image) data is difficult and time-consuming. It is shown that large amounts of data representing the geometry of virtual, but realistic 3-D microstructures can be generated using stochastic microstructure modeling. Combining the model output with physical simulations and data mining techniques, microstructure-property relationships can be quantitatively characterized. Exemplarily, we aim to predict effective conductivities given the microstructure characteristics volume fraction, mean geodesic tortuosity, and constrictivity. Therefore, we analyze 8119 microstructures generated by two different stochastic 3-D microstructure models. This is—to the best of our knowledge—by far the largest set of microstructures that has ever been analyzed. Fitting artificial neural networks, random forests and classical equations, the prediction of effective conductivities based on geometric microstructure characteristics is possible. © 2017 American Institute of Chemical Engineers AIChE J, 63: 4224–4232, 2017. © 2017 American Institute of Chemical Engineers","big data; effective conductivity; geodesic tortuosity; microstructure characteristics; predictive simulation; stochastic microstructure modeling","Data mining; Decision trees; Forecasting; Geodesy; Microstructure; Neural networks; Stochastic models; Stochastic systems; Tomography; Effective conductivity; Geodesic tortuosity; Microstructure characteristics; Predictive simulations; Stochastic microstructures; Big data",2-s2.0-85018325269
"Shi R.-H., Zhang S.","Quantum solution to a class of two-party private summation problems",2017,"Quantum Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026640076&doi=10.1007%2fs11128-017-1676-x&partnerID=40&md5=91792179f85fbf42194a94f9129f37dd","In this paper, we define a class of special two-party private summation (S2PPS) problems and present a common quantum solution to S2PPS problems. Compared to related classical solutions, our solution has advantages of higher security and lower communication complexity, and especially it can ensure the fairness of two parties without the help of a third party. Furthermore, we investigate the practical applications of our proposed S2PPS protocol in many privacy-preserving settings with big data sets, including private similarity decision, anonymous authentication, social networks, secure trade negotiation, secure data mining. © 2017, Springer Science+Business Media, LLC.","Privacy-preserving; Quantum computation; Quantum cryptography; Secure multi-party quantum computation","Big data; Cryptography; Data mining; Data privacy; Network security; Quantum computers; Anonymous authentication; Classical solutions; Communication complexity; Privacy preserving; Secure data mining; Summation problem; Third parties; Trade negotiations; Quantum cryptography",2-s2.0-85026640076
"Wang S., Hu Q., Wang F., Ai M., Zhong R.","A microtopographic feature analysis-based LiDAR data processing approach for the identification of Chu tombs",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029357626&doi=10.3390%2frs9090880&partnerID=40&md5=cd9387b0079ad7f40162b06d7a39b7b8","Most of the cultural sites hidden under dense vegetation in the mountains of China have been destroyed. In this paper, we present a microtopographic feature analysis (MFA)-based Light Detection and Ranging (LiDAR) data processing approach and an archaeological pattern-oriented point cloud segmentation (APoPCS) algorithm that we developed for the classification of archaeological objects and terrain points and the detection of archaeological remains. The archaeological features and patterns are interpreted and extracted from LiDAR point cloud data to construct an archaeological object pattern database. A microtopographic factor is calculated based on the archaeological object patterns, and this factor converts the massive point cloud data into a raster feature image. A fuzzy clustering algorithm based on the archaeological object patterns is presented for raster feature image segmentation and the detection of archaeological remains. Using the proposed approach, we investigated four typical areas with different types of Chu tombs in Central China, which had dense vegetation and high population densities. Our research results show that the proposed LiDAR data processing approach can identify archaeological remains from large-volume and massive LiDAR data, as well as in areas with dense vegetation and trees. The studies of different archaeological object patterns are important for improving the robustness of the proposed APoPCS algorithm for the extraction of archaeological remains. © 2017 by the authors.","Archaeological object; Feature extraction; Fuzzy cluster; LiDAR; Microtopographic feature analysis","Clustering algorithms; Data handling; Data mining; Extraction; Feature extraction; Fuzzy clustering; Image segmentation; Population statistics; Rasterization; Trees (mathematics); Vegetation; Archaeological features; Archaeological objects; Feature analysis; High population density; Lidar point cloud datum; Light detection and ranging; Point cloud segmentation; Processing approach; Optical radar",2-s2.0-85029357626
"Valkanas G., Lappas T., Gunopulos D.","Mining Competitors from Large Unstructured Datasets",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029350548&doi=10.1109%2fTKDE.2017.2705101&partnerID=40&md5=722a82d80a152bbb8a588df8e27ef024","In any competitive business, success is based on the ability to make an item more appealing to customers than the competition. A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness between two items? Who are the main competitors of a given item? What are the features of an item that most affect its competitiveness? Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution. In this paper, we present a formal definition of the competitiveness between two items, based on the market segments that they can both cover. Our evaluation of competitiveness utilizes customer reviews, an abundant source of information that is available in a wide range of domains. We present efficient methods for evaluating competitiveness in large review datasets and address the natural problem of finding the top-k competitors of a given item. Finally, we evaluate the quality of our results and the scalability of our approach using multiple datasets from different domains. © 1989-2012 IEEE.","Data mining; electronic commerce; information search and retrieval; web mining","Data mining; Electronic commerce; Information retrieval; Quality control; Competitive business; Customer review; Different domains; Effective solution; Formal definition; Information search and retrieval; Multiple data sets; Web Mining; Competition",2-s2.0-85029350548
"Ait-Mlouk A., Gharnati F., Agouti T.","An improved approach for association rule mining using a multi-criteria decision support system: a case study in road safety",2017,"European Transport Research Review",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026648269&doi=10.1007%2fs12544-017-0257-5&partnerID=40&md5=93e1d72992813690f3fdeb75311af074","Purpose: Road accidents have come to be considered a major public health problem worldwide. The aim of many studies is therefore to identify the main factors contributing to the severity of crashes. Methods: This paper examines a large-scale data mining technique known as association rule mining, which can predict future accidents in advance and allow drivers to avoid the dangers. However, this technique produces a very large number of decision rules, preventing decision makers from making their own selection of the most relevant rules. In this context, the integration of a multi-criteria decision analysis approach would be particularly useful for decision makers affected by the redundancy of the extracted rules. Conclusion: An analysis of road accidents in the province of Marrakech (Morocco) between 2004 and 2014 shows that the proposed approach serves this purpose; it may provide meaningful information that could help in developing suitable prevention policies to improve road safety. © 2017, The Author(s).","Association rules; Data mining; Multi-criteria decision analysis; Quality measurements; Road accident","Accident prevention; Accidents; Artificial intelligence; Association rules; Decision making; Decision support systems; Highway accidents; Motor transportation; Quality control; Roads and streets; Transportation; Decision makers; Decision rules; Large scale data; Multi-criteria decision analysis; Multi-criteria decision support systems; Prevention policy; Quality measurements; Road safety; Data mining",2-s2.0-85026648269
"Telikani A., Shahbahrami A.","Optimizing association rule hiding using combination of border and heuristic approaches",2017,"Applied Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017478837&doi=10.1007%2fs10489-017-0906-3&partnerID=40&md5=6e3dd5a4154b79c5ea1b80f0e9289677","Data sanitization process transforms the original database into a modified database to protect the disclosure of sensitive knowledge by reducing the confidence/support of patterns. This process produces side-effects on the sanitized database, where some non-sensitive patterns are lost or new patterns are produced. Recently, a number of approaches have been proposed to minimize these side-effects by selecting appropriate transactions/items for sanitization. The heuristic approach is applied to hide sensitive patterns both in association rules and in frequent itemsets. On the other hand, the border, exact, and evolutionary approaches have only been designed to hide frequent itemsets. In this paper, a new hybrid algorithm, called Decrease the Confidence of Rule (DCR), proposed to improve a border-based solution, namely MaxMin, using two heuristics to hide the association rules. To achieve this, first, a heuristic was formulated in combination with MaxMin solution to select victim items in order to control the impact of sanitization process on result quality. Then, the victim items were removed from transactions with the shortest length. Some experiments have been conducted on the four real datasets to compare performance of DCR with the Association Rule Hiding based on Intersection Lattice (ARHIL) algorithm. The experimental results showed that the proposed algorithm yielded fewer side-effects than ARHIL algorithm. In addition, its efficiency was better than the heuristic approach. © 2017, Springer Science+Business Media New York.","Association rule hiding; Association rule mining; Data mining; Data sanitization; Privacy preserving in data mining","Association rules; Data privacy; Database systems; Heuristic methods; Evolutionary approach; Heuristic approach; Hybrid algorithms; Intersection lattices; Privacy preserving; Sanitization; Sanitization process; Sanitized database; Data mining",2-s2.0-85017478837
"Gao D., Huang M.","Prediction of remaining useful life of lithium-ion battery based on multi-kernel support vector machine with particle swarm optimization",2017,"Journal of Power Electronics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029670819&doi=10.6113%2fJPE.2017.17.5.1288&partnerID=40&md5=0d29a8be70f1d671a87750356e50c0d7","The estimation of the remaining useful life (RUL) of lithium-ion (Li-ion) batteries is important for intelligent battery management system (BMS). Data mining technology is becoming increasingly mature, and the RUL estimation of Li-ion batteries based on data-driven prognostics is more accurate with the arrival of the era of big data. However, the support vector machine (SVM), which is applied to predict the RUL of Li-ion batteries, uses the traditional single-radial basis kernel function. This type of classifier has weak generalization ability, and it easily shows the problem of data migration, which results in inaccurate prediction of the RUL of Li-ion batteries. In this study, a novel multi-kernel SVM (MSVM) based on polynomial kernel and radial basis kernel function is proposed. Moreover, the particle swarm optimization algorithm is used to search the kernel parameters, penalty factor, and weight coefficient of the MSVM model. Finally, this paper utilizes the NASA battery dataset to form the observed data sequence for regression prediction. Results show that the improved algorithm not only has better prediction accuracy and stronger generalization ability but also decreases training time and computational complexity. © 2017 KIPE.","Lithium-ion battery RUL; Multi-kernel support vector machine; Particle swarm optimization algorithm; RUL prediction","Battery management systems; Big data; Data mining; Electric batteries; Forecasting; Ions; Lithium; NASA; Optimization; Particle swarm optimization (PSO); Support vector machines; Data mining technology; Data-driven prognostics; Generalization ability; Multi-kernel; Particle swarm optimization algorithm; Regression predictions; Remaining useful lives; Rul predictions; Lithium-ion batteries",2-s2.0-85029670819
"Pitchayaviwat T.","A study on clustering customer suggestion on online social media about insurance services by using text mining techniques",2017,"2016 Management and Innovation Technology International Conference, MITiCON 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031753938&doi=10.1109%2fMITICON.2016.8025228&partnerID=40&md5=947997c8f7fb6e3162cb7903b7500dc3","Now a day Social media communication become to important factor for business operation. Several Customer prefers to post their comment, suggestion, complaints about company's products and services to online media such as Facebook, Twitter, Social web board because it easy way to blast to public and increases pressure to product owner for responding. This is one factor that cooperate need to be concern and manage responding to customer services that match to customer requirements by analyzes customer suggestion on social media vice versa they can detect negative feedback or complaints early which, able to prevent their reputation. This study was collected text that contains customer suggestion on insurance services from various online social media and extract some specific word via Thai text segmentation and coverts text to Vector Space Model (VSM) based on TF-IDF. We performs experiment by used 800 records of textcrawler and implement two clustering models algorithm which include K-Means and Self-Organization Map (SOM) for clustering suggestion text into three cluster groups as follow Cluster-0 is about to customer feedback on Car Insurance Policy, Car Insurance Premium or Insurance Renewal, Cluster-1 is contains customer feedback on insurance claim services, Cluster-2 is about customer enquired general information. We use 'Davies-Bouldin index' method[3] for evaluating both clustering algorithms. A result of experiment shows that K-Means has a significant performance higher than SOM. Finally, The benefit of this study able to help insurance company improve their products and services and increase customer satisfaction and retention strategies planning. © 2016 IEEE.","Clustering; K-Means; SOM; Text Mining","Clustering algorithms; Data mining; Engineering research; Insurance; Sales; Social networking (online); Text processing; Vector spaces; Clustering; Customer requirements; Davies-Bouldin index; K-means; Products and services; Self-organization maps; Text mining; Text mining techniques; Customer satisfaction",2-s2.0-85031753938
"Desquesnes X., Elmoataz A.","Nonmonotonic Front Propagation on Weighted Graphs with Applications in Image Processing and High-Dimensional Data Classification",2017,"IEEE Journal on Selected Topics in Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028927585&doi=10.1109%2fJSTSP.2017.2731520&partnerID=40&md5=860ef47a45ffd57a9902edbf387e74e2","In this paper, we propose an adaptation of partial difference equations (PDEs) level set method for nonmonotonic front propagation on weighted graphs. This adaptation leads to a PDE, whose coefficients are data geometry dependent. Our motivation is to extend their applications to any discrete data that can be represented by a weighted graph. This paper follows several preliminaries of our works, and introduces several significant improvements: A simplified and explicit representation of a front on a weighted graph, a new formulation of the level set equation on weighted graphs considering both time-dependent and stationary versions of this equation in the case of signed velocities, and an efficient algorithm that generalized the fast marching to graphs with signed velocities. We propose to use this method for image processing and for high-dimensional data classification. © 2007-2012 IEEE.","Data classification; image processing; level set; non-monotonic front propagation; weighted graphs","Classification (of information); Clustering algorithms; Data mining; Difference equations; Graphic methods; Image classification; Mathematical models; Numerical methods; Signal processing; Surface discharges; Data classification; Front propagation; Level Set; Signal processing algorithms; Weighted graph; Image processing",2-s2.0-85028927585
"Baralis E., Cagliero L., Cerquitelli T., Garza P., Pulvirenti F.","Discovering profitable stocks for intraday trading",2017,"Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017430413&doi=10.1016%2fj.ins.2017.04.013&partnerID=40&md5=128b61c4ed2bdb98702115ae5957343e","Intraday traders buy and sell financial instruments in the short term, typically within the same trading day. Stocks are notable examples of financial instruments. However, since hundreds of stocks are listed on the stock exchange selecting on each trading day the most tradeable stocks is a challenging task, which is commonly addressed through manual inspection of historical stock prices and technical indicators. This paper aims at discovering tradeable stocks on a given trading day by analyzing the historical prices assumed by the same stocks or by other ones on the preceding days by means of regression and weighted sequence mining techniques. The use of regression and weighted sequence mining techniques allows traders to automatically consider a potentially large number of candidate stocks and to effectively analyze their price variations across consecutive days. The experimental results, which were achieved on data acquired from different markets and under different market conditions, show that sequence mining algorithms yield profits higher than both regression techniques and naive strategies. © 2017 Elsevier Inc.","Sequence mining; Stock data mining; Weighted data","Commerce; Costs; Data mining; Financial markets; Profitability; Regression analysis; Manual inspection; Market condition; Price variation; Regression techniques; Sequence mining; Technical indicator; Weighted data; Weighted sequences; Electronic trading",2-s2.0-85017430413
"Chen Q., Lan C., Chen B., Wang L., Li J., Zhang C.","Exploring Consensus RNA Substructural Patterns Using Subgraph Mining",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032449614&doi=10.1109%2fTCBB.2016.2645202&partnerID=40&md5=c0fa2361aecd5f04c6e24fb4a272fa74","Frequently recurring RNA ?> structural motifs play important roles in RNA folding process and interaction with other molecules. Traditional index-based and shape-based schemas are useful in modeling RNA secondary structures but ignore the structural discrepancy of individual RNA family member. Further, the in-depth analysis of underlying substructure pattern is insufficient due to varied and unnormalized substructure data. This prevents us from understanding RNAs functions and their inherent synergistic regulation networks. This article thus proposes a novel labeled graph-based algorithm RnaGraph to uncover frequently RNA substructure patterns. Attribute data and graph data are combined to characterize diverse substructures and their correlations, respectively. Further, a top-k graph pattern mining algorithm is developed to extract interesting substructure motifs by integrating frequency and similarity. The experimental results show that our methods assist in not only modelling complex RNA secondary structures but also identifying hidden but interesting RNA substructure patterns. © 2017 IEEE.","Data mining; RNA; subgraph; substructure; support","Data mining; Graphic methods; Nucleic acids; RNA; Supports; In-depth analysis; Regulation networks; RNA secondary structures; Structural motifs; Subgraph mining; Subgraphs; substructure; Substructure patterns; Bioinformatics",2-s2.0-85032449614
"Balameena M., Rohini K., Suseendran G.","Analysis of algorithms used in opinion mining",2017,"Journal of Advanced Research in Dynamical and Control Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029479697&partnerID=40&md5=4225c4e3d2fbe5df7b87449cd8212969","Now a days huge amount of data in the web for the internet users. In the users not only use the resources in the web, they also give the review and additional information. This review is not only important for a user but is also useful for an organization. What other people think-has always been an important piece of information for most of us during the decision making process. It becomes impossible to manually analyze the reviews, so we are going to effective opinion mining. In this paper we focus the types of algorithm using in opinion mining. These algorithms are helpful to challenge the decision making problem good or bad. © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.","Data mining; K-Nearest neighbor; Naive bayes algorithm; Opinion mining; Support vector machines",,2-s2.0-85029479697
"Karthika K., Devi R.","Developing a natural language interface with semantic matching to complex data",2017,"Journal of Advanced Research in Dynamical and Control Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031738333&partnerID=40&md5=d6e1a915a90a121b7f42b3d96684e6d9","Information Technology has been playing an important role in our lives. The major source of information is databases. Databases and its technology are having major impact on the highly developing use of computers. To retrieve information from a database, we need to formulate a query in such a way that the computer can understand and produce the necessary output that users required. The SQL norms are been pursued in almost all languages for RDBMS systems. Everybody can not able to write SQL queries that they may not be aware of the structure of database. So there may be a need for users whom were non-expert to query relational databases in their natural language not working with the values of the attributes. This idea of using natural language instead of SQL, has promoted the development for Natural Language Interface to the Database systems. The need of Natural Language Interface to the Database systems is increasing day by day as many more people access information through web browsers, Personal Digital Assistance and cell phones. This paper introduces an intelligent interface for the database. This theory proves that our Natural Language Interface to the Database systems are guaranteed to map a natural language query to corresponding SQL query. © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.","Data mining; Data reduction; Fuzzy logic; NLIDB; Prediction",,2-s2.0-85031738333
"Torra V.","Fuzzy microaggregation for the transparency principle",2017,"Journal of Applied Logic",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007109704&doi=10.1016%2fj.jal.2016.11.007&partnerID=40&md5=1e334dfb9575569abfebbb5810bdcb49","Microaggregation has been proven to be an effective method for data protection in the areas of Privacy Preserving Data Mining (PPDM) and Statistical Disclosure Control (SDC). This method consists of applying a clustering method to the data set to be protected, and then replacing each of the data by the cluster representative. In this paper we propose a new method for microaggregation based on fuzzy clustering. This new approach has been defined with the main goal of being nondeterministic on the assignment of cluster centers to the original data, and at the same time being simple in its definition. Being nondeterministic permits us to overcome some of the attacks standard microaggregation suffers. © 2016 Elsevier B.V.","Application of fuzzy sets theory; Data privacy; Fuzzy c-means; Fuzzy clustering; Microaggregation; Transparency attacks","Data mining; Fuzzy clustering; Transparency; Cluster centers; Clustering methods; Fuzzy C mean; Fuzzy sets theory; Microaggregation; New approaches; Privacy-preserving data mining; Statistical disclosure Control; Data privacy",2-s2.0-85007109704
"Thomaz G.M., Biz A.A., Bettoni E.M., Mendes-Filho L., Buhalis D.","Content mining framework in social media: A FIFA world cup 2014 case analysis",2017,"Information and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008882710&doi=10.1016%2fj.im.2016.11.005&partnerID=40&md5=e7cb988eedfb03caa02c696cd4b15bf4","This paper proposes a social media content mining framework that consists of seven phases. The framework was tested empirically during the FIFA World Cup 2014 at Curitiba (Brazil) as one of the main host city destinations. The research focused on the mining of Twitter content with tourist services ontology (hospitality, food and beverages, and transportation). In total, 58,686 valid messages were collected, analyzed, and associated with an application ontology. Content analysis demonstrated an accurate real-time reflection of tourism services. The framework is effective to collect relevant content and identify popular topics in social media toward strategic and operational tourism management. © 2016 Elsevier B.V.","Brazil; Content mining; FIFA world cup 2014; Social media; Tourist services; Twitter","Data mining; Ontology; Brazil; Content mining; Social media; Tourist services; Twitter; World cup; Social networking (online)",2-s2.0-85008882710
"Najar D., Mesfar S.","Opinion mining and sentiment analysis for Arabic on-line texts: application on the political domain",2017,"International Journal of Speech Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020705547&doi=10.1007%2fs10772-017-9422-4&partnerID=40&md5=52989a3e805f8c7511603c0f9ceca7c3","Since the continuous proliferation of the journalistic content online and the changing political landscape in many Arabic countries, we started our current research in order to implement a media monitoring system about the opinion mining in political field. This system allows political actors, despite of the large volume of online data, to be constantly informed about opinions expressed on the web in order to properly monitor their actual standing, orient their communication strategy and prepare the election campaigns. The developed system is based on a linguistic approach using NooJ’s linguistic engine to formalize the automatic recognition rules and apply them to a dynamic corpus composed of journalistic articles. The first implemented rules allow identifying and annotating the different political entities (political actors and organizations). Then these annotations are used in our system of media monitoring in order to identify the opinions associated with the extracted named entities. The system is mainly based on a set of local grammars developed for the identification of different structures of the political opinion phrases. These grammars are using the entries of the opinion lexicon that contain the different opinion words (verbs, adjectives, nouns) where each entry is associated with the corresponding semantic marker (polarity and intensity). Our developed system is able to identify and properly annotate the opinion holder, the opinion target and the polarity (positive or negative) of the phraseological expression (nominal or verbal) expressing the opinion. Our experiments showed that the adopted method of extraction is consistent with 0.83 F-measure. © 2017, Springer Science+Business Media New York.","Arabic language; Opinion mining; Politic; Sentiment analysis; TAL; Web media","Data mining; Linguistics; Natural language processing systems; Semantics; Social sciences; Arabic languages; Opinion mining; Politic; Sentiment analysis; Web media; Monitoring",2-s2.0-85020705547
"Bedboudi A., Bouras C., Kimour M.T.","An heterogeneous population-based genetic algorithm for data clustering",2017,"Indonesian Journal of Electrical Engineering and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031826711&doi=10.11591%2fijeei.v5i3.299&partnerID=40&md5=ad7cec36b5dd9618a16a90b5e79b1e64","As a primary data mining method for knowledge discovery, clustering is a technique of classifying a dataset into groups of similar objects. The most popular method for data clustering K-means suffers from the drawbacks of requiring the number of clusters and their initial centers, which should be provided by the user. In the literature, several methods have proposed in a form of k-means variants, genetic algorithms, or combinations between them for calculating the number of clusters and finding proper clusters centers. However, none of these solutions has provided satisfactory results and determining the number of clusters and the initial centers are still the main challenge in clustering processes. In this paper we present an approach to automatically generate such parameters to achieve optimal clusters using a modified genetic algorithm operating on varied individual structures and using a new crossover operator. Experimental results show that our modified genetic algorithm is a better efficient alternative to the existing approaches. © 2017, Institute of Advanced Engineering and Science. All rights reserved.","Clustering; Crossover; Data mining; Genetic algorithms; K-means",,2-s2.0-85031826711
"Costantino G., La Marra A., Martinelli F., Saracino A., Sheikhalishahi M.","Privacy-preserving text mining as a service",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030559869&doi=10.1109%2fISCC.2017.8024639&partnerID=40&md5=cd1d6fe1c9355700d1201e3ac6763461","Text mining is the process to automatically infer relevant information from semantically related text documents. This technique, which has applications from business intelligence to homeland security, terrorism and crime fight, might bring noticeable privacy issues when analyzed documents contain privacy sensitive information. In this paper, we propose a framework for privacy-preserving text analysis, which exploits Homomorphic Encryption, to analyze text documents in a privacy preserving manner. The proposed framework is designed to ensure that there is no disclosure of privacy sensitive information contained in the document to any party, including the analysis engine itself. Furthermore, we present two use cases of analysis based on bag-of-words classification, where the proposed framework manages to obtain good classification results without information disclosure. In particular the two different settings that are considered are: tweet analysis for detection of terrorist Twitter accounts, and out-box email analysis for detection of bot infected devices. Accuracy results with different classifiers, performances and a security analysis of our approach are presented and discussed. © 2017 IEEE.",,"Classification (of information); Cryptography; Data privacy; Security systems; Terrorism; Classification results; E-mail analysis; Ho-momorphic encryptions; Information disclosure; Privacy issue; Privacy preserving; Security analysis; Sensitive informations; Data mining",2-s2.0-85030559869
"Mukherjee A.P., Tirthapura S.","Enumerating Maximal Bicliques from a Large Graph Using MapReduce",2017,"IEEE Transactions on Services Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032269240&doi=10.1109%2fTSC.2016.2523997&partnerID=40&md5=dc9a50dc262d6982337c9217d71d20c0","We consider the enumeration of maximal bipartite cliques (bicliques) from a large graph, a task central to many data mining problems arising in social network analysis and bioinformatics. We present novel parallel algorithms for the MapReduce framework, and an experimental evaluation using Hadoop MapReduce. Our algorithm is based on clustering the input graph into smaller subgraphs, followed by processing different subgraphs in parallel. Our algorithm uses two ideas that enable it to scale to large graphs: (1) the redundancy in work between different subgraph explorations is minimized through a careful pruning of the search space, and (2) the load on different reducers is balanced through a task assignment that is based on an appropriate total order among the vertices. We show theoretically that our algorithm is work optimal, i.e., it performs the same total work as its sequential counterpart. We present a detailed evaluation which shows that the algorithm scales to large graphs with millions of edges and tens of millions of maximal bicliques. To our knowledge, this is the first work on maximal biclique enumeration for graphs of this scale. © 2017 IEEE.","biclique; Graph mining; hadoop; mapreduce; maximal biclique enumeration; parallel algorithm","Data mining; Parallel algorithms; Biclique; Data mining problems; Experimental evaluation; Graph mining; hadoop; Hadoop MapReduce; Map-reduce; Mapreduce frameworks; Clustering algorithms",2-s2.0-85032269240
"Hoffait A.-S., Schyns M.","Early detection of university students with potential difficulties",2017,"Decision Support Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018867661&doi=10.1016%2fj.dss.2017.05.003&partnerID=40&md5=225dc91ac8bffa81a2363bc34d2456fe","Using data mining methods, this paper presents a new means of identifying freshmen's profiles likely to face major difficulties to complete their first academic year. Academic failure is a relevant issue at a time when post-secondary education is ever more critical to economic success. We aim at early detection of potential failure using student data available at registration, i.e. school records and environmental factors, with a view to timely and efficient remediation and/or study reorientation. We adapt three data mining methods, namely random forest, logistic regression and artificial neural network algorithms. We design algorithms to increase the accuracy of the prediction when some classes are of major interest. These algorithms are context independent and can be used in different fields. Real data pertaining to undergraduates at the University of Liège (Belgium), illustrates our methodology. © 2017 Elsevier B.V.","Accuracy; Classification; Machine learning; Prediction; Remediation; Student attrition","Classification (of information); Data mining; Decision trees; Forecasting; Learning systems; Neural networks; Remediation; Students; Accuracy; Artificial neural network algorithm; Context independent; Data mining methods; Environmental factors; Logistic regressions; Postsecondary education; University students; Education",2-s2.0-85018867661
"Guan X., Liu G., Huang C., Liu Q., Wu C., Jin Y., Li Y.","An Object-Based Linear Weight Assignment Fusion Scheme to Improve Classification Accuracy Using Landsat and MODIS Data at the Decision Level",2017,"IEEE Transactions on Geoscience and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029007913&doi=10.1109%2fTGRS.2017.2737780&partnerID=40&md5=80f7f3635fcc895aa7f76a0eb3def55d","Landsat satellite images are extensively used in land-use studies due to their relatively high spatial resolution. However, the number of usable data sets is limited by the relatively long revisit interval and phenology effects can significantly reduce classification accuracy. Moderate Resolution Imaging Spectroradiometer (MODIS) images have higher temporal frequency and can provide extra time-series information. However, they are limited in their capability to classify heterogeneous landscapes due to their coarse spatial resolution. Fusion of different data sources is a potential solution for improving land-cover classification. This paper proposes a fusion scheme to combine Landsat and MODIS remote sensing data at the decision level. First, multiresolution segmentations on the two kinds of remote sensing data are performed to identify the landscape objects and are used as fusion units in subsequent steps. Then, fuzzy classifications are applied to each of the two different resolution data sets and the classification accuracies are evaluated. According to the performance of the two data sets in classification evaluation, a simple weight assignment technique based on the weighted sum of the membership of imaged objects is implemented in the final classification decision. The weighting factors are calculated based on a confusion matrix and the heterogeneity of detected land cover. The algorithm is capable of integrating the time-series spectral information of MODIS data with spatial contexts extracted from Landsat data, thus improving the land-cover classification accuracy. The overall classification accuracy using the fusion technique increased by 7.43&#x0025; and 10.46&#x0025; compared with the results from the individual Landsat and MODIS data, respectively. IEEE","Data mining; Decision fusion; Earth; fuzzy sets; image classification; image segmentation; MODIS; Remote sensing; remote sensing; Satellites; Spatial resolution; time-series analysis","Data mining; Earth (planet); Forestry; Fuzzy sets; Image classification; Image reconstruction; Image resolution; Image segmentation; Land use; Radiometers; Remote sensing; Satellite imagery; Satellites; Time series analysis; Classification evaluation; Decision fusion; Land cover classification; LANDSAT satellite images; Moderate resolution imaging spectroradiometer; MODIS; Multiresolution segmentation; Spatial resolution; Classification (of information)",2-s2.0-85029007913
"Marcheggiani D., Sebastiani F.","On the effects of low-quality training data on information extraction from clinical reports",2017,"Journal of Data and Information Quality",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029645626&doi=10.1145%2f3106235&partnerID=40&md5=37d15b68cbb4a121c645b4a2ec518fcb","In the last five years there has been a flurry of work on information extraction from clinical documents, that is, on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Many of these research works are about methods based on supervised learning, that is, methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, nowork has been devoted to investigating the effect of the quality of training data on the learning process for the clinical domain. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this article, we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data and by whose judgment we must abide) with the accuracy deriving from training data annotated by a different coder, equally expert in the subject matter. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. While the dataset used in the present work originated in a clinical context, the issues we study in this work are of more general interest. © 2017 ACM.","Annotation quality; Clinical narratives; Information extraction; Machine learning; Medical reports; Radiology reports","Artificial intelligence; Information analysis; Information retrieval; Information retrieval systems; Learning systems; Accuracy of information; Clinical narratives; Clinical practices; Information extraction systems; Learning methods; Medical reports; Radiology reports; Unstructured texts; Data mining",2-s2.0-85029645626
"Tootooni M.S., Dsouza A., Donovan R., Rao P.K., Kong Z.J., Borgesen P.","Classifying the Dimensional Variation in Additive Manufactured Parts from Laser-Scanned Three-Dimensional Point Cloud Data Using Machine Learning Approaches",2017,"Journal of Manufacturing Science and Engineering, Transactions of the ASME",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021228344&doi=10.1115%2f1.4036641&partnerID=40&md5=e1a0d9118f40191b1b81feafe442dc49","The objective of this work is to develop and apply a spectral graph theoretic approach for differentiating between (classifying) additive manufactured (AM) parts contingent on the severity of their dimensional variation from laser-scanned coordinate measurements (3D point cloud). The novelty of the approach is in invoking spectral graph Laplacian eigenvalues as an extracted feature from the laser-scanned 3D point cloud data in conjunction with various machine learning techniques. The outcome is a new method that classifies the dimensional variation of an AM part by sampling less than 5% of the 2 million 3D point cloud data acquired (per part). This is a practically important result, because it reduces the measurement burden for postprocess quality assurance in AM - parts can be laser-scanned and their dimensional variation quickly assessed on the shop floor. To realize the research objective, the procedure is as follows. Test parts are made using the fused filament fabrication (FFF) polymer AM process. The FFF process conditions are varied per a phased design of experiments plan to produce parts with distinctive dimensional variations. Subsequently, each test part is laser scanned and 3D point cloud data are acquired. To classify the dimensional variation among parts, Laplacian eigenvalues are extracted from the 3D point cloud data and used as features within different machine learning approaches. Six machine learning approaches are juxtaposed: sparse representation, k-nearest neighbors, neural network, naïve Bayes, support vector machine, and decision tree. Of these, the sparse representation technique provides the highest classification accuracy (F-score &gt; 97%). © Copyright 2017 by ASME.","additive manufacturing (AM); dimensional variation; fused filament fabrication (FFF); Laplacian eigenvalues; machine learning; sparse representation; spectral graph theory","3D printers; Artificial intelligence; Data mining; Decision trees; Design of experiments; Education; Eigenvalues and eigenfunctions; Graph theory; Laplace transforms; Learning systems; Nearest neighbor search; Quality assurance; Dimensional variations; fused filament fabrication (FFF); Laplacian eigenvalues; Sparse representation; Spectral graph theory; Classification (of information); Assembly; Filaments; Lasers; Measurement",2-s2.0-85021228344
"Boley M., Goldsmith B.R., Ghiringhelli L.M., Vreeken J.","Identifying consistent statements about numerical data with dispersion-corrected subgroup discovery",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025080144&doi=10.1007%2fs10618-017-0520-3&partnerID=40&md5=8b2ddb345a29f0750d33a516620777f6","Existing algorithms for subgroup discovery with numerical targets do not optimize the error or target variable dispersion of the groups they find. This often leads to unreliable or inconsistent statements about the data, rendering practical applications, especially in scientific domains, futile. Therefore, we here extend the optimistic estimator framework for optimal subgroup discovery to a new class of objective functions: we show how tight estimators can be computed efficiently for all functions that are determined by subgroup size (non-decreasing dependence), the subgroup median value, and a dispersion measure around the median (non-increasing dependence). In the important special case when dispersion is measured using the mean absolute deviation from the median, this novel approach yields a linear time algorithm. Empirical evaluation on a wide range of datasets shows that, when used within branch-and-bound search, this approach is highly efficient and indeed discovers subgroups with much smaller errors. © 2017, The Author(s).","Branch-and-bound search; Local pattern discovery; Subgroup discovery","Computer applications; Data mining; Branch and bound search; Empirical evaluations; Linear-time algorithms; Local patterns; Mean absolute deviations; Objective functions; Subgroup discovery; Variable dispersion; Dispersions",2-s2.0-85025080144
"Beloki Z., Artola X., Soroa A.","A scalable architecture for data-intensive natural language processing",2017,"Natural Language Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019017483&doi=10.1017%2fS1351324917000092&partnerID=40&md5=9b987a5671569cea397ff6b686d41095","Computational power needs have greatly increased during the last years, and this is also the case in the Natural Language Processing (NLP) area, where thousands of documents must be processed, i.e., linguistically analyzed, in a reasonable time frame. These computing needs have implied a radical change in the computing architectures and big-scale text processing techniques used in NLP. In this paper, we present a scalable architecture for distributed language processing. The architecture uses Storm to combine diverse NLP modules into a processing chain, which carries out the linguistic analysis of documents. Scalability requires designing solutions that are able to run distributed programs in parallel and across large machine clusters. Using the architecture presented here, it is possible to integrate a set of third-party NLP modules into a unique processing chain which can be deployed onto a distributed environment, i.e., a cluster of machines, so allowing the language-processing modules run in parallel. No restrictions are placed a priori on the NLP modules apart of being able to consume and produce linguistic annotations following a given format. We show the feasibility of our approach by integrating two linguistic processing chains for English and Spanish. Moreover, we provide several scripts that allow building from scratch a whole distributed architecture that can be then easily installed and deployed onto a cluster of machines. The scripts and the NLP modules used in the paper are publicly available and distributed under free licenses. In the paper, we also describe a series of experiments carried out in the context of the NewsReader project with the goal of testing how the system behaves in different scenarios. © Copyright Cambridge University Press 2017.",,"Architecture; Chains; Cluster computing; Data mining; Linguistics; Natural language processing systems; Text processing; Computing architecture; Distributed architecture; Distributed environments; Distributed languages; Linguistic annotations; Linguistic processing; NAtural language processing; Scalable architectures; Computer architecture",2-s2.0-85019017483
"Shim S.-H., Kim H., Sohn I.-S., Hwang H.-S., Kwon H.-S., Lee S.J., Lee J.Y., Kim S.-N., Lee K., Chang S.","Nationwide cervical cancer screening in Korea: Data from the national health insurance service cancer screening program and national cancer screening program, 2009–2014",2017,"Journal of Gynecologic Oncology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026730145&doi=10.3802%2fjgo.2017.28.e63&partnerID=40&md5=5f95f14c7ea1c3fe9b3244bf04e6db50","Objective: The rates of participation in the Korean nationwide cervical cancer screening program and the rates of abnormal test results were determined. Methods: The database of the National Health Insurance Service (NHIS) was used during the study period (2009–2014). Results: The participation rate increased from 41.10% in 2009 to 51.52% in 2014 (annual percentage change, 4.126%; 95% confidence interval [CI]=2.253–6.034). During the study period, women ≥70 years of age had the lowest rate of participation (range, 21.7%–31.9%) and those 30–39 years of age the second-lowest (27.7%–44.9%). The participation rates of National Health Insurance beneficiaries (range, 48.6%–52.5%) were higher than those of Medical Aid Program (MAP) recipients (29.6%–33.2%). The rates of abnormal results were 0.65% in 2009 and 0.52% in 2014, with a decreasing tendency in all age groups except the youngest (30–39 years). Every year the abnormal result rates tended to decrease with age, from the age groups of 30–39 years to 60–69 years but increased in women ≥70 years of age. The ratio of patients with atypical squamous cells of undetermined significance compared with those with squamous intraepithelial lesions increased from 2.71 in 2009 to 4.91 in 2014. Conclusion: Differences related to age and occurring over time were found in the rates of participation and abnormal results. Further efforts are needed to encourage participation in cervical cancer screening, especially for MAP recipients, elderly women and women 30–39 years of age. Quality control measures for cervical cancer screening programs should be enforced consistently. © 2017. Asian Society of Gynecologic Oncology, Korean Society of Gynecologic Oncology.","Early Detection of Cancer; Mass Screening; National Health Programs; Papanicolaou Test; Uterine Cervical Neoplasms","abnormal laboratory result; adult; age; aged; Article; cancer screening; data mining; female; human; Korea; mass screening; national health insurance; Papanicolaou test; public health; recipient; social participation; squamous intraepithelial lesion of the cervix; trend study; uterine cervix cancer",2-s2.0-85026730145
"Hou C., Nie F., Tao H., Yi D.","Multi-view unsupervised feature selection with adaptive similarity and view weight",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029389131&doi=10.1109%2fTKDE.2017.2681670&partnerID=40&md5=e3707758d4a690c0057d5a0d1af1cdc0","With the advent of multi-view data, multi-view learning has become an important research direction in both machine learning and data mining. Considering the difficulty of obtaining labeled data in many real applications, we focus on the multi-view unsupervised feature selection problem. Traditional approaches all characterize the similarity by fixed and pre-defined graph Laplacian in each view separately and ignore the underlying common structures across different views. In this paper, we propose an algorithm named Multi-view Unsupervised Feature Selection with Adaptive Similarity and View Weight (ASVW) to overcome the above mentioned problems. Specifically, by leveraging the learning mechanism to characterize the common structures adaptively, we formulate the objective function by a common graph Laplacian across different views, together with the sparse ℓ2,p-norm constraint designed for feature selection. We develop an efficient algorithm to address the non-smooth minimization problem and prove that the algorithm will converge. To validate the effectiveness of ASVW, comparisons are made with some benchmark methods on real-world datasets. We also evaluate our method in the real sports action recognition task. The experimental results demonstrate the effectiveness of our proposed algorithm. © 1989-2012 IEEE.","adaptive similarity and view weight; multiple view data mining; sports action recognition; unsupervised feature selection","Data mining; Laplace transforms; Learning systems; Sports; Action recognition; Adaptive similarities; Minimization problems; Multi-view learning; Multiple views; Real-world datasets; Traditional approaches; Unsupervised feature selection; Feature extraction",2-s2.0-85029389131
"Yang B., Liu X., Li Y., Zhao X.","Stochastic Blockmodeling and Variational Bayes Learning for Signed Network Analysis",2017,"IEEE Transactions on Knowledge and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023744205&doi=10.1109%2fTKDE.2017.2700304&partnerID=40&md5=92a35a0131214b740b981c1e86d6aa85","Signed networks with positive and negative links attract considerable interest in their studying since they contain more information than unsigned networks. Community detection and sign (or attitude) prediction are still primary challenges, as the fundamental problems of signed network analysis. For this, a generative Bayesian approach is presented wherein 1) a signed stochastic blockmodel is proposed to characterize the community structure in the context of signed networks, by explicit formulating the distributions of the density and frustration of signed links from a stochastic perspective, and 2) a model learning algorithm is advanced by theoretical deriving a variational Bayes EM for the parameter estimation and variation-based approximate evidence for the model selection. The comparison of the above approach with the state-of-the-art methods on synthetic and real-world networks, shows its advantage in the community detection and sign prediction for the exploratory networks. © 1989-2012 IEEE.","model selection; Network data mining; sign prediction; social network analysis; stochastic blockmodel","Approximation algorithms; Bayesian networks; Bioinformatics; Biological systems; Data mining; Data structures; Education; Forecasting; Learning algorithms; Population dynamics; Random processes; Social networking (online); Stochastic models; Bayes method; Biological system modeling; Computational model; Model Selection; Network data mining; Prediction algorithms; Stochastic block models; Stochastic systems",2-s2.0-85023744205
"Huang K., Wu Y.","An algorithm of mining high utility itemsets with vertical structures",2017,"Dalian Ligong Daxue Xuebao/Journal of Dalian University of Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031897318&doi=10.7511%2fdllgxb201705013&partnerID=40&md5=ce469d65f8d8da7ede3ea3bf4ffb1360","Mining high utility itemsets (HUIs) is one of popular tasks in field of association analysis. Most of HUIs mining algorithms need to generate a lot of candidate itemsets (CIs) which will affect the performance of algorithm. HUI-Miner can mine all the HUIs from a transaction database without generating CIs. However, this algorithm generates a large number of utility lists (ULs) and so many ULs not only consume too much storage space but also affect the operation performance. To solve this problem, itemsets lists (ILs), new data structures are proposed to maintain information of transaction and item utility. Three pruning strategies are proposed to reduce the number of ILs and can build the ILs just scanning the transaction database only once. A new algorithm namely MHUI is proposed which mines all the HUIs directly from the ILs without generating any CIs. The experimental results show that the proposed method outperforms the state-of-the-art algorithms in terms of runtime and memory consumption on three different sparse datasets. © 2017, Editorial Office of Journal of Dalian University of Technology. All right reserved.","Association analysis; Data mining; Frequent itemsets; High utility itemsets",,2-s2.0-85031897318
"Lin J.C.-W., Ren S., Fournier-Viger P., Hong T.-P., Su J.-H., Vo B.","A fast algorithm for mining high average-utility itemsets",2017,"Applied Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014957800&doi=10.1007%2fs10489-017-0896-1&partnerID=40&md5=85efd3e000f73388fbcd730c78192546","Mining high-utility itemsets (HUIs) in transactional databases has become a very popular research topic in recent years. A popular variation of the problem of HUI mining is to discover high average-utility itemsets (HAUIs), where an alternative measure called the average-utility is used to evaluate the utility of itemsets by considering their lengths. Albeit, HAUI mining has been studied extensively, current algorithms often consume a large amount of memory and have long execution times, due to the large search space and the usage of loose upper bounds to estimate the average-utilities of itemsets. In this paper, we present a more efficient algorithm for HAUI mining, which includes three pruning strategies to provide a tighter upper bound on the average-utilities of itemsets, and thus reduce the search space more effectively to decrease the runtime. The first pruning strategy utilizes relationships between item pairs to reduce the search space for itemsets containing three or more items. The second pruning strategy provides a tighter upper bound on the average-utilities of itemsets to prune unpromising candidates early. The third strategy reduces the time for constructing the average-utility-list structures for itemsets, which is used to calculate their upper bounds. Substantial experiments conducted on both real-life and synthetic datasets show that the proposed algorithm with three pruning strategies can efficiently and effectively reduce the search space for mining HAUIs, when compared to the state-of-the-art algorithms, in terms of runtime, number of candidates, memory usage, performance of the pruning strategies and scalability. © 2017, Springer Science+Business Media New York.","Data mining; High average-utility itemsets; Pruning strategies; Tighter upper bound","Artificial intelligence; Average utilities; High utility itemsets; Item sets; Pruning strategy; State-of-the-art algorithms; Synthetic datasets; Transactional database; Upper Bound; Data mining",2-s2.0-85014957800
"Caiuta R., Pozo A., Vergilio S.R.","Meta-learning based selection of software reliability models",2017,"Automated Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961782172&doi=10.1007%2fs10515-016-0195-9&partnerID=40&md5=54eed2f4fbe35fc6991a54efc036da89","The correct estimation of the software reliability level is fundamental to reduce efforts in the testing, maintenance and release activities. To help in this task, we find in the literature an increasing number of software reliability models (SRMs). However, none has proved to perform well considering different contexts. Due to this, the selection of the best model for a particular case is an important task. Most existing works on SRM selection need to test different models and decide based on how well the model fits the data and predicts the future events. Moreover, in general, they do not consider search-based models. Considering this fact, this paper introduces a Meta-learning approach for SRM selection. In such approach, some meta-features are used to indicate the best performing model. The approach is independent of the type of models to be selected, and can be used with different data mining algorithms. It includes the following activities: meta-knowledge extraction, meta-learning and classification. The activities meta-knowledge extraction and meta-learning are performed just once and generate a meta-classifier. Therefore, the meta-classifier is used to select the most adequate model for new projects (classification activity). The approach is evaluated in a set of experiments and the results do not show statistical difference between the Meta-learning approach and the choice of the best performing model. Otherwise, the results point out statistical difference between the Meta-learning approach and the choice of the worst performing model with a large stochastic difference according to the Vargha and Delaney Effect Size. © 2016, Springer Science+Business Media New York.","Data mining; Meta-learning; Software reliability models","Algorithms; Data mining; Extraction; Reliability; Software testing; Stochastic models; Stochastic systems; Adequate models; Data mining algorithm; Meta-classifiers; Meta-learning approach; Metalearning; Selection of software; Software reliability models; Statistical differences; Software reliability",2-s2.0-84961782172
"Akin M., Eyduran E., Niedz R.P., Reed B.M.","Developing hazelnut tissue culture medium free of ion confounding",2017,"Plant Cell, Tissue and Organ Culture",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020107830&doi=10.1007%2fs11240-017-1238-z&partnerID=40&md5=a35fd9c4e6344cf3eca80acb41cc46a7","The general approach for tissue culture medium optimization is to use salts as factors in experimental design and analysis. However, using salts as factors leads to ion confounding, making it difficult to detect the effects of individual ions on particular growth responses. This study focused on testing ions as factors for the medium optimization process. NH4 +, Ca2+, Mg2+, SO4 2− and PO4 3− ions were used as factors in a D-optimal design. K+ and NO3 − ions were used to bring the pH to neutral, and were also factors in the statistical analysis. The Chi-squared automatic interaction detection (CHAID) data mining algorithm was used to analyze shoot growth responses of ‘Barcelona’, ‘Jefferson’ and ‘Wepster’ hazelnuts. The CHAID analysis decision trees revealed significant variables and their interactions, and provided exact cut-off amounts for optimizing each of the ions. K+, NO3 −, and NH4 + had significant effects on shoot quality. NH4 + was of primary significance for shoot length followed by Mg2+, NO3 − and Ca2+. Multiplication was mainly affected by Ca2+ and genotype. For the least callus formation, NH4 + &gt;33.3 mM was required, but this higher concentration range did not provide good shoot quality or elongation. The critical cut-off values for good shoot quality, elongation, multiplication and medium callus formation for hazelnut are suggested to be: NO3 − ≤88 mM, NH4 + ≤20 mM, Ca2+ ≤5 mM, Mg2+ &gt;5 mM and K+ ≤46 mM. © 2017, Springer Science+Business Media Dordrecht.","CHAID algorithm; Corylus avellana; Hazelnut; Ions; Micropropagation","Calcium; Data mining; Decision trees; Factor analysis; Optimization; Salts; Tissue; Tissue culture; Tissue engineering; Automatic interaction detection; Corylus avellana; Culture medium optimization; Data mining algorithm; Experimental design and analysis; Hazelnut; Micropropagation; Significant variables; Ions",2-s2.0-85020107830
"Piscitelli A., Tarallo V., Guarino L., Sannia G., Birolo L., Pezzella C.","New lipases by mining of Pleurotus ostreatus genome",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029793855&doi=10.1371%2fjournal.pone.0185377&partnerID=40&md5=7f134c03a9da671231245a9587d6b659","The analysis of Pleurotus ostreatus genome reveals the presence of automatically annotated 53 lipase and 34 carboxylesterase putative coding-genes. Since no biochemical or physiological data are available so far, a functional approach was applied to identify lipases from P. ostreatus. In the tested growth conditions, four lipases were found expressed, with different patterns depending on the used C source. Two of the four identified proteins (Pleo-Lip241 and PleoLip369), expressed in both analysed conditions, were chosen for further studies, such as an in silico analysis and their molecular characterization. To overcome limits linked to native production, a recombinant expression approach in the yeast Pichia pastoris was applied. Different expression levels were obtained: PleoLip241 reached a maximum activity of 4000 U/L, whereas PleoLip369 reached a maximum activity of 700 U/L. Despite their sequence similarity, these enzymes exhibited different substrate specificity and diverse stability at pH, temperature, and presence of metals, detergents and organic solvents. The obtained data allowed classifying PleoLip241 as belonging to the “true lipase” family. Indeed, by phylogenetic analysis the two proteins fall in different clusters. PleoLip241 was used to remove the hydrophobic layer from wool surface in order to improve its dyeability. The encouraging results obtained with lipase treated wool led to forecast PleoLip241 applicability in this field. © 2017 Piscitelli et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"carboxylesterase; detergent; metal; organic solvent; triacylglycerol lipase; carboxylesterase; coloring agent; fungal protein; recombinant protein; triacylglycerol lipase; Article; computer model; controlled study; enzyme activity; enzyme specificity; enzyme stability; fungal genome; fungus culture; fungus growth; genetic code; hydrophobicity; Komagataella pastoris; nonhuman; pH; phylogeny; Pleurotus ostreatus; polyacrylamide gel electrophoresis; protein expression; protein purification; temperature; animal; chemistry; data mining; enzyme active site; enzymology; genetics; kinetics; metabolism; microbiology; molecular model; Pleurotus; wool; Animals; Carboxylesterase; Catalytic Domain; Coloring Agents; Data Mining; Fungal Proteins; Genome, Fungal; Industrial Microbiology; Kinetics; Lipase; Models, Molecular; Phylogeny; Pleurotus; Recombinant Proteins; Substrate Specificity; Wool",2-s2.0-85029793855
"Sun C.-Y., Lee A.J.T.","Tour recommendations by mining photo sharing social media",2017,"Decision Support Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020129581&doi=10.1016%2fj.dss.2017.05.013&partnerID=40&md5=878c0585753f06ae4a083b8ad508067e","With the increasing popularity of photo and video sharing social networks, more and more people have shared their photos or videos with their family members and friends. Therefore, in this paper, we propose a framework for recommending top-k tours to meet user's interest and time frame by using user-generated contents in a photo sharing social network. The proposed framework contains four phases. First, we cluster geotagged locations into landmarks, and further cluster these landmarks into areas by the mean-shift clustering method. Second, we employ the Latent Dirichlet Allocation model to categorize the hashtags posted by users into landmark topics, and then use these topics to characterize landmarks and users. Third, to recommend tours for a user, we compute the tendency (or score) of the user visiting each landmark by the landmark popularity, the attraction of landmark to the user, and how many users similar to the user visit the landmark. Finally, based on the scores computed, we develop a method to recommend top-k tours with highest scores for the user. Unlike most previous methods recommending tours landmark by landmark, our framework recommends tours area by area so that users can avoid going back and forth from one area to another and save plenty of time on transportation, which in turn can visit more landmarks. The experiment results show that our proposed method outperforms the Markov-Topic method in terms of average score and precision. Our proposed framework may help users plan their trips and customize a trip for each user. © 2017 Elsevier B.V.","Data mining; Latent Dirichlet Allocation model; Mean-shift clustering method; Photo sharing social network; Tour recommendation","Cluster analysis; Data mining; Network function virtualization; Statistics; Latent Dirichlet allocation; Mean-Shift Clustering; Photo sharing; Social media; Tour recommendation; User's interest; User-generated content; Video sharing; Social networking (online)",2-s2.0-85020129581
"Moro S., Rita P., Cortez P.","A text mining approach to analyzing Annals literature",2017,"Annals of Tourism Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023619363&doi=10.1016%2fj.annals.2017.07.011&partnerID=40&md5=66d58205f8ba8e142cb3d2c09cebcc94",[No abstract available],,"data mining; literature review",2-s2.0-85023619363
"Wadood A., Ghufran M., Khan A., Azam S.S., Uddin R., Waqas M., Saleem S.","The methicillin-resistant S. epidermidis strain RP62A genome mining for potential novel drug targets identification",2017,"Gene Reports",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020884151&doi=10.1016%2fj.genrep.2017.06.002&partnerID=40&md5=ac7501d43fc3669047f7569c69a2468d","In unique biofilm-forming methicillin-resistant (MRSE) strains of S. epidermidis have become a serious medical problem due to the emergence of antibiotic resistance strains. There is an essential need to develop novel drug targets to address the new intervention challenge for such multidrug-resistant bacteria. Here we utilized the available genomes and proteome datasets resources of MRSE S. epidermidis RP62A for comparative and subtractive genome analyses to identify novel drug targets for future drug discovery approaches. The comparative genome based molecular biology database resources scanning identified 255 proteins as essential human non-homologs targets for epidermidis RP62A that involve in essential metabolic pathways. Among these 12 targets were found to be involved in pathogen unique metabolic pathways. Druggability potential of each of these targets by Drug Bank database further identified 5 proteins as druggable essential proteins. As antimicrobial agents inflict beneficial gut microbiome and the antibiotics causes less agitation in gut flora are considered as best candidates. We pursued additional analysis and screen our lead targets proteins against metagenomes databases holding whole genome sequences of human gut microbiome. The negligible biological similarity with gut microbiota genomes datasets predicting the future implementation of our lead identified proteins as druggable targets for S. epidermidis RP62A that may not affect the essential humans gut microbiota. Based on these evidences, we are speculating that the putative druggable proteins addressed in this study are potent enough for their future evaluation as therapeutic targets to combat the S. epidermidis RP62A infections. © 2017","Druggability; Non-homologous essential proteins; S. epidermidis strain RP62A; Subtractive genome analysis","proteome; amino acid sequence; Article; bacterial genome; bacterial microbiome; bacterial strain; bioinformatics; cellular distribution; data mining; drug identification; drug targeting; essential gene; gene identification; genome analysis; human; metagenome; methicillin-resistant Staphylococcus epidermidis; molecular biology; nonhuman; priority journal; protein analysis",2-s2.0-85020884151
"Gu X., Angelov P.P., Kangin D., Principe J.C.","A new type of distance metric and its use for clustering",2017,"Evolving Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028032284&doi=10.1007%2fs12530-017-9195-7&partnerID=40&md5=bd31acd8bbed938180ac3f5dea12af45","In order to address high dimensional problems, a new ‘direction-aware’ metric is introduced in this paper. This new distance is a combination of two components: (1) the traditional Euclidean distance and (2) an angular/directional divergence, derived from the cosine similarity. The newly introduced metric combines the advantages of the Euclidean metric and cosine similarity, and is defined over the Euclidean space domain. Thus, it is able to take the advantage from both spaces, while preserving the Euclidean space domain. The direction-aware distance has wide range of applicability and can be used as an alternative distance measure for various traditional clustering approaches to enhance their ability of handling high dimensional problems. A new evolving clustering algorithm using the proposed distance is also proposed in this paper. Numerical examples with benchmark datasets reveal that the direction-aware distance can effectively improve the clustering quality of the k-means algorithm for high dimensional problems and demonstrate the proposed evolving clustering algorithm to be an effective tool for high dimensional data streams processing. © 2017, Springer-Verlag GmbH Germany.","Clustering; Cosine similarity; Distance metric; High dimensional data streams processing; Metric space","Benchmarking; Data communication systems; Data handling; Data mining; Geometry; Clustering; Cosine similarity; Distance metrics; High-dimensional data streams; Metric spaces; Clustering algorithms",2-s2.0-85028032284
"Phu V.N., Tran V.T.N., Chau V.T.N., Dat N.D., Duy K.L.D.","A decision tree using ID3 algorithm for English semantic analysis",2017,"International Journal of Speech Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020554813&doi=10.1007%2fs10772-017-9429-x&partnerID=40&md5=8a8086eb4b80446bad76b8c81d5145fc","Natural language processing has been studied for many years, and it has been applied to many researches and commercial applications. A new model is proposed in this paper, and is used in the English document-level emotional classification. In this survey, we proposed a new model by using an ID3 algorithm of a decision tree to classify semantics (positive, negative, and neutral) for the English documents. The semantic classification of our model is based on many rules which are generated by applying the ID3 algorithm to 115,000 English sentences of our English training data set. We test our new model on the English testing data set including 25,000 English documents, and achieve 63.6% accuracy of sentiment classification results. © 2017, Springer Science+Business Media, LLC.","Decision tree; English document opinion mining; English sentiment classification; ID3 algorithm, id3; Sentiment classification","Classification (of information); Decision trees; Information retrieval systems; Natural language processing systems; Semantics; Statistical tests; Trees (mathematics); Commercial applications; Emotional classification; ID3 algorithm; Opinion mining; Semantic analysis; Semantic classification; Sentiment classification; Training data sets; Data mining",2-s2.0-85020554813
"Wicker J., Kramer S.","The best privacy defense is a good privacy offense: obfuscating a search engine user’s profile",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025085865&doi=10.1007%2fs10618-017-0524-z&partnerID=40&md5=247afda6050dd89a6a7d18d84d0dc5ee","User privacy on the internet is an important and unsolved problem. So far, no sufficient and comprehensive solution has been proposed that helps a user to protect his or her privacy while using the internet. Data are collected and assembled by numerous service providers. Solutions so far focused on the side of the service providers to store encrypted or transformed data that can be still used for analysis. This has a major flaw, as it relies on the service providers to do this. The user has no chance of actively protecting his or her privacy. In this work, we suggest a new approach, empowering the user to take advantage of the same tool the other side has, namely data mining to produce data which obfuscates the user’s profile. We apply this approach to search engine queries and use feedback of the search engines in terms of personalized advertisements in an algorithm similar to reinforcement learning to generate new queries potentially confusing the search engine. We evaluated the approach using a real-world data set. While evaluation is hard, we achieve results that indicate that it is possible to influence the user’s profile that the search engine generates. This shows that it is feasible to defend a user’s privacy from a new and more practical perspective. © 2017, The Author(s).","Personalized ads; Privacy; Reinforcement learning; Search engines; Web mining","Data mining; Data privacy; Education; Internet service providers; Reinforcement learning; New approaches; Personalized ads; Personalized advertisements; Real-world; Service provider; Unsolved problems; User privacy; Web Mining; Search engines",2-s2.0-85025085865
"Owen J.R., Kemp D.","Social management capability, human migration and the global mining industry",2017,"Resources Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024124495&doi=10.1016%2fj.resourpol.2017.06.017&partnerID=40&md5=a3b4f8d559a2e13bd03e7ac10854fad3","This article examines the social management capability (SMC) of the global mining industry to identify, understand and manage complex social and environmental issues, such as human migration. Our contribution is based on the analysis and interpretation of two sets of qualitative data: (i) existing literature on SMCs and its relevance to demographic pressures in mining, and (ii) a series of industry-commissioned “deep dives” exploring high profile legacy cases. These sources provide a coherent picture of how the mining industry has positioned itself in responding to contentious social and environmental challenges. Our findings suggest that, considering the dynamic nature of human migration issues like in-migration and resettlement, the industry does not have sufficiently robust SMCs. The absence of these SMCs has resulted in ad-hoc strategies for managing high risk, high cost issues. In concluding, we argue that a major step change is needed within the industry in terms of developing and then rapidly advancing its SMCs. © 2017","Corporate social responsibility; Human migration; Mining; Social management capabilities; Social risk","Economics; Mining; Corporate social responsibilities (CSR); Dynamic nature; Global mining industry; Human migration; Qualitative data; Social and environmental; Social management; Social risks; Management; corporate social responsibility; environmental issue; human settlement; migration; mining industry",2-s2.0-85024124495
"Chiang T.-C., Cheng P.-Y., Leu F.-Y.","Prediction of technical efficiency and financial crisis of Taiwan’s information and communication technology industry with decision tree and DEA",2017,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961671387&doi=10.1007%2fs00500-016-2117-y&partnerID=40&md5=cf28afbf76e22a6cf2fb6572009279f1","This study aims to analyze the business performance and technical efficiency of Taiwan’s ICT industry with the Malmquist productivity index of data envelopment analysis. The regression method is used to verify the influence of ICT industry labor input and research input on yield. The tested units include Taiwan companies among the top 250 companies of the ICT industry in the world, as well as companies with great contributions to the ICT industry in Taiwan, for a total of 16 objects. Using the data mining classification method, the usage variables in the financial crisis model are utilized to predict whether the technology of the tested units is efficient. The research results are as follows. First, during the test period, 3, 3 and 6 companies have, respectively, increasing returns, constant returns, and decreasing returns to scale. This suggests that in Taiwan’s ICT industry, only 3 companies can continuously grow to seek the maximum benefit, while 6 are in a period with stable efficiency, and 6 are in a period with declined operating efficiency and increasing costs. Second, labor input has a significant positive correlation with yield, and the influence of labor input on yield is relative to other inputs. The influence of R&D on yield has a positive but insignificant correlation, which is different from previous research. This is because the effect of research input is not relatively obvious, thus requiring to increase the items to be input. Third, by combining two different variables, this study uses the financial crisis precaution model and data envelopment model to predict technical inefficiency before and after the financial tsunami of 2008. No matter whether before or after the financial tsunami of 2008, the financial crisis precaution model is more accurate than the data envelopment model in terms of prediction of technical inefficiency. © 2016, Springer-Verlag Berlin Heidelberg.","Data envelopment analysis; Data mining; Decision tree; Efficiency; Information and communication technology (ICT) industry; Malmquist index","Data envelopment analysis; Decision trees; Efficiency; Finance; Forecasting; Regression analysis; Trees (mathematics); Tsunamis; Business performance; Information and Communication Technologies; Information and communication technology industry; Malmquist indices; Malmquist productivity index; Mining classification; Positive correlations; Technical efficiency; Data mining",2-s2.0-84961671387
"Suleiman I., Arslan M., Alhajj R., Ridley M.","Prediction Model of School Readiness",2017,"Journal of Information and Knowledge Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025809314&doi=10.1142%2fS021964921750023X&partnerID=40&md5=acff57f81e2cdd6207685ecdbb151076","Studying the school readiness is an interesting domain that has attracted the attention of the public and private sectors in education. Researchers have developed some techniques for assessing the readiness of preschool kids to start school. Here we benefit from an integrated approach which combines Data Mining (DM) and social network analysis towards a robust solution. The main objective of this study is to explore the socio-demographic variables (age, gender, parents' education, parents' work status, and class and neighbourhood peers influence), achievement data (Arithmetic Readiness, Cognitive Development, Language Development, Phonological Awareness), and data that may impact school readiness. To achieve this, we propose to apply DM techniques to predict school readiness. Real data on 306 preschool children was used from four different elementary schools: (1) Life school for Creativity and Excellence a private school located in Ramah village, (2) Sisters of Saint Joseph missionary school located in Nazareth, (3) Franciscan missionary school located in Nazareth and (4) Al-Razi public school located in Nazareth, and white-box classification methods, such as induction rules were employed. Experiments attempt to improve their accuracy for predicting which children might fail or dropout by first, using all the available attributes; next, selecting the best attributes; and finally, rebalancing data and using cost sensitive classification. The outcomes have been compared and the models with the best results are shown. © 2017 World Scientific Publishing Co.","data analysis; data collection; data mining; prediction; School readiness; social network analysis",,2-s2.0-85025809314
"Bao L., Lo D., Xia X., Li S.","Automated Android application permission recommendation",2017,"Science China Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027009703&doi=10.1007%2fs11432-016-9072-3&partnerID=40&md5=f05e6447c4876003bacc141ba5953b9c","The number of Android applications has increased rapidly as Android is becoming the dominant platform in the smartphone market. Security and privacy are key factors for an Android application to be successful. Android provides a permission mechanism to ensure security and privacy. This permission mechanism requires that developers declare the sensitive resources required by their applications. On installation or during runtime, users are required to agree with the permission request. However, in practice, there are numerous popular permission misuses, despite Android introducing official documents stating how to use these permissions properly. Some data mining techniques (e.g., association rule mining) have been proposed to help better recommend permissions required by an API. In this paper, based on popular techniques used to build recommendation systems, we propose two novel approaches to improve the effectiveness of the prior work. The first approach utilizes a collaborative filtering technique, which is inspired by the intuition that apps that have similar features — inferred from their APIs — usually share similar permissions. The second approach recommends permissions based on a text mining technique that uses a naive Bayes multinomial classification algorithm to build a prediction model by analyzing descriptions of apps. To evaluate these two approaches, we use 936 Android apps from F-Droid, which is a repository of free and open source Android applications. We find that our proposed approaches yield a significant improvement in terms of precision, recall, F1-score, and MAP of the top-k results over the baseline approach. © 2017, Science China Press and Springer-Verlag GmbH Germany.","Android; association rule; collaborative filtering; permission recommendation; text mining","Android (operating system); Association rules; Collaborative filtering; Data mining; Text processing; Android; Android applications; Classification algorithm; Collaborative filtering techniques; Naive Bayes Multinomial; permission recommendation; Text mining; Text mining techniques; Mobile security",2-s2.0-85027009703
"Velampalli S., Jonnalagedda M.V.","Graph based knowledge discovery using MapReduce and SUBDUE algorithm",2017,"Data and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028331835&doi=10.1016%2fj.datak.2017.08.001&partnerID=40&md5=4a8aa16a39a867b261ac67a6f1bbaafa","Knowledge Discovery is the process of extracting useful and hidden information. Extracting knowledge from data represented in the form of graphs is emerging in this new generation. Graphs are used to model and solve many real world problems. In this work, we aim to show how skills data from resumes is modelled into a variant of graph data structure called conceptual graph using MapReduce programming model. Resumes are taken as data source because they are the ones containing skill-sets of candidates. Initial storage and pre-processing is done in a big data framework using Hadoop Distributed File System (HDFS) and MapReduce. SUB Structure Discovery Using Examples (SUBDUE), a popular graph mining algorithm is used for retrieving common skill-sets. The results obtained from real-world dataset of resumes clearly demonstrate the potential of graph mining algorithms in skill set analytics. Proposed approach is able to extract common skill-sets. Common skill-set extraction is useful for course curriculum designers as well as job seekers. © 2017 Elsevier B.V.","Analytics; Big data; Conceptual graph; Graph mining; HDFS; MapReduce; Resumes; Skill; SUBDUE","Big data; Digital storage; File organization; Graphic methods; Analytics; Conceptual graph; Graph mining; HDFS; Map-reduce; Resumes; Skill; SUBDUE; Data mining",2-s2.0-85028331835
"Huang H.-H., Wang J.-J., Chen H.-H.","Implicit opinion analysis: Extraction and polarity labelling",2017,"Journal of the Association for Information Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027544994&doi=10.1002%2fasi.23835&partnerID=40&md5=4cd01b0bb930e722840b47e945dc7250","Opinion words are crucial information for sentiment analysis. In some text, however, opinion words are absent or highly ambiguous. The resulting implicit opinions are more difficult to extract and label than explicit ones. In this paper, cutting-edge machine-learning approaches – deep neural network and word-embedding – are adopted for implicit opinion mining at the snippet and clause levels. Hotel reviews written in Chinese are collected and annotated as the experimental data set. Results show the convolutional neural network models not only outperform traditional support vector machine models, but also capture hidden knowledge within the raw text. The strength of word-embedding is also analyzed. © 2017 ASIS&T",,"Data mining; Learning systems; Neural networks; Convolutional neural network; Cutting edges; Hidden knowledge; Machine learning approaches; Opinion analysis; Opinion mining; Sentiment analysis; Support vector machine models; Deep neural networks; embedding; experimental model; extract; mining; nervous system; support vector machine",2-s2.0-85027544994
"Yuan Y., Huo L., Hogrefe D.","Two Layers Multi-class Detection method for network Intrusion Detection System",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030541787&doi=10.1109%2fISCC.2017.8024620&partnerID=40&md5=de693f68c7397b888dcd1e4f9a5a5052","Intrusion Detection Systems (IDSs) are powerful systems which monitor and analyze events in order to detect signs of security problems and take action to stop intrusions. In this paper, the Two Layers Multi-class Detection (TLMD) method used together with the C5.0 method and the Naive Bayes algorithm is proposed for adaptive network intrusion detection, which improves the detection rate as well as the false alarm rate. The proposed TLMD algorithm also addresses some difficulties in data mining situations such as handling imbalance datasets, dealing with continuous attributes, and reducing noise in training dataset. We compared the performance of the proposed TLMD method with that of existing algorithms, using the detection rate, accuracy as well as false alarm rate on the KDDcup99 benchmark intrusion detection dataset. The experimental results prove that the proposed TLMD method has a reduced false alarm rate and a good detection rate based on the imbalanced dataset. © 2017 IEEE.","C5.0 Method; Detection Rate; False Alarm Rate; Imbalance Data; Intrusion Detection Systems; Naive Bayes Algorithm","Alarm systems; Benchmarking; Classifiers; Computer crime; Data mining; Errors; Image resolution; Mercury (metal); C5.0 Method; Detection rates; False alarm rate; Imbalance datum; Intrusion Detection Systems; Naive-Bayes algorithm; Intrusion detection",2-s2.0-85030541787
"Nimmy S.F., Kamal M.S., Hossain M.I., Dey N., Ashour A.S., Shi F.","Neural Skyline Filtering for Imbalance Features Classification",2017,"International Journal of Computational Intelligence and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029085572&doi=10.1142%2fS1469026817500195&partnerID=40&md5=bfdcdd4f6d16abd8b2a33b31e58e459b","In the current digitalized era, large datasets play a vital role in features extractions, information processing, knowledge mining and management. Sometimes, existing mining approaches are not sufficient to handle large volume of datasets. Biological data processing also suffers for the same issue. In the present work, a classification process is carried out on large volume of exons and introns from a set of raw data. The proposed work is designed into two parts as pre-processing and mapping-based classification. For pre-processing, three filtering techniques have been used. However, these traditional filtering techniques face difficulties for large datasets due to the long required time during large data processing as well as the large required memory size. In this regard, a mapping-based neural skyline filtering approach is designed. Randomized algorithm performed the mapping for large volume of datasets based on objective function. The objective function determines the randomized size of the datasets according to the homogeneity. Around 200 million DNA base pairs have been used for experimental analysis. Experimental result shows that mapping centric filtering outperforms other filtering techniques during large data processing. © 2017 World Scientific Publishing Europe Ltd.","Bloom filter; mapping centric neural approach; naive approach; neural approach; Neural skyline filtering","Data handling; Filtration; Mapping; Biological data processing; Bloom filters; Classification process; Experimental analysis; naive approach; neural approach; Objective functions; Randomized Algorithms; Data mining",2-s2.0-85029085572
"Shaik M.V., Sujatha P.","Temporal query processig using SQL server",2017,"International Journal on Smart Sensing and Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027992240&partnerID=40&md5=76e1ecc76da4090c3b83ffe2bcc25775","Most data sources in real-life are not static but change their information in time. This evolution of data in time can give valuable insights to business analysts. Temporal data refers to data, where changes over time or temporal aspects play a central role. Temporal data denotes the evaluation of object characteristics over time. One of the main unresolved problems that arise during the data mining process is treating data that contains temporal information. Temporal queries on time evolving data are at the heart of a broad range of business and network intelligence applications ranging from consumer behaviour analysis, trend analysis, temporal pattern mining, and sentiment analysis on social media, cyber security, and network monitoring. Social networks (SN) such as Facebook, twitter, LinkedIn contains huge amount of temporal information. Social media forms a dynamic and evolving environment. Similar to real-world friendships, social media interactions evolve over time. People join or leave groups; groups expand, shrink, dissolve, or split over time. Studying the temporal behaviour of communities is necessary for a deep understanding of communities in social media(SM). In this paper we focus on the use of temporal data and temporal data mining in social networks.","Data mining; Social networks; Temporal data; Temporal database; Temporal query processing; Time stamp using SQL Server",,2-s2.0-85027992240
"Gong L., Yang R., Liu Q., Dong Z., Chen H., Yang G.","A Dictionary-Based Approach for Identifying Biomedical Concepts",2017,"International Journal of Pattern Recognition and Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016448915&doi=10.1142%2fS021800141757004X&partnerID=40&md5=1e74e58c1347ba46918c19cd4dff247e","In this research, we provided a dictionary-based approach for identifying biomedical concepts from the literature. The approach first crawled experimental corpus by E-utilities and built a concept dictionary. Then, we developed an algorithm called Variable-step Window Identification Algorithm (VWIA) for matching biomedical concepts based on preprocessing, POS tagging and the formation of phrase block. The approach could identify embedded biomedical concepts and new concepts, which could identify concepts more completely. The proposed approach obtain 95.0% F-measure overall for the test dataset. Thus, it is promising for the method of biomedical text mining. © 2017 World Scientific Publishing Company.","Biomedical text mining; concept recognition; dictionary-based apporach; VWIA algorithm","Biomedical engineering; Character recognition; Computational linguistics; Data mining; Natural language processing systems; Statistical tests; Biomedical text minings; Concept dictionaries; Concept recognition; F measure; Identification algorithms; PoS tagging; Variable step; Bioinformatics",2-s2.0-85016448915
"Albarakati H., Amamra A., Elfouly R., Ammar R.","Reconfigurable underwater embedded systems architectures",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030564930&doi=10.1109%2fISCC.2017.8024715&partnerID=40&md5=5d0cdc327c1116216e1e58629b9b16fa","Underwater sensor networks are bounded by data sensing, transmitting, and forwarding limitations. The transmitting of large volumes of data can require a large amount of time and power. This has led researchers to focus on the new technology of underwater computing systems, in which information is extracted under the water using embedded processors via data mining and/or data compression. In this paper, a set of underwater embedded system (UWES) architectures is developed that can handle different network configurations. These developed architectures have a single processing node and are assumed to be homogenous. An architecture is selected to match a given set of requirements including data rate, processing node capabilities, gathering nodes capabilities, and water depth. Analytical models are developed for each type of architecture, which estimate both end to end delay and power consumption. Simulations that verify the results and evaluate the performance of the architectures are also provided. © 2017 IEEE.","Architecture; End-to End Delay; Power Consumption; Underwater Embedded Systems (UWES); Underwater Sensor Networks (UWSN)","Architecture; Computer architecture; Data handling; Data mining; Electric power utilization; Network architecture; Reconfigurable architectures; Sensor networks; Computing system; Embedded processors; End to end delay; Network configuration; Processing nodes; Systems architecture; Underwater Embedded Systems (UWES); Underwater sensor networks; Embedded systems",2-s2.0-85030564930
"Yan K., Ryoo H.S.","Strong valid inequalities for Boolean logical pattern generation",2017,"Journal of Global Optimization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015625861&doi=10.1007%2fs10898-017-0512-2&partnerID=40&md5=315052ab380666059d80ae0b047d34be","0–1 multilinear programming (MP) captures the essence of pattern generation in logical analysis of data (LAD). This paper utilizes graph theoretic analysis of data to discover useful neighborhood properties among data for data reduction and multi-term linearization of the common constraint of an MP pattern generation model in a small number of stronger valid inequalities. This means that, with a systematic way to more efficiently generating Boolean logical patterns, LAD can be used for more effective analysis of data in practice. Mathematical properties and the utility of the new valid inequalities are illustrated on small examples and demonstrated through extensive experiments on 12 real-life data mining datasets. © 2017, Springer Science+Business Media New York.","0–1 linearization; 0–1 multilinear programming; Boolean logic; Clique; Hypercube; Logical analysis of data; Pattern","Data mining; Graph theory; Linearization; Boolean logic; Clique; Hypercube; Logical analysis of data; Multi-linear programming; Pattern; Information analysis",2-s2.0-85015625861
"Coussement K., Debaere S., De Ruyck T.","Inferior Member Participation Identification in Innovation Communities: The Signaling Role of Linguistic Style Use",2017,"Journal of Product Innovation Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021337921&doi=10.1111%2fjpim.12401&partnerID=40&md5=b748ae0d1c87f1230a5c72f0b074ab60","Community managers often struggle to ensure the viability of innovation communities (IC) due to their big data characteristics and inferior member participation, which result in minimal activity and low-quality input. In response to a recent call in the innovation literature for new approaches to dealing with the challenges of big data, we propose an IC-management strategy that relies on extracting linguistic-style cues from community posts to identify future inferior member participation. When future destructive IC behavior is signaled, the moderator can effectively select the correct member for corrective treatment to prevent negative community impact. This article uses text mining to extract self-interest-oriented and positive emotional writing style cues from 39,387 posts written by 1611 members of 10 ICs. Two multilevel regression models deliver novel insights into the relationship between these linguistic cues and the likelihood of inferior community participation (quantity and quality). First, a community member's use of a positive emotional writing style signals less inferior participation quantity and quality in the future. Second, a moderator's use of a self-interest-oriented writing style suggests more inferior participation quality, while a self-interest-oriented community indicates less inferior participation quality. Third, community managers should work to build a positive-emotion-driven community, as such communities experience constructive member participation. This article shows that community managers who struggle with their IC must realize that in addition to what people say, how they say it gives insights into the IC's viability. We conclude our study by revealing the theoretical and managerial implications for IC management and community moderators. © 2017 Product Development & Management Association",,"Big data; Integrated circuits; Linguistics; Managers; Moderators; Regression analysis; Community participation; Data characteristics; Innovation communities; Linguistic styles; Management strategies; Managerial implications; Positive emotions; Regression model; Data mining",2-s2.0-85021337921
"Wang J., He C., Liu Y., Tian G., Peng I., Xing J., Ruan X., Xie H., Wang F.L.","Efficient alarm behavior analytics for telecom networks",2017,"Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015958615&doi=10.1016%2fj.ins.2017.03.020&partnerID=40&md5=3756a87b9dee0bffc55ffa407911c77d","Locating network fault problems and filtering trivial alarms from important ones are the two main challenges in Network Operation Centers (NOCs). In this paper, we present an alarm behavior analysis and discovery system, AABD, that establishes flapping and parent–child (P–C) rules to reveal the operation patterns from a large number of alarms in telecom networks. These rules can be exploited to filter out unimportant alarms, conduct multi-dimensional analysis of the alarms and identify potential network problems. We propose two novel and effective algorithms to establish the flapping rules and P-C rules. The proposed system is validated using alarm datasets from five Internet service providers. Specifically, we verify the system and methodology in each of the five network domains, i.e., circuit-switched network (CS), packet-switched network (PS), 2G-radio access network (RAN-2G), 3G-radio access network (RAN-3G) and 4G-radio access network (RAN-4G), as these five domains can, to a great extent, form a complete network environment. More importantly, our system can establish a small number of rules, only dozens of flapping rules and P-C rules, and compress the alarms by approximately 84%, i.e., 84% of alarms will not be sent to the network operator. To summarize, the proposed system can help network operators respond to network faults in a timely fashion, locate the faults accurately and significantly reduce the time spent on these tasks. © 2017 Elsevier Inc.","Alarm analysis and discovery; Big data; Correlation; Data mining; Frequent pattern mining; Telecom","Big data; Correlation methods; Data mining; Filtration; Radio; Switching networks; 3G radio access network; Alarm analysis; Circuit-switched networks; Effective algorithms; Frequent pattern mining; Multi-dimensional analysis; Radio access networks; Telecom; Alarm systems",2-s2.0-85015958615
"Hemmati H., Fang Z., Mäntylä M.V., Adams B.","Prioritizing manual test cases in rapid release environments",2017,"Software Testing Verification and Reliability",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978372432&doi=10.1002%2fstvr.1609&partnerID=40&md5=187eb1cef75c03fd868f6b107cd0acf7","Test case prioritization is an important testing activity, in practice, specially for large scale systems. The goal is to rank the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects the maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, for example, manual system tests versus automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly performed in a black-box manner (manual testers do not have access to the source code). Therefore, in this paper, the existing test case prioritization techniques (e.g. diversity-based and history-based techniques) are examined and modified to be applicable on manual black-box system testing. An empirical study on four older releases of desktop Firefox showed that none of the techniques were strongly dominating the others in all releases. However, when nine more recent releases of desktop Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, were studied, a very significant difference between the history-based approach and its alternatives was observed. The higher effectiveness of the history-based approach compared with alternatives also held on 28 additional rapid releases of other Firefox projects – mobile Firefox and tablet Firefox. The conclusion of the paper is that test cases in rapid release environments can be very effectively prioritized for execution, based on their historical failure knowledge. In particular, it is the recency of historical knowledge that explains its effectiveness in rapid release environments rather than other changes in the process. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.","historical data; manual testing; rapid release; test case prioritization; text mining","Black-box testing; Budget control; Codes (symbols); Data mining; Large scale systems; Historical data; Manual testing; Rapid release; Test case prioritization; Text mining; Testing",2-s2.0-84978372432
"Pouriyeh S., Vahid S., Sannino G., De Pietro G., Arabnia H., Gutierrez J.","A comprehensive investigation and comparison of Machine Learning Techniques in the domain of heart disease",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030559131&doi=10.1109%2fISCC.2017.8024530&partnerID=40&md5=252ca7a9546b24bb4c42ffe3f02337bf","This paper aims to investigate and compare the accuracy of different data mining classification schemes, employing Ensemble Machine Learning Techniques, for the prediction of heart disease. The Cleveland data set for heart diseases, containing 303 instances, has been used as the main database for the training and testing of the developed system. 10-Fold Cross-Validation has been applied in order to increase the amount of data, which would otherwise have been limited. Different classifiers, namely Decision Tree (DT), Naïve Bayes (NB), Multilayer Perceptron (MLP), K-Nearest Neighbor (K-NN), Single Conjunctive Rule Learner (SCRL), Radial Basis Function (RBF) and Support Vector Machine (SVM), have been employed. Moreover, the ensemble prediction of classifiers, bagging, boosting and stacking, has been applied to the dataset. The results of the experiments indicate that the SVM method using the boosting technique outperforms the other aforementioned methods. © 2017 IEEE.","Decision Support Systems; Heart Disease Classification; Machine Learning Techniques","Artificial intelligence; Cardiology; Classification (of information); Data mining; Decision support systems; Decision trees; Diseases; Heart; Learning algorithms; Nearest neighbor search; Radial basis function networks; Statistical tests; Support vector machines; 10-fold cross-validation; Heart disease; Machine learning techniques; Mining classification; Multi layer perceptron; Prediction of heart disease; Radial Basis Function(RBF); Training and testing; Learning systems",2-s2.0-85030559131
"Luo Q., Peng Y., Li J., Peng X.","MWPCA-ICURD: density-based clustering method discovering specific shape original features",2017,"Neural Computing and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955568929&doi=10.1007%2fs00521-016-2208-9&partnerID=40&md5=c636c1759b5db8ec1b36e10160685050","Uncertain data exist in many application fields, and there are numerous recent efforts in processing uncertain data to get more reliable results, especially uncertainty processing in clustering method. However, it is one of the urgent challenges to discover clusters with specific shape features. So we present a clustering method for data with uncertainties, and it is called multivariate wavelet principal component analysis-improved clustering using references and density (MWPCA-ICURD), which utilizes feature extraction and density-based clustering. To cluster uncertain data with specific shape original features, the original features are extracted by MWPCA method, which combines digital wavelet decomposition and principal component analysis organically. Then, a density-based clustering method ICURD is explored to discover specific shape clusters. Experimental results illustrate its validation and feasibility. © 2016, The Natural Computing Applications Forum.","Clustering for data with uncertainties; Clustering using references and density; Principal component analysis; Wavelet transform","Data handling; Data mining; Feature extraction; Principal component analysis; Uncertainty analysis; Wavelet decomposition; Wavelet transforms; Application fields; Clustering for data with uncertainties; Clustering methods; Density-based Clustering; Reliable results; Shape features; Uncertain datas; Cluster analysis",2-s2.0-84955568929
"Zhao Y., Yang L.T., Zhang R.","A Tensor-Based Multiple Clustering Approach with Its Applications in Automation Systems",2017,"IEEE Transactions on Industrial Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029142715&doi=10.1109%2fTII.2017.2748800&partnerID=40&md5=cccd42452712a554ff4ec9f279b98de6","Multiple clustering analysis has the clear advantages to discover latent data pattern in big data from different views, so it has tremendous practical values in automation industries. However, most of current algorithms are difficult to group heterogeneous data to multiple clusterings according to the requirements of different applications. This paper presents a flexibly multiple clustering analytic and service framework, and a novel tensor-based multiple clusterings (TMC) approach. Heterogeneous data objects in Cyber-Physical-Social Systems (CPSS) are first represented as low-order tensors and a weight tensor construction approach is proposed to measure the importance of attributes combinations in heterogeneous feature spaces. Then, a selective weighted tensor distance (SWTD) is explored to cluster tensorized data objects for different applications. This paper, through real world applications such as a gas processing system and a smart bike maintenance system, illustrates TMC and evaluates its clustering performance. Experimental results reveal TMC can obtain higher-quality clustering results but with lower redundancies to meet different equirements of applications in automation systems. IEEE","Automation; automation systems; Big Data; Computer science; Cyber-Physical-Social Systems; Data mining; heterogeneous data; Maintenance engineering; multiple clusterings; Tensile stress; Tensor; Weight measurement",,2-s2.0-85029142715
"Gupta S., Gupta A.","A set of measures designed to identify overlapped instances in software defect prediction",2017,"Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009227462&doi=10.1007%2fs00607-016-0538-1&partnerID=40&md5=e2fb963be9e6d4c86dbedc11b3506bb9","The performance of the learning models will intensely rely on the characteristics of the training data. The previous outcomes recommend that the overlapping between classes and the presence of noise have the most grounded impact on the performance of learning algorithm, and software defect datasets are no exceptions. The class overlap problem is concerned with the performance of machine learning classifiers critical problem is class overlap in which data samples appear as valid examples of more than one class which may be responsible for the presence of noise in datasets. We aim to investigate how the presence of overlapped instances in a dataset influences the classifier’s performance, and how to deal with class overlapping problem. To have a close estimate of class overlapping, we have proposed four different measures namely, nearest enemy ratio, subconcept ratio, likelihood ratio and soft margin ratio. We performed our investigations using 327 binary defect classification datasets obtained from 54 software projects, where we first identified overlapped datasets using three data complexity measures proposed in the literature. We also include treatment effort into the prediction process. Subsequently, we used our proposed measures to find overlapped instances in the identified overlapped datasets. Our results indicated that by training a classifier on a training data free from overlapped instances led to an improved classifier performance on the test data containing overlapped instances. The classifiers perform significantly better when the evaluation measure takes the effort into account. © 2017, Springer-Verlag Wien.","Class overlapping; Data complexity measures; Data mining; Machine learning; Software defect prediction","Artificial intelligence; Data mining; Defects; Forecasting; Learning algorithms; Learning systems; Class overlapping; Classifier performance; Critical problems; Data complexity; Defect classification; Evaluation measures; Prediction process; Software defect prediction; Classification (of information)",2-s2.0-85009227462
"Brink J.A., Arenson R.L., Grist T.M., Lewin J.S., Enzmann D.","Bits and bytes: the future of radiology lies in informatics and information technology",2017,"European Radiology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014686654&doi=10.1007%2fs00330-016-4688-5&partnerID=40&md5=b822eb2a1e6038a6ce12fbc448599b1b","Abstract: Advances in informatics and information technology are sure to alter the practice of medical imaging and image-guided therapies substantially over the next decade. Each element of the imaging continuum will be affected by substantial increases in computing capacity coincident with the seamless integration of digital technology into our society at large. This article focuses primarily on areas where this IT transformation is likely to have a profound effect on the practice of radiology. Key points: • Clinical decision support ensures consistent and appropriate resource utilization. • Big data enables correlation of health information across multiple domains. • Data mining advances the quality of medical decision-making. • Business analytics allow radiologists to maximize the benefits of imaging resources. © 2017, European Society of Radiology.","Artificial intelligence; Clinical decision support; Data mining; Information technology; Medical informatics","Article; artificial intelligence; clinical decision support system; commercial phenomena; data base; futurology; health care; human; information science; information technology; Internet; priority journal; radiology",2-s2.0-85014686654
"Foithong S., Srinil P., Yangyuen K.T., Phattaraworamet T.","Rough-mutual feature selection based-on minimal-boundary and maximal-lower",2017,"2016 Management and Innovation Technology International Conference, MITiCON 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031727476&doi=10.1109%2fMITICON.2016.8025230&partnerID=40&md5=421bb545a3cf3bc6940b0c0bad35850c","Feature selection (FS) is an important preprocessing step for many applications in Data Mining. Most existing FS methods based on rough set theory focus on dependency function, which is based on lower approximation as for measuring the goodness of the feature subset. However, by determining only information from a positive region but neglecting a boundary region, mostly relevant information could be invisible. This paper, the minimal boundary region-maximal lower approximation (mBML) criterion, focuses on feature selection methods based on rough set and mutual information (MI) which use the different values among the lower approximation information and the information contained in the boundary region. The use of this criterion can result in higher predictive accuracy than those obtained using the measure based on the positive region alone. Experimental results are illustrated for crisp and real-valued data and compared with other FS methods in terms of subset size, runtime, and classification accuracy. © 2016 IEEE.",,"Approximation algorithms; Data mining; Engineering research; Feature extraction; Boundary regions; Classification accuracy; Feature selection methods; Lower approximation; Mutual informations; Pre-processing step; Predictive accuracy; Real-valued data; Rough set theory",2-s2.0-85031727476
"Gunupudi R.K., Nimmala M., Gugulothu N., Gali S.R.","CLAPP: A self constructing feature clustering approach for anomaly detection",2017,"Future Generation Computer Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009909010&doi=10.1016%2fj.future.2016.12.040&partnerID=40&md5=b018964050817ce889a45e6b85b6cfcb","The term internet of things is a buzz word these days and as per Google survey conducted recently, it has even dominated the buzz word big data predominantly. However, IoT area is still not matured and is throwing light on lot of research issues towards the data mining researchers. Security in IoT throws several challenges because of limited resources. In this context, IoT gains importance once again from data miners towards anomaly mining or intrusion detection. Intrusion detection is classified as NP-class in the literature even today. Algorithms addressing privacy and security issues in IoT must consider the complexities involved and hence require re-attention from all researchers. One more problem faced when judging for intrusion is the use of high dimensionality, classifier choice, and distance measure. For example, the traditional distance measure, such as Euclidean misjudges the similarity. In this paper, the objective is to design a fuzzy membership function to address both dimensionality and anomaly mining so as reduce the computational complexity and increase computational accuracies of classifier algorithms. We validate the proposed measure using several experimentations on NSL-KDD and DARPA datasets using kNN, J48 and CANN using Gaussian measure. Improved accuracies of classifiers on U2R and R2L attacks have been recorded in the experimental results obtained for experiments conducted. © 2017 Elsevier B.V.","Anomaly; Classifier; Intrusion; Outliers; Web of Things","Big data; Classifiers; Data mining; Intrusion detection; Membership functions; Mercury (metal); Anomaly; Classifier algorithms; Computational accuracy; Fuzzy membership function; High dimensionality; Intrusion; Outliers; Privacy and security; Internet of things",2-s2.0-85009909010
"Li B., Chan K.C.C., Ou C., Ruifeng S.","Discovering public sentiment in social media for predicting stock movement of publicly listed companies",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018378938&doi=10.1016%2fj.is.2016.10.001&partnerID=40&md5=220f938d50e3201a10b13ef8c6d74caa","The popularity of many social media sites has prompted both academic and practical research on the possibility of mining social media data for the analysis of public sentiment. Studies have suggested that public emotions shown through Twitter could be well correlated with the Dow Jones Industrial Average. However, it remains unclear how public sentiment, as reflected on social media, can be used to predict stock price movement of a particular publicly-listed company. In this study, we attempt to fill this research void by proposing a technique, called SMeDA-SA, to mine Twitter data for sentiment analysis and then predict the stock movement of specific listed companies. For the purpose of experimentation, we collected 200 million tweets that mentioned one or more of 30 companies that were listed in NASDAQ or the New York Stock Exchange. SMeDA-SA performs its task by first extracting ambiguous textual messages from these tweets to create a list of words that reflects public sentiment. SMeDA-SA then made use of a data mining algorithm to expand the word list by adding emotional phrases so as to better classify sentiments in the tweets. With SMeDA-SA, we discover that the stock movement of many companies can be predicted rather accurately with an average accuracy over 70%. This paper describes how SMeDA-SA can be used to mine social media date for sentiments. It also presents the key implications of our study. © 2017","Big data; Data mining; Parallel architecture; Sentiment analysis; SMeDA-SA; Social media analysis; Stock prediction; Twitter","Big data; Financial markets; Forecasting; Motion estimation; Parallel architectures; Social networking (online); Sentiment analysis; SMeDA-SA; Social media analysis; Stock predictions; Twitter; Data mining",2-s2.0-85018378938
"Trop T.","An overview of the management policy for marine sand mining in Israeli Mediterranean shallow waters",2017,"Ocean and Coastal Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021420804&doi=10.1016%2fj.ocecoaman.2017.06.013&partnerID=40&md5=524e265baf0c9cdbc572cb928f04ed25","In Israel, the ever-increasing interest in mining and dumping of marine sand in the shallow waters of the Mediterranean (up to depth of 30 m) on the one hand, and the growing concern for the marine environment on the other, have led to the formulation of various policy tools intended for the rational management of this resource. However, the comprehensiveness and sustainability of this policy, and its adherence to international obligations and customs, remains unclear. This paper provides a structured overview of the management policy governing the extraction and dumping of marine sand in the Israeli Mediterranean shallow waters, and the way environmental values are being taken into account in the regulatory process. It then examines the way in which two main international policies—UNCLOS (not yet ratified by Israel) and the protocol on ICZM (ratified by Israel), which provide principles and standards for the management of environmental risks associated with marine mining activities in the Mediterranean Sea—are transposed into local legal procedures and regulatory requirements. The study reveals that the Israeli marine sand regulatory framework embraced most of the environmental principles and guidelines laid down in the main international conventions. However, several essential issues still need to be addressed. At present, the use of marine sand is usually managed with one key activity in mind, without an all-encompassing policy and monitoring program. As a result, the impact of cumulative effect of extracting and dumping activities (the “big picture”) is overlooked. The study recommends to formulate a sound policy that can be adjusted for social/economic developments as they occur, and can facilitate the response to a wide range of future scenarios while adhering to a sustainability agenda. This policy should be based on up-to-date and standardized data gathered through a national monitoring program and stored in an accessible database. The analysis method and results can form a basis for discussion with other experts working in the field, and may be useful for future management decisions and for other coastal regions in the world. © 2017 Elsevier Ltd","Marine sand extraction; Marine sand management; Marine sand mining; Offshore sand mining; Seabed mining","Environmental management; Environmental regulations; Extraction; Laws and legislation; Sand; Sustainable development; International conventions; International obligations; International policies; Marine sands; National monitoring programs; Offshore sand minings; Regulatory requirements; Seabed minings; Ocean dumping; coastal zone management; environmental monitoring; environmental risk; environmental values; extraction; international agreement; legal system; marine policy; mining; offshore application; regulatory framework; seafloor; shallow water; sustainability; Israel; Mediterranean Coast [Israel]; Mediterranean Sea",2-s2.0-85021420804
"Hosseini N.","Evaluation of the rockburst potential in longwall coal mining using passive seismic velocity tomography and image subtraction technique",2017,"Journal of Seismology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015905138&doi=10.1007%2fs10950-017-9654-4&partnerID=40&md5=a717b44304717ca567ba9537992462d5","Rockburst is a typical dynamic disaster in underground coal mines which its occurrences relate to the mechanical quality of coal seam and surrounding rock mass and also the condition of stress distribution. The main aim of this paper is to study the potential of rockburst in a longwall coal mine by using of passive seismic velocity tomography and image subtraction technique. For this purpose, first by mounting an array of receivers on the surface above the active panel, the mining-induced seismic data as a passive source for several continuous days were recorded. Then, the three-dimensional tomograms using simultaneous iteration reconstruction technique (SIRT) for each day are created and by employing the velocity filtering, the overstressed zones are detected. In addition, the two-dimensional seismic velocity tomograms in coal seam level by slicing the three-dimensional tomograms are obtained. Then the state of stress changes in successive days by applying the image subtraction technique on these two-dimensional tomograms is considered. The results show that the compilation of filtered three-dimensional tomograms and subtracted images is an appropriate approach for detecting the overstressed zones around the panel and subsequent evaluation of rockburst potential. The research conclusion proves that the applied approach in this study in combination with field observations of rock mass status can effectively identify the rockburst-prone areas during the mining operation and help to improve the safety condition. © 2017, Springer Science+Business Media Dordrecht.","Images subtraction; Longwall; Passive seismic velocity tomography; Rockburst","coal mining; imaging method; longwall mining; rockburst; seismic data; seismic tomography; seismic velocity",2-s2.0-85015905138
"Martínez Rodríguez D., Nin J., Nuñez-del-Prado M.","Towards the adaptation of SDC methods to stream mining",2017,"Computers and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028910209&doi=10.1016%2fj.cose.2017.08.011&partnerID=40&md5=c01bbba16a0e022671fa996acd89a6b8","Most of the existing statistical disclosure control (SDC) standards, such as k-anonymity or l-diversity, were initially designed for static data. Therefore, they cannot be directly applied to stream data which is continuous, transient, and usually unbounded. Moreover, in streaming applications, there is a need to offer strong guarantees on the maximum allowed delay between incoming data and its corresponding anonymous output. In order to full-fill with these requirements, in this paper, we present a set of modifications to the most standard SDC methods, efficiently implemented within the Massive Online Analysis (MOA) stream mining framework. Besides, we have also developed a set of performance metrics to evaluate Information Loss and Disclosure Risk values continuously. Finally, we also show the efficiency of our new methods with a large set of experiments. © 2017 Elsevier Ltd","MOA Framework; Privacy; Statistical disclosure control; Stream mining; Stream processing","Computer science; Data privacy; Security of data; Information loss; MOA Framework; On-line analysis; Performance metrics; Statistical disclosure Control; Stream mining; Stream processing; Streaming applications; Birds",2-s2.0-85028910209
"Gursoy M.E., Inan A., Nergiz M.E., Saygin Y.","Differentially private nearest neighbor classification",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025447482&doi=10.1007%2fs10618-017-0532-z&partnerID=40&md5=d7b75ace4f3d0644e89abe2e8e334d26","Instance-based learning, and the k-nearest neighbors algorithm (k-NN) in particular, provide simple yet effective classification algorithms for data mining. Classifiers are often executed on sensitive information such as medical or personal data. Differential privacy has recently emerged as the accepted standard for privacy protection in sensitive data. However, straightforward applications of differential privacy to k-NN classification yield rather inaccurate results. Motivated by this, we develop algorithms to increase the accuracy of private instance-based classification. We first describe the radius neighbors classifier (r-N) and show that its accuracy under differential privacy can be greatly improved by a non-trivial sensitivity analysis. Then, for k-NN classification, we build algorithms that convert k-NN classifiers to r-N classifiers. We experimentally evaluate the accuracy of both classifiers using various datasets. Experiments show that our proposed classifiers significantly outperform baseline private classifiers (i.e., straightforward applications of differential privacy) and executing the classifiers on a dataset published using differential privacy. In addition, the accuracy of our proposed k-NN classifiers are at least comparable to, and in many cases better than, the other differentially private machine learning techniques. © 2017, The Author(s).","Data mining; Differential privacy; k-Nearest neighbors","Data mining; Data privacy; Education; Learning systems; Membership functions; Nearest neighbor search; Sensitivity analysis; Text processing; Classification algorithm; Differential privacies; Instance based learning; K-nearest neighbors; Machine learning techniques; Nearest neighbor classification; Sensitive informations; Standard for privacy protection; Classification (of information)",2-s2.0-85025447482
"Lidan J.","Dynamic evolution of technology innovation network from the perspective of structural holes",2017,"Boletin Tecnico/Technical Bulletin",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032875071&partnerID=40&md5=e33aaa1f606a2ed0efe6f398c52a089c","Non-redundant connection is the key to improve the ability of controlling resource and enhance technological competitive advantage for members in technology innovation network. Taking the IC industry as an example, the structural holes theory which constructed on the basis of non-redundant connection is introduced into the dynamic evolution of industrial technology innovation networks. Through data mining and network visualization, the dynamic evolution diagram of IC industry technology innovation network is obtained; then the article analyses the maximum effective size, the efficiency of optimization, the soaring constraint and the synkinetic hierarchy in periods of industrial technology innovation networks’ dynamic evolution. Based on analysis above, the dynamic evolution of IC industry technology innovation network can be described with five stages, which including “Embryonic immature stage – decentralization”, “Slow growing stage – emergence”, “fast growing stage –compaction”, “mature and stable stage – efficiency” and “Mitigation transformation stage –agglomeration”. It is found that the dynamic evolution of IC technology innovation network exists phenomenon as follows: the evolutionary characteristics are various, the optimization pathsare contradictory, the optimal states are asynchronous.","Dynamic evolution; IC; Industrial technology innovation network; Structural holes","Competition; Data mining; Data visualization; Efficiency; Engineering research; Integrated circuits; Competitive advantage; Dynamic evolution; Effective size; Growing stages; Industrial technology; Network visualization; Structural holes; Technology innovation; Innovation",2-s2.0-85032875071
"Rus V., Markov Z., Russell I.","Report on the 30th International Florida Artificial Intelligence Research Society Conference (FLAIRS-30)",2017,"AI Magazine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030548158&partnerID=40&md5=e76ed05645958cdad4b3ef7f000b9e01","The Florida Artificial Intelligence Society (FLAIRS) was founded in 1987 to promote and advance artificial intelligence research in the state of Florida and to foster the exchange of ideas and collaboration among the state's researchers from universities and industry through an annual conference. Shortly thereafter the FLAIRS conference, a general AI conference, grew to become a major venue for AI researchers from around the world to present their work. The conference continues its in-cooperation status with the Association for the Advancement of Artificial Intelligence. Continuing a long tradition of presenting and discussing state-of-the-art artificial intelligence research in a sociable atmosphere within a beautiful setting, the 30th International Florida Artificial Intelligence Research Soci- ety Conference (FLAIRS-30) took place May 22-24, 2017, in Marco Island, Florida, USA. It attracted 192 participants from 20 countries, with about one-third coming from outside the United States. The program included a general session with many excellent papers spanning a broad range of AI research areas and comprising traditional topics such as machine learning, reasoning, and optimization. Seventeen special tracks with several outstanding papers supplemented the program, bringing breadth to the general session. An integral part of the conference, the special tracks, are intended to provide researchers working in similar areas the opportunity to meet and to present work in those areas. These focused sessions also offer forums for interactions among a broader community of AI researchers. The special tacks program included sessions and papers on AI and Cyber- Security, AI in Games, Serious Games, and Multimedia, AI in Health-Care Informatics, Applications of Artificial Intelligence in Business and Industry, Applied Natural Language Processing, Artificial Intelligence for Big Social Data Analysis, Autonomous Robots and Agents, Case-Based Reasoning, Data Mining, Learning from Heterogeneous Data Analytics, Intelligent Learning Technologies, Intelligent Support for Decision Making, Natural Language Processing of Ancient and Other Low-Resource Languages, Nonclassical Logic, Recommender Systems, Semantic / Logics / Information. © 2017, Association for the Advancement of Artificial Intelligence.",,"Application programs; Artificial intelligence; Case based reasoning; Data mining; Decision making; Education; Intelligent robots; Learning algorithms; Learning systems; Multimedia systems; Semantics; Serious games; Artificial intelligence research; Health care informatics; Heterogeneous data; Intelligent learning; Intelligent support; Low resource languages; Non-classical logic; Outstanding paper; Natural language processing systems",2-s2.0-85030548158
"Pelekis N., Tampakis P., Vodas M., Doulkeridis C., Theodoridis Y.","On temporal-constrained sub-trajectory cluster analysis",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016983424&doi=10.1007%2fs10618-017-0503-4&partnerID=40&md5=086cb2ef6896e2c55bddc01d08835ace","Cluster analysis over Moving Object Databases (MODs) is a challenging research topic that has attracted the attention of the mobility data mining community. In this paper, we study the temporal-constrained sub-trajectory cluster analysis problem, where the aim is to discover clusters of sub-trajectories given an ad-hoc, user-specified temporal constraint within the dataset’s lifetime. The problem is challenging because: (a) the time window is not known in advance, instead it is specified at query time, and (b) the MOD is continuously updated with new trajectories. Existing solutions first filter the trajectory database according to the temporal constraint, and then apply a clustering algorithm from scratch on the filtered data. However, this approach is extremely inefficient, when considering explorative data analysis where multiple clustering tasks need to be performed over different temporal subsets of the database, while the database is updated with new trajectories. To address this problem, we propose an incremental and scalable solution to the problem, which is built upon a novel indexing structure, called Representative Trajectory Tree (ReTraTree). ReTraTree acts as an effective spatio-temporal partitioning technique; partitions in ReTraTree correspond to groupings of sub-trajectories, which are incrementally maintained and assigned to representative (sub-)trajectories. Due to the proposed organization of sub-trajectories, the problem under study can be efficiently solved as simply as executing a query operator on ReTraTree, while insertion of new trajectories is supported. Our extensive experimental study performed on real and synthetic datasets shows that our approach outperforms a state-of-the-art in-DBMS solution supported by PostgreSQL by orders of magnitude. © 2017, The Author(s).","Cluster analysis; Indexing; Moving objects; Temporal-constrained (sub-)trajectory clustering","Cluster analysis; Clustering algorithms; Data mining; Database systems; Filtration; Indexing (of information); Object-oriented databases; Query processing; Explorative data analysis; Indexing structures; Moving object database; Moving objects; Multiple clusterings; Temporal constraints; Trajectory clustering; Trajectory database; Trajectories",2-s2.0-85016983424
"Fang K., Shen C.","Full-flow-regime storage-streamflow correlation patterns provide insights into hydrologic functioning over the continental US",2017,"Water Resources Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030170711&doi=10.1002%2f2016WR020283&partnerID=40&md5=c91dd7ce425eec418140b2ee482ebdf5","Interannual changes in low, median, and high regimes of streamflow have important implications for flood control, irrigation, and ecologic and human health. The Gravity Recovery and Climate Experiment (GRACE) satellites record global terrestrial water storage anomalies (TWSA), providing an opportunity to observe, interpret, and potentially utilize the complex relationships between storage and full-flow-regime streamflow. Here we show that utilizable storage-streamflow correlations exist throughout vastly different climates in the continental US (CONUS) across low- to high-flow regimes. A panoramic framework, the storage-streamflow correlation spectrum (SSCS), is proposed to examine macroscopic gradients in these relationships. SSCS helps form, corroborate or reject hypotheses about basin hydrologic behaviors. SSCS patterns vary greatly over CONUS with climate, land surface, and geologic conditions. Data mining analysis suggests that for catchments with hydrologic settings that favor storage over runoff, e.g., a large fraction of precipitation as snow, thick and highly-permeable permeable soil, SSCS values tend to be high. Based on our results, we form the hypotheses that groundwater flow dominates streamflows in Southeastern CONUS and Great Plains, while thin soils in a belt along the Appalachian Plateau impose alimit on water storage. SSCS also suggests shallow water table caused by high-bulk density soil and flat terrain induces rapid runoff in several regions. Our results highlight the importance of subsurface properties and groundwater flow in capturing flood and drought. We propose that SSCS can be used as a fundamental hydrologic signature to constrain models and to provide insights thatlead usto better understand hydrologic functioning. © 2017. American Geophysical Union. All Rights Reserved.","base flow; catchment classification; GRACE; hydrologic machine learning; hydrologic signature; storage-streamflow relationship","Catchments; Data mining; Flood control; Floods; Geodetic satellites; Groundwater; Groundwater flow; Learning systems; Marine signal systems; Runoff; Soils; Stream flow; Baseflows; Catchment classifications; Complex relationships; GRACE; Gravity recovery and climate experiment satellites; Hydrologic signature; Subsurface properties; Terrestrial water storage; Digital storage; baseflow; catchment; classification; correlation; data mining; flow pattern; GRACE; groundwater flow; hydrological regime; hydrological response; machine learning; satellite data; streamflow; water storage; Appalachian Plateau; Great Plains; United States",2-s2.0-85030170711
"Kakol M., Nielek R., Wierzbicki A.","Understanding and predicting Web content credibility using the Content Credibility Corpus",2017,"Information Processing and Management",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018953320&doi=10.1016%2fj.ipm.2017.04.003&partnerID=40&md5=f630e1b989af3bca2e87f9e38f2e5fbb","The goal of our research is to create a predictive model of Web content credibility evaluations, based on human evaluations. The model has to be based on a comprehensive set of independent factors that can be used to guide user's credibility evaluations in crowdsourced systems like WOT, but also to design machine classifiers of Web content credibility. The factors described in this article are based on empirical data. We have created a dataset obtained from an extensive crowdsourced Web credibility assessment study (over 15 thousand evaluations of over 5000 Web pages from over 2000 participants). First, online participants evaluated a multi-domain corpus of selected Web pages. Using the acquired data and text mining techniques we have prepared a code book and conducted another crowdsourcing round to label textual justifications of the former responses. We have extended the list of significant credibility assessment factors described in previous research and analyzed their relationships to credibility evaluation scores. Discovered factors that affect Web content credibility evaluations are also weakly correlated, which makes them more useful for modeling and predicting credibility evaluations. Based on the newly identified factors, we propose a predictive model for Web content credibility. The model can be used to determine the significance and impact of discovered factors on credibility evaluations. These findings can guide future research on the design of automatic or semi-automatic systems for Web content credibility evaluation support. This study also contributes the largest credibility dataset currently publicly available for research: the Content Credibility Corpus (C3). © 2017 The Authors","Credibility evaluation; Credibility issues; Crowdsourcing; Evaluating web site content; Web credibility","Crowdsourcing; Data mining; Credibility assessment; Credibility evaluation; Credibility issues; Crowdsourced systems; Semi-automatic systems; Text mining techniques; Web credibility; Web site contents; Websites",2-s2.0-85018953320
"Lan W., Jia S., Song S., Li K.","Multi-classification spacecraft electrical signal identification method based on random forest",2017,"Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032617399&doi=10.13700%2fj.bh.1001-5965.2016.0661&partnerID=40&md5=ea51f3280a6387681853d6e5d94549a9","The spacecraft electrical signal characteristic data have problems such as large amount, high-dimensional features, high computational complexity and low identification rate. The feature extraction method of principal component analysis (PCA) and random forest (RF) algorithm was proposed to reduce the dimensionality of the original data, improve the computational efficiency and identification rate, and achieve rapid and accurate identification of spacecraft electrical signal data. The random forest algorithm has superior performance in dealing with high-dimensional data. However, considering the time complexity, the method of PCA was used to compress the data and reduce the dimension in order to ensure the accuracy of the classification and improve the computational efficiency. The experimental results show that compared with other algorithms, the proposed method shows excellent performance in accuracy, computational efficiency, and stability when dealing with spacecraft electrical signal data. © 2017, Editorial Board of JBUAA. All right reserved.","Electrical signal identification; Multi-classification; Principal component analysis (PCA); Random forest (RF); Spacecraft","Clustering algorithms; Data mining; Decision trees; Efficiency; Principal component analysis; Spacecraft; Electrical signal; Feature extraction methods; High dimensional data; High dimensional feature; Identification rates; Multi-classification; Random forest algorithm; Random forests; Computational efficiency",2-s2.0-85032617399
"Ebrahimi M., Yazdavar A.H., Sheth A.","Challenges of Sentiment Analysis for Dynamic Events",2017,"IEEE Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644590&doi=10.1109%2fMIS.2017.3711649&partnerID=40&md5=05d786c51d0bc89232f650a4c943a07d","Efforts to assess people's sentiments on Twitter have suggested that Twitter could be a valuable resource for studying political sentiment and that it reflects the offline political landscape. Many opinion mining systems and tools provide users with people's attitudes toward products, people, or topics and their attributes/aspects. However, although it may appear simple, using sentiment analysis to predict election results is difficult, since it is empirically challenging to train a successful model to conduct sentiment analysis on tweet streams for a dynamic event such as an election. This article highlights some of the challenges related to sentiment analysis encountered during monitoring of the presidential election using Kno.e.sis's Twitris system. © 2001-2011 IEEE.","artificial intelligence; deep learning; intelligent systems; machine learning; natural language processing; sentiment analysis; sentiment for dynamic events; sentiment for volatile content; social media analysis","Artificial intelligence; Data mining; Deep learning; Intelligent systems; Learning algorithms; Learning systems; Social networking (online); Dynamic events; Offline; Opinion mining; Presidential election; Sentiment analysis; Social media analysis; Volatile contents; Natural language processing systems",2-s2.0-85032644590
"Liu K., El-Gohary N.","Ontology-based semi-supervised conditional random fields for automated information extraction from bridge inspection reports",2017,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019933777&doi=10.1016%2fj.autcon.2017.02.003&partnerID=40&md5=7d0d98efc417264b23160c9d35c6aa12","A large amount of detailed data about bridge conditions and maintenance actions are buried in bridge inspection reports without being used. Information extraction and data analytics open opportunities to leverage this wealth of data for improved bridge deterioration prediction and enhanced maintenance decision making. This paper proposes a novel ontology-based, semi-supervised conditional random fields (CRF)-based information extraction methodology for extracting information entities describing existing deficiencies and performed maintenance actions from bridge inspection reports. The ontology facilitates the analysis of the text based on content and domain-specific meaning. The proposed semi-supervised CRF simultaneously captures the dependency structures as well as the distributions of labeled and unlabeled data in a concave machine-learning function. It learns from a small set of fixed labeled data and, at the same time, dynamically adapts itself to unseen instances by further learning from a large set of unlabeled data for both reduced human effort and high performance. The proposed algorithm achieved an average precision, recall and, F-1 measure of 94.1%, 87.7%, and 90.7%, respectively. © 2017 Elsevier B.V.","Bridges; Conditional random fields; Deterioration prediction; Information extraction; Maintenance decision making; Ontology; Semi-supervised machine learning","Artificial intelligence; Bridge decks; Bridges; Decision making; Deterioration; Information analysis; Information retrieval; Inspection; Learning systems; Maintenance; Ontology; Random processes; Supervised learning; Bridge deterioration; Conditional random field; Dependency structures; Deterioration prediction; Extracting information; Labeled and unlabeled data; Maintenance decision making; Semi-supervised; Data mining",2-s2.0-85019933777
"Bielecki A., Wójcik M.","Hybrid system of ART and RBF neural networks for online clustering",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018737489&doi=10.1016%2fj.asoc.2017.04.012&partnerID=40&md5=5c287213c8b4daeeccfb3e956d57c1b1","An online clustering task is considered for machine state monitoring purpose. In the previous authors’ researches a classical ART-2 network was tested for online classification of operational states in the context of a wind turbine monitoring. Some drawbacks, however, were found when a data stream size had been increased. This case is investigated in this paper. Classical ART-2 network can cluster data incorrectly when data points are compared by using Euclidean distance. Furthermore, ART-2 network can lose accuracy when data stream is processed for long time. The way of improving the ART-2 network is considered and two main steps of that are taken. At first, the stereographic projection is implemented. At the second step, a new type of hybrid neural system which consists of ART-2 and RBF networks with data processed by using the stereographic projection is introduced. Tests contained elementary scenarios for low-dimensional cases as well as higher dimensional real data from wind turbine monitoring. All the tests implied that an efficient system for online clustering had been found. © 2017 Elsevier B.V.","ART-2 neural network; Expectation-maximization algorithm; Hybrid system; Online classification; RBF neural network","Data communication systems; Data mining; Hybrid systems; Image segmentation; Maximum principle; Radial basis function networks; Wind turbines; ART-2 neural networks; Euclidean distance; Expectation-maximization algorithms; Higher-dimensional; On-line classification; Operational state; RBF Neural Network; Stereographic projection; Arts computing",2-s2.0-85018737489
"He M., Cheng Y., Qiu L., Zhao Z.","An Algorithm of Building Extraction in Urban Area Based on Improved Top-hat Transformations and LBP Elevation Texture",2017,"Cehui Xuebao/Acta Geodaetica et Cartographica Sinica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031892645&doi=10.11947%2fj.AGCS.2017.20170158&partnerID=40&md5=a3badbf1eda2d88fdafc4c6d8aef2978","Classification of building and vegetation is difficult solely by LiDAR data and vegetation in shadows can't be eliminated only by aerial images. The improved top-hat transformations and local binary patterns (LBP) elevation texture analysis for building extraction are proposed based on the fusion of aerial images and LiDAR data. Firstly, LiDAR data is reorganized into grid cell, the algorithm removes ground points through top-hat transform. Then, the vegetation points are extracted by normalized difference vegetation index (NDVI). Thirdly, according to the elevation information of LiDAR points, LBP elevation texture is calculated and achieving precise elimination of vegetation in shadows or surrounding to the buildings. At last, morphological operations are used to fill the holes of building roofs, and region growing for complete building edges. The simulation is based on the complex urban area in Vaihingen benchmark provided by ISPRS, the results show that the algorithm affording higher classification accuracy. © 2017, Surveying and Mapping Press. All right reserved.","Building extraction; Digital aerial images; LBP; LiDAR; Morphology","Buildings; Extraction; Image enhancement; Image processing; Mathematical morphology; Morphology; Optical radar; Vegetation; Aerial images; Building extraction; Classification accuracy; Complex urban area; Local binary patterns; Morphological operations; Normalized difference vegetation index; Top-hat transformation; Data mining; aerial photograph; algorithm; building; data assimilation; digital image; elevation; extraction method; lidar; NDVI; texture; urban area; vegetation",2-s2.0-85031892645
"Choudhary A., Naughton L.M., Montánchez I., Dobson A.D.W., Rai D.K.","Current status and future prospects of Marine Natural Products (MNPs) as antimicrobials",2017,"Marine Drugs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030631826&doi=10.3390%2fmd15090272&partnerID=40&md5=6ff0d4c7350e97d3891be78265e618b0","The marine environment is a rich source of chemically diverse, biologically active natural products, and serves as an invaluable resource in the ongoing search for novel antimicrobial compounds. Recent advances in extraction and isolation techniques, and in state-of-the-art technologies involved in organic synthesis and chemical structure elucidation, have accelerated the numbers of antimicrobial molecules originating from the ocean moving into clinical trials. The chemical diversity associated with these marine-derived molecules is immense, varying from simple linear peptides and fatty acids to complex alkaloids, terpenes and polyketides, etc. Such an array of structurally distinct molecules performs functionally diverse biological activities against many pathogenic bacteria and fungi, making marine-derived natural products valuable commodities, particularly in the current age of antimicrobial resistance. In this review, we have highlighted several marine-derived natural products (and their synthetic derivatives), which have gained recognition as effective antimicrobial agents over the past five years (2012–2017). These natural products have been categorized based on their chemical structures and the structure-activity mediated relationships of some of these bioactive molecules have been discussed. Finally, we have provided an insight into how genome mining efforts are likely to expedite the discovery of novel antimicrobial compounds. © 2017 by the authors. Licensee MDPI.","Antibacterial; Antifungal; Antimicrobial; Genome mining; Marine natural products (MNPs); Secondary metabolites","29 demethylgeodisterol 3 o sulfite; antiinfective agent; chrysophaentin A; chrysophaentin B; chrysophaentin C; chrysophaentin D; chrysophaentin E; chrysophaentin F; chrysophaentin G; cristatumin A; cristatumin D; dehydroechinulin; dodecanoic acid methyl ester; echinulin; geodisterol 3 o sulfite; halide; hemimycalin A; isocoumarin derivative; lyngbic acid; lyngbyoic acid; marine natural product; natural products and their synthetic derivatives; nucleoside derivative; polyketide; polypeptide antibiotic agent; pyranonigrin A; rubrumazine B; tardioxopiperazine A; unclassified drug; unindexed drug; variecolorin H; Alternaria brassicae; antibacterial activity; antifungal activity; bioinformatics; Chrysophaeum taylori; Chrysophyta; data mining; drug mechanism; drug potency; drug screening; genetic procedures; genome mining; genomic sequencing; genomics; methicillin resistant Staphylococcus aureus; methicillin susceptible Staphylococcus aureus; minimum inhibitory concentration; nonhuman; Penicillium; Penicillium brocae; Review; structure activity relation; structure analysis; Vibrio harveyi; Vibrio parahaemolyticus",2-s2.0-85030631826
"Lu Y., Okada S., Nitta K.","Modeling content structures of domain-specific texts with RUP-HDP-HSMM and its applications",2017,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029447398&doi=10.1587%2ftransinf.2017EDP7043&partnerID=40&md5=a71cf9af4f9d47b758fb161bff8ffc55","We propose a novel method, built upon the hierarchical Dirichlet process hidden semi-Markov model, to reveal the content structures of unstructured domain-specific texts. The content structures of texts consisting of sequential local contexts are useful for tasks, such as text retrieval, classification, and text mining. The prominent feature of our model is the use of the recursive uniform partitioning, a stochastic process taking a view different from existing HSMMs in modeling state duration. We show that the recursive uniform partitioning plays an important role in avoiding the rapid switching between hidden states. Remarkably, our method greatly outperforms others in terms of ranking performance in our text retrieval experiments, and provides more accurate features for SVM to achieve higher F1 scores in our text classification experiments. These experiment results suggest that our method can yield improved representations of domainspecific texts. Furthermore, we present a method of automatically discovering the local contexts that serve to account for why a text is classified as a positive instance, in the supervised learning settings. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.","Content structure; Hidden semi-Markov models; Local features; Rapid switching; Text mining","Classification (of information); Data mining; Hidden Markov models; Markov processes; Natural language processing systems; Random processes; Stochastic models; Stochastic systems; Text processing; Content structure; Hidden semi-Markov models; Local feature; Rapid switching; Text mining; Information retrieval",2-s2.0-85029447398
"Bessiere C., De Raedt L., Guns T., Kotthoff L., Nanni M., Nijssen S., O'Sullivan B., Paparrizou A., Pedreschi D., Simonis H.","The Inductive Constraint Programming Loop",2017,"IEEE Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032629075&doi=10.1109%2fMIS.2017.3711637&partnerID=40&md5=9178a6d574163fbe7da3930093535e2c","Constraint programming is used for a variety of real-world optimization problems, such as planning, scheduling, and resource allocation problems, all while we continuously gather vast amounts of data about these problems. Current constraint programming software doesn't exploit such data to update schedules, resources, and plans. The authors propose a new framework that they call the inductive constraint programming loop. In this approach, data is gathered and analyzed systematically to dynamically revise and adapt constraints and optimization criteria. Inductive constraint programming aims to bridge the gap between the areas of data mining and machine learning on one hand and constraint programming on the other. © 2001-2011 IEEE.","artificial intelligence; constraint programming; data mining; intelligent systems; machine learning","Artificial intelligence; Constraint theory; Data mining; Intelligent systems; Learning systems; Optimization; Constraint programming; Optimization criteria; Real-world optimization; Resource allocation problem; Computer programming",2-s2.0-85032629075
"Li Z., Xin Y., Cui X., Liu X., Wang L., Zhang W., Lu Q., Zhu H.","Optimization to the Phellinus experimental environment based on classification forecasting method",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030029842&doi=10.1371%2fjournal.pone.0185444&partnerID=40&md5=1b4700abbaba49cd05b945f3e67141b8","Phellinus is a kind of fungus and known as one of the elemental components in drugs to avoid cancer. With the purpose of finding optimized culture conditions for Phellinus production in the lab, plenty of experiments focusing on single factor were operated and large scale of experimental data was generated. In previous work, we used regression analysis and GA Gene-set based Genetic Algorithm (GA) to predict the production, but the data we used depended on experimental experience and only little part of the data was used. In this work we use the values of parameters involved in culture conditions, including inoculum size, PH value, initial liquid volume, temperature, seed age, fermentation time and rotation speed, to establish a high yield and a low yield classification model. Subsequently, a prediction model of BP neural network is established for high yield data set. GA is used to find the best culture conditions. The forecast accuracy rate more than 90% and the yield we got have a slight increase than the real yield. © 2017 Li et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Article; artificial neural network; classification algorithm; computer simulation; controlled study; data mining; data processing; decision making; fermentation; forecasting; genetic algorithm; nonhuman; Phellinus; plant yield; process optimization; algorithm; Basidiomycetes; environment; growth, development and aging; pH; statistical model; temperature; Algorithms; Basidiomycota; Computer Simulation; Environment; Hydrogen-Ion Concentration; Logistic Models; Neural Networks (Computer); Temperature",2-s2.0-85030029842
"Chi Y., Fu H.","Subspace Learning From Bits",2017,"IEEE Transactions on Signal Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028409769&doi=10.1109%2fTSP.2017.2712127&partnerID=40&md5=c901b8114eb15e30e58ab6fb77640901","Networked sensing, where the goal is to perform complex inference using a large number of inexpensive and decentralized sensors, has become an increasingly attractive research topic due to its applications in wireless sensor networks and internet-of-things. To reduce the communication, sensing, and storage complexity, this paper proposes a simple sensing and estimation framework to faithfully recover the principal subspace of high-dimensional data streams using a collection of binary measurements from distributed sensors, without transmitting the whole data. The binary measurements are designed to indicate comparison outcomes of aggregated energy projections of the data samples over pairs of randomly selected directions. When the covariance matrix is a low-rank matrix, we propose a spectral estimator that recovers the principal subspace of the covariance matrix as the subspace spanned by the top eigenvectors of a properly designed surrogate matrix, which is provably accurate as soon as the number of binary measurements is sufficiently large. An adaptive rank selection strategy based on soft thresholding is also presented. Furthermore, we propose a tailored spectral estimator when the covariance matrix is additionally Toeplitz, and show that the reliable estimation can be obtained from a substantially smaller number of binary measurements. Our results hold even when a constant fraction of the binary measurements is randomly flipped. Finally, we develop a low-complexity online algorithm to track the principal subspace when new measurements arrive sequentially. Numerical experiments are provided to validate the proposed approach. © 2017 IEEE.","binary sensing; Network sensing; principal subspace estimation; subspace tracking","Bins; Clustering algorithms; Complex networks; Computational complexity; Data mining; Digital storage; Matrix algebra; Spectroscopy; Wireless sensor networks; Binary measurements; binary sensing; Energy projections; High-dimensional data streams; Network-sensing; Numerical experiments; Principal subspace; Subspace tracking; Covariance matrix",2-s2.0-85028409769
"Wu Y., Li Y., Qian R.","NE-UserCF: Collaborative filtering recommender system model based on NMF and E2LSH",2017,"International Journal of Performability Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029358174&doi=10.23940%2fijpe.17.05.p6.610619&partnerID=40&md5=dbb72211a5d08f3a7a908c330e656c60","With the rapid development of big data and cloud computing, recommender systems (RSs) have gained significant attention in recent decades. However, there are still many challenges and drawbacks existed in RSs, such as complex and high-dimensional data, low recommendation accuracy, time-consuming and low-efficiency, which to a large extent restrict its applications. Non-negative Matrix Factorization algorithm (NMF) is a matrix factorization algorithm which finds the positive factorization of a given positive matrix. It can eliminate invalid and redundant features in user-rating matrix (URM), reduce URM's dimension. Exact Euclidean Locality Sensitive Hashing (E2LSH) is an advanced algorithm for solving the approximate or exact Near Neighbor Search in high dimensional spaces. It can cluster similar-interest users (SIUs) of URM efficiently. Therefore, the authors propose an improved recommender system model named NE-UserCF (NMF-E2LSH-UserCF) based on NMF and E2LSH to improve the quality and performance of recommendation. The authors first utilize the NMF to process original URM, get a new-URM without invalid and redundant features. Then use E2LSH to cluster users in new-URM based on their interests and produce the similar-interest-user matrix (SIUM). The authors further process the Top-10 recommendations by adopting the user-based collaborative filtering algorithm (UserCF). Finally evaluate experimental results by analyzing metrics Precision, Recall, Coverage and Popularity. Experiments indicate that NE-UserCF proposed in this paper improves the quality of recommendation and has a good performance. © 2017 Totem Publisher, Inc. All rights reserved.","Collaborative Filtering; E2LSH; NMF; Recommender Systems; UserCF","Big data; Clustering algorithms; Collaborative filtering; Data mining; Distributed computer systems; Factorization; Nearest neighbor search; Recommender systems; Collaborative filtering algorithms; Collaborative filtering recommender systems; E2LSH; High dimensional spaces; Locality sensitive hashing; Non-negative matrix factorization algorithms; Quality of recommendations; UserCF; Matrix algebra",2-s2.0-85029358174
"Jiang L., Yang C.C.","User recommendation in healthcare social media by assessing user similarity in heterogeneous network",2017,"Artificial Intelligence in Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015442931&doi=10.1016%2fj.artmed.2017.03.002&partnerID=40&md5=1609e3ff9228ff905dd1cfb408c6f659","Objective The rapid growth of online health social websites has captured a vast amount of healthcare information and made the information easy to access for health consumers. E-patients often use these social websites for informational and emotional support. However, health consumers could be easily overwhelmed by the overloaded information. Healthcare information searching can be very difficult for consumers, not to mention most of them are not skilled information searcher. In this work, we investigate the approaches for measuring user similarity in online health social websites. By recommending similar users to consumers, we can help them to seek informational and emotional support in a more efficient way. Methods We propose to represent the healthcare social media data as a heterogeneous healthcare information network and introduce the local and global structural approaches for measuring user similarity in a heterogeneous network. We compare the proposed structural approaches with the content-based approach. Results Experiments were conducted on a dataset collected from a popular online health social website, and the results showed that content-based approach performed better for inactive users, while structural approaches performed better for active users. Moreover, global structural approach outperformed local structural approach for all user groups. In addition, we conducted experiments on local and global structural approaches using different weight schemas for the edges in the network. Leverage performed the best for both local and global approaches. Finally, we integrated different approaches and demonstrated that hybrid method yielded better performance than the individual approach. Conclusion The results indicate that content-based methods can effectively capture the similarity of inactive users who usually have focused interests, while structural methods can achieve better performance when rich structural information is available. Local structural approach only considers direct connections between nodes in the network, while global structural approach takes the indirect connections into account. Therefore, the global similarity approach can deal with sparse networks and capture the implicit similarity between two users. Different approaches may capture different aspects of the similarity relationship between two users. When we combine different methods together, we could achieve a better performance than using each individual method. © 2017 Elsevier B.V.","Healthcare informatics; Heterogeneous network mining; Recommendation systems; Similarity analysis; Social media analytics","Health; Heterogeneous networks; Information retrieval; Information services; Recommender systems; Social networking (online); Websites; Content-based approach; Content-based methods; Health care informatics; Information networks; Information searching; Similarity analysis; Social media analytics; Structural information; Health care; Article; breast cancer; consumer; data mining; dental health; depression; heart disease; human; lung cancer; medical informatics; medical information; priority journal; skin cancer; social media; stomach cancer",2-s2.0-85015442931
"Ishfaq U., Khan H.U., Iqbal K.","Identifying the influential bloggers: A modular approach based on sentiment analysis",2017,"Journal of Web Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021762092&partnerID=40&md5=09fd0f7ba0d9807c7c54c4288fcaea82","The social web provides an easy and quick medium for public communication and online social interactions. In the web log, short as a blog, the bloggers share their views in the form of creating and commenting on blog posts. The bloggers who influence other users in a blogging community are known as the influential bloggers. Identification of such influential bloggers has vast applications in advertising, online marketing and e-commerce. This paper investigates the problem of identifying influential bloggers and presents a model which consists of two modules: Activity and Recognition. The activity module takes into account a blogger’s activity and recognition module measures a blogger’s influence in his/her social community. The integration of activity and recognition modules identifies the active as well as influential bloggers. The proposed model, MIBSA (Model to find Influential Bloggers using Sentiment Analysis), takes into account the existing and novel features of sentiment expressed in content generated by a blogger. The model is evaluated against the existing standard models using the real world blogging data. The results confirm that sentiment expressed in blog content plays an important role in measuring a blogger’s influence and should be considered as a feature for finding the top influential bloggers in the blogosphere. © Rinton Press.","Big data; Blog; Blogosphere; Influential bloggers; Sentiment Analysis Communicated by: B. White & M. bielikova; Social web","Big data; Data mining; Electronic commerce; Marketing; Blog; Bloggers; Blogospheres; Sentiment analysis; Social webs; Blogs",2-s2.0-85021762092
"Guzman E., Alkadhi R., Seyff N.","An exploratory study of Twitter messages about software applications",2017,"Requirements Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757129&doi=10.1007%2fs00766-017-0274-x&partnerID=40&md5=cbda7c25c5e3ab0bbedc09fbbdac5676","Users of the Twitter microblogging platform share a considerable amount of information through short messages on a daily basis. Some of these so-called tweets discuss issues related to software and could include information that is relevant to the companies developing these applications. Such tweets have the potential to help requirements engineers better understand user needs and therefore provide important information for software evolution. However, little is known about the nature of tweets discussing software-related issues. In this paper, we report on the usage characteristics, content and automatic classification potential of tweets about software applications. Our results are based on an exploratory study in which we used descriptive statistics, content analysis, machine learning and lexical sentiment analysis to explore a dataset of 10,986,495 tweets about 30 different software applications. Our results show that searching for relevant information on software applications within the vast stream of tweets can be compared to looking for a needle in a haystack. However, this relevant information can provide valuable input for software companies and support the continuous evolution of the applications discussed in these tweets. Furthermore, our results show that it is possible to use machine learning and lexical sentiment analysis techniques to automatically extract information about the tweets regarding their relevance, authors and sentiment polarity. © 2017, Springer-Verlag London Ltd.","Content analysis; Requirements engineering; Software evolution; Textmining; User feedback","Artificial intelligence; Data mining; Education; Learning systems; Requirements engineering; Social networking (online); Amount of information; Automatic classification; Content analysis; Descriptive statistics; Micro-blogging platforms; Software Evolution; Text-mining; User feedback; Application programs",2-s2.0-85023757129
"Marszałkowski J., Mokwa D., Drozdowski M., Rusiecki Ł., Narożny H.","Fast algorithms for online construction of web tag clouds",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026256327&doi=10.1016%2fj.engappai.2017.06.023&partnerID=40&md5=6646065893e70bcf10fe6a4292580e62","In this paper tag cloud construction for web exposition is studied. Construction of a tag cloud must simultaneously solve at least three interdisciplinary engineering problems: modeling and controlling graphics aesthetics, solving discrete two-dimensional layout optimization problem, and all these must be done on computationally constrained browser platform. We analyze the design choices in the earlier tag cloud studies and provide a taxonomy of algorithmic approaches to tag cloud building. Then, the design requirements for tag clouds on websites are defined. We propose to quantify tag cloud aesthetics by use of a novel objective function based on the rules of typography. Tag cloud construction is formalized as a combinatorial optimization problem with an irregular objective function. A set of algorithms is proposed and evaluated on a collection of tag sets from popular web pages. The methods that meet constraints of the browser platform are chosen. © 2017 Elsevier Ltd","2D packing; Automatic web page generation; Data visualization; Metaheuristics; Tag clouds; Web engineering","Combinatorial optimization; Constrained optimization; Data visualization; Metadata; Optimization; Websites; 2-D packing; Automatic web page generation; Meta heuristics; Tag clouds; Web engineering; Data mining",2-s2.0-85026256327
"Fouquet T., Cody R.B., Sato H.","Capabilities of the remainders of nominal Kendrick masses and the referenced Kendrick mass defects for copolymer ions",2017,"Journal of Mass Spectrometry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028951805&doi=10.1002%2fjms.3963&partnerID=40&md5=0c1d5dcb8c0642b2def1d4fa021c0248",[No abstract available],,"copolymer; ion; oligomer; data analysis; data mining; Kendrick mass defect analysis; Letter; mass spectrometry; matrix-assisted laser desorption-ionization mass spectrometry; polymerization; priority journal",2-s2.0-85028951805
"Martinho N., Silva L.C., Florindo H.F., Brocchini S., Barata T., Zloh M.","Practical computational toolkits for dendrimers and dendrons structure design",2017,"Journal of Computer-Aided Molecular Design",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029512089&doi=10.1007%2fs10822-017-0041-6&partnerID=40&md5=504c2f3efaea4b29057faa25728302f5","Dendrimers and dendrons offer an excellent platform for developing novel drug delivery systems and medicines. The rational design and further development of these repetitively branched systems are restricted by difficulties in scalable synthesis and structural determination, which can be overcome by judicious use of molecular modelling and molecular simulations. A major difficulty to utilise in silico studies to design dendrimers lies in the laborious generation of their structures. Current modelling tools utilise automated assembly of simpler dendrimers or the inefficient manual assembly of monomer precursors to generate more complicated dendrimer structures. Herein we describe two novel graphical user interface toolkits written in Python that provide an improved degree of automation for rapid assembly of dendrimers and generation of their 2D and 3D structures. Our first toolkit uses the RDkit library, SMILES nomenclature of monomers and SMARTS reaction nomenclature to generate SMILES and mol files of dendrimers without 3D coordinates. These files are used for simple graphical representations and storing their structures in databases. The second toolkit assembles complex topology dendrimers from monomers to construct 3D dendrimer structures to be used as starting points for simulation using existing and widely available software and force fields. Both tools were validated for ease-of-use to prototype dendrimer structure and the second toolkit was especially relevant for dendrimers of high complexity and size. © 2017, Springer International Publishing AG.","3D structure generation; Dendrimer; Dendrons; Linear polymers; Molecular modelling","dendrimer; dendron; drug carrier; monomer; unclassified drug; Article; automation; chemical structure; computer interface; computer model; data base; data mining; disulfide bond; machine learning; macromolecule; mathematical computing; molecular dynamics; nomenclature; priority journal; software",2-s2.0-85029512089
"Yan S., Wong K.-C.","Elucidating high-dimensional cancer hallmark annotation via enriched ontology",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026473374&doi=10.1016%2fj.jbi.2017.07.011&partnerID=40&md5=44beab53717a234d9279a6afaef618a5","Motivation Cancer hallmark annotation is a promising technique that could discover novel knowledge about cancer from the biomedical literature. The automated annotation of cancer hallmarks could reveal relevant cancer transformation processes in the literature or extract the articles that correspond to the cancer hallmark of interest. It acts as a complementary approach that can retrieve knowledge from massive text information, advancing numerous focused studies in cancer research. Nonetheless, the high-dimensional nature of cancer hallmark annotation imposes a unique challenge. Results To address the curse of dimensionality, we compared multiple cancer hallmark annotation methods on 1580 PubMed abstracts. Based on the insights, a novel approach, UDT-RF, which makes use of ontological features is proposed. It expands the feature space via the Medical Subject Headings (MeSH) ontology graph and utilizes novel feature selections for elucidating the high-dimensional cancer hallmark annotation space. To demonstrate its effectiveness, state-of-the-art methods are compared and evaluated by a multitude of performance metrics, revealing the full performance spectrum on the full set of cancer hallmarks. Several case studies are conducted, demonstrating how the proposed approach could reveal novel insights into cancers. Availability https://github.com/cskyan/chmannot © 2017 Elsevier Inc.","Hallmark of cancer; High dimension; Ontology; Text annotation","Ontology; Biomedical literature; Curse of dimensionality; Hallmark of cancer; High dimensions; Medical subject headings; State-of-the-art methods; Text annotations; Transformation process; Diseases; Article; cancer classification; cancer hallmark annotation; cancer research; case study; comparative effectiveness; computer analysis; data extraction; data mining; disease ontology; high throughput sequencing; human; information retrieval; knowledge discovery; malignant transformation; medical informatics; medical information system; Medical Subject Headings; multifactor dimensionality reduction; performance; priority journal",2-s2.0-85026473374
"Riesgo García M.V., Krzemień A., Manzanedo del Campo M.Á., Menéndez Álvarez M., Gent M.R.","Rare earth elements mining investment: It is not all about China",2017,"Resources Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020296564&doi=10.1016%2fj.resourpol.2017.05.004&partnerID=40&md5=20817e1e14963998916490a64b399822","China was in the past the main driver when analysing rare earth prices and their market, since it commercialised around the 90% of the world's supply. After a cut in its exports during 2010 and 2011, rare earth prices dramatically spiked. The world's reaction to this fact was the development of a huge amount of mining projects outside China, many of whom failed when prices fell again. Nevertheless, several of them survived. This paper analyses in first place the future trend of rare earth elements prices in order to contrast the stable tendency forecasted by different providers of price assessments and market data. Secondly, it studies in deep five ready-to-go rare earths mining projects around the world: Nechalacho Project (North-west Territories, Canada); Zandkopsdrift Project (Northern Cape, South Africa); Bear Lodge Project (Wyoming, USA); Kvanefjeld Project (Southern Greenland); and Dubbo Zirconia Project (New South Wales, Australia). The main purposes being to give an “order of magnitude” both technical and economic of this specific mining industry; to provide a tool for investors, potential investors and professional advisers addressing rare earth mining investment analysis; and to facilitate the development of preliminary economic assessments of future rare earth mining projects. These aims will also help to fight against several systemic problems of the rare earth market: lack of trust, market opacity, and short versus long-term approaches and profit orientation. Conclusions clearly show that despite the complexity of rare earth mineral deposits and the fact that their mining operation usually includes different by-products, the evaluation of rare earth mining investments does not present much more difficulty than in the case of single element mining projects. Forecasted prices used in these economic studies are the Achilles’ heel of nowadays rare earth mining investment analysis. Finally, although differences in demand of different rare earth elements will make really difficult to achieve “a priori” a market in balance, the five studied projects are anticipated to cover approximately the third part of the total rare earth consumption in the world. When their dysprosium oxide production was analysed, the resulting proportion for the most critical rare earth element based on its role in clean energy together with its biggest supply risk was almost the same, something optimistic regarding the achievement of a balanced market outside China. © 2017 Elsevier Ltd","Feasibility study; Mining project; Preliminary economic assessment; Price forecasting; Rare earth elements; Scoping study","Commerce; Costs; Dysprosium compounds; Economic analysis; Economics; Heterojunctions; Investments; Mineral resources; Mining; Rare earth elements; Risk assessment; Zirconia; Economic assessments; Feasibility studies; Mining projects; Price forecasting; Scoping; Rare earths; feasibility study; investment; mining industry; price determination; rare earth element; China",2-s2.0-85020296564
"Kaur N., Sood S.K.","A game theoretic approach for an IoT-based automated employee performance evaluation",2017,"IEEE Systems Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030468473&doi=10.1109%2fJSYST.2015.2469102&partnerID=40&md5=9b18dd4fef7c83f29609af74df9e128c","In the present scenario, performance evaluation of employees in industries is done manually, in which there are ample chances of biases. It is observed that manual employee evaluation systems can be efficiently eliminated by using ubiquitous sensing capabilities of Internet of things (IoT) devices to monitor industrial employees. However, none of the authors have used IoT data for automating performance evaluation systems of employees. Hence, this paper proposes a game theoretic approach for an IoT-based employee performance evaluation in industry. The system infers useful results about the performance of employees by mining data collected by the sensory nodes using the MapReduce model. The information hence obtained is then used to draw automated decisions for employees using game theory. The system is analyzed both experimentally and mathematically. The experimental evaluation compares the proposed system with other techniques of data mining and decision making. The results depict that the proposed system evaluates the performance of employees efficiently and shows a performance improvement over other techniques. The mathematical evaluation shows that correct evaluation of employees by the system effectively motivates employees in favor of the industry. Thus, the proposed system effectively and efficiently automates the employee evaluation system and decision-making process in the industry. © 2007-2012 IEEE.","Employees; game theory; industry; Internet of things (IoT); performance evaluation","Data mining; Decision making; Game theory; Industry; Personnel; Personnel rating; Decision making process; Employee performance evaluation; Experimental evaluation; Game-theoretic; Internet of Things (IOT); MapReduce models; performance evaluation; Performance evaluation system; Internet of things",2-s2.0-85030468473
"Soffer T., Kahan T., Livne E.","E-assessment of online academic courses via students' activities and perceptions",2017,"Studies in Educational Evaluation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992179530&doi=10.1016%2fj.stueduc.2016.10.001&partnerID=40&md5=4b5358a8a9a25366ea78e97ee4ce3ed7","The rapid growth of online courses in higher education has led to developments in the field of e-assessment. This paper presents a study, which examined the quality of online academic courses using a multidimensional assessment of students' activities and perceptions, using educational data mining and an online questionnaire. The assessment focused on four aspects: instructional, communication, course workload and overall learning experience. The course instructional model was found well-structured. The video lectures, assignments and materials designed for the online course were the most used and contributing learning resources. However, the number of students who entered the video lectures decreased as the course progressed. Low activity was found in the discussion forums. Students perceived the course workload as low. Overall, the learning experience was high and the students were highly satisfied. These findings provide insights that may assist in improving the quality of future online courses. © 2016 Elsevier Ltd","E-assessment; Educational data mining; Evaluation methods; Evaluation utilization; Online learning; Student evaluation",,2-s2.0-84992179530
"Hou J., Liu W.","Parameter independent clustering based on dominant sets and cluster merging",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017126203&doi=10.1016%2fj.ins.2017.04.006&partnerID=40&md5=deb733ecff00b052cb47b241bead00c4","Clustering is an important unsupervised learning approach with wide application in data mining, pattern recognition and intelligent information processing. However, existing clustering algorithms usually involve one or more user-specified parameters as input and their clustering results depend heavily on these parameters. In order to solve this problem, we present a parameter independent clustering algorithm based on the dominant sets algorithm and cluster merging. In the first step histogram equalization transformation is applied to solve the parameter dependence problem of the dominant sets algorithm. We provide the theoretic foundation of this method and discuss the implementation details. The clustering result is then refined with a cluster merging method, which is based on a new clustering quality evaluation criterion. We use extensive experiments on several datasets to validate each step and the whole procedures of our algorithm. It is shown that our parameter independent algorithm performs comparably to some existing clustering algorithms which benefit from user-specified parameters. © 2017 Elsevier Inc.","Cluster merging; Clustering; Dominant sets; Parameter independent","Data mining; Merging; Parameter estimation; Pattern recognition; Cluster-merging; Clustering; Clustering quality; Dominant set; Histogram equalizations; Intelligent information processing; Parameter dependence; Parameter independent; Clustering algorithms",2-s2.0-85017126203
"Shan W., Chen X.","Improved invasive weed optimization algorithm in sensor deployment for wireless sensor networks",2017,"Boletin Tecnico/Technical Bulletin",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032877112&partnerID=40&md5=85a474c1c8d9d4c2d5234526c2be9d43","The performance of improved invasive weed optimization algorithm (IIWO) and its application for coverage optimization in wireless sensor networks are discussed in this paper. Firstly, on the premise that connectivity among nodes was guaranteed, we established a mathematical model to achieve the coverage of objective area with wireless sensor networks. And this problem was transformed into function optimization based on this algorithm. Then, the invasive weed optimization algorithm was used to search the optimal deployment with the strong search performance. The cubic mapping chaotic operator was introduced to enhance the ability of local search and robustness, and the gauss mutation operator was used to keep the diversity of population. Lastly, the proposed algorithm had been verified through the numerical benchmark functions and coverage simulation. All the results showed that our proposed algorithm had fast convergence speed, nice robustness and strong ability of data mining. Hence, it had the ability to solve the problem of deployment problem in wireless sensor networks.","Cubic chaotic operator; Gauss mutation operator; Invasive weed optimization; Node distribution; Wireless sensor networks","Data mining; Problem solving; Sensor nodes; Wireless sensor networks; Coverage optimizations; Cubic chaotic operator; Diversity of populations; Fast convergence speed; Invasive weed optimization; Invasive Weed Optimization algorithms; Mutation operators; Node distribution; Optimization",2-s2.0-85032877112
"Benali K., Rahal S.A.","OntoDTA: Ontology-Guided Decision Tree Assistance",2017,"Journal of Information and Knowledge Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025438636&doi=10.1142%2fS0219649217500319&partnerID=40&md5=8f726da704119bde31c5ea8b39e275bb","The effective application of a Decision Tree (DT) process is beset with many difficult and technical decisions about the choice of algorithms, parameters, evaluation, etc. Therefore, we propose assistance by using ontologies for addressing the above-mentioned challenges that face the non-specialist DT miner (person). Ontologies have been used in various research areas such as computer science, including data mining tools. In this paper, we propose the realisation of a domain ontology for DT OntoDTA to empower the non-specialist DT miner throughout the key phases of the DT process. OntoDTA ontology contains the knowledge of DT and provides a common terminology that can be shared and processed by DT miners. © 2017 World Scientific Publishing Co.","Assistance; data mining; decision tree miner and ontology; decision trees",,2-s2.0-85025438636
"Douglas S., Walker A.","COAL MINING AND THE RESOURCE CURSE IN THE EASTERN UNITED STATES",2017,"Journal of Regional Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996867975&doi=10.1111%2fjors.12310&partnerID=40&md5=621cece03344f802c492cb1c63454e23","We measure the effect of resource-sector dependence on long-run income growth using the natural experiment of coal mining in 409 Appalachian counties selected for homogeneity. Using a panel data set (1970–2010), we find a one standard deviation increase in resource dependence is associated with 0.5–1 percentage point long-run and a 0.2 percentage point short-run decline in the annual growth rate of per capita personal income. We also measure the extent to which the resource curse operates through disincentives to education, and find significant effects, but this “education channel” explains less than 15 percent of the apparent curse. © 2016 Wiley Periodicals, Inc.",,"coal mining; growth rate; income; panel data; United States",2-s2.0-84996867975
"Yilmaz Eroglu D., Kilic K.","A novel Hybrid Genetic Local Search Algorithm for feature selection and weighting with an application in strategic decision making in innovation management",2017,"Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017515021&doi=10.1016%2fj.ins.2017.04.009&partnerID=40&md5=1079ac4ad646b630b980abd5f89091b3","In some applications, one needs not only to determine the relevant features but also provide a preferential ordering among the set of relevant features by weights. This paper presents a novel Hybrid Genetic Local Search Algorithm (HGA) in combination with the k-nearest neighbor classifier for simultaneous feature subset selection and feature weighting, particularly for medium-sized data sets. The performance of the proposed algorithm is compared with the performance of alternative feature subset selection algorithms and classifiers through experimental analyses in the various benchmark data sets publicly available on the UCI database. The developed HGA is then applied to a data set gathered from 184 manufacturing firms in the context of innovation management. The data set consists of scores of manufacturing firms in terms of various factors that are known to influence the innovation performance of manufacturing firms and referred to as innovation determinants, and their innovation performances. HGA is used to determine the relative significance of the innovation determinants. Our results demonstrated that the developed HGA is capable of eliminating the irrelevant features and successfully assess feature weights. Moreover, our work is an example how data mining can play a role in the context of strategic management decision making. © 2017 Elsevier Inc.","Data mining; Feature subset selection; Feature weighting; Hybrid genetic local search algorithm; Innovation management; Strategic decision support","Benchmarking; Data mining; Decision making; Decision support systems; Feature extraction; Innovation; Learning algorithms; Local search (optimization); Management science; Manufacture; Nearest neighbor search; Feature subset selection; Feature weighting; Hybrid genetic; Innovation management; Strategic decisions; Classification (of information)",2-s2.0-85017515021
"Briki M., Zhu Y., Gao Y., Shao M., Ding H., Ji H.","Distribution and health risk assessment to heavy metals near smelting and mining areas of Hezhang, China",2017,"Environmental Monitoring and Assessment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027868236&doi=10.1007%2fs10661-017-6153-6&partnerID=40&md5=7217ebef5483b2dc54c2d003f10b3b10","Mining and smelting areas in Hezhang have generated a large amount of heavy metals into the environment. For that cause, an evaluative study on human exposure to heavy metals including Co, Ni, Cu, Zn, Cr, As, Cd, Pb, Sb, Bi, Be, and Hg in hair and urine was conducted for their concentrations and correlations. Daily exposure and non-carcinogenic and carcinogenic risk were estimated. Sixty-eight scalp hair and 66 urine samples were taken from participants of different ages (6–17, 18–40, 41–60, and ≥ 65 years) living in the vicinity of an agricultural soil near mine and smelting areas. The results compared to the earlier studies showed an elevated concentration of Pb, Be, Bi, Co, Cr, Ni, Sb, and Zn in hair and urine. These heavy metals were more elevated in mining than in smelting. Considering gender differences, females were likely to be more affected than male. By investigating age differences in this area, high heavy metal concentrations in male’s hair and urine existed in age of 18–40 and ≥ 66, respectively. However, females did not present homogeneous age distribution. Hair and urine showed a different distribution of heavy metals in different age and gender. In some cases, significant correlation was found between heavy metals in hair and urine (P > 0.05 and P > 0.01) in mining area. The estimated average daily intake of heavy metals in vegetables showed a great contribution compared to the soil and water. Non-carcinogenic and carcinogenic risk values of total pathways in mining and smelting areas were higher than 1 and exceeded the acceptable levels. Thus, the obtained data might be useful for further studies. They can serve as a basis of comparison and assessing the effect of simultaneous exposure from heavy metals in mining and smelting areas, and potential health risks from exposure to heavy metals in vegetables need more consideration. © 2017, Springer International Publishing AG.","ADD; Age; Gender; Heavy metals; Risk assessment; Scalp hair; Urine","Body fluids; Cadmium; Health risks; Heavy metals; Lead; Nickel; Pollution; Risk assessment; Risk perception; Smelting; Vegetables; Zinc; Agricultural soils; Different distributions; Elevated concentrations; Gender; Heavy metal concentration; Potential health risks; Scalp hair; Urine; Copper; heavy metal; mercury; soil; soil pollutant; agriculture; analysis; chemistry; China; environmental exposure; environmental monitoring; female; food contamination; hair; human; male; mining; procedures; risk assessment; soil; soil pollutant; statistics and numerical data; vegetable; Agriculture; China; Environmental Exposure; Environmental Monitoring; Female; Food Contamination; Hair; Humans; Male; Mercury; Metals, Heavy; Mining; Risk Assessment; Soil; Soil Pollutants; Vegetables",2-s2.0-85027868236
"Gowanlock M., Blair D.M., Pankratius V.","Optimizing Parallel Clustering Throughput in Shared Memory",2017,"IEEE Transactions on Parallel and Distributed Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029533937&doi=10.1109%2fTPDS.2017.2675421&partnerID=40&md5=c340290316c3fc915ecc173d7bee3b89","This article studies the optimization of parallel clustering throughput in the context of variant-based parallelism, which exploits commonalities and reuse among variant computations for multithreading scalability. This direction is motivated by challenging scientific applications where scientists have to execute multiple runs of clustering algorithms with different parameters to determine which ones best explain phenomena observed in empirical data. To make this process more efficient, we propose a novel set of optimizations to maximize the throughput of Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a frequently used algorithm for scientific data mining in astronomy, geoscience, and many other fields. Our approach executes multiple algorithm variants in parallel, computes clusters concurrently, and leverages heuristics to maximize the reuse of results from completed variants. As scientific datasets continue to grow, maximizing clustering throughput with our techniques may accelerate the search and identification of natural phenomena of interest with computational support, i.e., Computer-Aided Discovery. We present evaluations on a whole spectrum of datasets, such as geoscience data on space weather phenomena, astronomical data from the Sloan Digital Sky Survey on intermediate-redshift galaxies, as well as synthetic datasets to characterize performance properties. Selected results show a 1,115 percent performance improvement due to indexing tailored for variant-based clustering, and a 2,209 percent performance improvement when applying all of our proposed optimizations. © 1990-2012 IEEE.","clustering throughput; computer-aided discovery; data mining; DBSCAN; Parallel clustering",,2-s2.0-85029533937
"Geva A., Gronsbell J.L., Cai T., Cai T., Murphy S.N., Lyons J.C., Heinz M.M., Natter M.D., Patibandla N., Bickel J., Mullen M.P., Mandl K.D., Abman S., Adatia I., Austin E.D., Feinstein J., Fineman J., Hanna B., Hopper R., Ivy D., Keller R., Krishnan U., Kulik T., Mullen M., Raj U., Rosenzweig E.B.","A Computable Phenotype Improves Cohort Ascertainment in a Pediatric Pulmonary Hypertension Registry",2017,"Journal of Pediatrics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020761881&doi=10.1016%2fj.jpeds.2017.05.037&partnerID=40&md5=26f75a59f992e11d0f9ad086841e5878","Objectives To compare registry and electronic health record (EHR) data mining approaches for cohort ascertainment in patients with pediatric pulmonary hypertension (PH) in an effort to overcome some of the limitations of registry enrollment alone in identifying patients with particular disease phenotypes. Study design This study was a single-center retrospective analysis of EHR and registry data at Boston Children's Hospital. The local Informatics for Integrating Biology and the Bedside (i2b2) data warehouse was queried for billing codes, prescriptions, and narrative data related to pediatric PH. Computable phenotype algorithms were developed by fitting penalized logistic regression models to a physician-annotated training set. Algorithms were applied to a candidate patient cohort, and performance was evaluated using a separate set of 136 records and 179 registry patients. We compared clinical and demographic characteristics of patients identified by computable phenotype and the registry. Results The computable phenotype had an area under the receiver operating characteristics curve of 90% (95% CI, 85%-95%), a positive predictive value of 85% (95% CI, 77%-93%), and identified 413 patients (an additional 231%) with pediatric PH who were not enrolled in the registry. Patients identified by the computable phenotype were clinically distinct from registry patients, with a greater prevalence of diagnoses related to perinatal distress and left heart disease. Conclusions Mining of EHRs using computable phenotypes identified a large cohort of patients not recruited using a classic registry. Fusion of EHR and registry data can improve cohort ascertainment for the study of rare diseases. Trial registration ClinicalTrials.gov: NCT02249923. © 2017 Elsevier Inc.","bioinformatics; computer-based model; pediatrics; pulmonary hypertension; registry","Article; child; childhood disease; cohort analysis; data mining; electronic health record; heart disease; human; learning algorithm; major clinical study; Massachusetts; natural language processing; neonatal respiratory distress syndrome; predictive value; prevalence; priority journal; pulmonary hypertension; register; retrospective study; school child; algorithm; Hypertension, Pulmonary; phenotype; sensitivity and specificity; United States; Algorithms; Child; Data Mining; Electronic Health Records; Humans; Hypertension, Pulmonary; Phenotype; Predictive Value of Tests; Registries; Retrospective Studies; Sensitivity and Specificity; United States",2-s2.0-85020761881
"Szelag B., Bartkiewicz L., Studziński J., Barbusiński K.","Evaluation of the impact of explanatory variables on the accuracy of prediction of daily inflow to the sewage treatment plant by selected models nonlinear",2017,"Archives of Environmental Protection",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029605346&doi=10.1515%2faep-2017-0030&partnerID=40&md5=020da1b3bafa7a092a29b31c9e4faf84","The aim of the study was to evaluate the possibility of applying different methods of data mining to model the inflow of sewage into the municipal sewage treatment plant. Prediction models were elaborated using methods of support vector machines (SVM), random forests (RF), k-nearest neighbour (k-NN) and of Kernel regression (K). Data consisted of the time series of daily rainfalls, water level measurements in the clarified sewage recipient and the wastewater inflow into the Rzeszow city plant. Results indicate that the best models with one input delayed by 1 day were obtained using the k-NN method while the worst with the K method. For the models with two input variables and one explanatory one the smallest errors were obtained if model inputs were sewage inflow and rainfall data delayed by 1 day and the best fit is provided using RF method while the worst with the K method. In the case of models with three inputs and two explanatory variables, the best results were reported for the SVM and the worst for the K method. In the most of the modelling runs the smallest prediction errors are obtained using the SVM method and the biggest ones with the K method. In the case of the simplest model with one input delayed by 1 day the best results are provided using k-NN method and by the models with two inputs in two modelling runs the RF method appeared as the best. © 2017 Archives of Environmental Protection.","Data Mining; forecasting inflow; k-nearest neighbour; Kernel regression; Random forest; wastewater treatment plant",,2-s2.0-85029605346
"Shastry K.A., Sanjay H.A., Deexith G.","Quadratic-radial-basis-function-kernel for classifying multi-class agricultural datasets with continuous attributes",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018391133&doi=10.1016%2fj.asoc.2017.04.049&partnerID=40&md5=a3495c5b1e9c8b4def1a34a869d5750b","Classification of agricultural data such as soil data and crop data is significant as it allows the stakeholders to make meaningful decisions for farming. Soil classification aids farmers in deciding the type of crop to be sown for a particular type of soil. Similarly, wheat variety classification assists in selecting the right type of wheat for a particular product. Current methods used for classifying agricultural data are mostly manual. These methods involve agriculture field visits and surveys and are labor-intensive, expensive, and prone to human error. Recently, data mining techniques such as decision trees, k-nearest neighbors (k-NN), support vector machine (SVM), and Naive Bayes (NB) have been used in classification of agricultural data such as soil, crops, and land cover. The resulting classification aid the decision making process of government organizations and agro-industries in the field of agriculture. SVM is a popular approach for data classification. A recent study on SVM highlighted the fact that using multiple kernels instead of a single kernel would lead to better performance because of the greater learning and generalization power. In this work, a hybrid kernel based support vector machine (H-SVM) is proposed for classifying multi-class agricultural datasets having continuous attributes. Genetic algorithm (GA) or gradient descent (GD) methods are utilized to select the SVM parameters C and γ. The proposed kernel is called the quadratic-radial-basis-function kernel (QRK) and it combines both quadratic and radial basis function (RBF) kernels. The proposed classifier has the ability to classify all kinds of multi-class agricultural datasets with continuous features. Rigorous experiments using the proposed method are performed on standard benchmark and real world agriculture datasets. The results reveal a significant performance improvement over state of the art methods such as NB, k-NN, and SVM in terms of performance metrics such as accuracy, sensitivity, specificity, precision, and F-score. © 2017 Elsevier B.V.","Agriculture; Classification; Hybrid; Kernels; SVM","Agricultural machinery; Agriculture; Crops; Data mining; Decision making; Decision trees; Functions; Genetic algorithms; Nearest neighbor search; Radial basis function networks; Soils; Support vector machines; Trees (mathematics); Continuous attribute; Decision making process; Government organizations; Hybrid; Kernels; Radial basis function kernels; Radial Basis Function(RBF); State-of-the-art methods; Classification (of information)",2-s2.0-85018391133
"Khammassi C., Krichen S.","A GA-LR wrapper approach for feature selection in network intrusion detection",2017,"Computers and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021106059&doi=10.1016%2fj.cose.2017.06.005&partnerID=40&md5=b6fd1bb689560f04290680f53ad16803","Intrusions constitute one of the main issues in computer network security. Through malicious actions, hackers can have unauthorised access that compromises the integrity, the confidentiality, and the availability of resources or services. Intrusion detection systems (IDSs) have been developed to monitor and filter network activities by identifying attacks and alerting network administrators. Different IDS approaches have emerged using data mining, machine learning, statistical analysis, and artificial intelligence techniques such as genetic algorithms, artificial neural networks, fuzzy logic, swarm intelligence, etc. Due to the high dimensionality of the exchanged data, applying those techniques will be extremely time consuming. Feature selection is needed to select the optimal subset of features that represents the entire dataset to increase the accuracy and the classification performance of the IDS. In this work, we apply a wrapper approach based on a genetic algorithm as a search strategy and logistic regression as a learning algorithm for network intrusion detection systems to select the best subset of features. The experiment will be conducted on the KDD99 dataset and the UNSW-NB15 dataset. Three different decision tree classifiers are used to measure the performance of the selected subsets of features. The obtained results are compared with other feature selection approaches to verify the efficiency of our proposed approach. © 2017 Elsevier Ltd","Anomaly detection; Classification; Decision tree; Feature selection; Genetic algorithm; Intrusion detection systems; KDD99; Logistic regression; UNSW-NB15; Wrapper approach","Artificial intelligence; Classification (of information); Computer crime; Data mining; Decision trees; Education; Feature extraction; Filtration; Fuzzy logic; Fuzzy neural networks; Genetic algorithms; Learning algorithms; Mercury (metal); Network security; Neural networks; Personal computing; Regression analysis; Swarm intelligence; Trees (mathematics); Anomaly detection; Intrusion Detection Systems; KDD99; Logistic regressions; UNSW-NB15; Wrapper approach; Intrusion detection",2-s2.0-85021106059
"Castro-Hernandez D., Paranjape R.","Classification of User Trajectories in LTE HetNets Using Unsupervised Shapelets and Multiresolution Wavelet Decomposition",2017,"IEEE Transactions on Vehicular Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029950392&doi=10.1109%2fTVT.2017.2679076&partnerID=40&md5=f46b49300fc390c64e9a886c3c46466b","The classification of user trajectories in Long-Term Evolution (LTE) heterogeneous networks (HetNets) is investigated in this paper. We propose a methodology to classify user trajectories based on the measurement reports submitted to the serving base station as part of the handover process; we propose to consider each measurement report as a time series. This methodology allows base stations to automatically and autonomously discover the radio-frequency (RF) conditions of their cell edge (e.g., signal strength degradation and interference levels). We propose the application of machine learning and data mining techniques to identify patterns in the reference signal received power measurement reports submitted by users as they approach the edge of the service area. Our time-series clustering algorithm based on unsupervised shapelets and multiresolution wavelet decomposition provided superior performance compared to a discrete Fourier transform (DFT)-based clustering algorithm. Our algorithm was able to provide clustering results with an average accuracy of 95%. Furthermore, the quality measure of the resulting clusters was up to 75% better, compared to the clustering results provided by the DFT-based algorithm. We also proposed a novel methodology to calculate a suitable number of clusters with no prior knowledge regarding the data; an average accuracy close to 90% was achieved. © 2017 IEEE.","Clustering; handover; heterogeneous networks (HetNets); mobility; self-optimizing","Base stations; Carrier mobility; Data mining; Discrete Fourier transforms; Fourier series; Heterogeneous networks; Learning systems; Long Term Evolution (LTE); Mobile telecommunication systems; Time series; Trajectories; Wavelet decomposition; Wireless telecommunication systems; Clustering; Clustering results; Handover; Heterogeneous network (HetNets); Measurement reports; Multiresolution wavelet decompositions; Self-optimizing; Time series clustering; Clustering algorithms",2-s2.0-85029950392
"Grover P., Kar A.K.","Big Data Analytics: A Review on Theoretical Contributions and Tools Used in Literature",2017,"Global Journal of Flexible Systems Management",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022085662&doi=10.1007%2fs40171-017-0159-3&partnerID=40&md5=c7d6773137db1f3352033e038ce86e81","The importance of data science and big data analytics is growing very fast as organizations are gearing up to leverage their information assets to gain competitive advantage. The flexibility offered through big data analytics empowers functional as well as firm-level performance. In the first phase of the study, we attempt to analyze the research on big data published in high-quality business management journals. The analysis was visualized using tools for big data and text mining to understand the dominant themes and how they are connected. Subsequently, an industry-specific categorization of the studies was done to understand the key use cases. It was found that most of the existing research focuses majorly on consumer discretionary, followed by public administration. Methodologically, a major focus in such exploration is in social media analytics, text mining and machine learning applications for meeting objectives in marketing and supply chain management. However, it was found that not much focus was highlighted in these studies to demonstrate the tools used for the analysis. To address this gap, this study also discusses the evolution, types and usage of big data tools. The brief overview of big data technologies grouped by the services they enable and some of their applications are presented. The study categorizes these tools into big data analysis platforms, databases and data warehouses, programming languages, search tools, and data aggregation and transfer tools. Finally, based on the review, future directions for exploration in big data has been provided for academic and practice. © 2017, Global Institute of Flexible Systems Management.","Big data; Big data technologies; Data science; Information management; Systematic literature review",,2-s2.0-85022085662
"Wei J., Zhang R., Yu Z., Hu R., Tang J., Gui C., Yuan Y.","A BPSO-SVM algorithm based on memory renewal and enhanced mutation mechanisms for feature selection",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018865058&doi=10.1016%2fj.asoc.2017.04.061&partnerID=40&md5=9e95e2802e57aa1b6f23c12b87c1b2c2","Feature selection (FS) is an essential component of data mining and machine learning. Most researchers devoted to get more effective method with high accuracy and fewer features, it has become one of the most challenging problems in FS. Certainly, some algorithms have been proven to be effectively, such as binary particle swarm optimization (BPSO), genetic algorithm (GA) and support vector machine (SVM). BPSO is a metaheuristic algorithm having been widely applied to various fields and applications successfully, including FS. As a wrapper method of FS, BPSO-SVM tends to be trapped into premature easily. In this paper, we present a novel mutation enhanced BPSO-SVM algorithm by adjusting the memory of local and global optimum (LGO) and increasing the particles’ mutation probability for feature selection to overcome convergence premature problem and achieve high quality features. Typical simulated experimental results carried out on Sonar, LSVT and DLBCL datasets indicated that the proposed algorithm improved the accuracy and decreased the number of feature subsets, comparing with existing modified BPSO algorithms and GA. © 2017 Elsevier B.V.","Feature selection; Mutation enhanced BPSO; SVM","Data mining; Genetic algorithms; Learning systems; Optimization; Particle swarm optimization (PSO); Support vector machines; Binary particle swarm optimization; Meta heuristic algorithm; Mutation enhanced BPSO; Mutation mechanism; Mutation probability; Novel mutations; Premature problem; Wrapper methods; Feature extraction",2-s2.0-85018865058
"Li H., Yang X., Skitmore M., Wang F., Forsythe P.","Automated classification of construction site hazard zones by crowd-sourced integrated density maps",2017,"Automation in Construction",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017356753&doi=10.1016%2fj.autcon.2017.04.007&partnerID=40&md5=2a2fb12fc306b25217b93e3701c061d8","Current onsite safety management always relies on time-consuming predefinitions of hazardous zones based on the managers' personal capabilities. However, in a typical labor-intensive industry such as construction, the workers themselves can provide a wealth of information for hazard identification. Historical accident-free working locations on site provide a valuable means of recognizing safe workplaces. This paper presents an approach to the automated classification of construction site zones derived from the location tracks of workers collected from a real-time location system (RTLS). Through data mining, filtering and analysis, the location tracks are transformed into grid density maps and continuous density maps. These illustrate the characteristics of spatial-temporal activities onsite as well as providing a visual representation of the distribution of safe and hazardous individual workplaces. A personnel hazard map is generated automatically based on historical accident-free location tracks from a field project using the proposed approach. Compared with the actual workplaces in terms of accuracy, precision, sensitivity and specificity, the evaluation result reveals that the hazardous areas on a construction site can be automatically classified to improve the workplace management of individual workers. The contributions of this research include an automated zone classification algorithm and an evaluation framework consisting of four indicators for hazard awareness onsite. © 2017 Elsevier B.V.","Automated; Density maps; Hazard identification; Zone classification","Accidents; Automation; Hazardous materials; Hazards; Human resource management; Location; Automated; Automated classification; Density maps; Hazard identification; Real time location systems; Sensitivity and specificity; Visual representations; Zone classifications; Data mining",2-s2.0-85017356753
"Thakar P., Mehta A., Manisha","A unified model of clustering and classification to improve students' employability prediction",2017,"International Journal of Intelligent Systems and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028536637&doi=10.5815%2fijisa.2017.09.02&partnerID=40&md5=9960dd55bd97c05a64380ac1108d9f6d","Data Mining is gaining immense popularity in the field of education due to its predictive capabilities. But, most of the prior effort in this area is only directed towards prediction of performance in academic results only. Nowadays, education has become employment oriented. Very little attempt is made to predict students' employability. Precise prediction of students' performance in campus placements at an early stage can identify students, who are at the risk of unemployment and proactive actions can be taken to improve their performance. Existing researches on students' employability prediction are either based upon only one type of course or on single University/Institute; thus is not scalable from one context to another. With this necessity, the conception of a unified model of clustering and classification is proposed in this paper. With the notion of unification, data of professional courses namely Engineering and Masters in Computer Applications students are collected from various universities and institutions pan India. Data is large, multivariate, incomplete, heterogeneous and unbalanced in nature. To deal with such a data, a unified predictive model is built by integrating clustering and classification techniques. Two- Level clustering (k-means kernel) with chi-square analysis is applied at the pre-processing stage for the automated selection of relevant attributes and then ensemble vote classification technique with a combination of four classifiers namely k-star, random tree, simple cart and the random forest is applied to predict students' employability. Proposed framework provides a generalized solution for student employability prediction. Comparative results clearly depict model performance over various classification techniques. Also, when the proposed model is applied up to the level of the state, classification accuracy touches 96.78% and 0.937 kappa value. © 2017 MECS.","Classification; Clustering; Data mining; Education; Employability; Prediction",,2-s2.0-85028536637
"Louridas M., Szasz P., Fecso A.B., Zywiel M.G., Lak P., Bener A.B., Harris K.A., Grantcharov T.P.","Practice does not always make perfect: need for selection curricula in modern surgical training",2017,"Surgical Endoscopy and Other Interventional Techniques",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018296152&doi=10.1007%2fs00464-017-5572-3&partnerID=40&md5=4aff2fe4242d7ae4c179872a7b146d89","Background: It is hypothesized that not all surgical trainees are able to reach technical competence despite ongoing practice. The objectives of the study were to assess a trainees’ ability to reach technical competence by assessing learning patterns of the acquisition of surgical skills. Furthermore, it aims to determine whether individuals’ learning patterns were consistent across a range of open and laparoscopic tasks of variable difficulty. Methods: Sixty-five preclinical medical students participated in a training curriculum with standardized feedback over forty repetitions of the following laparoscopic and open technical tasks: peg transfer (PT), circle cutting (CC), intracorporeal knot tie (IKT), one-handed tie, and simulated laparotomy closure. Data mining techniques were used to analyze the prospectively collected data and stratify the students into four learning clusters. Performance was compared between groups, and learning curve characteristics unique to trainees who have difficulty reaching technical competence were quantified. Results: Top performers (22–35%) and high performers (32–42%) reached proficiency in all tasks. Moderate performers (25–37%) reached proficiency for all open tasks but not all laparoscopic tasks. Low performers (8–15%) failed to reach proficiency in four of five tasks including all laparoscopic tasks (PT 7.8%; CC 9.4%; IKT 15.6%). Participants in lower performance clusters demonstrated sustained performance disadvantage across tasks, with widely variable learning curves and no evidence of progression towards a plateau phase. Conclusions: Most students reached proficiency across a range of surgical tasks, but low-performing trainees failed to reach competence in laparoscopic tasks. With increasing use of laparoscopy in surgical practice, screening potential candidates to identify the lowest performers may be beneficial. © 2017, Springer Science+Business Media New York.","Competence; Learning curves; Selection; Simulation training; Surgical trainees; Technical skills","adult; Article; clinical practice; comparative study; controlled study; data mining; demography; feedback system; female; human; laparoscopic surgery; learning; learning curve; male; medical education; medical student; priority journal; surgical training; young adult",2-s2.0-85018296152
"Chen X., Guo J., Tian K., Fan C., Pan X.","A study on the influence propagation model in topic attention networks",2017,"International Journal of Performability Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029391802&doi=10.23940%2fijpe.17.05.p15.721730&partnerID=40&md5=165c440ff6eecb664586ee921298bdc7","The social networks with the complex user relations and huge amount of data and hidden information, bring new opportunities and challenges for the study of information diffusion and influence maximization. In recent years, there are more and more researches on the influence maximization of topic preference. However, most of the existing researches only take the topic as an attribute of the users, and the importance of the topic in network structure is not considered. In view of this situation, firstly, this paper constructed a new topic attention network model fusing the social relation and the topic preference. Secondly, based on connected degree of set pair and Markov random walk model, we propose the calculated method of the topic preference for users, and then mining the seed set with influence by the greedy strategy. Thirdly, we propose the calculated method of the activation probability of the user based on the user relation and the topic preference, and propose the influence maximization algorithm TAN-CELF in topic attention networks. Finally, on Dou-ban network dataset, from three metrics ISST, ISRT and ISRNT, compare with algorithm L-GAUP and CELF, the experimental results show that algorithm TAN-CELF that is proposed by this paper has a higher performance on influence scope. © 2017 Totem Publisher, Inc. All rights reserved.","Connected degree; Influence maximization; Information diffusion; Markov random walk; Topic attention networks; Topic preference","Random processes; Connected degree; Influence maximizations; Information diffusion; Random Walk; Topic preferences; Data mining",2-s2.0-85029391802
"Kontogiannis T., Malakis S., McDonald N.","Integrating operational and risk information with system risk models in air traffic control",2017,"Cognition, Technology and Work",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018790464&doi=10.1007%2fs10111-017-0409-3&partnerID=40&md5=79ea4d441120c242419bc0be5f2dcd6b","The next generation Air Transport Management (ATM) requires performance-based safety management that is fully integrated into seamless operational management. To realize this objective, a framework is proposed in this article that structures risk information on ATM consoles in terms of four design principles: (i) integrating operational and safety-related data, (ii) managing interactions and minimizing transfer of risks to other sectors, (iii) generating risk patterns from accumulated experience and (iv) managing resources and safety barriers. The framework is applied to an approach control unit of a regional airport for the design of a performance dashboard that is linked to a safety module for managing hazards and safety barriers. In this sense, safety management is not an additional task to normal operations since risk information is integrated with operational data on a daily basis. Furthermore, the safety module combines traditional elements of bow-ties with new safety indices such as complexity metrics and risk patterns. © 2017, Springer-Verlag London.","Air Transport Management (ATM); Bow-ties; Complexity metrics; Data mining; Performance dashboard; Risk patterns; Safety management","Advanced traffic management systems; Air traffic control; Civil aviation; Computational complexity; Data mining; Risk assessment; Traffic control; Air transport management; Bow tie; Complexity metrics; Performance dashboard; Safety management; Safety engineering",2-s2.0-85018790464
"Li Y., Kwortnik R.","Categorizing Cruise Lines by Passenger Perceived Experience",2017,"Journal of Travel Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026789750&doi=10.1177%2f0047287516674602&partnerID=40&md5=50dea6b66af24f29230fac10df1dc5c3","In the travel and hospitality industries, categorization of products, brands, and experiences permits efficient comparison and evaluation that aids decision making—from consumer choice to organizational strategy. However, categories often involve self-categorization (e.g., marketer defined) that may not reflect the reality of dynamic markets and industries. In this study of the cruise industry, we derive a new categorization approach using consumer perceptions of their cruise experiences to challenge a long-standing industry typology. Results using a variety of statistical tests of J.D. Power data from more than 3,000 cruisers yield a new and more informative category structure and assignment of cruise lines to it. Analyses reveal differences between the new cruise categories in terms of determinants that influence customer choice. Discussion highlights benefits to travel practitioners of using dynamic and customer-based categories, as well as to researchers of applying advanced statistical techniques to expose unexpected and insightful patterns in secondary data. © 2016, © The Author(s) 2016.","categorization; cruise industry; customer experience; data mining; marketing","data mining; decision making; perception; statistical analysis; travel behavior; Vindula arsinoe",2-s2.0-85026789750
"Amiri L., Khazaei M., Ganjali M.","General location model with factor analyzer covariance matrix structure and its applications",2017,"Advances in Data Analysis and Classification",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973171254&doi=10.1007%2fs11634-016-0258-6&partnerID=40&md5=301d518c3f89147adbc008501891ea8b","General location model (GLOM) is a well-known model for analyzing mixed data. In GLOM one decomposes the joint distribution of variables into conditional distribution of continuous variables given categorical outcomes and marginal distribution of categorical variables. The first version of GLOM assumes that the covariance matrices of continuous multivariate distributions across cells, which are obtained by different combination of categorical variables, are equal. In this paper, the GLOMs are considered in both cases of equality and unequality of these covariance matrices. Three covariance structures are used across cells: the same factor analyzer, factor analyzer with unequal specific variances matrices (in the general and parsimonious forms) and factor analyzers with common factor loadings. These structures are used for both modeling covariance structure and for reducing the number of parameters. The maximum likelihood estimates of parameters are computed via the EM algorithm. As an application for these models, we investigate the classification of continuous variables within cells. Based on these models, the classification is done for usual as well as for high dimensional data sets. Finally, for showing the applicability of the proposed models for classification, results from analyzing three real data sets are presented. © 2016, Springer-Verlag Berlin Heidelberg.","Classification; Common factor loadings; Factor analyzer; General location model; The EM algorithm","Algorithms; Classification (of information); Clustering algorithms; Data mining; Location; Matrix algebra; Maximum likelihood; Maximum likelihood estimation; Principal component analysis; Categorical variables; Common factors; Conditional distribution; EM algorithms; Factor analyzers; Location modeling; Maximum likelihood estimate; Multivariate distributions; Covariance matrix",2-s2.0-84973171254
"González-Robledo J., Martín-González F., Sánchez-Barba M., Sánchez-Hernández F., Moreno-García M.N.","Multiclassifier Systems for Predicting Neurological Outcome of Patients with Severe Trauma and Polytrauma in Intensive Care Units",2017,"Journal of Medical Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026349769&doi=10.1007%2fs10916-017-0789-1&partnerID=40&md5=fa81847735d3a94a317fefd65c8527c1","This paper presents an ensemble based classification proposal for predicting neurological outcome of severely traumatized patients. The study comprises both the whole group of patients and a subgroup containing those patients suffering traumatic brain injury (TBI). Data was gathered from patients hospitalized in the Intensive Care Unit (ICU) of the University Hospital in Salamanca. Predictive models were induced from both epidemiologic and clinical variables taken at the emergency room and along the stay in the ICU. The large number of variables leads to a low accuracy in the classifiers even when feature selection methods are used. In addition, the presence of a much larger number of instances of one of the classes in the subgroup of TBI patients produces a significantly lesser precision for the minority class. Usual ways of dealing with the last problem is to use undersampling and oversampling strategies, which can lead to the loss of valuable data and overfitting problems respectively. Our proposal for dealing with these problems is based in the use of ensemble multiclassifiers as well as in the use of an ensemble playing the role of base classifier in multiclassifiers. The proposed strategy gave the best values of the selected quality measures (accuracy, precision, sensitivity, specificity, F-measure and area under the Receiver Operator Characteristic curve) as well as the closest values of precision for the two classes under study in the case of the classification from imbalanced data. © 2017, Springer Science+Business Media, LLC.","Data mining; Ensemble classifiers; Mortality; Multiclassifiers; Polytrauma; Severe trauma","Article; classifier; clinical feature; emergency ward; hospital patient; hospitalization; human; injury severity; major clinical study; multiple trauma; neurological intensive care unit; traumatic brain injury; university hospital",2-s2.0-85026349769
"Rafiei-Asl J., Nickabadi A.","TSAKE: A topical and structural automatic keyphrase extractor",2017,"Applied Soft Computing Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019957246&doi=10.1016%2fj.asoc.2017.05.014&partnerID=40&md5=67e31fa3ca198354deb3b3e8b3585efb","The keyphrases of a text entity are a set of words or phrases that concisely describe the main content of that text. Automatic keyphrase extraction plays an important role in natural language processing and information retrieval tasks such as text summarization, text categorization, full-text indexing, and cross-lingual text reuse. However, automatic keyphrase extraction is still a complicated task and the performance of the current keyphrase extraction methods is low. Automatic discovery of high-quality and meaningful keyphrases requires the application of useful information and suitable mining techniques. This paper proposes Topical and Structural Keyphrase Extractor (TSAKE) for the task of automatic keyphrase extraction. TSAKE combines the prior knowledge about the input langue learned by an N-gram topical model (TNG) with the co-occurrence graph of the input text to form some topical graphs. Different from most of the recent keyphrase extraction models, TSAKE uses the topic model to weight the edges instead of the nodes of the co-occurrence graph. Moreover, while TNG represents the general topics of the language, TSAKE applies network analysis techniques to each topical graph to detect finer grained sub-topics and extract more important words of each sub-topic. The use of these informative words in the ranking process of the candidate keyphrases improves the quality of the final keyphrases proposed by TSAKE. The results of our experiment studies conducted on three manually annotated datasets show the superiority of the proposed model over three baseline techniques and six state-of-the-art models. © 2017 Elsevier B.V.","Co-occurrence graph; Community detection; Keyphrase extraction; Topic model","Extraction; Indexing (materials working); Natural language processing systems; Text processing; Analysis techniques; Automatic discovery; Co-occurrence Graph; Community detection; Keyphrase extraction; NAtural language processing; Text categorization; Topic Modeling; Data mining",2-s2.0-85019957246
"Weißbach T., Golzmann A., Bennink S., Pradel G., Julius Ngwa C.","Transcript and protein expression analysis of proteases in the blood stages of Plasmodium falciparum",2017,"Experimental Parasitology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017169035&doi=10.1016%2fj.exppara.2017.03.006&partnerID=40&md5=feab6f89c34c93995e0f15cdfaf0c607","Proteases are crucial enzymes with varying roles in living organisms. In the malaria parasite Plasmodium falciparum, the role of proteases has been deciphered mainly in the asexual blood stages and shown to represent promising drug targets. However, little is known about their functions in the sexual blood stages, which are important for transmission of the disease from the human to the mosquito vector. Determination of their stage-specific expression during the malaria life-cycle is crucial for the effective design of multi-stage anti-malaria drugs aimed at eradicating the disease. In this study, we screened the P. falciparum genome database for putative proteases and determined the transcript and protein expression profiles of selected proteases in the plasmodial blood stages using semi-quantitative RT-PCR and indirect immunofluorescence assay. Database mining identified a total of 148 putative proteases, out of which 18 were demonstrated to be expressed in the blood stages on the transcript level; for 12 of these proteins synthesis was confirmed. While three of these proteases exhibit gametocyte-specific expression, two are restricted to the asexual blood stages and seven are found in both stages, making them interesting multi-stage drug targets. © 2017 Elsevier Inc.","Drug target; Gametocyte; Plasmodium falciparum; Proteases; Schizont; Transmission","proteinase; antiserum; peptide hydrolase; recombinant protein; antibody labeling; Article; controlled study; data mining; gametocyte; gender; gene expression profiling; genetic database; genetic transcription; immunofluorescence test; life cycle; nonhuman; Plasmodium falciparum; priority journal; protein analysis; protein degradation; protein expression; protein synthesis; reverse transcription polymerase chain reaction; RNA isolation; species difference; trophozoite; animal; enzymology; female; gene expression regulation; genetics; human; immunology; indirect fluorescent antibody technique; malaria falciparum; metabolism; mouse; parasitemia; parasitology; Plasmodium falciparum; rabbits and hares; Western blotting; Animals; Blotting, Western; Female; Fluorescent Antibody Technique, Indirect; Gene Expression Profiling; Gene Expression Regulation, Enzymologic; Humans; Immune Sera; Malaria, Falciparum; Mice; Parasitemia; Peptide Hydrolases; Plasmodium falciparum; Rabbits; Recombinant Proteins",2-s2.0-85017169035
"Liu Y., Bi J.-W., Fan Z.-P.","Multi-class sentiment classification: The experimental comparisons of feature selection and machine learning algorithms",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016156998&doi=10.1016%2fj.eswa.2017.03.042&partnerID=40&md5=4e4ab7279b457bee91cb2dc62abd5783","Multi-class sentiment classification has extensive application backgrounds, whereas studies on this issue are still relatively scarce. In this paper, a framework for multi-class sentiment classification is proposed, which includes two parts: 1) selecting important features of texts using the feature selection algorithm, and 2) training multi-class sentiment classifier using the machine learning algorithm. Then, experiments are conducted for comparing the performances of four popular feature selection algorithms (document frequency, CHI statistics, information gain and gain ratio) and five popular machine learning algorithms (decision tree, naïve Bayes, support vector machine, radial basis function neural network and K-nearest neighbor) in multi-class sentiment classification. The experiments are conducted on three public datasets which include twelve data subsets, and 10-fold cross validation is used to obtain the classification accuracy concerning each combination of feature selection algorithm, machine learning algorithm, feature set size and data subset. Based on the obtained 3600 classification accuracies (4 feature selection algorithms × 5 machine learning algorithms × 15 feature set sizes × 12 data subsets), the average classification accuracy of each algorithm is calculated, and the Wilcoxon test is used to verify the existence of significant difference between different algorithms in multi-class sentiment classification. The results show that, in terms of classification accuracy, gain ratio performs best among the four feature selection algorithms and support vector machine performs best among the five machine learning algorithms. In terms of execution time, the similar comparisons are also conducted. The obtained results would be valuable for further improving the existing multi-class sentiment classifiers and developing new multi-class sentiment classifiers. © 2017 Elsevier Ltd","Experimental comparison; Feature selection algorithms; Machine learning algorithms; Multi-class sentiment classification","Artificial intelligence; Classification (of information); Data mining; Decision trees; Feature extraction; Information retrieval systems; Learning systems; Nearest neighbor search; Radial basis function networks; Support vector machines; Text processing; 10-fold cross-validation; Classification accuracy; Document frequency; Experimental comparison; Feature selection algorithm; K-nearest neighbors; Radial basis function neural networks; Sentiment classification; Learning algorithms",2-s2.0-85016156998
"Huang C., Yang Y., Chen X., Wang C., Li Y., Zheng C., Wang Y.","Large-scale cross-species chemogenomic platform proposes a new drug discovery strategy of veterinary drug from herbal medicines",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029872930&doi=10.1371%2fjournal.pone.0184880&partnerID=40&md5=49cc11c71eb173eec886d9956c1e2f28","Veterinary Herbal Medicine (VHM) is a comprehensive, current, and informative discipline on the utilization of herbs in veterinary practice. Driven by chemistry but progressively directed by pharmacology and the clinical sciences, drug research has contributed more to address the needs for innovative veterinary medicine for curing animal diseases. However, research into veterinary medicine of vegetal origin in the pharmaceutical industry has reduced, owing to questions such as the short of compatibility of traditional natural-product extract libraries with high-throughput screening. Here, we present a cross-species chemogenomic screening platform to dissect the genetic basis of multifactorial diseases and to determine the most suitable points of attack for future veterinary medicines, thereby increasing the number of treatment options. First, based on critically examined pharmacology and text mining, we build a cross-species drug-likeness evaluation approach to screen the lead compounds in veterinary medicines. Second, a specific cross-species target prediction model is developed to infer drug-target connections, with the purpose of understanding how drugs work on the specific targets. Third, we focus on exploring the multiple targets interference effects of veterinary medicines by heterogeneous network convergence and modularization analysis. Finally, we manually integrate a disease pathway to test whether the cross-species chemogenomic platform could uncover the active mechanism of veterinary medicine, which is exemplified by a specific network module. We believe the proposed cross-species chemogenomic platform allows for the systematization of current and traditional knowledge of veterinary medicine and, importantly, for the application of this emerging body of knowledge to the development of new drugs for animal diseases. © 2017 Huang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"araboglycyrrhizin; artonin E; aurantiin; baicalin; beta citraurin; beta glucan; campesterol; cerevisterol; chemical compound; dehydroeburiconic acid; ergosterol; gancaolin H; glycoside A; glycyrrhetol; hispaglabridin B; isoglycyrol; kanzonol H; kanzonol Z; new drug; polysaccharide; poricoic acid D; poricoic acid DM; poricoic acid H; rutoside; saponin B2; stigmasterol; unclassified drug; unindexed drug; veterinary drug; vicenin 2; xambioona; animal disease; Article; chemistry; chemogenomics; cross species chemogenomic platform; data mining; drug development; drug industry; drug research; drug screening; genomics; herbal medicine; nonhuman; staphylococcal pneumonia; veterinary medicine; animal; Animal Diseases; drug development; genetics; herbal medicine; species difference; Animal Diseases; Animals; Drug Discovery; Herbal Medicine; Species Specificity; Veterinary Drugs",2-s2.0-85029872930
"Lucas G.M., Gratch J., Malandrakis N., Szablowski E., Fessler E., Nichols J.","GOAALLL!: Using sentiment in the world cup to explore theories of emotion",2017,"Image and Vision Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018756197&doi=10.1016%2fj.imavis.2017.01.006&partnerID=40&md5=a325fe1f95089edc0154a182d6f8420a","Sporting events evoke strong emotions among fans and thus act as natural laboratories to explore emotions and how they unfold in the wild. Computational tools, such as sentiment analysis, provide new ways to examine such dynamic emotional processes. In this article we use sentiment analysis to examine tweets posted during 2014 World Cup. Such analysis gives insight into how people respond to highly emotional events, and how these emotions are shaped by contextual factors, such as prior expectations, and how these emotions change as events unfold over time. Here we report on some preliminary analysis of a World Cup twitter corpus using sentiment analysis techniques. After performing initial tests of validation for sentiment analysis on data in this corpus, we show these tools can give new insights into existing theories of what makes a sporting match exciting. This analysis seems to suggest that, contrary to assumptions in sports economics, excitement relates to expressions of negative emotion. The results are discussed in terms of innovations in methodology and understanding the role of emotion for “tuning in” to real world events. We also discuss some challenges that such data present for existing sentiment analysis techniques and discuss future analysis. © 2017 Elsevier B.V.","Emotion models; Sentiment; Sentiment analysis; Sport; Twitter","Computation theory; Social networking (online); Sports; Computational tools; Contextual factors; Emotion models; Natural laboratories; Preliminary analysis; Sentiment; Sentiment analysis; Twitter; Data mining",2-s2.0-85018756197
"Zheng Y., Li X., Manor L.C., Cao H., Chen Q.","An integrative computational approach to evaluate genetic markers for chronic lymphocytic leukemia",2017,"Journal of Computational Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029397096&doi=10.1089%2fcmb.2017.0041&partnerID=40&md5=0305a3cc62cab865e42aa4637b1cdf0c","Recent studies reported hundreds of genes linked to chronic lymphocytic leukemia (CLL). However, many of these candidate genes were lack of replication and results were not always consistent. Here, we proposed a computational workflow to curate and evaluate CLL-related genes. The method integrates large-scale literature knowledge data, gene expression data, and related pathways/network information for quantitative marker evaluation. Pathway Enrichment, Sub-Network Enrichment, and Gene-Gene Interaction analysis were conducted to study the pathogenic profile of the candidate genes, with four metrics proposed and validated for each gene. By using our approach, a scalable CLL genetic database was developed including CLL-related genes, pathways, diseases and information of supporting references. The CLL case/control classification supported the effectiveness of the four proposed metrics, which successfully identified nine well-studied CLL genes (i.e., TNF, BCL2, TP53, VEGFA, P2RX7, AKT1, SYK, IL4, and MDM2) and highlighted two newly reported CLL genes (i.e., PDGFRA and CSF1R). The computational biology approach and the CLL database developed in this study provide a valuable resource that may facilitate the understanding of the genetic profile of CLL. © 2017, Mary Ann Liebert, Inc.","chronic lymphocytic leukemia; genetic network analysis; literature data mining; pathway enrichment analysis; sub-network enrichment analysis",,2-s2.0-85029397096
"Lin B., Huangfu Y., Lima N., Jobson B., Kirk M., O'Keeffe P., Pressley S.N., Walden V., Lamb B., Cook D.J.","Analyzing the relationship between human behavior and indoor air quality",2017,"Journal of Sensor and Actuator Networks",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029532934&doi=10.3390%2fjsan6030013&partnerID=40&md5=01714936b3da8fd5bc537c39bd9be3d5","In the coming decades, as we experience global population growth and global aging issues, there will be corresponding concerns about the quality of the air we experience inside and outside buildings. Because we can anticipate that there will be behavioral changes that accompany population growth and aging, we examine the relationship between home occupant behavior and indoor air quality. To do this, we collect both sensor-based behavior data and chemical indoor air quality measurements in smart home environments. We introduce a novel machine learning-based approach to quantify the correlation between smart home features and chemical measurements of air quality, and evaluate the approach using two smart homes. The findings may help us understand the types of behavior that measurably impact indoor air quality. This information could help us plan for the future by developing an automated building system that would be used as part of a smart city. © 2017 by the authors.","Data mining; Indoor air quality; Machine learning; Smart home environment",,2-s2.0-85029532934
"Nirmala Kumari R., Mala V.","Multi_level secure from web intrusion and query attacks on web database",2017,"International Journal on Smart Sensing and Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027974035&partnerID=40&md5=8b9f24aa36164245cb4e80d3bbcf8914","Most data frameworks and business applications assembled these days have a web frontend and they should be generally accessible to customers, representatives and accomplices around the globe, as the computerized economy is turning out to be increasingly pervasive in the worldwide economy. Strategy and a model instrument to assess web application security components. In this paper, we along these lines propose to make trusted equipment a top notch national in the safe information administration field. Additionally, we trust that cost-driven bits of knowledge and compositional standards will generally change the way frameworks and calculations are planned. We present an outsourced database model that permits customers to execute SQL questions with security and under administrative consistence imperatives by utilizing server-facilitated, sealed trusted equipment in basic inquiry preparing stages, along these lines expelling any confinements on the kind of bolstered inquiries.","Automatic protection; Data mining; False positives; Input validation vulnerabilities; Software security; Source code static analysis; Web applicationss",,2-s2.0-85027974035
"Rabcan J., Vaclavkova M., Blasko R.","Selection of appropriate candidates for a type position using C4.5 decision tree",2017,"Proceedings of the International Conference on Information and Digital Technologies, IDT 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030099260&doi=10.1109%2fDT.2017.8024318&partnerID=40&md5=570e1ccdf164e54056e7f21c58034137","Recruitment and selection of new employees rank to the important processes of human potential management and development. Especially the process of employee selection prepares proper conditions for a successful work performance and decides on a future progress-ability of the organizations. In a unique sector of private security, the precise realization of employee selection can solve one of the most frequent problem of the private security organizations: high fluctuation/employee turnover. The paper focuses on an experimental possibility to assign systematically, and with the high measure of exactness, the required competences of candidates to the specification of clients' protected interests or objects. In presented experiment study, built on the basis of benchmarking, the paper presents the concept of well-known decision trees to choose best candidates. Demonstration of proposed system and method is done by public available data, where we achieved 97.27% accuracy of classification. © 2017 IEEE.","Classification; Criteria; Data mining; Decision trees; Employee selection","Classification (of information); Decision trees; Human resource management; Trees (mathematics); Accuracy of classifications; C4.5 decision trees; Criteria; Employee selection; Experiment study; Recruitment and selection; Security organizations; Work performance; Data mining",2-s2.0-85030099260
"Pota M., Scalco E., Sanguineti G., Farneti A., Cattaneo G.M., Rizzo G., Esposito M.","Early prediction of radiotherapy-induced parotid shrinkage and toxicity based on CT radiomics and fuzzy classification",2017,"Artificial Intelligence in Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015690972&doi=10.1016%2fj.artmed.2017.03.004&partnerID=40&md5=a8dd4e70e38b495050baac1ad4154c79","Motivation Patients under radiotherapy for head-and-neck cancer often suffer of long-term xerostomia, and/or consistent shrinkage of parotid glands. In order to avoid these drawbacks, adaptive therapy can be planned for patients at risk, if the prediction is obtained timely, before or during the early phase of treatment. Artificial intelligence can address the problem, by learning from examples and building classification models. In particular, fuzzy logic has shown its suitability for medical applications, in order to manage uncertain data, and to build transparent rule-based classifiers. In previous works, clinical, dosimetric and image-based features were considered separately, to find different possible predictors of parotid shrinkage. On the other hand, a few works reported possible image-based predictors of xerostomia, while the combination of different types of features has been little addressed. Objective This paper proposes the application of a novel machine learning approach, based on both statistics and fuzzy logic, aimed at the classification of patients at risk of i) parotid gland shrinkage and ii) 12-months xerostomia. Both problems are addressed with the aim of individuating predictors and models to classify respective outcomes. Methods Knowledge is extracted from a real dataset of radiotherapy patients, by means of a recently developed method named Likelihood-Fuzzy Analysis, based on the representation of statistical information by fuzzy rule-based models. This method enables to manage heterogeneous variables and missing data, and to obtain interpretable fuzzy models presenting good generalization power (thus high performance), and to measure classification confidence. Numerous features are extracted to characterize patients, coming from different sources, i.e. clinical features, dosimetric parameters, and radiomics-based measures obtained by texture analysis of Computed Tomography images. A learning approach based on the composition of simple models in a more complicated one allows to consider the features separately, in order to identify predictors and models to use when only some data source is available, and obtaining more accurate results when more information can be combined. Results Regarding parotid shrinkage, a number of good predictors is detected, some already known and confirmed here, and some others found here, in particular among radiomics-based features. A number of models are also designed, some using single features and others involving models composition to improve classification accuracy. In particular, the best model to be used at the initial treatment stage, and another one applicable at the half treatment stage are identified. Regarding 12-months toxicity, some possible predictors are detected, in particular among radiomics-based features. Moreover, the relation between final parotid shrinkage rate and 12-months xerostomia is evaluated. The method is compared to the naïve Bayes classifier, which reveals similar results in terms of classification accuracy and best predictors. The interpretable fuzzy rule-based models are explicitly presented, and the dependence between predictors and outcome is explained, thus furnishing in some cases helpful insights about the considered problems. Conclusion Thanks to the performance and interpretability of the fuzzy classification method employed, predictors of both parotid shrinkage and xerostomia are detected, and their influence on each outcome is revealed. Moreover, models for predicting parotid shrinkage at initial and half radiotherapy stages are found. © 2017 Elsevier B.V.","Classification; Fuzzy logic; Parotid gland; Radiomics; Rule-based systems; Xerostomia","Artificial intelligence; Classification (of information); Computer circuits; Computerized tomography; Data mining; Dosimetry; Feature extraction; Forecasting; Fuzzy inference; Fuzzy rules; Fuzzy systems; Knowledge based systems; Learning systems; Medical applications; Medical imaging; Patient treatment; Radiotherapy; Shrinkage; Toxicity; Classification accuracy; Computed tomography images; Fuzzy classification methods; Interpretable fuzzy rules; Machine learning approaches; Parotid glands; Radiomics; Xerostomia; Fuzzy logic; Article; cancer radiotherapy; clinical article; clinical feature; controlled study; diagnostic accuracy; diagnostic test accuracy study; early diagnosis; fuzzy logic; fuzzy system; human; intensity modulated radiation therapy; parotid gland disease; parotid shrinkage; priority journal; radiation injury; sensitivity and specificity; xerostomia",2-s2.0-85015690972
"Ramirez A.G., Thompson I.M.","How will the ‘cancer moonshot’ impact health disparities?",2017,"Cancer Causes and Control",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026787218&doi=10.1007%2fs10552-017-0927-6&partnerID=40&md5=2f376603ce5214a51acea6dd227c2c56","In 1971, President Nixon signed into law the National Cancer Act (NCA), colloquially known as the “War on Cancer”, which pushed cancer onto the national agenda and is credited for many subsequent increases in the knowledge of the molecular, cellular, and genetic causes and effects of cancer. But even though cancer mortality has declined overall in intervening years after the NCA, cancer health disparities persist in the form of higher cancer incidence and mortality rates among certain cancer types and certain populations. Breast and cervical cancers disproportionately affect African American, Hispanic, and American Indian Women. Colorectal cancer is the second leading cause of death for Latinos (with men and women combined). Forty-five years after the NCA, how will the next enormous cancer initiatives—President Barack Obama’s Cancer Moonshot and the All of Us Research Program (formerly the Precision Medicine Initiative Cohort Program)—impact cancer health disparities? The emergence of precision medicine and the sharing of information across sectors are at the heart of these large national initiatives and hold vast potential to address complex health disparities that remain in incidence reporting, incidence, treatment, prognoses, and mortality among certain cancer types and racial/ethnic minorities, including African Americans and Hispanics/Latinos, compared to Whites. But clinical research efforts and data collection have historically lacked diverse representation for various reasons, posing a large risk to these national initiatives in their ability to develop diverse cohorts that adequately represent racial/ethnic minorities. Efforts to reduce disparities and increase diversity in study cohorts have emerged, from patient navigation, to use of mobile technology to collect data, to national consortiums dedicated to including diverse groups, to university training on health disparities. These efforts point to the need for the Cancer Moonshot and precision medicine leaders to develop a multifaceted approach to address disparities in health and healthcare to promote a diverse healthcare workforce, patient-centered care, maintenance of a database of information regarding the state of health disparities, and the institution of measurable goals for improving care across all ethnic groups. If these elements are included, it is possible that the Cancer Moonshot and precision medicine will benefit the entire population of our country. © 2017, Springer International Publishing AG.",,"cancer center; cancer classification; cancer control; cancer genetics; cancer incidence; cancer mortality; cancer prognosis; cancer research; cancer therapy; data mining; ethnicity; health care planning; health care policy; health disparity; health insurance; health promotion; human; information dissemination; Letter; medical education; medical technology; national health organization; priority journal; United States",2-s2.0-85026787218
"Droste A.M., Pape J.J., Overeem A., Leijnse H., Steeneveld G.J., Van Delden A.J., Uijlenhoet R.","Crowdsourcing urban air temperatures through smartphone battery temperatures in São Paulo, Brazil",2017,"Journal of Atmospheric and Oceanic Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029744817&doi=10.1175%2fJTECH-D-16-0150.1&partnerID=40&md5=fb91f630d47ca1ab3a1d52eadc92bfbc","Crowdsourcing as a method to obtain and apply vast datasets is rapidly becoming prominent in meteorology, especially for urban areas where routine weather observations are scarce. Previous studies showed that smartphone battery temperature readings can be used to estimate the daily and citywide air temperature via a direct heat transfer model. This work extends model estimates by studying smaller temporal and spatial scales. The study finds the number of battery readings influences the accuracy of temperature retrievals. Optimal results are achieved for 700 or more retrievals. An extensive dataset of over 10 million battery temperature readings for estimating hourly and daily air temperatures is available for São Paulo, Brazil. The air temperature estimates are validated with measurements from a WMO station, an Urban Flux Network site, and data from seven citizen weather stations. Daily temperature estimates are good (coefficient of determination ρ2 of 86%), and the study shows they improve by optimizing model parameters for neighborhood scales (&lt; 1 km2) as categorized in local climate zones (LCZs). Temperature differences between LCZs can be distinguished from smartphone battery temperatures. When validating the model for hourly temperature estimates, the model requires a diurnally varying parameter function in the heat transfer model rather than one fixed value for the entire day. The results show the potential of large crowdsourced datasets in meteorological studies, and the value of smartphones as a measuring platform when routine observations are lacking. © 2017 American Meteorological Society.","Data mining; Heat islands; Statistics; Urban meteorology","Climate models; Crowdsourcing; Data mining; Electric batteries; Heat transfer; Meteorology; Parameter estimation; Smartphones; Statistics; Urban growth; Coefficient of determination; Heat island; Meteorological studies; Temperature differences; Temperature retrieval; Temporal and spatial scale; Urban meteorology; Weather observations; Atmospheric temperature; air temperature; data mining; heat island; heat transfer; urban atmosphere; Brazil; Sao Paulo [Brazil]",2-s2.0-85029744817
"Soleymani M., Garcia D., Jou B., Schuller B., Chang S.-F., Pantic M.","A survey of multimodal sentiment analysis",2017,"Image and Vision Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028747080&doi=10.1016%2fj.imavis.2017.08.003&partnerID=40&md5=6605e568b7f315f7969361d23308f355","Sentiment analysis aims to automatically uncover the underlying attitude that we hold towards an entity. The aggregation of these sentiment over a population represents opinion polling and has numerous applications. Current text-based sentiment analysis rely on the construction of dictionaries and machine learning models that learn sentiment from large text corpora. Sentiment analysis from text is currently widely used for customer satisfaction assessment and brand perception analysis, among others. With the proliferation of social media, multimodal sentiment analysis is set to bring new opportunities with the arrival of complementary data streams for improving and going beyond text-based sentiment analysis. Since sentiment can be detected through affective traces it leaves, such as facial and vocal displays, multimodal sentiment analysis offers promising avenues for analyzing facial and vocal expressions in addition to the transcript or textual content. These approaches leverage emotion recognition and context inference to determine the underlying polarity and scope of an individual's sentiment. In this survey, we define sentiment and the problem of multimodal sentiment analysis and review recent developments in multimodal sentiment analysis in different domains, including spoken reviews, images, video blogs, human–machine and human–human interactions. Challenges and opportunities of this emerging field are also discussed leading to our thesis that multimodal sentiment analysis holds a significant untapped potential. © 2017","Affect; Affective computing; Computer vision; Human behavior analysis; Sentiment; Sentiment analysis","Behavioral research; Computer vision; Customer satisfaction; Data mining; Human computer interaction; Learning systems; Surveys; Affect; Affective Computing; Human behavior analysis; Sentiment; Sentiment analysis; Modal analysis",2-s2.0-85028747080
"Wellek S.","A critical evaluation of the current “p-value controversy”",2017,"Biometrical Journal",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019260586&doi=10.1002%2fbimj.201700001&partnerID=40&md5=9fa30ad77eeee5d90d7ce78a4a8150c8","This article has been triggered by the initiative launched in March 2016 by the Board of Directors of the American Statistical Association (ASA) to counteract the current p-value focus of statistical research practices that allegedly “have contributed to a reproducibility crisis in science.” It is pointed out that in the very wide field of statistics applied to medicine, many of the problems raised in the ASA statement are not as severe as in the areas the authors may have primarily in mind, although several of them are well-known experts in biostatistics and epidemiology. This is mainly due to the fact that a large proportion of medical research falls under the realm of a well developed body of regulatory rules banning the most frequently occurring misuses of p-values. Furthermore, it is argued that reducing the statistical hypotheses tests nowadays available to the class of procedures based on p-values calculated under a traditional one-point null hypothesis amounts to ignoring important developments having taken place and going on within the statistical sciences. Although hypotheses testing is still an indispensable part of the statistical methodology required in medical and other areas of empirical research, there is a large repertoire of methods based on different paradigms of inference that provide ample options for supplementing and enhancing the methods of data analysis blamed in the ASA statement for causing a crisis. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim","Bayesian inference; Data mining; Measures of evidence; Multiplicity correction; Prediction; Reproducibility of experiments",,2-s2.0-85019260586
"Soleimani R., Saeedi Dehaghani A.H., Bahadori A.","A new decision tree based algorithm for prediction of hydrogen sulfide solubility in various ionic liquids",2017,"Journal of Molecular Liquids",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025474599&doi=10.1016%2fj.molliq.2017.07.075&partnerID=40&md5=d1dfd60699231ab2e7b712db995a0676","The solubility of acidic components at various temperatures and pressures in ionic liquids (ILs) is one of the decisive property needed for the appraisal of ILs as potential substitutes for alkanolamines in industrial natural gas sweetening processes, therefore its modeling encompasses scientific and commercial interest. To that end, in the present work, an advanced machine learning approach called stochastic gradient boosting (SGB) tree technique is employed in the calculation of hydrogen sulfide (H2S) solubility in 11 different ILs within the (303.15 to 363.15) K temperature and (0.0608 to 2.0168) MPa pressure range as a function of critical temperature, critical pressure and acentric factor of ILs accompanied with operational temperature and pressure. A collection of 465 experimental data points were assembled from the literatures. The statistical parameters including correlation coefficient (R) of 0.999543 and mean relative absolute error (MRAE), 0.022198, of the results form dataset values exhibit the high precision of the applied method. Furthermore, the prediction competence of the SGB model has been compared to two well-known equation of states (EOS) as well as Genetic Expression Programming (GEP) and least squares support vector machine (LSSVM) models. According to the results of comparative studies, it was found that the SGB model is more robust, reliable and efficient than other existing techniques for improved analysis and design of natural gas sweetening process. © 2017","Hydrogen sulfide; Ionic liquids; Prediction; Solubility; Stochastic gradient boosting","Adaptive boosting; Data mining; Decision trees; Equations of state; Forecasting; Genetic programming; Ionic liquids; Liquids; Natural gas; Natural gas substitutes; Solubility; Stochastic systems; Sulfur compounds; Sulfur determination; Trees (mathematics); Correlation coefficient; Decision-tree based algorithms; Genetic Expression Programming; Least squares support vector machines; Machine learning approaches; Operational temperature; Statistical parameters; Stochastic gradient boosting; Hydrogen sulfide",2-s2.0-85025474599
"Kim J.-Y., Liu N., Tan H.-X., Chu C.-H.","Unobtrusive Monitoring to Detect Depression for Elderly with Chronic Illnesses",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028815950&doi=10.1109%2fJSEN.2017.2729594&partnerID=40&md5=984d19ee519e95f748b7da18d2bcdde6","Mental health related disorders are common diseases, especially among the elder. Among the various mental health diseases, one potential threat to ageing-in-place is the risk of depression. In this paper, we propose a simple unobtrusive sensing system using passive infra-red motion sensors to monitor the activities of daily living of elderly, who are living alone. A feature extraction module comprising of three layers-states, events, and activities, and the corresponding algorithms are proposed to extract features. Four popular classification models-neural network, C4.5 decision tree, Bayesian network, and support vector machine are then applied to detect the severity of depression. We implement and test the algorithms on sensor data collected over three months from 20 elderly, each in different daily living conditions. Our evaluation shows that the proposed algorithms are effective in detecting both normal condition and mild depression with up to 96% accuracy, using neural network as the classification algorithm. The sensing system is non-intrusive and cost-effective, with the potential of use for long-term depression monitoring and detection of early symptoms of mental related disorders. This enables caregivers to provide timely interventions to elderly, who are at risk of depression. © 2001-2012 IEEE.","depression detection; Feature extraction; sensor technologies; smart homes; unobtrusive monitoring","Automation; Bayesian networks; Cost effectiveness; Data mining; Decision trees; Extraction; Health risks; Image retrieval; Intelligent buildings; Monitoring; Object recognition; Support vector machines; Senior citizens; Sensor systems; Sensor technologies; Smart homes; Unobtrusive monitoring; Feature extraction",2-s2.0-85028815950
"Hoogendoorn M., Berger T., Schulz A., Stolz T., Szolovits P.","Predicting Social Anxiety Treatment Outcome Based on Therapeutic Email Conversations",2017,"IEEE Journal of Biomedical and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029911921&doi=10.1109%2fJBHI.2016.2601123&partnerID=40&md5=34c977d50712daa9202b7d4ac320bb4f","Predicting therapeutic outcome in the mental health domain is of utmost importance to enable therapists to provide the most effective treatment to a patient. Using information from the writings of a patient can potentially be a valuable source of information, especially now that more and more treatments involve computer-based exercises or electronic conversations between patient and therapist. In this paper, we study predictive modeling using writings of patients under treatment for a social anxiety disorder. We extract a wealth of information from the text written by patients including their usage of words, the topics they talk about, the sentiment of the messages, and the style of writing. In addition, we study trends over time with respect to those measures. We then apply machine learning algorithms to generate the predictive models. Based on a dataset of 69 patients, we are able to show that we can predict therapy outcome with an area under the curve of 0.83 halfway through the therapy and with a precision of 0.78 when using the full data (i.e., the entire treatment period). Due to the limited number of participants, it is hard to generalize the results, but they do show great potential in this type of information. © 2013 IEEE.","Anxiety; mental health; natural language processing (NLP); predictive modeling","Data mining; Forecasting; Learning algorithms; Learning systems; Medical computing; Modeling languages; Natural language processing systems; Anxiety; Area under the curves; Computer-based exercise; Mental health; Predictive modeling; Therapeutic outcomes; Treatment outcomes; Wealth of information; Patient treatment; Article; e-mail; emotion assessment; human; machine learning; major depression; mental health; natural language processing; psychotherapy; social phobia; Social Phobia Scale; treatment outcome; trend study; written communication",2-s2.0-85029911921
"Anagnostopoulos A., Atassi R., Becchetti L., Fazzone A., Silvestri F.","Tour recommendation for groups",2017,"Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988429050&doi=10.1007%2fs10618-016-0477-7&partnerID=40&md5=45ec7cffd99c7e08a4c280484da053f1","Consider a group of people who are visiting a major touristic city, such as NY, Paris, or Rome. It is reasonable to assume that each member of the group has his or her own interests or preferences about places to visit, which in general may differ from those of other members. Still, people almost always want to hang out together and so the following question naturally arises: What is the best tour that the group could perform together in the city? This problem underpins several challenges, ranging from understanding people’s expected attitudes towards potential points of interest, to modeling and providing good and viable solutions. Formulating this problem is challenging because of multiple competing objectives. For example, making the entire group as happy as possible in general conflicts with the objective that no member becomes disappointed. In this paper, we address the algorithmic implications of the above problem, by providing various formulations that take into account the overall group as well as the individual satisfaction and the length of the tour. We then study the computational complexity of these formulations, we provide effective and efficient practical algorithms, and, finally, we evaluate them on datasets constructed from real city data. © 2016, The Author(s).","Group recommendation; Orienteering problem; Tour recommendation for groups","Computer applications; Data mining; Group recommendations; Orienteering problem; Points of interest; Tour recommendation for groups; Viable solutions; Computational efficiency",2-s2.0-84988429050
"Chan Y.-T., Wang S.-J., Tsai C.-H.","Extracting foreground ensemble features to detect abnormal crowd behavior in intelligent video-surveillance systems",2017,"Journal of Electronic Imaging",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021629623&doi=10.1117%2f1.JEI.26.5.051402&partnerID=40&md5=fe409a014b2e80e19aa3c33c419e3519","Public safety is a matter of national security and people's livelihoods. In recent years, intelligent video-surveillance systems have become important active-protection systems. A surveillance system that provides early detection and threat assessment could protect people from crowd-related disasters and ensure public safety. Image processing is commonly used to extract features, e.g., people, from a surveillance video. However, little research has been conducted on the relationship between foreground detection and feature extraction. Most current video-surveillance research has been developed for restricted environments, in which the extracted features are limited by having information from a single foreground; they do not effectively represent the diversity of crowd behavior. This paper presents a general framework based on extracting ensemble features from the foreground of a surveillance video to analyze a crowd. The proposed method can flexibly integrate different foreground-detection technologies to adapt to various monitored environments. Furthermore, the extractable representative features depend on the heterogeneous foreground data. Finally, a classification algorithm is applied to these features to automatically model crowd behavior and distinguish an abnormal event from normal patterns. The experimental results demonstrate that the proposed method's performance is both comparable to that of state-of-the-art methods and satisfies the requirements of real-time applications. © 2017 SPIE and IS&T.","abnormal behavior detection; crowd analysis; feature extraction; intelligent video surveillance system","Behavioral research; Classification (of information); Data mining; Extraction; Feature extraction; Image processing; Monitoring; National security; Video signal processing; Abnormal behavior detections; Active protection system; Classification algorithm; Crowd analysis; Intelligent video surveillance systems; Real-time application; State-of-the-art methods; Surveillance systems; Security systems",2-s2.0-85021629623
"Shams M., Baraani-Dastjerdi A.","Enriched LDA (ELDA): Combination of latent Dirichlet allocation with word co-occurrence analysis for aspect extraction",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015394728&doi=10.1016%2fj.eswa.2017.02.038&partnerID=40&md5=aa25de563160b9ade296c9f6ba116d8e","Aspect extraction is one of the fundamental steps in analyzing the characteristics of opinions, feelings and emotions expressed in textual data provided for a certain topic. Current aspect extraction techniques are mostly based on topic models; however, employing only topic models causes incoherent aspects to be generated. Therefore, this paper aims to discover more precise aspects by incorporating co-occurrence relations as prior domain knowledge into the Latent Dirichlet Allocation (LDA) topic model. In the proposed method, first, the preliminary aspects are generated based on LDA. Then, in an iterative manner, the prior knowledge is extracted automatically from co-occurrence relations and similar aspects of relevant topics. Finally, the extracted knowledge is incorporated into the LDA model. The iterations improve the quality of the extracted aspects. The competence of the proposed ELDA for the aspect extraction task is evaluated through experiments on two datasets in the English and Persian languages. The experimental results indicate that ELDA not only outperforms the state-of-the-art alternatives in terms of topic coherence and precision, but also has no particular dependency on the written language and can be applied to all languages with reasonable accuracy. Thus, ELDA can impact natural language processing applications, particularly in languages with limited linguistic resources. © 2017 Elsevier Ltd","Aspect extraction; Co-occurrence relations; Latent Dirichlet Allocation (LDA); Sentiment analysis; Topic modeling","Extraction; Iterative methods; Natural language processing systems; Statistics; Co-occurrence; Extraction techniques; Latent Dirichlet allocation; Latent dirichlet allocations; Linguistic resources; Natural language processing applications; Sentiment analysis; Topic Modeling; Data mining",2-s2.0-85015394728
"Garrard P., Nemes V., Nikolic D., Barney A.","Motif discovery in speech: Application to monitoring alzheimer’s disease",2017,"Current Alzheimer Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029629647&doi=10.2174%2f1567205014666170309121025&partnerID=40&md5=499efbff0cdc7bf801ce4df46479db2c","Background: Perseveration - repetition of words, phrases or questions in speech - is commonly described in Alzheimer’s disease (AD). Measuring perseveration is difficult, but may index cognitive performance, aiding diagnosis and disease monitoring. Continuous recording of speech would produce a large quantity of data requiring painstaking manual analysis, and risk violating patients’ and others’ privacy. A secure record and an automated approach to analysis are required. Objectives: To record bone-conducted acoustic energy fluctuations from a subject’s vocal apparatus using an accelerometer, to describe the recording and analysis stages in detail, and demonstrate that the approach is feasible in AD. Methods: Speech-related vibration was captured by an accelerometer, affixed above the temporomandibular joint. Healthy subjects read a script with embedded repetitions. Features were extracted from recorded signals and combined using Principal Component Analysis to obtain a one-dimensional representation of the feature vector. Motif discovery techniques were used to detect repeated segments. The equipment was tested in AD patients to determine device acceptability and recording quality. Results: Comparison with the known location of embedded motifs suggests that, with appropriate parameter tuning, the motif discovery method can detect repetitions. The device was acceptable to patients and produced adequate signal quality in their home environments. Conclusion: We established that continuously recording bone-conducted speech and detecting perseverative patterns were both possible. In future studies we plan to associate the frequency of verbal repetitions with stage, progression and type of dementia. It is possible that the method could contribute to the assessment of disease-modifying treatments. © 2017 Bentham Science Publishers.","Alzheimer’s disease; Bone-conducted speech; Motif discovery; Perseveration; Principal component analysis","accelerometer; acoustic analysis; aged; algorithm; Alzheimer disease; Article; bone conduction; clinical article; data mining; feasibility study; human; human experiment; monitoring; normal human; perseveration; principal component analysis; priority journal; signal processing; speech; verbal behavior; vibration",2-s2.0-85029629647
"Murdock J., Allen C., Börner K., Light R., McAlister S., Ravenscroft A., Rose R., Rose D., Otsuka J., Bourget D., Lawrence J., Reed C.","Multi-level computational methods for interdisciplinary research in the HathiTrust Digital Library",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029596517&doi=10.1371%2fjournal.pone.0184188&partnerID=40&md5=5b11d4e83f94bc910a9157e7fbb7de31","We show how faceted search using a combination of traditional classification systems and mixed-membership topic models can go beyond keyword search to inform resource discovery, hypothesis formulation, and argument extraction for interdisciplinary research. Our test domain is the history and philosophy of scientific work on animal mind and cognition. The methods can be generalized to other research areas and ultimately support a system for semi-automatic identification of argument structures. We provide a case study for the application of the methods to the problem of identifying and extracting arguments about anthropomorphism during a critical period in the development of comparative psychology. We show how a combination of classification systems and mixed-membership models trained over large digital libraries can inform resource discovery in this domain. Through a novel approach of “drill-down” topic modeling—simultaneously reducing both the size of the corpus and the unit of analysis—we are able to reduce a large collection of fulltext volumes to a much smaller set of pages within six focal volumes containing arguments of interest to historians and philosophers of comparative psychology. The volumes identified in this way did not appear among the first ten results of the keyword search in the HathiTrust digital library and the pages bear the kind of “close reading” needed to generate original interpretations that is the heart of scholarly work in the humanities. Zooming back out, we provide a way to place the books onto a map of science originally constructed from very different data and for different purposes. The multilevel approach advances understanding of the intellectual and societal contexts in which writings are interpreted. © 2017 Murdock et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"classification; extraction; heart; human; human experiment; humanities; interdisciplinary research; library; philosophy; psychology; writing; data mining; interdisciplinary education; procedures; research; theoretical model; Data Mining; Humans; Interdisciplinary Studies; Libraries, Digital; Models, Theoretical; Research",2-s2.0-85029596517
"Fu C., Zhang P., Jiang J., Yang K., Lv Z.","A Bayesian approach for sleep and wake classification based on dynamic time warping method",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946771780&doi=10.1007%2fs11042-015-3053-z&partnerID=40&md5=e0dd3f14d730d3e84e046ed25e22f021","Sleep plays a significant role in human’ smental and physical health. Recently, the associations between lack of sleep and weight gain, development of cancer and many other health problems have been recognized. Then monitoring the sleep and wake state all night is becoming a hotspot issue. Traditionally it classified by a PSG recording which is very costly and uncomfortable. Nowadays, with the advance of internet of things, many convenient wearable devices are being used for medical use, like measuring the heart rate (HR), blood pressure and other signals. With the sleep quality monitor problem, the key question is how to discriminate the sleep and weak stage from these signals. This paper proposed a Bayesian approach based on dynamic time warping (DTW) method for sleep and wake classification. It used HR and surplus pulse O2 (SPO2) signals to analyze the sleep states and the occurrence of some sleep-related problems. DTW is an algorithm that searches an optimal alignment between time series with scaling and shifting and Bayesian methods have been successfully used for object classification in many study. In this paper, a three-step process is used for sleep and wake classification. In the first step, the DTW is used to extract features of the original HR and SPO2 signals. Then a probabilistic model is introduced for using the Bayesian classification for uncertain data. And in the classification step, the DTW features are used as the training dataset in the Bayesian approach for sleep and wake classification. Finally, a case study form a real-word applications, collected from the website of the Sleep Heart Health Study, is presented to shown the feasibility and advantages of the DTW-based Bayesian approach. © 2015, Springer Science+Business Media New York.","Bayesian classification; Dynamic time warping; Internet of things; Multimedia; Sleep and wake classification; Wearable devices","Bayesian networks; Biomedical signal processing; Blood pressure; Data mining; Health; Internet; Internet of things; Sleep research; Wakes; Wearable technology; Bayesian approaches; Bayesian classification; Dynamic time warping; Multimedia; Object classification; Optimal alignments; Probabilistic modeling; Wearable devices; Classification (of information)",2-s2.0-84946771780
"Fernández A., Carmona C.J., José Del Jesus M., Herrera F.","A pareto-based ensemble with feature and instance selection for learning from multi-class imbalanced datasets",2017,"International Journal of Neural Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021173836&doi=10.1142%2fS0129065717500289&partnerID=40&md5=b5bafde200aa81e6d37dc5c9bfe1bb03","Imbalanced classification is related to those problems that have an uneven distribution among classes. In addition to the former, when instances are located into the overlapped areas, the correct modeling of the problem becomes harder. Current solutions for both issues are often focused on the binary case study, as multi-class datasets require an additional effort to be addressed. In this research, we overcome these problems by carrying out a combination between feature and instance selections. Feature selection will allow simplifying the overlapping areas easing the generation of rules to distinguish among the classes. Selection of instances from all classes will address the imbalance itself by finding the most appropriate class distribution for the learning task, as well as possibly removing noise and difficult borderline examples. For the sake of obtaining an optimal joint set of features and instances, we embedded the searching for both parameters in a Multi-Objective Evolutionary Algorithm, using the C4.5 decision tree as baseline classifier in this wrapper approach. The multi-objective scheme allows taking a double advantage: the search space becomes broader, and we may provide a set of different solutions in order to build an ensemble of classifiers. This proposal has been contrasted versus several state-of-the-art solutions on imbalanced classification showing excellent results in both binary and multi-class problems. © 2017 World Scientific Publishing Company.","ensembles; feature selection; Imbalanced classification; instance selection; multi-class; multi-objective evolutionary algorithms; overlapping","Bins; Data mining; Decision trees; Education; Evolutionary algorithms; Feature extraction; ensembles; Imbalanced classification; Instance selection; Multi objective evolutionary algorithms; multi-class; overlapping; Classification (of information)",2-s2.0-85021173836
"Beran D., Byass P., Gbakima A., Kahn K., Sankoh O., Tollman S., Witham M., Davies J.","Bringing all together for research capacity building in LMICs – Authors' reply",2017,"The Lancet Global Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027219541&doi=10.1016%2fS2214-109X%2817%2930303-0&partnerID=40&md5=aabc37b5d4e24accf595c92528191f11",[No abstract available],,"capacity building; cultural factor; data mining; funding; human; Letter; low income country; medical research; middle income country; priority journal; publication; research; research funding; scientific literature; scientist; university hospital",2-s2.0-85027219541
"Li P.-F., Zhou G.-D.","Three-Layer Joint Modeling of Chinese Trigger Extraction with Constraints on Trigger and Argument Semantics",2017,"Journal of Computer Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029811211&doi=10.1007%2fs11390-017-1780-5&partnerID=40&md5=c0129fd27a2291a3817f29c0bb3aa04f","As a subtask of information extraction (IE), which aims to extract structured information from texts, event extraction is to recognize event trigger mentions of a predefined event type and their arguments. In general, event extraction can be divided into two subtasks: trigger extraction and argument extraction. Currently, the frequent existences of unannotated trigger mentions and poor-context trigger mentions impose critical challenges in Chinese trigger extraction. This paper proposes a novel three-layer joint model to integrate three components in trigger extraction, i.e., trigger identification, event type determination, and event subtype determination. In this way, different kinds of evidence on distinct pseudo samples can be well captured to eliminate the harmful effects of those un-annotated trigger mentions. In addition, this paper introduces various types of linguistically driven constraints on the trigger and argument semantics into the joint model to recover those poor-context trigger mentions. The experimental results show that our joint model significantly outperforms the state-of-the-art Chinese trigger extraction and Chinese event extraction as a whole. © 2017, Springer Science+Business Media, LLC.","argument semantics; information extraction; joint modeling; trigger extraction; trigger semantics","Data mining; Information retrieval; Semantics; Critical challenges; Event extraction; Event trigger; Harmful effects; Joint modeling; State of the art; Structured information; Three component; Information analysis",2-s2.0-85029811211
"Ghobadi H., Thainimit S., Gansawat D., Sugino N.","Computer-aided analysis for breast cancer detection in thermography",2017,"2016 Management and Innovation Technology International Conference, MITiCON 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031744273&doi=10.1109%2fMITICON.2016.8025216&partnerID=40&md5=dd8dd82f65cf5d01a540562c7a55ba63","The purpose of this study is to develop a texture-based algorithm for breast cancer classification in Thermography and performance evaluation for the purposed method. The aim of screening a disease is early detection in order to decrease the rate of mortality. CAD systems can be reckonable diagnostic tools for this importance and Infra-red imaging is expressed as a portable, non-invasive, non-contact and radiation-free diagnostic tool among them. © 2016 IEEE.","breast cancer; decision tree; thermogramy; trainable weka segmentation","Computer aided analysis; Computer aided design; Data mining; Decision trees; Diagnosis; Diagnostic products; Diseases; Engineering research; Infrared imaging; Breast Cancer; Breast cancer classifications; Breast cancer detection; CAD system; Diagnostic tools; Non-contact; thermogramy; Thermography (imaging)",2-s2.0-85031744273
"Li X., Li X., Khyam M.O., Ge S.S.","Robust Welding Seam Tracking and Recognition",2017,"IEEE Sensors Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028913816&doi=10.1109%2fJSEN.2017.2730280&partnerID=40&md5=4e535a6c63dcf0805f2fe0da36e26496","In the process of automatic welding based on structured light vision, the precise localization of the welding seam in an image has an important influence on the quality of the welding. However, in practice, there is much interference, such as spatter and arc, which introduces great challenges for accurate welding seam localization. In this paper, we considered welding seam localization problem as visual target tracking and based on that, we proposed a robust welding seam tracking algorithm. Prior to the start of welding, the seam is separated using a cumulative gray frequency, which is utilized to adaptively determine the initial position and size of the search window. During the welding process, large seam motion range may result in only a portion of the welding seam exists in the search window. To prevent that, a tracking-by-detection method is used to calculate the location of the search window. Usually, the intersection of seam and noise, e.g., spatter, has a severe influence on the accuracy of feature points localization. In order to solve this problem, a sequence gravity method (SGM) for extracting a smoother center line of welding seam is proposed, which is able to reduce the impact of interference. The double-threshold recursive least square method is used to fit the curve obtained by SGM with the aim of improving the real-time performance and accuracy of the system. Finally, the superiority of the proposed algorithm is well demonstrated by comparison with other solutions for seam tracking and recognition through extensive experiments. © 2001-2012 IEEE.","double-threshold recursive least square fitting; sequence gravity method; Tracking by detection; welding tracking","Data mining; Electric welding; Feature extraction; Least squares approximations; Seam welding; Sensors; Surface discharges; Target tracking; Wave interference; Gravity method; Localization problems; Real time performance; Recursive least square (RLS); Structured-light vision; Tracking by detections; Visual target tracking; Welding seam tracking; Welding",2-s2.0-85028913816
"Chen Y.-L., Chang C.-L., Yeh C.-S.","Emotion classification of YouTube videos",2017,"Decision Support Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019978884&doi=10.1016%2fj.dss.2017.05.014&partnerID=40&md5=379e77feafb344ff1d50f19b113ada50","Watching online videos is a major leisure activity among Internet users. The largest video website, YouTube, stores billions of videos on its servers. Thus, previous studies have applied automatic video categorization methods to enable users to find videos corresponding to their needs; however, emotion has not been a factor considered in these classification methods. Therefore, this study classified YouTube videos into six emotion categories (i.e., happiness, anger, disgust, fear, sadness, and surprise). Through unsupervised and supervised learning methods, this study first categorized videos according to emotion. An ensemble model was subsequently applied to integrate the classification results of both methods. The experimental results confirm that the proposed method effectively facilitates the classification of YouTube videos into suitable emotion categories. © 2017 Elsevier B.V.","Data mining; Machine learning; Sentiments analysis; YouTube","Data mining; Classification methods; Classification results; Emotion classification; Leisure activities; Sentiments analysis; Supervised learning methods; Video categorization; YouTube; Learning systems",2-s2.0-85019978884
"Ran F.A.","Adaptation of CRISPR nucleases for eukaryotic applications",2017,"Analytical Biochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006728530&doi=10.1016%2fj.ab.2016.10.018&partnerID=40&md5=946cfc10c74b921fdaa960979a2d5c81",[No abstract available],"C2cx; Cas9; Cpf1; CRISPR; Genome editing","endonuclease; nuclease; bacterial genome; CRISPR Cas system; data mining; eukaryote; family; human; nonhuman; orthology; priority journal; Review",2-s2.0-85006728530
"Holmes J.H., Sacchi L., Bellazzi R., Peek N.","Artificial Intelligence in Medicine AIME 2015",2017,"Artificial Intelligence in Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024483061&doi=10.1016%2fj.artmed.2017.06.011&partnerID=40&md5=f2a815d2584bb9b93a82b83ceafdee2a",[No abstract available],,"artificial intelligence; Bayesian learning; biomedicine; cancer radiotherapy; data mining; decision support system; Editorial; head and neck cancer; human; knowledge discovery; medical society; medicine; methodology; Parkinson disease; phenotype; practice guideline; priority journal; scientific literature; scientist; surgical technique; toxicity; workflow",2-s2.0-85024483061
"Oscar N., Fox P.A., Croucher R., Wernick R., Keune J., Hooker K.","Machine Learning, Sentiment Analysis, and Tweets: An Examination of Alzheimer's Disease Stigma on Twitter",2017,"The journals of gerontology. Series B, Psychological sciences and social sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028285280&doi=10.1093%2fgeronb%2fgbx014&partnerID=40&md5=615b2b73f7381522e7f01b8b9f0e26d3","Objectives: Social scientists need practical methods for harnessing large, publicly available datasets that inform the social context of aging. We describe our development of a semi-automated text coding method and use a content analysis of Alzheimer's disease (AD) and dementia portrayal on Twitter to demonstrate its use. The approach improves feasibility of examining large publicly available datasets.Method: Machine learning techniques modeled stigmatization expressed in 31,150 AD-related tweets collected via Twitter's search API based on 9 AD-related keywords. Two researchers manually coded 311 random tweets on 6 dimensions. This input from 1% of the dataset was used to train a classifier against the tweet text and code the remaining 99% of the dataset.Results: Our automated process identified that 21.13% of the AD-related tweets used AD-related keywords to perpetuate public stigma, which could impact stereotypes and negative expectations for individuals with the disease and increase ""excess disability"".Discussion: This technique could be applied to questions in social gerontology related to how social media outlets reflect and shape attitudes bearing on other developmental outcomes. Recommendations for the collection and analysis of large Twitter datasets are discussed.","Attitudes; Data mining; Social media; Stigma","aged; ageism; algorithm; Alzheimer disease; data mining; disability; female; human; machine learning; male; psychology; public opinion; social media; social stigma; United States; very elderly; Aged; Aged, 80 and over; Ageism; Algorithms; Alzheimer Disease; Data Mining; Disability Evaluation; Female; Humans; Machine Learning; Male; Public Opinion; Social Media; Social Stigma; United States",2-s2.0-85028285280
"Magdalinos P., Barmpounakis S., Spapis P., Kaloxylos A., Kyprianidis G., Kousaridas A., Alonistioti N., Zhou C.","A context extraction and profiling engine for 5G network resource mapping",2017,"Computer Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021962684&doi=10.1016%2fj.comcom.2017.06.003&partnerID=40&md5=d8349114e17742ce053882d04b1000ec","Future 5G network ecosystems comprise a plethora of 3GPP and non 3GGP Radio Access Technologies - RATs. Deployment scenarios envision a multi-layer use of macro, micro and femto-cells where multi-mode end devices, supporting different applications, are served by different technologies. The association of end devices to the most appropriate RAT/layer will therefore become a tantalizing process necessitating the introduction of mechanisms that decide and execute an optimal mapping. The latter is of paramount importance since sub-optimal configuration of network components will affect overall network performance. Towards this end, we introduce the Context Extraction and Profiling Engine (CEPE), a knowledge discovery (KDD) framework catering for the extraction and exploitation of user behavioral patterns from network and service information. An eNB exploits the knowledge scheme derived by CEPE in order to improve the placement of end devices to RATs/layers. In the context of this paper, we provide a thorough analysis of existing standards, research papers and patents, discuss the main innovation of our proposal and highlight the differences with existing schemes. Building on use cases involving mobility management mechanisms that typically affect device to technology mapping (i.e. cell (re)selection, handover) we provide an extensive set of experiments that demonstrate the validity and viability of our idea. Overall evaluation showcases that CEPE achieves high quality results thus emerging as a viable approach for network optimization in future 5G environments. © 2017 Elsevier B.V.","5G network; Data mining; Network optimization; User profiling","Behavioral research; Engines; Extraction; Mapping; Quality control; Queueing networks; Context extractions; Deployment scenarios; G-networks; Mobility management; Network optimization; Radio access technologies; Service information; User profiling; Data mining",2-s2.0-85021962684
"Mattson D.C.","Usability evaluation of the digital anger thermometer app",2017,"Health Informatics Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027224896&doi=10.1177%2f1460458216642504&partnerID=40&md5=4df209528855804dedd1a6d890c7f597","The digital anger thermometer is a prototype for a mobile application (app) for use with adults in anger management treatment. The digital anger thermometer incorporates standards of software development in addition to anger management resources from the Substance Abuse and Mental Health Services Administration. The digital anger thermometer underwent a usability study conducted by five expert reviewers. The results indicate that it is easy to learn, efficient, and ergonomically sound. However, it does not offer support features or user-error tolerance. The digital anger thermometer prototype requires additional usability studies and comparative research in order for it to become an actual mental health app. © The Author(s) 2016.","databases and data mining; e-health; evidence-based practice; health information on the web; mobile health",,2-s2.0-85027224896
"Jin S., Cheung K.-C., Sit P.-S.","Task- and non-task-specific factors classifying problem-solving experts and novices: Comparing students of the top ten high-performing eastern and western economies in PISA 2012",2017,"Contemporary Educational Research Quarterly",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030549029&doi=10.6151%2fCERQ.2017.2503.03&partnerID=40&md5=59fbdef29997d15262b140fbc17f81ea","The purpose of this study is to make use of the computergenerated log files to derive task-specific indicator variables of problem-solving processes of an exemplary problem task (TRAFFIC) to examine factors of relative importance and thereby classify and differentiate high-performing problem-solving experts from lowperforming problem-solving novices. Added to the task-specific indicators are non-task-specific variables collated from questionnaires administered in the Programme for International Student Assessment (PISA) 2012 study.","Classification and regression tree (CART); Educational data mining; Log file; PISA; Problem solving",,2-s2.0-85030549029
"Yadav S., Yoneda M., Tamura M., Susaki J., Ishikawa K., Yamashiki Y.","A satellite-based assessment of the distribution and biomass of submerged aquatic vegetation in the optically shallow basin of Lake Biwa",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029689811&doi=10.3390%2frs9090966&partnerID=40&md5=a0ec3ce2340cdac9df0f2b4f1bb1cac9","Assessing the abundance of submerged aquatic vegetation (SAV), particularly in shallow lakes, is essential for effective lake management activities. In the present study we applied satellite remote sensing (a Landsat-8 image) in order to evaluate the SAV coverage area and its biomass for the peak growth period, which is mainly in September or October (2013 to 2016), in the eutrophic and shallow south basin of Lake Biwa. We developed and validated a satellite-based water transparency retrieval algorithm based on the linear regression approach (R2 = 0.77) to determine the water clarity (2013-2016), which was later used for SAV classification and biomass estimation. For SAV classification, we used Spectral Mixture Analysis (SMA), a Spectral Angle Mapper (SAM), and a binary decision tree, giving an overall classification accuracy of 86.5% and SAV classification accuracy of 76.5% (SAV kappa coefficient 0.74), based on in situ measurements. For biomass estimation, a new Spectral Decomposition Algorithm was developed. The satellite-derived biomass (R2 = 0.79) for the SAV classified area gives an overall root-mean-square error (RMSE) of 0.26 kg dry weight (DW) m-2. The mapped SAV coverage area was 20% and 40% in 2013 and 2016, respectively. Estimated SAV biomass for the mapped area shows an increase in recent years, with values of 3390 t (tons, dry weight) in 2013 as compared to 4550 t in 2016. The maximum biomass density (4.89 kg DW m-2) was obtained for a year with high water transparency (September 2014). With the change in water clarity, a slow change in SAV growth was noted from 2013 to 2016. The study shows that water clarity is important for the SAV detection and biomass estimation using satellite remote sensing in shallow eutrophic lakes. The present study also demonstrates the successful application of the developed satellite-based approach for SAV biomass estimation in the shallow eutrophic lake, which can be tested in other lakes. © 2017 by the authors.","Remote sensing; SAV biomass; Shallow lake; Submerged aquatic vegetation (SAV); Water transparency","Binary trees; Biomass; Data mining; Decision trees; Eutrophication; Image resolution; Lakes; Mean square error; Remote sensing; Satellites; Transparency; Vegetation; Classification accuracy; Root mean square errors; Satellite remote sensing; Shallow lakes; Spectral decomposition algorithm; Spectral mixture analysis; Submerged aquatic vegetations; Water transparency; Ecology",2-s2.0-85029689811
"Lewis M.G., Guddattu V., Kamath A., Biju S., Noronha J., Nayak B., Nair N.S.","Pooling of effect estimates obtained from various study designs in systematic reviews of public health interventions: A Bayesian approach to meta-analysis",2017,"Clinical Epidemiology and Global Health",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008462052&doi=10.1016%2fj.cegh.2016.12.001&partnerID=40&md5=0ac2d48e7be6af9b502f81134414ec5a","Background Randomised controlled trials (RCTs) are gold standard in assessing the effectiveness of a clinical intervention because of their high internal validity. However, the same does not hold true for interventions conducted at the population level like public health interventions. Well-designed RCTs are not easy to conduct at population level. Similarly, well planned, high-quality non-RCTs or observational studies can complement RCTs. Because of this, several systematic reviews of public health interventions are assessed with other study designs, namely non-RCTs and observational studies. In such situations, studies of similar study design are pooled together to obtain an overall effect estimate. This is inevitable, because the principle of meta-analysis does not offer an opportunity for combining effect estimates coming from various study designs. If the meta-analysis performed for each study design provides contrasting results, then this introduces a quandary for the decision makers and public health policy makers to call for a decision. Objective The present study aims to integrate the results coming from a variety of study designs in order to obtain a single estimate of effect of intervention. Methodology Bayesian approach to meta-analysis was used by formulating prior distribution from observational studies or non-RCTs and likelihood function from RCTs. Five systematic reviews of public health intervention were used to demonstrate the methodology. Results/conclusions By formulating prior distribution from observational studies, the posterior estimates were found to be different than that from the results of RCTs or other study designs. The posterior pooled-estimate was found to be precise and the width of the credible interval narrowed. Inclusion of the relevant observational studies (or non-RCTs) in the systematic review is a potential advantage for evaluating the effectiveness of public health intervention. © 2016 INDIACLEN","Bayesian meta-analysis; Non-randomised controlled trials; Observational studies; Public health interventions; Randomised controlled trials","alarm monitor; Article; Bayes theorem; cataract; clinical effectiveness; data mining; depression; evidence based medicine; health care; home safety; human; injury; meta analysis (topic); observational study; priority journal; public health; randomized controlled trial (topic); risk assessment; smoke; study design; systematic review; weight gain",2-s2.0-85008462052
"Sartini L., Besio G., Cassola F.","Spatio-temporal modelling of extreme wave heights in the Mediterranean Sea",2017,"Ocean Modelling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026824913&doi=10.1016%2fj.ocemod.2017.07.001&partnerID=40&md5=159254d5b188f1a0c10b329eaf3b841e","This study reveals characteristics of the Mediterranean Seas wave climate by illustrating the results of a spatial assessment of extreme significant wave heights. The assessment was based on a 37-year wind and wave hindcast database covering the entire area at 10-km resolution. A point-wise GEV (Generalized Extreme Values) model was employed to generate the assessments results. Overall, the spatial model proved capable of providing an accurate description of extreme return levels of significant wave heights and their spatial variability, especially on a basin-wide scale and with greatest precision on the mesoscale. However, return level estimates are found less reliable in certain coastal areas because the traditional point-wise approach is not refined enough to address the entire wave spectrum of an area as complex as the Mediterranean Sea. Therefore, MSLP (Mean Sea Level Pressure) fields were incorporated as covariates to improve localized assessment. These covariates represent meteorological forcing and allow analysis of the role of different cyclonic regimes in defining wave features and their spatial variability. Finally, the broad temporal span and high spatial resolution of the hindcast database allow for EOF (Empirical Orthogonal Function) and CCA (Canonical Correlation Analysis) analysis of wind and waves fields. This analysis validates spatial wave distribution assessment, revealing that the main processes governing the Mediterranean Seas wave climate can be attributed to four main modes. © 2017 Elsevier Ltd","Composite likelihood; Data mining; GEV; Mediterranean Sea; Multiple scales; Spatial extremes","Data mining; Orthogonal functions; Sea level; Water waves; Canonical correlation analysis; Composite likelihood; Empirical Orthogonal Function; Generalized extreme value; Mediterranean sea; Multiple scale; Spatial extremes; Spatio-temporal modelling; Ocean currents; data mining; hindcasting; probability; sea level pressure; significant wave height; spatial resolution; spatial variation; spatiotemporal analysis; wave climate; wave field; wave spectrum; wind field; Mediterranean Sea",2-s2.0-85026824913
"Ashok Kumar P.M., Vaidehi V.","A transfer learning framework for traffic video using neuro-fuzzy approach",2017,"Sadhana - Academy Proceedings in Engineering Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026840811&doi=10.1007%2fs12046-017-0705-x&partnerID=40&md5=0ebbcc9f6376717ad2a38976d59dfb4d","One of the main challenges in the Traffic Anomaly Detection (TAD) system is the ability to deal with unknown target scenes. As a result, the TAD system performs less in detecting anomalies. This paper introduces a novelty in the form of Adaptive Neuro-Fuzzy Inference System-Lossy-Count-based Topic Extraction (ANFIS-LCTE) for classification of anomalies in source and target traffic scenes. The process of transforming the input variables, learning the semantic rules in source scene and transferring the model to target scene achieves the transfer learning property. The proposed ANFIS-LCTE transfer learning model consists of four steps. (1) Low level visual items are extracted only for motion regions using optical flow technique. (2) Temporal transactions are created using aggregation of visual items for each set of frames. (3) An LCTE is applied for each set of temporal transaction to extract latent sequential topics. (4) ANFIS training is done with the back-propagation gradient descent method. The proposed ANFIS model framework is tested on standard dataset and performance is evaluated in terms of training performance and classification accuracies. Experimental results confirm that the proposed ANFIS-LCTE approach performs well in both source and target datasets. © 2017, Indian Academy of Sciences.","ANFIS-LCTE; knowledge transfer; low-level features; topic extraction; traffic anomaly","Adaptive optics; Backpropagation; Classification (of information); Data mining; Fuzzy neural networks; Fuzzy systems; Knowledge management; Semantics; ANFIS-LCTE; Knowledge transfer; Low-level features; Topic extraction; Traffic anomalies; Fuzzy inference",2-s2.0-85026840811
"Wu Y., Minkus T., Ross K.","Taking the pulse of US college campuses with location-based anonymous mobile apps",2017,"ACM Transactions on Intelligent Systems and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029784828&doi=10.1145%2f3078843&partnerID=40&md5=6b219e4e336ed5a58d7df64c41c35e14","We deploy GPS hacking in conjunction with location-based mobile apps to passively survey users in targeted geographical regions. Specifically, we investigate surveying students at different college campuses with Yik Yak, an anonymous mobile app that is popular on US college campuses. In addition to being campus centric, Yik Yak's anonymity allows students to express themselves candidly without self-censorship. We collect nearly 1.6 million Yik Yak messages (""yaks"") from a diverse set of 45 college campuses in the United States.We use natural language processing to determine the sentiment (positive, negative, or neutral) of all of the yaks. We employ supervised machine learning to predict the gender of the authors of the yaks and then analyze how sentiment differs among the two genders on college campuses.We also use supervised machine learning to classify all the yaks into nine topics and then investigate which topics are most popular throughout the US and how topic popularity varies on the different campuses. The results in this article provide significant insight into how campus culture and student's thinking varies among US colleges and universities. © 2017 ACM.","Data mining; Social networks","Artificial intelligence; Data mining; Education; Geographical regions; Learning algorithms; Learning systems; Natural language processing systems; Personal computing; Social networking (online); Supervised learning; Surveys; Campus cultures; College campus; Colleges and universities; Location based; Mobile app; Mobile apps; Supervised machine learning; Students",2-s2.0-85029784828
"Qin Y., Yu Z., Wang Y., Gao S., Shi L.","Detecting micro-blog user interest communities through the integration of explicit user relationship and implicit topic relations",2017,"Science China Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018792180&doi=10.1007%2fs11432-015-0899-6&partnerID=40&md5=d0e881fc043c0f48e237c0e7853c4ba3","In order to effectively utilize the explicit user relationship and implicit topic relations for the detection of micro-blog user interest communities, a micro-blog user interest community (MUIC) detection approach is proposed. First, through the analysis of the follow relationship between users, we have defined three types of such relationships to construct the user follow-ship network. Second, taking the semantic correlation between user tags into account, we construct the user interest feature vectors based on the concept of feature mapping to build a user tag based interest relationship network. Third, user behaviors, such as reposting, commenting, replying, and receiving comments from others, are able to provide certain guidance for the extraction of micro-blog topics. Hence, we propose to integrate the four mentioned user behaviors that are considered to provide guidance information for the traditional latent Dirichlet allocation (LDA) model. Thereby, in addition to the construction of a topic-based interest relationship network, a guided topic model can be built to extract the topics in which the user is interested. Finally, with the integration of the afore-mentioned three types of relationship network, a micro-blog user interest relationship network can be created. Meanwhile, we propose a MUIC detection algorithm based on the contribution of the neighboring nodes. The experiment result proves the effectiveness of our approach in detecting MUICs. © 2017, Science China Press and Springer-Verlag Berlin Heidelberg.","contribution of the neighboring nodes; feature mapping; guided topic model; implicit topic","Blogs; Data mining; Mapping; Semantics; Statistics; Detection algorithm; Detection approach; Feature mapping; implicit topic; Latent dirichlet allocations; Neighboring nodes; Relationship networks; Topic Modeling; Behavioral research",2-s2.0-85018792180
"Miah S.J., Gammack J., Hasan N.","Extending the framework for mobile health information systems Research: A content analysis",2017,"Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018182670&doi=10.1016%2fj.is.2017.04.001&partnerID=40&md5=77ba9c712665f24697c455138c0bf60b","Whilst researchers and professionals recognise that mobile health (M-health) systems offer unprecedented opportunities, most existing work has comprised individual project-based developments in specialised areas. Existing review articles generally utilise medical literature and categories: none investigates M-health from an information systems (IS) design point of view. Identifying application areas, design issues and IS research techniques will demonstrate models, issues, approaches and gaps to inform future research. A comprehensive analysis of the literature from this viewpoint is thus valuable, both for theoretical progression and for guiding real-world innovative system developments. Drawing from key IS and healthcare multidisciplinary journals we analyse recent (2010–2016) articles concerning M-health application developments and their associated design or development issues, with particular focus on the use of contemporary research methods. Our analysis suggests that M-health is an emerging field to which, although underused, contemporary approaches such as design science research are particularly appropriate. We identify eight application categories, eleven design issues (security, privacy, literacy, accessibility, acceptability, reliability, usability, confidentiality, integrity, knowledge sharing and flexibility) as well as the stakeholders and development techniques involved. This goes beyond previous frameworks, and theoretically integrates the central role of IS design within the sub-field. © 2017 Elsevier Ltd","Content analysis; Design science; Information systems design methodologies; M-health; Mobile-based innovations","Computer supported cooperative work; Data mining; Design; Health; Information systems; mHealth; Application development; Comprehensive analysis; Content analysis; Design science; Design-science researches; Development technique; Health information systems; Mobile Health (M-Health); Medical information systems",2-s2.0-85018182670
"Fu H.-Y., Hu O., Zhang Y.-M., Zhang L., Song J.-J., Lu P., Zheng Q.-X., Liu P.-P., Chen Q.-S., Wang B., Wang X.-Y., Han L., Yu Y.-J.","Mass-spectra-based peak alignment for automatic nontargeted metabolic profiling analysis for biomarker screening in plant samples",2017,"Journal of Chromatography A",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025823649&doi=10.1016%2fj.chroma.2017.07.044&partnerID=40&md5=71255fcf64b6c5172cfd17cd332f897c","Nontargeted metabolic profiling analysis is a difficult task in a routine investigation because hundreds of chromatographic peaks are eluted within a short time, and the time shift problem is severe across samples. To address these problems, the present work developed an automatic nontargeted metabolic profiling analysis (anTMPA) method. First, peaks from the total ion chromatogram were extracted using modified multiscale Gaussian smoothing method. Then, a novel peak alignment strategy was employed based on the mass spectra and retention times of the peaks in which the maximum mass spectral correlation coefficient path was extracted using a modified dynamic programming method. Moreover, an automatic landmark peak-searching strategy was employed for self-adapting time shift modification. Missing peaks across samples were grouped and registered into the aligned peak list table for final refinement. Finally, the aligned peaks across samples were analyzed using statistical methods to identify potential biomarkers. Mass spectral information on the screened biomarkers could be directly imported into the National Institute of Standards and Technology library to select the candidate compounds. The performance of the anTMPA method was evaluated using a complicated plant gas chromatography–mass spectrometry dataset with the aim of identifying biomarkers between the growth and maturation stages of the tested plant. © 2017 Elsevier B.V.","Automatic data mining; Biomarker; Complicated sample analysis; Nontargeted metabolic profiling; Plant sample","Chromatographic analysis; Chromatography; Dynamic programming; Gas chromatography; Mass spectrometry; Metabolism; Screening; Chromatographic peaks; Dynamic programming methods; Gas chromatography-mass spectrometry; Gaussian smoothing; Metabolic profiling; National Institute of Standards and Technology; Plant samples; Sample analysis; Biomarkers; analytic method; Article; automatic nontargeted metabolic profiling analysis; controlled study; limit of quantitation; mass fragmentography; measurement accuracy; metabolic parameters; normal distribution; plant growth; priority journal; retention time",2-s2.0-85025823649
"Joshi A., Bhattacharyya P., Carman M.J.","Automatic sarcasm detection: A survey",2017,"ACM Computing Surveys",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030700921&doi=10.1145%2f3124420&partnerID=40&md5=eb2942f813a3e0fa00e8680b1fe93cca","Automatic sarcasm detection is the task of predicting sarcasm in text. This is a crucial step to sentiment analysis, considering prevalence and challenges of sarcasm in sentiment-bearing text. Beginning with an approach that used speech-based features, automatic sarcasm detection has witnessed great interest from the sentiment analysis community. This article is a compilation of past work in automatic sarcasm detection. We observe three milestones in the research so far: semi-supervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and incorporation of context beyond target text. In this article, we describe datasets, approaches, trends, and issues in sarcasm detection. We also discuss representative performance values, describe shared tasks, and provide pointers to future work, as given in prior works. In terms of resources to understand the state-of-the-art, the survey presents several useful illustrations - most prominently, a table that summarizes past papers along different dimensions such as the types of features, annotation techniques, and datasets used. © 2017 ACM.","Opinion; Sarcasm; Sarcasm detection; Sentiment; Sentiment analysis","Data mining; Opinion; Pattern extraction; Performance value; Sarcasm; Semi-supervised; Sentiment; Sentiment analysis; State of the art; Surveys",2-s2.0-85030700921
"Appel O., Chiclana F., Carter J., Fujita H.","A Consensus Approach to the Sentiment Analysis Problem Driven by Support-Based IOWA Majority",2017,"International Journal of Intelligent Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012917935&doi=10.1002%2fint.21878&partnerID=40&md5=8b15cb152baffedf089f47312b8a83a3","In group decision making, there are many situations where the opinion of the majority of participants is critical. The scenarios could be multiple, like a number of doctors finding commonality on the diagnose of an illness or parliament members looking for consensus on an specific law being passed. In this article, we present a method that utilizes induced ordered weighted averaging (IOWA) operators to aggregate a majority opinion from a number of sentiment analysis (SA) classification systems, where the latter occupy the role usually taken by human decision-makers as typically seen in group decision situations. In this case, the numerical outputs of different SA classification methods are used as input to a specific IOWA operator that is semantically close to the fuzzy linguistic quantifier ‘most of’. The object of the aggregation will be the intensity of the previously determined sentence polarity in such a way that the results represent what the majority think. During the experimental phase, the use of the IOWA operator coupled with the linguistic quantifier ‘most’ ((IOWAmost) proved to yield superior results compared to those achieved when utilizing other techniques commonly applied when some sort of averaging is needed, such as arithmetic mean or median techniques. © 2017 Wiley Periodicals, Inc.",,"Classification (of information); Data mining; Decision making; Linguistics; Numerical methods; Classification methods; Classification system; Fuzzy linguistic quantifiers; Group Decision Making; Induced ordered weighted averaging operators; Linguistic quantifiers; Numerical output; Sentiment analysis; Fuzzy inference",2-s2.0-85012917935
"Liu Y., Peng J., Wang Y.","Diversification of land surface temperature change under urban landscape renewal: A case study in the main city of Shenzhen, China",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029379538&doi=10.3390%2frs9090919&partnerID=40&md5=f627c6dc26481d6ba7942464223402d5","Unprecedented rapid urbanization in China during the past several decades has been accompanied by extensive urban landscape renewal, which has increased the urban thermal environmental risk. However, landscape change is a sufficient but not necessary condition for land surface temperature (LST) variation. Many studies have merely highlighted the correlation between landscape pattern and LST, while neglecting to comprehensively present the spatiotemporal diversification of LST change under urban landscape renewal. Taking the main city of Shenzhen as a case study area, this study tracked the landscape renewal and LST variation for the period 1987-2015 using 49 Landsat images. A decision tree algorithm suitable for fast landscape type interpretation was developed to map the landscape renewal. Analytical tools that identified hot-cold spots, the gravity center, and transect of LST movement were adopted to identify LST changes. The results showed that the spatial variation of LST was not completely consistent with landscape change. The transformation from Green landscape to Grey landscape usually increased the LST within a median of 0.2 °C, while the reverse transformation did not obviously decrease the LST (the median was nearly 0 °C). The median of LST change from Blue landscape to Grey landscape was 1.0 °C, corresponding to 0.5 °C in the reverse transformation. The imbalance of LST change between the loss and gain of Green or Blue landscape indicates the importance of protecting natural space, where the benefits in terms of temperature mitigation cannot be completely substituted by reverse transformation. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Decision tree; Landscape transformation; Temperature mitigation; Urbanization","Data mining; Decision trees; Geomorphology; Surface measurement; Surface properties; Trees (mathematics); Decision-tree algorithm; Environmental risks; Land surface temperature; Landscape transformation; Rapid urbanizations; Reverse Transformation; Spatial variations; Urbanization; Atmospheric temperature",2-s2.0-85029379538
"Nissim N., Shahar Y., Elovici Y., Hripcsak G., Moskovitch R.","Inter-labeler and intra-labeler variability of condition severity classification models using active and passive learning methods",2017,"Artificial Intelligence in Medicine",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018180910&doi=10.1016%2fj.artmed.2017.03.003&partnerID=40&md5=475fd517407a2642819f349967397af3","Background and objectives Labeling instances by domain experts for classification is often time consuming and expensive. To reduce such labeling efforts, we had proposed the application of active learning (AL) methods, introduced our CAESAR-ALE framework for classifying the severity of clinical conditions, and shown its significant reduction of labeling efforts. The use of any of three AL methods (one well known [SVM-Margin], and two that we introduced [Exploitation and Combination_XA]) significantly reduced (by 48% to 64%) condition labeling efforts, compared to standard passive (random instance-selection) SVM learning. Furthermore, our new AL methods achieved maximal accuracy using 12% fewer labeled cases than the SVM-Margin AL method. However, because labelers have varying levels of expertise, a major issue associated with learning methods, and AL methods in particular, is how to best to use the labeling provided by a committee of labelers. First, we wanted to know, based on the labelers’ learning curves, whether using AL methods (versus standard passive learning methods) has an effect on the Intra-labeler variability (within the learning curve of each labeler) and inter-labeler variability (among the learning curves of different labelers). Then, we wanted to examine the effect of learning (either passively or actively) from the labels created by the majority consensus of a group of labelers. Methods We used our CAESAR-ALE framework for classifying the severity of clinical conditions, the three AL methods and the passive learning method, as mentioned above, to induce the classifications models. We used a dataset of 516 clinical conditions and their severity labeling, represented by features aggregated from the medical records of 1.9 million patients treated at Columbia University Medical Center. We analyzed the variance of the classification performance within (intra-labeler), and especially among (inter-labeler) the classification models that were induced by using the labels provided by seven labelers. We also compared the performance of the passive and active learning models when using the consensus label. Results The AL methods: produced, for the models induced from each labeler, smoother Intra-labeler learning curves during the training phase, compared to the models produced when using the passive learning method. The mean standard deviation of the learning curves of the three AL methods over all labelers (mean: 0.0379; range: [0.0182 to 0.0496]), was significantly lower (p = 0.049) than the Intra-labeler standard deviation when using the passive learning method (mean: 0.0484; range: [0.0275–0.0724). Using the AL methods resulted in a lower mean Inter-labeler AUC standard deviation among the AUC values of the labelers’ different models during the training phase, compared to the variance of the induced models’ AUC values when using passive learning. The Inter-labeler AUC standard deviation, using the passive learning method (0.039), was almost twice as high as the Inter-labeler standard deviation using our two new AL methods (0.02 and 0.019, respectively). The SVM-Margin AL method resulted in an Inter-labeler standard deviation (0.029) that was higher by almost 50% than that of our two AL methods The difference in the inter-labeler standard deviation between the passive learning method and the SVM-Margin learning method was significant (p = 0.042). The difference between the SVM-Margin and Exploitation method was insignificant (p = 0.29), as was the difference between the Combination_XA and Exploitation methods (p = 0.67). Finally, using the consensus label led to a learning curve that had a higher mean intra-labeler variance, but resulted eventually in an AUC that was at least as high as the AUC achieved using the gold standard label and that was always higher than the expected mean AUC of a randomly selected labeler, regardless of the choice of learning method (including a passive learning method). Using a paired t-test, the difference between the intra-labeler AUC standard deviation when using the consensus label, versus that value when using the other two labeling strategies, was significant only when using the passive learning method (p = 0.014), but not when using any of the three AL methods. Conclusions The use of AL methods, (a) reduces intra-labeler variability in the performance of the induced models during the training phase, and thus reduces the risk of halting the process at a local minimum that is significantly different in performance from the rest of the learned models; and (b) reduces Inter-labeler performance variance, and thus reduces the dependence on the use of a particular labeler. In addition, the use of a consensus label, agreed upon by a rather uneven group of labelers, might be at least as good as using the gold standard labeler, who might not be available, and certainly better than randomly selecting one of the group's individual labelers. Finally, using the AL methods: when provided by the consensus label reduced the intra-labeler AUC variance during the learning phase, compared to using passive learning. © 2017 Elsevier B.V.","Active learning; Condition; Electronic health records; Labeling; Phenotyping; Severity; Variance","Artificial intelligence; Gold; Labeling; Learning systems; Patient monitoring; Patient treatment; Statistics; Active Learning; Condition; Electronic health record; Phenotyping; Severity; Variance; Aluminum; accuracy; Article; classification; conceptual framework; data mining; electronic health record; gold standard; learning curve; machine learning; mathematical model; priority journal; risk reduction; support vector machine",2-s2.0-85018180910
"Puranam D., Narayan V., Kadiyali V.","The effect of calorie posting regulation on consumer opinion: A flexible latent dirichlet allocation model with informative priors",2017,"Marketing Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031404921&doi=10.1287%2fmksc.2017.1048&partnerID=40&md5=3b6d0dad3d274d0f7728b449643fe442","In 2008, New York City mandated that all chain restaurants post calorie information on their menus. For managers of chain and standalone restaurants, as well as for policy makers, a pertinent goal might be to monitor the impact of this regulation on consumer conversations. We propose a scalable Bayesian topic model to measure and understand changes in consumer opinion about health (and other topics).We calibrate the model on 761,962 online reviews of restaurants posted over eight years. Our model allows managers to specify prior topics of interest such as “health” for a calorie posting regulation. It also allows the distribution of topic proportions within a review to be affected by its length, valence, and the experience level of its author. Using a difference-in-differences estimation approach, we isolate the potentially causal effect of the regulation on consumer opinion. Following the regulation, there was a statistically small but significant increase in the proportion of discussion of the health topic. This increase can be attributed largely to authors who did not post reviews before the regulation, suggesting that the regulation prompted several consumers to discuss health in online restaurant reviews. © 2017 INFORMS.","Bayesian estimation; Data mining; Word-of-mouth",,2-s2.0-85031404921
"Zhao Y., Liu B., He L., Bai W., Yu X., Cao X., Luo L., Rong P., Zhao Y., Li G., Liu B.","A novel classification method for aid decision of traditional Chinese patent medicines for stroke treatment",2017,"Frontiers of Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019171533&doi=10.1007%2fs11684-017-0511-1&partnerID=40&md5=c66bd665cbff802ce84e133fdd78d70d","Traditional Chinese patent medicines are widely used to treat stroke because it has good efficacy in the clinical environment. However, because of the lack of knowledge on traditional Chinese patent medicines, many Western physicians, who are accountable for the majority of clinical prescriptions for such medicine, are confused with the use of traditional Chinese patent medicines. Therefore, the aid-decision method is critical and necessary to help Western physicians rationally use traditional Chinese patent medicines. In this paper, Manifold Ranking is employed to develop the aid-decision model of traditional Chinese patent medicines for stroke treatment. First, 115 stroke patients from three hospitals are recruited in the cross-sectional survey. Simultaneously, traditional Chinese physicians determine the traditional Chinese patent medicines appropriate for each patient. Second, particular indicators are explored to characterize the population feature of traditional Chinese patent medicines for stroke treatment. Moreover, these particular indicators can be easily obtained byWestern physicians and are feasible for widespread clinical application in the future. Third, the aid-decision model of traditional Chinese patent medicines for stroke treatment is constructed based on Manifold Ranking. Experimental results reveal that traditional Chinese patent medicines can be differentiated. Moreover, the proposed model can obtain high accuracy of aid decision. © 2017, Higher Education Press and Springer-Verlag Berlin Heidelberg.","aid decision; data mining; manifold ranking; stroke; traditional Chinese patent medicines","classification; data mining; disease model; experimental model; female; hospital; human; major clinical study; male; patent; physician; stroke patient",2-s2.0-85019171533
"Correa Marrero M., van Dijk A.D.J., de Ridder D.","Sequence-based analysis of protein degradation rates",2017,"Proteins: Structure, Function and Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020463589&doi=10.1002%2fprot.25323&partnerID=40&md5=6171a426fc69bdefeb91749950bc1f68","Protein turnover is a key aspect of cellular homeostasis and proteome dynamics. However, there is little consensus on which properties of a protein determine its lifetime in the cell. In this work, we exploit two reliable datasets of experimental protein degradation rates to learn models and uncover determinants of protein degradation, with particular focus on properties that can be derived from the sequence. Our work shows that simple sequence features suffice to obtain predictive models of which the output correlates reasonably well with the experimentally measured values. We also show that intrinsic disorder may have a larger effect than previously reported, and that the effect of PEST regions, long thought to act as specific degradation signals, can be better explained by their disorder. We also find that determinants of protein degradation depend on the cell types or experimental conditions studied. This analysis serves as a first step towards the development of more complex, mature computational models of degradation of proteins and eventually of their full life cycle. Proteins 2017; 85:1593–1601. © 2017 Wiley Periodicals, Inc. © 2017 Wiley Periodicals, Inc.","data mining; intrinsic disorder; machine learning; multivariate regression; protein metabolism; protein turnover; proteolysis; sequence analysis; support vector machine","amino acid sequence; amino terminal sequence; Article; cell membrane; cellular distribution; controlled study; correlational study; enzyme specificity; priority journal; protein degradation; sequence alignment; sequence analysis; sequence homology; algorithm; chemistry; genetics; sequence analysis; protein; proteome; Algorithms; Amino Acid Sequence; Proteins; Proteolysis; Proteome; Sequence Analysis, Protein",2-s2.0-85020463589
"Hernández-Solana Á., Perez-Diaz-De-Cerio D., Valdovinos A., Valenzuela J.L.","Proposal and evaluation of BLE discovery process based on new features of bluetooth 5.0",2017,"Sensors (Switzerland)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028693089&doi=10.3390%2fs17091988&partnerID=40&md5=f97512237947e54f59a418698b1a66f3","The device discovery process is one of the most crucial aspects in real deployments of sensor networks. Recently, several works have analyzed the topic of Bluetooth Low Energy (BLE) device discovery through analytical or simulation models limited to version 4.x. Non-connectable and non-scannable undirected advertising has been shown to be a reliable alternative for discovering a high number of devices in a relatively short time period. However, new features of Bluetooth 5.0 allow us to define a variant on the device discovery process, based on BLE scannable undirected advertising events, which results in higher discovering capacities and also lower power consumption. In order to characterize this new device discovery process, we experimentally model the real device behavior of BLE scannable undirected advertising events. Non-detection packet probability, discovery probability, and discovery latency for a varying number of devices and parameters are compared by simulations and experimental measurements. We demonstrate that our proposal outperforms previous works, diminishing the discovery time and increasing the potential user device density. A mathematical model is also developed in order to easily obtain a measure of the potential capacity in high density scenarios. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","BLE; Discovery latency; Internet of Things (IoT); Neighbor discovery; Non-detection probability","Bluetooth; Internet of things; Marketing; Probability; Sensor networks; Bluetooth low energies (BLE); Device discovery; Discovery latency; Internet of Things (IOT); Lower-power consumption; Neighbor discovery; Non-detection; Potential capacity; Data mining",2-s2.0-85028693089
"Calude C.S., Longo G.","The Deluge of Spurious Correlations in Big Data",2017,"Foundations of Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960126500&doi=10.1007%2fs10699-016-9489-4&partnerID=40&md5=aec97fac79bc9d14d46949eaa68e1885","Very large databases are a major opportunity for science and data analytics is a remarkable new field of investigation in computer science. The effectiveness of these tools is used to support a “philosophy” against the scientific method as developed throughout history. According to this view, computer-discovered correlations should replace understanding and guide prediction and action. Consequently, there will be no need to give scientific meaning to phenomena, by proposing, say, causal relations, since regularities in very large databases are enough: “with enough data, the numbers speak for themselves”. The “end of science” is proclaimed. Using classical results from ergodic theory, Ramsey theory and algorithmic information theory, we show that this “philosophy” is wrong. For example, we prove that very large databases have to contain arbitrary correlations. These correlations appear only due to the size, not the nature, of data. They can be found in “randomly” generated, large enough databases, which—as we will prove—implies that most correlations are spurious. Too much information tends to behave like very little information. The scientific method can be enriched by computer mining in immense databases, but not replaced by it. © 2016, Springer Science+Business Media Dordrecht.","Algorithmic information theory; Big data; Correlation; Ergodic theory; Ramsey theory","Correlation methods; Database systems; Graph theory; Information theory; Ontology; Algorithmic information theory; Arbitrary correlation; Causal relations; Data analytics; Ergodic theory; Ramsey theory; Scientific method; Very large database; Big data",2-s2.0-84960126500
"Nilsson L., Widerlund A.","Tracing nitrogen cycling in mining waters using stable nitrogen isotope analysis",2017,"Applied Geochemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020263429&doi=10.1016%2fj.apgeochem.2017.05.025&partnerID=40&md5=dda96012c830bacedd29b261e4f43b12","We show how we used stable nitrogen and oxygen isotopes in ammonium and nitrate to identify and quantify nitrogen transformation and nitrogen sources at the LKAB mining site in northern Sweden. Stable nitrogen isotope analysis worked as an excellent tool for tracing nitrogen cycling in rapidly moving process waters. The isotope analysis was performed on the mining process waters at seven different key points along the water flow and we identified nitrification, ammonia volatilisation, and ammonium adsorption as nitrogen transformation processes. The source of nitrogen is historically explained as undetonated ammonium-nitrate based explosives. We used nitrate nitrogen and oxygen isotopes to quantify four nitrogen sources in the accumulated water in the mine as well as three sources in an above ground process water reservoir. The nitrate isotope data showed that most of the nitrate (70–80%) in the accumulated water underground originated from a sampling point located close to the surface and only a minor fraction (5–20%) originated directly from undetonated explosives (direct dissolution of NH4NO3 and nitrification of NH4). Nitrate from natural groundwater formed roughly 12% of mine water nitrate. In the above ground process water reservoir isotope data indicated another source of nitrogen coming from undetonated explosives. © 2017","Isotope geochemistry; Isotopes; Kiruna; Mining; Nitrogen; Nitrogen cycling; Process-water; Stable nitrogen isotopes; Stable oxygen isotopes; Tracing","Exploratory geochemistry; Explosives; Flow of water; Groundwater; Groundwater geochemistry; Isotopes; Mining; Nitrates; Nitrification; Nitrogen; Oxygen; Reservoirs (water); Isotope geochemistry; Kiruna; Nitrogen cycling; Process water; Stable nitrogen isotope; Stable oxygen isotopes; Tracing; Gas adsorption; ammonium nitrate; dissolution; explosive; formation water; geochemical survey; groundwater flow; mining industry; nitrification; nitrogen cycle; oxygen isotope; stable isotope; volatilization; Sweden",2-s2.0-85020263429
"Isidro C.M., McIntyre N., Lechner A.M., Callow I.","Applicability of earth observation for identifying small-scale mining footprints in a wet tropical region",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029365351&doi=10.3390%2frs9090945&partnerID=40&md5=20ffec33e94df65dcc7d75cc1320ef3b","The unpredictable climate in wet tropical regions along with the spatial resolution limitations of some satellite imageries make detecting and mapping artisanal and small-scale mining (ASM) challenging. The objective of this study was to test the utility of Pleiades and SPOT imagery with an object-based support vector machine (OB-SVM) classifier for the multi-temporal remote sensing of ASM and other land cover including a large-scale mine in the Didipio catchment in the Philippines. Historical spatial data on location and type of ASM mines were collected from the field and were utilized as training data for the OB-SVM classifier. The classification had an overall accuracy between 87% and 89% for the three different images-Pleiades-1A for the 2013 and 2014 images and SPOT-6 for the 2016 image. The main land use features, particularly the Didipio large-scale mine, were well identified by the OB-SVM classifier, however there were greater commission errors for the mapping of small-scale mines. The lack of consistency in their shape and their small area relative to pixel sizes meant they were often not distinguished from other land clearance types (i.e., open land). To accurately estimate the total area of each land cover class, we calculated bias-adjusted surface areas based on misclassification values. The analysis showed an increase in small-scale mining areas from 91,000 m2-or 0.2% of the total catchment area-in March 2013 to 121,000 m2-or 0.3%-in May 2014, and then a decrease to 39,000 m2-or 0.1%-in January 2016. © 2017 by the authors.","Artisanal and small-scale mining; Didipio catchment; Geographic-object-based image analysis; Object-based support vector machine; Philippines; Wet tropical region","Catchments; Classification (of information); Land use; Remote sensing; Runoff; Satellite imagery; Support vector machines; Tropical engineering; Tropics; Artisanal and small scale mining; Geographic object-based image analysis; Object based; Philippines; Tropical regions; Mapping",2-s2.0-85029365351
"Elbaz-Poulichet F., Resongles E., Bancon-Montigny C., Delpoux S., Freydier R., Casiot C.","The environmental legacy of historic Pb-Zn-Ag-Au mining in river basins of the southern edge of the Massif Central (France)",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024484578&doi=10.1007%2fs11356-017-9669-y&partnerID=40&md5=b0129e927cfaeaa8d6461ae5292a9c9d","The main rivers (Aude, Orb, Herault) that discharge into the Gulf of Lions and the west bank tributaries of the Rhone River including the Gardon have former non-ferrous metal mines in their upper drainage basin. Using unpublished data and data from the literature, this study provides an integrated overview of the contamination of water and sediment along the continent-sea continuum and of its impacts on the biota and on human health. In the upper part of these basins, water and stream sediments are enriched in metal(-loids) compared to median European concentrations. Arsenic is the main contaminant in the rivers Aude and Gardon d’Anduze, Sb in the Orb and Gardon d’Alès, and Tl in the Herault river. A rapid reduction in dissolved and particulate concentrations was systematically observed along the river due to dilution and precipitation. The high concentrations of metal(-loid)s observed suggest that the former mining activity still represents a potential threat for the environment, but the lack of high temporal resolution monitoring, especially during Mediterranean floods, prevents accurate assessment of metal fluxes from these rivers to the Mediterranean Sea. Studies dedicated to the impacts on human health are too rare, given that studies have shown a higher rate of arsenic-specific cancer near Salsigne mine in the Aude River basin and cases of saturnism in children in the upper Herault River basin. These studies underline the need to take environmental health issues into consideration not only in these watersheds but around the entire Mediterranean basin, which harbors numerous metalliferous ores that have been mined for millennia. © 2017, Springer-Verlag GmbH Germany.","Historical mining legacy; Mediterranean continent-sea continuum; Metalloids; Metals","cancer; concentration (composition); drainage basin; environmental issue; gold; lead; Mediterranean environment; metal; metalloid; mining; public health; river basin; river discharge; silver; watershed; zinc; France; Massif Central; Mediterranean Sea; Rhone River",2-s2.0-85024484578
"Yusof S.M., Sidi F., Ibrahim H., Affendey L.S.","Data warehouse conceptual design-a literature survey",2017,"Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029806315&doi=10.3923%2fjeasci.2017.4208.4214&partnerID=40&md5=be4f5a16b0b9b4807221d61d74d7ac53","Data Warehouse (DW) can provide an excellent approach in transforming Online Transaction Processing (OLTP) data into useful and reliable information to support organization's decision making. As such, it can be a basis for data analysis techniques such as multidimensional analysis and data mining. However, DW design process has been known as a complex task that requires systematic and structured approach to guarantee its success. Therefore, there have been various methodologies proposed to carry out the design process which can be classified into requirement-driven, data-driven and hybrid approaches. In this study, a literature survey was made to obtain related works by a set of pertinent key words related to DW conceptual design. The objective of this survey is to provide the state of the art of DW conceptual design methodologies in narrative and summarized forms. The main contribution is to provide understanding of the trend, issues and solutions proposed to date in DW conceptual design and along the way to discover the novel and great contribution works that form an important basis in the DW conceptual design. © Medwell Journals, 2017.","Conceptual design; Data warehouse; DW conceptual; Multidimensional model; Provide; Surve",,2-s2.0-85029806315
"Prajapati D.J., Garg S., Chauhan N.C.","MapReduce Based Multilevel Consistent and Inconsistent Association Rule Detection from Big Data Using Interestingness Measures",2017,"Big Data Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028454921&doi=10.1016%2fj.bdr.2017.07.001&partnerID=40&md5=4bdea1be5dd3702010c8b5764c312b8f","Multilevel association rule mining in distributed environment plays an important role in big data analysis for making marketing strategy. Multilevel association rule provides more significant information than single level rule, and also discovers the conceptual hierarchy of knowledge from the hierarchical dataset. In this era of internet, various online marketing sites and social networking sites are generating enormous amount of data so that it becomes very difficult to process and analyze it using conventional approaches as it consumes more time. This paper overcomes the computing limitation of single node by distributing the task on multi-node cluster. The proposed method initially extracts multilevel association rules including level-crossing for each zone using distributed multilevel frequent pattern mining algorithm (DMFPM). These generated multilevel association rules are so large that it becomes complex to analyze it. Thus, MapReduce based multilevel consistent and inconsistent rule detection (MR-MCIRD) algorithm is proposed to detect the consistent and inconsistent multilevel rules from big hierarchical data which provide useful and actionable knowledge to the domain experts. These pruned interesting rules also give useful knowledge for better marketing strategy. The extracted multilevel consistent and inconsistent rules are evaluated and compared based on different interestingness measures presented together with experimental results that lead to the final conclusions. © 2017 Elsevier Inc.","Distributed frequent pattern mining; Interestingness measures; MapReduce; Multilevel association rules",,2-s2.0-85028454921
"Gullo F., Ponti G., Tagarelli A., Greco S.","An information-theoretic approach to hierarchical clustering of uncertain data",2017,"Information Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016950791&doi=10.1016%2fj.ins.2017.03.030&partnerID=40&md5=473e899642e3bb77212c51abec6995ad","Uncertain data clustering has become central in mining data whose observed representation is naturally affected by imprecision, staling, or randomness that is implicit when storing this data from real-word sources. Most existing methods for uncertain data clustering follow a partitional or a density-based clustering approach, whereas little research has been devoted to the hierarchical clustering paradigm. In this work, we push forward research in hierarchical clustering of uncertain data by introducing a well-founded solution to the problem via an information-theoretic approach, following the initial idea described in our earlier work [26]. We propose a prototype-based agglomerative hierarchical clustering method, dubbed U-AHC, which employs a new uncertain linkage criterion for cluster merging. This criterion enables the comparison of (sets of) uncertain objects based on information-theoretic as well as expected-distance measures. To assess our proposal, we have conducted a comparative evaluation with state-of-the-art algorithms for clustering uncertain objects, on both benchmark and real datasets. We also compare with two basic definitions of agglomerative hierarchical clustering that are treated as baseline methods in terms of accuracy and efficiency of the clustering results, respectively. Main experimental findings reveal that U-AHC generally outperforms competing methods in accuracy and, from an efficiency viewpoint, is comparable to the fastest baseline version of agglomerative hierarchical clustering. © 2017 Elsevier Inc.","Clustering; Hierarchical clustering; Information theory; Mixture models; Probability distributions; Uncertain data","Cluster analysis; Efficiency; Image segmentation; Information theory; Probability distributions; Agglomerative hierarchical clustering; Clustering; Density-based Clustering; Hier-archical clustering; Information-theoretic approach; Mixture model; State-of-the-art algorithms; Uncertain datas; Clustering algorithms",2-s2.0-85016950791
"Viswanathan M., Raj G.S.","Mining health record of the patients",2017,"International Journal of Civil Engineering and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029895438&partnerID=40&md5=42c6b2382ffcd659d979a39839f7d1cb","Data mining is the way toward getting to crude data into helpful data. Data are any certainties, numbers, or content that can be gotten to by a Computer. Here the general health examination is the principle part of the human services framework on the planet; recognize the patient before they go to chance it is the critical to keep the patient. The most essential test is taking in the grouping model for chance predication lies the patient data that constitutes most of the gathered informational collection. In the framework the unlabeled data portrays which tolerant member in the social insurance examinations whose wellbeing conditions can shift extraordinarily from solid records. There is no ground truth for separating their conditions of wellbeing. A graph based SHG-Health Semi-supervised Heterogeneous Graph on Health for risk predictions to group a logically create circumstance with the major of the data unlabeled. A productive iterative calculation is composed and the evidence of merging is given. Broad investigations in view of both health examination data sets and engineered informational indexes are performed to demonstrate the viability and effectiveness of our technique. © IAEME Publication.","Mining; Risk diseases; Shg-health semi-supervised heterogeneous graph",,2-s2.0-85029895438
"Jamróz D., Niedoba T., Surowiak A., Tumidajski T., Szostek R., Gajer M.","Application of Multi-Parameter Data Visualization by Means of Multidimensional Scaling to Evaluate Possibility of Coal Gasification",2017,"Archives of Mining Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031823230&doi=10.1515%2famsc-2017-0034&partnerID=40&md5=280a40b7e61a6b9d0b3a9acf74084d22","The application of methods drawing upon multi-parameter visualization of data by transformation of multidimensional space into two-dimensional one allow to show multi-parameter data on computer screen. Thanks to that, it is possible to conduct a qualitative analysis of this data in the most natural way for human being, i.e. by the sense of sight. An example of such method of multi-parameter visualization is multidimensional scaling. This method was used in this paper to present and analyze a set of seven-dimensional data obtained from Janina Mining Plant and Wieczorek Coal Mine. It was decided to examine whether the method of multi-parameter data visualization allows to divide the samples space into areas of various applicability to fluidal gasification process. The ""Technological applicability card for coals"" was used for this purpose [Sobolewski et al., 2012; 2013], in which the key parameters, important and additional ones affecting the gasification process were described. © 2017 Dariusz Jamróz et al., published by De Gruyter Open.","coal gasification; jigging; MDS; multidimensional data; multidimensional scaling; multidimensional visualization",,2-s2.0-85031823230
"Elfert P., Eichelberg M., Troger J., Britz J., Alexandersson J., Bieber D., Bauer J., Teichmann S., Kuhn L., Thielen M., Sauer J., Munzberg A., Rosch N., Woizischke J., Diekmann R., Hein A.","DiDiER - Digitized services in dietary counselling for people with increased health risks related to malnutrition and food allergies",2017,"Proceedings - IEEE Symposium on Computers and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030546438&doi=10.1109%2fISCC.2017.8024512&partnerID=40&md5=2167b1b07937a2d33d5c4bb86a5da963","The goal of the DiDiER project is to verifiably improve services in the field of dietary counselling. This will be achieved by digitising information to increase counselling intensity and to improve workflows for the service provider. The project will develop an IT-based support system for dietary counselling, covering two use cases, facilitation and support of the work of nutritionists in ambulatory allergological nutrition counselling and of nutritionists involved in the care of geriatric patients, especially of those with frailty. One of the project's significant features is that the user's sensitive data remain under his or her personal control at all times. © 2017 IEEE.","Advanced Medical Visualization Techniques; Assistive Technology (AT); Cloud computing applications for eHealth; Data and Visual Mining for Diagnostics; Food Allergy; Frailty; Improved Therapeutic and Rehabilitation Methods","Allergies; Data visualization; Diagnosis; Health risks; Medical computing; Assistive technology; Ehealth; Food allergies; Frailty; Medical visualization; Rehabilitation methods; Visual mining; Nutrition",2-s2.0-85030546438
"Guo K., Tang Y., Zhang P.","CSF: Crowdsourcing semantic fusion for heterogeneous media big data in the internet of things",2017,"Information Fusion",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012245087&doi=10.1016%2fj.inffus.2017.01.008&partnerID=40&md5=6b6846b6e95762daa797b3e793b3bf18","With the rising popularity of social media in the context of environments based on the Internet of things (IoT), semantic information has emerged as an important bridge to connect human intelligence with heterogeneous media big data. As a critical tool to improve media big data retrieval, semantic fusion encounters a number of challenges: the manual method is inefficient, and the automatic approach is inaccurate. To address these challenges, this paper proposes a solution called CSF (Crowdsourcing Semantic Fusion) that makes full use of the collective wisdom of social users and introduces crowdsourcing computing to semantic fusion. First, the correlation of cross-modal semantics is mined and the semantic objects are normalized for fusion. Second, we employ the dimension reduction and relevance feedback approaches to reduce non-principal components and noise. Finally, we research the storage and distribution mechanism. Experiment results highlight the efficiency and accuracy of the proposed approach. The proposed method is an effective and practical cross-modal semantic fusion and distribution mechanism for heterogeneous social media, provides a novel idea for social media semantic processing, and uses an interactive visualization framework for social media knowledge mining and retrieval to improve semantic knowledge and the effect of representation. © 2017 Elsevier B.V.","Big data; Crowdsourcing computing; Internet of things; Semantic fusion; Social media","Crowdsourcing; Digital storage; Internet of things; Semantic Web; Semantics; Social networking (online); Visualization; Automatic approaches; Distribution mechanism; Interactive visualizations; Internet of thing (IOT); Principal Components; Semantic fusion; Semantic information; Social media; Big data",2-s2.0-85012245087
"Li W., Adachi T.","Quantitative estimation of resource nationalism by binary choice logit model for panel data",2017,"Resources Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030570539&doi=10.1016%2fj.resourpol.2017.07.002&partnerID=40&md5=f2e652211bcf896408323715188ad27c","Since the beginning of this century, another new wave of resource nationalism with unprecedented high frequency and wide area had been profoundly affecting natural resources industry's room of development. It attracted increased attention among researchers and business investors. Numerous descriptive case studies on the issue indicated us how specific economic, political and other factors interacted and further drove resource nationalism strategy and policy making. However, as far as we know, none of them were able to quantitatively dig out the mutual factors across countries that work uniformly in producing resource nationalism. The objective of the study is quantifying significant factors dominating the occurrence of resource nationalism for important metal and energy resources producing countries at global level by binary choice logit model for panel data. Besides finding out the significant variables and their marginal effects to resource nationalism from 2000 to 2013, the regression helps predict up to 89 resource producing countries’, 5 types of base metals’, 4 types of precious metals’, and 3 types of energy resources’ probability of resource nationalism during 2003–2012. The study is a primary trial of researching on resource nationalism and provides some insights for theoretical building and genetic simulation on the issue. © 2017 Elsevier Ltd","Binary choice logit model; Economic explanation; Fossil energy; Metal resources; Mining countries; Resource nationalism","Bins; Energy resources; Fossil energy; High frequency HF; Logit modeling; Marginal effects; Quantitative estimation; Resource nationalism; Resources industries; Significant variables; Metals",2-s2.0-85030570539
"Janusz A., Grzegorowski M., Michalak M., Wróbel Ł., Sikora M., Ślęzak D.","Predicting seismic events in coal mines based on underground sensor measurements",2017,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021052337&doi=10.1016%2fj.engappai.2017.06.002&partnerID=40&md5=8a9bcc6541754c78d1ef10d62e4c4f39","In this paper, we address the problem of safety monitoring in underground coal mines. In particular, we investigate and compare practical methods for the assessment of seismic hazards using analytical models constructed based on sensory data and domain knowledge. For our case study, we use a rich data set collected during a period of over five years from several active Polish coal mines. We focus on comparing the prediction quality between expert methods which serve as a standard in the coal mining industry and state-of-the-art machine learning methods for mining high-dimensional time series data. We describe an international data mining challenge organized to facilitate our study. We also demonstrate a technique which we employed to construct an ensemble of regression models able to outperform other approaches used by participants of the challenge. Finally, we explain how we utilized the data obtained during the competition for the purpose of research on the cold start problem in deploying decision support systems at new mining sites. © 2017 Elsevier Ltd","Cold-start problem; Decision support systems; Feature engineering; Predictive analytics; Seismic events prediction; Time series data","Artificial intelligence; Coal; Coal industry; Decision support systems; Forecasting; Ignition; Learning systems; Predictive analytics; Regression analysis; Seismology; Starting; Time series; Coal mining industry; Cold start problems; Feature engineerings; Seismic event; Sensor measurements; State-of-the-art machine learning methods; Time-series data; Underground coal mine; Coal mines",2-s2.0-85021052337
"Hargrave C.O., James C.A., Ralston J.C.","Infrastructure-based localisation of automated coal mining equipment",2017,"International Journal of Coal Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028041769&doi=10.1007%2fs40789-017-0180-3&partnerID=40&md5=20ded552d604f558bfaaf8bbd330af22","A novel radar-based system for longwall coal mine machine localisation is described. The system, based on a radar-ranging sensor and designed to localise mining equipment with respect to the mine tunnel gate road infrastructure, is developed and trialled in an underground coal mine. The challenges of reliable sensing in the mine environment are considered, and the use of a radar sensor for localisation is justified. The difficulties of achieving reliable positioning using only the radar sensor are examined. Several probabilistic data processing techniques are explored in order to estimate two key localisation parameters from a single radar signal, namely along-track position and across-track position, with respect to the gate road structures. For the case of across-track position, a conventional Kalman filter approach is sufficient to achieve a reliable estimate. However for along-track position estimation, specific infrastructure elements on the gate road rib-wall must be identified by a tracking algorithm. Due to complexities associated with this data processing problem, a novel visual analytics approach was explored in a 3D interactive display to facilitate identification of significant features for use in a classifier algorithm. Based on the classifier output, identified elements are used as location waypoints to provide a robust and accurate mining equipment localisation estimate. © 2017, The Author(s).","Automation; Localisation; Longwall mining; Machine learning; Radar; Underground; Waypoint navigation",,2-s2.0-85028041769
"Orozco-Aceves M., Tibbett M., Standish R.J.","Correlation between soil development and native plant growth in forest restoration after surface mining",2017,"Ecological Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020314565&doi=10.1016%2fj.ecoleng.2017.06.004&partnerID=40&md5=69368ef19d9bf1f18c07d2a99b0084de","Recovery of soil conditions in ecosystems that have been modified by human activities can happen in time, but empirical data indicating how soil development influences plant growth are limited. Here, we studied the changes in 23 properties of jarrah-forest soils restored after bauxite mining (1.5, 7, 22 years post-restoration) and their correlative effects on the growth of bossiaea, a plant that grows in the jarrah-forest of Western Australia. We found that physico-chemical and biological properties of restored soils were different compared with properties of unmined soils, and were correlated with time post-restoration. In turn, biomass produced by bossiaea seedlings was correlated with time post-restoration. Differential correlative effects of restored soils properties translated into higher biomass produced by bossiaea in soils aged 22 years compared with biomass produced in younger soils and in unmined soils. Biomass variation of bossiaea seedlings grown in restored soils was matched to the physico-chemical and biological properties of the soils to detect correlative effects of individual soil properties on plant growth. We found that biomass produced by bossiaea seedlings was positively correlated with the number of bacterial-feeding nematodes in soils, a likely consequence of increased microbial biomass, and with the rate of CO2 produced by microbial respiration of phenolic compounds. Our data suggest recovery of physico-chemical and key biological properties of soil through restoration after bauxite mining, but overall, at 22 years soils were not recovered to reference conditions. Our study has implications for restoration projects aiming to promote soil development in addition to plant growth. © 2017 Elsevier B.V.","Bacterial energy channel; Community-level physiological profile; Disturbance ecology; Fungal energy channel; Nematode channel ratio; Phospholipid-fatty acid profile; Plant-soil feedback; Soil biology; Soil chemistry","Bacteria; Bauxite deposits; Biomass; Carbon dioxide; Conservation; Ecology; Fatty acids; Forestry; Phospholipids; Plants (botany); Recovery; Restoration; Seed; Community level physiological profiles; Disturbance ecology; Energy channels; Nematode channel ratio; Phospholipid fatty acids; Soil biology; Soil chemistry; Soils; bacterium; bauxite; biological analysis; biomass; correlation; energy balance; environmental disturbance; environmental restoration; forest ecosystem; forest soil; fungus; growth response; legume; mining; native species; nematode; pedogenesis; physicochemical property; physiological response; seedling; soil property; soil remediation; Australia; Jarrah Forest; Western Australia; Bacteria (microorganisms); Bossiaea; Eucalyptus marginata; Nematoda",2-s2.0-85020314565
"Chen H.-L., Tsou Y.-T., Tai B.-C., Li S.-C., Huang Y.-N., Yu C.-M., Chiu Y.-S.","Developments and applications of data deidentification technology under big data",2017,"Journal of Electronic Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031907665&doi=10.11989%2fJEST.1674-862X.60804064&partnerID=40&md5=d8de63dbb5abfd430aad0f6a6811a144","In this age characterized by rapid growth in the volume of data, data deidentification technologies have become crucial in facilitating the analysis of sensitive information. For instance, healthcare information must be processed through deidentification procedures before being passed to data analysis agencies in order to prevent any exposure of personal details that would violate privacy. As such, privacy protection issues associated with the release of data and data mining have become a popular field of study in the domain of big data. As a strict and verifiable definition of privacy, differential privacy has attracted noteworthy attention and widespread research in recent years. In this study, we analyze the advantages of differential privacy protection mechanisms in comparison to traditional deidentification data protection methods. Furthermore, we examine and analyze the basic theories of differential privacy and relevant studies regarding data release and data mining.","Deidentification, differential privacy",,2-s2.0-85031907665
"Wang P., Ge R., Xiao X., Cai Y., Wang G., Zhou F.","Rectified-Linear-Unit-Based Deep Learning for Biomedical Multi-label Data",2017,"Interdisciplinary Sciences: Computational Life Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026856487&doi=10.1007%2fs12539-016-0196-1&partnerID=40&md5=aa9ff8a97231a07bf0580ceb3a16fa43","Disease diagnosis is one of the major data mining questions by the clinicians. The current diagnosis models usually have a strong assumption that one patient has only one disease, i.e. a single-label data mining problem. But the patients, especially when at the late stages, may have more than one disease and require a multi-label diagnosis. The multi-label data mining is much more difficult than a single-label one, and very few algorithms have been developed for this situation. Deep learning is a data mining algorithm with highly dense inner structure and has achieved many successful applications in the other areas. We propose a hypothesis that rectified-linear-unit-based deep learning algorithm may also be good at the clinical questions, by revising the last layer as a multi-label output. The proof-of-concept experimental data support the hypothesis, and the community may be interested in trying more applications. © 2016, International Association of Scientists in the Interdisciplinary Areas and Springer-Verlag Berlin Heidelberg.","Clinical diagnosis; Deep learning; Multi-label classification; Rectified linear unit; Single-label classification",,2-s2.0-85026856487
"García-Sánchez F., Paredes-Valverde M., Valencia-García R., Alcaraz-Mármol G., Almela A.","KBS4FIA: Leveraging advanced knowledge knowledge-based systems for financial information analysis",2017,"Procesamiento de Lenguaje Natural",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028847611&partnerID=40&md5=53c79e6b74d25eaa1d04a1405d277d9c","Decision making takes place in an environment of uncertainty. Therefore, it is necessary to have information which is as accurate and complete as possible in order to minimize the risk that is inherent to the decision-making process. In the financial domain, the situation becomes even more critical due to the intrinsic complexity of the analytical tasks within this field. The main aim of the KBS4FIA project is to automate the processes associated with financial analysis by leveraging the technological advances in natural language processing, ontology learning and population, ontology evolution, opinion mining, the Semantic Web and Linked Data. This project is being developed by the TECNOMOD research group at the University of Murcia and has been funded by the Ministry of Economy, Industry and Competitiveness and the European Regional Development Fund (ERDF) through the Spanish National Plan for Scientific and Technical Research and Innovation Aimed at the Challenges of Society. © 2017 Sociedad Espanola para el Procesamiento del Lenguaje Natural.","Knowledge acquisition; Linked data; Natural language processing; Ontologies; Opinion mining",,2-s2.0-85028847611
"Mitra B., Sural S., Vaidya J., Atluri V.","Migrating from RBAC to temporal RBAC",2017,"IET Information Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027489569&doi=10.1049%2fiet-ifs.2016.0258&partnerID=40&md5=a3a79180d9f7c46158a179b40b7ba180","The last two decades have witnessed an emergence of role-based access control (RBAC) as the de facto standard for access control. However, for organisations already having a deployed RBAC system, in many cases it may become necessary to associate a temporal dimension with the existing access control policies due to changing organisational requirements. In such cases, migration from RBAC to a temporal extension of RBAC becomes essential. Temporal RBAC (TRBAC) is one such RBAC extension. The process of creating a set of roles for implementing a TRBAC system is known as temporal role mining. Existing temporal role mining approaches typically assume that TRBAC is being deployed from scratch and do not consider it as a migration from an existing RBAC policy. In this study, the authors propose two temporal role mining approaches that enable migration from RBAC to TRBAC. These approaches make use of conventional (non-temporal) role mining algorithms. Apart from aiding the migration process, deriving the roles in this manner allows the flexibility of minimising any desired role mining metric. They experimentally evaluate the performance of both of the proposed approaches and show that they are both efficient and effective. © The Institution of Engineering and Technology 2016.",,"Computer networks; Information systems; Security of data; Access control policies; De facto standard; Migration process; Organisational; Role minings; Role-based Access Control; Temporal dimensions; Temporal extensions; Access control",2-s2.0-85027489569
"Baldo M.A., Stortini A.M., Moretto L.M., Ongaro M., Roman M., Ugo P.","Electrochemical preparation of standard solutions of Pb(II) ions in ionic liquid for analysis of hydrophobic samples: The olive oil case",2017,"Talanta",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019432312&doi=10.1016%2fj.talanta.2017.05.039&partnerID=40&md5=f335dc5908c85b33ae5d12098d8a3f58","In this paper we present an electrochemical approach to prepare standard solutions of metal ions in a room temperature ionic liquid (IL), which can find useful application for analysis in hydrophobic matrices. The method, developed here for the case of lead ions, is based on the galvanostatic dissolution of a lead anode dipped directly in a suitable IL, namely tri-hexyl(tetradecyl)phosphonium bis (trifluoromethylsulfonyl) imide ([P14,6,6,6]+[NTf2]-). After each oxidation step, the metal dissolution process in the IL solutions was monitored by cyclic voltammetric measurements at a glassy carbon disk electrode. The results indicated that the peak current relevant to the reduction of the electro-generated Pb(II) increased linearly while increasing the oxidation time. By varying the oxidation time from 200 to 6000 s, a set of Pb(II)/[P14,6,6,6]+[NTf2]- solutions at concentrations ranging between 10 and 300 μg g−1 was prepared. To validate the efficiency of the electrochemical procedure to produce metal ion standard solutions, the Pb content was quantified by developing a microwave digestion procedure specifically suitable for the IL medium, followed by ICP-QMS analysis in the digested standards. The results indicated a satisfactory agreement between concentrations found by ICP-QMS and calculated from electrochemical data, with a coulometric efficiency of Pb(II) generation in ionic liquid ≥95.6%. Finally, the applicability of the Pb(II)/IL solutions as standards for analyses in hydrophobic media was tested by determining, by ICP-QMS, the Pb content in an extra-virgin olive oil spiked with known amounts of a Pb(II)/IL standard. Satisfactory Pb recoveries, ≥96%, were measured. © 2017 Elsevier B.V.","Analytical standards; Electrochemical dissolution; Ionic liquid; Lead; Olive oil","Carbon; Dissolution; Efficiency; Electrodes; Hydrophobicity; Ionic liquids; Lead; Liquids; Metal analysis; Metal ions; Metals; Olive oil; Oxidation; Standards; Coulometric efficiencies; Cyclic voltammetric measurements; Electrochemical data; Electrochemical dissolution; Electrochemical preparation; Extra virgin olive oil; Microwave digestion; Room temperature ionic liquids; Solution mining",2-s2.0-85019432312
"Busse B.","Integrating UAS and Multibeam Echosounder Data",2017,"Hydro International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030637055&partnerID=40&md5=d7a5116ac178ea05fc62c6cb3c94661e","Knowing the volume of material present in a gravel quarry can make the difference between profit and loss. A gravel quarry in Hartheim am Rhein, southwest Germany, is partially covered by artificial lakes. To determine the volumes of both the above-water gravel dumping grounds and the water in the lakes, high-precision data captured by a UAS was combined with multibeam echosounder data obtained by boat. The author describes acquisition of the two types of data, how they were integrated and the benefits for the mining company. © 2017, Geomares Publishing, The Netherlands.",,,2-s2.0-85030637055
"Arneitz P., Leonhardt R., Schnepp E., Heilig B., Mayrhofer F., Kovacs P., Hejda P., Valach F., Vadasz G., Hammerl C., Egli R., Fabian K., Kompein N.","The HISTMAG database: Combining historical, archaeomagnetic and volcanic data",2017,"Geophysical Journal International",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023739193&doi=10.1093%2fgji%2fggx245&partnerID=40&md5=8ce9374b1a76902ad6f6bf655276b186","Records of the past geomagnetic field can be divided into twomain categories. These are instrumentalhistorical observations on the one hand, and field estimates based on the magnetizationacquired by rocks, sediments and archaeological artefacts on the other hand. In this paper, a newdatabase combining historical, archaeomagnetic and volcanic records is presented. HISTMAGis a relational database, implemented in MySQL, and can be accessed via a web-based interface(http://www.conrad-observatory.at/zamg/index.php/data-en/histmag-database). It combinesavailable global historical data compilations covering the last ∼500 yr as well asarchaeomagnetic and volcanic data collections from the last 50 000 yr. Furthermore, newhistorical and archaeomagnetic records, mainly from central Europe, have been acquired. Intotal, 190 427 records are currently available in the HISTMAG database, whereby the majorityis related to historical declination measurements (155 525). The original database structurewas complemented by new fields, which allow for a detailed description of the different datatypes. A user-comment function provides the possibility for a scientific discussion about individualrecords. Therefore, HISTMAG database supports thorough reliability and uncertaintyassessments of the widely different data sets, which are an essential basis for geomagneticfield reconstructions. A database analysis revealed systematic offset for declination recordsderived from compass roses on historical geographical maps through comparison with otherhistorical records, while maps created for mining activities represent a reliable source. © The Authors 2017. Published by Oxford University Press on behalf of The Royal Astronomical Society.","Archaeomagnetism; Europe; Magnetic field variations through time; Palaeomagnetism","Geomagnetism; Multimedia systems; Archaeological artefacts; Archaeomagnetism; Europe; Geomagnetic fields; Magnetic field variations; Palaeomagnetism; Relational Database; Web-based interface; Volcanoes; database; geomagnetic field; historical record; magnetization; paleomagnetism; Central Europe; Rosa",2-s2.0-85023739193
"Karimi A., Brown G.","Assessing multiple approaches for modelling land-use conflict potential from participatory mapping data",2017,"Land Use Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020726226&doi=10.1016%2fj.landusepol.2017.06.004&partnerID=40&md5=6871da00ca32268b7bf8b2be1acced24","Spatial social data collected through participatory mapping are increasingly used to assess social dimensions for land use planning and management. However, there has been limited research to evaluate alternative approaches to identify potential land-use conflict. Using data from Queensland, Australia, we applied multiple approaches (land-use preferences, weighted preferences, combined place values and land-use preferences, and value compatibility scoring to identify land-use conflict potential and to assess these methods for four different land uses (residential development, tourism development, mining, and conservation). The performance of these approaches were evaluated using selected reference sites in the study area to determine which spatial attributes and methods were most predictive of conflict potential. Weighted preferences, and combined place values and land-use preferences were most effective for all land use types. The conflict mapping results for mining and conservation were sensitive to the number of place value and land-use preference points available for analysis and the number of individuals participating in the mapping process. To determine the inferential quality of conflict mapping results, we operationalised confidence levels based on the number of unique participants that mapped preferences in a given location. Overall, the highest confidence in mapped results was observed for tourism development, followed by mining, conservation, and residential development. Confidence levels varied across the study area and by reference sites. The findings of this study increase the external validity of preference-based conflict mapping methods while demonstrating a means to assess the inferential quality of conflict mapping results. The generation of confidence levels can assist in the prioritization and allocation of planning resources to places with both high conflict potential and high confidence. © 2017 Elsevier Ltd","Conservation; Development; Land-use conflict; Land-use preferences; Place values; Public participation GIS","GIS; land management; land use conflict; land use planning; mapping; participatory approach; Australia; Queensland",2-s2.0-85020726226
"Yuan L.-X., Li S.-J., Jiang Y.-P.","Remote Sensing Scene Classification Using a Preclassification Strategy and an Improved Structural Feature",2017,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030683160&doi=10.1109%2fJSTARS.2017.2707519&partnerID=40&md5=a11f33a21fa7df186e22035840aa7f9f","Due to the high intraclass variability and the low interclass disparity in high-resolution remote sensing (RS) image scenes, high-resolution RS scene classification is a challenging task. The performance of scene classification not only relies on discriminative feature representation but also needs appropriate classification strategies. In this paper, a scene preclassification strategy based on unsupervised learning is proposed, which divides scenes into two groups. The division method called dynamic-sphere division method (DSDM) is based on a dynamic-sphere division and an inside-sphere membership assessment. For the group with lower membership, after introducing the spatial location and scale of the scale invariant feature transformation (SIFT) descriptor to form a transaction, frequent itemset mining and an improved feature selection criterion are implemented to reduce the redundancy from the aspects of feature quantity and feature dimension, and a more discriminative structural feature histogram FMS-hist is finally obtained. Both the radius of the dynamic-sphere and the final optimal feature dimension are automatically selected according to the inflection point of the corresponding curves. Experimental results based on two representative data sets show that the proposed DSDM can select the suitable group, the proposed FMS-hist is superior to the bag-of-SIFT-based models. The holistic procedure can further enhance the scene classification accuracy. © 2008-2012 IEEE.","Dynamic-sphere division method with membership (DSDM); feature mining (FM); feature selection (FS); preclassification; scene classification","Feature extraction; Remote sensing; Spheres; Corresponding curve; Discriminative features; Feature mining; Frequent itemset mining; High resolution remote sensing; preclassification; Scale invariant feature transformations; Scene classification; Classification (of information); image classification; machine learning; pattern recognition; remote sensing; unsupervised classification",2-s2.0-85030683160
"Donoso F.A., Austin K.J., McAree P.R.","Three new Iterative Closest Point variant-methods that improve scan matching for surface mining terrain",2017,"Robotics and Autonomous Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028297467&doi=10.1016%2fj.robot.2017.05.003&partnerID=40&md5=211e9ea6b0e49a04830462b357629fab","The Iterative Closest Point (ICP) algorithm seeks to minimize the misalignment between two point cloud data sets. A limitation of many ICP algorithms is that they work well for some contexts, yet perform poorly in others. Previous work has suggested that the ability of ICP variants to find correspondence was hindered by the presence of geometric disorder in the scene. This paper introduces three new methods based on characterizing the geometric properties of a point using information of its nearest neighbours. Two methods are entropy based and quantify the geometric disorder (eigentropy) in order to improve the filtering of data and thereby remove points that are likely to provide spurious associations. The third method is a point matching method using normals to preferentially work with planar areas of a point cloud. A set of 73,728 ICP variants obtained by combination/permutation of 26 methods are evaluated. These variants were evaluated using a scan matching exercise requiring construction of terrain maps based on data from a mobile sensing platform in an open-cut mining environment. The proposed methods improve ICP performance, as measured by accuracy, precision, and computational efficiency. Notably, five ICP variants, each featuring the new methods of this paper, simultaneously met the solution requirements for three different terrain scenes. It is asserted that being able to characterize the geometric disorder in the point clouds improves the capability of ICP to establish associations between points. © 2017 Elsevier B.V.","Eigentropy; Entropy measures; Iterative Closest Point; Point cloud registration algorithms; Terrain mapping","Computational efficiency; Entropy; Geometry; Landforms; Eigentropy; Entropy measure; Iterative Closest Points; Point cloud registration; Terrain mapping; Iterative methods",2-s2.0-85028297467
"Gontaszewska-Piekarz A.","In search of the oldest lignite mine in Ziemia Lubuska (western Poland) [W poszukiwaniu najstarszej kopalni wegla brunatnego na Ziemi Lubuskiej]",2017,"Przeglad Geologiczny",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030453821&partnerID=40&md5=108157691d7158eaefb3d2cd9ed4ab41","The paper presents available information about the beginnings of lignite mining in Ziemia Lubuska, western Poland. After studies on remained German archival materials from ""Landesarchiv Sachsen-Anhalt"" and German literaturefrom the 19th century, some new data about the oldest mining has been found. The oldest acts of conferment ofmining fields (""Mutung "") in the discussed region refer to the field ""Caroline ""field in Glisno of1820. The paper presents the history of the oldest mine in the Ziemia Lubuska region, i.e. ""Caroline & Herrmann "" in Glisno near of Sul^cin. The mine functioned in the years 1820-1862 and later as ""Consolidierte Max "" untill 1869. The paper also describes the discovery of lignite deposits in the 19fh century near Sulqcin, focusing on the detailed history of the village of Glisno, which was closely connected with lignite. In the l$h century, in addition to a lignite mine, a silkfactory operated in Glisno, which used lignite to steam production, and a famous lignite-silt-spa located in a palace. The history of mining in Glisno has been almost completely forgotten. Unfortunately, there is a scarcity of preserved archival or literature materials relating to the mine.","Glisno; History of mining; Lignite mining; Ziemia lubuska",,2-s2.0-85030453821
"Phu V.N., Chau V.T.N., Tran V.T.N.","SVM for English semantic classification in parallel environment",2017,"International Journal of Speech Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019176913&doi=10.1007%2fs10772-017-9421-5&partnerID=40&md5=359d89d4b7a21a2b187009531e8e4eb3","Semantic analysis is very important and very helpful for many researches and many applications for a long time. SVM is a famous algorithm which is used in the researches and applications in many different fields. In this study, we propose a new model using a SVM algorithm with Hadoop Map (M)/Reduce (R) for English document-level emotional classification in the Cloudera parallel network environment. Cloudera is also a distributed system. Our English testing data set has 25,000 English documents, including 12,500 English positive reviews and 12,500 English negative reviews. Our English training data set has 90,000 English sentences, including 45,000 English positive sentences and 45,000 English negative sentences. Our new model is tested on the English testing data set and we achieve 63.7% accuracy of sentiment classification on this English testing data set. © 2017, Springer Science+Business Media New York.","Cloudera; Distributed system; English document opinion mining; English sentiment classification; Parallel environment; SVM algorithm","Information retrieval systems; Semantics; Statistical tests; Cloudera; Distributed systems; Opinion mining; Parallel environment; Sentiment classification; SVM algorithm; Classification (of information)",2-s2.0-85019176913
"Frey R.M., Xu R., Ilic A.","Mobile app adoption in different life stages: An empirical analysis",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009963958&doi=10.1016%2fj.pmcj.2017.01.006&partnerID=40&md5=8f95585c5ce406263d12044ed87177a5","The analysis of individuals’ current life stages is a powerful approach for identifying und understanding patterns of human behavior. Different stages imply different preferences and consumer demands. Thus, life stages play an important role in marketing, economics, and sociology. However, such information is difficult to be obtained especially in the digital world. This work thus contributed to both theory and practice from two aspects. First, we conducted a large-scale empirical study with 1435 participants and showed that a person's mobile app adoption pattern is strongly influenced by her current life stage. Second, we presented a data-driven, highly-scalable, and real-time approach of predicting an individual's current life stage based on the apps she has installed on smartphone. Result showed that our predictive models were able to predict life stages with 241.0% higher precision and 148.2% higher recall than a random guess on average. © 2017 Elsevier B.V.","Life cycle; Life stage; Mobile app adoption; Prediction; Reality mining","Behavioral research; Life cycle; Social sciences; Adoption patterns; Empirical analysis; Empirical studies; Life stages; Mobile app; Predictive models; Reality minings; Theory and practice; Forecasting",2-s2.0-85009963958
"Thandu S.C., Bharti P., Chellappan S., Yin Z.","Leveraging multi-modal smartphone sensors for ranging and estimating the intensity of explosion events",2017,"Pervasive and Mobile Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021649985&doi=10.1016%2fj.pmcj.2017.06.012&partnerID=40&md5=8d40f3c0df80c695be8653cb103bd97f","Our society, unfortunately, is increasingly becoming exposed to explosion events that could have serious consequences. While explosion events like intentionally triggered bombs cause obvious harm to life and property, other explosions intended for benign purposes in quarries and construction zones may also cause unintended harm as a result of emanating seismic vibrations. As of today, detecting explosions, ranging them, and estimating their intensities are all accomplished only by seismometers that sense the associated ground vibrations and pressure changes as a result of their triggering. Unfortunately, seismometers are bulky, expensive and unsuitable for the ubiquitous use. In this paper, our broad motivation is to demonstrate the feasibility of leveraging the pervasive sensing and processing capabilities of modern smartphones to analyze explosion events. Within this context, we specifically address the problem of ranging and estimating the intensity of an explosion by leveraging the accelerometer and pressure sensors in the smartphone. To do so, we emplaced a number of smartphones in the vicinity of real explosion blasts conducted at a university mining laboratory, where the material blasted was Dynamite with Ammonium Nitrate Fuel Oil (ANFO). We then collected the corresponding accelerometer and pressure readings sensed by the phone. We extracted a number of novel features, and designed a machine learning based algorithmic framework for ranging and estimating the intensity of the explosion event. After an extensive validation, we find that the average-case error in ranging (i.e., estimating the distance to the source of the explosion event) and estimating the intensity of explosive material (in terms of its charge weight) are 8.24% and 7.37%, respectively. We also present perspectives on encoding our algorithm as a smartphone app, identify several critical challenges that will be encountered in real-time data processing of smartphone accelerometers and pressure sensors in the context of pervasive sensing of explosions, and also identify other practical issues like the diversity of smartphones. To the best of our knowledge, our work is pioneering in demonstrating the feasibility of using smartphones to analyze explosion events. We believe there are significant societal benefits emanating from our work. © 2017 Elsevier B.V.","Explosions; Machine learning; Participatory sensing; Ranging; Smartphones","Accelerometers; Artificial intelligence; Data handling; Education; Explosives; Learning systems; Pressure sensors; Range finding; Seismographs; Seismology; Signal encoding; Smartphones; Algorithmic framework; Average case error; Critical challenges; Explosive materials; Participatory Sensing; Processing capability; Real-time data processing; Seismic vibrations; Explosions",2-s2.0-85021649985
"Morimoto T., Kawasaki Y.","Forecasting Financial Market Volatility Using a Dynamic Topic Model",2017,"Asia-Pacific Financial Markets",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025103535&doi=10.1007%2fs10690-017-9228-z&partnerID=40&md5=86e1e9b7ca119779e9bfdef758a43c7a","This study employs big data and text data mining techniques to forecast financial market volatility. We incorporate financial information from online news sources into time series volatility models. We categorize a topic for each news article using time stamps and analyze the chronological evolution of the topic in the set of articles using a dynamic topic model. After calculating a topic score, we develop time series models that incorporate the score to estimate and forecast realized volatility. The results of our empirical analysis suggest that the proposed models can contribute to improving forecasting accuracy. © 2017, Springer Japan KK.","Big data; Dynamic topic model; Forecasting; Online news; Realized volatility; Topic score",,2-s2.0-85025103535
"Federico P., Heimerl F., Koch S., Miksch S.","A Survey on Visual Approaches for Analyzing Scientific Literature and Patents",2017,"IEEE Transactions on Visualization and Computer Graphics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029288777&doi=10.1109%2fTVCG.2016.2610422&partnerID=40&md5=7184a493dcb4d7420e482db5e3a22668","The increasingly large number of available writings describing technical and scientific progress, calls for advanced analytic tools for their efficient analysis. This is true for many application scenarios in science and industry and for different types of writings, comprising patents and scientific articles. Despite important differences between patents and scientific articles, both have a variety of common characteristics that lead to similar search and analysis tasks. However, the analysis and visualization of these documents is not a trivial task due to the complexity of the documents as well as the large number of possible relations between their multivariate attributes. In this survey, we review interactive analysis and visualization approaches of patents and scientific articles, ranging from exploration tools to sophisticated mining methods. In a bottom-up approach, we categorize them according to two aspects: (a) data type (text, citations, authors, metadata, and combinations thereof), and (b) task (finding and comparing single entities, seeking elementary relations, finding complex patterns, and in particular temporal patterns, and investigating connections between multiple behaviours). Finally, we identify challenges and research directions in this area that ask for future investigations. © 2017 IEEE.","documents; patents; scientific literature; survey; Visualization","Flow visualization; Mining; Surveying; Surveys; Visualization; Application scenario; Bottom up approach; documents; Interactive analysis; patents; Scientific articles; Scientific literature; Scientific progress; Patents and inventions",2-s2.0-85029288777
"Storey P.J., Sochi T., Bastin R.","Recombination coefficients for O ii lines in nebular conditions",2017,"Monthly Notices of the Royal Astronomical Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021832393&doi=10.1093%2fmnras%2fstx1189&partnerID=40&md5=6924394c032e3e4d240af497fd2d70b1","We present the results of a calculation of recombination coefficients for O2++ e- using an intermediate coupling treatment that fully accounts for the dependence of the distribution of population among the ground levels of O2+ on electron density and temperature. The calculation is extended down to low electron temperatures, where dielectronic recombination arising from Rydberg states converging on the O2+ ground levels is an important process. The data that consist of emission coefficients for 8889 recombination lines and recombination coefficients for the ground and metastable states of O+ are in Cases A, B and C, and are organized as a function of the electron temperature and number density, as well as wavelength. An interactive FORTRAN 77 data server is also provided as an accessory for mining the line emission coefficients and obtaining Lagrange interpolated values for any choice of the two variables between the explicitly provided values for any set of wavelengths. Some illustrations of the application of the new data to nebular observations are also provided. © 2017 The Author. Published by Oxford University Press on behalf of the British Society for Antimicrobial Chemotherapy. All rights reserved.","Astrochemistry; Atomic data; Atomic processes; Methods: numerical -HII regions; Planetary nebulae: general; Plasmas; Radiation mechanisms: general; Radiation mechanisms: non-thermal",,2-s2.0-85021832393
"Wu T., Zhang C., Zhao Y.","A study on faults diagnosis and early-warning method of tailings reservoir monitoring points based on intelligent discovery",2017,"International Journal of Performability Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029381079&doi=10.23940%2fijpe.17.05.p13.697710&partnerID=40&md5=53d6dcc9f151468e44a4963aaa54032c","The tailings reservoir is a major hazard source with high potential energy, which may cause an artificial debris flow. The stability of the tailings reservoir is extremely important to the normal operation of the mining enterprises and the safety of people's lives and property. In order to settle the problem that traditional manual monitoring is scattered, not timely, and difficult to manage, this article takes Huangmailing tailings as an example, and establishes the CMST model to optimize the network topology connection of the tailings monitoring points. BP neural network algorithm is used to discuss the intelligent discovery and early warning of the faults on-line monitoring system of tailings. In this way, the fault-points and the causes can be perceived quickly and accurately, and the risk of the tailings' safety accident can be reduced. It can be proved by the experimental results and two years stable operation of the system that BP neural network algorithm can accurately predict the value of safety monitoring data. © 2017 Totem Publisher, Inc. All rights reserved.","BP neural network; CMST model; Faults diagnosis; Intelligent discovery; Tailings","Fault detection; Neural networks; Potential energy; Safety engineering; Tailings; Topology; BP neural networks; Early-warning method; Faults diagnosis; Intelligent discovery; Manual monitoring; Mining enterprise; On-line monitoring system; Tailings reservoirs; Monitoring",2-s2.0-85029381079
"Chang J., Yu J., Su B.","Numerical Simulation and Application of Mine TEM Detection in a Hidden Water-bearing Coal Mine Collapse Column",2017,"Journal of Environmental and Engineering Geophysics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030094015&doi=10.2113%2fJEEG22.3.223&partnerID=40&md5=b56d59d9576e576c4df6a15ad5b56c6a","A hidden water-bearing collapse column (WBCC), directly exposed during coal mining, is a potential source of serious mine flooding. Presently, the transient electromagnetic method (TEM) is widely used, which shows good performance in detecting WBCCs in coal mines. However, traditional numerical simulation approaches have failed to be adapted to complicated underground environments affected by roadways, and the application of boundary conditions require a large grid. Considering this, the authors chose the convolutional perfectly matched layer (CPML) as a boundary condition, whose effects were found to be superior to the traditional Dirichlet boundary condition. Then, based on stratigraphic data related to coal measures located in North and Mid-eastern China, the whole-space geoelectric model was established. The whole-space TEM response of the WBCC at different depths of the stope face floor and ahead of the driving face were also simulated by the finite-difference time-domain method. The numerical simulation results indicate that the underground roadway exerted a substantial influence in early periods, yet the effects became negligible with time, according to the inductive potential decline curve. In addition, the contour distribution of apparent resistivity for different geoelectrical models of the WBCC was consistent with the models. The closer the roof of the collapse column was to the coal seam floor, the lower the apparent resistivity. Moreover, TEM was applied to hidden water-bearing collapse column detection in real underground coal mines..",,"Boundary conditions; Coal; Coal deposits; Finite difference time domain method; Floors; Numerical models; Stratigraphy; Time domain analysis; Apparent resistivity; Convolutional perfectly matched layer; Dirichlet boundary condition; Geoelectric models; Numerical simulation approaches; Transient electromagnetic methods; Underground coal mine; Underground environment; Coal mines; boundary condition; coal mining; coal seam; collapse structure; electromagnetic method; geoelectric field; numerical model; simulation; transmission electron microscopy; China",2-s2.0-85030094015
"Hailer M.K., Peck C.P., Calhoun M.W., West R.F., James K.J., Siciliano S.D.","Assessing human metal accumulations in an urban superfund site",2017,"Environmental Toxicology and Pharmacology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021880866&doi=10.1016%2fj.etap.2017.06.001&partnerID=40&md5=2027b1558c2188ace85233d2310347b2","Butte, Montana is part of the largest superfund site in the continental United States. Open-pit mining continues in close proximity to Butte's urban population. This study seeks to establish baseline metal concentrations in the hair and blood of individuals living in Butte, MT and possible routes of exposure. Volunteers from Butte (n = 116) and Bozeman (n = 86) were recruited to submit hair and blood samples and asked to complete a lifestyle survey. Elemental analysis of hair and blood samples was performed by ICP-MS. Three air monitors were stationed in Butte to collect particulate and filters were analyzed by ICP-MS. Soil samples from the yards of Butte volunteers were quantified by ICP-MS. Hair analysis revealed concentrations of Al, As, Cd, Cu, Mn, Mo, and U to be statistically elevated in Butte's population. Blood analysis revealed that the concentration of As was also statistically elevated in the Butte population. Multiple regression analysis was performed for the elements As, Cu, and Mn for hair and blood samples. Soil samples revealed detectable levels of As, Pb, Cu, Mn, and Cd, with As and Cu levels being higher than expected in some of the samples. Air sampling revealed consistently elevated As and Mn levels in the larger particulate sampled as compared to average U.S. ambient air data. © 2017 Elsevier B.V.","Exposure assessment; Metal mixtures; Metals; Superfund","aluminum; arsenic; cadmium; copper; lead; manganese; metal; molybdenum; selenium; strontium; uranium; zinc; adult; air monitoring; air sampling; Article; bioaccumulation; blood analysis; controlled study; elemental analysis; female; hair analysis; human; male; mass spectrometry; mining; Montana; particulate matter; priority journal; soil analysis; suspended particulate matter",2-s2.0-85021880866
"Wetwitoo J., Kato H.","Inter-regional transportation and economic productivity: a case study of regional agglomeration economies in Japan",2017,"Annals of Regional Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020716772&doi=10.1007%2fs00168-017-0833-6&partnerID=40&md5=0ad2f9ed8af256b79c4147850604ef31","This study investigates the benefit of agglomeration to regional productivity, highlighting the issue of accessibility with empirical data from Japan. We analyze empirically the impacts of agglomeration on regional economic return using an econometric approach, assuming three types of agglomeration economics: urbanization agglomeration, localization agglomeration, and mixed agglomeration. We estimate the agglomeration elasticities of 11 industries using inter-regional transportation network data and regional socioeconomic panel data for 1981, 1986, 1991, 1996, 2001, and 2006, covering 47 prefectures in Japan. Our results show that, on average, the indirect benefit of regional productivity improvement from localization agglomeration tends to be more significant than that from urbanization agglomeration. While the mining industry enjoys significant benefit from urbanization rather than localization agglomeration and the transportation/communication industry enjoys significant benefit from localization rather than urbanization agglomeration, finance/insurance and real estate can benefit from both agglomeration economies. We further find negative elasticities in the agriculture and service industries; this could be partly due to the industries’ characteristics. A case study on Japan shows the importance of coordination between land-use and transportation investment for maximizing regional productivity through agglomeration. © 2017, Springer-Verlag Berlin Heidelberg.","C33; D24; O18; R40","accessibility; agglomeration; econometrics; empirical analysis; panel data; productivity; regional economy; socioeconomic conditions; transportation; Japan",2-s2.0-85020716772
"Liu J., Luo B., Qin Q., Yang G.","Alike scene retrieval from land-cover products based on the label co-occurrence matrix (LCM)",2017,"Remote Sensing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029349311&doi=10.3390%2frs9090912&partnerID=40&md5=2e347f51a3608fd8493501ccd674c706","The management and application of remotely sensed data has become much more difficult due to the dramatically growing volume of remotely sensed imagery. To address this issue, content-based image retrieval (CBIR) has been applied to remote sensing image retrieval for information mining. As a consequence of the growing volume of remotely sensed imagery, the number of different types of image-derived products (such as land use/land cover (LULC) databases) is also increasing rapidly. Nevertheless, only a few studies have addressed the exploration and information mining of these products. In this letter, for the sake of making the most use of the LULC map, we propose an approach for the retrieval of alike scenes from it. Based on the proposed approach, we design a content-based map retrieval (CBMR) system for LULC. The main contributions of our work are listed below. Firstly, the proposed system can allow the user to select a region of interest as the reference scene with variable shape and size. In contrast, in the traditional CBIR/CBMR systems, the region of interest is usually of a fixed size, which is equal to the size of the analysis window for extracting features. In addition, the user can acquire various retrieval results by specifying the corresponding parameters. Finally, by combining the signatures in the base signature library, the user can acquire the retrieval result faster. © 2017 by the authors.","Content-based map retrieval; Land-cover datasets; Similarity","Content based retrieval; Image reconstruction; Image segmentation; Land use; Remote sensing; Co-occurrence-matrix; Content-based; Content-Based Image Retrieval; Land cover datasets; Remote sensing image retrieval; Remotely sensed data; Remotely sensed imagery; Similarity; Image retrieval",2-s2.0-85029349311
"López L.A.U., Guijarro A.M., Valdivia M.T.M., Barco P.M.","REDES REDES : Digital entities recognition: Enrichment and tracking by language technologies [REDES: Reconocimiento de Entidades Digitales: Enriquecimiento y Seguimiento mediante Tecnologías del Lenguajey]",2017,"Procesamiento de Lenguaje Natural",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028858762&partnerID=40&md5=59a0af4f49e8347e456837fa639f3e6e","The main objective of this project is to develop an integration model able to define and create digital entities profiles. Such digital entities will include not only the basic, but also their linguistic and social features by means of using and integrating different information sources available. More specifically, three will be the Web sources: unstructured and structured data, but and also linked open data. Starting from this huge and heterogeneous amount of information, digital entities will be generated by means of the design and development of tools, resources and techniques based on NLP. Such entities will consist in a structure of semantic information where to place such data (with special attention to the spatial dimensions (geographical location) and temporal (variation of data that compose the entity during time). © 2017 Sociedad Espanola para el Procesamiento del Lenguaje Natural.","Digital entity; Natural language processing; NLP; Opinion mining; Sentiment analysis; Sentiment enrichment",,2-s2.0-85028858762
"Gaisberger H., Kindt R., Loo J., Schmidt M., Bognounou F., Da S.S., Diallo O.B., Ganaba S., Gnoumou A., Lompo D., Lykke A.M., Mbayngone E., Nacoulma B.M.I., Ouedraogo M., Ouédraogo O., Parkouda C., Porembski S., Savadogo P., Thiombiano A., Zerbo G., Vinceti B.","Spatially explicit multi-threat assessment of food tree species in Burkina Faso: A fine-scale approach",2017,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029839081&doi=10.1371%2fjournal.pone.0184457&partnerID=40&md5=6301d3094e111c3a715c4b042ba6629f","Over the last decades agroforestry parklands in Burkina Faso have come under increasing demographic as well as climatic pressures, which are threatening indigenous tree species that contribute substantially to income generation and nutrition in rural households. Analyzing the threats as well as the species vulnerability to them is fundamental for priority setting in conservation planning. Guided by literature and local experts we selected 16 important food tree species (Acacia macrostachya, Acacia Senegal, Adansonia digitata, Annona Senegalensis, Balanites aegyptiaca, Bombax costatum, Boscia Senegalensis, Detarium microcarpum, Lannea microcarpa, Parkia biglobosa, Sclerocarya birrea, Strychnos spinosa, Tamarindus indica, Vitellaria paradoxa, Ximenia americana, Ziziphus mauritiana) and six key threats to them (overexploitation, overgrazing, fire, cotton production, mining and climate change). We developed a species-specific and spatially explicit approach combining freely accessible datasets, species distribution models (SDMs), climate models and expert survey results to predict, at fine scale, where these threats are likely to have the greatest impact. We find that all species face serious threats throughout much of their distribution in Burkina Faso and that climate change is predicted to be the most prevalent threat in the long term, whereas overexploitation and cotton production are the most important short-term threats. Tree populations growing in areas designated as ‘highly threatened’ due to climate change should be used as seed sources for ex situ conservation and planting in areas where future climate is predicting suitable habitats. Assisted regeneration is suggested for populations in areas where suitable habitat under future climate conditions coincides with high threat levels due to short-term threats. In the case of Vitellaria paradoxa, we suggest collecting seed along the northern margins of its distribution and considering assisted regeneration in the central part where the current threat level is high due to overexploitation. In the same way, population-specific recommendations can be derived from the individual and combined threat maps of the other 15 food tree species. The approach can be easily transferred to other countries and can be used to analyze general and species specific threats at finer and more local as well as at broader (continental) scales in order to plan more selective and efficient conservation actions in time. The concept can be applied anywhere as long as appropriate spatial data are available as well as knowledgeable experts. © 2017 Gaisberger et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Acacia; Acacia macrostachya; access to information; Adansonia; Adansonia digitata; agroforestry; Annona senegalensis; Article; Balanites; Balanites aegyptiaca; Bombax; Bombax costatum; Boscia senegalensis; Burkina Faso; climate; climate change; cotton; demography; Detarium microcarpum; ecosystem regeneration; endangered species; environmental exploitation; environmental impact; environmental planning; environmental protection; fire; grazing; household; Lannea microcarpa; mining; nonhuman; Parkia biglobosa; planting density; prediction; rural area; Sclerocarya birrea; Senegalia senegal; species difference; species distribution; Strychnos; Strychnos spinosa; tamarind; tree; Vitellaria paradoxa; Ximenia americana; Ziziphus mauritiana; Anacardiaceae; Annona; ecosystem; food; Olacaceae; procedures; Acacia; Adansonia; Anacardiaceae; Annona; Balanites; Bombax; Burkina Faso; Climate Change; Conservation of Natural Resources; Ecosystem; Food; Olacaceae; Tamarindus",2-s2.0-85029839081
"Xiao L., Stromer-Galley J., Sándor Á.","Toward the Automated Detection of Individuals’ Rationales in Large-Scale Online Open Participative Activities: A Conceptual Framework",2017,"Group Decision and Negotiation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006365123&doi=10.1007%2fs10726-016-9516-4&partnerID=40&md5=b7a73550c51a61e7c9b758ca70cb9eb7","In large-scale online open participative (LSOOP) activities, participants can join and leave at any time, and they often do not have a history of working together. Although the communication history is usually accessible to the participants in the environment, it is time consuming for them to process the communication data because of the large volume of messages. These characteristics make it difficult for one to keep track of, identify, and interpret the others’ ideas, opinions, and their rationales in LSOOP activities. We argue for a computational approach that automatically identifies and extracts the rationales from LSOOP communication data and presents them to the participants through rationale-based awareness tools. In this paper we bring together different and hitherto independent lines of research, and propose to use them in a conceptual framework integrating three analytical aspects related to the detection of rationales: linguistic, informational, and argumentative and communicative. We also review the design effort on offering rationale-based awareness in the LSOOP activities. © 2016, Springer Science+Business Media Dordrecht.","Argumentation mining; Awareness support; Large-scale online open participative (LSOOP) activities; Rationale detection",,2-s2.0-85006365123
"Kapur V.V., Das D.P., Bajpai S., Prasad G.V.R.","First mammal of Gondwanan lineage in the early Eocene of India [Premiers mammifères de la lignée gondwanienne dans l’Éocène inférieur de l'Inde]",2017,"Comptes Rendus - Palevol",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016446119&doi=10.1016%2fj.crpv.2017.01.002&partnerID=40&md5=f5c999093e862a1cdb5c2da93b7c1ffd","Based on well-preserved lower dentition, a new adapisoriculid from the Cambay Shale Formation (basal Eocene, ∼54.5 Ma) in the open cast lignite mine of Vastan, Gujarat State, western India, is described. Indolestes kalamensis gen. et sp. nov adds significantly to the diversity of basal eutherians from Vastan as it represents a family hitherto not known from the Eocene of the Indian Subcontinent. Phylogenetic analysis suggests that Indolestes is derived relative to Deccanolestes and Afrodon, but primitive relative to the European adapisoriculids Bustylus and Adapisoriculus. The new data from the early Eocene provide evidence for continued survival of a Gondwanan mammal lineage following the Deccan volcanic activity (Cretaceous–Paleogene transition) in the Indian Subcontinent. © 2017 Académie des sciences","Adapisoriculidae; Cambay Shale; Early Eocene; Gondwana; India; Mammal","dentition; Eocene; Gondwana; lignite; mammal; new record; opencast mining; paleoecology; phylogenetics; shale; survival; volcanic eruption; Deccan; Gujarat; India; India; Vastan Lignite Mine; Eutheria; Mammalia",2-s2.0-85016446119
"Wei W., Wu X.-Q., Ke W., Xu S., Feng B., Hu B.-T.","Relationship Between pH and Electrochemical Corrosion Behavior of Thermal-Sprayed Ni-Al-Coated Q235 Steel in Simulated Soil Solutions",2017,"Journal of Materials Engineering and Performance",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028959130&doi=10.1007%2fs11665-017-2906-1&partnerID=40&md5=59c1669a70bc90702b84fb6a4b0173b1","Electrochemical corrosion behavior of a thermal-sprayed Ni-Al-coated Q235 steel was investigated in the simulated soil solutions at different pH values using measurements of potentiodynamic polarization curves and electrochemical impedance spectroscopy as well as surface analyses including x-ray diffraction analysis, scanning electron microscope equipped with an energy-dispersive x-ray spectroscopy and x-ray photoelectron spectroscopy. The results showed that the corrosion resistance of the Ni-Al-coated Q235 steel was dependent on the pH of the test solution. From pH = 3.53 to pH = 4.79, the corrosion resistance of the coated steel increased rapidly. In the pH range from 4.79 to 12.26, the corrosion resistance exhibited no significant change. At pH 13.25, the corrosion resistance of the sample was found to decrease. The calculated corrosion rate of Ni-Al-coated Q235 steel was lower than that of the uncoated Q235 steel and galvanized steel in all the test solutions. Over a wide range of pH values, the Ni-Al-coated Q235 steel exhibited extremely good corrosion resistance. The experimental data together with the potential-pH diagrams provided a basis for a detailed discussion of the related corrosion mechanisms of the coated steel. © 2017, ASM International.","corrosion resistance; grounding grids; Ni-Al coating; pH value; thermal spraying","Aluminum; Aluminum alloys; Aluminum coatings; Binary alloys; Corrosion; Corrosion rate; Corrosion resistance; Corrosive effects; Electrochemical impedance spectroscopy; Energy dispersive spectroscopy; Galvanizing; Nickel; Nickel alloys; pH; Scanning electron microscopy; Soil moisture; Solution mining; Thermal spraying; X ray diffraction analysis; X ray photoelectron spectroscopy; X ray spectroscopy; Corrosion mechanisms; Electrochemical corrosion behavior; Energy dispersive X ray spectroscopy; Grounding grids; Ni-al coatings; pH value; Potential-pH diagrams; Potentiodynamic polarization curves; Electrochemical corrosion",2-s2.0-85028959130
"Cao B., Zhu S., Qiu Z., Cao B.","More Rigorous Correction of Refraction Effects in Two-media Stereophoto-grammetry",2017,"Cehui Xuebao/Acta Geodaetica et Cartographica Sinica",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031943415&doi=10.11947%2fj.AGCS.2017.20170119&partnerID=40&md5=02b1aa81b639218218487d2cf24dfb6a","A more rigorous algorithm is presented for correction of refraction effects in two-media stereo photogrammetry. The mid-point of the shortest line segment joining two aerial corresponding rays of a point on an underwater object is used as a photogrammetric intersection point which doesn't exist when the two rays are non-intersecting. As a result, the uncertainty of the intersection point is removed, the positional relationship between the intersection point and the true object point becomes definite, and the refraction correction formula from the intersection point to the true object point can be strictly derived. The bad effect on the refraction correction is firstly analyzed, which caused by that the two rays are non-intersecting. Then the positional relationship between the intersection point and the true object point is studied. After that, the formulas regarding water depth and geodetic coordinates of points on an underwater object are deduced, that is often known as correction of refraction effects. Finally, the algorithm is tested by two experiments using the data of World View-2. The results show that the algorithm is suitable for any case in which whether or not the two aerial corresponding rays of an underwater object point are intersecting, and it can significantly improve the measurement accuracy of underwater object's elevation. © 2017, Surveying and Mapping Press. All right reserved.","Correction of refraction effects; Photogrammetric intersection point; True object point; Two-media stereophotogrammetry; Underwater object positioning","Cutting machines (mining); Photogrammetry; Intersection points; Object points; Refraction effects; Stereophotogrammetry; Underwater objects; Refraction",2-s2.0-85031943415
"Fajkus M., Nedoma J., Kepak S., Vasinek V., Martinek R., Bilik P., Vanus J.","Energetically passive fiber-optic sensor system for the security windows of the buildings",2017,"Proceedings of the 9th International Scientific Symposium on Electrical Power Engineering, ELEKTROENERGETIKA 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029724188&partnerID=40&md5=289d7413c5292fdb25b85a627f597550","An important element within any buildings, halls and warehouses is protection against intrusions and theft of expensive materials and funds stored in these areas. Currently, there are the many solutions for detecting unwanted intrusion. This article provides a simple solution based on the performed real measurements of the security the perimeter, which is based on fiber Bragg grating that is additionally implemented into structure of the plastic windows. In the case of attempts to break into the protected object, the system records the vibration response in the form of deformation which is transmitted to the Bragg sensor. The article presents two approaches to evaluating the data: the power measurement approach (it uses the point chirped FBG sensor with possible use temperature range from -30 °C up to 60 °C) and spectral measurement approach (it can monitor up to 80 FBG sensors simultaneously). The advantages of the fiber-optic technology against the conventional solutions are obvious: presented solution is not detectable by conventional methods (EMI-free) and it is energetically passive.","Energetically passive sensors; Fiber Bragg sensor (FBG); Fiber-optic sensor; Perimeter; Security","Electric sensing devices; Fiber Bragg gratings; Fiber optics; Fibers; Solution mining; Conventional methods; Fiber Bragg sensors; Fiber-optic technology; Passive sensor; Perimeter; Security; Spectral measurement; Vibration response; Fiber optic sensors",2-s2.0-85029724188
"Wibowo A., Winarko E., Azhari","Xtroad: The tweet extraction method for profiling road and traffic conditions",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029474330&partnerID=40&md5=68e1c1a1fb500672d39dc3eb09d9d385","Twitter contains many natural language text elements that can be used for many purposes. One purpose is to produce the information about road and traffic conditions. The goal of this research is to propose a new extraction method for Indonesia Twitter text called xTRoad. By applying the xTRoad method, the national roads at Jakarta city could be identified by the other roadways nearby those which are connected in the particular region. The identification of the road and traffic conditions is completed in less than four minutes. The obtained information has the attributes of time, date, day, road names, incidents, weather and traffic conditions. All elements can be used to form the profile of road and traffic conditions. The results of this study showed that the obtained profiles have many dimensions such as the congestion conditions per thirty minutes, the types of barriers that occur on national roads, the trend of traffic jam in 24 hours, etc. The similarity level for comparison between the traffic conditions from the extracted text and video data has an accuracy of 62.8%. © 2006-2017 Asian Research Publishing Network (ARPN).","Natural language processing; Road conditions; Text mining; Traffic; Twitter extraction; xTRoad",,2-s2.0-85029474330
"Ivanov E.V., Lebedeva E.Y., Petrovskaya S.G., Baranov V.V., Kravchenko A.N., Ivanova N.G.","Volume-related interaction parameters for dilute solutions of Mebicaret (2,4-dimethyl-6,8-diethylglycoluril) in normal and heavy water between 278.15 K and 318.15 K",2017,"Journal of Molecular Liquids",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021873989&doi=10.1016%2fj.molliq.2017.07.003&partnerID=40&md5=f633e7a91a332c6f8641fd416f23bb31","Density measurements on solutions of 2,4-dimethyl-6,8-diethyl-2,4,6,8-tatraazabicyclo[3.3.0]octane-3,7-dione or Mebicaret in normal (H2O) and heavy (D2O) water were carried out with a precision of 0.012 kg·m− 3 using the Anton Paar DMA 5000 M vibrating tube densimeter. All experiments were performed within the temperature range between 278.15 K and 318.15 K (with a step of 10 K) at the pressure to be ca. 99.6 kPa. The solution aquamolality was ranged from 0.02 to 0.2 mol ∙(55.51 mol of solvent)− 1 for each of H/D isotopically distinguishable systems compared. The standard (partial at infinite dilution) molar volumes (V2 o) and expansibilities (Ep , 2 o) of the solute as well as volume-related second virial coefficients (v22) were derived from the data on a solution density. The behavior of solvent isotope effects (IEs) on the considered quantities was described given the structure- and interaction-related peculiarities of the Mebicaret molecule as well as H2O and D2O media. The volume-related interaction characteristics for the solute of interest were discussed using the results of our previous densimetric and calorimetric studies with the participation of similar N-tetraalkylated glycoluril-derivatives. © 2017","Mebicaret; Normal and heavy water; Volumetric properties","Heavy water; Volumetric analysis; Infinite dilution; Interaction characteristics; Interaction parameters; Mebicaret; Second virial coefficients; Solvent isotope effects; Vibrating tube densimeters; Volumetric properties; Solution mining",2-s2.0-85021873989
"Vijayaraghavan J., Jude A.B., Thivya J.","Effect of copper slag, iron slag and recycled concrete aggregate on the mechanical properties of concrete",2017,"Resources Policy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030535981&doi=10.1016%2fj.resourpol.2017.06.012&partnerID=40&md5=66270a3c93550092a0502fac329f9e13","Resource concern operations were evident all around the world in each and every sector including civil constructions, where more resources get exploited recently with the development of globalized infrastructures. With these developments bring out new challenges on mainly construction materials. Hence, many studies ultimately reporting various waste management strategies which involves both eco efficient parameters. In the row, recent studies have been paid their focus on the construction materials which includes fine and course aggregates. Generally, the existing studies focused on alternatives for these construction aggregates, but still there is enough room to explore further. Hence, this study considers as an opportunity to investigate the effect of using alternatives for both fine and course aggregates with copper slag (30%, 40% and 50%), iron slag (30%, 40% and 50%)and recycled concrete aggregate (20%, 25% and 30%) with various proportions of mix by the partial replacement of sand and gravel respectively. The mechanical properties were concluded and compared among conventional concrete with proposed mix under the timeline of 28 curing days. Results sought to indicate the appropriate proportion of mix which providing higher mechanical strength. This study was conducted on two phases namely experimental and numerical analysis. Initially, experimental study were made on the above considered proportions and further the obtain data were manipulated with numerical modeling for reliability. From the study, it has been concluded that 40% of copper slag, 40% iron slag and 25% of recycled concrete aggregate possess more strength than conventional concrete mix. However, further research work opportunities were suggested in the conclusion of the study to shed more light on effective sustainability on construction materials. © 2017 Elsevier Ltd","Copper slag; Iron slag; Recycled concrete aggregate; TOPSIS","Concretes; Copper; Curricula; Iron; Mechanical properties; Mining laws and regulations; Recycling; Slags; Waste management; Conventional concrete; Copper slag; Experimental and numerical analysis; Iron slags; Properties of concretes; Recycled concrete aggregates; TOPSIS; Waste management strategies; Concrete aggregates",2-s2.0-85030535981
"Halldin C.N., Blackley D.J., Petsonk E.L., Laney A.S.","Pneumoconioses radiographs in a large population of U.S. coal workers: Variability in a reader and b reader classifications by using the international labour office classification",2017,"Radiology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027418494&doi=10.1148%2fradiol.2017162437&partnerID=40&md5=7b88c9573e5a81312c0cb1b06cb8fb93","Purpose: To assess the level of concordance between chest radiographic classifications of A and B Readers in a national surveillance program offered to U.S. coal miners over an approximate 36-year period. Materials and Methods: The National Institute for Occupational Safety and Health (NIOSH) Coal Workers' Health Surveillance Program (CWHSP) is a surveillance program with nonresearch designation and is exempt from Human Subjects Review Board approval (11-DRDS-NR03). Thirty-six years of data (1979-2015) from the CWHSP were analyzed, which included all conventional screen-film radiographs with a classification by at least one A Reader and one B Reader. Agreement was assessed by using κ statistics; prevalence ratios were used to describe differences between A and B Reader determinations of image technical quality, small opacity profusion, and presence of large opacities and pleural abnormalities. Results: The analysis included 79 185 matched A and B Reader chest radiograph classifications. A majority of both A and B Readers were radiologists (74.2% [213 of 287] vs 64.7% [22 of 34]; P = .04). A and B Readers had minimal agreement on technical image quality (κ = 0.0796; 95% confidence interval [CI]: 0.07, 0.08) and the distribution of small opacity profusion (subcategory κ, 0.2352; 95% CI: 0.22, 0.25). A Readers classified more images as ""good"" quality (prevalence ratio, 1.38; 95% CI: 1.35, 1.41) and identified more pneumoconiosis (prevalence ratio, 1.22; 95% CI: 1.20, 1.23). Conclusion: A Readers classified substantially more radiographs with evidence of pneumoconiosis and classified higher small opacity profusion compared with B Readers. These observations reinforce the importance of multiple classifications by readers who have demonstrated ongoing competence in the International Labour Office classification system to ensure accurate radiographic classifications. © RSNA, 2017.",,"classification; coal mining; diagnostic imaging; government; human; observer variation; occupational disease; occupational health; organization and management; pneumoconiosis; reproducibility; standards; thorax radiography; United States; Coal Industry; Humans; Observer Variation; Occupational Diseases; Occupational Health; Pneumoconiosis; Radiography, Thoracic; Reproducibility of Results; United States; United States Occupational Safety and Health Administration",2-s2.0-85027418494
"Zhang L., Zheng L., Peng T.-Q.","Structurally embedded news consumption on mobile news applications",2017,"Information Processing and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021652331&doi=10.1016%2fj.ipm.2017.04.009&partnerID=40&md5=3fe18272fcf04164a1e638c38eda174c","This study uses a longitudinal dataset extracted from a mobile news application and adopts a multilevel design to examine the evolution of diversity of individuals’ news consumption and to identify the factors that underlie such evolution. A decreasing trend in news consumption diversity is observed among users. The news consumption diversity of individuals is positively related to global information diversity. Furthermore, the news consumption diversity of males exhibits a stronger tendency to be influenced by global information diversity than that of females. The decreasing trend in news consumption diversity is less remarkable among males than among females. This study complements traditional motivation-driven perspectives of news consumption by mining the structural antecedents of news consumption diversity and further emphasizes the social implications of mobile news technology. Lastly, practical implications and limitations are discussed. © 2017 Elsevier Ltd","Audience fragmentation; Diversity; Mobile internet; News consumption","Information management; Audience fragmentation; Diversity; Global informations; Mobile Internet; Multi-level designs; News consumption; Social implication; Data processing",2-s2.0-85021652331
"Nicol M., Basson P.","The anodic behaviour of covellite in chloride solutions",2017,"Hydrometallurgy",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024115153&doi=10.1016%2fj.hydromet.2017.06.018&partnerID=40&md5=a9738ffd4848ddaf141de0ddec17da9e","A detailed study of the anodic behaviour of synthetic covellite in acidic chloride solutions has been conducted as part of an overall program on the fundamental aspects of the heap leaching of copper sulfide minerals. The anodic behaviour in chloride solutions is characterized by active (but slow) dissolution at low potentials below about 0.65 V, passivation at potentials above 0.70 V that continues to potentials greater than 1.4 V above which rapid transpassive dissolution occurs. These characteristics are also typical of chalcopyrite under similar conditions. The anodic characteristics at potentials in the region of the measured mixed potentials in the presence of copper(II) show that there are two peaks in the voltammetric sweeps at about 0.65 V and 0.75 V, the magnitude of which increase with increasing chloride concentration. The two peaks merge into a single peak at very high chloride concentrations. Potentiostatic current-time transients at various potentials in the region of the mixed potential show the slow passivation typical of chalcopyrite and the data can similarly be described quantitatively in terms of rate limiting solid-state diffusion. The rate of oxidative dissolution of covellite under ambient temperature conditions is slow but about an order of magnitude greater than that of chalcopyrite. The voltammetric response of chalcopyrite after pre-treatment with a chloride solution containing a low concentration of copper(II) shows the presence of the same two peaks observed for covellite confirming the surface conversion of chalcopyrite to covellite. A mechanism similar to that previously proposed for the dissolution of chalcopyrite has been described in terms of which the formation of polysulfides (typically CuS2) by dissolution of a fraction of the copper in the covellite lattice is responsible for the passivation that increases slowly as the polysulfide layer increases in thickness. Novel measurements of the mixed potential and the solution potential at a covellite surface at the bottom of a capillary have been used to simulate the situation within ore particles. The results have been simulated using a simple linear diffusion model that show that the potential at the covellite surface can be 50–100 mV lower than that in the bulk of the solution. This has important consequences for the rate of heap leaching. © 2017 Elsevier B.V.",,"Chloride minerals; Chlorine compounds; Copper; Diffusion in solids; Dissolution; Leaching; Passivation; Polysulfides; Sulfide minerals; Acidic chloride solution; Chloride concentrations; Current-time transient; Linear diffusion models; Oxidative dissolution; Temperature conditions; Transpassive dissolution; Voltammetric response; Solution mining",2-s2.0-85024115153
"Sountharrajan S., Karthiga M., Suganya E., Rajan C.","Automatic classification on bio medical prognosisof invasive breast cancer",2017,"Asian Pacific Journal of Cancer Prevention",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029767561&doi=10.22034%2fAPJCP.2017.18.9.2541&partnerID=40&md5=a07793fa6774ee53a17cf33f64dddd1b","Breast Cancer one of the appalling diseases among the middle-aged women and it is a foremost threatening death possibility cancer in women throughout the world. Earlier prognosis and preclusion reduces the conceivability of death. The proposed system beseech various data mining techniques together with a real-time input data from a biosensor device to determine the disease development proportion. Surface acoustic waves (SAW) biosensor empowers a label-free, worthwhile and straight detection of HER-2/neu cancer biomarker. The output from the biosensor is fed into the proposed system as an input along with data collected from Winconsin dataset. The complete dataset are processed using data mining classification algorithms to predict the accuracy. The exactness of the proposed model is improved by ranking attributes by Ranker algorithm. The results of the proposed model are highly gifted with an accuracy of 79.25% with SVM classifier and an ROC area of 0.754 which is better than other existing systems. The results are used in designing the proper drug thereby improving the survivability of the patients.","Human epidermal growth receptor; Receiver operating curve; Support vector machine; Surface acoustic wave",,2-s2.0-85029767561
"Lakshmi Sudha K., Arun C.","DPRA-FC: Dynamic probabilistic rate allocation and flow control algorithm during congestion in Wireless Sensor Network",2017,"Journal of Computational and Theoretical Nanoscience",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027989367&doi=10.1166%2fjctn.2017.6731&partnerID=40&md5=65f3ae6ffdf325dfcdab2e7ac4238aba","Wireless Sensor Network emerges as a solution to replace the large network deployment. WSN grow due to the low power embedded systems and substitute the huge network infrastructure. However, the Data collection and data mining play a vital role in decision-making during congestion. In the existing WSN systems, congestion controls obtain through intra-clustering and Omnidirectional antenna. In modern congestion control, WSN applies with multi cross-layer techniques to avoid infrastructure cost and to increase the efficiency of packet delivery. In the proposed system, we apply probability control algorithm with three-tier logic to control congestion. The three tier logic works in between the MAC and Network layer for efficient congestion control through varying the flow control of data size and rate. The flow control of data size and rate maximize the throughput and packet delivery with low power consumption. Copyright © 2017 American Scientific Publishers All rights reserved.","Congestion controlling; Network throughput; Probabilistic delivered by ing ta to: elsevier bv cross layer coding; Wireless sensor network",,2-s2.0-85027989367
"Mevada V., Patel S., Pandya J., Joshi H., Patel R.","Whole genome sequencing and annotation of halophilic Salinicoccus sp. BAB 3246 isolated from the coastal region of Gujarat",2017,"Genomics Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021098377&doi=10.1016%2fj.gdata.2017.06.006&partnerID=40&md5=a9b2cc5f505ebb9994cd2e431c72a2fc","Salinicoccus sp. BAB 3246 is a halophilic bacterium isolated from a marine water sample collected from the coastal region of Gujarat, India, from a surface water stream. Based on 16sRNA sequencing, the organism was identified as Salinicoccus sp. BAB 3246 (Genebank ID: KF889285). The present work was performed to determine the whole genome sequence of the organism using Ion Torrent PGM platform followed by assembly using the CLC genomics workbench and genome annotation using RAST, BASys and MaGe. The complete genome sequence was 713,204 bp identified by with second largest size for Salinicoccus sp. reported in the NCBI genome database. A total of 652 degradative pathways were identified by KEGG map analysis. Comparative genomic analysis revealed Salinicoccus sp. BAB 3246 as most highly related to Salinicoccus halodurans H3B36. Data mining identified stress response genes and operator pathway for degradation of various environmental pollutants. Annotation data and analysis indicate potential use in pollution control in industrial influent and saline environment. © 2017",,"1 methylnaphthalene; 1,4 dichlorobenzene; 4,4' isopropylidenediphenol; anthracene; atrazine; benzoic acid; biphenyl; caprolactam; carbazole derivative; ethylbenzene; flestolol; fluorene; geraniol; hexachlorocyclohexane; limonene; lysine; naphthalene; pinene; RNA 16S; sea water; surface water; tetrachloroethylene; toluene; trinitrotoluene; xylene; Article; bacterial strain; bacterial virulence; carbohydrate synthesis; coastal plain; comparative genomic analysis; comparative genomic hybridization; contig mapping; controlled study; genome annotation; Gujarat; halophilic bacterium; membrane transport; molecular genetics; nonhuman; nucleotide sequence; pollution; priority journal; protein metabolism; RNA sequence; Salinicoccus; sequence analysis; sporogenesis; stress; whole genome sequencing",2-s2.0-85021098377
"Aleroud A., Karabatis G.","Contextual information fusion for intrusion detection: a survey and taxonomy",2017,"Knowledge and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013083663&doi=10.1007%2fs10115-017-1027-3&partnerID=40&md5=0654d3444d1f52756ec6b949627e1524","Research in cyber-security has demonstrated that dealing with cyber-attacks is by no means an easy task. One particular limitation of existing research originates from the uncertainty of information that is gathered to discover attacks. This uncertainty is partly due to the lack of attack prediction models that utilize contextual information to analyze activities that target computer networks. The focus of this paper is a comprehensive review of data analytics paradigms for intrusion detection along with an overview of techniques that apply contextual information for intrusion detection. A new research taxonomy is introduced consisting of several dimensions of data mining techniques, which create attack prediction models. The survey reveals the need to use multiple categories of contextual information in a layered manner with consistent, coherent, and feasible evidence toward the correct prediction of cyber-attacks. © 2017, Springer-Verlag London.","Context; Contextual information; Cyber-security; Intrusion detection; Netflows; Semantics",,2-s2.0-85013083663
"Kumaraswamy S., Manjula S.H., Venugopal K.R.","Secure cloud based privacy preserving dataminning platform",2017,"Indonesian Journal of Electrical Engineering and Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031040681&doi=10.11591%2fijeecs.v7.i3.pp830-838&partnerID=40&md5=f526bffe9ec017c30e8832bbc9103acf","The adoption of cloud environment for various application uses has led to security and privacy concern of user’s data. To protect user data and privacy on such platform is an area of concern. Many cryptography strategy has been presented to provide secure sharing of resource on cloud platform. These methods tries to achieve a secure authentication strategy to realize feature such as self-blindable access tickets, group signatures, anonymous access tickets, minimal disclosure of tickets and revocation but each one varies in realization of these features. Each feature requires different cryptography mechanism for realization. Due to this it induces computation complexity which affects the deployment of these models in practical application. Most of these techniques are designed for a particular application environment and adopt public key cryptography which incurs high cost due to computation complexity. To address these issues this work present an secure and efficient privacy preserving of mining data on public cloud platform by adopting party and key based authentication strategy. The proposed SCPPDM (Secure Cloud Privacy Preserving Data Mining) is deployed on Microsoft azure cloud platform. Experiment is conducted to evaluate computation complexity. The outcome shows the proposed model achieves significant performance interm of computation overhead and cost. © 2017 Institute of Advanced Engineering and Science. All rights reserved.","Cloud computing; Content sharing; Cryptography; Privacy preseving",,2-s2.0-85031040681
"Xiao Q., Bertino E.","Detecting deceptive engagement in social media by temporal pattern analysis of user behaviors: a survey",2017,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020430138&doi=10.1002%2fwidm.1210&partnerID=40&md5=0a7622a57a645ced61cea48a19ac36ab","Deceptive engagement in social media, such as spamming, commenting, or rating with automatic scripts, spreading fabricated facts, seriously affects users’ trust on online services. Given the large volumes of information generated by users, effectively spotting users involved such deceptive engagement has become a challenging problem. Recent research has shown that techniques for analyzing temporal behavioral patterns are critical to address such problem. In this study, we survey recent advances in these techniques. We first summarize three approaches to model temporal information. Then, by using representative application examples, we discuss recent approaches with respect to their applications to real-world large-scale social media. With a focus on the temporal perspective, we then discuss advantages and challenges of each approach. WIREs Data Mining Knowl Discov 2017, 7:e1210. doi: 10.1002/widm.1210. For further resources related to this article, please visit the WIREs website. © 2017 John Wiley & Sons, Ltd",,"Behavioral research; Surveys; Application examples; Behavioral patterns; Large volumes; On-line service; Recent researches; Temporal information; Temporal pattern; User behaviors; Social networking (online)",2-s2.0-85020430138
"Petrescu M., Lauer B.","Qualitative marketing research: The state of journal publications",2017,"Qualitative Report",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028932626&partnerID=40&md5=3fb763673e7040b004563cf9260f0558","Qualitative methods in marketing have become essential not only for their classical advantage in consumer behavior, but also for their benefits in dealing with big data and data Qualitative methods in marketing have become essential not only for their classical advantage in consumer behavior, but also for their benefits in dealing with big data and data mining. Research from International Data Corporation (IDC) shows that when it comes to online data, unstructured content accounts for 90% of all digital information. Under these circumstances, this study provides a literature review and analysis on the role and relation of qualitative methods with quantitative methods in marketing research. The paper analyzes research articles that include qualitative studies in the top marketing journals during the last decade and focuses on their topic, domain, methods used and whether they used any triangulation with quantitative methods. Starting from this analysis, the study provides recommendations that can help better integrate qualitative methods in marketing research, academics and practice.mining. Research from International Data Corporation (IDC) shows that when it comes to online data, unstructured content accounts for 90% of all digital information. Under these circumstances, this study provides a literature review and analysis on the role and relation of qualitative methods with quantitative methods in marketing research. The paper analyzes research articles that include qualitative studies in the top marketing journals during the last decade and focuses on their topic, domain, methods used and whether they used any triangulation with quantitative methods. Starting from this analysis, the study provides recommendations that can help better integrate qualitative methods in marketing research, academics and practice. © 2017, Nova Southeastern University. All rights reserved.","Marketing; Qualitative Analysis; Triangulation",,2-s2.0-85028932626
